## Section 11. Fairness &amp; NonDiscrimination

## Objective:

To (a) identify and mitigate risk of disproportionately unfavorable Outcomes for protected (Sub)populations; and (b) minimise the unequal distribution of Product and Model errors to prevent reinforcing and/or deriving social inequalities and/or ills, and (c) promote compliance with existing anti-discrimination laws and statutes.

## What do we mean when we refer to Fairness?

Fairness is a complex socio-technical challenge for which there is no single generic definition. Broadly speaking -

Fairness is about identifying bias in a machine learning Model or Product and mitigating discrimination with respect to sensitive, and (usually) legally protected attributes such as ethnicity, gender, age, religion, disability, or sexual orientation.

Algorithmic discrimination can take many forms and may occur unintentionally. Machine learning Products might unfairly allocate opportunities, resources, or information, and they might fail to provide the same quality of service to some people as they do to others.

The conversation about fairness distinguishes between group fairness and individual fairness measures. Group fairness ensures some form of statistical parity (e.g. equal calibration, equal false positive/negative rate) across protected groups. Individual fairness requires that individuals who are similar with respect to the predictive task be assigned similar outcomes regardless of the sensitive attribute.

## Why is Fairness relevant?

Machine learning Products are increasingly used to inform high-stakes decisions that impact people's lives.It is therefore important that ML-driven decisions do not reflect discriminatory behavior toward certain populations. It is the responsibility of data science practitioners and business leaders to design machine learning Products that minimizes bias and promotes inclusive representation.

Some business leaders express concerns about a potential increase in the risk of reputational damage and legal allegations in case of discriminatory 'black box' Models. AI fairness can substantially reduce these concerns. Another reason for taking AI fairness seriously is the development of regulatory frameworks for AI. For example, the European Commission published a white paper on AI in 2020, which was followed in 2021 by a regulatory framework proposal for AI in the European Union.

## How to apply Fairness?

Fairness should be considered throughout the product lifecycle. Given that AI systems are usually designed to evolve with experience, fairness should be closely monitored during deployment as well as during product development. The Technical Best Practices Guidelines provide detailed guidance into implementing fairness in your AI products.

## 11.1 Product Definitions

## Objective

To (a) identify and mitigate risk of disproportionately unfavorable Outcomes for protected (Sub)populations; and (b) minimise the unequal distribution of Product and Model errors to prevent reinforcing and/or deriving social inequalities and/or ills, and (c) promote compliance with existing anti-discrimination laws and statutes.

|         |                                                     | Control:                                                                                                                                                                                                                                                                                                                                                                                       | Aim:                                                                                                                                                                                                                        |
|---------|-----------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 11.1.1. | (Sub)populations Definition                         | Define (Sub)populations that are subject to Fairness concern, with input from Domain and/or legal experts when relevant.                                                                                                                                                                                                                                                                       | To (a) ensure that vulnerable and affected populations are appropriately identified in all subsequent Fairness testing and Model build; and (b) highlight associated risks that might occur in the Product Lifecycle.       |
| 11.1.2. | (Sub)population Data                                | Gather data on (Sub)population membership. If a proxy approach is used, ensure the performance of the proxy is adequate in this context.                                                                                                                                                                                                                                                       | To (a) facilitate Fairness testing pre- and post-Model deployment; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                            |
| 11.1.3. | (Sub)population Outcome Perceptions                 | Document and assess whether scored (Sub)populations would view Model Outcomes as favorable or not, using input from subject matter experts and stakeholders in affected (Sub)populations. Document and assess any divergent views amongst (Sub)populations.                                                                                                                                    | To (a) ensure uniformity in (Sub) population outcome perception, if applicable; (b) highlight Outcome effects for different (Sub)populations; and (c) highlight associated risks that might occur in the Product Lifecycle. |
| 11.1.4. | Erroneous Outcome Consequence Estimation Divergence | Document and assess the results of erroneous (false positive & false negative) outcome consequences, both real and perceived, specifically in terms of divergence between relevant (Sub) populations. If material divergence present, take measures to harmonise Outcome perceptions and/or mitigate erroneous Outcome consequences in Model design, exploration, development, and production. | To (a) ensure uniformity in erroneous Outcomes for (Sub) populations; (b) highlight outcome effects for different (Sub) populations; and (c) highlight associated risks that might occur in the Product Lifecycle.          |
| 11.1.5. | Positive Outcome Spread                             | Document and assess the degree to which Model positive outcomes can be distributed to non-scored (Sub)population, when contextually appropriate. If present, take measures to promote Model Outcome distribution in Model design, exploration, development, and production.                                                                                                                    | To (a) ensure the non-prejudicial spread of positive Model Outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                          |

| 11.1.6.   | Enduring Bias Estimation     | Document and assess whether exclusions from Product usage might perpetuate pre-existing societal inequalities between (Sub)populations. If present, take measures to mitigate societal inequalities perpetuation in Model design, exploration, development, and production.   | To (a) ensure the non-prejudicial spread of Model Outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                       |
|-----------|------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 11.1.7.   | Appropriate Fairness Metrics | Consult Domain experts to inform which Fairness metrics are contextually most appropriate for the Model when conducting Fairness testing.                                                                                                                                     | To (a) ensure that fairness testing and subsequent Model changes (i) result in outcome changes which are relevant for (Sub)populations; and/or (ii) are consistent with regulatory guidance and context- specific best practices; and (b) highlight associated risks that might occur in the Product Lifecycle. |
| 11.1.8.   | Model Implications           | Document and assess the downside risks of Model misclassification/inaccuracy for modeled populations. Use the relative severity of these risks to inform the choice of Fairness metrics.                                                                                      | To (a) ensure that improving in the chosen Fairness metrics achieves the greatest Fairness in Model decisioning after deployed; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                   |
| 11.1.9.   | Fairness Testing Approach    | Document and assess the Fairness testing methodologies that will be applied to Model and/or candidate Models, along with any applicable thresholds for statistical/ practical significance, acceptable performance loss tolerance, amongst other metrics.                     | To (a) prevent Fairness testing methodology and associated thresholds change during Model review; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                 |

## 11.2 Exploraition

## Objective

To identify and control for Fairness and Non-Discrimination risks based on the available datasets.

|         |                             | Control:                                                                                                                                                                                                                                                                            | Aim:                                                                                                                |
|---------|-----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|
| 11.2.1. | (Sub)population Data Access | Keep separate Model development data and (Sub)population membership data (if applicable Regulations allow the possession and processing of such in the first place), especially if the use of (Sub)population data in the Model is prohibited or would introduce fairness concerns. | To (a) guarantee that (Sub) population membership data does not inadvertently leak into a Model during development. |

| 11.2.2.             | Univariate Assessments                    | Document and perform univariate assessments of relationship between (Sub)populations and Model input Features, including appropriate correlation statistics.                                                                                                                                                                                                                                                                                                                                                                                                       | To (a) identify input Feature trends associated with (Sub)populations; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                           |
|---------------------|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 11.2.3.             | Prohibited Data Sources                   | Develop and maintain an index of data sources or features that should not be made available or utilized because of the risks of harming (Sub)populations, specifically Protected Classes.                                                                                                                                                                                                                                                                                                                                                                          | To (a) prohibit the actioning of data sources that will disproportionately prejudice (Sub)populations; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                           |
| 11.2.4.             | Data Representativeness                   | Ensure the membership rates of (Sub) populations in Model development data align with expectations and that data is representative of Domain populations.                                                                                                                                                                                                                                                                                                                                                                                                          | To (a) guarantee that Model performance and Fairness testing during model development will provide a consistent picture of Model performance after deployment; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                   |
| 11.2.5.             | (Sub)population Proxies and Relationships | Document and assess the relationship between potential input Features and (membership of) (Sub)populations of interest based on, amongst other things, (i) reviews with diverse Domain experts, (ii) explicit encoding of (Sub) population membership, (iii) correlation analyses, (iv) visualization methods. If relationships exist, the concerned input Features should be excluded from Model datasets, unless a convincing case can be made that an (adapted version of) the input Feature will not adversely affect any (Sub)populations, and document this. | To (a) prevent Model decisions based directly or indirectly on protected attributes or protected class membership; (b) reduce the risk of Model bias against relevant (Sub)populations; (c) understand any differences in data distributions across (Sub) populations before development begins; and (c) highlight associated risks that might occur in the Product Lifecycle. |
| Associated Controls | Associated Controls                       | Review the following controls with particular attention in the context of bias and fairness with respect to protected (Sub)populations: Section 12.2.2. - Missing and Bad Data Assessment. Section 13.2.4. - Selection Function; which is concerned with accurate representation of (Sub)populations.                                                                                                                                                                                                                                                              | Review the following controls with particular attention in the context of bias and fairness with respect to protected (Sub)populations: Section 12.2.2. - Missing and Bad Data Assessment. Section 13.2.4. - Selection Function; which is concerned with accurate representation of (Sub)populations.                                                                          |

## 11.3. Development

## Objective

To minimise the unequal distribution of Product and Model errors for (Sub)populations during Model  development in the most appropriate manner.

|         |                                               | Control:                                                                                                                                                                                                                                                                                                                                                                                                              | Aim:                                                                                                                                                                                                                                                                                                                                                                                  |
|---------|-----------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 11.3.1. | Explainability (xAI) (Sub)population Outcomes | Keep separate Model development data and (Sub)population membership data (if applicable Regulations allow the possession and processing of such in the first place), especially if the use of (Sub) population data in the Model is prohibited or would introduce fairness concerns.                                                                                                                                  | To (a) guarantee that (Sub) population membership data does not inadvertently leak into a Model during development.                                                                                                                                                                                                                                                                   |
| 11.3.2. | Model Architecture and Interpretability       | Choose Model architecture that maximizes interpretability and identification of causes of unfairness. Consider different methodologies within the same Model architecture (ex. monotonic XGBoost, explainable neural networks). Evaluate whether Product Aims can be accomplished with a more interpretable Model.                                                                                                    | To (a) provide information that can guide Model-builders; (b) ensure that Model decisions are made in line with expectations; (c) allow Product Subjects and/or End Users to understand why they received corresponding Outcomes; (d) help inform the causes of Fairness issues if issues are detected; and (e) highlight associated risks that might occur in the Product Lifecycle. |
| 11.3.3. | Fairness Testing of Outcomes                  | Focus fairness testing initially on outcomes that are immediately experienced by (Sub)populations. For example, if a model uses a series of sub-Models to generate a score and a threshold is applied to that score to determine an Outcome, focus on Fairness issues related to that Outcome. If issues are identified, then diagnose the issue by moving 'up-the-chain' and testing the Model score and sub-Models. | To (a) ensure that the testing performed best reflects what will happen when Models are deployed in the real world; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                     |
| 11.3.4. | Disparate Impact Testing                      | If applicable, test Model(s) for disparate impact. Evaluate whether Model(s) predict a Positive Outcome at the same rate across (Sub)populations.                                                                                                                                                                                                                                                                     | To (a) ensure that (Sub)population members are receiving the Positive Outcome as often as their peers; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                  |
| 11.3.5. | Equalized Opportunity Testing                 | If applicable, test Model(s) for equalized opportunity. Evaluate whether Model(s) predict a Positive Outcome for (Sub) population members that are actually in the positive class at the same rates as across (Sub)populations.                                                                                                                                                                                       | To (a) ensure that (Sub)population members who should receive the Positive Outcome are receiving the Positive Outcome as often as their peers; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                          |

| 11.3.6.   | Equalized Odds Testing                       | If applicable, test Model(s) for equalized odds. Evaluate whether Model(s) predict a Positive & Negative Outcome for (Sub) population members that are actually in the positive & negative class respectively at the same rates across (Sub)populations.   | To (a) ensure that (i) protected (Sub)populations who should receive the Positive Outcome are receiving the Positive Outcome as often as other (Sub)populations, and (ii) protected (Sub)populations who should not receive the Positive Outcome are not receiving the Positive Outcome as often as other (Sub)populations; and (b) highlight associated risks that might occur in the Product Lifecycle.   |
|-----------|----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 11.3.7.   | Conditional Statistical Parity Testing       | If applicable, test Model(s) for conditional statistical parity. Evaluate whether Model(s) predict a Positive Outcome at the same rate across (Sub)populations given some predefined set of 'legitimate explanatory factors'.                              | To (a) ensure that (Sub)populations members are receiving the Positive Outcome just as often as (Sub) populations with similar underlying characteristics; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                    |
| 11.3.8.   | Calibration Testing Across (Sub) populations | If applicable, test Model(s) for calibration. Evaluate whether (Sub)populations members with the same predicted Outcome have an equal probability of actually being in the positive class.                                                                 | To (a) ensure that Subpopulations each have the same likelihood of deserving the Positive Outcome for a given Model prediction; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                               |
| 11.3.9.   | Differential Validity Testing                | If applicable, test Model(s) for differential validity. Evaluate whether Model performance varies meaningfully by (Sub) population, with a special focus on any groups that are underrepresented in modelling data.                                        | To (a) ensure that the Model's predictive abilities aren't isolated in or concentrated to (Sub)population members; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                            |
| 11.3.10.  | Feature Selection Fairness Review            | Evaluate the impact of removing or modifying potentially problematic input Features on Fairness metrics and Model quality.                                                                                                                                 | To (a) assess whether more fair alternative Models can be made that fulfill Model objectives; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                                 |
| 11.3.11.  | Modeling Methodology Fairness Review         | Evaluate the impact of changing Modelling methodology choices (f.e. algorithm, segmentation, hyperparameters, etc.) on Fairness metrics and Model quality.                                                                                                 | To (a) assess whether more fair alternative Models can be made that fulfill the Model objectives; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                             |

## 11.4 Production

## Objective

To maintain operationalised Fairness at the level established during Model Development.

|         |                                | Control:                                                                                                                                                                                                                                                                                                                                                                                                | Aim:                                                                                                                                                                                                            |
|---------|--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 11.4.1. | Domain Population Stability    | Continually assess the stability of the Domain population being scored, both in terms of its composition relative to the Model development population, and the quality of the Model by class.                                                                                                                                                                                                           | To (a) ensure the continued accuracy of Fairness tests and metrics; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                               |
| 11.4.2. | Fairness Testing Schedule      | Define a policy for timing of re- assessment of Model fairness that includes re-testing at regular intervals and/or established trigger events (e.g. any modifications to Model inputs or structure, changes to the composition of the modeled population, impactful policy changes).                                                                                                                   | To (a) detect issues with Model Fairness that may not have existed during pre-deployment of the Model; and (b) highlight associated risks that might occur in the Product Lifecycle.                            |
| 11.4.3. | Input Data Transparency        | Ensure that Product Subjects have the ability to observe attributes relied on in the modeling decision and correct inaccuracy. Collect data around this process and use it to identify issues in the data sourcing/aggregation pipeline.                                                                                                                                                                | To (a) ensure that the Model is making decisions on accurate data; (b) learn whether there are problems with Model's data assets; and (c) highlight associated risks that might occur in the Product Lifecycle. |
| 11.4.4. | Feature Attribution            | Ensure that Product Subjects can understand why the Model made the decision it did, or how the Model output contributed to the decision. Ideally, an understanding would include which features were most important in the decision and give some guidance as to how the subject could improve in the eyes of the Model. (See Section 13 - Representativeness & Specification for further information.) | To (a) ensure that Product Subjects (i) have some level of trust/ understanding in the Model that affect them and (ii) feel that they have agency over the process and that Model Outcomes are not arbitrary.   |
| 11.4.5. | Product Subject Appeal Process | Incorporate a 'right of appeal' procedure into the Model's deployment, where Product Subjects can request a human review of the modeling decision. Collect data around this process and use it to inform Model design choices.                                                                                                                                                                          | To (a) ensure that Product Subjects are, at a minimum, made aware of the results of Model decisions; and (b) allow inaccurate predictions to be corrected.                                                      |
| 11.4.6. | Feature attribution Monitoring | As part of regularly scheduled review, or more frequently, monitor any changes in feature attribution or other explainable metric by sub-population. (See Section 15 - Monitoring & Maintenance for further information.                                                                                                                                                                                | To (a) detect reasons for changes in Model performance, as well as any changes earlier in the data pipeline; and (b) highlight associated risks that might occur in the Product Lifecycle.                      |

## Section 12. Data Quality

## Objective:

To ensure Data Quality and prevent unintentional effects, changes and/or deviations in Product and Model outputs associated with poor Product data.

## What is Data Quality?

Like many other concepts in Machine learning and data science, Data quality is something without a single and widely accepted definition. Nonetheless, we think of -

Data Quality as data which is fit for use for its intended purpose and satisfies business, system and technical requirements.

In technical terms, data quality can be a measure of its completeness, accuracy, consistency, reliability and whether it is up-to-date.

Data integrity is sometimes used interchangeably with data quality. However, data integrity is a broader concept than data quality and can encompass data quality, data governance and data protection.

## Why is Data quality important?

It is not difficult for all stakeholders involved in a Project to agree that good data quality is of prime importance. Bad data quality means a business or an organization may not have a good grasp on whether they are successful in meeting prior set objectives or not. Bad data quality results in poor analytical solutions, wrong insights and conclusions. This translates into inadequate response to market opportunities, an inability to timely react to customers' requests, increased costs, and last, but not least, potential shortcomings in meeting compliance requirements. In short, poor data results in poor products and poor decisions. This is undesirable.

## The How of Data quality

Data quality is something that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage in isolation.

## 12.1 Exploration

## Objective

To determine if the quality of the data shall be sufficient, or can be made sufficient, to achieve the Product Definitions.

|         |                                       | Control:                                                                                                                                                                                                                                                                                                                                                                                                     | Aim:                                                                                                                                                                                                                                                                                                        |
|---------|---------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 12.1.1. | Data Definitions                      | Document and ensure all subtleties of definitions of all data dimensions are clear, inclusive of but not limited to gathering methods, allowed values, collection frequency, etc. If not, acquire such knowledge, or discard the dimension.                                                                                                                                                                  | To (a) assess and prevent unjustified assumptions about the meaning of a data dimension or its values; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                        |
| 12.1.2. | Data Modeling                         | Document and ensure all relationships between (the fields of) different datasets are clear, in the light of their Data Definitions. (See Section 12.1.1 - Data Definitions for further information.) If this 'Data Model' is not clear or available, create it, or discard the datasets.                                                                                                                     | To (a) prevent the creation and/or combination of invalid datasets; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                           |
| 12.1.3. | Missing and Bad Data Assessment       | Document and assess (a) the occurrence rates and (b) co-variances of missing values and nonsensical values throughout the Model data. If either is significant, investigate causes and consider discarding affected data dimension(s) or commit dedicated research and development to mitigating measures for affected data dimension(s). (See Section 12.3.1. - Live Data Quality for further information.) | To assess (a) the risk of low quality data introducing bias to Model data and/or Outcomes; and (b) whether Model dataset(s) quality is sufficient for Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle.                                                    |
| 12.1.4. | Data Veracity Uncertainty & Precision | Document and assess the veracity and precision of data. If compromised, uncertain and/or unknown, document and assess (i) the causes and sources hereof and (ii) statistical accuracy .Incorporate appropriate statistical handling procedures, such as calibration, and appropriate control mechanisms in Model, or discard the data dimension.                                                             | To assess (a) the risk of low quality data introducing bias to Model data and/or outcomes; (b) a priori the plausibly achievable performance; (c) whether the Model dataset(s) quality is sufficient for Product Definitions; and (d) highlight associated risks that might occur in the Product Lifecycle. |

## 12.2 Development

## Objective

To determine if Model performance is affected or biased due to data quality issues.

|         |                               | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Aim:                                                                                                                                                                                                                                       |
|---------|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 12.2.1. | Missing and Bad Data Handling | Document and assess how missing and nonsensical data (a) are handled in the Model, through datapoint exclusion or data imputation; (b) affect the Selection Function through datapoint removal; (c) affect Model performance and Fairness for subpopulations through data imputation. If (Sub)populations are unequally affected, take additional measures to increase data quality and/or improve Model resilience. Consult Domain experts during assessment and mitigation. | To (a) prevent introducing bias to Model Outcomes due to low quality data; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                   |
| 12.2.2. | Error - Quality Correlation   | Document and assess whether low-quality datapoints (those with low-confidence, uncertain, nonsensical, missing and/or imputed attributes) correlate with high (rates of) error, and how this affects (Sub)populations. If so, take additional measures to increase data quality and/or improve Model performance for specific (Sub)populations.                                                                                                                               | To (a) prevent introducing bias to Model Outcomes due to low quality data; (b) whether the Model dataset(s) quality is sufficient for Product Definition(s); and (c) highlight associated risks that might occur in the Product Lifecycle. |

## 12.3 Production

## Objective

To ensure the quality of incoming data to the Product during operations.

|         |                   | Control:                                                                                                                                                                                                                                                                                                                                                                  | Aim:                                                                                                                                                                                  |
|---------|-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 12.3.1. | Live Data Quality | Document and assess whether live incoming data with low quality (low- confidence, uncertain, nonsensical, missing and/or imputed attributes) can be handled appropriately by the Model on the per-Data Subject level. If not, implement additional measures, and/or re-assess validity of Product Definition(s) in view of non-applicability to low quality live subsets. | To (a) assess and control that all Product Subjects can be supported appropriately by the live Product; and (b) highlight associated risks that might occur in the Product Lifecycle. |

## Section 13. Representativeness &amp; Specification

## Objective:

To (a) ensure that Product data and Model(s) are representative of, and accurately specified for, Product Domain as far as is reasonably practical; and (b) guard against unintentional Product and Model behaviour and Outcomes as far as is reasonably practical.

## What is Representativeness and Specification?

Representativeness is a concept that is often used in statistics and machine learning with regards to the data we use to train a Model. A representative data sample is a set from a larger statistical population that adequately replicates the larger group according to a characteristic or quality under study. Put less metaphorically -

Representativeness means the ability of the Model and its data to adequately replicate and represent that characteristics of its operational environment.

It should not be confused with representation learning (also known as feature learning) in machine learning. The latter refers to a set of techniques for automatically detecting feature patterns and in fact replaces manual feature engineering.

Specification is a less known term. In our context -

Specification refers to ensuring the appropriate degrees of freedom in the Model.

For example, we have selected the appropriate cost function for the problem at hand, the target variable is appropriate and not a proxy for what we are really interested in measuring, etc. It is like representativeness but for the Model, and not the data. Unlike the performance robustness section, many of the controls here will be difficult to precisely measure quantitatively. However, we should still try to consider as many scenarios as possible and minimize all risks stemming from not addressing them rigorously.

## Why is Representativeness and Specification important?

If the data is not representative with relation to the goal of the Product, it will not serve us well. It will result in poor performing Models when deployed, and it will inherently contain bias (not in the fairness and nondiscrimination sense but in relation to sampling). This can lead to misleading conclusions and unrealistic assumptions and expectations. Correct specifications on the other hand relates to selecting appropriate and rigorous features, selection function, and target, etc. This ensures that the Model we develop is rigorous, robust and has a properly specified number of parameters.

## The How of Representativeness and Specification

Representativeness and Specification is something that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage in isolation.

## 13.1 Product Definitions

## Objective

To (a) ensure the pragmatic formulation and accurate specification of Product Definition(s); (b) minimise Model simplifications, assumptions and ambiguities; and (c) ensure adequate vigil of the non-reducible ones throughout the Product Lifecycle.

|         |                                      | Control:                                                                                                                                                                                               | Aim:                                                                                                                                                                                                            |
|---------|--------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 13.1.1. | R&S Product Definition(s) Assessment | Document and assess whether recorded Product Definition(s) are complete, unambiguous and representative of intended Product Outcomes. If they are not, refine them as much as is reasonably practical. | To (a) enable reliable execution of all further research, development and assessments; and (b) highlight associated risks that might occur in the Product Lifecycle.                                            |
| 13.1.2. | Product Assumptions                  | Document and assess Product assumptions, the likelihood of their appropriateness, their continued validity, and inherent risks.                                                                        | To (a) detect, mitigate and review Product assumptions and their inherent risks; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                  |
| 13.1.3. | Product Simplifications              | Document and assess Product simplifications, the likelihood of their appropriateness, and their inherent risks.                                                                                        | To (a) detect, mitigate and review Product simplifications and their inherent risks; and (b) highlight associated risks that might occur in the Product Lifecycle.                                              |
| 13.1.4. | Product Limits                       | Document and assess the limitations of the Product's application and the applicability of Product Definitions.                                                                                         | To (a) detect and review Model limitations in light of (i) Model assumptions and (ii) Model simplifications; and (b) highlight associated risks that might occur in the Product Lifecycle.                      |
| 13.1.5. | R&S Problem Definition Review        | R&S Product Definition(s) ought to be reviewed continually, specifically when significant Model changes occur.                                                                                         | To ensure that R&S Product Definition(s) are kept up-to- date to ensure their continued effectiveness, suitability, and accuracy; and (b) highlight associated risks that might occur in the Product Lifecycle. |

## 13.2 Exploration

## Objective

To (a) ensure that Model dataset(s) correspond to the Product Definition in sufficient detail, completeness and without material unambiguity; and (b) to identify associated risks in order to ensure an adequate vigil throughout the Product Lifecycle.

|         |                                       | Control:                                                                                                                                                                                                                                                                                                                                                                                                | Aim:                                                                                                                                                                                                                                                                                  |
|---------|---------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 13.2.1. | Data Subjectivity                     | Document and assess whether the Model dataset(s) contain subjective components. If subjective components are present, take measures to handle or avoid subjectivity risks in Product and/ or Model design as much as is reasonably practical.                                                                                                                                                           | To (a) assess and control for the accuracy of the specification of Model inputs, manipulations, Outcomes, and interpretations to ensure the unambiguous applicability of Model(s) in Product Domain(s); and (b) highlight associated risks that might occur in the Product Lifecycle. |
| 13.2.2. | Heterogeneous Variable Simplification | Document and assess whether Model datasets contain, or Model components produce, simplified input Features that represent inherently heterogeneous concepts in Product Domains. If simplified, take measures to reflect the heterogeneity of Product Domains as much as is reasonably practical.                                                                                                        | To (a) detect, review and control for the simplification of heterogeneous input Variables; (b) prevent generalization and spurious correlation; and (c) highlight associated risks that might occur in the Product Lifecycle.                                                         |
| 13.2.3. | Hidden Variables                      | Document and assess whether Model datasets are missing, or Model components hide relevant attributes of Product Subjects or systemic Variables with respect to Product Domains. If hidden, obtain additional data and/ or account for the hidden Variables in modelling as much as is reasonably practical.                                                                                             | To (a) assess and control for hidden correlations and causal relations in Model datasets and Variables and/ or risks of relations being spurious, ambiguous and/or confounding; and (b) highlight associated risks that might occur in the Product Lifecycle.                         |
| 13.2.4. | Selection Function                    | Document and assess the propensity of subpopulations and subpopulation members to be (accurately) recorded in Model datasets, with particular care for (i) unrecorded individuals, (ii) Protected Classes, and (iii) survivorship effects. Incorporate the Selection Function in Model development and evaluation in particular during Fairness & Non- Discrimination, Performance Robustness controls. | To (a) assess and control for the accuracy of Model and Model datasets in representing (Sub) populations; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                               |

| 13.2.5.   | Feature Constraints   | Evaluate whether any constraints should be applied to input Features, such as monotonicity or constraints on input Feature interactions in consultation with Domain experts. If determined, utilise identified constraints.   | To (a) ensure that (i) Model Outcomes are maximally interpretable and (ii) Model behavior for individual Model Subjects is consistent with Domain experience; and (b) highlight associated risks that might occur in the Product Lifecycle.   |
|-----------|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|

## 13.3 Development

## Objective

To (a) ensure that Model design is sufficiently specified to represent Product Domain(s) and the Product Definition(s) as much as is reasonably practical; and (b) minimise the risks of (i) adverse effects from the Model's optimisation leading to unintended loopholes and local optima, and (ii) mis-balancing competing optimisation requirements in Model design and development.

|         |                                                      | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                                                                                                           |
|---------|------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 13.3.1. | Target Subjectivity                                  | Document and assess whether the Target Feature(s) objectively represent Product Domain(s). If subjective, consider refining Product Definition(s), choosing a different Target Feature, or taking measures to promote the objectivity of Product Outcomes.                                                                                                                                                                                                      | To (a) ensure that Product Outcomes are representative of subpopulations and applications, and are not misinterpreted; (b) ensure that Models are optimized only and precisely according to Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle. |
| 13.3.2. | Target Proxies                                       | Document and assess whether the Target Feature(s) are proxies for the true Target(s) of Interest in Product Domain(s). If Target Features are proxies, take measures to ensure and review non- divergence of Product Outcomes with regard to Product Definitions.                                                                                                                                                                                               | To (a) ensure that Product Outcomes are representative of subpopulations and applications, and are not misinterpreted; (b) ensure that Models are optimized only and precisely according to Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle. |
| 13.3.3. | Target Proxy vs. True Target of Interest Contrasting | If the Target Feature is a proxy (i) document and assess whether the true Target(s) of Interest correlate with protected attributes and classes, including through hidden systemic Variables as much as is reasonably practical; and (ii) document and assess whether the true Target(s) of Interest and the proxy Target Feature(s) correlate differently with the Model datasets. If true, take measures to mitigate this as much as is reasonably practical. | To (a) ensure that the Model design is oriented to the true Target(s) of Interest; and (b) highlight associated risks that might occur in the lack thereof in the Product Lifecycle.                                                                                                           |

| 13.3.4.   | Heterogeneous Target Variable Simplification   | Document and assess whether the Target Feature is a simplification of, or contains a subset of, true Target(s) of Interest. If true, consider refining Product Definitions, recovering the heterogeneity, or failing that, take measures to mitigate and review this as much as is reasonably practical.                                                                                                                                                                                                                                                                  | Idem Section 11.3.1-2; and to (a) detect and control for risks of generalization and spurious correlation creation.                                                                                                                                                                                         |
|-----------|------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 13.3.5.   | Cost Function Specification & Optimisation     | Document and assess the risk propensity for - (i) optimizing for subset of objectives to the detriment of other Product objectives, (ii) optimizing for Outcomes that are unintended and/or not aligned with any Product objectives, (iii) feedback loops (when containing nested optimization loops), and (iv) Model confinement in adverse or less- than-optimal parameter or solution space - through Model cost function and optimisation procedures during the Product Lifecycle. If risks occur, take measures to mitigate them as much as is reasonably practical. | To (a) ensure the adequate optimisation of Product Definitions through an assessment of the cost function and optimization procedure; (b) to respect the boundary conditions and requirements set by the Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle. |
| 13.3.6.   | Importance Weighting                           | Document and assess whether Model data points are weighted by design or as collateral effect.                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | To (a) ensure the adequate optimisation of Product Definitions through an assessment of the cost function and optimization procedure; (b) to respect the boundary conditions and requirements set by the Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle  |
| 13.3.7.   | Asymmetric Error Weights                       | Document and assess whether Model errors, and error rates, are weighted asymmetrically in the Model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | To (a) ensure the adequate optimisation of Product Definitions through an assessment of the cost function and optimization procedure; (b) to respect the boundary conditions and requirements set by the Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle  |
| 13.3.8.   | Feature Weighting                              | Document and assess whether Model features are weighted by design or as collateral effect.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | To (a) ensure the adequate optimisation of Product Definitions through an assessment of the cost function and optimization procedure; (b) to respect the boundary conditions and requirements set by the Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle  |

| 13.3.9.   | Output Interpretation(s)     | Document and assess whether the interpretation of the Model Outcomes are clearly, completely and unambiguously defined. If not, take measures to promote Outcome interpretation(s) clarity and completeness as much as is reasonably practical.                    | To (a) guard against the misinterpretation and/or misapplication of Model Outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                 |
|-----------|------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 13.3.10.  | Time-dependent Data Modeling | Document and assess whether all time- dependent aspects of data generation (including but not limited to gathering, calibration, cleaning, and annotation), data modeling and data usage are specified and incorporated in Model design and Product Definition(s). | To (a) prevent data leakage and other forms of 'time traveling' information leading to inaccurate representations of the data and/or Data Subjects; and (b) highlight associated risks that might occur in the Product Lifecycle. |

## 13.4 Production

## Objective

To ensure that the Implementation of the Product and Model(s) align with and represent Product Definition(s) and Product Domain(s).

|         |                          | Control:                                                                                                                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                                                                                                                                                                                                         |
|---------|--------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 13.4.1. | Asymmetric Error Costs   | Document and assess whether Product Domain(s) costs produced by different Model errors types are accounted for in Product implementation and application in software and processes. If not, take measures to ensure that they are.                                                                                              | To (a) ensure that Product Domain(s) and Product Subjects consequences are accurately considered when implementing Product outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                           |
| 13.4.2. | Output Interpretation(s) | Document and assess whether Product Outcomes can be clearly, completely and unambiguously interpreted by the non-technical parties and whether these interpretations remain representative of Product Definition(s) and Model inner workings. If not, take measures to ensure that they are as much as is reasonably practical. | To (a) prevent (i) misinterpretation of Product Outcomes, (ii) the application of Products in contexts and/or to Subjects for which their appropriateness and/or quality is unconfirmed, unknown, and/or unsatisfactory, (iii) the intentional and/or unintentional misuse of Product components and Outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle. |

## Section 14. Performance Robustness

## Objective:

To warrant Model Outcomes and prevent unintentional Model behaviour a priori under operational conditions as far as is reasonably practical.

## What is Performance Robustness?

Model robustness is the property of an algorithm that, when tested on a training sample and on a similar testing sample, the performance is similar. In other words, a robust model is one for which the testing error is similar to the training error. Performance robustness takes into account prospective scenarios where (one of more) inputs or assumptions are (drastically) changed due to unforeseen circumstances and, in light of these, the ability of the Model to  to still consistently generate accurate  output.  Put more holistically -

Performance Robustness means the ability of the Model to generate consistent, accurate results across different sampling tests and in light of changes in operational circumstances.

## Why do we need Performance Robustness?

A Model that is not robust will hopefully not end up being used and deployed. Good performance in training, but significantly worse performance when tested on real data, is one of the reasons many proof-of-concepts do not end up being utilized. A Model which is not robust will inevitably deteriorate over time. Its predictions and recommendations will deviate from the ground truth and the end users will lose trust in the Model and may stop utilizing it altogether. This is the optimistic case. More worrisome is when users of the Model continue to use a poor performing Model and are unaware of its poor accuracy or precision, but still take it into account when making (important) judgement calls. In scenarios where there is no human-in-the-loop, detecting poor performance robustness can be even more difficult and time costly. This will result in more unknown harm, which is naturally hard to detect and determine.   So, it is clear that it is in everyone's interest to ensure the Model's performance robustness.

## How to ensure Performance Robustness?

Though performance robustness needs to be of a certain level to even consider deploying the Model, it is something that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage in isolation.

## 14.1 Product Definitions

## Objective

To prevent performance loss due to Product Definition changes.

|         |                                 | Control:                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                 |
|---------|---------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 14.1.1. | Product Definition(s) Stability | Document and assess the stability of historic and prospective Product Definition(s) and Product Aim(s). If unstable, take measures to redefine or, failing that, to correct for or mitigate as much as is reasonably practical. | To (a) ensure that Product Definition(s) and Models remain stable and up-to-date in light of Product Domain Stability; and (b) highlight associated risks that might occur in the Product Lifecycle. |
| 14.1.2. | Product Domain Stability        | Document and assess the stability of historic and prospective Product Domain(s). If unstable, revise Product Definition(s) accordingly to ensure Product consistency and stability.                                             | To (a) ensure that Product Definition(s) and Models remain stable and up-to-date in light of Product Domain Stability; and (b) highlight associated risks that might occur in the Product Lifecycle. |

## 14.2 Exploration

## Objective

To prevent performance loss due to (a) data and/or data definition instability; (b) volatile data elements; and/or (c) prospective increases in scale.

|         |                                    | Control:                                                                                                                                                                                                                                         | Aim:                                                                                                                                                                                                                                                                                                                                                                 |
|---------|------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 14.2.1. | Data Drift Assessment              | Document and assess historic and prospective changes in data distribution, inclusive of missing and nonsensical data. If data drift is apparent and/or expected in the future, implement mitigating measures as much as is reasonably practical. | To (a) assess and promote the stability of data distributions (data drift); (b) determine the need for data distributions monitoring, risk-based mitigation strategies and responses, drift resistance and adaptation simulations and optimization, and data distribution calibration; and (c) highlight associated risks that might occur in the Product Lifecycle. |
| 14.2.2. | Data Definition Temporal Stability | Document and assess - both technically and conceptually - historic and prospective changes of each data dimension definition. If unstable, consider refining Product Definitions and/or limiting usage of unstable data dimensions.              | To (a) assess and control for the need for Model design adaptation based on data definition stability; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                 |

| 14.2.3.   | Outlier Occurrence Rates                   | Document and assess outliers, their causes, and occurrence rates as a function of their location in data space. If numerous and persistent, include mitigating measures in Model design accordingly.                                                                                                                                                                  | To (a) identify outliers and assess the need for Model design adaptation; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                       |
|-----------|--------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 14.2.4.   | Selection Function Temporal Stability      | Document and assess the historic and prospective behaviour of Selection Function(s) of Model data. (See Section 13.2.4. - Selection Function for more information.) If unstable, take measures to account for past and future changes, and/or promote the consistency and representativeness of Model datasets and data gathering as much as is reasonably practical. | To (a) assess and control for hard-to-measure changes to the relation between Model datasets and Product Domain(s); (b) identify the risk of hard-to-diagnose Model performance degradation and bias throughout Product Lifecycle (to be controlled by 14.3.6); and (c) highlight associated risks that might occur in the Product Lifecycle. |
| 14.2.5.   | Data Generating Process Temporal Stability | Document and assess the historic and prospective behaviour of data generating processes, and their influence on the Selection Function. If unstable, take measures to account for past and future changes and/or promote the stability and consistency of data generation processes as much as is reasonably practical.                                               | To (a) assess and control for hard-to-measure changes to the relation between Model datasets and Product Domain(s); (b) identify the risk of hard-to-diagnose Model performance degradation and bias throughout Product Lifecycle (to be controlled by 14.3.6); and (c) highlight associated risks that might occur in the Product Lifecycle  |

## 14.3 Development

## Objective

To characterize, determine and control for Model performance variation, risks and robustness under live conditions a priori and throughout the Product Lifecycle.

|         |                                     | Control:                                                                                                                                                                                                                          | Aim:                                                                                                                                                                                                  |
|---------|-------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 14.3.1. | Target Feature Definition Stability | Document and assess - both technically and conceptually - the historic and prospective stability of the Target Feature definition. If unstable, consider refining Product Definitions and/or choosing a different Target Feature. | To (a) assess the need for Model design and Product Definition adaptation based on Target Feature definition stability; and (b) highlight associated risks that might occur in the Product Lifecycle. |

| 14.3.2.   | Blind Performance Validation               | Document and validate that Model Performance can always be reproduced on never-before-seen hold-out data- subsets and prove that these hold-out data-subsets are never used to guide Model and Product design choices by comparing Model performance on the hold-out dataset. If performance cannot be reproduced on never-before-seen hold-out data-subset, take measures to improve robustness and Model fitting as much as is reasonably practical.                                       | To (a) ensure Model performance robustness against insufficient generalization capabilities on live data (such as overfitting); and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                  |
|-----------|--------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 14.3.3.   | Error Distributions                        | Document and assess error and/or residual distributions along as many dimensions and/or subsets as is practically feasible. If distributions are too broad and/or too unequal between subsets, improve Model(s).                                                                                                                                                                                                                                                                             | To (a) assess and control for performance influence of data points and/or groups; (b) assess and control for the distribution of errors to influence - (i) performance robustness as a function of data drift, (ii) the systematic performance of minority data-subsets, and (iii) the risks of unacceptable errors and/or catastrophic failure; and (c) highlight associated risks that might occur in the Product Lifecycle. |
| 14.3.4.   | Output Edge Cases                          | Document and assess the causes, occurrence probabilities, overall performance impact of Edge Cases output by Model(s), inclusive of on Model training and design. If their influence is significant, improve model design. If occurrence is high, increase Model, code and data quality control.                                                                                                                                                                                             | To (a) assess and control for the impact of Output Edge Cases on Model design, bugs and performance; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                                             |
| 14.3.5.   | Performance Root Cause Analysis            | Document and assess Model performance Root Cause Analysis as well as its testing method. If Root Cause Analysis is ineffective, simplify Model and/or increase diagnostics like logging and tracking.                                                                                                                                                                                                                                                                                        | To (a) assess and control for Model performance changes and assist in Model design, development, and debugging; (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                                      |
| 14.3.6.   | Model Drift & Model Robustness Simulations | Document and perform simulations of Model training and retraining cycles, using historic and synthetic data. Document and assess the effects of temporal changes to, amongst other things, the Selection Function, Data Generating Process and Data Drift on the drift in performance and error distributions of said simulations. If Model drift is apparent, document and perform further simulations for Model drift response optimization, and/or consider refining Product Definitions. | To (a) assess and control for Model propensity for Model drift; (b) determine the robustness of Model performance as a function of data changes; (c) determine appropriate Product response to drift; and (d) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                            |

| 14.3.7.   | Catastrophic Failures                            | Document and assess the prevalence of predictions with High Confidence Values, but large Evaluation Errors. If apparent, improve Model to avoid these, and/or implement processes to mitigate these as much as is reasonably practical.                                                                                                                                                                                                                                                                  | To (a) assess the propensity of the Model for catastrophic failures; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                                   |
|-----------|--------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 14.3.8.   | Performance Uncertainty and Sensitivity Analysis | Document and assess the probability distribution of the model performance using cross-validation, statistical and simulation techniques under - (a) the assumption that the distribution of training and validation data is representative of the distribution of live data; and (b) multiple realistic variations to the Model data due to both statistical and contextual causes. If Model performance variation is high, improve Model and/or take measures to mitigate performance variation impact. | To (a) assess and control for the range of expected values of Model performance under both constant and changing conditions; (b) assess and control for whether trained model performance is consistent with these ranges; (c) identify main sources of uncertainty and variation for further control; and (d) highlight associated risks that might occur in the Product Lifecycle. |
| 14.3.9.   | Outlier Handling                                 | Document and assess the effect of various outlier handling procedures on (a) Performance Robustness and (b) Representativeness & Specification. Ensure that only procedures are implemented that positively affect both.                                                                                                                                                                                                                                                                                 | To (a) ensure that outlier removal is not used to heedlessly improve test-time performance only and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                        |

## 14.4 Production

## Objective

To ensure the future satisfaction of Product Definition(s) through the technical and functional implementation of the Product Model(s) and systems.

|         |                            | Control:                                                                                                                                                                                                                                                                             | Aim:                                                                                                                                                                                                  |
|---------|----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 14.4.1. | Real World Robustness      | Document and assess potential future change in the applied effects of the Product, such as through diminishing returns and/ or psychological effects. If significant change or decrease is expected, consider refining Product Definitions and/or develop procedures for mitigation. | To (a) assess and control for the variation in applied effects of the Product on Product Definition(s) and performance; and (b) highlight associated risks that might occur in the Product Lifecycle. |
| 14.4.2. | Performance Stress Testing | Perform and document experiments designed to attempt to induce failures in the Product and/or Model, for example, but not limited to, by supplying large quantities of or unusual data to the training or inferencing phases.                                                        | To (a) identify and control for risks associated with operational scenario's outside of regimes encountered during Model development.                                                                 |

## Section 15. Monitoring &amp; Maintenance

## Objective:

To ensure that Products and Models remain within acceptable operational bounds.

## What is Monitoring and maintenance?

Machine learning -

Monitoring refers to the processes of tracking and analysing the performance of a model over time and once it is deployed in production

It provides early warning signals for performance issues. Maintenance is closely related to monitoring but is a more actionable concept.

Maintenance relates to the activities we need to perform upon detecting or suspecting possible deterioration in the performance of the model.

Though it's a process closely related to Models in production, note that maintenance and monitoring steps need to be designed and addressed in early stages of the Product Lifecycle too.

## Why is Monitoring and maintenance important?

Monitoring and maintenance is not only important but it is a 'must have' for any Product that is deployed in a production environment. Over time, the 'live' data will differ in small or significant ways from the historical data used to train the Model. Trends and preferences will change too. The way certain data sources are measured and coded will also change over time: new data sources are added, while others become unavailable. Therefore, we need to continuously, real-time monitor the Models that are deployed. A Model that is not maintained or updated over time eventually deteriorates, makes errors and could lead to a loss of trust and varying degrees of harm (if the domain in question is  a high-stakes decision domain).

## The How of Monitoring and maintenance

Model monitoring and maintenance though most commonly discussed in the deployment phase, is something that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage in isolation.

## 15.1 Product Definitions

## Objective

To (a) track Model performance in production; and (b) ensure desired Model performance.

|         |                       | Control:                                                                                                                                                                                                                                                                                                               | Aim:                                                                                                             |
|---------|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|
| 15.1.1. | Monitoring Objectives | Based on Product Definition(s), document and assess Product and Model monitoring objectives, inclusive of which Product and/or Model elements need close monitoring attention, such as Model data and code. Document and assess the associated risks of failing to achieve Model and/or Product Monitoring Objectives. | To (a) define Product and Model monitoring objectives; and (b) highlight associated risks for failed monitoring. |
| 15.1.2. | Monitoring Risks      | Document and assess the associated risks of failing to achieve Monitoring Objectives.                                                                                                                                                                                                                                  | To (a) define Product and Model monitoring risks.                                                                |

## 15.2 Exploration

## Objective

To (a) define robust Product and/or Model monitoring requirements, inclusive of concerns related to Features and skews of the data; and (b) ensure the continued monitoring of Products and/or Models throughout their lifecycles.

|         |                                                               | Control:                                                                                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                                                                                                                                      |
|---------|---------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 15.2.1. | Data Source Mismatch: Training & Production Data              | Define and deploy methods to detect the degree to which data sources and Features, in Model training and production data, match one another. If mismatch is detected, take measures to ensure that data sources and Features are adequately matched in both Model training and production data. | To (a) reduce nonsensical predictions of the Model due to (i) missing data, (ii) lack of data incorporated, or (iii) data measurement scaling, encoding and/or meaning; (b) to reduce the discrepancy between training and production data; and (c) highlight associated risks that might occur in the Product Lifecycle. |
| 15.2.2. | Data Definitions and Measurements: Training & Production Data | Define and deploy methods by which to detect the degree to which data sources in Model training and production have the same definitions and measurement scales .                                                                                                                               | To (a) reduce nonsensical predictions of the Model due to (i) missing data, (ii) lack of data incorporated, or (iii) data measurement scaling, encoding and/or meaning; (b) to reduce the discrepancy between training and production data; and (c) highlight associated risks that might occur in the Product Lifecycle. |
| 15.2.3. | Data Dependencies and Upstream Changes                        | Derive and implement change assessments for changes in data due to - (i) one or multiple internal or external sources (partial) updates, (ii) substantial source change, and/or (iii) changes in data production and/or delivery.                                                               | To (a) reduce nonsensical predictions of the Model due to (i) missing data, (ii) lack of data incorporated, or (iii) data measurement scaling, encoding and/or meaning; (b) to reduce the discrepancy between training and production data; and (c) highlight associated risks that might occur in the Product Lifecycle. |

| 15.2.4.   | Data Drift Detection                                          | Define and deploy monitoring metrics and thresholds for detecting sudden and/or gradual, short term and/or long term changes in data distributions, giving priority to those that can detect past observed changes. (See Section 12.2.1- Missing and Bad Data Handling for further information). Document and assess distribution families, statistical moments, similarity measures, trends and seasonalities.   | To (a) prevent predictions from diverging from training data and/ or Product Definitions by assessing whether production data is representative of older data; and (b) highlight associated risks that might occur in the Product Lifecycle.                         |
|-----------|---------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 15.2.5.   | Product and/or Product Domain Changes: Trends and Preferences | Define and deploy (a) monitoring methods for detecting changes in Product Domain(s) and/or Product Definition(s); and (b) timeframes and/ or contextual triggers for reassessment of Product Domain(s) and Product Definition(s) continued stability.                                                                                                                                                             | To (a) ensure Models capture accurate, relevant, and current trends and preferences in Product Domain(s); (b) reduce Model 'blind spots' and better capture malicious events/attempts; and (c) highlight associated risks that might occur in the Product Lifecycle. |

## 15.3 Development

## Objective

To (a) create metrics for (i) Model performance and (ii) Model performance deterioration; and (b) ensure the continued monitoring of Products and/or Models throughout their lifecycles.

|         |                                                                | Control:                                                                                                                                                          | Aim:                                                                                                                                                                                                                                                                                                                                                                                     |
|---------|----------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 15.3.1. | Model Performance Deterioration Thresholds                     | Document, assess, and set thresholds for Model performance deterioration in consultation with Stakeholders.                                                       | To (a) ensure clear guidelines and indices of Model failure and performance deterioration; (b) reduce the risk of unacknowledged Model failure and performance deterioration; (c) reduce the likelihood of Model decay, ensure robustness and good performance in terms of selected metrics and scenarios; and (d) highlight associated risks that might occur in the Product Lifecycle. |
| 15.3.2. | Product Contextual Indicators: Model Performance Deterioration | Document, assess, and set Product and Product Domain specific indicators of Model performance deterioration, inclusive of technical and non-technical indicators. | To (a) ensure clear guidelines and indices of Model failure and performance deterioration; (b) reduce the risk of unacknowledged Model failure and performance deterioration; (c) reduce the likelihood of Model decay, ensure robustness and good performance in terms of selected metrics and scenarios; and (d) highlight associated risks that might occur in the Product Lifecycle. |

| 15.3.3.   | Reactive Model Maintenance Indicators   | Document, assess, and set thresholds for Model failure and reactive maintenance                                                                                                                          | To (a) ensure clear guidelines and indices of Model failure and performance deterioration; (b) reduce the risk of unacknowledged Model failure and performance deterioration; (c) reduce the likelihood of Model decay, ensure robustness and good performance in terms of selected metrics and scenarios; and (d) highlight associated risks that might occur in the Product Lifecycle.   |
|-----------|-----------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 15.3.4.   | Awareness of feedback loops             | Define and deploy as far as is reasonably practical (a) methods to detect whether feedback loops are occuring, and/or (b) technical and non-technical warning indicators for increased risk of the same. | As per Section 17 - Security: to prevent (in)direct adverse social and environmental effects as a consequence of self-reinforcing interactions with the Model(s); and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                            |

## 15.4 Production

## Objective

To (a) identify operational maintenance metrics; and (b) ensure timely update, re-train and re-deployment of Model(s).

|         |                                                   | Control:                                                                                                                                                                                                                                                                                                               | Aim:                                                                                                                                                                                                                                                      |
|---------|---------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 15.4.1. | Operational Performance Thresholds                | Define and set metrics and tolerance intervals for operational performance of Models and Products, such as, amongst other things, latencies, memory size, CPU and GPU usage.                                                                                                                                           | To (a) prevent unavailable and unreliable service; (b) enable quick detection of bugs in the code; (c) ensure smooth integration of the Model with the rest of the systems; and (d) highlight associated risks that might occur in the Product Lifecycle. |
| 15.4.2. | Continuous Delivery of Metrics: Model Performance | Continuously report on and record metrics about Model performance, predictions, errors, Features, and associated performance metrics to relevant Stakeholders (as decided upon in Section 13.2 _- Representativeness & Specification: Exploration and Section 13.3 - Representativeness & Specification: Development). | To (a) enable rapid identification of Model decay, and/or red flags and bugs in Model and/or data pipelines; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                |

| 15.4.3.   | Model Decay & Data Updates   | Operationalise procedures to mitigate Data Drift and/or Model decay (as described in Section 14.2 _- Performance Robustness: Exploration and Section 14.3 - Performance Robustness: Development).                                                                                                                                                                                                                 | To (a) ensure timely implementation of any changes required in data and/or Modelling pipelines; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                 |
|-----------|------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 15.4.4.   | Model Re- training           | Operationalise procedures on how Model re-training ought to be conducted as well as approached, inclusive of, amongst other things, - (1) when will (i) a new Model be deployed, and/or (ii) a Model with the same hyperparameters but trained on new data; and/or (2) when operationalizing re-trained Models ought they be run in parallel with older Models and/or do to gracefully decommission older Models. | To (a) ensure timely implementation of any changes required in data and/or Modelling pipelines; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                 |
| 15.4.5.   | Create Contingency Plans     | Develop and put in place contingency plans in case of technical failures and out-of- bounds behaviour based on (a) bounds and threshold set in other controls; and (b) risk assessment of failure modes.                                                                                                                                                                                                          | To (a) prevent adverse effects from failures and unexpected behaviour by providing clear instructions on roll-back, mitigation and remediation; and (b) highlight associated risks that might occur in the Product Lifecycle. |

## Section 16. Explainability

## Objective:

To ensure Model functions and outputs are explainable and justifiable as far as is practically reasonable in order to (a) foster explainability for Stakeholders, (b) promote Model trust, (c) facilitate Model debugging and understanding, and (d) promote compliance with existing laws and statutes.

## What do we mean when we refer to Explainability?

There is not one agreed-upon definition of explainability but the working definition we have adopted is that

Explainability refers to making the behavior and decisions of a complex machine learning model understandable to humans.

Closely related to the concept of explainability is interpretability. Interpretability refers to the degree to which a human can inherently understand the cause of a Model's decision. In other words, interpretability relates to using Models that are transparent and can be inherently understood by humans; while explainability concerns making complex, non-transparent models understandable to humans. Many researchers and practitioners use the terms interchangeably.

Transparency is another closely related concept to explainability and interpretability. It is the broadest of the three. Transparency refers to the openness of the workings and/or processes and/or features of data, Models and the overall project (the Product). Transparency can be both comprehensible or incomprehensible depending on its content. Transparency does not necessarily mean comprehension: this is important and why it differs from explainability and interpretability. Again, transparency just refers to the openness of the workings and/or processes and/or features of data, Models and the overall project - whether technical or not.

## Why is Explainability relevant?

When we talk about machine learning used for high-stakes decisions, there is a strong agreement that it is extremely important for the public and for machine learning practitioners to understand the inner workings and decision-making of Models. This is because through such understandings, we can ensure that machine learning is done fairly or, rather, that it does not generate unfair or harmful consequences. To put it more simply, we can ensure human oversight and correction over machine learning operations. Explainability also is very important for promoting trust and social acceptance of machine learning. People do not often trust and accept things they do not understand. Through explainability, we can help people understand machine learning and, in turn, trust it.

## How to apply Explainability?

In order to generate thorough and thoughtful explainability, it must be considered continuously throughout all stages of the product lifecycle. This means that explainability must be addressed at the (a) Product Definition(s), (b) Exploration, (c) Development and (d) Production stages of machine learning operations.

## 16.1 Product Definitions

## Objective

To (a) ensure the transparency of Product Definitions; (b) foster multi-stakeholder buy-in through explanations; and (c) reduce ethical risks in Product Definition(s) decision-making and Model Runs.

|         |                                         | Control:                                                                                                                                                                                                                                                                                                              | Aim:                                                                                                                                                                       |
|---------|-----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 16.1.1. | Explainability Aims                     | Having consideration for (a) Product Definition(s), (b) the explanations and/or transparency sought, (c) the Model adopted, and (d) datasets used, document and assess the explainability aims of the Model.                                                                                                          | To (a) clearly document the explainability and transparency aims of the Model; and (b) highlight associated risks that might occur in the Product Lifecycle.               |
| 16.1.2. | Explainability Stakeholder              | Document and assess the internal and external Stakeholders affected by the Model.                                                                                                                                                                                                                                     | To identify the Model explainability Stakeholders; and (b) highlight associated risks that might occur in the Product Lifecycle.                                           |
| 16.1.3. | Explainability Risks Assessment         | Document and assess the individual risks of failing to provide model explainability, inclusive of a legal liability and Explainability Stakeholders mistrust.                                                                                                                                                         | To identify the risks of failing to provide Model explainability; and (b) highlight associated risks that might occur in the Product Lifecycle.                            |
| 16.1.4. | Legal Requirements for Interpretability | Document and assess any specific legal requirements for Explainability in consultation with legal experts.                                                                                                                                                                                                            | To (a) ensure that minimum standards for explainability are met and legal risk is addressed; and (b) highlight associated risks that might occur in the Product Lifecycle. |
| 16.1.5. | Explainability Requirements             | Document and assess the explainability and transparency requirements and levels in light of (a) Explainability Aims, (b) Explainability Stakeholders, and (c) Explainability Risks, taking care that the elicitation of said requirements involves appropriate guidance, education and understanding of Stakeholders. | To (a) clearly document the explainability requirements of the Model; and (b) highlight associated risks that might occur in the Product Lifecycle.                        |

## 16.2 Exploration

## Objective

To identify and document Model explainability and transparency requirements, inclusive of Stakeholder needs.

|         |                                             | Control:                                                                                                                                                                                                                                                                                                                                                                                 | Aim:                                                                                                                                                                                                                |
|---------|---------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 16.2.1. | Stakeholder Appraisal                       | Document and conduct (a) ad-hoc interviews, (b) structured surveys and/ or (c) workshops with Explainability Stakeholders about their Model and Product concerns and literacy.                                                                                                                                                                                                           | To (a) generate Explainability Stakeholders analytics in order to map Model explainability requirements and demands; and (b) highlight associated risks that might occur in the Product Lifecycle.                  |
| 16.2.2. | Stakeholder Appraisal Analysis              | Document, analyse and map the outcomes of the Stakeholder Appraisal against the Explainability Aims and Explainability Risks.                                                                                                                                                                                                                                                            | To (a) map and analyse Model explainability requirements and demands in light of the needs of Explainability Stakeholders; and (b) highlight associated risks that might occur in the Product Lifecycle.            |
| 16.2.3. | Explainability Matrix                       | Document, assess, and derive a matrix evaluating and ranking the metrics and/ or criteria of explanations needed for based on the (a) Stakeholder Appraisal Analyse, (b) Explainability Aims, (c) Explainability Risks, and (d) Explainability Requirements, inclusive of explanations accuracy, fidelity, consistency, stability, comprehensibility, certainty, and representativeness. | To (a) derive a clear matrix from which to assess Model explainability requirements; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                  |
| 16.2.4. | Explainability Feature Selection            | Document and analyse the degree of Feature explainability needed in light of the Explainability Matrix.                                                                                                                                                                                                                                                                                  | To (a) identify the requisite degree of Feature explainability needed; and (b) highlight associated risks that might occur in the Product Lifecycle, such as later stage Model retraining due to feature ambiguity. |
| 16.2.5. | Explainability Modelling Mapping & Analysis | Document and analyse the technical needs of Model explainability in light of the Explainability Matrix, inclusive of considerations of global vs. local explainability and/or pre-modelling explainability, modelling explainability, and post-hoc modelling explainability                                                                                                              | To (a) identify the technical needs of the Explainability Matrix; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                     |
| 16.2.6. | Explanation Frequency & Delivery Assessment | Document and assess the frequency, most suitable and practically reasonable methods of communicating Model explainability in light of the Explainability Matrix and Stakeholder Appraisal Analysis.                                                                                                                                                                                      | To (a) identify the most appropriate method of communicating Model explainability in order to promote explainability comprehension; and (b) highlight associated risks that might occur in the Product Lifecycle.   |

## 16.3 Development

## Objective

To ensure that Model design represents the explainability requirements and demands of transparency aims as much as is reasonably practical.

|         |                                                       | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Aim:                                                                                                                                                                                                                                                             |
|---------|-------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 16.3.1. | Explainability Feature Selection Assessment           | Conduct a Feature analysis of the Explainability Feature Selection in order to remove correlated and dependent Features.                                                                                                                                                                                                                                                                                                                                                | To (a) interrogate the assumption of zero Feature dependency in explainability modelling; (b) prevent misleading Model explainability and transparency; and (c) highlight associated risks that might occur in the Product Lifecycle.                            |
| 16.3.2. | Global Explainability Model Run                       | Document and run as many types of global explainability Models as is reasonably practical, such as Feature importances, Feature interactions, global surrogate Models, perturbation-based techniques or gradient-based techniques. When there is doubt about the stability of the techniques being used, test their quality through alternative parameterizations or by comparing across techniques.                                                                    | To (a) generate global explainability of the model; (b) help promote model debugging; (c) ensure explainability fidelity and stability through numerous explainability model runs; and (d) highlight associated risks that might occur in the Product Lifecycle. |
| 16.3.3. | Local Explainability Model Run                        | Document and run as many types of local explainability Models as is reasonably practical, such as perturbation-based techniques or gradient-based techniques or, for more specific examples, Local Interpretable Model-Agnostic Explanations (LIME), SHAP values, Anchor explanations amongst others. When there is doubt about the stability of the techniques being used, test their quality through alternative parameterizations or by comparing across techniques. | To (a) generate global explainability of the model; (b) help promote model debugging; (c) ensure explainability fidelity and stability through numerous explainability model runs; and (d) highlight associated risks that might occur in the Product Lifecycle. |
| 16.3.4. | Visual Explanations Assessment                        | Develop visual aids to present and represent Model explainability and transparency insights, such as Tabular Graphics, Partial Dependency Plots, Individual Conditional Expectations, and/or Accumulated Local Effects plot.                                                                                                                                                                                                                                            | To promote explainability comprehension.                                                                                                                                                                                                                         |
| 16.3.5. | Example-based and Contrastive Explanations Assessment | Develop example-based and contrastive explanations to present and represent Model explainability insights, such as the underlying distribution of the data or select particular instances.                                                                                                                                                                                                                                                                              | To promote explainability comprehension, such as of complex data distributions and/ or datasets for Explainability audiences.                                                                                                                                    |

## 16.4 Production

## Objective

To monitor and track the performance of the explanations and trigger when any of the explainability approaches need to be re-trained.

|         |                                          | Control:                                                                                                                                                                                                                   | Aim:                                                                                                                                           |
|---------|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|
| 16.4.1. | Explainability Model Thresholds          | Set clear performance thresholds and limitations for explainability Model(s).                                                                                                                                              | To (a) define parameters for the continued suitability and performance of explainability Model(s); and (b) highlight associated risks.         |
| 16.4.2. | Explainability Model Review & Monitoring | Periodically, or when significant Model changes occur, review implemented explainability Model(s) in light of Explainability Model Thresholds.                                                                             | To (a) ensure the continued suitability and performance of explainability Model(s) and their explanations; and (b) highlight associated risks. |
| 16.4.3. | Explanation Tracking & Monitoring        | Document and conduct (a) ad-hoc interviews, (b) structured surveys, and/or (c) workshops with Explainability Stakeholders on explanations provided and adjust outcomes in Section 14 - Performance Robustness accordingly. | To ensure the continued effectiveness and suitability of provided Model explanations.                                                          |