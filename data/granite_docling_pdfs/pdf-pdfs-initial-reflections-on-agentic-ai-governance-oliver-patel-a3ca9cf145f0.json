{
  "doc_id": "pdf-pdfs-initial-reflections-on-agentic-ai-governance-oliver-patel-a3ca9cf145f0",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Initial reflections on agentic AI governance - Oliver Patel.pdf",
  "title": "Initial reflections on agentic AI governance - Oliver Patel",
  "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable, and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement, and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nICYMI: visit this page to download my free 20-page AI Usage Policy Playbook and register interest for my upcoming AI Usage Policy Bootcamp .\n\nThis week's edition is an essay on agentic AI governance . It covers:\n\n- ✅ What is agentic AI?\n- ✅ How agentic AI is used today and how it could be used in future?\n- ✅ What novel risks does agentic AI pose?\n- ✅ How do these risks challenge existing AI governance frameworks?\n- ✅ What new policies, standards, and guardrails are required to address these challenges and mitigate risk?\n\nThe key message is that urgent work is required, within the AI governance community, to refine and update our approach to AI governance and risk management in the\n\nagentic AI era.\n\nI am very keen for comments and feedback, to guide my thinking and work in this emerging field. However, I also don't want to waste anyone's time. Therefore, if you don't want to read a 4,000-word essay on agentic AI governance, then this may not be for you.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week.\n\n## Initial reflections on agentic AI governance\n\nThis essay outlines my initial reflections on agentic AI governance. The phrase 'initial reflections' is designed to serve as a health warning for all readers.\n\nLarge language model (LLM) based AI agents are a relatively new technology. There are not many enterprise use cases in production, at least compared to generative AI and traditional machine learning.\n\nFurthermore, there are not yet any laws, standards, frameworks, or guidelines which directly address or stipulate how the novel risks of agentic AI should be mitigated.\n\nFinally, not much has been researched or published on the topic of agentic AI ethics and governance. However, I will reference some of what has been written throughout this essay.\n\nConsidering the above, it is reasonable for you to ask why I am writing and publishing this today.\n\nWell, the main reason is that agentic AI is coming-whether the AI governance community is ready or not. As we should all appreciate by now, technology is not going to wait for us to figure things out.\n\nAs a community, we had several years to discuss, align on, and codify the key ethical principles, policies, and standards for AI, before enterprise adoption of machine learning became mainstream.\n\nAnd, although our response and adaptation timelines were accelerated for generative AI, the launch of ChatGPT was a landmark moment that made it obvious there was pressing work for us to do.\n\nWith agentic AI, the challenge is twofold. Not only is time in short supply, I also do not anticipate that there will be a watershed moment that highlights the urgency of agentic AI governance. It is, and will continue to, creep up on us and permeate our organisations.\n\nFor this reason, there is a serious possibility that we may fail to identify and grasp the unique challenges and risks which agentic AI poses in time for the inevitable proliferation of use cases, adoption at scale, and democratisation.\n\nThis could leave our organisations and stakeholders exposed, with an AI governance and risk management framework that is no longer fit for purpose in the agentic AI era.\n\nI hope that my 'initial reflections' essay can help to address this challenge. However, please take everything I say with a pinch of salt, as this is not a peer-reviewed paper, and there is a lot of work for me to do to fully wrap my head around this complex topic.\n\n## What is agentic AI?\n\nYou will perhaps be unsurprised to read that there is not yet an internationally agreed definition of agentic AI. Here are some industry definitions, to get us started:\n\n'AI agents are software systems that use AI to pursue goals and complete tasks on behalf of users' Google\n\n'Agentic AI systems can accomplish a specific goal with limited supervision' IBM\n\n' Agentic AI systems act autonomously, make decisions, and adapt dynamically to complex environments' Kieran Gilmurray\n\nCentral to all definitions of agentic AI is the concept of proactive and autonomous completion of tasks.\n\nIn his new book Kieran Gilmurray outlines the three waves of AI: predictive AI, generative AI, and agentic AI.\n\nWith traditional machine learning, models generate predictive outputs, like scores, classifications, and recommendations. With generative AI, models generate content, like text, video, or code. These predictive or generative AI outputs are typically provided to humans via a user interface and are then used by those humans to assist, augment, or optimise their work.\n\nWith agentic AI systems, the work itself may no longer be done by the humans.\n\nAgentic AI systems can autonomously develop plans, solve problems, retrieve data, leverage memory, use tools, and execute tasks in a range of other applications which they are integrated with.\n\nThey do so by constantly processing inputs, learning from, and adapting to their environment, and proactively determining the best course of action to take. Hence the notion of 'agency'.\n\nAI agents are powered by LLMs and APIs. The LLM enables the system to process the initial instructions and mimic reasoning to break down complex problems into a series of smaller steps to be executed. The APIs enable integration with a range of other applications, tools, and databases, to retrieve data and make things happen.\n\nBecause agents are LLM-powered, they are unpredictable and non-deterministic, in contrast to more traditional forms of software automation and workflows, like RPA; more on that below.\n\n## What are the use cases for agentic AI?\n\nAs you can imagine, the potential spectrum of agentic AI use cases is limitless.\n\nHowever, there are not a huge number of AI agents in production today. This is still a\n\nnovel (and by extension relatively unreliable and untested) technology.\n\nIBM has declared 2025 as the year of agentic exploration. Each organisation will likely witness a proliferation of PoCs and pilots this year, just like we have seen with generative AI over the past two and a half years.\n\nIn this early wave of agentic AI use cases, there is an emphasis on assistive and productivity enhancing tasks, such as research, summarisation, and information retrieval. A survey by LangChain finds that the most popular agentic AI use cases today are research and summarisation, personal assistance and productivity, customer service, and code generation.\n\nIn many cases, AI agents are already working behind the scenes, without our awareness, to improve the performance and effectiveness of the most widely used generative AI applications, like Perplexity and ChatGPT.\n\nFor example, the current crop of 'deep research' tools function by leveraging teams of AI agents which collaborate with each other to scour the internet and other data sources to retrieve, collate, assess, merge, and summarise relevant information, to augment and enhance the final AI-generated 'report' or response which the user receives.\n\nThe architecture of such applications has become much more sophisticated than one model receiving a prompt, performing inference, and generating a predictive output.\n\nLooking ahead, the thinking is that agentic and multi-agentic systems will be able to take on increasingly complex tasks and projects, such as managing customer service interactions, planning and booking holidays, planning, creating, and posting social media content, and managing investment portfolios.\n\nBefore we get there, we need a bulletproof approach to agentic AI governance.\n\n## The novel risks and challenges of agentic AI\n\nAlthough there is currently ample hype (some of which is inevitably overblown), this is not an excuse to ignore or disregard this technological trend.\n\nAgentic AI poses unique risks, which the AI governance community cannot afford to overlook.\n\nThe risks of AI stem primarily from the way the technology is used and the real-world impact this use can have.\n\nTherefore, even if agentic AI is based upon the same underlying technology as generative AI (i.e., LLMs), this does not mean it will be used in the same way.\n\nThe deployment and use of agentic AI, and thus its impact on people, organisations, and society, will be markedly different to what has come before.\n\nIncreasingly autonomous capability enables novel AI use cases, such as control of computers and automation of dynamic, data-intensive processes in sensitive areas like supply chain management and logistics planning.\n\nFurthermore, it is conceivable that, in future, knowledge workers will have access to their own personalised AI agent, to assist with all aspects of their work.\n\nTaken together, these examples represent a meaningful shift from how AI is used today.\n\nTo illustrate the novel risks, I will focus on four themes of the utmost importance for agentic AI:\n\n1.  'Human out of the loop'\n2.  Autonomous task execution and performance\n\n3.  Adaptiveness and unpredictability\n4.  Data, privacy, and cyber security\n\nIn the agentic AI era, all AI risks are amplified. Virtually all existing AI risk themes, such as bias, transparency, copyright, explainability, alignment, sustainability, and labour market disruption remain as relevant-if not more so-than ever.\n\nHowever, my goal here is to focus on the most novel challenges posed by agentic AI.\n\n## 1. Human out of the loop\n\nIt is not an exaggeration to state that the purpose of agentic AI is to take the human out of the loop.\n\nWhy plan and book your own holiday when an AI agent can do it for you? Why respond to all of your fans and followers across multiple social media platforms when an AI agent can take care of the correspondence? Why employ hundreds of call centre workers when an army of autonomous agents can do the job?\n\nHowever, this is in direct tension with the concept of human in the loop, which is a foundational pillar of AI governance.\n\nBy delegating and outsourcing tasks to AI agents, humans may be freed to focus their\n\ntime and energy elsewhere. However, AI agents will also be trusted to take on increasingly important tasks, with diminishing human oversight.\n\nThe key risk is that we become overly trusting of agentic AI systems and take the human out of the loop to a degree which becomes dangerous.\n\nIn the quest for efficiency gains, we may underestimate the level of human oversight required for safe agentic deployment.\n\nThis risk is especially pertinent at first, as we do not truly understand the limitations and capabilities of agentic AI systems.\n\nAn associated challenge, discussed below, will be the complexity of refining and updating our approach to human oversight. This will require defining exactly when human review and approval is required before an action can be taken.\n\nMoreover, if the AI agent executes the action, it may become even harder to determine which human, or entity, should be held accountable for it. This will need to be codified at the outset of agentic development.\n\n## 2. Autonomous task execution and performance\n\nIf you thought AI hallucinations were bad, wait till you learn about 'cascading hallucinations'. OWASP describes this as when an 'AI agent generates inaccurate information, which is then reinforced through its memory, tool use, or multi-agent interactions, amplifying misinformation across multiple decision-making steps'.\n\nThis can lead to self-reinforcing destructive behaviours and systemic failures in agentic AI performance.\n\nAgentic AI raises the stakes for the hallucination problem. An LLM hallucination is primarily a problem if the user naively fails to verify the accuracy of the output before relying on or using it. However, an LLM hallucination which directly informs and shapes the course of action taken by an AI agent (or team of agents) could have severe consequences, if that agent is being trusted to execute tasks in a high-risk domain.\n\nThe more autonomous AI agents become, and the more we trust those agents to take over tasks and projects in sensitive areas, the greater the risk and negative impact of malfunction, error, and performance degradation.\n\nAlthough AI performance concerns are nothing new, without appropriate controls, such as human oversight and observability, there is a risk that the agents we begin to trust let us down, without us even realising at first.\n\nFor example, if the AI agent is autonomously handling and responding to customer complaints, how many inappropriate interactions and unnecessary follow ups could occur before this is flagged and addressed?\n\nIt is also critical that the appropriate agent performs the appropriate task. In the modern enterprise there will be many agents, working together and supervising each other in complex hierarchies, based on their pre-defined roles, permissions, and guardrails. Therefore, the reliable performance of agents towards the top of the hierarchy is critical for the performance and effectiveness of all the other agents.\n\nBy taking the human out of the loop and allowing AI agents to autonomously execute tasks, it is undeniable that there are immense potential efficiency and productivity gains.\n\nHowever, with increased autonomy comes increased risk. There may be some domains where we simply cannot afford agentic AI mistakes.\n\nA broader question is whether we are building these exciting new tools on the relatively shaky foundation of a technology which was ultimately designed to predict the next word, rather than perform important tasks.\n\nResearchers from Hugging Face have sounded the alarm bell and argued that fully autonomous agents should not be developed, due to the unacceptable potential risks resulting from system inaccuracy, privacy and security breaches, spread of false information, and loss of human control.\n\n## 3. Adaptiveness and unpredictability\n\nAI agents are unpredictable precisely because they are proactive. If we knew or could consistently guess what they were going to do, they would not have agency in any meaningful sense.\n\nLLMs are non-deterministic, which means models can generate different outputs in response to the same inputs. This leads to unexpected, unpredictable, and unreliable outputs. This will inevitably be reflected in the behaviour and performance of AI agents, which will rely on the effectiveness of LLMs and their ability to accurately predict the next word.\n\nGiven the proactive and dynamic way in which AI agents respond and adapt to their environment, it could become virtually impossible to predict and anticipate how they will behave, and therefore the risks that could emerge.\n\nThis makes AI risk assessment, and therefore AI risk mitigation, much more challenging than it is today. It also requires much more continuous and comprehensive monitoring\n\nof AI performance.\n\nIt is hard enough to predict the risks of downstream general-purpose AI usage, let alone the potential behaviour and risks of autonomous agents, which are integrated with a range of other applications, and empowered to solve complex and open-ended problems in whichever way they see fit.\n\n## 4. Data, privacy and cyber security\n\nData, privacy and cyber security risks are nothing new for AI. However, these risks are exacerbated by agentic AI.\n\nAgentic AI systems could easily mine and retrieve data from sources which they were not supposed to have access to, or sources which are not permitted to be used for AI processing or text and data mining. This could include copyrighted material without an appropriate license, or sensitive personal data originally collected for a different, more narrow purpose.\n\nFurthermore, there is also the risk of AI agents disclosing and revealing data to people who were not authorised to have access to it. Agents will be performing and automating increasingly personalised tasks, such as booking medical appointments, whilst having access to and being trained on huge amounts of personal data, such as medical records.\n\nThis elevates the risk of data breaches and information leakage. Therefore, encoding privacy by design and data governance guardrails will be a challenging but necessary part of agentic AI governance.\n\nAgentic AI systems will also become attack surfaces. Nefarious actors will undoubtedly attempt to take control of, and manipulate, the autonomous systems which themselves may control important spheres of business activity. There is a lot you can do if you control an agentic system which itself can control computers with access to sensitive data and applications.\n\nAlso, in situations where AI agents are trusted to both generate and execute code, the risk of autonomously executed (and non-vetted) malicious code creeping in to production applications increases.\n\nIn a recent report, OWASP outlines 15 unique threats which agentic AI systems are vulnerable to. This includes cascading hallucination attacks, resource overload, tool misuse, and rogue agents in multi-agent systems.\n\n## Policies, guardrails and controls to manage agentic AI risks\n\n'Traditional' AI governance frameworks, such as the EU AI Act and NIST AI Risk Management Framework, were developed during a time when traditional machine\n\nlearning and then generative AI was prevalent. They do not directly address many of the novel risks and challenges of agentic AI, discussed above. Indeed, if the EU AI Act was being drafted today, I am certain that some of my below points would be directly addressed in the law.\n\nHowever, we cannot rely on, or wait for, the regulators to come and save us with new guidance or updated standards. They will, rightly so, expect industry to figure out how to develop and implement agentic AI in a manner which is safe, secure, and respects existing laws, like the EU AI Act.\n\nNone of the risks highlighted above represent insurmountable problems. I have full faith in the ingenuity of the AI governance community to solve them.\n\nBelow, I will sketch out the six most important considerations for AI governance professionals seeking to refine, update, and implement policies, guardrails, and controls, to meet the challenge of managing risk in the agentic AI era. This includes:\n\n1.  Action permissions and thresholds\n2.  Integrations and data access\n3.  Hierarchy and approval matrixes\n4.  Monitoring, observability, and orchestration\n5.  Human oversight, accountability, and control\n\n## 6.  Use cases and risk assessments\n\n## 1. Action permissions and thresholds\n\nConfiguring action permissions appropriately is an essential part of agentic AI governance.\n\nAutonomy is a spectrum. Just because an agent can do something does not mean we should let it. We can determine exactly which actions an agent can and cannot perform.\n\nThe potential behaviour and permissions of agents can be restricted at the system and API level.\n\nIf there are certain tasks which an agent could in theory perform in a given system or environment, or certain tools the agent could use, and we do not want the agent to do so, we can specify and encode these restrictions.\n\nThis sounds simple enough at first. For example, in a financial context, we may not want the agent to execute or process any transaction, or make any decision, which carries a financial value over a certain amount. Similarly, we may want to restrict the ability of an agent to execute AI-generated code in certain applications.\n\nWhat is more challenging is to define generally applicable policy principles, which can be used to determine, for any use case in any domain, what type of action permissions and restrictions we should impose, and what thresholds we should use to guide this. Some potential action permission threshold categories could be:\n\n- Financial value\n- Direct impact on people\n- Number of people impacted\n- Impact on patients\n- Importance of the decision or action on the business\n- Potential ethical risk (e.g., EU AI Act high-risk AI system categories)\n\n## 2. Integrations and data access\n\nOn a similar note, we can also determine and restrict which applications an agent is integrated with, as well as which data it has access to.\n\nAs well as enabling privacy by design and data governance, this also supports the points raised above relating to access restrictions.\n\nIf an agent is unable to access certain data and/or is not integrated with the application where a particular task is performed or tool is used, then it will be unable to use that data or tool to do something which we do not want it to do.\n\nAgain, we will need to formulate generally applicable policy principles which can steer us in our assessment of which applications agents should and should not be integrated with, as well as which datasets should and should not augment their knowledge base.\n\n## 3. Hierarchy and approval matrixes\n\nIn the modern enterprise there will be countless agents working together, in complex hierarchies of agentic collaboration, supervision, and oversight.\n\nThere will need to be a clearly defined RACI or matrix for AI agents, which outlines the roles, responsibilities, and segregation of duties. It is crucial that agent Y only performs tasks within its permitted duties and that agent X does the same.\n\nAgents towards the top of the hierarchy will be empowered to review, approve, authorise, and restrict the work of other agents. And agents lower down in the hierarchy should not be allowed to operate in a way which circumvents the authority of their superiors.\n\nThis will require both complex engineering and architectural design, as well as a new conceptual framework for AI governance professionals to lean on.\n\n## 4. Monitoring, observability, and orchestration\n\nWe are moving from MLOps and LLMOps to AgentOps.\n\nIn the 'old world', MLOps is used to validate, test, and monitor model performance, including robustness and accuracy. This primarily focuses on the predictive outputs that are generated and how they perform across a range of key metrics.\n\nWith AgentOps, the goal is to automate the monitoring, oversight, and orchestration of agentic and multi-agentic AI systems, so we can keep tabs on what actions they are performing, how they are behaving, which tools they are using, the impact this is having, and ultimately, whether we can trust them to keep working on our behalf. There should also be visibility as to whether any agents are operating contrary to their guardrails and permissions.\n\nAssessing and evaluating agentic AI performance also entails additional complexity, at least compared with traditional machine learning performance evaluation.\n\nThis is because the actual tasks that agents perform are much more wide-ranging, varied, and hard to anticipate, given the proactive nature of agentic AI. Therefore, we\n\nwill need updated and rigorous performance and accuracy metrics, which can account for the variety of possible tasks and agent could perform.\n\n## 5. Human oversight, accountability and control\n\nWhat we mean by human oversight will also require a refresh.\n\nIt will no longer make sense to mandate human review and approval of each AIgenerated output, when the purpose of agentic AI is to take the human out of the loop, to automate processes and drive efficiencies. If the goal is AI autonomy, humans cannot review everything.\n\nHowever, this does not mean human oversight is no longer relevant.\n\nFor example, human oversight could mean reviewing the ultimate output of an agentic AI system, such as a 'deep research report or generated code (as opposed to all the outputs generated and decisions taken to reach its conclusion and generate that final output).\n\nHuman oversight could also mean having a human in the loop to review actions, tasks, and decisions which meet a certain threshold or risk level, which could be aligned to the action permissions and thresholds detailed above. We will need clearly defined\n\ntouch points for human oversight and review, and it will be more nuanced than what we have today.\n\nFinally, humans must always have the ability to override or shut down an agentic AI system, no matter how much autonomy we have empowered it with.\n\nAccording to a LangChain survey on agentic AI usage in 2025, very few companies are allowing agents to freely read, write, and delete data and information from the applications they are operating in and the databases they have access to. Rather, agents are given read-only permissions, with human approval required for significant actions.\n\n## 6. Use cases and risk assessments\n\nFinally, it is important to determine which agentic AI use cases should be off limits, at least for now.\n\nThe EU AI Act serves as a useful starting point. AI agents should obviously not be used to for anything which constitutes a prohibited AI practice. Furthermore, I would also advise extreme caution in using agents to autonomously perform tasks, which can have a material impact on decision making or a process, in a domain relating to highrisk AI systems, such as recruitment, critical infrastructure safety management, or determining eligibility for welfare payments.\n\nFor one, there is no evidence that agentic AI systems can yet be trusted to perform to a high enough standard required for these sensitive domains.\n\nFurthermore, it will be challenging to comply with the AI Act's obligation for deployers to assign human oversight and the GDPR's restrictions on solely automated decisionmaking, whilst also leveraging autonomous agentic AI systems to automate decisionmaking in sensitive and high-risk domains.\n\nHowever, you will need to look beyond EU law in your work to determine what use cases are appropriate, inappropriate, and off limits for your organisation. Consider the fundamentals of what agents can and cannot do, as well as their strengths and weaknesses.\n\nGoogle, for example, highlights that agents struggle with and should not be used for tasks requiring empathy and emotional intelligence, complex human interactions, high-stakes ethical decision-making, and the navigation of unpredictable physical environments.\n\nOnce you have figured this all out, you will also need to update your approach to risk assessments, as well as your supporting guidance and training. The key question which needs to be answered throughout is when is it safe to use agentic AI and when is it not?\n\n*\n\nThe purpose of this essay is to highlight some of the novel risks and governance challenges of agentic AI.\n\nWhilst I am not proposing a complete overhaul of AI governance frameworks and policies, the considerations I have outlined above should serve as a starting point for refining and updating your organisation's approach to AI governance in the agentic AI era.\n\nIf you have made it this far, I would greatly appreciate comments and feedback. Thank you!\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n40 Likes ∙ 1 Restack\n\n<!-- image -->",
  "fetched_at_utc": "2026-02-09T13:54:30Z",
  "sha256": "a3ca9cf145f064817518ca29902f1753d09e6edbcbc23e45927bdfeea9063c17",
  "meta": {
    "file_name": "Initial reflections on agentic AI governance - Oliver Patel.pdf",
    "file_size": 814142,
    "mtime": 1767775345,
    "docling_errors": []
  }
}