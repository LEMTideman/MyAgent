{"doc_id": "pdf-pdfs-15-things-you-must-know-about-ai-governance-in-china-oliver-patel-cc4e5d1af4b9", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\15 Things You Must Know About AI Governance in China - Oliver Patel.pdf", "title": "15 Things You Must Know About AI Governance in China - Oliver Patel", "text": "<!-- image -->\n\nHey\n\nðŸ‘‹\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nFor more frequent updates, be sure to follow me on LinkedIn.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts.\n\nThis week's edition focuses on China.\n\nIt's still only January, but 2025 has already been an action packed and turbulent year for AI.\n\nOver the past week, there were several high profile developments-and too many potential newsletter topics to keep up with ( tiny violin time ðŸŽ» ).\n\nThis included President Trump's $500 billion Stargate AI announcement, the repeal and replacement of President Biden's Executive Order on AI, and insider updates on EU AI\n\nAct implementation and the AI Office's General-Purpose AI Code of Practice.\n\nHowever, the shockwaves which DeepSeek's AI models have sent through global markets, and the impact this could have on the AI industry, is the biggest story.\n\nDeepSeek is a Chinese AI company, established in May 2023, which spun off from a quant focused hedge fund. Since then, DeepSeek has released some of the most advanced and capable AI models we have seen. This includes DeepSeek-V3 and, most recently, DeepSeek-R1, a reasoning model which rivals competitor models developed by U.S. AI labs, despite much lower reported training, inference and usage costs. Furthermore, all of DeepSeek's models are open source, which means the model weights can be accessed and used for free.\n\nWe do not yet know much about DeepSeek's approach to AI governance and safety (that will be the focus of a future newsletter). However, this represents an opportune moment to reflect on the context in which it has emerged.\n\nWhilst Chinese AI labs have been seriously active in pushing the frontier, there have been many consequential AI governance and safety initiatives in China aiming to keep pace.\n\n## 15 Things You Must Know About AI Governance in China\n\n## China's AI strategy\n\n1. China's ambition is to be the global AI leader by 2030. DeepSeek's progress should come as no surprise. China has made no secret about its aspirations of global technological leadership in AI. The 'New Generation AI Development Plan', published by China's State Council in 2017, states that 'by 2030, China's AI theories, technologies, and applications should achieve world-leading levels, making China the world's primary AI innovation center'.\n2. The government strongly supports open source AI . Promoting an open innovation ecosystem is a core part of how China aims to achieve their strategic ambition for AI leadership by 2030. The 2017 New Generation AI Development Plan states that the government should 'encourage AI enterprises and research institutions to build open source platforms for public open AI research and development'. Leading Chinese tech companies frequently release open source models (like DeepSeek's models and Alibaba's Qwen model series), with support, fanfare and financial backing from the state's media and institutions. Only time will tell, but perhaps the early success of DeepSeek is vindicating this approach.\n\n## AI regulatory landscape\n\n3. China has adopted several national AI governance laws. Alongside the EU, China has perhaps adopted more AI governance specific laws, which apply across\n\nits territory, than any other nation (although, to be fair, this depends on the definition of an 'AI governance law'). Whilst there is no comprehensive law like the EU AI Act, China has adopted 3 important regulations:\n\n- a.  Interim Administrative Measures for Generative AI Services [entered into force August 2023];\n- b.  Regulations on the Administration of Deep Synthesis Internet Information Technology [entered into force January 2023]; and\n- c.  Internet Information Service Algorithmic Recommendation Management Provisions [entered into force March 2022].\n4. There are strict prohibitions on creating and sharing deepfakes. The Deep Synthesis regulations govern the development and use of AI to create and disseminate images, audio, video and other content produced by generative AI systems. This law was issued in November 2022, amid increasing fears about the use of AI to spread misinformation and disinformation. Key requirements include registering deep synthesis algorithms in China's central algorithm registry and not using deep synthesis services to produce, publish or transmit 'fake news information' or information which is 'prohibited, harms the image of the nation or harms the societal public interest'.\n5. The Algorithmic Recommendation Law governs online content. The first AI governance specific law adopted in China focuses on how algorithms disseminate\n\n- and recommend online content, as well how AI is used to manage workers and ecommerce platforms. Again, AI models in scope must be registered in the central algorithm registry. Furthermore, users must be notified about recommendation algorithms and they must also be able to 'opt out' and turn off personalised recommendations.\n6. The Generative AI Services Law requires model evaluations. Perhaps the most consequential AI law for global companies operating in China is the Interim Administrative Measures for Generative AI Services. The focus of this law is on providers of 'public-facing' generative AI services. It is relevant for companies both developing and using generative AI. For example, public generative AI services must 'uphold the Core Socialist Values' and 'use data and foundational models that have lawful sources'. In practice, this means that many AI models developed by Western organisations, and trained on information sources which are prohibited in China, cannot be made available to the public. Other requirements include conducting robust security and safety evaluations prior to releasing new models. Generative AI services and models which are not public or consumer facing, such as those used solely internally within companies, or for research and development activities, are not in scope.\n7. 300+ generative AI systems have been approved for public use. Under the Generative AI Services Law, Generative AI systems must be approved by the Cyberspace Administration of China (CAC), before they can be released to the\n\npublic. According to Concorida AI, 302 such systems have been approved for public use, as of 27th January 2025. This is a rigorous process, which involves companies providing the government with direct access to the model, to facilitate comprehensive tested, evaluated and assessed.\n\nAlgorithmic registration, and approval in the case of public generative AI, is a feature of all three of China's AI governance laws. The below image from Nicholas Welch (ChinaTalk) highlights how the process works.\n\n<!-- image -->\n\n8. DeepSeek, ByteDance, Baidu, Alibaba and Tencent are among the key players. There is now a flourishing and deep ecosystem of Chinese tech companies, which are developing and releasing models that, in some cases, rival those released by U.S. tech giants. The whole world now knows about DeepSeek and its V3 and R1 models. However, this is just the tip of the iceberg. For example, Baidu's Ernie Bot, the closest domestic rival to ChatGPT, passed over 200 million users last year. And Tencent's Hunyuan models are integrated into hundreds of applications, including WeChat. Despite the progress, it is fair to say that these companies have disclosed less about their approach to AI governance and safety than their Western counterparts.\n\n## International AI governance\n\n9. China has endorsed important global AI governance initiatives. Despite forging a unique and distinct policy stance on AI, China has endorsed and signed up to various international AI governance initiatives. For example, China attended the UK-hosted AI Safety Summit in 2023, and signed the 'Bletchley Declaration', which calls for 'increased transparency by private actors developing frontier AI capabilities'. China also backed the United Nations General Assembly resolution on 'Safe, Secure and Trustworthy AI for Sustainable Development', alongside over 120 other countries.\n\n10. President Xi supports an international AI governance institution. At the Third Belt and Road Forum for International Cooperation, which was held in October 2023, President Xi Jinping announced the launch of the 'Global AI Governance Initiative' in his opening speech. As part of this initiative, China signalled its support for 'the United Nations framework to establish an international institution to govern AI, and to coordinate efforts to address major issues concerning international AI development, security, and governance'.\n\n## AI safety and standardisation\n\n11. The CCP considers AI as a significant public security risk. The Third Plenum is one of the most significant events in the Chinese political calendar. It is the third plenary session during each five-year political cycle, and it is the forum in which the Central Committee of the Chinese Communist Party (CCP) focuses on longterm economic and social policy and reforms. A CCP resolution adopted at the July 2024 Third Plenum states that China will 'institute oversight systems to ensure the safety of AI'. According to Matt Sheehan, it is telling that the call for AI safety oversight is contained in a broader section of the resolution on national and public security risks. However, we do not yet know much about exactly what these risks are considered to be.\n12. China has ambitions to release 50+ AI standards by 2026. China's ascendance in technical standards development was one of my key themes for AI governance\n\n- in 2024. China's stated goal is to be an AI standard setter, not a standard taker, and they are pursuing this in a focused way. The National Information Security Standardisation Technical Committee (TC260) has been extremely active, announcing plans for 50+ AI standards by 2026 and releasing several last year, including on generative AI safety.\n13. Generative AI and AI Safety standards have already been published . In 2024, two consequential AI governance standards were published:\n- a.  China's TC260 released the Technical Document on Basic Safety Requirements for Generative Artificial Intelligence Services, to support regulatory compliance and generative AI model approvals.\n- b.  China's TC260 also published the AI Safety Governance Framework.\n\n## Looking ahead\n\n14. Comprehensive national AI legislation is on the horizon . There have been various reports and hints that China is working on comprehensive, national AI legislation. For example, in May 2024, China's State Council announced that an AI law was 'under preparation'. Since then, additional details have been sparse.\n15. China believes that failing to develop (AI) is the greatest threat. China has long viewed economic development and technological innovation as central to its long term interests and global ascendance. Matt Sheehan notes that the phrase 'failing to develop is the greatest threat to national security' is often repeated by\n\nChinese politicians, scholars and advisors in AI policy debates. Given that 74% of global generative AI patents are filed in China, it seems like this is being taken seriously. However, as this article has demonstrated, so is AI governance and safety.\n\nImage from MRonline highlighting proportion of generative AI patents filed in China\n\nSubscribe to Enterprise AI Governance to stay updated on AI governance in China\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T13:27:34Z", "sha256": "cc4e5d1af4b9991877675556a4fbf76093643a55dc5cbdae03ce3d32e43e6f63", "meta": {"file_name": "15 Things You Must Know About AI Governance in China - Oliver Patel.pdf", "file_size": 1591363, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-a-practical-guide-to-ai-and-copyright-oliver-patel-19cc0438337e", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\A Practical Guide to AI and Copyright - Oliver Patel.pdf", "title": "A Practical Guide to AI and Copyright - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nFor more frequent updates, be sure to follow me on LinkedIn.\n\nThis special edition of Enterprise AI Governance contains everything you need to know to navigate the thorny issue of AI and copyright . It is packed full of practical resources, including:\n\n- âœ… 5 practical steps to mitigate risks\n- âœ… A primer on AI and copyright\n- âœ… International regulatory snapshot ðŸ‡ºðŸ‡¸ ðŸ‡ªðŸ‡º ðŸ‡¬ðŸ‡§ ðŸ‡¯ðŸ‡µ ðŸ‡¨ðŸ‡³ ðŸ‡°ðŸ‡·\n- âœ… Relevant litigation and fair use arguments\n- âœ… Generative AI and Copyright Cheat Sheet ( developed in partnership with Copyright\n\nClearance Center ) - scroll to the bottom for this free subscriber only pdf download!\n\nIf this topic interests you, then check out this LinkedIn Live event I am speaking at today (Thursday 27 th  February) at 3pm GMT / 10am EST. It's a fireside chat on ' Bridging Innovation and Integrity in Responsible AI' , hosted by Copyright Clearance Center. We'll be covering how to build and implement an enterprise AI governance programme, as well as copyright and AI challenges. 800+ people have signed up so far and it would be great to see you there!\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week.\n\n## 5 practical steps to mitigate risks\n\nFrequent readers of this newsletter know that I try to front-load value and actionable advice. Therefore, before my deeper analysis of the AI and copyright issue, here are 5 practical steps organisations can take today to mitigate copyright risks.\n\n1. Licensing: use data and content explicitly licensed for use in AI systems. Where appropriate, renegotiate contracts to enable this.\n\n2. Contractual safeguards: negotiate robust clauses, such as IP indemnity protection, in contracts with AI providers. However, be wary that these indemnity clauses do not apply in all circumstances, nor do they offer complete protection.\n3. Data lineage: ensure copyright considerations are a core part of AI risk assessments. These assessments should track and record the lineage of all data used for AI training and development.\n4. AI literacy: scale employee training and awareness, particularly on document and multimedia uploads and prompting. Now that AI is at the fingertips of all employees, anyone can exacerbate copyright infringement risk.\n5. EU AI Act compliance: General-Purpose AI (GPAI) model providers must prepare for their obligation to publish training data summaries. This is applicable from August 2025 (or 2027 for models already on the market). The forthcoming GPAI Model Code of Practice and associated template will outline how to do this. AI deployers and downstream providers (which integrate GPAI models into new AI systems) should conduct robust due diligence and choose their providers wisely.\n\n## A primer on AI and copyright\n\n## What is copyright?\n\nCopyright is a form of intellectual property that protects original works of authorship, including articles and books, to encourage and promote culture, science, and innovation.\n\nUnder copyright law, copyright owners, such as authors and publishers, have exclusive rights regarding how their work is used, shared, or reproduced.\n\nCopyright laws attempt to strike a balance between ensuring fair compensation for creators of original work, whilst enabling wider society to benefit from that work, for example by permitting engagement, commentary, and transformation.\n\nThroughout history, technological developments, such as the advent of radio, television, and the internet, have fundamentally altered how content is developed, distributed, and consumed.\n\nEach new wave of technological change has been accompanied with meaningful changes in copyright laws and the way they are applied.\n\n## AI and copyright: what is the issue?\n\nRapid advances in the field of machine learning, and generative AI in particular, raise important legal questions and challenges relating to copyright.\n\nThese challenges are relevant for all organisations which use AI.\n\nCopyright infringement is a significant risk of AI development and deployment. This is because of the vast amount of published material protected by copyright, such as books, videos, music, news articles, and software, required to train certain AI models.\n\nThose AI models can also reproduce copyrighted material in their outputs.\n\nIf AI-generated outputs closely resemble original works (which formed part of the training data) this can infringe copyright.\n\nCopyrighted material can also be used-whether intentionally or unintentionally-to prompt, augment, or fine-tune AI.\n\nTherefore, although the large foundation model developers are currently in the spotlight, many other organisations could get caught in the crosshairs of copyright litigation.\n\nOrganisations must also consider whether their AI-generated output (e.g., content) is protected by copyright. In most jurisdictions, the key factor is the level and nature of human involvement in generating this output.\n\nAI-assisted works may be copyrightable, but purely AI-generated works may not be.\n\nThe copyright risks of AI are apparent in the dozens of lawsuits copyright owners have brought against unauthorised use of their protected works in AI systems.\n\n## Training AI models\n\nTraining AI models, such as Large Language Models (LLMs), often involves the collection and processing of large amounts of copyright protected material.\n\nLLM training involves breaking down this text into tokens, each of which is assigned a unique integer ID (i.e., a mathematical representation of the text, which the machine can process).\n\nDuring training, the model processes these tokens, identifying patterns and structures, effectively compressing and retaining representations of the training data, including potentially copyrighted content, within its parameters.\n\nDuring inference, the model predicts the next token which should appear in the sequence, based upon the statistical patterns and structures it has been exposed to.\n\nThis process allows the model to generate new text based on learned patterns.\n\nThe key question is whether the copying and use of this data, for AI training and\n\ninference, infringes on copyright. There are strong arguments both for and against, which I summarise below.\n\n## Copyright infringement risk across the AI lifecycle\n\n- AI model training : many AI models are trained on large datasets, often incorporating copyrighted material.\n- AI model fine-tuning: the fine-tuning and adjustment of pre-trained AI models can be done leveraging copyright protected material.\n- Retrieval-augmented generation: RAG applications can incorporate or reproduce copyrighted material as part of their retrieval corpus or augmented outputs.\n- Inputs and prompts: users can input or upload copyrighted material as part of prompts, such as images, documents, or text excerpts.\n- AI inference &amp; outputs: AI-generated content may reproduce or closely resemble copyrighted material, potentially infringing.\n\n## AI-generated outputs\n\nWhilst some organisations will primarily be concerned with whether their AI development activities are copyright infringing, many others will be keen to understand whether their AI-generated outputs are copyright protected (i.e., copyrightable).\n\nThe extent to which AI-generated outputs can be copyright protected varies. In most jurisdictions, human authorship is required.\n\nThis is because copyright is a legal right afforded to humans. Therefore, without human authorship, there is no copyright.\n\nThis has been reinforced in several legal cases, which determined that AI itself cannot be the author of a copyright protected work. In other words, AI-generated work is not copyrightable if no human was involved.\n\n'AI-assisted' works may be copyrightable, whereas purely AI-generated works are most likely not. What matters is the nature and degree of human involvement and creativity.\n\nFor example, the U.S. Copyright Office's position is that merely inputting a prompt is insufficient for copyright protection. This is because simple prompting does not usually enable the user to exert meaningful control over the nature of the output.\n\nHowever, this changes if the human's work or creativity is perceptible in the AI output, or if the human can modify, arrange, or shape the AI-generated output in a meaningful way. In such cases, copyright protection may be granted for a humanassisted AI generated work. This will usually be determined on a case-by-case basis.\n\n## International regulatory snapshot\n\nThere is no global copyright law. Rather, there is significant global divergence in copyright laws and their application to AI.\n\nAlso, the applicable legal framework is determined by the jurisdiction in which the copying happened.\n\nTherefore, it is not possible to make blanket claims about whether AI development or deployment infringes copyright, as it will always depend on the relevant jurisdiction, the material which has been used, and exactly how that material has been used.\n\nCrucially, there are exceptions in copyright law, such as 'fair use' in the U.S. These exceptions stipulate that, in specific scenarios, copyrighted material can be used without permission.\n\nGiven that there is broad international consensus on the requirement for human authorship for AI output to be copyrightable, the below will instead focus on whether the use of copyright protected material in AI training, development, and deployment is permitted under applicable exceptions.\n\nðŸ‡ºðŸ‡¸ USA: There is no specific law or regulation regarding AI and copyright, so existing exclusive rights (like the copyright holder's right of reproduction) and exceptions and limitations (like fair use) apply. Whether AI model training constitutes fair use (and is therefore not copyright infringing) is determined by courts on a case-by-case basis. When evaluating whether an activity constitutes fair use, U.S. courts consider four key factors:\n\n1.  Purpose and character of the use.\n2.  Nature of the copyrighted work.\n3.  Amount of the portion taken.\n4.  Market effects.\n\nðŸ‡ªðŸ‡º EU: The EU Copyright Directive includes two text and data mining (TDM) exceptions -one for scientific research and another that allows broader use (including for commercial purposes), unless the copyright holder opts out. The TDM exception could apply to AI training, but this has not been tested in courts.\n\nThe EU AI Act reinforces that existing EU copyright law applies to AI systems and introduces transparency requirements regarding the training data used. Specifically, providers of general-purpose AI models will be obliged to publish summaries of their training data and implement a copyright compliance policy.\n\nðŸ‡¬ðŸ‡§ UK: The UK launched a Copyright and AI consultation in December 2024, which outlined a proposal to adopt a framework akin to the EU's (i.e., with a data mining exception covering commercial use). This would create a more permissive regime than is currently in place. The consultation closed earlier this week. The government's proposals were widely criticised by artists and publishers, but backed by AI companies.\n\nðŸ‡¯ðŸ‡µ Japan: Japan has a nuanced approach. While Article 30-4 of the Copyright Act permits AI training in some cases, this is not a blanket exemption, and it has limitations. For example, if copying the material for \"enjoyment purposes\" or prejudice to rights holders are involved. Such prejudice includes 'material impact on the relevant markets'. Furthermore, AI-generated outputs that are \"similar\" and \"dependent\" on copyrighted works can still be infringing.\n\nðŸ‡¨ðŸ‡³ China: China has a stricter AI and copyright legal framework. There are no explicit exemptions for TDM or AI training. Enforcement by Chinese courts is expected to be relatively strict.\n\nðŸ‡°ðŸ‡· Republic of Korea: The Korean Copyright Commission encourages AI companies to proactively secure licenses, prior to using copyrighted material for AI training.\n\n## Relevant litigation and fair use arguments\n\nThere are dozens of ongoing legal cases around the world adjudicating AI and copyright disputes.\n\nOne tracker lists 39 relevant cases, including Getty Images v Stability AI, The New York Times v Microsoft and OpenAI, and Farnsworth v Meta.\n\nMany cases are in the U.S., where courts are determining whether various uses of copyrighted material in AI systems are infringing or constitute fair use.\n\nThere are strong arguments on both sides of the debate.\n\nThose arguing that AI training does not meet the criteria for fair use highlight that original copyright protected materials (e.g., text) are copied for training and then stored, as mathematical representations, in model parameters, which is why LLMs can 'memorise' and reproduce training data content, thereby infringing the right to reproduction.", "fetched_at_utc": "2026-02-09T13:27:57Z", "sha256": "19cc0438337e0849724f06c418deeff13415aa03a51681f9a2caa26718a5afa6", "meta": {"file_name": "A Practical Guide to AI and Copyright - Oliver Patel.pdf", "file_size": 910317, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-ai-governance-in-practice-report-2024-iapp-8da99f96370f", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Governance in Practice Report 2024 - IAPP.pdf", "title": "AI Governance in Practice Report 2024 - IAPP", "text": "<!-- image -->\n\n## AI Governance in Practice Report 2024\n\nTable of contents\n\n## What's inside?\n\n| Executive summary .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .         | .  .  .3   |\n|-----------------------------------------------------------------------------------------------------------------------|------------|\n| Part I. Understanding AI and governance. . . . . . . . . . . . .                                                      | . . 6      |\n| Part II. The data challenge. . . . . . . . . . . . . . . . . . . . . . . . . .                                        | . 15       |\n| Part III. The privacy and data protection challenge  .  .  .  .                                                       | .  .23     |\n| Part IV. The transparency, explainability and interpretability challenge. . . . . . . . . . . . . . . . . . . . . . . | . 32       |\n| Part V. The bias, discrimination and fairness challenge .                                                             | .  .41     |\n| Part VI. The security and robustness challenge. . . . . . . .                                                         | . 50       |\n| Part VII. AI safety .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . | .  .55     |\n| Part VIII. The copyright challenge .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .                            | .  .61     |\n| Part IX. Third-party AI assurance. . . . . . . . . . . . . . . . . . . .                                              | . 65       |\n| Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                               | . 69       |\n| Contacts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                             | . 70       |\n\n|\n\n## Executive summary\n\n## Recent and rapidly advancing breakthroughs in machine learning technology have forever transformed the landscape of AI.\n\nAI systems have become powerful engines capable of autonomous learning across vast swaths of information and generating entirely new data. As a result, society is in the midst of significant disruption with the surge in AI sophistication and the emergence of a new era of technological innovation.\n\nAs businesses grapple with a future in which the boundaries of AI only continue to expand, their leaders face the responsibility of managing the various risks and harms of AI, so its benefits can be realized in a safe and responsible manner.\n\nCritically, these benefits are accompanied by serious considerations and concerns about the safety of this technology and the potential for it to disrupt the world and negatively impact individuals when left unchecked. Confusion about how the technology works, the introduction and proliferation of bias in algorithms, dissemination of misinformation, and privacy rights violations represent only a sliver of the potential risks.\n\nThe practice of AI governance is designed to tackle these issues. It encompasses the growing combination of principles, laws, policies, processes, standards, frameworks, industry best practices and other tools incorporated across the design, development, deployment and use of AI.\n\n<!-- image -->\n\n<!-- image -->\n\nWhile relatively new, the field of AI governance is maturing, with government authorities around the world beginning to develop targeted regulatory requirements and governance experts supporting the creation of accepted principles, such as the Organisation for Economic Co-Operation and Development's AI Principles, emerging best practices and tools for various uses of AI in different domains.\n\nThere are many challenges and potential solutions for AI governance, each with unique proximity and significance based on an organization's role, footprint, broader riskgovernance profile and maturity. This report aims to inform the growing, increasingly empowered and increasingly important\n\n<!-- image -->\n\n2021\n\ncommunity of AI governance professionals about the most common and significant challenges to be aware of when building and maturing an AI governance program. It offers actionable, real-world insights into applicable law and policy, a variety of governance approaches, and tools used to manage risk. Indeed, some of the challenges to AI governance overlap and run through a range of themes. Therefore, an emerging solution for one thematic challenge may also be leveraged for another. Conversely, in certain circumstances, specific challenges and associated solutions may conflict and require reconciliation with other approaches. Some of these potential overlaps and conflicts have been identified throughout the report.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nQuestions about whether and when organizations should prioritize AI governance are being answered: \"yes\" and \"now,\" respectively. This report is, therefore, focused on how organizations can approach, build and leverage AI governance in the context of the increasingly voluminous and complex applicable landscape.\n\n<!-- image -->\n\nUzma Chaudhry IAPP AI Governance Center Research Fellow\n\n<!-- image -->\n\nNina Bryant FTI Technology Senior Managing Director\n\n<!-- image -->\n\nJoe Jones IAPP Director of Research and Insights\n\n<!-- image -->\n\nLuisa Resmerita FTI Technology Senior Director\n\nAshley Casovan IAPP AI Governance Center Managing Director\n\n<!-- image -->\n\nMichael Spadea FTI Technology Senior Managing Director\n\n<!-- image -->\n\n|\n\n## Part I. Understanding AI and governance\n\n## Components of an AI system and their governance\n\nTo understand how to govern an AI system, it is important to first understand what an AI system is. The EU AI Act, for example, defines an AI system as \"a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments.\"\n\nAs indicated in the OECD's Framework for the Classification of AI systems, AI systems are comprised of data used to train and operate a system, model, output and context. While a model is a fundamental building block of an AI system, a single model seldom operates in isolation. Instead, multiple AI models come together and interact with each other to form complex AI systems. Additionally, AI systems are often designed to interact with other systems for sharing data, facilitating seamless integration into real-world environments. This results in a network of AI systems, each with its specialized models, working together to achieve a larger goal.\n\nWith AI poised to revolutionise many aspects of our lives, fresh cooperative governance approaches are essential. Effective collaboration between regulatory portfolios, within nations as well as across borders, is crucial: both to safeguard people from harm and to foster innovation and growth.\n\nKate Jones U.K. Digital Regulation Cooperation Forum CEO\n\n<!-- image -->\n\n## Navigating AI governance sources\n\nGiven the complexity and transformative nature of AI, significant work has been done by law and policymakers on what is now a vast and growing body of principles, laws, policies, frameworks, declarations, voluntary commitments, standards and emerging best practices that can be challenging to navigate. Many of these various sources interact with each other, either directly or by virtue of the issues covered.\n\nAI principles, such as the OECD's AI Principles or UNESCO's Recommendation on the Ethics of AI, can shape global standards, especially when national governments pledge to voluntarily incorporate such guidance into their domestic AI governance initiatives. They provide a nonbinding, principled approach to guide legal, policy and industry efforts toward tackling thematic challenges. Algorithm Watch created an inventory of these principles, identifying 167 reports.\n\nLaws and regulations include existing legislation that is not specific but is nonetheless applicable to AI, as well as emerging legislation that more specifically addresses the governance of AI systems, such as the EU AI Act. The EU AI Act is the world's first comprehensive AI regulation. Although jurisdictional variations can be observed across the emerging global AI regulatory landscape, many draft regulations adopt a risk-based approach similar to the EU AI Act.\n\nThe EU AI Act mandates AI governance standards based on the risk classification of AI systems and the organization's role as an AI actor. Certain AI systems are deemed to pose unacceptable risk and are prohibited by law, subject to very narrow exceptions. The bulk of the requirements imposed by the act apply to providers of high-risk AI systems, although deployers and resellers, namely distributers and importers, are are also subject to direct obligations.\n\nThe act imposes regulatory obligations at enterprise, product and operational levels, such as establishing appropriate accountability structures, assessing system impact, providing technical documentation, establishing risk management protocols and monitoring performance, among other key requirements. In the context of the growing variety of generative AI use cases and adoption of solutions embedding generative AI such as MS Copilot, general purpose AI-specific provisions are another crucial component of the EU AI Act. Depending on their capabilities, reach and computing power, certain GPAI systems are considered to present systemic risk and attract broadly similar obligations to those applicable to high-risk AI systems.\n\nAI governance is about to get a lot harder. The internal complexity of governing AI is growing as more internal teams adopt AI, new AI features are built, and the systems get complex, but at the same time, the external complexity is also set to grow rapidly with new regulations, customer demands, and safety research evolving.\n\nThe organizations who have invested in structured AI governance already have a leg up and will continue to have a competitive advantage.\n\nAndrew Gamino-Cheong Trustible AI Co-founder and Chief Technology Officer\n\n## IAPP Global AI Law and Policy Tracker\n\n<!-- image -->\n\nThis map shows the jurisdictions in focus and covered by the IAPP Global AI Law and Policy Tracker. It does not represent the extent to which jurisdictions around the world are active on AI governance legislation. Tracker last updated January 2024.\n\n<!-- image -->\n\nIn addition to binding legislation, voluntary AI frameworks, such as the National Institute of Standards and Technology's AI Risk Management Framework and the International Organization for Standardization's AI Standards, offer structured and actionable guidance stakeholders can elect to use to support their work on implementing AI governance. Voluntary commitments are often developed to bring different stakeholders closer to a shared understanding of identifying, assessing and managing risks. Standards serve as benchmarks that can demonstrate compliance with regulatory requirements.\n\nInternational declarations and commitments memorialize shared commitments, often between governments, to specific aspects or broad swathes of AI governance. While not binding, such commitments can, at a minimum, indicate a country's support for and intention to advance AI governance in particular or general ways, even at the highest of levels.\n\nNavigating a growing body of draft AI laws, regulations, standards and frameworks can be challenging for organizations pioneering with AI. By understanding their unique AI risk profile and adopting a risk-based approach, organizations can build a robust and scalable AI governance framework that can be deployed across jurisdictions.\n\n<!-- image -->\n\nThe following are examples of some of the most prominent and consequential AI governance efforts:\n\n|                      | â†’   | OECDAI Principles                                                                         |\n|----------------------|-----|-------------------------------------------------------------------------------------------|\n|                      | â†’   | European Commission's Ethics Guidelines for Trustworthy AI                                |\n|                      | â†’   | UNESCO Recommendation on the Ethics of AI                                                 |\n|                      | â†’   | The White House Blueprint for an AI Bill of Rights                                        |\n|                      | â†’   | G7 Hiroshima Principles                                                                   |\n| Laws and regulations | â†’   | EU AI Act                                                                                 |\n| Laws and regulations | â†’   | EU Product Liability Directive, proposed                                                  |\n| Laws and regulations | â†’   | EU General Data Protection Regulation                                                     |\n| Laws and regulations | â†’   | Canada - AI and Data Act, proposed                                                        |\n| Laws and regulations | â†’   | U.S. AI Executive Order 14110                                                             |\n| Laws and regulations | â†’   | Sectoral U.S. legislation for employment, housing and consumer finance                    |\n| Laws and regulations | â†’   | U.S. state laws, such as Colorado AI Act, Senate Bill 24-205                              |\n| Laws and regulations | â†’   | China's Interim Measures for the Management of Generative AI Services                     |\n| Laws and regulations | â†’   | The United Arab Emirates Amendment to Regulation 10 to include new rules on               |\n| Laws and regulations |     | Processing Personal Data through Autonomous and Semi-autonomous Systems                   |\n| Laws and regulations | â†’   | Digital India Act                                                                         |\n|                      | â†’   | OECD Framework for the classification of AI Systems                                       |\n|                      | â†’   | NIST AI RMF                                                                               |\n|                      | â†’   | NIST Special Publication 1270: Towards a Standard for Identifying and Managing Bias in AI |\n|                      | â†’   | Singapore AI Verify                                                                       |\n|                      | â†’   | The Council of Europe's Human Rights, Democracy, and the Rule of Law Assurance            |\n|                      |     | Framework for AI systems                                                                  |\n|                      | â†’   | Bletchley Declaration                                                                     |\n|                      | â†’   | The Biden-Harris Administration's voluntary commitments from leading AI companies         |\n|                      | â†’   | Canada's guide on the use of generative AI                                                |\n| Standards efforts    | â†’   | ISO/IEC JTC 1 SC 42                                                                       |\n| Standards efforts    | â†’   | The Institute of Electrical and Electronics Engineers Standards Association P7000         |\n| Standards efforts    | â†’   | The European Committee for Electrotechnical Standardization AI standards for EU AI Act    |\n| Standards efforts    | â†’   | The VDEAssociation's AI Quality and Testing Hub                                           |\n| Standards efforts    | â†’   | The British Standards Institution and Alan Turing Institute AI Standards Hub              |\n| Standards efforts    | â†’   | Canada's AI and Data Standards Collaborative                                              |\n\n<!-- image -->\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nINDIVIDUALS AND SOCIETY Risk of bias detrimental\n\nLEGAL AND REGULATORY\n\nor other impact on individuals\n\nRisk of contractual obligations noncompliance with legal and\n\n## The AI governance imperative\n\nWith private investment, global adoption rates and regulatory activity on the rise, as well as the growing maturity of the technology, AI is increasingly becoming a strategic  priority for organizations and governments worldwide. Organizations of all sizes and industries are increasingly engaging with AI systems at various stages of the technology product supply chain.\n\n## AI Risks\n\n<!-- image -->\n\n<!-- image -->\n\n## FINANCIAL\n\nRisk of financial implications, e.g., fines, legal or operational costs, or lost profit\n\nThe exceptional dependence on high volumes of data and endless practical applicability that make AI technology a disruptive opportunity can also generate uniquely multifaceted risks for businesses and individuals. These include legal, regulatory, reputational and/or financial risks to organizations, but also risks to individuals and the wider society.\n\n|\n\n<!-- image -->\n\n## Enterprise governance\n\nAI governance starts with defining the corporate strategy for AI by documenting:\n\n- â†’ Target operating models to set out clear roles and responsibilities for AI risk.\n- â†’ Compliance assessments to establish program maturity and remediation priorities.\n- â†’ Accountability processes to record and demonstrate compliance.\n- â†’ Policies and procedures to formulate policy standards and operational procedures.\n- â†’ Horizon scanning to enhance and align the program with ongoing regulatory developments.\n\n## Product governance\n\nAI governance also requires enterprise policy standards to be applied at the product level. Organizations can ensure their AI products match their enterprise strategy by using:\n\n<!-- image -->\n\n- â†’ System impact assessments to identify and address risk prior to product development or deployment.\n- â†’ Quality management procedures tailored to the software development life cycle to address risk by design.\n- â†’ Risk and controls frameworks to define AI risk and treatment based on widely recognised standards such as ISO and NIST.\n- â†’ Conformity assessments and declarations to demonstrate their products are compliant.\n- â†’ Technical documentation including standardized instructions of use and technical product specifications.\n- â†’ Post-market monitoring plans to monitor product compliance following market launch.\n- â†’ Third-party due diligence assessments to identify possible external risk and inform selection.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Operational governance\n\nThe organization's AI strategy must ultimately be operationalized throughout the business through the development of:\n\n- â†’ Performance monitoring protocols to ensure systems perform adequately for their intended purposes.\n- â†’ Transparency and human oversight initiatives to ensure individuals are aware and can make informed choices when they interact with AI systems or when AI-powered decisions are made.\n- â†’ Incident management plans to identify, escalate and respond to serious incidents, malfunctions and national risks impacting AI systems and their operation.\n- â†’ Communication strategies to ensure transparency toward internal and external stakeholders in relation to the organization's AI practices.\n- â†’ Training and awareness programs to enable staff with roles and responsibilities for AI governance to help them understand and perform their respective roles.\n- â†’ Skills and capabilities development to assess human resources capabilities and review or design job requirements.\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\nUnderstanding that AI systems, like all products, follow a life cycle is important as there are governance considerations across the life cycle. The NIST AI RMF sets out a comprehensive articulation of the AI system\n\n<!-- image -->\n\nlife cycle and includes considerations for testing, evaluation, validation, verification and key stakeholders for each phase. A more simplified sample life cycle is included above, along with some top-level considerations.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n## â†’ HOW TO Navigate developers from deployers\n\nVarious contractual and regulatory obligations may arise depending on whether an organization is a vendor or buyer, or if it sources external services such as hardware, cloud or data collection, for the development and operations of its AI system.\n\nIn both current legislation and proposed legislation we are starting to see different obligations for those who provide and supply AI versus those who deploy AI. Understanding whether you are a developer and/or deployer is important to ensuring you meet compliance obligations. Once this is understood, it is possible to establish AI-governance processes for procurement, including evaluations and contracts to avoid taking on additional liabilities.\n\n<!-- image -->\n\nPrior IAPP research found more than 70% of organizations rely at least somewhat on third-party AI, so the responsibility for ensuring the AI system is safe and responsible may be spread across multiple roles.\n\n- â†’  The World Economic Forum put together a useful toolkit to help those who are procuring AI systems.\n\n<!-- image -->\n\n|\n\n## Part II. The data challenge\n\n## Data is an integral part of training and operating an AI system.\n\nMost AI requires sizeable amounts of high-quality data, especially during the training phase to maximize the model's performance, as well as to ensure the desired and accurate output. With the advancement of new AI technologies,  models are requiring increasingly more data, which may come from a variety of sources. Given the importance of the data used to fuel the AI system, it is important to understand what data is being used; how, where and by whom it was collected; from whom it was collected; if it is the right data for the desired outcome; and how it will be managed throughout the life cycle.\n\n## Accessing data and identifying data sources\n\nUnderstanding where data comes from and how it is collected is not only necessary for AI systems, but also for building trust in AI by ensuring the lawfulness of data collection and processing. Such documentation can assist with data transparency and improve the AI system's auditability as well.\n\nAlthough data may originate from multiple sources, it can be broadly categorized into three types: first-party data, public data and third-party data.\n\n<!-- image -->\n\n## First-party data\n\nThis refers to data collected directly from individuals by an organization through their own interactions and transactions. Such data may originate from sources such as website visits, customer feedback and surveys, subscriptions, and customer relationship management systems, among others. This data is extremely valuable for organizations as it provides direct and firsthand insights into individuals' behavior.\n\nFirst-party data can be collected from various sources. Identifying the data channels and documenting the source will not only help the organization determine what types of data, e.g., text, numerical, image or audio, will be collected from each source, but also alert the legal team about where legal compliance will be required on the organization's part.\n\n## Public data\n\nThis refers to data that is available to the wider public and encompasses a range of sources, such as publicly available government records, publications, and open source and web-scraped data. Public data is a valuable resource for researchers and innovators as it provides readily available information. Public data can come from multiple sources.\n\nWhile it is arduous and cumbersome to maintain data lineage for public datasets, it is important for upholding organizational reputation and fostering user trust, legal compliance and AI safety overall. A lack of understanding of where data comes from eventually leads to a lack of understanding of the training dataset and model performance, which can reinforce the black-box problem. Therefore, in the interest of transparency, tracking and documenting public-data sources as much as possible may prove beneficial for the organization, as it can later support other transparency efforts, such as drawing up data, model or system cards.\n\nMoreover, without knowledge of public-data sources, the organization may inadvertently train the AI system on personal, sensitive or proprietary data. From the privacy standpoint, this can be problematic in cases of data leakage, where personally identifiable data may be exposed. AI security challenges may also be amplified if data was procured from unsafe public sources, as that carries the risk of introducing malicious bugs into the system. It may also lead to biases in the AI system.\n\nEthical development practices start with responsible data acquisition and management systems, as well as review processes that track the lineage of sourced data.\n\nChristina Montgomery IBM Vice President and Chief Privacy and Trust Officer\n\nWithout understanding the quality of the data being ingested into an AI model, you may not know the quality of the output. Companies must establish and define what 'data quality' involves and consists of, as this determination is highly contextual for any organization, and can depend on business goals, use cases, focus areas and fitness for purpose.\n\nRegardless of context, there are minimum baseline attributes which can and should be established: accuracy, completeness, consistency and validity. Timeliness and uniqueness may also be important to establishing fitness for purpose.\n\nDera Nevin FTI Technology Managing Director\n\n<!-- image -->\n\nAn organization can begin by establishing a clear understanding of how and why public data is being collected, how it aligns with the purposes the AI system will fulfil, if and how system accuracy will be affected by using public data, what the trustworthy sources for gathering public data are, if the organization has rights to use the public data, and other legal considerations that may have to be taken into account, particularly given that public data is treated differently across jurisdictions.\n\n## Third-party data\n\nThis refers to data obtained or licensed by the organization from external entities that collect and sell data, such as data brokers. Datasets purchased from brokers are webbed together from a wide range of sources. While this may have the benefit of providing insights into a wider user base, the insights may not be accurate or may be missing key data. It may lack direct insights into customer behavior, as brokers do not interact with the organization's customer base.\n\nThird-party data can also include open-source data, available through open-source data catalogues. Sometimes these databases are provided by government or academic institutions with a clear understanding of how the data was collected and how it can be used, including a clear use license. Open-source data collected through other community efforts may not follow the same collection and distribution practices. As when using all data, it is important to know where the data came from, how it was collected, in which context it is meant to be used and what rights you have to use it.\n\n## Data quality\n\nThe quality of data that AI is trained and tested on directly impacts the quality of the outputs and performance, so ensuring the data is high quality can help lay the initial foundations for a safe and responsible AI system. Measuring data quality often includes a few baseline considerations.\n\nAccuracy confirms the correctness of data. That is, whether the data collected is based on real-world insights. Completeness refers to checking for missing values, determining the usability of the data, and looking for any over or underrepresentation in the data sample. Validity ensures data is in a format that is compatible with intended use. This may include valid data types, metadata, ranges and patterns. Consistency refers to the relationships between data from multiple sources and includes checking if the data shows consistent trends and values it represents. Ideally, this process of ensuring data quality is documented to support transparency, explainability, data fairness, auditability, understanding of the data phase of the life cycle and system performance.\n\n<!-- image -->\n\n## Appropriate use\n\nOne of the most significant challenges when designing and developing AI systems is ensuring the data used is appropriate for the intended purpose. Often data is collected with one intention in mind or within a specific demographic area, and, while it might appear to be a useful dataset, upon further analysis it might include data that does not match the industry or geographic area of operation. When data is not fit for purpose, it can skew the AI system's predictions or outcomes.\n\nWhen thinking about appropriate use, consider the proportionality of data required for the desired outcome. Often, there are occurrences of collecting or acquiring more data than necessary to achieve the outcome. It is important to understand if it is even necessary to collect and use certain data in your AI system.\n\nManaging unnecessary data, especially data that may contain sensitive attributes, can increase an organization's risk of a breach or harm resulting from the use of AI.\n\n## Law and policy considerations\n\nApproaches can be categorized according to how the data was collected.\n\n<!-- image -->\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n## First-party data\n\nWhere first-party data amounts to personal or sensitive data, relevant provisions may be triggered under the data protection and privacy legislation of the jurisdictions where the organization carries out its business, where the processing takes place or where the individuals concerned are located.\n\nThe EU General Data Protection Regulation, for instance, has a default prohibition against processing of personal data, unless such processing falls under one of the six bases for lawful processing under Article 6(1): consent, contractual performance, vital interest, legal obligation, public task and legitimate interest pursued by a controller or third party.\n\n## Public data\n\nWeb scraping may involve compliance with the terms of service and privacy policies of websites. Otherwise, when an organization is aware the public dataset contains personal or sensitive information, lawfulness of use may require compliance with relevant data protection or privacy laws, such as by acquiring valid consent.\n\nWhile web scraping, it is possible for copyrighted data to be collected to train AI systems.\n\nAnother type of public data is open-source data, which is publicly available software that may include both code and datasets. Although accessible to the public, open-source software is often made available by the organization through various open-source licensing schema. In addition to complying with the terms of the licenses, organizations using open-source data may also consider conducting their own due diligence to ensure the datasets were acquired lawfully, are safe to use and were assessed for bias mitigation.\n\n## Third-party data\n\nAs organizations have neither proximity to how third-party data was first collected nor direct control over the data governance practices of third parties, an organization can benefit from carrying out its own legal due diligence and third-party risk management. The extent and intensity of this exercise will largely depend on the organization's broader governance and riskmanagement approach and the relevant facts.\n\nLegal due diligence may include verification of the personal data's lawful collection by the data broker, review of contractual obligations and licenses, and identification of protected intellectual property interests. When data is licensed, the organization will first have to lawfully procure rights to use data through a licensing agreement. This will help maintain data provenance and a clear understanding of data ownership. The lawful and informed use of such data at subsequent stages of the AI life cycle will also be governed by the license.\n\nWith growing public concerns and increased regulation aimed at developing trustworthy, transparent and performative AI systems, an internal data governance program is integral to understanding and documenting metadata prior to usage, and to identifying risks associated with lawful data use.\n\nChristina Montgomery IBM Vice President and Chief Privacy and Trust Officer\n\n<!-- image -->\n\n## â†’ SPOTLIGHT\n\n## Joint statement by international data protection and privacy authorities on web scraping\n\nIn August 2023, 12 international data protection and privacy authorities released a joint statement to address data scraping on social media platforms and other publicly accessible websites.\n\nThe joint statement outlined:\n\n- â†’ Key privacy risks associated with data scraping, such as targeted cyberattacks, identity fraud, monitoring and profiling individuals, unauthorized political or intelligence gathering, and unwanted direct marketingÂ or spam.\n- â†’ How social media companies and other websites should protect individuals' personal information from unlawful data scraping, such as through data security measures and multilayered technical and procedural controls to mitigate the risk.\n- â†’ Steps individuals can take to minimize the privacy risks of scraping, including reading a website's privacy policy, limiting information posted online, and understanding and managing privacy settings.\n\n<!-- image -->\n\nSome key takeaways from the joint statement include:\n\n- â†’ Publicly accessible personal information is still subject to data protection and privacy laws in most jurisdictions.\n- â†’ Social media companies and other website operators hosting publicly accessible personal data have legal obligations to protect personal information on their platforms from unlawful data scraping.\n- â†’ Accessing personal information through mass data scraping can constitute reportable data breaches in many jurisdictions.\n- â†’ Individuals can take steps to prevent their personal information from being scraped, and social media companies have a role to play in empowering users to engage with social media services in a manner that upholdsÂ privacy.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\nIBM believes it is essential for data management practices tied to AI development to include advanced filtering and curation techniques to identify untrustworthy, protected/sensitive, explicit, biased/nonrepresentative or otherwise unwanted data.\n\nChristina Montgomery IBM Vice President and Chief Privacy and Trust Officer\n\n## Implementing AI governance\n\nNumerous strategies are being leveraged to manage data in the context of AI.\n\n## Data management plans\n\nAlongside ensuring the lawfulness of data acquisition, there are numerous measures an organization can take to keep track of where the data used to train AI systems comes from. Such organizational practices are especially important with the advent of generative AI, where training data is merged from numerous sources.\n\nDeveloping a comprehensive plan for how data is managed across an organization is a foundational element to managing all AI systems. Some considerations for data management plans include understanding what data is being used in which system; how it is collected, retained and disposed; if there is lawful consent to use the data; and who is responsible for ensuring the appropriate oversight.\n\nIt is likely your organization is already keeping track of the data used across the organization. While there are additional considerations involved when using data for AI systems as discussed above, it is possible to add to your existing data workflows or management practices. It is important to consider the use and management of data used for AI systems at every stage of the life cycle as there are different concerns and implications to consider during different stages. If your organization does not already have a data management practice, resources such as those from Harvard Biomedical Data Management can help you get started.\n\nAdditionally, the data management plan should identify relevant data standards, such as ISO 8000 for data quality, to set appropriate controls and targets for your organization to meet. Data standards for aspects of AI are under development through various initiatives at the NIST, ISO/IEC and other national standards bodies.\n\n<!-- image -->\n\n## Data labels\n\nGrowing in importance, data labels are tools that can require organizations to provide information on how data was collected and used to train AI models. They are transparency artifacts of AI datasets that explain the processes and rationale for using certain data and explain how it was used in training, design, development and use. This will help explain if the data being used is fit for purpose, if it is representative of the demographics being served with the AI system and if the data meets relevant data quality standards.\n\nIdeally data labels are requirements of a robust data management process, which includes data quality and data impact assessments. While data labels are intended to provide documentation and awareness of the data being used, they can also assist with the assessment and review process. These tools should be aligned where possible within the organization to avoid redundant efforts.\n\nData-source maintenance through documentation and inventories can help organizations keep track of where the data is acquired and carry out relevant legal due diligence at first-party or third-party levels.\n\n## Dedicated processes and functions\n\nWhen third-party data is used, it is important to follow the terms of service and provide attribution where possible. This will also help inform users of the AI system where the data originated. Where possible, when data is being used from a third party, an appropriate data sharing agreement with clear terms of use for both parties is highly recommended. This helps to resolve any liability issues that may arise as a result of using the system.\n\n<!-- image -->\n\n<!-- image -->\n\n|\n\n## Part III. TheÂ privacy and data protection challenge\n\n## Privacy and data protection governance practices are woven into the AI life cycle.\n\nGiven that AI is a data-dependent enterprise and that privacy law governs the processing of personal data, privacy laws have emerged as a prominent mechanism for managing the key AI governance challenges. After all, information privacy seeks to provide a framework \"for making ethical choices about how we use new technologies.\"\n\nIndeed, national data protection authorities have been among the first to intervene and bring enforcement actions when AI-based products were thought to harm consumers. For example, Italy's data protection authority, the Garante, imposed a temporary ban on ChatGPT after concluding the service was in violation of the GDPR for lacking a legal basis for processing and age-verification mechanism.\n\nThe enforcement landscape for AI governance is incredibly unsettled. Which regulators will lead on what and how they will collaborate or conflict is subject to heavy debate and will differ by country, creating heightened uncertainty for organizations. Whether or not privacy regulators have the lead remit, they will play a key role given the centrality of data to AI governance.\n\nCaitlin Fennessy IAPP Vice President and Chief Knowledge Officer\n\n<!-- image -->\n\n## Law and policy considerations\n\nThe OECD's Guidelines Governing the Protection of Privacy and Transborder Flows of Personal Data - developed in 1980 and revised in 2013 - enshrine eight principles that have served as the foundation for most global privacy and data protection laws written over the past several decades, including landmark legislation such as the GDPR. These eight principles include collection limitation, data quality, purpose specification, use limitation, security safeguards, openness, individual participation and accountability.\n\nMany DPAs around the world already put forth guidance on how AI systems can work to align themselves with these foundational principles of information privacy. Yet, as Australia's Office of the Victorian Information Commissioner noted in a resource on issues and challenges of AI and privacy, \"AI presents challenges to the underlying principles upon which the (OECD Privacy) Guidelines are based.\" To better understand where these challenges currently exist, each of these principles is discussed below in the context of their applicability to - and potential conflict with - the development of AI.\n\n## Collection limitation\n\nThe principle of collection limitation states, \"There should be limits to the collection of personal data and any such data should be obtained by lawful and fair means and, where appropriate, with the knowledge or consent of the data subject.\" It most readily translates to the concept and practice of data minimization. GDPR Article 5(1)(c) emanates from this idea that data, at the collection stage, should have some predefined limit or upper bound. Specifically, data collection should be \"... limited to what is necessary in relation to the purposes for which they are processed.\" As many observers have noted, this is one of the privacy principles for which there appears to be an \"inherent conflict\" with AI systems that rely on the collection and analysis of large datasets. Performing adequate AI bias testing, for example, requires collecting more data than might otherwise be collected.\n\nAt Mastercard, we are testing innovative tools and technologies to address some of the potential tensions between privacy and AI governance. For instance, we know that a lot of data is needed, including sometimes sensitive data, for AI to produce unbiased, accurate and fair outcomes.\n\nHow do you reconcile this with the principle of data minimization and the need for individual's explicit consent? We are exploring how the creation of synthetic data can help, so as to achieve all desired objectives at the same time.\n\nCaroline Louveaux\n\nMastercard Chief Privacy and Data Responsibility Officer\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Data quality\n\nThis is the principle that \"Personal data should be relevant to the purposes for which they are to be used, and, to the extent necessary for those purposes, should be accurate, complete and kept up-to-date.\" Data quality is the privacy principle with which AI may be most in synchrony. The accuracy of AI model outputs depends significantly on the quality of their inputs. A breakdown in AI governance can lead to data becoming inconsistent and error-laden, underscoring the need for AI-based systems to orient themselves around the principle of data quality. Data brokers and other companies can become the target of enforcement actions for failing to ensure the accuracy of the data they collect and sell.\n\n## Purpose specification\n\nThe principle of purpose specification states, \"The purposes for which personal data are collected should be specified ... and the subsequent use limited to the fulfilment of those purposes ...\" Indeed, as the U.K. Information Commissioner's Office explained in the context of its consultation on purpose limitation in the generative AI life cycle, purposes of data processing \"must be specified and explicit: organizations need to be clear about why they are processing personal data.\" This need for clarity applies not only to internal documentation and governance structures, but in communication with the people to whom the personal data relates. In sum, organizations should be able to explain what personal data they process at each stage and why it is needed to meet the specified purpose.\n\nA conflict with the purpose specification principle can arise if and when a developer wants to use the same training dataset to train multiple models. The ICO advises developers reusing training data to consider whether the purpose of training a new model is compatible with the original purpose of collecting the training data. Considering the reasonable expectations of those whose data is being reused can help an organization make a compatibility assessment. Currently, the ICO considers collating repositories of web-scraped data, developing a generative AI model and developing an application based on such a model to constitute different purposes under data protection law.\n\n<!-- image -->\n\n## Use limitation\n\nRelated to purpose specification, use limitation is the principle that states personal data \"should not be disclosed, made available or otherwise used for purposes other than those specified,\" except with the consent of the data subject or by the authority of law. Purposes of use must be specified at or before the time of the collection, and subsequent uses must not be incompatible with the initial purposes of collection.\n\nThis is another principle that is challenged by AI systems, with potential regulatory gaps left by both the EU GDPR and EU AI Act. Proposals to address these gaps have included restricting the training of models only to stated purposes and requiring alignment between training data collection and the purpose of a model.\n\n## Security safeguards\n\nUniting the fields of privacy, data protection and cybersecurity for decades is the principle that \"Personal data should be protected by reasonable security safeguards against such risks as loss or unauthorized access, destruction, use, modification or disclosure of data.\" Ensuring the security of personal data collected and processed is a key to building and maintaining trust within the digital economy.\n\nRemedying problems of security and safety is and will remain a critical challenge for AI. Ensuring the actions of an AI system align \"with the values and preferences of humans\" is central to keeping these systems safe. Yet, many AI systems remain susceptible to hacking and so-called \"adversarial attacks,\" which are inputs designed to deceive an AI system, as well as data poisoning, evasion attacks and model extraction. Examples include forcing chatbots to provide answers to responses to harmful prompts or getting a self-driving vehicle's cameras to misclassify a stop sign as a speed-limit sign.\n\n## Openness\n\nThe right to be informed and the principle of transparency are touchstones of global privacy and data protection laws. Beginning at the collection stage and enduring throughout the life cycle of processing, these rights form the basis of organization's transparency obligations. They often require organizations to disclose various types of information, from the types of data collected and how it is used to the availability of data subjects' rights and how to exercise them to the logic involved and potential consequences of any automated decision-making or profiling the organization engages in. The \"black-box\" nature of many AI systems can make this principle challenging to navigate and adhere to.\n\nAs all AI and machine learning models are 100% data dependent, the models must be fed highquality, valid, verifiable data with the appropriate velocity. As obvious as that may be, the challenges around establishing the governance requirements that ensure the appropriate use of private data may be far more complex. Modelers should absolutely be applying the minimization principle of identifiable data as they train. Adding private data that could leak or cause bias needs to be thought through early in the design process.\n\nScott Margolis\n\nFTI Technology Managing Director\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Individual rights\n\nIndividual rights in privacy law commonly include the rights to access, opt in/opt out, erasure, rectification and data portability, among others. Many privacy laws contain rights for individuals to opt-out of automated decision-making underpinned by AI systems.\n\n## Accountability\n\nAccountability is arguably one of the most important principles when it comes to operationalizing organizational governance. Accountability is based on the idea that there should be a person and/or entity that is ultimately responsible for any harm resulting from the use of the data, algorithm and AI system's underlying processes.\n\n## Implementing AI governance\n\nThe practice and professionalization of AI governance is a highly specialized, stand-alone field requiring multidisciplinary expertise. A holistic approach to AI governance requires support from established subject-matter areas, including data protection and information governance practitioners. Data from past IAPP research shows 73% of organizations are leveraging their existing privacy expertise to manage AI governance. This is not surprising, as data is a critical component of AI. Good AI governance weaves privacy and data governance practices into the AI life cycle alongside AI-specific issues. This chapter demonstrates the overlapping nature of privacy and AI governance.\n\nApproaching the implementation of AI governance by adapting existing governance structures and processes enables organizations to move forward quickly, responsibly and with minimal disruption to innovation and the wider business. Target processes that may already be established by organization's data protection program include: accountability, inventories, privacy by design and risk management.\n\n|\n\n<!-- image -->\n\n## Accountability\n\nPrivacy compliance programs are likely to have established roles and responsibilities for those with direct and indirect responsibility for privacy compliance. These are likely supported by policies and procedures to help individuals fulfil the expectations of their role. Senior management contributions are likely channeled through privacy committees, with mechanisms in place to support risk-based escalation, reporting on key metrics and decision-making.\n\nPrivacy leaders often have a direct line to CEOs and boards of directors, as well as a matrixed structure of privacy champions across the organization to enable a multidisciplinary approach to privacy governance and ensure data protection needs are considered by product and service teams. This structure is well-suited to, and can be leveraged for, AI governance given the need for leadership engagement and skills spanning legal, design, product and technical disciplines.\n\nWhere AI systems process personal data, those with accountability for privacy compliance will need to ensure their existing privacy compliance processes are set up to address the intersection between AI and privacy. This will include considering data inventory,\n\n<!-- image -->\n\ntraining, privacy by design and other topics further outlined in this section.\n\n## Inventories\n\nPersonal data inventories have long been the foundation of establishing a successful privacy program and a key requirement of privacy regulations. Knowing your data, how it is collected and used, and being able to demonstrate this remains a core part of accountability. Organizations have also matured in their approaches, from lengthy spreadsheets to technology-enabled approaches.\n\nWhere AI systems use personal data, the data inventory can play a crucial role. Organizations that have captured additional privacy compliance metadata alongside the minimum regulatory requirements may find their personal data inventories particularly useful in the age of AI. Additional uses of this metadata could include a single source of truth for lawful basis to identify if additional use within AI models is permitted, accuracy metrics on personal data to support AI models to make accurate inferences based on the latest personal data and a top-down view on processes relying on automated decision-making that can be aligned with AI registries.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\nLegal professionals need to keep an open and flexible mind - technology brings new challenges but also new solutions. General counsel should position themselves as the center of a multidisciplinary team of stakeholders across their organizations, including product design, compliance, data and privacy, which can deploy to manage multifaceted data risks. Companies that strive for established best privacy practice will more easily be able to comply with the rising standards of global privacy laws.\n\nTim de Sousa FTI Technology, Managing Director, Australia\n\nEffective AI governance is underpinned by AI inventories with similar functionalities to those of data inventories. AI registers can help organizations keep track of their AI development and deployment. Some functional requirements that overlap with data inventories include the ability to connect into the system-development life cycle, maintenance and regular updates by multiple users, and logging capability to ensure integrity.\n\n## Privacy by design\n\nBy embedding privacy at the outset, privacy by design continues to be a critical part of how organizations address privacy concerns. In implementing privacy by design, privacy functions may take steps to map and embed privacy into areas such as system-development life cycles, project initiation and development approaches within an organization, risk management and approval workflows, and stage gates.\n\nSteps may include developing AI-specific risk-assessment workflows into existing risk-assessment processes, enhancing existing control catalogs with AI and privacy controls, or updating approval workflows to include stakeholders with AI accountabilities.\n\nAdditionally, the growing maturity of privacy enhancing technologies and their increasing traction as technical measures within organizations may have benefits for the development of AI. With some PETs potentially helping organizations reduce inherent risk of data use, an organization may be able to maximize the strategic use of its data. Examples include using differential privacy in training machine-learning models, federated learning and synthetic data.\n\n## Risk management\n\nThe risk-based approach often adopted by global privacy regulations has been distilled into organizational risk-management efforts, which put privacy impact assessments at the heart of deciding whether an organization can reduce harm from personal data processing through the implementation of organizational and technical measures. Privacy risk can also stem from wider privacy compliance activities and lessons learned in areas such as vendor risk, incident management and data subject requests management.\n\n<!-- image -->\n\nPrivacy risk may already feed into wider enterprise riskmanagement programs, such as information technology and cybersecurity risk and control frameworks. These can be enhanced to accommodate the complex types and sources of AI risk into a unified risk-management framework at the enterprise level. This approach can also facilitate crucial visibility across different subject-matter practice areas across the business and enable a more effective analysis and treatment of AI risk.\n\nAs AI risk-management approaches mature, AI governance professionals face choices between embedding algorithmic impact assessments alongside or within PIAs. The need to align AI risk management with broader enterprise risk-management efforts is of equal importance. AI governance professionals will likely need to update enterprise risk-management strategies and frameworks to clearly factor in AI-related risks and document ongoing AI risks and remediations in a formal risk register.\n\n## Risk-assessments\n\nA wide range of AI risk assessments are often talked about in the emerging global AI governance landscape.\n\nSome of these assessments are required by existing data protection legislation, such as the GDPR, while others may emerge from AI-specific laws, policies and voluntary frameworks. For the latter, laws and policies often provide AI governance solutions with knowledge of the overlap.\n\n<!-- image -->\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n## â†’ SPOTLIGHT\n\n## AI governance assessments: A closer look at EU DPIAs and FRIAs\n\n## GDPR: DPIAs\n\nData protection impact assessments are required under GDPR Article 35. DPIAs are particularly important where systematic and extensive evaluation of personal or sensitive aspects of natural persons through automated systems or profiling leads to legal consequences for that person. Incorporating these assessments within the AI-governance life cycle can help organizations identify, analyze and minimize data-related risks and demonstrate accountability.\n\nDPIAs at a minimum contain:\n\n- â†’ A systematic description of the anticipated processing, its purpose and pursued legitimate interest.\n- â†’ A necessity and proportionality assessment in relation to the intended purpose for processing.\n- â†’ An assessment of the risks to fundamental rights and freedoms.\n- â†’ Measures to be taken to safeguard security risks and protect personal data.\n\n<!-- image -->\n\n## EU AI Act: FRIAs\n\nUnder the EU AI Act, FRIAS are required to be carried out in accordance with Article 27 by:\n\n- â†’ Law enforcement when they use real-time remote biometric identification AI systems, which are a prohibited AI practice under Article 5.\n- â†’ Deployers of high-risk AI systems that are governed by public law, private operators that provide public services and operators deploying certain high-risk AI systems referred to in Annex III, point 5 (b) and (c), such as banking or insurance entities.\n\nFRIAs are required only for the first use of the high-risk AI system, and the act permits deployers to rely on previously conducted FRIAs, provided all information about the system is up to date. FRIAs must consist of:\n\n- â†’ Descriptions of the deployer's processes in line with intended use and purpose of the high-risk AI system.\n- â†’ Descriptions of the period and frequency of the high-risk AI system's use.\n- â†’ Categories of individuals or groups likely to be affected by the high-risk system.\n- â†’ Specific risks of harm that are likely to affect individuals or groups.\n- â†’ Descriptions of the human oversight measures in place according to instructions of use.\n- â†’ Measures to be taken when risk materializes into harm, including arrangements for internal governance and complaint mechanisms.\n\nHowever, AI governance solutions often foresee the overlap with existing practices, and this is no different under the EU AI Act. FRIAs, for instance, do not need to be conducted for aspects covered under existing legislation. As such, if a DPIA and FRIA have an overlapping aspect, that aspect needÂ only be covered under DPIA.\n\n<!-- image -->\n\n|\n\n## Part IV. TheÂ transparency, explainability and interpretability challenge\n\n## The black-box problem\n\nOne reason for the lack of trust associated with AI systems is the inability of users, and often creators, of AI systems to have a clear understanding of how AI works. How does it arrive at a decision? How do we know the prediction is accurate? This is often referred to as the \"black-box problem\" because the model is either too complex for human comprehension or it is closed and safeguarded by intellectual property.\n\nAI techniques, such as deep learning, are becoming increasingly complex as they learn from terabytes of data, and the number of parameters has grown exponentially over the years. In July 2023, Meta released its Llama 2 model with a parameter count at 70 billion. Google's PaLM parameter count is reported to be as large as 540 billion. Due to the self-learning abilities of AI, including their size and complexity, the black-box problem is increasingly difficult to solve and often requires a trade-off to simplify aspects of the system.\n\nTransparency is a term of broad scope, which can include the need for technical and nontechnical documentation across the life cycle. Having strong product documentation in place can also provide commercial benefits by supporting the product sales cycle and helping providers to navigate prospective clients' due diligence protocols.\n\n|\n\n<!-- image -->\n\nIn the open-source context, transparency can also refer to providing access to code or datasets in the open-source community to be used by AI systems. Transparency objectives can also include informing users when they are interacting with an AI system or identifying when content was AI generated. Independent of how the term is used, transparency is a key tenet of AI governance due to the desire to understand how AI systems are built, managed and maintained. It is crucial that clear and comprehensive documentation is available to those who design and use these systems to ensure trust and help identify where an error was made if an issue occurs.\n\nExplainability refers to the understanding of how a black-box model, i.e., an incomprehensible or proprietary model, works. While useful, the difficulty with black-box models is that the explanation may not be entirely accurate or faithful to the underlying model, given its incomprehensibility. When full explainability is not possible due to the factors mentioned above, an alternative is interpretability.\n\nInterpretability, on the other hand, refers to designing models that inherently make the reasoning process of the model understandable. It encourages designing models that are not black boxes, with decision or prediction processes that are comprehensible to domain experts. In other words, interpretability is applied ante hoc. While it does away with the problems of explainable models, interpretable models are often domain specific and require significant effort to develop in terms of domain expertise.\n\n## Law and policy considerations\n\nOne proposed solution to the black-box challenge has been codifying approaches to and requirements for transparency, explainability and interpretability in law or policy initiatives. Regulatory and voluntary governance tools that have established requirements for tackling the black-box problem through transparency and explainability include the EU GDPR and AI Act, NIST AI RMF, U.S. Executive Order 14110, China's Interim Measures for the Management of Generative AI Services, and Singapore's AI Verify.\n\n<!-- image -->\n\n<!-- image -->\n\n|\n\nThe EU is first out of the gate with comprehensive AI legislation but the EU AI Act is just the tip of the regulatory iceberg. MoreÂ guidance is coming and many laws enacted since the early 2000s, and under the recent European Data Strategy, will have to be considered in AI governance programs. The EU will continue to promote its approach to regulating AI on the global stage, furthering the Brussels effect on digital regulation.\n\nIsabelle Roccia IAPP Managing Director, Europe\n\n<!-- image -->\n\n## EU GDPR\n\nArguably one of the first legislative requirements for AI governance, GDPR Articles 13(2)(f), 14(2) (g) and 15(1)(h) refer to providing meaningful information about the logic underpinning automated decisions, as well as information about the significance and envisaged consequences of the automated decisionmaking for the individual. This is further supported by Article 22 and Recital 71, which state such decision-making should be subject to safeguards, such as through the right to obtain an explanation to challenge an assessment.\n\n## EU AI Act\n\nThe EU AI Act takes a risk-based approach to transparency, with documentary and disclosure requirements attaching to high-risk and generalpurpose AI systems.\n\nIt mandates drawing up technical documentation for high-risk AI systems, and requires high-risk AI systems to come with instructions for use that disclose various information, including characteristics, capabilities and performance limitations. To make high-risk AI systems more traceable, it also requires AI systems to be able to automatically allow for the maintenance of logs throughout the AI life cycle.\n\nSimilarly, the AI Act places documentation obligations on providers of general-purpose AI systems with and without systemic risks. This includes maintenance of technical documentation, including results from training, testing and evaluation. It also requires up-todate information and documentation to be maintained for providers of AI systems who intend to integrate GPAI into their system. Providers of GPAI systems with systemic risks must also publicly disclose sufficiently detailed summaries of the content used for training GPAI.\n\nWith certain exceptions, the EU AI Act provides individuals with the right to an explanation from deployers of individual decision-making \"on the basis of the output from a high-risk AI system ... which produces legal effects or similarly significantly affects that person in a way that they consider to have an adverse impact on their health, safety or fundamental rights.\"\n\nIn addition to the documentary and disclosure requirements, the AI Act seeks to foster transparency by mandating machine-readable watermarks. Article 50(2) requires machinereadable watermarks for certain AI systems and GPAI systems, so content can be detected as AI generated or to inform users when they are interacting with AI.\n\n<!-- image -->\n\n## NIST AI RMF\n\nThe NIST AI RMF sees transparency, explainability and interpretability as distinct characteristics of AI systems that support each other. Under the RMF, transparency is meant to answer the \"what,\" explainability the \"how\" and interpretability the \"why\" of a decision.\n\n- â†’ Accountability and transparency: The RMF defines transparency as the extent to which information about an AI system and its outputs are made available to individuals interacting with AI, regardless of whether they are aware of it. Meaningful transparency includes the disclosure of appropriate levels of information at different stages of the AI life cycle, tailored to the knowledge or role of the individual interacting with the system. This could include design decisions, the model's training data and structure, intended use-cases, and how and when deployment, post-deployment or end-user decisions were made and by whom. The RMF requires AI transparency to consider human-AI interaction, such as by notifying the human if a potential or actual adverse outcome is detected.\n- â†’ Explainable and interpretable AI: The RMF defines explainability as a representation of the underlying mechanisms of the AI system's operation, while it defines interpretability as the meanings assigned to the AI outputs in the context of their designed functional purpose. Lack of explainability can be managed by describing how the system functions by tailoring such descriptions to the knowledge, roles and skills of the individual, whereas lack of interpretability can be managed by describing why the AI system gave a specific output.\n\nIt's important to align on a set of ethical AI principles that are operationalized through tangible responsible AI practices, rooted in regulations, e.g. EU AI Act, and best practice frameworks, e.g. NIST AI RMF, when developing AI features. At Workday, we take a risk-based approach to responsible AI governance.\n\nOur scalable risk evaluation dictates relevant guidelines such as requirements to map, measure, and manage unintended consequences including bias. Within the Workday AI Feature Fact Sheets, Workday provides transparency to customers on each feature such as, whereÂ relevant, how they were assessed for bias.\n\nThese safeguards are intended to document our efforts to develop AI features that are safe and secure, human centered, and transparent and explainable.\n\nBarbara Cosgrove Workday Vice President, Chief Privacy Officer\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## U.S. Executive Order 14110\n\nU.S. Executive Order 14110 approaches transparency through an AI safety perspective. Under Section 4, the safety and security of AI technology is to be ensured through certain transparency measures, such as the requirements to share results of safety tests and other important information with the U.S. government, that have been imposed on developers of the most powerful AI systems. Watermarks to label AI-generated content are also required under the order, with the purpose of protecting Americans from AI-enabled fraud and deception.\n\n## China's Interim Measures for the Management of Generative AI Services\n\nArticle 10 of China's Interim Measures for the Management of Generative AI Services requires providers of AI services to clarify and disclose the uses of the services to user groups and to guide their scientific understanding and lawful use of generative AI. Watermarking AI-generated content is also a requirement under Article 11.\n\n## Singapore's AI Verify\n\nSingapore's AI Verify is a voluntary testing framework on AI governance for organizational use comprised of two parts: a testing framework grounded in 11 internationally accepted principles grouped into five pillars and a toolkit to execute technical tests.\n\nTransparency and explainability themes are among the 11 principles embedded in AI Verify. The framework addresses the transparency problem by providing impacted individuals with appropriate information about AI use in a technological system so they can make informed decisions on whether to use that AI enabled system. Explainability, on the other hand, is achieved through an understanding of how an AI model reaches a decision, so individuals are aware of the factors that contributed to a resulting output. Transparency is assessed through documentary evidence and explainability is assessed through technical tests.\n\n|\n\n<!-- image -->\n\n## Implementing AI governance\n\nOrganizations have been active in coming up with tools and techniques to address the black-box transparency and explainability challenge.\n\n## Model and system cards\n\nModel cards are short documents that accompany an AI model to provide transparent model reporting by disclosing information about the model. Information may include explanations about intended use, performance metrics and benchmarked evaluation in various conditions such as across different cultures, demographics or race. In addition to providing transparency, model cards are also meant to discourage use of models outside their intended uses. At the industry level, use of model cards is becoming more prominent as evidenced by publicly accessible model cards for Meta and Microsoft's Llama 2, OpenAI's GPT-3 and Google's face-detection model.\n\nIt may not always be easy to explain a model in a short document. Model cards are to serve a broad audience and, therefore, standardizing explanations may prove either too simplistic for one audience or too complicated for another. Moreover, organizations should also be mindful of how much information they reveal in the cards to prevent adversarial attacks and mitigate security risks.\n\nAI models are often part of a larger system comprised of a group of models and technologies that work together to give outputs. As a result, model cards can fall short of providing a more nuanced picture of how different models interact together within the system. That is where system cards can help achieve better insights.\n\n<!-- image -->\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nSystem cards explain how a group of AI models and other AI and non-AI technologies work together as part of an AI system to achieve specific tasks. Meta released 22 system cards explaining how AI powers its Facebook and Instagram platforms. Each card has four sections that detail:\n\n- â†’ An overview of the AI system.\n- â†’ How the system works by summarizing the steps involved in creating experiences on Facebook and Instagram.\n- â†’ How the shown content can be customized.\n- â†’ How AI delivers content as part of the whole system.\n\nAI systems learn from their environments and constantly evolve, so the way they work also changes over time, requiring updates to the system cards. Like with model cards, reducing technical concepts to a standardized language that serves all audiences can be challenging for system cards, and system cards can also attract security threats based on the amount and type of information shared.\n\nThe utility of model and system cards can go beyond meeting transparency challenges. Maintaining standardized records about the model itself can facilitate communication and collaboration between various stakeholders throughout the life cycle. This can also help with bias and security-risk mitigation. They are also useful for making comparisons with future versions of the models to track improvements. The cards provide a documented record of design, development and deployment, so they can facilitate attribution of responsibility for various decisions and outcomes related to the model or system. Auditors can use them not only to gain a holistic understanding of the system itself, but also to zoom in on the processes and decisions made during different phases of the life cycle.\n\n|\n\nAt IBM, we believe that open technology and collaboration are essential to further the responsible adoption of AI. An open approach can support efforts to develop and implement leading technical methods, such as those used during the testing and evaluation of dataÂ andÂ AI systems.\n\nChristina Montgomery IBM Vice President and Chief Privacy and Trust Officer\n\n<!-- image -->\n\n<!-- image -->\n\n## Open-source AI\n\nAnother approach to addressing the black-box challenge is making AI open source. This requires making the source code public and allowing users to view, modify and distribute it freely. Open access can be especially useful for researchers and developers, as there is more potential for scrutiny by a wider, diverse and collaborative community of experts. In turn, that can lead to improvements to the transparency of algorithms, the detection of risks and the offering of solutions if things go wrong. Open-source AI can also improve technology access and drive collaborative innovation, which may otherwise be limited by proprietary algorithms.\n\n## Watermarking\n\nWith the rise of generative AI, it is becoming increasingly difficult to distinguish AI-generated content from human-created content. To ensure transparency, the watermarking or labeling of AI generated content has been legally mandated under the EU AI Act, U.S. Executive Order 14110 and state-level requirements, and China's Interim Measures for Management of Generative AI.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\n## INDUSTRY EXAMPLE\n\nGoogle uses a technology called SynthID, which directly embeds watermarks into Google's text-to-image generator Imagen. Meta has moved toward labeling AI-generated images on Facebook, Instagram and Threads. Although Meta already adds the label \"Imagined with AI\" on images generated through its AI feature, it now also aims to work with industry partners on common standards to add multilingual labels on synthetic content generated with tools of other companies that users post on Meta's platforms. Specifically, Meta is relying on Partnership on AI's best practices, the Coalition for Content Provenance and Authenticity's Technical Specifications and the International Press Telecommunications Council's  Technical Standards to add invisible markers at scale to label AI generated content by tools of companies such as Google, Microsoft and OpenAI.\n\n<!-- image -->\n\nWatermarking is gaining traction as a way for organizations to promote transparency and ensure safety against harmful content, such as misinformation and disinformation. Companies are embedding watermarks on AI-generated content. Watermarks are invisible to the human eye, but are machine readable and can be detected by computers as AI generated.\n\nWhile watermarking is becoming a popular technique for transparency, it is still not possible to label all AI generated content. Moreover, techniques to break watermarks also exist.\n\n<!-- image -->\n\nFocusing on the building blocks of AI governance - like appropriate documentation of AI models and systems - is important because those foundations are necessary to enable risk management, impact assessment, and third-party auditing.\n\n## Miranda Bogen\n\nCenter for Democracy and Technology AI Governance Lab Director\n\n|\n\n## Part V. The bias, discrimination and fairness challenge\n\n## Hidden and harmful biases may lurk within an AI system.\n\nBias, discrimination and fairness are among the most important challenges of AI governance, given their potentially very significant real-world impacts on individuals and communities. Leaving this challenge unaddressed can lead to discriminatory outcomes and perpetuate inequalities at scale. Healthy AI governance must promote legal and ethical norms including human rights, professional responsibility, human-centered design and control of technology, community development and nondiscrimination.\n\nWhile the automation of human tasks using AI has the advantages of scalability, efficiency and accuracy, it is accompanied by the challenge of algorithmic bias, whereby a systematic error manifests through an inaccuracy in the algorithm. It occurs when an algorithm systematically or repeatedly misses certain groups of people more than others. With transparency challenges around how or why an input turns into a particular output, biases in the algorithm can be difficult to trace and identify.\n\nInstances of algorithmic bias have been well documented in policing, criminal sentencing and hiring. Algorithmic bias can impact even the most well-intentioned AI systems, and it can enter a model or system in numerous ways.\n\n<!-- image -->\n\n## Ways biases may get into the AI system\n\nBiases may get into the AI system in multiple ways during the input, training and output stages.\n\n## At the input stage\n\n- â†’ Historical data. If historical data used to train algorithms is biased, then the algorithm may learn those biases and perpetuate them. For example, if an AI recruitment tool is trained on historical data containing gender or racial biases, those biases will be reflected in the tool's hiring decisions or predictions.\n- â†’ Representation bias. Biases can also enter the algorithm through data that either overrepresents or underrepresents social groups. This can make the algorithmic decisions less accurate and create demographic or social disparities.\n- â†’ Inaccurate data. The accuracy of data can be impaired if it is outdated or insufficient. Such data falls short of fully representing current realities, leading to inaccurate results, which may also lead to reinforcement of historical biases.\n\n## At the training stage\n\n- â†’ Model. Biases can arise when they are an intrinsic part of the model itself. For example, models developed through traditional programming, i.e., those manually coded by human designers, can have intrinsic biases if they are not based on real-world insights.\n\n<!-- image -->\n\nAn algorithm assisting with university admissions may be biased if the human designer programmed it to give a higher preference to students from private schools over students from public schools. Intrinsic biases may be difficult to spot in AI models, as they are a result of self-learning and make correlations across billions of data points, which are often part of a black box.\n\n- â†’ Parameters. The model adjusts its parameters, such as weights and biases in neural networks, during the training process based on the training data. Bias can manifest when the values assigned to these parameters inadvertently reinforce the bias present in the training data or the decisions made by the designers during architecture selection. In an algorithm for university admissions, for example, the attributes of leadership and competitiveness can reflect a gender stereotype present in the training data with the algorithm favoring male candidates over female ones. However, bias in parameters can also manifest more stealthily, such as through proxies. In absence of certain data, the algorithm will make correlations to make sense of the missing data. An algorithm for loan approval, for example, may disproportionately assign more weight to certain zip codes and the model may inadvertently perpetuate racial or ethnic bias by rejecting loan applications using zip codes as a proxy.\n\nAt Microsoft, we are steadfast in our commitment to developing AI technologies that are not only innovative but also trustworthy, safe, and secure. We believe that the true measure of our progress is not just in the capabilities we unlock, but in the assurance that the digital experiences we create will enhance rather than compromise the humanÂ experience.\n\nJulie Brill,\n\nMicrosoft Chief Privacy Officer, Corporate Vice President\n\n<!-- image -->\n\n## At the output stage\n\n- â†’ Self-reinforcing biases. A feedback loop is a process through which the AI system continues to learn based on the outputs it generates. The output goes back into the system as an input, which can influence the system's behavior or performance in some positive or negative way. While feedback loops can foster continuous learning and allow the system to adapt to its deployed environment, they can also lead to self-reinforcing biases if the outputs of the algorithm itself are biased. For example, if an algorithm consistently rejects loan applications for women and consistently approves them for men, there may be a gender bias at play, and the algorithm could fall into a loop where it learns from the biased outputs and continues to reinforce the biased pattern.\n- â†’ Human oversight. Although it is necessary to have humans in the loop throughout the life cycle of the AI system, there is a risk that human biases can reenter the algorithm. For example, human control over a system's final output is necessary, but bias can externally impact the output based on the human interpretation applied to that final output.\n- â†’ Automation bias. Automation bias refers to the human tendency to overly rely on automated outputs. This leads to people trusting the recommendations of algorithms without questioning or verifying their accuracy or being mindful of the system's limitations and errors. This can be especially dangerous when confirmation bias about protected characteristics is at play. That is, users are more likely to accept the outputs when they align with their preexisting beliefs.\n\nBias detection and mitigation is particularly challenging in the context of foundation models due to their size and complex architectures.\n\n<!-- image -->\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n## â†’ SPOTLIGHT Joint statement by US Federal Agencies\n\nIn April 2023, a joint statement made by four federal agencies, namely the EEOC, Department of Justice, Federal Trade Commission and Consumer Financial Protection Bureau, reiterated the U.S.'s commitment to the principles of fairness, equality and justice, which are deeply embedded in federal laws. In April 2024, five additional cabinet-level agencies joined that pledge.\n\nThe joint statement now includes the Department of Education, Department of Health and Human Services, Department of Homeland Security, Department of Housing and Urban Development, and Department of Labor. The Consumer Protection Branch of the Justice Department's Civil Division also joined the pledge. Enforcement of discrimination through automated means is managed by these federal agencies.\n\n<!-- image -->\n\n## Law and policy considerations\n\nMany existing equalities and antidiscrimination laws apply to AI systems and many emerging initiatives specific to AI governance include provisions on bias.\n\nDepending on the jurisdiction where the organization operates, liability could also fall under relevant civil rights, human rights or constitutional freedoms of that jurisdiction.\n\nIn the U.S., civil rights can be protected through private rights of action by individuals. For example, according to the guidance provided by the U.S. Equal Employment Opportunity Commission, private rights of action against discrimination through algorithms could occur under the Americans with Disability Act and Title VII of the Civil Rights Act. Under both the ADA and Title VII, employers can be exposed to liability even where their algorithmic decisionmaking tools are designed or administered by another entity. When individuals think their rights under either of those laws have been violated, they can file a charge of discrimination with EEOC.\n\n## OECD AI Principles\n\nThe principle of \"human-centered values and fairness\" in the OECD AI Principles requires respect for the rule of law, human rights and democratic values across the life cycle of the AI system, through respect for nondiscrimination and equality, diversity, fairness, and social justice. This is to be implemented through safeguards, like context-appropriate human determination that is consistent with the state of the art. The OECD AI Policy Observatory maintains a catalogue on tools and metrics for practically aligning AI with OECD's principles, including bias and fairness.\n\n|\n\n<!-- image -->\n\n## UNESCO Recommendations on the Ethics of AI\n\nPrinciples 28, 29 and 30 of UNESCO's Recommendations on the Ethics of AI encourage AI actors to promote access to technology to diverse groups, minimize the reinforcement or perpetuation of discriminatory or biased outcomes throughout the life cycle of the AI systems, and reduce the global digital divide. Among the tools provided by UNESCO for the practical implementation of its recommendations is the ethical impact assessment, which is designed primarily for government officials involved in the procurement of AI systems but can also be used by companies to assess if an AI system aligns with UNESCO's Recommendations.\n\n## EU AI Act\n\nThe EU AI Act provides a relevant framework for data governance of high-risk AI systems under Article 10, which permits training, validation and testing of datasets to examine the possibility of biases that affect the health and safety of persons and negatively impact fundamental rights or lead to discrimination. To deal with the challenge of self-reinforcing biases, Article 15(4) also requires the elimination or reduction of biases emanating from feedback loops in high-risk AI systems after they have been put on the market or into service. The EU AI Act calls for consideration of the European Commission's Ethics Guidelines for Trustworthy AI, which are voluntary guidelines seeking to promote \"diversity, non-discrimination and fairness.\"\n\n## Singapore\n\nSingapore's AI Verify tackles bias via the principle of \"ensuring fairness.\" This principle is made up of the pillars of data governance and fairness. While there are no specific tests for data governance in the toolkit, if the model is not giving biased outputs based on protected characteristics, fairness can be ensured by checking the model against ground truth. Process checks include the verification of documentary evidence that there is a strategy for fairness metrics and that the definition of sensitive attributes is consistent with the law.\n\n<!-- image -->\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n## â†’ SPOTLIGHT\n\n## US FTC Enforcement priorities and concerns\n\nWe have made no secret of our enforcement priorities and concerns.\n\n1. There is no AI exemption from the laws on the books and businesses need to develop and deploy AI tools in ways that allow for an open and competitive market and protect consumers fromÂ potential harms.\n2. We are scrutinizing existing and emerging bottlenecks across the AI design stack to ensure that businesses aren't using monopoly power to block innovation and competition.\n\n<!-- image -->\n\n3. We are acutely aware that behavioral advertising, brought on by web 2.0, fuels the endless collection of user data and recognize that model training is emerging as another feature that could further incentivize surveillance.\n\n4. We are squarely focused on aligning liability with capability and control, looking upstream and across layers of the AI stack to pinpoint which actor is driving or enabling the lawbreaking.\n\n5. We are focused on crafting effective remedies in cases that establish bright-line rules on the development, use and management of AI inputs, such as prohibiting the uses of inaccurate orÂ highly-sensitive data when training models.\n\nSamuel Levine\n\nFTC Bureau of Consumer Protection Director\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## The US\n\nIn the U.S., antidiscrimination laws that also extend to AI are scattered across various sectors, such as employment, housing and civil rights.\n\n- â†’ Employment. Under the Americans with Disabilities Act, employers are prohibited from using algorithmic decision-making tools that could violate the act, such as in not providing reasonable accommodations, intentionally or unintentionally screening out an individual with a disability, or adopting disability-related inquiries and medical examinations.\n- â†’ Housing. In 2023, to ensure fairness in housing, the Biden-Harris Administration issued a proposed rule against racial bias in algorithmic home valuations, empowering consumers to take action against appraisal bias, increasing transparency and leveraging federal data to inform policy and improve enforcement against appraisal bias.\n- â†’ Consumer finance. The CFPB confirmed companies are not absolved of their legal\n\nresponsibilities under existing legislation, such as the Equal Credit Opportunity Act, when they use AI models to make lending decisions. Remedies include compensating the victim, providing injunctive relief to stop unlawful conduct, or banning persons or companies from future participation in the marketplace.\n\n- â†’ Voluntary frameworks. The NIST Special Publication 1270: Towards a Standard for Identifying and Managing Bias in AI lays down governance standards for managing AI bias. These include monitoring the system for biases, making feedback channels available so users can flag incorrect or harmful results for which they can seek recourse, putting policies and procedures in place for every stage of the life cycle, maintaining model documentation to ensure accountability, and embedding AI governance within the culture of the organization.\n\n<!-- image -->\n\n## â†’ SPOTLIGHT FTC enforcement action against Rite Aid\n\nOn 19 Dec. 2023, the FTC issued an  enforcement action against Rite Aid's discriminatory use of facial recognition technology. Rite Aid deployed facial recognition surveillance systems for theft deterrence without assessing the accuracy or bias of the system.\n\nThe FTC placed a five-year moratorium on Rite Aid's use of facial recognition, and if after five years Rite Aid chooses to use this technology again, it will have to implement the FTC's governance plan detailed in the order. The enforcement decision also included an order for disgorgement, that is, to delete or destroy any photos and videos including any data, models or algorithms used for surveillance.\n\n<!-- image -->\n\nRite Aid recorded thousands of false-match alerts, and the FTC's gender-based analysis revealed Black, Asian, Latino and women consumers were more likely to be harmed by Rite Aid's surveillance technology.\n\nThis case serves as good indication of the nature and intensity of liability that deployers and providers of AI in the U.S. may be exposed to for deploying discriminatory AI systems.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n## Implementing AI governance\n\nOne overarching practice used to mitigate biases is the promotion of diversity and inclusivity among teams working across the life cycle of the AI system. Personnel composition is often supported by organization-level principles for safe and responsible AI, many of which internal AI ethics policies, e.g., Google, IBM and Microsoft.\n\n## Bias testing\n\nOne way to minimize bias in AI systems is by testing the systems. While there are numerous ways to test for bias in AI systems, it is important to understand what is being evaluated. Demographic parity may be different than equality objectives. Establish goals based on the desired system outcomes to start, and then establish an appropriate technique for testing bias within the system. For example, does fairness mean an equal number of males and females will be screened for a new position on your team, or that candidates with the most distinguished resumes are identified as ideal applicants independent of their gender, race, experience, etc.\n\n<!-- image -->\n\nIt is important to note that often testing for bias will require the use of personal information to determine if fairness objectives are being met. As such, there may be a privacy-bias trade-off, as safeguarding privacy through data minimization creates challenges for mitigating biases in AI systems. Some  considerations when balancing privacy while mitigating bias include:\n\n- â†’ Intentionally collecting sensitive data directly in the design phase so it is ready for the testing phase. This can be done by procuring consent from data subjects and disclosing the purpose for the collection and processing of their data.\n- â†’ Creating intentional proxies to test how the system makes correlations without sensitive data, such as for demographic features.\n- â†’ Buying missing data from data brokers, public data or other datasets in compliance with privacy and data governance policies.\n\nFairness tests and debiasing methods are not created equally - as an AI deployer or governance professional, it is critically important to use tools and methods that fundamentally align with equality and nondiscrimination law in your jurisdiction.\n\n## Brent Mittelstadt\n\nUniversity of Oxford Internet Institute Director of Research, Associate Professor and Senior Research Fellow\n\n|\n\n## Part VI. TheÂ security and robustness challenge\n\n## The pace, scale, and reach of AI development and integration demands strong security.\n\nCompromises to the security of AI could result in manipulation of outputs, stolen sensitive information or interference with system operations. Unsecured AI can result in financial losses, reputational damage and even physical harm. For example, exploiting the vulnerabilities of medical AI could lead to a misdiagnosis, and adversarial attacks on autonomous vehicles could lead to road traffic accidents.\n\nAlthough AI security overlaps with and suffers from traditional cybersecurity risks, cybersecurity is often about protecting computer systems and networks from attacks, whereas AI security is about guarding the AI system's components, namely the data, model and outputs. When it comes to AI security, malicious actors can enable adversarial attacks by exploiting the inherent limitations of AI algorithms.\n\n|\n\n<!-- image -->\n\n## Adversarial attacks\n\nAdversarial attacks are a deliberate attempt to manipulate models in a way that leads to incorrect or harmful outputs. The intention behind the attack could be to lead the model toward misclassification or cause harm, and all it may take to trick the model is a slight switching of pixels or adding a bit of noise. Some types of adversarial attacks include:\n\n- â†’ Evasion attacks. The aim of evasion attacks is to deceive the model into misclassifying data, such as by adding a small perturbation to the input image, as in the MIT example, leading to an incorrect output with high confidence.\n- â†’ Data poisoning. This can happen in various ways, such as by switching the labels of labeled data or injecting entirely new data into the dataset. However, for this to work, the adversary will have to first gain access to training data. Data poisoning can also help attackers create backdoors so they can manipulate model behavior in the future.\n- â†’ Model extraction. The aim of model extraction is model theft by reverse engineering to reveal the hidden mechanism of the model or sensitive information, or to make the model vulnerable to further attacks. This is done by feeding carefully\n\n<!-- image -->\n\ncrafted queries to a black-box model to analyze its outputs and steal its functionality. This can help the adversary copy the model and make financial gains.\n\nAI vulnerabilities can also be exploited through open-source software and third-party risks.\n\n- â†’ Open-source software can be manipulated in many ways, such as through supply-chain attacks, in which open-source AI libraries are targeted by malicious code that is planted as a legitimate update or functionality. Although open-source software suggests everything has been made publicly available, the original developers can restrict access to some parts of the software in the license agreement. In such cases, hackers may resort to model extraction. Even if an AI system is not open source, the project may rely on a complex ecosystem of open-source tools, exposing itself to a potential attack surface that malicious actors can exploit.\n- â†’ A lack of control and visibility over thirdparty governance practices makes risk mitigation more difficult, including with respect to security. Third-party vendors may have weaker security standards and practices, making them more vulnerable to data breaches, supply chain attacks and system hacks, among other security risks.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Law and policy considerations\n\nRegulatory and voluntary governance tools that have established requirements for tackling AI security issues include the NIS2 Directive, U.S. Executive Order 14110, the NIST AI RMF and the NIST Cybersecurity Framework.\n\n## NIS2\n\nThe NIS2 Directive replaces the EU Network and Information Security Directive from 2016. It aims to boost resilience and incident-response capacities in public and private sectors through risk management and reporting obligations. Some cybersecurity requirements under Article 21 include policies on risk analysis and system security, incident handling, supply-chain security, policies and procedures to assess cybersecurity risk-management effectiveness, cyber hygiene practices, policies on the use of cryptography, and encryption.\n\n## EU AI Act\n\nAs with most AI themes, under the EU AI Act, security takes a risk-based approach. As such, security and robustness requirements vary based on if the system is high risk or if it is a GPAI system with systemic risks.\n\n- â†’ High-risk AI systems. The EU AI Act lays down detailed security obligations for accuracy, security and robustness of high-risk AI systems. Technical and organizational measures are to be placed to ensure high-risk systems are resilient toward errors, faults and inconsistencies. Possible solutions include back-up or fail-safe plans.\n\nThe act also foresees risks emerging at the third-party level, requiring resilience against unauthorized third-party attempts to alter use, outputs or performance by exploiting the system vulnerabilities. Technical solutions to handle such security risks must be appropriate to circumstances and risk. These can include measures to prevent, detect, respond to, resolve and control data poisoning, model poisoning, adversarial examples and model evasion, confidentiality attacks, or model flaws.\n\nAdditionally, the EU AI Act obliges providers of high-risk AI systems to ensure they undergo conformity assessments that demonstrate compliance with requirements for high-risk systems.\n\n- â†’ Obligations for providers of GPAI systems with systemic risks. The EU AI Act lists the security requirements for high-impact AI systems. Requirements include:\n- Evaluating models in accordance with standardized protocols, such as conducting and documenting adversarial testing to identify and mitigate systemic risks.\n- Monitoring, documenting and reporting serious incidents to the AI Office.\n- Ensuring GPAI models with systemic risks and their physical infrastructures have adequate levels of cybersecurity.\n\n|\n\nThere is an urgent need to respond to the complex challenges of AI governance by professionalizing the field. A professionalized workforce can take AI governance from theory to practice, spread trustworthy and standardized practices across industries and borders, and remain adaptable to swiftly changing technologies and risks.\n\nJ. Trevor Hughes IAPP President and CEO\n\n<!-- image -->\n\n<!-- image -->\n\n## US Executive Order 14110\n\nThe U.S. AI Executive Order 14110 calls on developers of the most powerful AI systems to share their safety results and other critical information with the U.S. government. It also calls on the NIST to develop rigorous standards for extensive red-team testing to ensure safety before public release.\n\n## NIST AI RMF\n\nThe NIST AI RMF identifies common security concerns such as data poisoning and exfiltration of models, training data or other intellectual property through AI system endpoints. Under the AI RMF, a system is said to be secure when it can maintain confidentiality, integrity and availability through protection mechanisms that prevent unauthorized access and use. Practical implementation can be achieved through the NIST Cybersecurity Framework and RMF.\n\n## NIST Cybersecurity Framework\n\nThe NIST Cybersecurity Framework is a voluntary framework that provides standards, guidelines and best practices for organizations to mitigate cybersecurity risks. The framework is organized under five key functions: identify, protect, detect, respond and recover.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Implementing AI governance\n\nDue diligence in the identification of security risks throughout the life cycle of the system is an important activity, especially when a thirdparty vendor is involved. Due diligence can only ever inform. With appropriate information, an organization can seek contract terms with thirdparty vendors that mandate:\n\n- â†’ Making the vendor's security practices compatible with the organization's own standards.\n- â†’  Monitoring system robustness regularly through security assessments or audits to identify third-party risks and ensure the vendor is complying with the organization's security standards.\n- â†’ Limiting access to third-party vendors only for the services they need to perform.\n\n## Red teaming\n\nRed teaming is the process of testing the security of an AI system through an adversarial lens by removing defender bias. It involves the simulation of adversarial attacks on the model to evaluate it against certain benchmarks, \"jailbreak\" it and make it behave in unintended ways. Red teaming reveals security risks, model flaws, biases, misinformation and other harms, and the results of such testing are passed along to the model developers for remediation. Developers use red teaming to bolster and secure their product before releasing it to the public.\n\n## Secure data sharing practices\n\nDifferential privacy is primarily a privacyenhancing technique that also has security benefits, it analyzes group data while preserving individual privacy by adding controlled noise to the data and blurring individual details. So, even if an attacker were to steal this data, they would not be able to link it back to specific individuals, minimizing harm. As such, differential privacy can limit the utility of stolen data. However, that impact to the utility of the data can also impact organizations with lawful and legitimate interests in processing the data. Moreover, differential privacy can also be a costly technique to implement, especially where large datasets are concerned.\n\n## HITL\n\nHuman in the loop refers to incorporating human expertise and oversight into the algorithmic decision-making process. Although HITL may provide a gateway for human biases to reenter the algorithm when making judgements about final outputs, in the context of AI security, HITL can make incident detection and response more efficient. This is especially true where subtle manipulations or attacks that the model may not have been trained to identify are involved. HITL allows for continuous monitoring and verification, however, optimal use of this approach rests on balancing the contradictions that may arise to address bias or safety and security.\n\n|\n\n## Part VII. AIÂ safety\n\n## AI safety is a cornerstone but somewhat mercurial principle for realizing safe and responsible AI.\n\nVarious themes, particularly value alignment, transparency and AI security, eventually culminate into the broader theme of AI safety. Given that safety is an all-encompassing theme, it has no settled global definition. It may include preventing so-called existential risks posed by artificial general intelligence. For some, such as the Center for AI Safety, AI risk is categorized based on malicious use, the AI race, rogue behavior and organizational risks. For others, such as the country signatories to the Bletchley Declaration, and most recently, for parties to the Seoul Declaration for Safe, Innovative and Inclusive AI, it is about managing risks and being prepared for unexpected risks that may arise from frontier AI. AI safety can also be the term used to describe minimizing AI harms from misinformation, disinformation and deepfakes, and the unintended behavior of an AI system, especially advanced AI systems.\n\n<!-- image -->\n\n## Law and policy considerations\n\nThe importance of AI safety is reflected in the fact that, for some jurisdictions, it has been embedded as a main theme in national strategies toward AI. The Biden-Harris Administration's Executive Order 14110 focuses on developing \"Safe, Secure and Trustworthy\" AI. In 2023, the U.K. brought world leaders together for first AI Safety Summit, and the country's approach toward AI is focused on the safety of advanced AI systems, or \"frontier AI.\" Safety is also an important factor under the EU AI Act, which is reflected in the security and robustness requirements for high-impact GPAI systems and high-risk AI systems.\n\n## AI safety institutes\n\nRecently, the NIST announced it would establish the U.S. AI Safety Institute. To support this institute, the NIST also created an AI Safety Institute Consortium, which brought more than 200 organizations together to develop guidelines and standards for AI measurement and policy that can lay the foundation for AI safety globally.\n\n<!-- image -->\n\nAmong many security- and safety-related initiatives, the AISIC is tasked with enabling collaborative and interdisciplinary research and establishing a knowledge and data sharing space for AI stakeholders. More specifically, the AISIC will develop new guidelines, tools, methods, protocols and best practices to facilitate the evolution of industry standards for AI safety. The AISIC will also develop benchmarks for evaluating AI capabilities, especially harmful ones.\n\nThe U.K. government established an AI Safety Institute to build a sociotechnical infrastructure that can minimize risks emerging from unexpected advancements in AI technology. The institute has been entrusted with three main functions: developing and conducting evaluations on advanced AI systems, driving foundational AI research, and facilitating the exchange of information.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Bletchley Declaration\n\nThe 2023 U.K. AI Safety Summit brought together international governments, leading AI companies and civil society groups to discuss frontier AI risks and ways to promote AI safety. As a demonstration of their commitments to AI safety, participating nations also signed the Bletchley Declaration, which makes various affirmations to cooperate globally on innovation, sustainable development, economic growth, protection of human rights and fundamental freedoms, and building public trust and confidence in AI technology.\n\n## EU AI Act\n\nThe security requirements for general-purpose AI systems under the AI Act are also focused on regulating \"systemic risks.\" The EU AI act defines this risk as one emerging from highimpact general purpose models that \"significantly impact the internal market, and with actual or reasonably foreseeable negative effects on public health, safety, public security, fundamental rights, or the society as a whole, that can be propagated at scale across the value chain.\"\n\n## AI Safety Standards\n\nISO/IEC Guide 51:2014 provides requirements and recommendations for drafters of standards to include safety aspects in those standards. It applies to safety aspects pertaining to people, environments or both.\n\nWe are generating 2.5 quintillion bytes of data globally per day. Much of this is flowing into our internet. Therefore, generative AI models are dynamic and the applications that are built on top of them will move. It is up to the organizations to ensure that the movement meets their standards.\n\n## Dominique Shelton Leipzig\n\nMayer Brown Partner, Cybersecurity &amp; Data Privacy and Leader, Global Data Innovation &amp; AdTech\n\n|\n\n<!-- image -->\n\n## â†’ SPOTLIGHT Compute governance\n\nOn a broader level, AI safety also refers to regulating compute, i.e., the power source of AI systems, as regulating AI at its source increases the visibility of its technical capabilities. Unlike AI models, which can be replicated exponentially and without control, compute must be purchased and is quantifiable. As computing chips are manufactured through highly concentrated supply chains and dominated by only a few companies, regulatory interventions can be more focused. Such regulation can purposefully occur with AI safety in mind to control the allocation of resources for AI projects by subsidizing or limiting access to compute or by building guardrailsÂ into hardware.\n\nWith compute governance gaining traction because of advanced AI systems, compute thresholds, i.e., numerical measures of computing power, are also being set legally, which helps distinguish AI systems with high capabilities from other AI systems.\n\n<!-- image -->\n\n<!-- image -->\n\nFor instance, U.S. Executive Order 14110 requires  models using computing power greater than 10 26  integer and models using biological sequence data and computing power greater than 10 23  integer to provide the government with information and reports on the models testing and security on an ongoing basis.\n\nSimilarly, under the EU AI Act, GPAI is presumed to have high-impact capabilities when cumulative compute used for training is greater than 10 25  floating-point operations. When a model meets this threshold, the provider must notify the Commission, as meeting the threshold leads to the presumption that this is a GPAI system with systemic risk. This means the model can have a significant impact on the internal market, and actual or reasonably foreseeable negative effects on health, safety, fundamental rights or society. Providers need to comply with requirements on  model evaluation, adversarial testing, assessing and mitigating systemic risks, and reporting any serious incidents.\n\n|\n\n<!-- image -->\n\n## Implementing AI governance\n\nThe organizational practices for security and robustness discussed in this report, such as red teaming for adversarial testing, HITL and privacy-preserving technologies, can apply to AI safety. Similarly, organizational practices and laws requiring transparency and explainability, specifically watermarks, also apply to AI safety.\n\n## Prompt engineering\n\nOne of OpenAI's safety practices includes prompt engineering to help generative AI understand prompts in a given context. This practice is aimed at minimizing harmful and undesired outputs from generative AI, and it helps developers exercise more control over user interactions with AI to reduce misuse at the user level. Moreover, as part of product safety standards, OpenAI also has put in place usage policies.\n\n## Reports and complaints\n\nAnother safety practice of OpenAI is allowing users to report issues that can be monitored and responded to by human operators. This is not yet a popular practice. A 2023 study carried\n\n<!-- image -->\n\nout by TrustibleAI found out of 100 random organizations, three provided an individual appeals process between the individual and the company. It is possible internal governance and complaint mechanisms may become more common post-EU AI Act, given that, under Article 27 (f), deployers of AI systems must carry out FRIAs of internal governance and complaint mechanisms where a risk has materialized into a harm.\n\n## Safety by design\n\nTo combat abusive AI-generated content, Microsoft is focused on building strong safety architecture through the safety by design approach, which can be applied at the AI platform, model and application levels. Some efforts include red teaming, preemptive classifiers, blocking abusive prompts, automated testing and rapid bans of users who abuse the system. With regard to balancing freedom of speech against abusive content, Microsoft is also committed to identifying and removing deceptive and abusive content on LinkedIn, Microsoft Gaming Network and other services.\n\nHumans control AI, not the other way around. Generative AI models drift. The only way for companies to know when/ how they are drifting is to continuously test, monitor and audit the AI applications for high risk use cases- every second of every minute of every day. This is the only way to ensure that the model output comports with the organization's pre-installed guardrails for accuracy, health and safety, privacy, bias.\n\nDominique Shelton Leipzig\n\nMayer Brown Partner, Cybersecurity &amp; Data Privacy and Leader,\n\nGlobal Data Innovation &amp; AdTech\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Safety policies\n\nIn preparation for the U.K. AI Safety Summit, Meta released an overview of its AI safety policies, specifically in relation to its generative AI Llama model. In addition to model evaluations and red-team analysis, the policy also detailed Meta's model reporting and sharing, reporting structure for vulnerabilities found after model release, post-deployment monitoring for patterns of misuse, identifiers of AI generated material, data input controls and audits, and priority research on societal, safety and security risks.\n\n## Industry best practices\n\nPartnership on AI has invested extensively in AI safety research and resources. Some of its work includes Guidance for Safe Foundation Model Deployment. This framework is a living document targeted at model providers on ways to operationalize AI safety for responsible deployment. The framework provides custom guidance providers of foundation models can follow throughout the deployment process that is appropriate for their model's capabilities. Another resource is PAI's SafeLife, which is a benchmark focused on avoiding negative side effects in complex environments. SafeLife is a reinforcement learning environment that tests the \"safety of reinforcement learning agents and the algorithms that train them.\" It allows agents to navigate a complex environment to accomplish a primary task. The aim is to create a \"space for comparisons and improving techniques for training non-destructive agents.\"\n\n|\n\n## Part VIII. TheÂ copyright challenge\n\n## Generative AI is raising new challenges for copyright law.\n\nCopyright refers to the rights that creators have over the expression of their artistic or intellectual works. Although it is not possible to provide an exhaustive list of \"works\" covered by copyright legislation, globally copyright protection has been extended to include a wide range of works, such as literature, music, architecture and film. In the context of modern technology, computer software programs, e-books, online journal publications and the content of websites such as news reports and databases are also copyrightable.\n\nThe clear establishment of intellectual property rights around both inputs and outputs for generative AI models is of crucial importance to creative artists and the creative industries. In the face of dramatically growing machine capabilities, we need to make sure that incentives for human creation remain strong.\"\n\nLord Tim Clement-Jones\n\nU.K. House of Lords Liberal Democrat Peer and Spokesperson for Science, Innovation and Technology\n\n<!-- image -->\n\n## Law and policy considerations\n\nIn most countries, and especially those party to the Berne Convention, copyright protection is obtained automatically upon creation of the work. In other words, copyright registration is not necessary for proprietarily safeguarding artistic and intellectual works. Regardless, while offering automatic copyright protection, many countries, including the U.S., also allow voluntary copyright registration.\n\nCopyright provides owners two types of rights: economic rights, through which the owner can make financial gains by authorizing use of their work by others through a license, and moral rights, which include noneconomic interests such as the right to claim authorship of a work or to oppose changes to a work that could harm the owner's reputation.\n\nCopyright protects artistic and intellectual works by preventing others from copying, adapting, distributing, performing or publicly displaying the work, or creating derivative works. When an individual does any of these without the authorization of the rights' owner, this may constitute copyright infringement.\n\nThe use of copyright protected content requires the authorization of the original author, unless a statutory copyright exception applies. A legitimate exception to copyright infringement in some jurisdictions is fair use or fair dealing. This is a limitation on the exclusive rights of a copyright holder, which sometimes allows the use of the work without the right holder's permission.\n\nIn the U.S., fair use is statutorily defined under 17 U.S. Code Â§ 107, and four factors assist courts in making a fair-use determination. These include purpose and character of use, nature of copyrighted work, substantiality of use, and impact of use on the potential market of the copyrighted work. Similarly, Singapore's Copyright Act of 2021 also includes a fair-use exemption and takes into account the same four factors as the U.S. courts. Singapore's old copyright law also had a fifth factor, which considered the possibility of obtaining a work within a reasonable time at an ordinary commercial price. However, under the new law, the fifth factor may be considered by courts only when relevant.\n\nThe U.K. also has a permitted exemption to copyright infringement termed fair dealing. There is no statutory definition for fair dealing as, depending on the case, it will always be a matter of fact, degree and impression. Other factors the U.K. courts previously considered to determine fair dealing include the effect on the market for the original work and whether the amount of work copied was reasonable and appropriate.\n\nCommon remedies that can be granted by a court ruling on copyright infringement include injunctions, damages for the loss suffered, statutory damages, infringer's profits, surrender or destruction of infringing articles, and attorney fees and costs.\n\nThough copyright has emerged as one of the first and foremost frontiers between AI and intellectual property, the full gamut of IP rights are engaged by AI, and specifically generative AI: design rights, performers' rights, patents and trademarks. Anthropocentric approaches to IP will butt up against AI's learning techniques, its scale and the nature of its outputs, leaving much uncertainty, complexity and variety in the implementation of AI and IP governance.\n\nJoe Jones IAPP Director of Research and Insights\n\n<!-- image -->\n\n## â†’ SPOTLIGHT\n\n## Generative AI copyright litigation in the U.S.\n\nTwo main lines of argument are emerging in ongoing AI copyright litigation in the U.S.\n\nPetitioners are arguingÂ that:\n\n- â†’ Defendants made copies of copyrighted works when ingesting them for training foundation models.\n- â†’ As the generated outputs were trained on copyrighted material, theÂ outputs themselves are also infringing derivative works.\n\nMore specifically, in a lawsuit against OpenAI, the New York Times argued that OpenAI and Microsoft's generative AI tools were built by copying years of journalistic work without permission or payment, and both companies are making high profits through their generative AI tools, which now compete with the news outlet as reliable sources of information.\n\nOpenAI's motion to dismiss the lawsuit provides background on fair use law, and it argues courts have historically used fair use to protect useful innovations and copyright is not a veto right over transformative technologies that leverage existing work internally.\n\n<!-- image -->\n\nThe assessment of fair use is likely to include an evaluation of exactly what was or is being copied, whether ingestion of copyrighted material amounts to transformative use, the substantiality of the copying and the economic harm caused by using copyrighted material in developing generative AI models on the potential market for the copyrighted work.\n\nSimilarly, in Tremblay v. OpenAI, various authors alleged copyright infringement based on the ingestion of training data that copied the works of the authors without consent, credit or compensation. A California court recently rejected claims on vicarious copyright infringements, Digital Millennium Copyright Act violations, negligence andÂ unjust enrichment.\n\n<!-- image -->\n\n|\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Implementing AI governance\n\nNumerous copyright-safety solutions and harm-mitigation strategies are emerging, notwithstanding the uncertainty present due to pending litigation.\n\n- â†’ Opt outs. As foundation models are trained on vast amounts of data online, organizations may not be aware that their copyrighted material is used for training. In those scenarios, when organizations are concerned about their webpages being scraped, an opt-out process, like that of OpenAI, may be a workable strategy to mitigate the risk of unwanted scraping.\n- â†’ Liability considerations. Given the fear of potentially becoming a copyright infringer as a user of generative AI, commercial users may avoid engaging with providers of generative AI services.\n- â†’ Explore technical guardrails. Organizations can also make use of technical guardrails that help them respect the copyrights of authors. Microsoft incorporated guardrails such as content filters, operational monitoring, classifiers, abuse detection and other technologies to reduce the likelihood of Copilot returning copyright-infringing content.\n- â†’ Generative AI requirements. To increase transparency around data used to train generative AI models, including copyrighted data, certain jurisdictions such as the EU require system providers to publish detailed summaries of the content used for training their models. Further, with respect to copyright compliance, the EU AI Act requires providers to implement a copyright policy mandating protocols to identify and observe applicable copyright laws.\n\n|\n\n## Part IX. Third-party AI assurance\n\n## AI assurance methods are crucial for demonstrating accountability and establishing trust.\n\nIn a recent report released by the U.K. government, assurance is defined as \"the process of measuring, evaluating and communicating something about a system or process, documentation, a product, or an organisation.\" Many of the AI governance implementation mechanisms discussed in this report are forms of assurance.\n\nWhile establishing core competencies within an organization is beneficial to create strong AI-governance foundations across the different lines of defense, utilization of third-party AI assurance mechanisms may be an important or necessary consideration depending on the type of AI used and  the organization's knowledge and capacity.\n\nIntegrating third-party assurance into an AI-governance strategy is a consideration at various stages of the life cycle.\n\n|\n\n<!-- image -->\n\n## Types of third-party assurance\n\nSome of the most practical tools for the realization of safe and responsible AI are emerging from third-party AI assurance methods.\n\n## Assessment\n\nAssessments are key mechanisms to evaluate various aspects of an AI system, including to determine the risk of a system or identify the source of bias or determine the reason a system is making inaccurate predictions. Various services and off-the-shelf products can be integrated into AI governance practices based on what an organization is trying to determine from its assessment.\n\nCertain assessments must be conducted by the third party providing the system to their customers, such as conformity assessments and impact assessments focusing on the impacts of the datasets used and the model itself. From a deployer's perspective, third-party due diligence enquiries should be integrated into the organization's existing third-party risk management program and include screening at both the vendor enterprise and product levels.\n\n## Testing and validation\n\nTesting techniques such as statistical tests to evaluate demographic fairness, assess system performance or detect generative AI that may lead to copyright breaches are becoming widely available through various third-party vendors. Before choosing a vendor, it is important to have a clear understanding of what the test is for and whether the context - which includes the type of AI used, applicable jurisdictions and the domain operating in - will impact the types of tests to run.\n\n## Conformity assessments\n\nConformity assessments are reviews completed by internal or external review functions to evaluate whether a product, system, process or individual adheres to an established set of requirements. This is typically performed in advance of a product or system being placed on the market. While most assessments focus on evaluating aspects of AI systems, conformity assessments have been designed to evaluate quality-management systems, a set of processes for those who build and deploy AI systems, and individuals who are involved in the development, management or auditing of AI systems.\n\nFrom a deployer's perspective, the third-party due diligence process should include vendor inquiries into product documentation, such as technical specifications, user guides, conformity assessments and impact assessments.\n\nRisk assessments should be done at several phases of development, starting with the proposal/idea phase.\n\nIt's easier to incorporate some 'responsible by design' features early on, rather than tack them on at the end. For example, filtering for toxic content in your training data, before a model is trained, can be more effective than trying to catch toxic generated content afterwards.\n\nIn contrast, a full impact assessment should be done once a model is fully developed and evaluated, because it's hard to assess the impact without a lot of information about the final system.\n\nAndrew Gamino-Cheong Trustible AI Co-founder and Chief Technology Officer\n\nOrganizations need a clear understanding of how AI risk will affect their business through third-party relationships. They should proactively review their inventory of vendors and identify those that provide AI solutions or components. They also need to be aware of the development plans for all third-party products, including whether, how, and when AI will be integrated. With that understanding, partners, vendors and their products may need to be reassessed to account for AI risk with updated due diligence processes.\n\nAmber Gosney FTI Technology Managing Director\n\n<!-- image -->\n\n## Impact assessments\n\nThe risk profile of AI systems can vary widely based on the technical capabilities and intended purposes of the system, as well as the particular context of their implementation. Evaluating and mitigating the impacts of an AI system is therefore a shared responsibility that must be owned by providers and deployers alike in practice. The organization deploying a third-party AI system will have a closer understanding of the specific context and impacts of deploying the system. Similarly, the third-party vendor is best placed to evaluate the impacts of the training, testing and validation datasets, the model and infrastructure used to design and develop the system.\n\n## AI/algorithmic auditing\n\nWhile there is not yet a formal audit practice as seen in financial services, there is a growing call for those who audit AI systems to demonstrate a common set of competencies, such as with a certification or formal designation. These audits may incorporate other third-party mechanisms discussed above to evaluate AI systems and ensure they are safe, secure, legally compliant and meet requisite standards, among other things. The\n\nNational Telecommunications and Information Administration released recommendations for federal agencies to use audit and auditors for the use of high-risk AI systems.\n\nCanada's proposed Bill C-27, the Digital Charter Implementation Act, identifies that the Minister of Innovation, Science and Industry can issue an independent audit if they have reasonable grounds to believe requirements outlined in the act have not been met. This may encourage organizations to ensure compliance via preventative thirdparty audits. Additionally, Canada identified the importance of international standards to help support the desired objectives of the act.\n\n## Certifications\n\nCertifications are marks or declarations provided after evaluations or audits are performed against standards or conformity assessments. The mark indicates the AI system adheres to certain specified requirements. It is important to note certifications can also be provided to quality-management systems used throughout the life cycle of an AI system or to individuals, demonstrating that they met a set of competencies.\n\n<!-- image -->\n\n## â†’ SPOTLIGHT\n\nAlgorithmic audits as airplane cockpits\n\nAt ORCAA, we use the analogy of an airplane cockpit to talk about algorithmic audits. In an airplane cockpit, the dials and gauges take measurements that relate to possible failure modes.\n\nFor instance, the fuel gauge says if the plane is about to run out of gas, and the attitude indicator says if it is going to dive or roll. These dials have 'redlines': threshold values that, if exceeded, mean the pilot needs to intervene. The auditor's job is to design a 'cockpit' for a given algorithmic system. This involves identifying failure modes -- how the system could result in harm to various stakeholders -- and building 'dials' that measure conditions that lead to failures. At ORCAA, we have developed frameworks for doing these critical tasks.\n\n<!-- image -->\n\nSome other aspects of this analogy are worth noting. A cockpit identifies problems but does not fix them. An indicator light will say an engine is out, but it won't say how to repair or restart the engine. Likewise, an algorithmic cockpit should indicate when a failure is imminent, but it is the job of the system deployer, the 'pilot,' to intervene. A cockpit is a critical piece of airplane safety, but it's not the whole picture. Planes are tested extensively before being put into service, both during the design phase and when they roll off the assembly line and are periodically taken out of service for regular inspections and maintenance.\n\nLikewise, algorithmic cockpits, which are critical for safety while the system is deployed, should be complemented by predeployment testing and regular inspections and maintenance during deployment.\"\n\nCathy O'Neil\n\nO'Neil Risk Consulting &amp; Algorithmic Auditing CEO\n\n<!-- image -->\n\n|\n\n## Conclusion\n\n## Bringing it all together and putting it into action.\n\nOrganizations may seek to leverage existing organizational risk frameworks to tackle AI risk at enterprise, product and operational levels. Tailoring their approach to AI governance to their specific AI product risks, business needs and broader strategic objectives can help organizations establish the building blocks of trustworthy and responsible AI. A key goal of the AI governance program is to facilitate responsible innovation. Flexibly adapting existing governance processes can help businesses to move forward with exploring the disruptive competitive opportunities that AI technologies present, while minimizing associated financial, operational and reputational risks.\n\n|\n\n## Contacts\n\n## Connect with the team\n\nUzma Chaudhry IAPP AI Governance Center Research Fellow uchaudhry@iapp.org\n\n## Joe Jones\n\nIAPP Director of Research and Insights jjones@iapp.org\n\nAshley Casovan IAPP AI Governance Center Managing Director acasovan@iapp.org\n\n## Nina Bryant\n\nFTI Technology Senior\n\nManaging Director nina.bryant@fticonsulting.com\n\n## Luisa Resmerita\n\nFTI Technology Senior Director luisa.resmerita@fticonsulting.com\n\n## Michael Spadea\n\nFTI Technology Senior Managing Director micheal.spadea@fticonsulting.com\n\nLynsey Burke IAPP Research and Insights Project Specialist lburke@iapp.org\n\n## Follow the IAPP on social media\n\nD  C  Q  E\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nPublished June 2024.\n\nIAPP disclaims all warranties, expressed or implied, with respect to the contents of this document, including any warranties of accuracy, merchantability, or fitness for a particular purpose. Nothing herein should be construed as legal advice.\n\nÂ© 2024 IAPP. All rights reserved.", "fetched_at_utc": "2026-02-09T13:30:03Z", "sha256": "8da99f96370f05741e0255869ed3febeeb97361e3e77933572f8f59c35bbd08a", "meta": {"file_name": "AI Governance in Practice Report 2024 - IAPP.pdf", "file_size": 39709902, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-ai-openness-a-primer-for-policymakers-oecd-31d8c2826e76", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Openness - A Primer For Policymakers - OECD.pdf", "title": "AI Openness - A Primer For Policymakers - OECD", "text": "<!-- image -->\n\n## AI OPENNESS: A PRIMER FOR POLICYMAKERS\n\n## OECD ARTIFICIAL INTELLIGENCE PAPERS\n\nAugust 2025 No. 44\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Foreword\n\nThis report was approved and declassified by written procedure by the Global Partnership on Artificial Intelligence  (GPAI)  on  25  June  2025  and  prepared  for  publication  by  the  OECD  Secretariat.  Earlier versions  were  discussed  by  the  OECD  Working  Party  on  AI  Governance  (AIGO)  in  outline  form  in November 2023, and in draft form in June 2024. It was also discussed by the GPAI in December 2024.\n\nNote to Delegations:\n\nThis document is also available on O.N.E Members &amp; Partners under the reference code: DSTI/DPC/GPAI(2024)3/FINAL\n\nRevised version, August 2025\n\nPage 29:\n\nThe bibliographic entry for AndrÃ©, C. et al. (2025) has been amended to reflect the correct author .\n\nThis document, as well as any data and map included herein, are without prejudice to the status of or sovereignty  over  any  territory,  to  the  delimitation  of  international  frontiers  and  boundaries  and  to  the name of any territory, city or area.\n\nThe  statistical  data  for  Israel  are  supplied  by  and  under  the  responsibility  of  the  relevant  Israeli authorities. The use of such data by the OECD is without prejudice to the status of the Golan Heights, East Jerusalem and Israeli settlements in the West Bank under the terms of international law.\n\nCover image: Â© Kjpargeter/Shutterstock.com\n\nÂ© OECD 2025\n\n## Corrigendum\n\n<!-- image -->\n\n## Attribution 4.0 International (CC BY 4.0)\n\nThis  work  is  made  available  under  the  Creative  Commons  Attribution 4.0  International  licence.  By  using  this  work,  you  accept  to  be  bound  by  the  terms  of  this  licence (https://creativecommons.org/licenses/by/4.0/).\n\nAttribution -you must cite the work.\n\nTranslations -you must cite the original work, identify changes to the original and add the following text: In the event of any discrepancy between the original work and the translation, only the text of original work should be considered valid.\n\nAdaptations -you must cite the original work and add the following text: This is an adaptation of an original work by the OECD. The opinions expressed and arguments employed in this adaptation should not be reported as representing the official views of the OECD or of its Member countries.\n\nThird-party material -the licence does not apply to third-party material in the work. If using such material, you are responsible for obtaining permission from the third party and for any claims of infringement.\n\nYou must not use the OECD logo, visual identity or cover image without express permission or suggest the OECD endorses your use of the work.\n\nAny dispute arising under this licence shall be settled by arbitration in accordance with the Permanent Court of Arbitration (PCA) Arbitration Rules 2012. The seat of arbitration shall be Paris (France). The number of arbitrators shall be one.\n\n## Acknowledgements\n\nThis report was prepared under the aegis of the OECD Working Party on AI Governance (AIGO) and the Global Partnership on AI (GPAI). Karine Perset, Luis Aranda, and Guillermo HernÃ¡ndez (OECD Artificial Intelligence and Emerging Digital Technologies Division) led the report development and drafting, under the  supervision  of  Audrey  Plonk,  Deputy  Director  of  the  OECD  Science,  Technology  and  Innovation Directorate. An earlier version of this report was drafted by Elizabeth Seger (Oxford University).\n\nThe paper benefitted significantly from the oral and written contributions of AIGO and GPAI delegates as well  as  experts  from  the  OECD.AI  network  of  experts.  The  authors  would  like  to  extend  their  sincere gratitude  to  the  Delegations  of  Australia,  Brazil,  Canada,  Chile,  Croatia,  the  European  Commission, France, Germany, India, Israel, Mexico, New Zealand, Poland, Slovenia, Sweden, Thailand, TÃ¼rkiye, the United  Kingdom,  and  the  United  States  for  their  invaluable  insights.  In  particular,  they  gratefully acknowledge contributions from Simon van Hoeve (Australia); Franklin Rodrigues Hoyer (Brazil); Ricardo Baeza-Yates (Chile); Juraj Bilic (Croatia); Jonas Roule (France); Abhishek Singh (India); Ziv Katzir (Israel); Juraj Corba (Slovakia); Polonca Blaznik and Martin Marzidovsek (Slovenia); Jesse Dunietz and David Turnbull (US); Carlos MuÃ±oz Ferrandis (BigScience); Loise Mwarangu (AI Centre of Excellence, Kenya); Marko Grobelnik (Jozef Stefan Institute); and Stuart Russell (UC Berkeley).\n\nThe Secretariat would also like to thank stakeholder groups at the OECD for their input, including Pam Dixon  (Civil  Society  Information  Society  Advisory -CSISAC);  Nicole  Primmer  and  Maylis  Berviller (Business at OECD -BIAC); Sarah Jameson and Aida Ponce (Trade Union Advisory Committee -TUAC); and Sebastian Hallensleben and Jibu Elias (Internet Technical Advisory Committee -ITAC).\n\nFinally, the authors thank all those who have contributed to the report throughout its development. This includes Jeff Mollins, Lucia Russo, Kasumi Sugimoto, Sarah BÃ©rubÃ©, and Nikolas Schmidt (OECD/STI); Manuel Betin and Peter Gal (OECD/ECO); and Richard May (OECD/DAF). The authors also thank Anaisa Goncalves,  Shellie  Laffont  and  Andreia  Furtado  for  editorial  support;  the  overall  quality  of  this  report benefitted significantly from their engagement.\n\n## Table of contents\n\n| Foreword                                                                                                                                                | 2   |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------|-----|\n| Acknowledgements                                                                                                                                        | 4   |\n| Abstract                                                                                                                                                | 7   |\n| RÃ©sumÃ©                                                                                                                                                  | 8   |\n| Executive summary                                                                                                                                       | 9   |\n| Introduction                                                                                                                                            | 11  |\n| 1. Delving into AI openness                                                                                                                             | 12  |\n| 1.1. The term open-source AI is a misleading legacy                                                                                                     | 12  |\n| 1.2. Degrees of AI openness: The more model components are publicly released, the easier it is for other actors to reproduce, modify, and use the model | 14  |\n| 1.3. Licensing choices influence access levels, innovation speed, and the potential for beneficial and harmful uses                                     | 15  |\n| 1.4. Clarifying key AI terms: generative AI and foundation models                                                                                       | 15  |\n| 1.5. This report explores the trends, benefits and risks of open-weight foundation models                                                               | 15  |\n| 2. Evolution of open-weight models                                                                                                                      | 17  |\n| 3. Benefits and risks of openly releasing the weights of foundation models                                                                              | 23  |\n| 3.1. Illustrative benefits                                                                                                                              | 23  |\n| 3.2. Illustrative risks                                                                                                                                 | 24  |\n| 3.3. Marginal benefits and risks as part of holistic risk assessments                                                                                   | 26  |\n| 4. Conclusions                                                                                                                                          | 28  |\n| References                                                                                                                                              | 29  |\n| Tables                                                                                                                                                  |     |\n| Table 1.1. Components of the Linux Foundation's Model Openness Framework                                                                                | 13  |\n\nï¼\n\n12\n\n14\n\n15\n\n15\n\n15\n\n23\n\n24\n\n26\n\n13\n\n## 6 ï¼ AI OPENNESS: A PRIMER FOR POLICYMAKERS\n\n| Figures                                                                                                                                                                                          |    |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n| Figure 2.1. The supply of foundation models has increased consistently, with open-weight models representing over half of commercially available models                                          | 17 |\n| Figure 2.2. The United States, China and France are at the forefront of open-weight model development, with the largest offerings coming from providers in the US, the Netherlands and Singapore | 19 |\n| Figure 2.3. Over half of foundation model providers are in the United States                                                                                                                     | 20 |\n| Figure 2.4. Significant gains in the quality of open-weight models                                                                                                                               | 22 |\n| Boxes                                                                                                                                                                                            |    |\n| Box 1.1. Further research is needed to refine and assess open access gradients of AI systems                                                                                                     | 14 |\n| Box 2.1. The AIKoD database on active generative AI foundation models                                                                                                                            | 18 |\n\n17\n\n19\n\n20\n\n22\n\n14\n\n18\n\n## Abstract\n\nThis report explores the concept of openness in AI, including relevant terminology and how different degrees of openness can exist. It explains why the term \"open source,\" a term rooted in software, does not fully capture the complexities specific to AI. This report analyses current trends in open-weight foundation models using experimental data, illustrating both their potential benefits and associated risks. It incorporates the concept of marginality to further inform this discussion. By presenting information clearly and concisely, the report seeks to support policy discussions on how to balance the openness of generative AI foundation models with responsible governance.\n\n## RÃ©sumÃ©\n\nCe rapport explore le concept d'ouverture dans l'IA, y compris la terminologie pertinente  et  comment  diffÃ©rents  degrÃ©s  d'ouverture  peuvent  exister.  Il explique pourquoi le terme \"open source\", un terme ancrÃ© dans le logiciel, ne  capture  pas  pleinement  les  complexitÃ©s  spÃ©cifiques  Ã   l'IA.  Ce  rapport analyse les tendances actuelles des modÃ¨les de fondation Ã  poids ouverts en utilisant des donnÃ©es expÃ©rimentales, illustrant Ã  la fois leurs avantages potentiels et les risques associÃ©s. Il intÃ¨gre le concept de marginalitÃ© pour enrichir  cette  discussion.  En  prÃ©sentant  l'information  de  maniÃ¨re  claire  et concise, le rapport vise Ã  soutenir les discussions politiques sur la maniÃ¨re d'Ã©quilibrer l'ouverture des modÃ¨les de fondation gÃ©nÃ©rative de l'IA avec une gouvernance responsable.\n\n## Executive summary\n\nA clearer understanding of AI openness terminology is essential. This report examines key concepts and definitions to foster a common understanding of AI openness, acknowledging that precise terminology is vital for researchers, developers, and policymakers as AI technologies continue to develop.\n\nThe  term ' open  source ' in  AI  has  limitations: The  term  'open  source' originates  from  software development and does not accurately describe AI systems. Unlike traditional software, AI 'source code' may  refer  to  inference  code,  training  code,  or  both -and  each  can  be  made  publicly  available independently. Furthermore, AI models can have other critical components such as model weights and training data, which can also be shared or kept private.\n\nAI openness exists on a spectrum: It is not binary but ranges from fully closed systems with restricted access  to  fully  open  models  that  permit  unrestricted  access,  modification,  and  use.  This  spectrum encompasses various system components, including data, code, and documentation. Recognising this range is essential for understanding the policy implications of different levels of openness across these components.\n\nDefinition of open-weight AI models in this report: This report uses the term open-weight models to refer to foundation models with publicly available trained weights. These models can generate content and perform  a  variety  of  tasks  across  different  applications.  While  licensing  is  an  important  aspect  of  the discussion  surrounding  the  availability  of  AI  models,  this  report  focuses  on  open  weights  due  to  their growing relevance in policy discussions about the benefits and risks associated with these models.\n\nMarket trends and global distribution: Since early 2023, the number of foundation models has surged, with  open-weight  models  now  making  up  over  half  of  the  market.  The  United  States  leads  in  the development of open-weight foundation models, followed by the People's Republic of China (hereafter ' China ' ) and France. The Netherlands and Singapore serve as key provider hubs due to their advanced cloud capabilities, highlighting the global nature of AI deployment.\n\nRapid  improvements  in  quality: Open-weight  foundation  models  have  significantly  improved  in performance since early 2024, achieving higher scores on common benchmarks. While these advances offer substantial benefits, they also increase potential risks, underscoring the importance of monitoring openness in AI development.\n\nBenefits of open-weight models: Releasing model weights can provide significant advantages, including enabling external evaluation and accountability, accelerating research and innovation, fostering competition, facilitating access to AI technologies, and supporting sensitive data management. However, realising these benefits often depends on access to sufficient computing resources, data, and skilled talent.\n\nRisks of open-weight foundation models: Open-weight models also pose significant risks, including the potential for malicious activities such as deepfakes, advanced cyberattacks, large-scale generation of child sexual abuse material (CSAM) and non-consensual intimate imagery (NCII). There is also the speculative risk of misuse in areas like biology or chemistry. The availability of model weights can empower malicious actors to fine-tune these models for unintended uses or harmful purposes. Furthermore, modifying model weights  can  enable  malicious  actors  to  bypass  some  of  the  safeguards  put  in  place  by  the  original developers.\n\nDeciding on openness: Decisions to release model weights should carefully consider potential benefits and risks. Falling compute costs and more accessible fine-tuning methods lower the barriers to both use and misuse, enhancing the potential advantages of open-weight models while also increasing the risk of harmful applications. It is essential to evaluate the marginal risks and benefits of releasing model weights\n\n## 10 ï¼ AI OPENNESS: A PRIMER FOR POLICYMAKERS\n\nin relation to closed models and existing technologies. However, this should be done as part of a broader, holistic risk assessment framework that can adapt to evolving capabilities and usage patterns. Developers and other relevant stakeholders should consider whether the benefits outweigh the risks in a given context and consider the potential opportunity costs of not releasing open-weight models.\n\n## Introduction\n\nSince the 1990s, open-source software (OSS) has proliferated alongside, and often within, commercial software,  encouraging  co-operation,  promoting  software  adoption  including  in  developing  countries  by lowering costs, balancing the market power of major software companies, fostering innovation, enabling upskilling,  and  improving  software  quality  through  community  feedback  (Langenkamp  et  al.,  2022[1]; Engler, 2021[2]; Engler, 2021[3]). Given these and other benefits, the OSS tradition has been inherited by certain groups in the AI community, where an AI model (or some elements of it) are released publicly for anyone to download, modify and distribute, often under the terms of a licence.\n\nThere is ongoing debate about the risks, benefits, and trade-offs of making AI models or their components publicly available, particularly regarding increasingly advanced AI foundation models that exhibit generalpurpose capabilities. The debate has gained momentum following recent launches of open-weight models, including Deepseek R1, OpenAI's GPT -OSS, and Alibaba's Qwen.\n\nNumerous  beneficial  uses  of  foundation  models  are  developing  in  healthcare  (Fries  et  al.,  2022[4]), customer support (OpenAI, 2023[5]), immersive gaming  (Marr, 2023[6]), or personalised education (Marr, 2023[7]).  Access  restrictions  to  AI  models  could  stifle  innovation,  limit  external  evaluation,  hinder  the widespread distribution of AI benefits, and concentrate control over future AI technology in the hands of a small number of actors (Goldman, 2023[8]; Creative Commons et al., 2023[9]). However, foundation models can also  be  misused  and  deployed  by  malicious  actors,  for  example,  to  generate  child  sexual  abuse material, infringe intellectual property and privacy rights, or conduct convincing scams in which victims believe they are interacting with trusted friends and family.\n\nOnce a model is open, many safeguards to prevent misuse can be circumvented, and actors with sufficient expertise and computing resources could \"fine-tune\" it to enhance its propensity for misuse. Furthermore, fully removing open models after their release or adding guardrails retroactively to prevent newly identified risks may prove challenging. Restrictions on releasing AI models and their different components could raise questions about intellectual property rights and may enhance security, incentivising innovation and limiting the proliferation of risks.\n\nThis report examines the potential risks and benefits of open-weight models -understood as foundation models for which the weights are publicly available -keeping in mind that as the development and adoption of foundation models continue to advance amidst other technological and societal changes, the balance between the risks and benefits of releasing their weights may change.\n\nAlthough this report focuses on open-weight foundation models, it is important to note that AI models that are not generalpurpose, or 'advanced' in the sense described above, can also pose risks. For example, Urbina et al. (2022[10]) showed that standard, narrow AI tools used within the pharmaceutical industry could be repurposed to assist with the design of biochemical weapons.\n\nThe report is organised as follows. Section 1 defines key terms and scope, delving into the different levels of openness in AI and explaining why the term ' open source ', originally used for software, may not fully apply in the AI context. Section 2 analyses current trends in open-weight models using experimental data from the OECD. Section 3 illustrates the potential benefits and risks of open-weight models and presents the concept of marginality. Section 4 concludes.\n\n## 1. Delving into AI openness\n\nThis  section  delves  into  the  concepts  and  definitions  that  are  essential  for  establishing  a  shared understanding  of  AI  openness.  As  the  field  of  AI  continues  to  evolve,  clear  terminology  becomes increasingly important for researchers, developers, and policymakers alike.\n\n## 1.1. The term open-source AI is a misleading legacy\n\nThe term 'open -source' originates from 'open -source software' (OSS). 'Open -source' was defined in 1998 as a 'social contract' (and later a certification) describing software designed to be publicly available and released under a license that sets the conditions for using, modifying, and distributing the source code. According to the Open Source Initiative (OSI), an open-source software license must meet ten key criteria, which include free source code access, permission for derived works, and no limits on who may use the software or for what purpose (Perens, 1999[11]; Choose a License, 2023[12]) .  In  principle, 'o pen source ' does not necessarily preclude a related commercial activity.\n\nMultiple definitions of open-source AI exist today. OSI released a draft open-source AI definition leveraging the OECD definition of an AI system:\n\n- ' An open-source AI is an AI system made available under terms and in a way that grant the freedoms to: use the system for any purpose and without having to ask for permission; study how  the  system  works  and  inspect  its  components;  modify  the  system  for  any  purpose, including  to  change  its  output;  and  share  the  system  for  others  to  use  with  or  without modification, for any purposes. These freedoms apply both to a fully functional system and to discrete elements of a system. A precondition to exercising these freedoms is to have access to the preferred form for make modification to the system' (OSI, 2025[13]).\n\nThe Linux Foundation has also proposed an open-source AI definition that, unlike the OSI draft, entails sharing information about the underlying components (LinuxFoundation, 2024[14]):\n\n- ' Open-source artificial  intelligence  (AI)  models  enable  anyone  to  reuse  and  improve  an  AI model. Open-source AI models include the model architecture (in source code format), model weights  and  parameters,  and  information  about  the  data  used  to  train  the  model  that  are collectively published under licenses, allowing any recipient, without restriction, to use, study, distribute,  sell,  copy,  create  derivative  works  of,  and  make  modifications  to,  the  licensed artifacts or modified versions thereof ' .\n\nThe open-source softwar e concept of 'free and publicly downloadable source code' does not translate directly to AI due to differences in how AI models are built compared to traditional software (Finley, 2011[15]; Sijbrandij, 2023[16]) . For A I models, 'source code' could refer to either the inference code, the training code, or both, and the two can be shared independently. AI models have additional components beyond source code, including model weights and training data, all of which can either be shared or kept private and independent from the source code and from each other.\n\nIn  particular,  source  code  and  model  weights represent two distinct concepts.  Referring to weights as \"open source\" is misleading, as they do not constitute source code. Source code comprises instructions\n\nfor executing specific tasks, whereas weights are the results of training and fine-tuning processes applied to data. Licenses intended for source code do not directly apply to AI model weights (Sijbrandij, 2023[16]).\n\nFor  these  and  other  considerations,  the  term  \"open-source  AI\"  is  currently  debated.  Some  interpret  it according to the OSI definition of open source, while others see it as encompassing a range of access options, from \"non-gated downloadable\" models to fully open models. Fully open models, like GPT-J, make all training and inference code, weights, and documentation publicly available. They can be freely used, modified, and distributed, including for commercial purposes. Non-gated downloadable models provide some components, such as training code and model weights, while withholding others. This is in contrast with gated downloadable models that restrict access to certain users.\n\nThis debate is illustrated by the framing of AI models like LLaMA, LLaMA2, Dolly, or StableLM, which use the term 'open source' in a way that is inconsistent with OSI's definition of open -source software (Finley, 2011[15];  Maffulli,  2023[17]).  Some  developers  claim their models are open source simply because their weights  are  available  for  download,  even  though  their  licenses  may  restrict  certain  use  cases  and distribution.\n\nThe Linux Foundation has proposed a Model Openness Framework (MOF) to help evaluate and classify the openness of AI models, by assessing which components of the model are publicly released and under what licenses (Table 1.1). It defines three progressively broader classes of model openness:\n\n- Class III -Open Model: The minimum bar for entry, Class III requires the public release of the core model (architecture, parameters, basic documentation) under open licenses. This allows model consumers to use, analyse, and build on the model, but limits insight into the development process.\n- Class II -Open Tooling: Building on Class III, this tier includes the full suite of code used to train, evaluate, and run the model, plus key datasets. Releasing these components enables the community to better validate the model and investigate issues. It is a significant step towards reproducibility.\n- Class I -Open Science: The apex, Class I entails releasing all artifacts following open science principles. In addition to the Class II components, it includes the raw training datasets, a thorough research paper detailing the entire model development process, intermediate checkpoints, log files, and more. This provides unparalleled transparency into the end-to-end development pipeline, empowering collaboration, auditing, and cumulative progress (LinuxFoundation, 2024[18]).\n\nTable  1.1. Components of the Linux Foundation 's Model Openness Framework\n\nThe framework identifies 17 critical components for a complete model release.\n\n| Code                | Data                         | Documentation      |\n|---------------------|------------------------------|--------------------|\n| Evaluation code     | Datasets                     | Data card          |\n| Pre-processing code | Evaluation data              | Research paper     |\n| Model architecture  | Sample model outputs         | Evaluation results |\n| Libraries and tools | Model weights and parameters | Model card         |\n| Training code       | Model metadata               | Technical report   |\n| Inference code      | Configuration file           |                    |\n\nNote: For each component, the MOF stipulates the use of standard open licenses based on the artifact type: open-source licenses for code (e.g., Apache 2.0, MIT), open-data licenses for datasets and model parameters (e.g., CDLA-Permissive, CC-BY), and open-content licenses for documentation and content/unstructured data (e.g., CC-BY). Sample model outputs can be code or data. Source: Adapted from Linux Foundation (2024[18]).\n\n## 14 ï¼ AI OPENNESS: A PRIMER FOR POLICYMAKERS\n\n## 1.2. Degrees of AI openness: The more model components are publicly released, the easier it is for other actors to reproduce, modify, and use the model\n\nCurrently,  there  is  a  spectrum  of  model  release  options  available  which  encompass  its  various components, ranging from fully closed systems, where access to the model and its underlying data is highly restricted,  to  fully  open  models,  which  allow  unrestricted  access  and  modification  by  users  (Box  2.1). Understanding  this  spectrum  is  crucial  for  evaluating  the  implications  of  different  access  levels  on innovation, collaboration, and security and other considerations.\n\n## Box  1.1. Further research is needed to refine and assess open access gradients of AI systems\n\nThe landscape of AI model release options is diverse, encompassing a spectrum from fully closed models to various degrees  of  openness,  including  gradual  or  staged  releases,  hosted  access,  cloud-based  API  access,  downloadable models,  and  fully  open  models.  By  systematically  examining  the  different  release  strategies,  researchers  and policymakers  can  identify  the  associated  benefits  and  drawbacks  of  each  approach,  including  issues  related  to transparency and security.\n\nFurther research is essential to advance the responsible deployment of AI technologies. This research should aim to establish good practices for model sharing that balance openness and security, protect intellectual property and sensitive data, and promote more equitable access and responsible use of AI systems. Such efforts will contribute to a more informed and strategic approach to AI model dissemination.\n\nSource: Solaiman (2023[19]); Sastry (2023[20]); Liang et al. (2022[21]) .\n\nAccess to model architecture and trained weights, when combined with inference code, is enough for using a pre-trained model to perform specific tasks. Downstream developers can write their own inference code or even generate it using tools such as ChatGPT, and it does not need to match the original code used by the model developer. Having access to model weights also allows downstream developers to fine-tune and optimise the model for specific tasks and applications or modify its behaviour as needed. Additionally, releasing parts of the training code, such as the code for cleaning and loading training data, can help developers reproduce the training weights and utilise the model more easily.\n\nSometimes, an AI developer publicly releases the training and inference code for a model, but not the trained model weights (Meta, 2023[22]; Vincent, 2023[23]). In such cases, actors with sufficient computing resources and data access could train an equivalent model and use inference code to run it. However, few actors currently have the computing resources needed to train advanced foundation models.\n\nRecently,  several  developers  have  restricted  access  to  some  of  their  models  due  to  concerns  about competition and potential misuse. These developers may choose to keep their models completely private, like DeepMind's  Chinchilla (Hoffmann  et al.,  2022[24])  or  share  them  in  a  controlled  manner,  such  as OpenAI's GPT -4  (OpenAI,  2023[25]) and Anthropic's  Claude  2 (Anthropic,  2023[26])  through  application programming interfaces (APIs)  (Brockman et al.,  2023[27]). This approach allows them to enforce  user restrictions and maintain better control over features. In contrast, some developers have advocated for more open models. Meta, for example, announced the 'open -source' release of I-JEPA (Assran et al., 2023[28]),  Llama  2  (Inskeep  and  Hampton,  2023[29];  Milmo,  2023[30]),  and  Llama  3,  but  faced  criticism including from OSI for the access restrictions placed on its models (Anandira, 2024[31]). These examples illustrate the difference between the traditional definition of open-source software and the varying degrees of openness in AI models.\n\n## 1.3. Licensing choices influence access levels, innovation speed, and the potential for beneficial and harmful uses\n\nLicensing in ' open-source ' AI governs many terms under which AI models, including their weights and associated code, can be used, modified, and distributed. Unlike proprietary models that restrict access to discrete users or that customise licensing access conditions or provide exclusivity, open-source AI offers various licensing schemes that grant different nonexclusive standardised conditions to all users, generally for free. Permissive licenses, such as Apache 2.0 or MIT, typically allow for broad usage, modification, and commercialization with minimal restrictions beyond attribution and disclaimers (OSI, 2025[32]). Conversely, more restrictive or \"copyleft\" licenses, like certain versions of Creative Commons, may require that any derivative  works  also  be  shared  under  the  same  or  similar  open-source  terms,  aiming  to  ensure  the continued  openness of the AI ecosystem (Commons, 2025[33]). The choice of license can significantly impact collaboration, innovation, adoption, and the potential for beneficial and harmful uses of AI models.\n\nWhile a detailed discussion of specific AI model licensing is outside the scope of this analysis, it's crucial to acknowledge their pivotal role in shaping the current and future landscape. Licensing decisions directly influence who can access and build upon AI models, impacting the pace of development and the potential for  both  beneficial  and  harmful  applications.  For  instance,  more  permissive  nonexclusive  licenses  can accelerate innovation by allowing widespread experimentation and integration into commercial products, but they might also offer fewer safeguards against misuse by malicious actors. Conversely, more restrictive licenses may often be necessary to incentivise model development and investment for specific market needs, but they could also inadvertently limit collaboration. Understanding the nuances of AI licensing is essential for formulating effective policies relating to the release and use of these technologies.\n\n## 1.4. Clarifying key AI terms: generative AI and foundation models\n\nAI models are actionable representations of all or part of the external context or environment of an AI system (encompassing, for example, processes, objects, ideas, people and/or interactions taking place in context). AI models use data and/or expert knowledge provided by humans and/or automated tools to represent, describe and interact with real or virtual environments (OECD, 2022[34]).\n\nGenerative  AI  (genAI)  models create  new  outputs  (e.g.,  text,  code,  audio,  images,  video),  often  in response to prompts, based on their training data (OECD, 2023[35]).\n\nFoundation models ,  sometimes referred to as or categoris ed under ' general-purpose ' AI  models, are machine learning models that can be adapted to perform a wide range of downstream tasks, such as tasks involving text synthesis, behaviour prediction, image analysis and content generation (Bommasani et al., 2022[36]; Jones, 2023[37]). Foundation models can be standalone or integrated into a variety of downstream AI systems and models, either directly or after additional training referred to as 'fine -tuning.' There are two primary types of foundation models: generative models, which learn the patterns and distribution of input data to create new, plausible outputs (such as OpenAI's GPTs) , and discriminative models, which predict data labels by distinguishing between different classes in a dataset (like Google's BERT) (OECD, 2022[34]). It is important to note that not all generative or discriminative models qualify as foundation models.\n\n## 1.5. This report explores the trends, benefits and risks of open-weight foundation models\n\nFor the purposes of this report, 'open -weight models' refe r to foundation models for which at least the trained  model  weights  are  publicly  available  for  download  for  local  deployments.  These  models  are\n\n## 16 | AI OPENNESS: A PRIMER FOR POLICYMAKERS\n\ncharacterised by their ability to generate content, perform various tasks, and adapt to different applications based on the data they have been trained on.\n\nAs  noted  above,  there  are  discriminative  foundation  models,  and  generative  AI  models  that  are  not foundation models. Additionally, there are benefits and risks associated with openly sharing the weights of machine  learning  models  that  are  neither  generative  nor  foundation  models.  The  decision  to  focus on generative AI foundation models responds to feedback from national delegations on earlier versions of this report, the substantial efforts required to collect relevant data (see Section 2), and the relevance of this technology in current policy discussions.\n\nLicensing  considerations  are  beyond  the  scope  of  this  report.  While  licensing  undeniably  shapes  how models  are  used  and  shared,  its  legal  and  jurisdictional  complexity  makes  it  impractical  to  address comprehensively  in  this  report.  Nonetheless,  licensing  remains  a  critical  factor  in  the  deployment  and deployment of open-weight models and may warrant dedicated analysis in future work. By narrowing our scope, we aim to provide a clearer, more actionable discussion on open-weight foundation models.\n\n## 2. Evolution of open-weight models\n\nSince OpenAI launched GPT-3.5 (ChatGPT) in November 2022, the market for generative AI models has experienced  significant  growth,  fuelled  by  new  developers  entering  the  field  and  existing  developers introducing new models. Figure 2.1 reveals a marked acceleration in the global supply of generative AI foundation models, particularly from mid-2024 onward. Notably, open-weight models have not only kept pace with this growth but now account for approximately 55% of all available models as of April 2025. This suggests a trend toward a greater development and provision of open-weight models. This data focusses on foundation models made available commercially by one or more providers through an API endpoint, and is the product of OECD research on developments in AI markets (Box 2.1).\n\n## Figure  2.1. The supply of foundation models has increased consistently, with open-weight models representing over half of commercially available models\n\nNumber of unique closed and open-weight models made commercially available from providers worldwide each month through an API endpoint.\n\n<!-- image -->\n\nNote: Open weights indicate that the trainable weights -parameters optimised during training and fine-tuning -are made available for download for local deployments. See AndrÃ© et al. (2025[38]) for more details on the methodology.\n\nSource:  OECD.AI  (2025),  data  from  the  AIKoD  experimental  database  (internal),  last  updated  2025-04-30,  accessed  on  2025-05-14, https://oecd.ai/.\n\n## 18 ï¼ AI OPENNESS: A PRIMER FOR POLICYMAKERS\n\nFigure 2.2  illustrates  the  evolving  landscape  of  open-weight  model  development  and  provision  across countries.  The  United  States  leads  in  both  dimensions,  reflecting  its  robust  AI  ecosystem  and  cloud infrastructure. China and France also emerge as key developers, while the Netherlands and Singapore\n\n<!-- image -->\n\nï¼\n\nstand out as major provider hubs, despite having fewer domestic developers. This divergence underscores the  global  nature  of  AI  deployment,  where  models  are  often  hosted  in  countries  with  advanced  cloud capabilities, regardless of their origin. The data also reveals a growing international dispersion of model provision.\n\nFigure  2.2. The United States, China and France are at the forefront of open-weight model development, with the largest offerings coming from providers in the US, the Netherlands and Singapore\n\n<!-- image -->\n\n## 20 ï¼ AI OPENNESS: A PRIMER FOR POLICYMAKERS\n\nNote: \"Developers\" are firms that pre-train and fine-tune an AI model, which is then made available commercially by one or more providers through an API endpoint for users, such as OpenAI, Anthropic, and Google. 'Provider' refers to the cloud service company that supports and hosts the model. \"Country\" refers to where either the developers (panel a) or the providers (panel b) are based. Some companies, like OpenAI, Google, Microsoft, and Amazon, both develop and provide models, while others, like Meta, only develop models. Additionally, some companies, such as ReplicateAI, PerplexityAI, and Deep Infra, only provide models created by other developers. A single developer can offer multiple models in different formats and can distribute them via different cloud providers. See AndrÃ© et al. (2025[38]) for more details on the methodology. Source:  OECD.AI  (2025),  data  from  the  AIKoD  experimental  database  (internal),  last  updated  2025-04-30,  accessed  on  2025-05-14, https://oecd.ai/.\n\nFigure 2.3 highlights the geographic concentration of foundation model providers, revealing United States dominance, which accounts for over half of all providers globally. This leadership is also present in the development of open-weight models. While several other countries such as China, the United Kingdom, France, and Germany also contribute to the ecosystem, their presence is markedly smaller.\n\n## Figure  2.3. Over half of foundation model providers are in the United States\n\na) Provider count per country, all foundation models.\n\n<!-- image -->\n\n## b) Provider count per country, open-weight models.\n\n<!-- image -->\n\n<!-- image -->\n\nNote: 'Provider' refers to the cloud service company that supports and hosts the model. See AndrÃ© et al. (2025[38]) for more details on the methodology.\n\nSource:  OECD.AI  (2025),  data  from  the  AIKoD  experimental  database  (internal),  last  updated  2025-04-30,  accessed  on  2025-05-14, https://oecd.ai/.\n\nThe supply of generative AI foundation models is predominantly driven by text-to-text models, including coding assistants and models with multi-modal features. These models account for 78% of all offerings. Text-to-image models, which focus on image generation, represent 18% of the total supply, while audioto-text models make up a smaller share at 2.5%. Notably, the average quality of open-weight text-to-text models -including foundation models and all their variants and updates -has seen rapid improvement since early 2024, as illustrated in  Figure 2.4. This trend highlights the significant advancements in the performance of LLMs over a short period of time.\n\n## AI OPENNESS: A PRIMER FOR POLICYMAKERS ï¼ 21\n\n## 22 ï¼ AI OPENNESS: A PRIMER FOR POLICYMAKERS\n\n## Figure  2.4. Significant gains in the quality of open-weight models\n\nAverage quality (performance) of text-to-text open-weight models over time.\n\n<!-- image -->\n\nNote: The quality index is a normalised weighted measure that reflects how well a model performs a task, based on standard benchmarks of the industry acquired from Artificial Analysis and Hugging Face. Text-totext models take the average across Hugging Face's MMLU scores, Arena ELO scores, and the GPQA value. Quality is averaged across all model offerings, even when the same model is offered by multiple providers. Please note that this is an average, so a drop in quality may happen if the proportion of lower-quality models increases. See AndrÃ© et al. (2025[38]) for more details on the methodology.\n\nSource:  OECD.AI  (2025),  data  from  the  AIKoD  experimental  database  (internal),  last  updated  2025-04-30,  accessed  on  2025-05-14, https://oecd.ai/.\n\nThe data in this section indicates that open-weight models are increasing in both quantity and quality. Recent  launches  of  openweight  models,  such  as  OpenAI's  GPT -OSS,  underscore  this  point.  This highlights  the  need  to  monitor  openness  in  AI  development  and  deployment  as  a  key  dimension  of technological access and policy approaches.\n\n## 3. Benefits and risks of openly releasing the weights of foundation models\n\nThis section highlights some of the benefits and risks associated with open-weight models. Rather than providing a comprehensive overview, it aims to illustrate key advantages, such as enhanced innovation and collaboration, alongside potential challenges, including privacy concerns and the risk of malicious use. By presenting this information clearly and concisely, the section seeks to inform policy discussions on balancing the openness of generative AI foundation models with responsible AI policy approaches.\n\n## 3.1. Illustrative benefits\n\nThe benefits of open-weight models encompass a range of illustrative examples, presented here in no particular order:\n\n- Facilitate beneficial innovation: Open-weight models can accelerate AI research and development,  driving  innovation  and  the  integration  of  new  downstream  applications  (Creative Commons et al., 2023[9]; Engler, 2021[2]; Engler, 2021[3]). These models allow developers to build on existing technologies, promoting collaboration and experimentation that can lead to significant advancements  across  industries.  This  collaborative  environment  accelerates  innovation  and encourages the exploration of new use cases, ultimately broadening AI's impact on everyday life (Gans, 2025[39]).\n- External evaluation : Open-weight models facilitate independent evaluations of projects by wider communities  of  developers  and  contributions  from  individuals,  thus  enabling  more  robust evaluations of performance and risk. Involving the wider AI community also facilitates audit and analysis of open-weight models and their components ( e.g. , training data, weights, documentation) which can help detect bugs, biases, and other issues (Creative Commons et al., 2023[9]; Bommasani et al., 2023[40]). Also, assessing whether the model performs as well as its creators claim is a key form of evaluation.\n- Enable  efficiencies  in  AI  development: Open-weight  models  enable  large-scale  collaborative efforts and allow downstream developers to optimise existing models instead of having to start from scratch  for  each  new  application -which  may  help  reduce  resource  consumption  and  costs associated to AI development (HuggingFace, 2023[41]).\n- Facilitate talent development: Opening the weights of foundation models could enhance talent development. More people being able to interact with pre-trained cutting edge-models may, over time, lead to a larger AI talent pool and, in the longer run, help address the digital divide by enabling initiatives from across regions.\n- Expand  use,  adoption,  and  market  choice: Open-weight  models  expand  the  developer community  and  foster  competition by lowering barriers to innovation and  entry, enabling collaboration and providing opportunities for skill development. This encourages participation from individuals in new or less common regions and backgrounds, leading to the creation of applications\n\n## 24 ï¼ AI OPENNESS: A PRIMER FOR POLICYMAKERS\n\nthat  address  the  specific  needs  of  various  user  groups,  such  as  generative  AI  tools  for  various languages and cultural contexts. This could in turn enable a wider range of people to use and benefit from AI applications.\n\n- Support  sensitive  data  management: Open-weight  models  facilitate  the  adoption  of  AI  by businesses  and  governments  that  may  lack  the  resources  to  develop  proprietary  AI  solutions independently and possess sensitive data that cannot be shared with vendors of closed-weight models (The White House, 2025[42]).\n- Enable on-device solutions: Direct  access  to  model  weights  facilitates  on-device  deployment, crucial for offline functionality or in environments with stringent privacy requirements. This eliminates reliance on internet connectivity and third-party API access and could help to address concerns around data transmission and control (Zheng, Chen and Bin, 2024[43]).\n- Improve digital security and safeguards: Releasing foundation model weights can strengthen cybersecurity by enabling red teams to legally test and simulate potential attack scenarios using the same tools adversaries might exploit. This enhances adversary emulation, allowing for more realistic and effective defence strategies. Moreover, open-weight models can be more easily customised to implement safeguards tailored to specific operational contexts.\n- Prevent unintended and harmful behaviours: Open-weight foundation models can help prevent unintended and harmful behaviours, such as the generation of child sexual abuse material (CSAM) and privacy violations. By providing greater access to model weights, architectures and training data and processes, researchers can better identify the causes of such behaviours, align AI outputs with user values, and improve the detection of AI-generated content. This transparency allows for more effective evaluation and fine-tuning of models, ultimately leading to trustworthy AI systems that are less likely to produce harmful results (Al-Kharusi et al., 2024[44]; Thiel, 2023[45]; Hendrycks et al., 2022[46]).\n- Enhance alignment and explainability research: Alignment research seeks to ensure that  AI systems reflect user or developer preferences and values, often requiring model fine-tuning through methods like reinforcement learning. While this fine-tuning can be done via APIs (OpenAI, 2023[47]), these  interfaces  may  not  always  provide  sufficient  information  about  the  underlying  models  for meaningful  analysis.  Additionally,  some  aspects  of  explainability  research  necessitate  direct modifications to model parameters and activation patterns, which require full or nearly full access to the models.\n- Distribute influence: Open-weight model development allows a wide community to influence AI's evolution.  This  distribution  has  economic,  social,  and  political  implications,  which  may  enable broader sharing of AI's potential benefits (Bommasani et al., 2023[40]; Howard, 2023[48]; LAION.ai, 2023[49]).\n\nWhile the benefits of open-weight models are significant, their realisation may be limited by access to computing power, data resources, and available talent.\n\n## 3.2. Illustrative risks\n\nThe  risks  of  open-weight  models  encompass  a  range  of  illustrative  examples,  presented  here  in  no particular order:\n\n- Downstream impacts and proliferation of risks : Both open- and closed-weight foundation models can be used or combined with other tools, models or services (e.g., such as the Internet or thirdparty APIs) in unintended or harmful ways. However, open-weight models increase the accessibility for a wider range of users -including malicious actors -to fine-tune, modify and deploy models\n\nwithout the original developer's control .  This  greater accessibility  can accelerate the spread and amplification of existing risks, unresolved issues and vulnerabilities (Bran et al., 2023[50]; Boiko et al.,  2023[51]).  Additionally,  safeguards  implemented  by  foundation  model  developers -including guardrails  to  alleviate  inaccurate  outcomes -may  be  weakened  or  removed  when  models  are altered downstream, potentially leading to new or unforeseen issues. While closed models also carry risks, their restricted access can limit the scale and speed at which these risks propagate.\n\n- Challenges  in  monitoring  and  fixing: While  structured  API  access  allows  for  monitoring  and potential harm detection, open-weight models rely on downstream developers to implement fixes. This can hinder effective integration of updates, as developers may avoid updating due to lack of skills or resources or simply to retain certain  model functionalities, complicating the resolution of risks and vulnerabilities downstream. Once model weights are publicly available, complete recalling of the model and removal of all copies is unfeasible (Bengio et al., 2025[52]).\n- Model vulnerability exposure: Releasing the weights of one model can reveal vulnerabilities in other models and enable more sophisticated jailbreaking . For example, researchers have developed techniques  that  leverage  the  weights  of  a  model  to  create  \"adversarial  suffixes\",  which  are sequences that, when appended to a query, compel the model to produce harmful content. This method, developed using open-weight models, is transferable and can also be applied to closedweights models. Thus, releasing one model's weights could e xpose vulnerabilities in others (Zou et al., 2023[53]).\n- Intellectual property violations: Releasing the weights of foundation models may present risks to intellectual  property  rights,  primarily  concerning  the  unauthorised  reproduction,  adaptation,  and commercial  exploitation  of  copyrighted  material  used  in  their  training  data.  Research  has demonstrated that LLMs can indeed memorise and extract copyrighted content to varying extents, with  this  information  being  stored  within  the  model  parameters  (Cooper  et al.,  2025[54]).  Once weights are public, the original developers lose control over how the models are used or altered, making it challenging to track and prevent misuse that could lead to copyright violations.\n- Malicious  use: Releasing  the  weights  of  foundation  models  can  increase  their  vulnerability  to malicious use. With access to a model's weights and architecture, individuals with the necessary knowledge, skills,  and  compute  resources  can  write  or  modify  inference  code  to  run  the  model without the protocols typically implemented by closed model providers. This allows them to fine-tune the model, potentially allowing or enhancing harmful outputs. Fine-tuning can also be performed on closed models through an API. However, such fine-tuning can be monitored, e.g., the API owner can inspect the contents of the fine-tuning data set, which may allow them to prevent or at least detect  malicious  activity.  Fine-tuning  open-weight  models  generally  requires  a  higher  level  of technical expertise compared to proprietary, \"ready-to-use\" AI services. Examples of malicious use include:\n- o Digital  security  risks :  Releasing  model  weights  could  provide  both  malicious  actors  and cybersecurity red teams with increased novel means to conduct, analyse, and emulate offensive cyber  activities  (Mirsky  et al.,  2021[55];  Buchanan,  2020[56]).  While  open  weights  can  benefit cybersecurity defenders, they often require complex infrastructure that is not affordable for many cyber actors. However, small open-weight models can help malicious actors automate phishing campaigns, conduct opensource intelligence (OSINT) research, and perform routine programming tasks. Because offensive cyber tasks often look similar to software engineering in defensive  cybersecurity,  developers  of  both  open-weight  and  proprietary  models  find  it challenging to effectively  prevent  misuse.  Some researchers argue that  because foundation models are more complex than regular software, releasing model weights may favour attackers, as  quick  access  may  allow  them  to  exploit  vulnerabilities  more  rapidly,  while  developing\n\nsolutions takes time and resources. Even when solutions are created, they may not fully resolve the issues  (Shevlane and Dafoe, 2020[57]).\n\n- o Generation of child sexual abuse material (CSAM) and non-consensual intimate imagery (NCII): Research indicates that releasing the weights of image-generation models like Stable Diffusion  has  led  to  a  significant  rise  in  the  creation  of  NCII  and  CSAM,  highlighting  the challenges of monitoring the use of open-weight models (Thiel, Stroebel and Portnoff, 2023[58]). While techniques such as prompt filtering and output filtering can reduce the likelihood of harm from generated CSAM or NCII, it is more difficult to enforce the use of such techniques with open-weight models than with models provided as a service. The widespread distribution of CSAM and NCII creates significant harm to women, teenagers, and other vulnerable groups.\n- o Privacy risks: The accessibility of foundation model weights trained on sensitive or personally identifiable information increases privacy risks by enabling more effective membership inference (i.e., attackers exploit open model weights to determine if a specific data point was part of the training set); attribute inference (i.e., attackers leverage open model weights to deduce sensitive characteristics about the aggregate training data); and exploitation of memorisation vulnerabilities (i.e., attackers leverage open weights to facilitate the identification and extraction of specific private data points inadvertently memorised by the model). These methods could result in the extraction of sensitive training data from the model (Kandpal et al., 2024[59]; Nasr et al., 2023[60]; Carlini et al., 2022[61]).\n- Unpredictable  agentic  deployments :  Some  current  and  emerging  applications  of  foundation models allow these models to access external tools to interact with their environment (Yong, Shi and Zhang, 2025[62]). This approach leverages the broad knowledge and generative capabilities of a foundation model as an autonomous agent. Releasing the weights of a highly capable foundation model could facilitate a substantial expansion in the range of tools and environments it can access, leading to more complex attribution challenges and unpredictable interactions in both physical and virtual environments.\n\nSome of the risks associated with releasing foundation model weights are still speculative. For example, some experts  suggest  that  open-weight  models  could  be  misused  in  biological  or  chemical  contexts, although the evidence remains inconclusive (Mouton, Lucas and Guest, 2024[63]; Peppin et al., 2025[64]). Additionally,  the  risks  posed  by  open-weight  models  share  similarities  with  those  posed  by  other technologies,  such  as  Internet  search  engines  or  closed-weight  models,  and  these  risks  could  be heightened or reduced by releasing model weights.\n\n## 3.3. Marginal benefits and risks as part of holistic risk assessments\n\nEmphasising the importance of assessing risks and benefits \"on the margin\" -the additional risks and benefits associated with releasing foundation model weights, compared to risks posed by closed models or existing technology -is crucial for understanding the true impact of open-weight models (Bengio et al., 2025[52]). This approach allows stakeholders to evaluate how these models compare to existing tools and practices -both AI and non-AI -as well as to consider the potential outcomes in their absence (Kapoor et al., 2024[65]).\n\nFor example, if an open-weight model enhances productivity in content creation compared to traditional methods, the marginal benefit presents an argument in favour of open release. Conversely, if the risks associated  with  malicious  use  are  significantly  greater  than  those  posed  by  existing  technologies,  it becomes essential to evaluate whether the new models offer improved risk mitigation strategies or greater benefits that justify their release.\n\nIt is also helpful for decision-making based on marginal impact to consider alternative methods other than releasing model weights for achieving the same benefits, and alternatives other than withholding model weights for mitigating identified risks (Whittlestone and Ovadya, 2020[66]).\n\nBy focusing on marginal assessments, decision-makers can better gauge whether the advantages of openweight  models  outweigh  the  potential  downsides.  Addressing  marginal  risk  is  crucial  to  ensure  that interventions  are  appropriate  and  proportional  to  the  level  of  risk  involved  (Kapoor  et al.,  2024[65]). However, it's important to recognise that this is just one approach to risk assessment. Depending on the context,  other  baselines  may  be  more  appropriate.  A  key  concern  with  relying  solely  on  marginal comparisons is the  potential for  a  'boiling  frog'  effect,  where  overall  risk  tolerance  inc reases as each successive model is compared to an increasingly permissive baseline, especially as model capabilities evolve or usage patterns shift. A more holistic and adaptive risk framework is needed to ensure that AI development and deployment remains trustworthy.\n\n## 4. Conclusions\n\nReleasing foundation model weights has many benefits, such as allowing external evaluation, speeding up innovation, and spreading control over a potentially transformative technology. Open-source practices in  the  software  industry  have  shown  substantial  benefits,  distributing  influence  over  the  direction  of technological  innovation  and  facilitating  the  development  of  products  that  can  reach  new  audiences. However, releasing foundation model weights also presents potential for malicious use and unintended consequences,  such  as  cyberattacks,  sexual  abuse,  and  violation  of  intellectual  property  and  privacy rights. Because of the significant potential risks and benefits, foundation models need careful consideration when they are shared and used.\n\nOpening the weights of foundation models does not inherently result in malicious use, as these models can  have  both  positive  and  negative  impacts  depending  on  the  context  and  application.  To  better understand the trade-offs of open-weight models, it is important to evaluate each one in the context of their specific applications and compare these benefits and risks to those of existing tools. This approach helps identify any additional -or marginal -benefits and risks that may arise.\n\nWhile  initial  assessments  suggest  that  certain  risks  associated  with  open-weight  models,  such  as  the generation of malicious content, share similarities with existing digital tools, rapid advancements in AI -including  significantly  decreasing  compute  costs  (Hobbhahn  and  Besiroglu,  2022[67])  and  increasingly accessible  fine-tuning  techniques  (Yang  et al.,  2024[68]) -could  dramatically  lower  the  technical  and financial barriers for a broader range of actors, including those seeking to develop harmful applications or bypass security mechanisms.\n\nTherefore, developers should carefully weigh the decision to release model weights, considering the full range of marginal benefits and risks and the full range of options for achieving the benefits or mitigating the risks.\n\nWhile  this  report  examines  the  availability  of  open-weight  foundation  models,  future  research  could investigate the actual usage of these models and the underlying reasons for their application, particularly focusing  on  the  economic  implications  and  spillover  effects  associated  with  their  development  and deployment. Analysing who produces these models -ranging from large tech companies to academic institutions -and who uses them, including governments, startups and individual consumers, is crucial for understanding the dynamics of the evolving AI ecosystem.\n\n## References\n\n| Al-Kharusi, Y. et al. (2024), 'Open -Source Artificial Intelligence Privacy and Security: A Review', Computers , Vol. 13/12, p. 311, https://doi.org/10.3390/computers13120311.                                                                                                                                                   | [44]   |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|\n| Anandira, H. (2024), 'Meta Platforms under fire over open -source AI branding ', Mobile World Live , https://www.mobileworldlive.com/ai-cloud/meta-platforms-under-fire-over-open- source-ai-branding/.                                                                                                                           | [31]   |\n| AndrÃ©, C. et al. (2025), 'Developments in Artificial Intelligence markets: New indicators based on model characteristics, prices and providers: New indicators based on model characteristics, prices and providers' , OECD Artificial Intelligence Papers , No. 37, OECD Publishing, Paris, https://doi.org/10.1787/9302bf46-en. | [38]   |\n| Anthropic (2023), Claude 2 , Anthropc, https://www.anthropic.com/index/claude-2.                                                                                                                                                                                                                                                  | [26]   |\n| Assran, M. et al. (2023), Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture , https://doi.org/10.48550/arXiv.2301.08243v3.                                                                                                                                                                      | [28]   |\n| Bengio, Y. et al. (2025), 'International AI Safety Report (DSIT 2025/001, 2025)', https://www.gov.uk/government/publications/international-ai-safety-report-2025.                                                                                                                                                                 | [52]   |\n| Boiko et al. (2023), Emergent autonomous scientific research capabilities of large language models , http://arxiv.org/abs/2304.05332.                                                                                                                                                                                             | [51]   |\n| Bommasani et al. (2022), On the Opportunities and Risks of Foundation Models , http://arxiv.org/abs/2108.07258.                                                                                                                                                                                                                   | [36]   |\n| Bommasani, R. et al. (2023), Considerations for Governing Open Foundation Models , https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf.                                                                                                                                                     | [40]   |\n| Bran et al. (2023), ChemCrow: Augmenting large-language models with chemistry tools , http://arxiv.org/abs/2304.05376.                                                                                                                                                                                                            | [50]   |\n| Brockman, G. et al. (2023), Introducing ChatGPT and Whisper APIs , https://openai.com/blog/introducing-chatgpt-and-whisper-apis.                                                                                                                                                                                                  | [27]   |\n| Buchanan, B. (2020), A National Security Research Agenda for Cybersecurity and Artificial Intelligence , https://cset.georgetown.edu/publication/a-national-security-research-agenda- for-cybersecurity-and-artificial-intelligence/.                                                                                             | [56]   |\n\nï¼\n\n[44]\n\nï¼\n\n| Carlini, N. et al. (2022), 'Membership Inference Attacks From First Principles', 2022 IEEE Symposium on Security and Privacy (SP) , pp. 1897-1914, https://doi.org/10.1109/sp46214.2022.9833649.                              | [61]   |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|\n| Choose a License (2023), Licenses , Choose a License, https://choosealicense.com/licenses/.                                                                                                                                   | [12]   |\n| Commons, C. (2025), About CC Licenses: The CC License options , https://creativecommons.org/share-your-work/cclicenses/ (accessed on 10 May 2025).                                                                            | [33]   |\n| Cooper, A. et al. (2025), Extracting memorized pieces of (copyrighted) books from open- weight language models , arXiv, https://arxiv.org/abs/2505.12546.                                                                     | [54]   |\n| Creative Commons et al. (2023), Supporting Open Source and Open Science in the EU AI Act , https://creativecommons.org/2023/07/26/supporting-open-source-and-open-science- in-the-eu-ai-act/.                                 | [9]    |\n| DiBona, C., S. Ockman and M. Stone (eds.) (1999), The Open Source Definition , O'Reilly.                                                                                                                                      | [11]   |\n| Engler, A. (2021), How open-source software shapes AI policy , Brookings, https://www.brookings.edu/articles/how-open-source-software-shapes-ai-policy/.                                                                      | [2]    |\n| Engler, A. (2021), The EU's attempt to regulate open -source AI is counterproductive , Brookings, https://www.brookings.edu/blog/techtank/2022/08/24/the-eus-attempt-to- regulate-open-source-ai-is-counterproductive.        | [3]    |\n| Finley, K. (2011), How to Spot Openwashing , readwrite, https://readwrite.com/how_to_spot_openwashing/.                                                                                                                       | [15]   |\n| Fries et al. (2022), How Foundation Models Can Advance AI in Healthcare , Stanford HAI, https://hai.stanford.edu/news/how-foundation-models-can-advance-ai-healthcare.                                                        | [4]    |\n| Gans, J. (2025), Growth in AI Knowledge , National Bureau of Economic Research (NBER), https://www.nber.org/system/files/working_papers/w33907/w33907.pdf.                                                                    | [39]   |\n| Goldman, S. (2023), Hugging Face, GitHub and more unite to defend open source in EU AI legislation , VentureBeat, https://venturebeat.com/ai/hugging-face-github-and-more-unite- to-defend-open-source-in-eu-ai-legislation/. | [8]    |\n| Hendrycks, D. et al. (2022), Unsolved Problems in ML Safety , https://arxiv.org/abs/2109.13916 (accessed on 24 July 2023).                                                                                                    | [46]   |\n| Hobbhahn, M. and T. Besiroglu (2022), Trends in GPU Price-Performance , https://epochai.org/blog/trends-in-gpu-price-performance (accessed on 23 July 2023).                                                                  | [67]   |\n| Hoffmann, J. et al. (2022), 'Training Compute - Optimal Large Language Models', arXiv , https://arxiv.org/abs/2203.15556.                                                                                                     | [24]   |\n| Howard, J. (2023), AI Safety and the Age of Dislightenment , https://www.fast.ai/posts/2023- 11-07-dislightenment.html (accessed on 31 July 2023).                                                                            | [48]   |\n| HuggingFace, C. (2023), Supporting Open Source and Open Science in the EU AI Act , https://huggingface.co/blog/assets/eu_ai_act_oss/supporting_OS_in_the_AIAct.pdf.                                                           | [41]   |\n\n[61]\n\nï¼\n\n[29]\n\n| Inskeep, S. and O. Hampton (2023), Meta leans on 'wisdom of crowds' in AI model release , NPR, https://www.npr.org/2023/07/19/1188543421/metas-nick-clegg-on-the-companys- decision-to-offer-ai-tech-as-open-source-softwa.                                                                                                                                                  | [29]   |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|\n| Jones, E. (2023), What is a foundation model? , Ada Lovelace Institute, https://www.adalovelaceinstitute.org/resource/foundation-models-explainer/.                                                                                                                                                                                                                          | [37]   |\n| Kandpal, N. et al. (2024), 'User Inference Attacks on Large Language Models', https://arxiv.org/pdf/2310.09266.                                                                                                                                                                                                                                                              | [59]   |\n| Kapoor, S. et al. (2024), On the Societal Impact of Open Foundation Models , https://arxiv.org/pdf/2403.07918v1.                                                                                                                                                                                                                                                             | [65]   |\n| LAION.ai (2023), A Call to Protect Open-Source AI in Europe , https://laion.ai/notes/letter-to- the-eu-parliament (accessed on 21 September 2023).                                                                                                                                                                                                                           | [49]   |\n| Langenkamp et al. (2022), 'How Open Source Machine Learning Software Shapes AI', Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society , https://doi.org/10.1145/3514094.3534167.                                                                                                                                                                           | [1]    |\n| Liang, P. et al. (2022), The Time Is Now to Develop Community Norms for the Release of Foundation Models , Stanford CERM, https://crfm.stanford.edu/2022/05/17/community- norms.html.                                                                                                                                                                                        | [21]   |\n| LinuxFoundation (2024), Embracing the Future of AI with Open Source and Open Science Models , https://lfaidata.foundation/blog/2024/10/25/embracing-the-future-of-ai-with-open- source-and-open-science-models/ (accessed on 21 January 2025).                                                                                                                               | [14]   |\n| LinuxFoundation (2024), Introducing the Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI , https://lfaidata.foundation/blog/2024/04/17/introducing-the-model-openness-framework- promoting-completeness-and-openness-for-reproducibility-transparency-and-usability-in-ai/ (accessed on 21 January 2025). | [18]   |\n| Maffulli, S. (2023), Meta's LLaMa 2 license is not Open Source , open source initiative, https://opensource.org/blog/metas-llama-2-license-is-not-open-source.                                                                                                                                                                                                               | [17]   |\n| Marr, B. (2023), Digital Twins, Generative AI, And The Metaverse , Forbes, https://www.forbes.com/sites/bernardmarr/2023/05/23/digital-twins-generative-ai-and-the- metaverse.                                                                                                                                                                                               | [6]    |\n| Marr, B. (2023), The Amazing Ways Duolingo Is Using AI And GPT-4 , Forbes, https://www.forbes.com/sites/bernardmarr/2023/04/28/the-amazing-ways-duolingo-is- using-ai-and-gpt-4/.                                                                                                                                                                                            | [7]    |\n| Meta (2023), Introducing LLaMA: A foundational, 65-billion-parameter large language model , https://ai.facebook.com/blog/large-language-model-llama-meta-ai/.                                                                                                                                                                                                                | [22]   |\n| Milmo, D. (2023), Nick Clegg defends release of open-source AI model by Meta , The Guardian, https://www.theguardian.com/technology/2023/jul/19/nick-clegg-defends- release-open-source-ai-model-meta-facebook.                                                                                                                                                              | [30]   |\n| Mirsky, Y. et al. (2021), The Threat of Offensive AI to Organizations , http://arxiv.org/abs/2106.15764 (accessed on 19 September 2023).                                                                                                                                                                                                                                     | [55]   |\n\n[63]\n\n| Mouton, C., C. Lucas and E. Guest (2024), 'The Operational Risks of AI in Large -Scale Biological Attacks: Results from a Red- Team Study', RAND Research Report , https://www.rand.org/pubs/research_reports/RRA2977-2.html.   | [63]   |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|\n| Nasr, M. et al. (2023), 'Scalable Extraction of Training Data from (Production) Language Models', https://arxiv.org/abs/2311.17035.                                                                                             | [60]   |\n| OECD (2023), 'AI language models: Technological, socio-economic and policy considerations' , OECD Digital Economy Papers , No. 352, OECD Publishing, Paris, https://doi.org/10.1787/13d38f92-en.                                | [35]   |\n| OECD (2022), 'OECD Framework for the Classification of AI systems' , OECD Digital Economy Papers , No. 323, OECD Publishing, Paris, https://doi.org/10.1787/cb6d9eca-en.                                                        | [34]   |\n| OpenAI (2023), Fine-tuning: learn how to customize a model for your application , https://platform.openai.com.                                                                                                                  | [47]   |\n| OpenAI (2023), GPT- 4 is OpenAI's most advanced system, producing safer and more useful responses' , OpenAI, https://openai.com/product/gpt-4.                                                                                  | [25]   |\n| OpenAI (2023), Inworld AI , https://openai.com/customer-stories/inworld-ai.                                                                                                                                                     | [5]    |\n| OSI (2025), The Open Source AI Definition - draft v. 0.0.9 , https://opensource.org/ai/drafts/open-source-ai-definition-draft-v-0-0-9 (accessed on                                                                              | [13]   |\n| 21 January 2025). OSI (2025), The Open Source Definition , https://opensource.org/osd/ (accessed on                                                                                                                             | [32]   |\n| Peppin, A. et al. (2025), The Reality of AI and Biorisk , https://arxiv.org/pdf/2412.01946v3.                                                                                                                                   | [64]   |\n| Sastry, G. (2023), Beyond 'Release' vs. 'Not Release' , Stanford CERM, https://crfm.stanford.edu/commentary/2021/10/18/sastry.html.                                                                                             | [20]   |\n| Shevlane, T. and A. Dafoe (2020), The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse? , http://arxiv.org/abs/2001.00463 (accessed on 7 December 2022).                              | [57]   |\n| Sijbrandij, S. (2023), AI weights are not open 'source' , https://opencoreventures.com/blog/2023-06-27-ai-weights-are-not-open-source/.                                                                                         | [16]   |\n| Solaiman, I. (2023), The Gradient of Generative AI Release: Methods and Considerations , http://arxiv.org/abs/2302.04844.                                                                                                       | [19]   |\n| The White House (2025), America's AI Action Plan , https://www.whitehouse.gov/wp- content/uploads/2025/07/Americas-AI-Action-Plan.pdf.                                                                                          | [42]   |\n| Thiel, D. (2023), 'Identifying and Eliminating CSAM in Generative ML Training Data and Models', Stanford Digital Repository , https://doi.org/10.25740/kh752sm9123.                                                             | [45]   |\n| Thiel, D., M. Stroebel and R. Portnoff (2023), 'Generative ML and CSAM: Implications and Mitigations', Stanford Digital Repository , https://purl.stanford.edu/jv206yg3793.                                                     | [58]   |\n| Urbina, F. et al. (2022), 'Dual use of artificial -intelligence- powered drug discovery', Nature Machine Intelligence , Vol. 4/3, pp. 189-191, https://doi.org/10.1038/s42256-022-00465-9.                                      | [10]   |\n\n| Vincent, J. (2023), Meta's powerful AI language model has leaked o nline - what happens now? , The Verge, https://www.theverge.com/2023/3/8/23629362/meta-ai-language-model- llama-leak-online-misuse.   | [23]   |\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|\n| Whittlestone, J. and A. Ovadya (2020), The tension between openness and prudence in AI research , http://arxiv.org/abs/1910.01170 (accessed on 24 July 2023).                                            | [66]   |\n| Yang, M. et al. (2024), Low-Rank Adaptation for Foundation Models: A Comprehensive Review , arXiv, https://arxiv.org/abs/2501.00365.                                                                     | [68]   |\n| Yong, X., G. Shi and P. Zhang (2025), 'Towards Agentic AI Networking in 6G : A Generative Foundation Model-as- Agent Approach', IEEE Communications Magazine , https://arxiv.org/abs/2503.15764.         | [62]   |\n| Zheng, Y., Y. Chen and Q. Bin (2024), A Review on Edge Large Language Models: : Design, Execution, and Applications. , https://arxiv.org/pdf/2410.11845v1.                                               | [43]   |\n| Zou, A. et al. (2023), Universal and Transferable Adversarial Attacks on Aligned Language Models , http://arxiv.org/abs/2307.15043 (accessed on 2 August 2023).                                          | [53]   |", "fetched_at_utc": "2026-02-09T13:31:17Z", "sha256": "31d8c2826e761290272197204f4c5872ab26709089ecaf783c11d0a29963b289", "meta": {"file_name": "AI Openness - A Primer For Policymakers - OECD.pdf", "file_size": 2863069, "mtime": 1770641284, "docling_errors": []}}
{"doc_id": "pdf-pdfs-ai-risk-management-singapore-506923eb3328", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Risk Management - Singapore.pdf", "title": "AI Risk Management - Singapore", "text": "<!-- image -->\n\nInformation Paper December 2024\n\n## ARTIFICIAL INTELLIGENCE MODEL RISK MANAGEMENT\n\nOBSERVATIONS FROM A THEMATIC REVIEW\n\n<!-- image -->\n\n## Contents\n\n| 1.                          | Overview                                     |   3 |\n|-----------------------------|----------------------------------------------|-----|\n| 2.                          | Background                                   |   3 |\n| 3.                          | Objectives and Key Focus Areas               |   7 |\n| 4.                          | Governance and Oversight                     |  10 |\n| 5.                          | Key Risk Management Systems and Processes    |  12 |\n| 5.1                         | Identification                               |  12 |\n| 5.2                         | Inventory                                    |  13 |\n| 5.3                         | Risk Materiality Assessment                  |  15 |\n| 6                           | Development and Deployment                   |  16 |\n| 6.1                         | Standards and Processes                      |  16 |\n| 6.2                         | Data Management                              |  18 |\n| 6.3                         | Development                                  |  21 |\n| 6.4                         | Validation                                   |  30 |\n| 6.5                         | Deployment, Monitoring and Change Management |  31 |\n| 7                           | Other Key Areas                              |  36 |\n| 7.1                         | Generative AI                                |  36 |\n| 7.2                         | Third-Party AI                               |  43 |\n| 8                           | Conclusion                                   |  44 |\n| Annex A - Definitions       | Annex A - Definitions                        |  46 |\n| Annex B - Useful References | Annex B - Useful References                  |  49 |\n\n<!-- image -->\n\n## 1. Overview\n\n- 1.1 This information paper sets out good practices relating to Artificial Intelligence (AI)  (including  Generative  AI) 1 model  risk  management  (MRM)   2 that  were observed during a recent thematic review of selected banks. The information paper  focuses  on  the  following  key  areas 3 :  AI  governance  and  oversight;  AI identification,  inventorisation  and  risk  materiality  assessment;  as  well  as  AI development, validation, deployment, monitoring and change management.\n- 1.2 While  the  thematic  review  focused  on  selected  banks,  the  good  practices highlighted in this information paper should generally apply to other financial institutions (FIs), which should take reference from these when developing and deploying AI.\n\n## 2. Background\n\nIndustry use of AI and Generative AI and associated risks\n\n- 2.1 The  launch  of  ChatGPT  in  November  2022  and  recent  advancements  in  AI, particularly Generative AI, has led to an increased interest in leveraging AI and Generative  AI  in  the  banking  and  broader  financial  sector.  Prior  to  these developments, FIs have used AI in a wide range of areas and use cases. Key areas where we observed significant use of AI by banks during the thematic review include  risk  management,  customer  engagement  and  servicing,  as  well  as  to\n\n1 Generative AI is a subset of AI, and an AI or Generative AI system can comprise one or more AI or Generative AI models and other machine-based components. For the purposes of this paper, AI generally refers to both AI and Generative AI models and systems. Where a point pertains specifically to an AI model or an AI system, or to Generative AI, we will use the respective terms explicitly in the paper. We define the terms AI and Generative AI, as well as AI model, system and use case in greater detail in Annex A.\n\n2 In line with the footnote above and recognising that the AI MRM is intrinsically linked to the risk management of systems in which AI models are used,  when we refer to AI  MRM or AI risk management in this paper, it  generally  refers to the risk management of AI models and systems.\n\n3 The aim of this information paper is not to cover all aspects of model risk management, but to focus on good practices in areas that are more relevant to AI MRM.\n\n<!-- image -->\n\nsupport internal operational processes. For example, we have seen banks use AI, particularly decision tree-based machine learning (ML) models such as XGBoost, LightGBM  and  CatBoost 4 ,  in  financial  risk  management  to  detect  abnormal financial market movements, or to estimate loan prepayment rates. They are also commonly used in anti-money laundering (AML) systems to detect suspicious transactions,  and  in  fraud  detection  systems.  In  customer  engagement  and servicing,  banks  use  AI  to  predict  customer  preferences,  personalise  financial product  recommendations  and  manage  customer  feedback.  AI  is  also  widely used to support internal operational processes across a wide range of business functions, for example, to automate checking and verification processes (e.g., for customer information), prioritise incident management (e.g., triaging IT incidents for attention), or forecast demand for services (e.g., ATM cash withdrawals).\n\n- 2.2 While the use of AI in these areas can enhance operational efficiency, facilitate risk  management  and  enhance  financial  services,  they  can  also  increase  risk exposure if not developed or deployed responsibly. Potential risks include:\n- Financial risks , e.g., poor accuracy of AI used for risk management could lead to poor risk assessments and consequent financial losses.\n- Operational risks , e.g., unexpected behaviour of AI used to automate financial operations could lead to operational disruptions or errors in critical processes.\n- Regulatory risks ,  e.g., poor performance of AI used to support AML efforts could lead to non-compliance with regulations.\n- Reputational risks ,  e.g.,  wrong or inappropriate information from AI-based customer-facing systems, such as chatbots, could lead to customer complaints and negative media attention, and consequent reputational damage.\n\n4 Decision tree-based ML models make predictions based on a tree-like structure learnt from data. Models such as XGBoost, LightGBM and CatBoost utilise a series of decision trees together with a boosting technique. Each decision tree in the series focuses on the errors made by a prior decision tree to improve predictions. Such models are also explainable as the relative importance of different features to model predictions can be extracted.\n\n<!-- image -->\n\n- 2.3 While natural language processing (NLP) 5 and computer vision (CV) 6 techniques were already in use in the financial sector prior to the emergence of Generative AI 7 for text or image-related tasks, recent Generative AI models such as OpenAI's GPT 8 large  language  models  (LLMs)  and  DALL-E 9 image generation models, or Anthropic's Claude LLMs 10 offer better performance in tasks such as clustering documents. They have also enabled new use cases, e.g., to generate text content and images for marketing, or to process multimodal data 11 for financial analysis.\n- 2.4 Based on the thematic review, use of Generative AI in banks appears to still be at an  early  stage.  The  current  focus  is  on  the  use  of  Generative  AI  to  assist  or augment humans for productivity enhancements, and not in applying Generative AI  to  direct  customer  facing  applications.  Use  cases  being  explored  by  banks include  risk  management  (e.g.,  detecting  emerging  risks  in  text  information); customer engagement and service (e.g., summarising customer interactions or generating  marketing  content);  and  research  and  reporting  (e.g.,  investment analyses).  Banks  are  also  exploring  the  use  of  Generative  AI  in  copilots 12 to support staff, for example, in coding, or for general text-related tasks such as summarisation and answering queries based on information in internal knowledge repositories.\n- 2.5 With  Generative  AI,  existing  risks  associated  with  AI  may  be  amplified 13 .  For example, Generative AI's potential for hallucinations and unpredictable\n\n5 Natural  language  processing  (NLP)  is  commonly  used  to  refer  to  techniques  that  process,  analyse,  make  predictions  or generate outputs relating to human language, both in its written and spoken forms.\n\n6 Computer vision (CV) is commonly used to refer to techniques that enable machines to process and generate outputs based on visual information from the world.\n\n7 For  example,  for  news  sentiment  analysis,  information  extraction,  clustering  documents  based  on  underlying  topics,  or digitising physical documents.\n\n8 Generative Pre-trained Transformers (GPT) are a family of Generative AI models developed by OpenAI, that includes models such GPT 4 and GPT-4o.\n\n9 DALL-E is a Generative AI model that generates images from text prompts or descriptions.\n\n10 Claude models are a family of Generative AI models developed by Anthropic and include models such as Claude 3.5 Haiku and Sonnet.\n\n11 Multimodal data refers to datasets that comprise multiple types of data, e.g., text, images, audio or video.\n\n12 In the context of Generative AI, the term copilot is typically used to refer to Generative AI being used to assist or augment humans on specific tasks.\n\n13 More details on risks associated with Generative AI have already been covered extensively in Project MindForge 's white paper on ' Emerging Risks and Opportunities of Generative AI for Banks ' and will not be repeated in this information paper. The whitepaper can be accessed at https://www.mas.gov.sg/schemes-and-initiatives/project-mindforge.\n\n<!-- image -->\n\nbehaviours may pose significant risks if Generative AI is used in mission-critical areas. The  complexity  of  Generative  AI  models  and  lack  of established explainability techniques also creates challenges for understanding and explaining decisions, while the diverse and often opaque data sources used in Generative AI training, coupled with difficulties in evaluating bias of Generative AI outputs, could lead to unfair decisions.\n\n## MAS' Efforts on Responsible AI for the Financial Sector\n\n- 2.6 Alongside the growing use of AI in the financial sector and such associated risks, MAS  had  established  key  principles  to  guide  financial  institutions  in  their responsible use of AI.\n- 2.7 In  2018,  MAS  co-created  the principles of  Fairness,  Ethics,  Accountability  and Transparency (FEAT) with the financial industry to promote the deployment of AI and  data  analytics  in  a  responsible  manner.  To  provide  guidance  to  FIs  in implementing FEAT, MAS started working with an industry consortium on the Veritas Initiative 14 in November 2019. The Veritas Initiative aimed to support FIs in incorporating the FEAT Principles into their AI and data analytics solutions, and has  released  assessment  methodologies,  a  toolkit,  and  accompanying  case studies.\n- 2.8 With the emergence of Generative AI, Project MindForge 15 , which is also driven by the Veritas Initiative, was established to examine the risks and opportunities of  Generative  AI.  The  first  phase  of  Project  MindForge  was  supported  by  a consortium  of  banks  and  released  a  risk  framework  for  Generative  AI  in November 2023.\n- 2.9 More recently, MAS released an information paper relating to Generative AI risks in July 2024 16 . The paper provides an overview of key cyber threats arising from Generative AI, the risk implications, and mitigation measures that FIs could take\n\n14 See https://www.mas.gov.sg/schemes-and-initiatives/veritas\n\n15 See https://www.mas.gov.sg/schemes-and-initiatives/project-mindforge\n\n16 See https://www.mas.gov.sg/regulation/circulars/cyber-risks-associated-with-generative-artificial-intelligence\n\n<!-- image -->\n\nto address such risks. The paper covered areas enabled by Generative AI, such as deepfakes, phishing and malware, as well as threats to deployed Generative AI, such as data leakage and model manipulation.\n\n## 3. Objectives and Key Focus Areas\n\n- 3.1 This information paper, which focuses on AI MRM, is part of MAS' incremental efforts to ensure responsible use of AI in the financial sector. A key difference between an AI-based system and other systems is the use of one or more AI models within the system, which potentially increases uncertainties in outcomes. Robust MRM of such AI models is important to support the responsible use of AI.\n- 3.2 As  the  maturity  of  AI  MRM  may  vary  significantly  across  different  FIs,  MAS conducted a thematic review of selected banks' AI MRM practices in mid-2024. The objective was to gather good practices for sharing across the industry.\n- 3.3 Based on information gathered during the review, MAS observed good practices by banks in these key focus areas 17 :\n\n## Â· Section 4: Oversight and Governance of AI\n\n- -Updating of existing policies and procedures of relevant risk management functions to strengthen AI governance;\n- -Establishing cross-functional oversight forums to ensure that evolving AI risks are appropriately managed across the bank;\n- -Articulating clear statements and principles to govern areas such as the fair, ethical, accountable and transparent use of AI; and\n\n17 For the purposes of the subsequent parts of this information paper, the good practices relating to AI  would also apply to Generative AI as practicable. Specific considerations relating to Generative AI will be covered in Section 7.1.\n\n<!-- image -->\n\n- -Building capabilities in AI across the bank to support both innovation and risk management.\n\n## Â· Section 5: Key Risk Management Systems and Processes\n\n- -Identifying AI usage and risks across the bank so that commensurate risk management can be applied;\n- -Utilising AI inventories, which provide a central view of AI usage across the bank to support oversight; and\n- -Assessing the materiality of risks that AI poses using key risk dimensions so that relevant controls can be applied proportionately.\n\n## Â· Section 6: Development and Deployment of AI\n\n- -Establishing standards and processes for key areas that are important for the  development  of  AI,  such  as  data  management,  robustness  and stability, explainability and fairness, reproducibility and auditability;\n- -Conducting  independent  validation  or  peer  reviews 18 of AI  before deployment based on risk materialities; and\n- -Instituting  pre-deployment  checks  and  monitoring  of  deployed  AI  to ensure that it behaves as intended, and application of appropriate change management standards and processes where necessary.\n\n18 The terms validations and reviews are usually used interchangeably by banks to refer to assessments or checks of the AI model development process, whether by an independent party, or another peer developer. More details on such validations and reviews are provided in Section 6.4.\n\nOverview of Key Thematic Focus Areas\n\n<!-- image -->\n\n<!-- image -->\n\n- 3.4 These key focus areas are generally also applicable to Generative AI, as well as AI (including Generative AI) from third-party providers. Nonetheless, there may be additional  considerations  for  Generative  AI,  as  well  as  AI  from  third-party providers. Hence, additional observations on good practices relating to Generative AI and third-party AI are also outlined in Sections 7.1 and 7.2 of this information paper respectively.\n- 3.5 The risks posed by AI and Generative AI extend beyond MRM and relate to nonAI specific areas such as general data governance and management, technology and cyber risk management, as well as third party risk management. These are not covered in this information paper, and existing regulatory requirements and supervisory  expectations,  including  but  not  limited  to  notices,  guidelines  or information  papers  on  data  governance,  technology  and  outsourcing  risk management would apply, where relevant 19 .\n\n19 Links to relevant publications are provided in Annex B.\n\n<!-- image -->\n\n## 4. Governance and Oversight\n\n## Overview\n\nWhile existing control functions continue to play key roles in AI risk management, most banks have updated governance structures, roles and responsibilities, as well as policies and processes to address AI risks and keep pace with AI developments. Good practices include:\n\n- establishing cross-functional oversight forums to avoid gaps in AI risk management;\n- updating control standards, policies and procedures, and clearly setting out roles and responsibilities to address AI risks;\n- developing clear statements and guidelines to govern areas such as fair, ethical, accountable and transparent use of AI across the bank; and\n- building capabilities  in  AI  across  the  bank  to  support both  innovation  and risk management.\n\nExisting governance structures and such good practices are important to help support Board and Senior Management in exercising oversight over the bank's use of AI, and ensure that the bank's risk management is robust and commensurate with its state of use of AI.\n\n- 4.1 While  existing  risk  governance  frameworks  and  structures 20 continue  to  be relevant  for  AI  governance  and  risk  management,  a  number  of  banks  have established  cross-functional  AI  oversight  forums.  Such  forums  serve  as  key platforms for coordinating governance and oversight of AI usage across various functions. They also play an important role in addressing emerging challenges and potential gaps in risk management as the AI landscape evolves, and ensuring that standards and processes, such as relevant AI development and deployment standards, are aligned across the bank.\n\n20 Aside  from  MRM,  risk  governance  frameworks  and  structures  from  other  areas  that  are  usually  relevant  to  AI  risk management include (but are not limited to) data, technology and cyber, third-party risk management, as well as legal and compliance.\n\n<!-- image -->\n\n- 4.2 The  mandates  of  these  forums  often  include  establishing  a  consistent  and comprehensive  framework  for  managing  AI  risks,  evaluating  use  cases  that require broader cross-functional inputs, and reviewing AI governance requirements to ensure they keep pace with the state of AI usage in the bank. Data and analytics, risk management, legal and compliance, technology, audit, as well as other relevant business and corporate functions, are typically represented at such cross-functional oversight forums.\n- 4.3 A number of banks have also found value in compiling policies and procedures that are relevant to AI into a central guide to ensure that consistent standards for AI are applied across the bank. As more AI use cases are rolled out in banks, and the state of AI technology evolves, the use of AI may accentuate existing risks or introduce new risks. Hence, most banks have reviewed and, where necessary, updated existing policies and procedures to keep pace with the increasing use of AI  across  the  bank,  or  new  AI  developments,  e.g.,  updating  policies  and procedures  relating  to  performance  testing  of  AI  for  new  use  cases,  or establishing  new  policies  and  procedures  for  AI  models  that  are  dynamically updated based on new data.\n- 4.4 Given the broad range of use cases for AI, and the potential for inappropriate use,  most  banks  have  set  out  central  statements  and  principles  on  how  they intend to use AI responsibly, including developing guidelines to govern areas such as  fair,  ethical,  accountable,  and  transparent  use  of  AI 21 .  Such  efforts  are important in setting the tone and establishing clear guidance on how AI should be  used  appropriately  across  the  bank,  and  to  prevent  potential  harms  to consumers  and  other  stakeholders  arising  from  the  use  of  AI.  In  addition  to central  statements  and  principles,  some  banks  have  also  taken  steps  to operationalise such central statements and principles by mapping them to key\n\n21 More details on these areas can be found in MAS ' publications relating to the FEAT principles under the Veritas Initiative. Similar principles covering areas relating to the responsible or ethical use of AI in the financial sector have also been published in other jurisdictions , e.g., the Hong Kong Monetary Authority (HKMA) issued guiding principles for the use of big data analytics and AI covering governance and accountability, fairness, transparency and disclosure, and data privacy and protection in 2019; De  Nederlandsche  Bank  (DNB)  issued  the  SAFEST  principles  on  soundness,  accountability,  fairness,  ethics,  skills,  and transparency in 2019.\n\n<!-- image -->\n\ncontrols,  which  are  in  turn  mapped  to  the  relevant  functions  responsible  for these controls.\n\n- 4.5 Given the growing interest in AI, banks also recognised the need to develop AI capabilities  and  have  established  plans  to  upskill  both  their  staff  and  senior executives. Aside from building awareness, banks have developed AI training that facilitate staff in leveraging and using AI in an effective and responsible manner. Some  banks  have  also  set  up  AI  Centres  of  Excellence  to  drive  innovation, promote best practices and build AI capabilities across the bank.\n\n## 5. Key Risk Management Systems and Processes\n\n## Overview\n\nMost  banks  have  recognised  the  need  to  establish  or  update  key  risk  management systems and processes for AI, particularly in the following areas:\n\n- policies and procedures for identifying AI usage and risks across the bank, so that commensurate risk management can be applied;\n- systems  and  processes  to  ensure  the  completeness  of  AI  inventories,  which capture the approved scope of use and provide a central view of AI usage to support oversight; and\n- assessment of the risk materiality of AI that covers key risk dimensions, such as AI's  impact on the bank and stakeholders, the complexity of AI used, and the bank's reliance on AI, so that relevant controls can be applied proportionately.\n\n## 5.1 Identification\n\n- 5.1.1 Identifying where AI is used is important so that the relevant governance and risk management  controls  can  be  applied.  Even  when  using  widely  accepted definitions, such as the Organisation for Economic Co-operation and\n\n<!-- image -->\n\nDevelopment's definition  of  AI 22 ,  considerable  ambiguity  remains  around  the definition of AI due to its broad and evolving scope.\n\n- 5.1.2 Most banks leveraged definitions in existing MRM policies and procedures as a foundation for identifying AI models 23 , and extended or adapted these definitions to account for AI-specific characteristics. Some banks shared that the uncertainty of  model  outputs  is  a  common  source  of  risk  for  both  AI  and  conventional models 24 , and that the presence of such uncertainties was a key feature that was usually considered when identifying AI. MRM control functions also typically play a  key  role  in  AI  identification,  often  serving  as  the  key  control  function responsible for AI identification systems  and  processes, e.g., setting  up attestation processes, or acting as the final arbiter in determining whether AI is being used. Some banks have also developed tools or portals to facilitate the process of identifying and classifying AI across the bank in a consistent manner.\n\n## 5.2 Inventory\n\n- 5.2.1 Banks mostly maintain a formal AI inventory 25 with a comprehensive record of where AI is used in the bank. A key area that an AI inventory supports, alongside the relevant policies, procedures and systems, is to ensure that AI are only used within the scope in which they have been approved for use, e.g., the purpose, jurisdiction, use case, application, system, and other conditions for which they have been developed, validated and deployed. This is critical because unapproved  usage  of  AI,  particularly  in  higher-risk  use  cases,  can  lead  to\n\n22 The OECD's definition of AI: An AI system is a machine -based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.\n\n23 This could entail step-bystep guide to facilitate the identification of techniques that meet the bank's definition of AI.\n\n24 Models usually refer to quantitative algorithms, methods or techniques that process input data into quantitative estimates which may be used for analysis or decision making. Apart from AI models, which typically refer to machine or deep learning models, banks also routinely utilise conventional models, such as economic, financial, or statistical models. Some quantitative algorithms, methods or techniques, such as logistic regressions, are commonly regarded as both AI and statistical models. A more detailed definition of models can be found in Annex A.\n\n25 Most banks have established software systems for their AI inventories that not only record where AI is used in the bank, but may also include additional features outlined above, such as automated tracking of approvals and issues, and identification of inter-dependences between AI. A small number of banks still rely on spreadsheets for their AI inventories, but this approach is more prone to operational issues, e.g., outdated records, and would not allow for the additional features outlined above.\n\n<!-- image -->\n\nunintended consequences. For example, AI approved for use in one jurisdiction should not automatically be treated as approved for use in others as the data, assumptions and considerations may not be similar, and the AI may not perform as expected in a different context.\n\n- 5.2.2 A few banks also utilised their AI inventory system to track the use of AI through their  lifecycle,  and  to  establish  checkpoints  for  different  risk  management processes at the various stages of the AI lifecycle. A few banks also used the AI inventory to support the identification and monitoring of aggregate AI risks and interdependencies across different AI models and systems. The AI inventory may also serve as a central repository for AI artifacts needed for model maintenance, validation and incident or issue management.\n- 5.2.3 Most  banks  have  established  clear  policies  on  the  scope  of  AI  assets  to  be inventoried,  the  roles  responsible  for  maintaining  the  inventory,  and  the processes for updating it. AI models are typically included within regular model inventories but specific tags or fields added to identify AI and capture AI-relevant attributes. One bank built an AI use case inventory that aggregated information from the AI model inventory and other inventories or repositories relating to assets and controls in areas such as data, technology and operational management. This provided the bank with a comprehensive and clear view of the linkages between AI models and other relevant assets and controls.\n- 5.2.4 Across  banks,  AI  inventories  generally  capture  key  attributes  such  as  the AI's purpose and description, scope of use, jurisdiction, model type, model output 26 , upstream and downstream dependencies, model status, risk materiality rating, approvals obtained for validation and deployment, responsible AI requirements, waiver or dispensation details 27 , use of personally identifiable information (PII) 28 , personnel  responsible such as owners,  sponsors, users, developers,  and validators. For third-party AI, additional attributes such as the AI provider, model\n\n26 Model output refers to the type of output generated by the AI model. For example, the model output attribute could be the likelihood of customer attrition, or the credit score of a customer.\n\n27 Waiver  or  dispensation  details  refer  to  information  about  exceptions/special  permissions  granted,  regarding  the development or deployment of AI, that deviate from the bank's standard policies and procedures.\n\n28 For example, full name, national identification number, personal mobile number.\n\n<!-- image -->\n\nversion, endpoints utilised, as well as other details from the AI developers 29 may also be included.\n\n## 5.3 Risk Materiality Assessment\n\n- 5.3.1. Risk materiality assessments are critical for banks to calibrate their approach to risk management of AI across the diverse areas in which AI can be used (e.g., to map the risk materiality of AI to the depth and scope of validation and monitoring required). In assessing risk materiality, most banks considered both quantitative and qualitative risk dimensions that could generally be grouped into three broad categories:\n- a. Impact on the bank, its customers or other stakeholders, including but not limited to financial, operational, regulatory and reputational impact. A few banks developed granular, function-specific definitions of impact to provide greater clarity.\n- b. Complexity due to the nature of the AI model or system, or the novelty of the area or use case in which AI is being applied.\n- c. Reliance on AI, which takes into account the autonomy granted to the AI, or the involvement of humans in the loop as risk mitigants.\n- 5.3.2        Most  banks  have  also  established  processes  to  review  that  risk  materialities assigned  to  AI  remain  appropriate  over  time.  Similarly,  quantitative  and qualitative  measures  and  methods  used to  assign  risk  materialities  were  also reviewed, e.g., measures used to quantify financial impact would be updated if the nature of the business in which AI was used had evolved.\n\n29 These may be provided in AI or AI model cards, which are documents or information usually released alongside open-source AI models that facilitate transparency and accountability by providing essential information on key areas such as the AI model's purpose, performance, limitations, ethical considerations. More information on details that may be included in such cards are available in papers such as https://link.springer.com/chapter/10.1007/978-3-031-68024-3\\_3 .\n\n<!-- image -->\n\n## 6 Development and Deployment\n\n## Overview\n\nMost banks have established standards and processes for development, validation, and deployment of AI to address key risks.\n\n- For development of AI, key areas that banks paid greater attention to include data management,  model  selection,  robustness  and  stability,  explainability  and fairness, as well as reproducibility and auditability.\n- For validation, banks required independent validations or reviews of AI of higher risk materiality prior to deployment, to ensure that development and deployment standards  have  been  adhered  to.  For  AI  of  lower  risk  materiality,  most  banks conducted peer reviews that are calibrated to the risks posed by the use of AI prior to deployment.\n- To ensure that AI would behave as intended when deployed and that any data and model drifts are detected and addressed, banks performed pre-deployment checks, closely monitored deployed AI based on appropriate metrics, and applied appropriate change management standards and processes.\n\n## 6.1 Standards and Processes\n\n- 6.1.1. To  support  robust  risk  management  of  AI  across  its  lifecycle,  banks  have established standards and processes in the key areas of development, validation, deployment,  monitoring  and  change  management.  Most  banks  built  upon existing MRM standards and processes for development, validation, deployment, monitoring  and  change  management,  but  updated  these  standards  and processes to address risks posed by AI.\n- 6.1.2. Key  standards  and  processes  relating  to  conventional  model  development, validation,  deployment,  monitoring  and  change  management  that  banks\n\n<!-- image -->\n\ngenerally regard as relevant to AI are listed below 30 . Observations on key areas of focus for AI, and how banks have adapted or updated these standards and processes in these areas to address AI risks will be outlined in the subsequent sections.\n\n- a. Data management -Determining suitability of data, such as the representativeness  of data for the  intended  objective,  assessment  of completeness, reliability, quality, and relevance of data, and approaches for determining training and testing datasets.\n- b. Model selection - Defining the intended objective of the model and justifying how the selection and design of the model is relevant and appropriate for achieving the desired objective, including the selection of architectures 31 and techniques 32 that are appropriate for the use case and objective.\n- c. Performance  evaluation -  Setting  appropriate  evaluation  approaches  and thresholds,  and  assessing  the  model's  ability  to  perform  under  a  range  of conditions in accordance with its intended usage and objective.\n- d. Documentation - Providing sufficient detail to facilitate reproducibility by an independent party, including details on data sources, lineage, and processing steps; model architecture and techniques; evaluation and testing approaches and results.\n\n30 As highlighted previously, even prior to the use of AI models, banks already utilised conventional models, such as economic, financial, or statistical models, and would have instituted model risk management standards and processes for such models. While  these  standards  and  processes  may  have  preceded  the  use  of  AI  models  in  the  bank,  their  general  principles  and considerations may also be applicable to AI models.\n\n31 Model architecture, in the context of AI, relates to the underlying structure and design of the model. It could involve choosing between decision tree-based models such as XGBoost, which were previously described in Section 2, or neural network-based models such as recurrent neural network or transformer models, based on various considerations. For example, decision treebased models may be more suitable for structured data, such as tabular data, while recurrent neural network or transformer models may be more suitable for text or time-series data as they are designed for sequential data.\n\n32 Techniques may include methods that are used to train a model from the data. In the context of AI, these may include supervised learning techniques that use labelled data during training to learn how to generate predictions, or unsupervised learning  techniques  which  learn  general  patterns  from  unlabelled  data.  For  more  details  on  supervised  and  unsupervised learning, please refer to Annex A.\n\n<!-- image -->\n\n- e. Validation - Setting out the depth of review expected of validators across the areas above; frameworks for determining the prioritisation and frequency of validation (including any revalidation conducted on deployed models).\n- f. Mitigating  model  limitations -  Frameworks  and  processes  for  testing  key assumptions, identifying limitations and their expected impact, and establishing appropriate mitigants which are commensurate with the impact of the limitations.\n- g. Monitoring  and  change  management -Setting  appropriate  tests  and thresholds  to  evaluate  the  ongoing  performance  of  a  deployed  model, including the frequency of monitoring; as well as the processes to be followed (e.g., additional validations and approvals) for changes made to a deployed model.\n4. 6.1.3. When implementing standards and processes for risk management of AI, most banks established baseline standards and processes that applied to all AI across the  bank,  regardless  of  risk  materiality.  For  AI  that  were  of  greater  risk materiality, or where there were requirements specific to the use case, baseline standards and processes would be supplemented by enhanced standards and processes. For example, additional evaluation or enhanced validation standards and processes could apply to AI used for risk and regulatory use cases where there may be heightened requirements on performance evaluation or thresholds. The alignment of baseline standards and processes across the bank helped  ensure  that  key  model  risks  were  addressed  consistently  for  AI  with similar characteristics and risks regardless of where they were used in the bank.\n\n## 6.2 Data Management\n\n- 6.2.1 Robust  data management  is  essential to  support  the  development  and deployment  of  AI.  General  bank-wide  data  governance  and  management\n\n<!-- image -->\n\nstandards and processes 33 would apply to data used for AI. For example, whether data  was  used  for  reporting  purposes  or  for  AI  systems,  the  same  data governance committees generally oversee approvals and management of data issues. Similarly, standards and processes for key data management controls such as basic data quality checks would also apply. However, to address AI-specific requirements, all banks had established additional data management standards and processes to ensure that data used for AI development and deployment are fit for purpose. An overview of key data management areas for AI development and  deployment  that  most  banks  generally  focused  on  are  listed  below. Standards  or  processes  relating  to  data  management  that  are  specific  to  AI development, validation, deployment, monitoring or change management are covered in the subsequent sections.\n\n- a. Appropriateness  of  data  for AI  use  cases -Ensuring  data used  for development and deployment of AI are suitable for the context in which the AI is used, including assessing the use of such data against fairness and ethical considerations.\n- b. Representativeness  of  data  for  development -  Ensuring  data  selected  for training and testing AI models are representative of the real-world conditions, including stressed conditions, under which the AI would be used.\n- c. Robust  data  engineering  during  development -  Ensuring  data  processing steps, 34 which  may  include  additional  data  quality  checks 35 ,  feature\n\n33 Please  see MAS' information  paper  on  Data  Governance  and  Management  Practices  for  more  details  on  general  data governance and management standards and processes. The paper covered governance and oversight, data management function, data quality and data issues management, which would also apply to data used for AI. Other relevant regulations and publications include the Personal Data Protection Act (PDPA), which comprises various requirements on data privacy governing the collection, use, disclosure and care of personal data, and provides a baseline standard of protection for personal data in Singapore;  and  Advisory  Guidelines  on  Use  of  Personal  Data  in  AI  Recommendation  and  Decision  Systems  issued  by  the Personal Data Protection Commission (PDPC) in March 2024. Please refer to Annex B for the relevant links.\n\n34 Examples of data processing steps include missing value imputation, replacement of outlier values and standardisation or normalisation of data values.\n\n35 To ensure data quality, key areas such as data relevance, accuracy, completeness and recency may be assessed.\n\n<!-- image -->\n\nengineering 36 , augmentation and labelling 37 of datasets, are robust and free of bias, and that the integrity and lineage of data are checked and tracked across these data engineering steps.\n\n- d. Robust data pipelines for deployment - Establishing robust controls around data pipelines for deployment, including continuous monitoring of the quality of data passed to deployed AI, as well as checks for anomalies, drifts, and potential bias that may have an impact on performance or fairness.\n- e. Documentation of data-related aspects for reproducibility and auditability -Ensuring key data management steps, such as data sourcing, data selection, data lineage, data processing, approvals and remediation actions taken for data issues are documented to enable reproducibility and auditability.\n3. 6.2.2 Some banks have also established additional data management standards and processes in the areas below:\n- a. To ensure that data is being used appropriately when developing or deploying AI, a few banks have required approvals to be obtained for high-risk data use cases, such as data use where a third party may have access to the bank's internal  data,  use  of  employee  data  for  monitoring,  or  the  collection  of biometric data to identify individuals.\n\n36 Features refer to the attributes of data points in a dataset, e.g., for data relating to a loan, the income of the obligor and outstanding value of the loan are two possible attributes or features. Feature engineering refers to the process of selecting, modifying or creating new features from the original attributes of a  dataset to improve an AI model's performance ,  e.g., normalising income of the obligor and outstanding value of the loan to a common scale ranging from 0 to 1; or creating new derived features, such as a debt-to-income ratio, from existing attributes.\n\n37 When training AI models for a specific task, such as predicting a credit default or recommending a suitable financial product to a customer, we need data that includes the input variables (e.g., data relating to a past loan, or customer history), as well as a target variable (e.g., whether there was a credit default for the loan, or a recommendation that the customer accepted). Data labelling refers to the process of assigning such target variables,  typically based on past historical data or via human annotation.\n\n<!-- image -->\n\n- b. To  support  data  reusability  and  reduce  the  time  needed  for  feature engineering across the bank, as well as enhance consistency and accuracy in model development, a few banks have also built feature marts 38 .\n- c. To account for the greater use of unstructured data 39 , there were also ongoing efforts to more effectively manage such unstructured data, such as improving metadata management and tagging for unstructured data to enable better data governance 40 . Most of the data management areas outlined in paragraph 6.2.1 are also generally applicable to unstructured data, where relevant.\n\n## 6.3 Development\n\n## Model Selection\n\n- 6.3.1 Given  the  trade-offs  of  adopting  more  complex  AI  models  (e.g.,  higher uncertainties, limited explainability), most banks required developers to justify their  selection  of  a  more  complex  AI  model  over  a  conventional  model  or  a simpler  AI  model 41 , (e.g., balancing the  need  for  performance  against explainability  for  a  specific  use-case).  Some  banks  required  developers  to  go beyond qualitative justifications, and develop challenger models (which could be either conventional  or simpler AI  models)  to  explicitly  demonstrate  the performance uplift of the AI model over the challenger model as part of this justification.\n\n38 A feature mart is a centralised repository or database that stores curated, pre-processed and reusable features (variables or attributes) that can be used for training models. Aside from supporting data reusability, feature marts may also help improve data  governance  by  maintaining  metadata  on  each  feature,  including  details  on  its  sources,  transformations,  lineage  and quality. Feature marts may also allow for version control, ensuring that any updates to features are tracked.\n\n39 Unstructured data refers to information that does not follow a predefined format or organised structure, making it  more difficult to store and analyse using traditional databases or methods for structured data. Unstructured data typically includes data  types  such  as  text,  images,  videos,  and  audio.  While  the  use  of  unstructured  data  is  not  new  to  banks,  e.g.,  using surveillance videos from cameras at ATMs, the use of such data is growing due to Generative AI.\n\n40 These  may also include updating and adapting  other areas such as data discovery and classification, access  rights, data lifecycle management, data sanitisation and validation, and security controls for unstructured data.\n\n41  For example, a developer who wishes to use a more complex neural network-based deep learning model may be required to justify the need for such an AI model over a simpler tree-based machine learning model or a logistic regression model, and consider the trade-offs based on the use case requirements.\n\n<!-- image -->\n\n## Robustness and Stability\n\n- 6.3.2 In  assessing  the  overall  suitability  of  AI  models,  banks  placed  heavy  focus  on ensuring  that  AI  models  were  both  robust  and  stable 42 ,  and  accordingly  paid significant  attention  to  i)  the  selection  and  processing  of  datasets  used  for training and testing AI models; ii) determining appropriate approaches, measures and thresholds for evaluating AI models; and iii) mitigating overfitting risks 43 that often arise due to the complexity of AI models. We outline some of the practices in these key areas below.\n\nSelection and Processing of Datasets for Training and Testing\n\n- 6.3.3 Datasets  chosen  for  training  and  testing  or  evaluation 44 of  AI  models  were expected to be representative of the full range of input values and environments under which the AI model was intended to be used. Training and testing datasets were  also  checked  to  ensure  that  their  distributions  or  characteristics  are similar 45 .\n- 6.3.4 Most  banks  also  invested  efforts  in  collecting  testing  datasets  that  allowed predictions or outputs from AI models to be tested or evaluated in the bank's context as far  as  possible.  For  example,  curating  datasets  that  allowed  for  AI model generated answers to queries from customers to be compared against answers  from  in-house  human  experts,  or  getting  actual  feedback  from  the bank's customers on the quality of these AI model generated answers.\n\nEvaluation Approaches, Measures and Thresholds\n\n42 The concepts of robustness and stability in AI systems often overlap and what these terms cover can vary. For the purpose of  this  information  paper,  robustness  refers  to AI's ability  to  achieve  its  desired  level  of  performance  under  real-world conditions, while stability refers to the consistent performance of AI across a representative range of real-world scenarios. These concepts are also related to the reliability of the AI system or model.\n\n43 Overfitting is when an AI model learns the training data overly well, to the point where it performs extremely well on training data but very poorly on new data that it has not seen in the training dataset. Intuitively, this may mean that the model has memorised the training examples rather than learning general patterns, resulting in poor performance in real-world conditions. 44 The terms ' testing ' and ' evaluation ' of AI models are commonly used interchangeably to refer to the assessment of the performance of AI models on datasets that it had not been trained on.\n\n45 This issue is also commonly referred to as training-testing skew, which are discrepancies between the distribution of data used to train an AI model and the distribution of data it encounters during testing.\n\n<!-- image -->\n\n- 6.3.5 Given that AI is developed to meet specific business needs or objectives, banks' standards and processes on the robustness and stability of AI models generally required  testing  or  evaluation  approaches  to  be  aligned  with  the  intended outcomes  that  the  AI  models  were  meant  to  support.  The  exact  approaches selected could differ depending on the nature of the AI models, as well as the needs of the use case. For example, assessing a fraud detection model's ability to flag out known fraud cases by comparing against ground truth in historical data, or the usefulness of a financial product recommendation model through human feedback.\n- 6.3.6 Correspondingly, while there are many established performance measures for AI models 46 , banks paid significant attention to aligning the choice of performance measures  with  the  intended  outcomes  that  the  AI  models  were  meant  to support. In some  cases,  this could involve trade-offs between  different performance measures. For example, if the intended outcome was to detect as many instances of fraud as possible, performance measurement would need to focus more on the proportion of false negatives (i.e. fraudulent instances that were  not  detected),  even  though  this  may  come  at  the  expense  of  a  higher proportion of false positives (i.e. instances falsely flagged by the model as being fraudulent).\n- 6.3.7 Other tests that banks may include to ensure robustness and stability 47 include the following:\n- a. Sensitivity analysis to understand how predictions or outputs of AI models change under different permutations of data inputs. This also helps to identify important  features  that  significantly  influence  predictions  or  outputs,  and facilitate explanations of the behaviour of AI models.\n\n46 There are a wide range of performance measures for AI models, and these are often specific to the task or use-case; for example, recall, precision, or F1 for classification tasks, mean absolute error or root mean squared error for regression tasks, mean average precision or mean reciprocal rank for recommendation tasks.\n\n47 The objectives of some of these tests overlap, and may also relate to data management aspects that we outlined earlier. Nonetheless, we list all the tests that we observed across the banks for completeness.\n\n<!-- image -->\n\n- b. Stability analysis to compare the stability of data distributions and predictions or outputs, e.g., assessing whether the distribution of a training dataset from an  earlier  period  matches  the  distribution  of  testing  datasets  from  more recent periods, and how differences affect the performance of AI models.\n- c. Sub-population analysis ,  which  are  evaluations  of how  AI models perform across  different  sub-populations  or  subsets  within  the  datasets  (e.g.,  to identify any significant differences in performance  between  different customer segments). Such analysis of sub-populations or subsets within the datasets help to identify potential issues that might not be obvious in the aggregated testing dataset, as well as potential sources of bias, which could support fairness assessments of AI models where necessary (e.g., where subpopulations relate to protected features or attributes such as race or gender).\n- d. Error  analysis to  identify  potential  patterns  in  prediction  errors  (e.g., misclassified  instances),  which  helps  to  understand  the  limitations  of  AI models.\n- e. Stress testing the response of AI models to edge cases or inputs outside the typical  range  of  values  used  in  training.  This  allowed  banks  to  better determine  performance  boundaries  and  identify  limitations  of  AI  models. Some  banks  also  tested  the  behaviour  of  AI  models  in  the  context  of unexpected inputs or conditions. Examples included adversarial testing or red teaming types of exercises. Such testing is especially important in the context of AI models used in high risk or customer-facing applications, as it allowed the bank to establish conditions under which AI models would not perform as expected or could introduce potential security or ethical concerns.\n5. 6.3.8 Most banks would establish criteria or thresholds for performance measures, to define what was considered acceptable performance. Such thresholds need to be  clearly  defined  and  documented,  as  well  as  mutually  agreed  upon  by developers and validators. Such thresholds were usually use case specific, and could also be used subsequently to facilitate validation, pre-deployment checks, as well as monitoring and change management.\n\n<!-- image -->\n\n## Mitigating Overfitting Risks\n\n- 6.3.9 The large number of parameters and inherent complexity of AI models increases the  risks  of  them  overfitting  on  training  data  (in-sample  data)  and  hence performing  poorly  when  deployed  on  out-of-sample  data.  Banks  employed  a variety of mitigants to address this risk:\n- a. Model selection -Generally favouring AI models of lower complexity unless there are clear justifications to do otherwise; or adopting approaches that constrained the complexity of AI models 48 .\n- b. Feature selection - Applying explainability methods to identify the key input features  or  attributes  that  are  important  for  the  AI  model  predictions  or outputs 49 and assessing that they are intuitive from a business and/or user perspective 50 .\n- c. Model evaluation - Additional performance testing requirements to test the performance  of  AI  models  on  unseen  data  where  possible,  such  as  crossvalidation techniques 51 and testing against more out-of-sample/out-of-time 52 datasets.\n\n## Explainability\n\n- 6.3.10 All banks identified explainability as a key area of focus for AI, particularly for use cases  where  end-users  or  customers  need  to  understand  key  features  or attributes  in  the  data  influencing  predictions  of  AI  models.  For  example, explainability would be more important in higher risk materiality use cases where\n\n48 Examples include regularisation techniques or limiting the number and depth of trees for gradient boosting trees. Such techniques generally try to limit the number of parameters used so that the trained model is less complex. For example, some regularisation techniques force less important parameters to values of zero.\n\n49 As discussed in the next section on explainability methods.\n\n50 Additional justification would typically be required to retain features or attributes that were not intuitive, or which did not meaningfully contribute to the overall performance of the models. Such data may introduce more noise, and cause the AI model to overfit on the noise, leading to poor performance in real world conditions.\n\n51 Cross-validation generally refers to techniques to evaluate models by resampling the dataset for training and testing. An example would be K-fold cross-validation (which involves splitting the dataset into K parts for K training and testing rounds).\n\n52 An out-of-sample testing dataset is a subset of data not used in model training, whereas an out-of-time testing dataset is a subset of data obtained from a time period distinct from the time period of the subset of data used in training the model.\n\n<!-- image -->\n\nbank  staff  making  decisions  based  on  predictions  of  AI  models  need  to understand the key features or attributes 53 contributing to the prediction; or in use cases where a customer may ask for reasons for being denied a financial service.  Hence,  development  standards  for  AI  across  all  banks  had  been expanded to include a section on explainability.\n\n- 6.3.11 Explainability requirements in banks' standards and processes generally required developers to apply global and/or local 54 explainability methods to identify the key  features  or  attributes  used  as  inputs  to  AI  models  and  their  relative importance; assess whether these features or attributes were intuitive from a business  and/or  user  perspective;  and  provide  additional  justification  for retaining features or attributes which were not intuitive. Such methods could also help identify the usage of potentially sensitive features as part of fairness assessments.  Some  banks  had  set  out  a  list  of  global  and  local  explainability methods  that  could  be  applied  to  explain  the  outputs 55 of  AI  models.  Such methods could be directly applied during development as part of the feature selection process, or used within explainability tools developed as part of the AI system so that either global and/or local explanations can be provided alongside predictions or outputs generated by AI models post-deployment.\n- 6.3.12 In terms of the level of explainability required for different use cases, some banks established standards and processes to clearly define the minimum level of global and/or  local  explainability  required  for  different  use  cases.  For  these  banks,\n\n53 An example of a feature or attribute in this context could be the income of the customer.\n\n54 Global explainability is the ability to understand the overall functioning of the model by identifying how input features drive model outputs at an overall model level. Local explainability is the ability to identify how input features drive the model output for a specific observation or instance. Taking a fraud detection model as an example, global explainability methods allow for identification of the most important features, such as the high values of transactions, used to detect fraudulent transactions for the model in general. However, the key features that are important for a specific transaction (i.e. the local instance) may not necessarily be the same, e.g., the value of the transaction may be small for a specific instance but the transaction is still detected  as  a  fraudulent  transaction  due  to  specific  characteristics  of  the  parties  involved  in  the  transaction,  such  as  an unfamiliar geographic location of one of the parties. Local explainability methods help to identify such features for the local instance.\n\n55 Common  examples  of  explainability  methods  include  SHAP  (for  global  and  local  explainability)  and  LIME  (for  local explainability). SHAP generates Shapley values for each feature based on its contribution to a given model output. A globallevel explanation can be generated by generating a summary plot of the Shapley values of the key features, across the entire set  of  model  outputs.  LIME  is  based  on  training  a  separate  model  for  the  local  instance  that  needs  to  be  explained.  The explanation that is generated is based on the separately trained model.\n\n<!-- image -->\n\nfactors  considered  when  applying  a  higher  standard  of  global  and/or  local explainability included risk materiality or the extent to which AI-driven decisions were likely to require explanations (e.g., to the bank's customers) for  the  use case. For example, AI models used for credit decisioning could require the most exacting  standards  for  global  and  local  explainability,  requiring  developers  to carefully consider all features used as inputs and provide justifications for their use, as well as the ability for users to easily identify key features influencing any given  prediction  post-deployment.  Other  banks  required  global  and/or  local explainability to be explored across all AI, but allowed users and owners to decide on the acceptable level of explainability, and justify their decision based on the use case.\n\n## Fairness\n\n- 6.3.13 The outputs of AI models are inherently influenced by the patterns learnt from its training data. If the training data contained biases that unfairly represent or disadvantage  specific  groups  of  individuals,  AI  models  may  perpetuate  these unfair  biases  in  its  predictions  or  outputs.  This  could  lead  to  decisions  or recommendations that disproportionately and unfairly impact certain demographic groups.\n- 6.3.14 The earlier section on data management had outlined the need for fairness to be considered  during  development,  and  for  checks  and  monitoring  of  potential biases  during  deployment.  More  specifically,  during  AI  development,  for  use cases  that  could  have  a  significant  impact  on  individuals,  most  banks  would undertake a formal assessment on whether specific groups of individuals could be  systematically  disadvantaged  by  AI-driven  decisions.  The  scope  of  such assessments  could  vary  between  banks  depending  on  the  relevant  rules, regulations  or  expectations  applicable  to  the  bank 56 ,  and  between  use  cases depending on the risk materiality of the AI.\n\n56 Examples of such expectations on fairness for AI used by banks across jurisdictions include the Principles to Promote Fairness, Ethics, Accountability and Transparency (FEAT) in the use of Artificial Intelligence and Data Analytics in Singapore's Financial Sector, published by MAS in 2018; General Principles for the use of Artificial Intelligence in the Financial Sector, published by\n\n<!-- image -->\n\n- 6.3.15 Generally,  the  approach  for  assessing  fairness  used  by  banks  involved  the following steps:\n- a. Defining  a  list  of  protected  features  or  attributes,  for  which  use  of  such features  or  attributes  in  AI  models  would  require  additional  analysis  and justification.  Common  examples  of  such  protected  features  or  attributes include gender, race or age.\n- b. Determining whether such features or attributes 57 were used in training AI models. Based on this assessment, to define groups of individuals at risk of being systematically disadvantaged by the AI-driven decisions (at-risk groups).\n- c. Where  necessary,  determining  the  extent  to  which  AI-driven  decisions systematically disadvantaged against at-risk groups. The was usually assessed via fairness measures (e.g., fairness measures that are available in the toolkit released by the Veritas Initiative).\n- d. Where necessary, providing adequate justifications on the use of protected features  or  attributes  in  AI  models  (e.g.,  trade-offs  against  the  intended objectives of the AI model 58 ).\n\n## Reproducibility and Auditability\n\n- 6.3.16 Reproducibility and auditability 59 of  AI  development are essential for ensuring accountability and building trust in AI systems. To facilitate reproducibility and auditability of AI, most banks expanded existing documentation requirements to incorporate the relevant AI development processes and considerations. A list of\n\nDe Nederlandsche Bank in 2019; and the High-level Principles on Artificial Intelligence, published by the Hong Kong Monetary Authority in 2019.\n\n57 These could include proxy attributes that are heavily correlated with such protected attributes.\n\n58 This could be supported by, for example, analysis on the difference in performance between an AI model which included these protected features or attributes, and an AI model which did not. An informed assessment could then be made on whether this difference in performance was necessary to achieving the model's intended objective, taking into consideration the level of potential harm done to at-risk groups arising from the use of the AI model.\n\n59 Reproducibility refers to ' the ability of an independent verification team to produce the same results using the same AI method based on the documentation made by the organisation ', while audibility refers to 'the readiness of an AI system to undergo an assessment of its algorithms, data and design processes ' ( Model AI Governance Framework, IMDA Singapore.)\n\n<!-- image -->\n\nkey  documentation  requirements  for  AI  commonly  seen  across  banks  are  as follows:\n\n- a. Data - Documentation of key data management steps is important to facilitate reproducibility  and  auditability.  During  development,  key  information  that would usually  be  documented  include  datasets  and  data  sources  used  for model  development  and  evaluation,  details  of  how  these  datasets  were assessed as fit-for-purpose, processed ahead of model training, and split into relevant training, testing and/or validation 60 datasets.\n- b. Model training - Details of how the AI model was trained or fit to the training dataset. Such details could include codes (along with software packages/environment used and their relevant versions), key settings (e.g., hyperparameters 61 used and the approach for selecting hyperparameters 62 ), random seed values 63 and any other configurations required for a third party to reproduce the training process.\n- c. Model  selection -  Details  of  how  the  performance  of  the  AI  model  was evaluated and how the final model was selected. Such details could include the  evaluation  approaches,  thresholds  and  datasets  applied 64 and  the corresponding results, comparisons of performance across multiple AI models and justifications for selecting the final model.\n- d. Explainability -  Global  and/or  local  explainability  methods  used,  feature selection process, analysis of results, as well as description of key features selected and additional justifications for inclusion of certain key features (e.g., features that may not have appeared to be important to a human expert).\n\n60 Testing and validation datasets refer to datasets used to evaluate the performance of the model outside of the dataset used to train the model. This should be distinguished from independent validation, which is the process of independently assessing the overall suitability of the model.\n\n61 E.g., number of trees and maximum tree depth for gradient boosted trees.\n\n62 E.g., grid search, random search of hyperparameters.\n\n63 AI models usually need to be initialised with a random set of numbers (e.g., for the model parameters) before training, and documenting the random seed value that is used to initialise the AI models is necessary to reproduce the AI model's behaviour and results.\n\n64 As detailed in the earlier sub-section on Robustness &amp; Stability.\n\n<!-- image -->\n\n- e. Fairness - Metrics and associated thresholds, results of fairness assessments and justifications for the use of any protected features or attributes.\n2. 6.3.17 Alongside documentation requirements in the relevant standards and processes, most banks also set up documentation templates that developers were required to follow for consistency . Such templates were typically designed by the bank's MRM function. Templates could differ between business domains (as different performance  tests  or  metrics  could  apply)  or  between  AI  of  different  risk materialities (as documentation requirements could be higher for AI of higher risk materiality).\n\n## 6.4 Validation\n\n- 6.4.1 Independent validation provides an objective and unbiased assessment of the suitability, performance and limitations of AI. It acts as an important challenge to developers, and ensures that the relevant standards and processes have been adhered to when developing AI.\n- 6.4.2 The validation process typically involves an independent unit 65 reviewing the AI development  process  and  documentation,  assessing  that  AI  performs  and behaves  as  intended,  and  undertaking  pre-deployment  checks.  Actions  to address issues identified during validation, such as the application of suitable adjustments or other mitigating or compensatory controls, would typically be proposed by developers and agreed to by validators before deploying AI.\n- 6.4.3 Building on their conventional MRM  processes, banks have equipped independent  validation  functions  with  the  skills  and  incentives  needed  to conduct independent review of AI used in the bank, which include investments in efforts to ensure that independent validation staff have the relevant technical expertise for AI.\n\n65 For example, t he Federal Reserve/Office of the Comptroller of the Currency's SR Letter 11 -7 on Supervisory Guidance on Model Risk Management states that validation should generally be done by individuals not responsible for development or use and do not have a stake in whether a model is determined to be valid.\n\n<!-- image -->\n\n- 6.4.4 Banks  adopted  a  range  of  approaches  in  establishing  independent  validation requirements  across  different  AI.  One  bank  required  all  AI  to  be  subject  to independent validation, with the depth and rigour of validation varying based on the AI's risk materiality rating. Most other banks required independent validation only for AI of higher risk materiality, with other AI subject only to peer review 66 . Even for AI of lower risk materiality, the involvement of either an independent validator or peer reviewer allowed for some degree of challenge that helped to better manage the added uncertainties and risks posed by AI, and check that such AI was developed in accordance with the bank's standards and processes.\n\n## 6.5 Deployment, Monitoring and Change Management\n\n## Pre-Deployment Checks\n\n- 6.5.1. Aside  from  checks  during  the  validation  process,  pre-deployment  checks  and tests are important to ensure that the AI has been correctly implemented and produces  the  intended  results  before  being  deployed  for  use.  Banks  placed significant focus on implementing controls for the deployment of AI to ensure that the AI functions as intended in the production environment 67 . These controls were  usually  based  on  existing  technology  risk  management  guidelines.  For example,  banks  would  apply  standard  software  development  lifecycle  (SDLC) processes to ensure that the AI application or system was secure, free from error and performed as intended before deployment 68 .  Some banks also conducted additional checks to ensure that the deployed AI 's scope, output and performance, and associated controls align with that of the validated AI:\n- a. Additional tests , such as:\n\n66 As  compared  to  independent  validation,  peer  reviews  were  usually  conducted  by  a  non-independent  function  (e.g.,  a different development team in the same unit/reporting line as the original model developers).\n\n67 A production environment is a live operational setting where deployed systems, such as deployed AI models, are run under real world conditions to deliver services or perform tasks for end-users.\n\n68 Please see MAS' Technology Risk Management Guidelines for further details on the adoption of sound and robust practices for  the  management  of  technology  risk  in  these  areas:  https://www.mas.gov.sg/regulation/guidelines/technology-riskmanagement-guidelines\n\n<!-- image -->\n\n- i. forward  testing ,  which  are  experimental  runs  using  a  limited  set  of production data or with a limited set of users, for selected high materiality use cases to assess the behaviour of AI in an environment similar to when the AI is fully deployed; and\n- ii. live  edge  case  testing to  assess  how  AI  handles  edge  cases  in  the production environment, which helps to verify that AI can handle a variety of improbable but plausible scenarios when deployed.\n- b. Automated  pipelines ,  such  as  setting  up  automated  deployment  and continuous integration/continuous deployment (CI/CD) pipelines 69 to minimise human error and maintaining a  consistent process  for  how  AI  is deployed, monitored, and maintained, which is important for AI given the need for regular data and model updates.\n- c. Process management, which includes checks to ensure that key processes important for the deployed AI, such as human oversight, backup models, and other  appropriate  controls  and  contingencies,  are  in  place;  and  business process  change  management,  such  as  training  users  to  understand  AI limitations and to use AI appropriately.\n5. 6.5.2. Non-AI  specific  pre-deployment  checks 70 remain  relevant,  hence  key  control functions, such as those in the areas of technology, data, legal and compliance, third-party  and  outsourcing,  would  also  confirm  that  the  checks  have  been undertaken and sign off before AI is deployed into production.\n\n69 Continuous integration/continuous deployment (CI/CD) pipelines automate the process of building, testing, and deploying code changes, and reduce the potential of errors arising from manual interventions. Approvals and checks are also usually integrated into the CI/CD process to ensure that new code pushed into production are checked for errors. More details on CI/CD, as well as other related terms such as MLOps and AIOps are provided in Annex A.\n\n70 For example, checks relating to cyber-security, or compliance with outsourcing policies.\n\n<!-- image -->\n\n## Monitoring Metrics and Thresholds\n\n- 6.5.3. Monitoring  is  particularly  critical  for  AI  given  their  dynamic  nature  and  the potential  for  AI  model  staleness  due  to  drifts 71 in  either  data  or  the  model behaviour over time. All banks paid significant focus to the ongoing monitoring of their AI to ensure that they continue to operate as intended post-deployment. Key measures that were monitored generally follow those that were covered during  development  and  validation,  and  include  robustness,  stability,  data quality, and fairness measures.\n- 6.5.4. Measures  used  for  monitoring  were  tracked  against  predefined  thresholds, usually determined at the development and validation stages, to ensure models perform  within  acceptable  boundaries.  Some  banks  have  also  implemented tiered thresholds, for example, additional early warning thresholds to pre-empt model deterioration, and different thresholds to determine when retraining or a full redevelopment of the AI may be necessary.\n- 6.5.5. Most banks also have a process or system for reporting, tracking and resolving issues or incidents if breaches or anomalies arise from the monitoring process. Banks  generally  track  issues  or  incidents  from  discovery  to  resolution,  and incorporate a relevant escalation process based on the materiality of the issue or incident. The resolution process may include AI model retraining, redevelopment,  or  decommissioning  as  possible  outcomes.  Where  a  major redevelopment  was  undertaken,  revalidation  and  approval  would  be  needed before the updated model could be redeployed.\n\n71 AI models can perform poorly when they become stale due to factors such as data drift, concept drift or model drift, which are essentially due to changes in the data distributions, relationships between input data and predictions/outputs, or the general environment in which the AI model is being used. More details on data, concept and model drifts are provided in Annex A.\n\n<!-- image -->\n\n## Contingency Plans\n\n- 6.5.6. All banks would generally have standards and processes relating to contingency plans for AI, particularly those supporting high-risk or critical functions 72 . These plans, which may not be specific to AI, typically outline fallback options, such as alternative  systems  or  manual  processes,  and  would  be  subject  to  regular reviews and testing to ensure readiness for rapid activation when necessary. For mission-critical AI applications 73 , a few banks may also have kill switches in place. Kill switches are used to deactivate AI if they exceed risk tolerances, and require clear contingency plans to be quickly rolled out.\n\n## Review and Revalidations\n\n- 6.5.7. Aside from ongoing monitoring, banks also conducted periodic reviews of their portfolio of AI.  Key aspects that that were usually reviewed include changes in the model s' materiality, risks, scope and usage, performance, assumptions and limitations, and identification and remediation of issues.\n- 6.5.8. Banks  also  have  standards  and  processes  for  ongoing  revalidations  of  AI  in production, with the intensity and frequency based on the materiality of the AI. In  general,  AI  deemed  critical  to  risk  management,  regulatory  compliance, business operations, or customer outcomes are revalidated more frequently and intensely.\n\n## Change Management\n\n- 6.5.9. Standards  and  processes  relating  to  AI  change  management  are  needed  to ensure that what constitutes a change is clearly defined, and that the appropriate development  and  validation  requirements  are  applied.  Most  banks  required\n\n72 Such contingency plans may not apply specifically to AI, but to technology systems in general. Nonetheless, they may require additional considerations in the case of AI, e.g., AI-specific performance monitoring thresholds to determine when to trigger the contingency plan, or a backup plan that involves another AI system or model.\n\n73 For example, for AI that are used for trading.\n\n<!-- image -->\n\nsignificant or material changes 74 to AI in production to be reviewed and approved by  the  control  functions  prior  to  implementation,  so  as  to  ensure  that  any modifications made to the model do not negatively impact its performance. To manage changes to AI, banks have also established systems and processes for version control of both internal and third-party AI (which do not only cover code relating to AI, but also data and other artifacts such as hyperparameters and the trained model parameters or weights). Version control enables banks to track changes across different aspects of AI and roll-back to previous versions of AI where necessary 75 .  Most  banks  have  also  set  up  processes  for  third-party  AI providers to provide notifications of version updates 76 .\n\n6.5.10. AI for certain use cases, such as fraud detection, may need to be changed or updated more frequently 77 , due to drifts in the data or the behaviour of the AI model  over  time.  To  deal  with  such  frequent  changes,  some  banks  have established systems and processes for the automatic updating of such AI. Such AI, which some banks refer to as ' dynamic AI ' , need to be subject to enhanced requirements and controls to ensure that change management is well governed. Key  additional  requirements  and  controls  include  justifications  for  enabling automatic updating of AI, clearly defining what can be updated automatically, for example,  restricting  changes  to  the  retraining  of  AI  model  with  more  recent datasets, but not allowing for changes to AI model architectures or hyperparameters.  Such  dynamic  AI  would  also  be  subject  to  enhanced  risk\n\n74 Examples of significant or material changes include fundamental changes to AI model architectures or training techniques. Such changes may necessitate an in-depth revalidation, compared to less significant changes, such as retraining the AI model with more recent data, which may only require checks on AI performance to ensure the AI is still behaving as expected.\n\n75 While we cover version control here under change management where the AI is already deployed, it is important to note that version control for AI also plays a key role during the development and validation stages. For example, version controls are needed to support iterative improvements and collaboration during development, and also help to ensure reproducibility and auditability during validation.\n\n76 While banks generally try to require third-party providers to notify them of any changes to the AI model or service, there may be circumstances where such notifications may not happen, e.g., the third-party provider may not notify end-users on changes that they view as immaterial. We have observed banks trying to address this by setting out clearer terms in their legal agreements, for example, adding a clause that requires the third-party provider to notify banks on any upcoming changes to the AI model or system.\n\n77 For  example,  if  we  compare  a  fraud  detection  use  case  with  an  NLP  use  case  such  as  summarisation  of  customer  call transcripts, data relating to the behaviour of scammers would  usually change much more frequently than data relating to customer calls due to the active efforts of scammers to evade detection.\n\n<!-- image -->\n\nmanagement requirements, such as enhanced data management standards, e.g., additional checks on data quality and drifts, as well as enhanced performance monitoring requirements, e.g., more stringent monitoring notification thresholds.\n\n## 7 Other Key Areas\n\n## 7.1 Generative AI\n\n## Overview\n\nWhile the use of Generative AI in banks is still in the early stages, banks generally try to apply existing governance and risk management structures and processes where relevant and practicable, and balance innovation and risk management by adopting:\n\n- Strategies and approaches, where they leverage on the general-purpose nature of Generative AI by focusing on the development of key enabling modules or services; limit  the  current  scope  of  Generative  AI  to  use  cases  for  assisting/augmenting humans or improving internal operational efficiencies that are not direct customer facing; and building capacity and capabilities by establishing pilot and experimentation frameworks;\n- Process  controls,  such  as  setting  up  cross-functional  risk  control  checks  at  key stages of the Generative AI lifecycle; establishing more detailed development and validation guidelines for different Generative AI task archetypes; requiring human oversight for Generative AI decisions; and paying close attention to user education and training on the limitations of Generative AI tools; and\n- Technical  controls,  such  as  selection,  testing  and  evaluation  of  Generative  AI models in the context of the bank's use cases; developing reusable modules to facilitate testing and evaluation; assessing different aspects of Generative AI model performance and risks; establishing input and output filters as guardrails to address toxicity,  bias  and  privacy issues; and mitigating data security risks via measures such as the use of private clouds or on-premise servers, data loss prevention tools, and limiting the access of Generative AI to more sensitive information.\n\n<!-- image -->\n\n- 7.1.1. In  addition  to  the  key  areas  highlighted  in  the  prior  sections,  there  are  some aspects  relating  to  Generative  AI  (compared  to  conventional  AI)  that  require further consideration:\n- a. Higher uncertainties associated with Generative AI -The risks of hallucinations and unexpected behaviours by Generative AI given its greater complexity may lead to less robust and stable performance, and was a key concern highlighted by banks. This concern was particularly pronounced for use cases of higher risk materiality or those that are directly customer-facing, where greater reliability was required.\n- b. Difficulties in evaluating/testing Generative AI and mitigating its limitations\n- -Compared  to  conventional  AI,  which  were  typically  used  by  banks  for specific use cases that the AI models had been trained for, Generative AI are more general-purpose in nature and can be used in a wider range of use cases in the bank. However, there may be a lack of easily available ground truths 78 in  some of these newer use cases to evaluate and test Generative AI. Use cases involving Generative AI also typically involve unstructured data, such as text  data,  for  which  there  are  significantly  more  possible  permutations, compared to structured data usually used for conventional AI. This makes it challenging  to  foresee  all  potential  scenarios  and  perform  comprehensive testing and evaluations 79 .\n- c. Lack of transparency from Generative AI providers - Unlike conventional AI models,  which  are  often  developed  and trained  internally  by  the  bank's developers, Generative AI used by banks were pre-dominantly based on pretrained models from external providers. As disclosure standards relating to such AI are still evolving globally, banks may lack full access to essential risk\n\n78 Ground truth refers to reliable or factual information that serves as a standard against which the outputs or predictions of AI models, including Generative AI models, can be evaluated.\n\n79 For  example, it  is  significantly  harder  to  evaluate  the  quality  of  a  summary  or  of  an  image  generated  by  Generative  AI, compared to evaluating the accuracy of a simple yes/no prediction from conventional AI. It is also harder to foresee all possible permutations of text or images that may be used as inputs to Generative AI, as well as all possible permutations of text or images that may be generated by Generative AI.\n\n<!-- image -->\n\nmanagement information, such as details about the underlying data used in model  training  and  testing,  as  well  as  the  extent  of  evaluation  or  testing applied to these models.\n\n- d. Challenges in explainability and fairness  with Generative AI -The  lack  of transparency from external providers may also contribute to challenges in understanding and explaining the outputs  and behaviour of Generative AI, and ensuring that the outputs generated by Generative AI are fair. There is also a general lack of established methods currently for explaining Generative AI outputs and assessing their fairness.\n2. 7.1.2. Most banks are in the process of reviewing and updating parts of their AI model risk management framework for Generative AI to balance the benefits and risks of its use.\n3. 7.1.3. The  subsequent  paragraphs  outline  observations  from  the  thematic  on  key approaches and controls that banks have adopted to balance innovation and risks based on the current state of use of Generative AI. It should be noted that these approaches and controls will need to be updated as Generative AI technology evolves,  and  that  risk  management  efforts  will  need  to  be  scaled  accordingly based on the state of Generative AI use across the institution.\n\n## Strategies and Approaches\n\n- 7.1.4. Some  banks  have  invested  significant  effort  in  identifying  and  building  key enabling  services  and  modules  for  Generative  AI  that  can  be  utilised  across multiple use cases, e.g., vector databases 80 , retrieval systems 81 , evaluation and\n\n80 Data, particularly unstructured data, such as text and images, need to be encoded into numerical representations before they  can  be  used  for  AI  or  Generative  AI.  Such  numerical  representations  are  commonly  referred  to  as  vectors.  Vector databases are specialised database systems designed to store, index, and efficiently query such data.\n\n81 Retrieval systems help to search information repositories and retrieve the most relevant information for a specific task. For example, to help answer a query relating to information in a corporate information repository, the retrieval system will help to search for the most relevant pieces of information in the corporate information repository. The retrieved information is then usually used as context for the Generative AI model to generate an answer from.\n\n<!-- image -->\n\ntesting modules 82 . Such an approach enables scalability, reduces time and costs for implementation, and facilitates the development of more robust and stable Generative AI.\n\n- 7.1.5. To manage the potential impact of Generative AI risks, such as hallucinations, most banks have started with a more limited scope of use, focusing on the use of Generative  AI  for  assisting  or  augmenting  humans,  or  improving  internal operational efficiencies, rather than deploying Generative AI in direct customerfacing  applications  without  a  human-in-the-loop.  Banks  felt  that  such  an approach would allow them to learn how to utilise Generative AI effectively and understand its limitations, while managing the potential impact of risks posed by Generative AI.\n- 7.1.6. Similarly, to gain greater comfort with the use of Generative AI, most banks have established clear policies and procedures for Generative AI pilots and experimentation frameworks. Aside from helping the bank to build capacity and capabilities while managing risks associated with Generative AI, such pilots and experimentation frameworks are needed to evaluate and test Generative AI in real-world  scenarios  and  understand  how  Generative  AI  would  behave  when deployed. Such pilots are typically bound by time and user limits 83 .\n\n## Process Controls\n\n- 7.1.7. To address the cross-cutting nature of Generative AI use cases and risks, as well as the fast-evolving landscape, some banks have instituted cross-functional risk control checks at key stages of the Generative AI lifecycle.\n- 7.1.8. As most Generative AI use cases usually fall within a few task archetypes, e.g., summarisation, information extraction, conversational agents, question answering, one bank established detailed development and validation guidelines\n\n82 An example of such a module could be a separately trained AI model that estimates the probability of an answer generated by an LLM being a hallucination.\n\n83 Aside from setting time and user limits, other requirements that may apply to such pilots or experiments include setting clear criteria for success at the end of the pilot, conditions on the terms of use for owners and end-users, and close monitoring of usage patterns and outputs for anomalies and to ensure compliance with the limited scope of usage.\n\n<!-- image -->\n\n- specific to different Generative AI task archetypes to support development and validation processes.\n- 7.1.9. Due to the uncertainties associated with Generative AI, banks continue to require human oversight or have a human-in-the-loop when using Generative AI to aid in decision-making.    Extensive  user  education  and  training  on  the  limitations  of Generative AI tools was another key area of focus.\n\n## Technical Controls\n\n- 7.1.10. As most Generative AI models used by banks, whether closed or open-source, originate from third parties, selection of the appropriate model continues to be an  important  step  for  most  banks.  To  assess  suitability,  some  banks  would typically  start  by  conducting  significant  research  on  the  capabilities  of  these models  for  their  needs,  including  utilising  public  benchmarks  and  the  latest research  papers  to  guide  decisions.  Testing  and  evaluation  of  Generative  AI models in the context of the bank's use cases was also an important area of focus.\n- 7.1.11. More  advanced banks would undertake a range of assessments, from standalone,  functional  to  end-to-end  assessments.  Standalone  assessments involve the evaluation of the Generative AI model itself. This is usually based on publicly  available  data  or  resources,  such  as  evaluation  results  in  research articles, model leaderboards, or using open-source evaluation datasets. Functional assessments involve evaluations of Generative AI model performance on tasks and contexts specific to the bank, e.g., evaluating the performance of a Generative  AI  model  when  used  for  retrieval  of information  from  the  bank's repository. Finally, end-to-end assessments would evaluate the performance of the entire Generative AI system, which may involve multiple Generative AI or AI models.\n- 7.1.12. Such banks also paid significant attention to establishing methods for assessing different  aspects  of  Generative  AI  model  performance  such  as  accuracy,\n\n<!-- image -->\n\n- relevance, and bias 84 ,  as well as creating reusable modules to facilitate testing and evaluation.\n- 7.1.13. The  more  advanced  banks  also  paid  significant  attention  to  curating  testing datasets that were specific to the use cases and tasks that Generative AI models were being used for in the bank. Such testing datasets were critical to ensuring that  Generative  AI  models  and  systems  were  fit-for-purpose  in  the  bank 's context. For example, if Generative AI was used for summarising complaints from the bank's customers, the performance of Generative AI o n general summarisation  tasks  may  not  be  indicative of  its  performance  in  the  bank's context as it may not have been trained on such complaints that are not in the public domain, and the complaints may also contain information specific to the bank, e.g., the bank's services. To ensure the proper evaluation of Generative AI in the bank's context , the bank will need to curate bank-specific testing datasets from  the  bank's  internal  historical  data,  or  use expert  human  annotators  to generate good quality summaries for a set of customer complaints to evaluate against.  Such  testing  datasets  are  also  important  for  monitoring  the  ongoing performance of Generative AI models, and for evaluating newer Generative AI models as part of the onboarding process. Other key tests that banks adopted included model vulnerability testing to assess cyber security risks 85 ,  as  well  as stability  and  sensitivity  testing  to  ensure  consistent  performance.  Human feedback also played a key role in testing, evaluating and monitoring Generative AI performance.\n- 7.1.14. Most banks have established input and output guardrails that utilise filters to manage risks relating to areas such as toxicity, biasness, or leakage of sensitive information.  Such  filters  may  use  rules  or  AI  to  detect  such  undesired  or inappropriate  information.  For  example,  input  filters  may  be  used  to  reject requests with toxic language, or replace PII information in requests with generic\n\n84 In  this  context,  accuracy  refers  to  whether  the  generated  text  aligns  with  factual  information;  relevance  refers  to  how pertinent the generated text is to the specific query; and bias refers to scenarios where the generated text may be biased to specific groups of people, e.g., the generated content may favour one gender over another.\n\n85 These were discussed at length in MAS' information paper on Cyber Risks Associated with Generative Artificial Intelligence and will not be repeated here. See Annex B for link to the paper.\n\n<!-- image -->\n\nplaceholders. Output filters may be used to detect biasness or toxic language in the  outputs  of  Generative  AI  and  trigger  a  review  by  a  human  or  another Generative AI model, or redact PII information in the outputs of Generative AI before they are presented to the user. Similarly, some banks also focused efforts on developing guardrails that were reusable.\n\n- 7.1.15. Banks  mitigated  data  security  risks  when  using  Generative  AI  by  either  using private  cloud  solutions  for  Generative  AI  models,  or  open-source  models  onpremise,  which  keep  sensitive  data  within  controlled  environments  (either dedicated cloud resources not shared with other organisations, or on-premise servers) which can reduce the risks of exposure of data to external parties. Legal agreements with solution providers, data loss prevention tools, as well as limits on  the  classification  of  data  that  could  be  used  in  Generative  AI  were  also important to mitigate data security risks.\n- 7.1.16. Another common area that banks were exploring to address Generative AI risks were  grounding  methods 86 such  as  retrieval  augmented  generation  (RAG) 87 where the outputs of Generative AI models are constrained based on internal knowledge bases, and source citations are provided to allow end-users to check for the accuracy of Generative AI outputs.\n\n86 Grounding methods help to ground or anchor the Generative AI outputs to factual, verifiable information, which can help reduce hallucinations and improve robustness.\n\n87 Retrieval-Augmented Generation (RAG) methods typically retrieve relevant information from a pre-defined knowledge base, and provide the retrieved information as context to the Generative AI model for the generation of outputs. For example, to generate an answer to a question, information relevant to the question would be first retrieved, and the retrieved information would then be provided as context to an LLM. The LLM would usually be instructed to answer the question based on the retrieved information. Links to the retrieved information could also be provided as source citations in the answer. There is however still the possibility of hallucinations occurring even with such approaches.\n\n<!-- image -->\n\n## 7.2 Third-Party AI\n\n## Overview\n\nExisting  third-party  risk  management  standards  and  processes 88 continue  to  play  an important role in banks' efforts to mitigate risks associated with third-party AI. As far as practicable, most banks also extended controls for internally developed AI to third-party AI. When considering the use of third-party AI, banks would weigh the potential benefits against the risks of using third-party AI. To address the additional risks arising from thirdparty AI, banks were exploring areas such as:\n\n- conducting compensatory testing;\n- enhancing contingency planning;\n- updating legal agreements; and\n- investing in training and other awareness efforts.\n- 7.2.1 The use of third-party AI is increasingly common among banks, particularly in the context of Generative AI where most banks utilise Generative AI models that were pre-trained by an external party. However, the use of such third-party AI and Generative AI presents additional risks, such as unknown biases from pretraining  data,  data  protection  concerns,  as  well  as  concentration  risks  due  to increased interdependencies, e.g., from multiple FIs or even third-party providers relying on common underlying Generative AI models. The lack of transparency is often cited as a key challenge in managing such third-party risks. Third-party AI providers  may  be  reluctant  to  disclose  proprietary  information  about  their training  data  or  algorithms,  hindering  banks'  efforts  in  risk  assessment  and ongoing monitoring.\n- 7.2.2 To mitigate these additional risks, banks were exploring various approaches, such as:\n\n<!-- image -->\n\n- a. Compensatory testing - conducting rigorous testing of third-party AI models using  various  datasets  and  scenarios  to  verify  the  model's robustness  and stability in the bank's context , and to detect potential biases.\n- b. Contingency  planning -  developing  robust  contingency  plans  to  address potential failures, unexpected behaviour of third-party AI, or discontinuing of support  by  vendors.  This  can  include  having  backup  systems  or  manual processes in place to ensure business continuity.\n- c. Legal agreements - updating contracts with third-party AI providers to include clauses such as those pertaining to performance guarantees, data protection, the right to audit, and notification when AI is introduced (or not incorporating AI without the bank's agreement) in existing third-party providers ' solutions. Such clauses could facilitate clearer expectations and responsibilities.\n- d. Awareness  efforts -investing  in  training  of  staff  on  AI  literacy  and  risk awareness  to  improve  understanding  and  mitigation  of  risks;  conducting surveys with third-party providers to gather more information about whether AI  is  being  used  in  their  products  or  services,  and  thirdparty  providers' practices, including their AI development and risk management processes.\n\n## 8 Conclusion\n\n- 8.1. Robust oversight and governance of AI, supported by comprehensive identification, inventorisation of AI and appropriate risk materiality assessment, as  well  as  rigorous  development,  validation  and  deployment  standards  and processes are important areas that FIs need to focus on when using AI. As the AI landscape continues to evolve, AI MRM frameworks will need to be regularly reviewed and updated, and risk management efforts scaled up based on the state of AI use.  Aside from AI MRM, controls in non AI-specific areas such as general data  governance  and  management,  technology,  cyber  and  third  party  risk management, and legal and compliance will also need to be reviewed to take AI developments into account.\n\n<!-- image -->\n\n- 8.2. As the AI landscape continues to  evolve, MAS will continue to work with the industry to help facilitate and uplift AI and Generative AI governance and risk management efforts across the financial industry, through information sharing efforts such as this paper to promulgate industry best practices, and industry collaborations such as Project MindForge. MAS is also considering supervisory guidance  for  all  FIs  next  year,  building  upon  the  focus  areas  covered  in  this information paper.\n\n<!-- image -->\n\n## Annex A - Definitions\n\n- Model -A model is a method, system or approach which converts assumptions and input  data  into  quantitative  estimates,  decisions,  or  decision  recommendations (based on the Global Associate of Risk Professionals '  definition of a model) .  Apart from AI models , which typically refer to machine or deep learning models which we define below, banks also routinely utilise conventional models , such as economic, financial, or statistical models. Some models, such as logistic regression models, are commonly used in both statistical and AI fields and may be regarded as both AI and conventional models.\n- Artificial Intelligence (AI) -An AI system is a machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness  after  deployment  (based  on  the  Organisation  for  Economic  Cooperation  and  Development 's definition  of  AI). Such  a  definition  would  include Generative AI. An AI or Generative AI system can be based on one or multiple AI or Generative AI models and may also involve other machine-based components.\n- AI Use Case -An AI or Generative AI use case usually refers to a specific real-world context that the AI or Generative AI model or system is applied to. For example, an AI  recommendation  model  or  system  that  is  applied  to  a  financial  product recommendation use case.\n- Machine learning -Machine learning is a subset of AI where the AI directly learns from data. The machine learning model learns model parameters (or model weights) to  transform  inputs  into  estimates  or  outputs  from  the  data  by  updating  these parameters iteratively based on an objective. For example, the machine learning model may be provided with historical  data  that  consists  of  the  information  on customers, e.g., income and existing value of debt (which we refer to as input data), and whether the customer had defaulted on a loan obligation (which we refer to as the target variable or label). The machine learning model can then be trained by learning model parameters that allow it to transform input data to target variables or labels with maximum accuracy (or minimum error).\n\n<!-- image -->\n\n- Deep learning -Deep learning is a subset of machine learning, usually based on neural networks (that were inspired by how neurons in the brain recognise complex patterns in data) that comprise multiple layers of neurons. Deep learning models are able to learn more complex patterns due to the many layers of neurons in the model.\n- Discriminative versus Generative AI models -AI models that generate predictions, e.g., predicting a credit default based on customer information, or recommending a financial  product  based  on  customer  information,  are  usually  referred  to  as discriminative AI models. This is in contrast to Generative AI models that are usually used to generate content such as text, images, audio or videos.\n- CI/CD,  DevOps,  MLOps,  AIOps,  LLMOps -Continuous  integration/continuous deployment (CI/CD) or DevOps pipelines automate the process of building, testing, and deploying code changes. These terms are closely related to the term MLOps, which is used to describe tools and systems that help to automate the process of building, testing, deploying and monitoring the performance of machine learning systems. More recent terms such as AIOps and LLMOps have also been used to describe  such  tools  and  systems  for  AI  in  general  or  for  Large  Language  Models (LLM).\n- Data Drift - This occurs when the statistical properties of the distribution of the data changes. For example, the underlying distribution of customer data may have drifted or changed over time due to changes in the lifestyles of customers. Hence, an AI model that was trained on data from a more distant time period may not perform as well on data from a more recent time period due to data drift. A common measure of  how  much  a  population  distribution  has  changed  over  time  is  the  Population Stability Index (PSI).\n- Concept Drift - This occurs when the underlying relationships between the features in input data and what the AI model is being used to predict or generate changes. For example, customer preferences for financial products may have shifted due to broad  industry  changes  (e.g.,  a  shift  in  the  relationships  between  customer information and their preferences for financial products), and an AI model used to generate financial product recommendations may no longer perform as well due to\n\n<!-- image -->\n\n- such  concept  drifts.  A  common  measure  of  concept  drift  is  the  Characteristic Stability Index (CSI).\n- Model Drift - Model drift is a broader term that usually encompasses both data drift and concept drift, as well as other factors that can cause a model's performance to degrade  over  time.  Aside  from  measures  such  as  PSI  and  CSI,  monitoring  the statistical characteristics of AI predictions can also be used to detect drifts in general.\n- Supervised learning -Supervised learning is a machine learning approach where a model is trained on a labelled dataset. In this process, each data point includes input features  paired  with  the  corresponding  output  (label).  The  model  learns  to  map inputs to outputs by comparing its predictions with the actual labels and updating the model parameters iteratively. Classification, which involves the prediction of classes or categories, and regression, which involves the prediction of continuous values, are common examples of supervised learning.\n- Unsupervised  learning -Unsupervised  learning  is  a  machine  learning  approach where a model discovers patterns in data without the use of labels. An example of unsupervised learning is clustering, where data points are grouped together based on their inherent similarities or dissimilarities.\n\n<!-- image -->\n\n## Annex B - Useful References\n\n## Publications for the Financial Sector issued by MAS\n\n- MAS FEAT Principles: https://www.mas.gov.sg/publications/monographs-or-informationpaper/2018/feat\n- Veritas Initiative: https://www.mas.gov.sg/schemes-and-initiatives/veritas\n- Project MindForge: https://www.mas.gov.sg/schemes-and-initiatives/project-mindforge\n- Information  Paper  on  Implementation  of  Fairness  Principles  in  Financial  Institutions'  use  of Artificial  Intelligence/Machine  Learning: https://www.mas.gov.sg/publications/monographs-orinformation-paper/2022/implementation-of-fairness-principles-in-financial-institutions-use-ofartificial-intelligence-and-machine-learning\n- Information Paper on Cyber Risks Associated with Generative Artificial Intelligence: https://www.mas.gov.sg/regulation/circulars/cyber-risks-associated-with-generative-artificialintelligence\n- Information Paper on Data Governance and Management Practices: https://www.mas.gov.sg/publications/monographs-or-information-paper/2024/datagovernance-and-management-practices\n- Technology Risk Management Guidelines: https://www.mas.gov.sg/regulation/guidelines/technology-risk-management-guidelines\n- Business Continuity Management Guidelines : https://www.mas.gov.sg/regulation/guidelines/guidelines-on-business-continuity-management\n- Notice and Guidelines on Third-Party Risk Management: https://www.mas.gov.sg/regulation/third-party-risk-management\n- Information Paper on Operational Risk Management - Management of Third Party Arrangements: https://www.mas.gov.sg/publications/monographs-or-information-paper/2022/operational-riskmanagement---management-of-third-party-arrangements\n\n<!-- image -->\n\n## Non-Financial Sector Specific Publications\n\n- AI Verify: AI governance testing framework and software toolkit: https://www.aiverifyfoundation.sg/what-is-ai-verify/\n- Project Moonshot: https://www.aiverifyfoundation.sg/project-moonshot/\n- Model Governance Framework for Generative AI: https://www.aiverifyfoundation.sg/resources/mgf-gen-ai/\n- Trusted Data Sharing Framework: https://www.imda.gov.sg/how-we-can-help/datainnovation/trusted-data-sharing-framework\n- Personal Data Protection Act (PDPA): https://www.pdpc.gov.sg/overview-of-pdpa/thelegislation/personal-data-protection-act\n- Advisory Guidelines on use of Personal Data in AI Recommendation and Decision Systems: https://www.pdpc.gov.sg/guidelines-and-consultation/2024/02/advisory-guidelines-on-use-ofpersonal-data-in-ai-recommendation-and-decision-systems\n- Guidelines and Companion  Guide on Securing AI Systems: https://www.csa.gov.sg/TipsResource/publications/2024/guidelines-on-securing-ai", "fetched_at_utc": "2026-02-09T13:32:46Z", "sha256": "506923eb3328c3d97780cd326fac2b7eed34d3469bf2104f4a07f2482bd8171a", "meta": {"file_name": "AI Risk Management - Singapore.pdf", "file_size": 765210, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-ai-risk-management-framework-nist-8840ff2438cf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Risk Management Framework - NIST.pdf", "title": "AI Risk Management Framework - NIST", "text": "<!-- image -->\n\nNIST AI 100-1\n\n<!-- image -->\n\n## Artificial Intelligence Risk Management Framework (AI RMF 1.0)\n\n<!-- image -->\n\n## NIST AI 100-1\n\n## Artificial Intelligence Risk Management Framework (AI RMF 1.0)\n\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1\n\nJanuary 2023\n\n<!-- image -->\n\nU.S. Department of Commerce Gina M. Raimondo, Secretary\n\nNational Institute of Standards and Technology Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology\n\nCertain commercial entities, equipment, or materials may be identified in this document in order to describe an experimental procedure or concept adequately. Such identification is not intended to imply recommenda- tion or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the entities, materials, or equipment are necessarily the best available for the purpose.\n\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1\n\n## Update Schedule and Versions\n\nThe Artificial Intelligence Risk Management Framework (AI RMF) is intended to be a living document.\n\nNIST will review the content and usefulness of the Framework regularly to determine if an update is appropriate; a review with formal input from the AI community is expected to take place no later than 2028. The Framework will employ a two-number versioning system to track and identify major and minor changes. The first number will represent the generation of the AI RMF and its companion documents (e.g., 1.0) and will change only with major revisions. Minor revisions will be tracked using '.n' after the generation number (e.g., 1.1). All changes will be tracked using a Version Control Table which identifies the history, including version number, date of change, and description of change. NIST plans to update the AI RMF Playbook frequently. Comments on the AI RMF Playbook may be sent via email to AIframework@nist.gov at any time and will be reviewed and integrated on a semi-annual basis.\n\n## Table of Contents\n\n| Executive Summary                                                                                                            | Executive Summary                                                                                                            | Executive Summary                                                                                                            | 1              |\n|------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|----------------|\n| Part 1: Foundational Information                                                                                             | Part 1: Foundational Information                                                                                             | Part 1: Foundational Information                                                                                             | 4              |\n| 1 Framing Risk                                                                                                               | 1 Framing Risk                                                                                                               | 1 Framing Risk                                                                                                               | 4              |\n| 1.1                                                                                                                          |                                                                                                                              | Understanding and Addressing Risks, Impacts, and Harms                                                                       | 4              |\n| 1.2                                                                                                                          | Challenges for AI Risk Management                                                                                            | Challenges for AI Risk Management                                                                                            | 5              |\n|                                                                                                                              | 1.2.1                                                                                                                        | Risk Measurement                                                                                                             | 5              |\n|                                                                                                                              | 1.2.2                                                                                                                        | Risk Tolerance                                                                                                               | 7              |\n|                                                                                                                              | 1.2.3                                                                                                                        | Risk Prioritization                                                                                                          | 7              |\n|                                                                                                                              | 1.2.4                                                                                                                        | Organizational Integration and Management of Risk                                                                            | 8              |\n| 2 Audience                                                                                                                   | 2 Audience                                                                                                                   | 2 Audience                                                                                                                   | 9              |\n| AI Risks and Trustworthiness                                                                                                 | AI Risks and Trustworthiness                                                                                                 | AI Risks and Trustworthiness                                                                                                 | 12             |\n| 3.1                                                                                                                          | Valid and                                                                                                                    | Reliable                                                                                                                     | 13             |\n| 3.2                                                                                                                          | Safe                                                                                                                         |                                                                                                                              | 14             |\n|                                                                                                                              | 3.3                                                                                                                          | Secure and Resilient                                                                                                         | 15             |\n|                                                                                                                              | 3.4                                                                                                                          | Accountable and Transparent                                                                                                  | 15             |\n| 3.5                                                                                                                          |                                                                                                                              | Explainable and Interpretable                                                                                                | 16             |\n|                                                                                                                              | 3.6                                                                                                                          | Privacy-Enhanced                                                                                                             | 17             |\n|                                                                                                                              | 3.7                                                                                                                          | Fair - with Harmful Bias Managed                                                                                             | 17             |\n|                                                                                                                              |                                                                                                                              |                                                                                                                              | 19             |\n| 4 Effectiveness of the AIRMF                                                                                                 | 4 Effectiveness of the AIRMF                                                                                                 | 4 Effectiveness of the AIRMF                                                                                                 | 20             |\n| Part 2: Core and Profiles AI RMFCore                                                                                         | Part 2: Core and Profiles AI RMFCore                                                                                         | Part 2: Core and Profiles AI RMFCore                                                                                         |                |\n| 5                                                                                                                            | 5                                                                                                                            | 5                                                                                                                            | 20             |\n| 5.1                                                                                                                          |                                                                                                                              | Govern                                                                                                                       | 21             |\n|                                                                                                                              | 5.2                                                                                                                          | Map                                                                                                                          | 24             |\n|                                                                                                                              | 5.3                                                                                                                          | Measure                                                                                                                      | 28             |\n| 5.4                                                                                                                          |                                                                                                                              | Manage                                                                                                                       | 31             |\n| 6 AI RMFProfiles                                                                                                             | 6 AI RMFProfiles                                                                                                             | 6 AI RMFProfiles                                                                                                             | 33             |\n| Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3                                                              | Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3                                                              | Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3                                                              | 35             |\n| Appendix B: How AI Risks Differ from Traditional Software Risks                                                              | Appendix B: How AI Risks Differ from Traditional Software Risks                                                              | Appendix B: How AI Risks Differ from Traditional Software Risks                                                              | 38             |\n| Appendix C: AI Risk Management and Human-AI Interaction                                                                      | Appendix C: AI Risk Management and Human-AI Interaction                                                                      | Appendix C: AI Risk Management and Human-AI Interaction                                                                      | 40             |\n| Appendix D: Attributes of the AIRMF                                                                                          | Appendix D: Attributes of the AIRMF                                                                                          | Appendix D: Attributes of the AIRMF                                                                                          | 42             |\n| List of Tables                                                                                                               | List of Tables                                                                                                               | List of Tables                                                                                                               | List of Tables |\n| Table 1 Categories and subcategories for the GOVERN function.                                                                | Table 1 Categories and subcategories for the GOVERN function.                                                                | Table 1 Categories and subcategories for the GOVERN function.                                                                | 22             |\n| Table 2 Categories and subcategories for the MAP function.                                                                   | Table 2 Categories and subcategories for the MAP function.                                                                   | Table 2 Categories and subcategories for the MAP function.                                                                   | 26             |\n| Table 3 Categories and subcategories for the MEASURE function. Table 4 Categories and subcategories for the MANAGE function. | Table 3 Categories and subcategories for the MEASURE function. Table 4 Categories and subcategories for the MANAGE function. | Table 3 Categories and subcategories for the MEASURE function. Table 4 Categories and subcategories for the MANAGE function. | 32             |\n\n## List of Figures\n\n- Fig. 1 Examples of potential harms related to AI systems. Trustworthy AI systems and their responsible use can mitigate negative risks and contribute to benefits for people, organizations, and ecosystems.\n- Fig. 2 Lifecycle and Key Dimensions of an AI System. Modified from OECD (2022) OECD Framework for the Classification of AI systems - OECD Digital Economy Papers. The two inner circles show AI systems' key dimensions and the outer circle shows AI lifecycle stages. Ideally, risk management efforts start with the Plan and Design function in the application context and are performed throughout the AI system lifecycle. See Figure 3 for representative AI actors.\n- Fig. 3 AI actors across AI lifecycle stages. See Appendix A for detailed descriptions of AI actor tasks, including details about testing, evaluation, verification, and validation tasks. Note that AI actors in the AI Model dimension (Figure 2) are separated as a best practice, with those building and using the models separated from those verifying and validating the models.\n- Fig. 4 Characteristics of trustworthy AI systems. Valid &amp; Reliable is a necessary condition of trustworthiness and is shown as the base for other trustworthiness characteristics. Accountable &amp; Transparent is shown as a vertical box because it relates to all other characteristics.\n- Fig. 5 Functions organize AI risk management activities at their highest level to govern, map, measure, and manage AI risks. Governance is designed to be a cross-cutting function to inform and be infused throughout the other three functions.\n\n5\n\n10\n\n11\n\n12\n\n20\n\n## Executive Summary\n\nArtificial intelligence (AI) technologies have significant potential to transform society and people's lives - from commerce and health to transportation and cybersecurity to the environment and our planet. AI technologies can drive inclusive economic growth and support scientific advancements that improve the conditions of our world. AI technologies, however, also pose risks that can negatively impact individuals, groups, organizations, communities, society, the environment, and the planet. Like risks for other types of technology, AI risks can emerge in a variety of ways and can be characterized as long- or short-term, highor low-probability, systemic or localized, and high- or low-impact.\n\nThe AI RMF refers to an AI system as an engineered or machine-based system that can, for a given set of objectives, generate outputs such as predictions, recommendations, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy (Adapted from: OECD Recommendation on AI:2019; ISO/IEC 22989:2022).\n\nWhile there are myriad standards and best practices to help organizations mitigate the risks of traditional software or information-based systems, the risks posed by AI systems are in many ways unique (See Appendix B). AI systems, for example, may be trained on data that can change over time, sometimes significantly and unexpectedly, affecting system functionality and trustworthiness in ways that are hard to understand. AI systems and the contexts in which they are deployed are frequently complex, making it difficult to detect and respond to failures when they occur. AI systems are inherently socio-technical in nature, meaning they are influenced by societal dynamics and human behavior. AI risks - and benefits can emerge from the interplay of technical aspects combined with societal factors related to how a system is used, its interactions with other AI systems, who operates it, and the social context in which it is deployed.\n\nThese risks make AI a uniquely challenging technology to deploy and utilize both for organizations and within society. Without proper controls, AI systems can amplify, perpetuate, or exacerbate inequitable or undesirable outcomes for individuals and communities. With proper controls, AI systems can mitigate and manage inequitable outcomes.\n\nAI risk management is a key component of responsible development and use of AI systems. Responsible AI practices can help align the decisions about AI system design, development, and uses with intended aim and values. Core concepts in responsible AI emphasize human centricity, social responsibility, and sustainability. AI risk management can drive responsible uses and practices by prompting organizations and their internal teams who design, develop, and deploy AI to think more critically about context and potential or unexpected negative and positive impacts. Understanding and managing the risks of AI systems will help to enhance trustworthiness, and in turn, cultivate public trust.\n\nSocial responsibility can refer to the organization's responsibility 'for the impacts of its decisions and activities on society and the environment through transparent and ethical behavior' (ISO 26000:2010). Sustainability refers to the 'state of the global system, including environmental, social, and economic aspects, in which the needs of the present are met without compromising the ability of future generations to meet their own needs' (ISO/IEC TR 24368:2022). Responsible AI is meant to result in technology that is also equitable and accountable. The expectation is that organizational practices are carried out in accord with ' professional responsibility ,' defined by ISO as an approach that 'aims to ensure that professionals who design, develop, or deploy AI systems and applications or AI-based products or systems, recognize their unique position to exert influence on people, society, and the future of AI' (ISO/IEC TR 24368:2022).\n\nAs directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283), the goal of the AI RMF is to offer a resource to the organizations designing, developing, deploying, or using AI systems to help manage the many risks of AI and promote trustworthy and responsible development and use of AI systems. The Framework is intended to be voluntary , rights-preserving, non-sector-specific, and use-case agnostic, providing flexibility to organizations of all sizes and in all sectors and throughout society to implement the approaches in the Framework.\n\nThe Framework is designed to equip organizations and individuals - referred to here as AI actors - with approaches that increase the trustworthiness of AI systems, and to help foster the responsible design, development, deployment, and use of AI systems over time. AI actors are defined by the Organisation for Economic Co-operation and Development (OECD) as 'those who play an active role in the AI system lifecycle, including organizations and individuals that deploy or operate AI' [OECD (2019) Artificial Intelligence in Society-OECD iLibrary] (See Appendix A).\n\nThe AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies continue to develop, and to be operationalized by organizations in varying degrees and capacities so society can benefit from AI while also being protected from its potential harms.\n\nThe Framework and supporting resources will be updated, expanded, and improved based on evolving technology, the standards landscape around the world, and AI community experience and feedback. NIST will continue to align the AI RMF and related guidance with applicable international standards, guidelines, and practices. As the AI RMF is put into use, additional lessons will be learned to inform future updates and additional resources.\n\nThe Framework is divided into two parts. Part 1 discusses how organizations can frame the risks related to AI and describes the intended audience. Next, AI risks and trustworthiness are analyzed, outlining the characteristics of trustworthy AI systems, which include\n\nvalid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy enhanced, and fair with their harmful biases managed.\n\nPart 2 comprises the 'Core' of the Framework. It describes four specific functions to help organizations address the risks of AI systems in practice. These functions GOVERN , MAP , MEASURE , and MANAGE - are broken down further into categories and subcategories. While GOVERN applies to all stages of organizations' AI risk management processes and procedures, the MAP , MEASURE , and MANAGE functions can be applied in AI system-specific contexts and at specific stages of the AI lifecycle.\n\nAdditional resources related to the Framework are included in the AI RMF Playbook, which is available via the NIST AI RMF website:\n\nhttps://www.nist.gov/itl/ai-risk-management-framework.\n\nDevelopment of the AI RMF by NIST in collaboration with the private and public sectors is directed and consistent with its broader AI efforts called for by the National AI Initiative Act of 2020, the National Security Commission on Artificial Intelligence recommendations, and the Plan for Federal Engagement in Developing Technical Standards and Related Tools. Engagement with the AI community during this Framework's development - via responses to a formal Request for Information, three widely attended workshops, public comments on a concept paper and two drafts of the Framework, discussions at multiple public forums, and many small group meetings - has informed development of the AI RMF 1.0 as well as AI research and development and evaluation conducted by NIST and others. Priority research and additional guidance that will enhance this Framework will be captured in an associated AI Risk Management Framework Roadmap to which NIST and the broader community can contribute.\n\n## Part 1: Foundational Information\n\n## 1. Framing Risk\n\nAI risk management offers a path to minimize potential negative impacts of AI systems, such as threats to civil liberties and rights, while also providing opportunities to maximize positive impacts. Addressing, documenting, and managing AI risks and potential negative impacts effectively can lead to more trustworthy AI systems.\n\n## 1.1 Understanding and Addressing Risks, Impacts, and Harms\n\nIn the context of the AI RMF, risk refers to the composite measure of an event's probability of occurring and the magnitude or degree of the consequences of the corresponding event. The impacts, or consequences, of AI systems can be positive, negative, or both and can result in opportunities or threats (Adapted from: ISO 31000:2018). When considering the negative impact of a potential event, risk is a function of 1) the negative impact, or magnitude of harm, that would arise if the circumstance or event occurs and 2) the likelihood of occurrence (Adapted from: OMB Circular A-130:2016). Negative impact or harm can be experienced by individuals, groups, communities, organizations, society, the environment, and the planet.\n\n'Risk management refers to coordinated activities to direct and control an organization with regard to risk' (Source: ISO 31000:2018).\n\nWhile risk management processes generally address negative impacts, this Framework offers approaches to minimize anticipated negative impacts of AI systems and identify opportunities to maximize positive impacts. Effectively managing the risk of potential harms could lead to more trustworthy AI systems and unleash potential benefits to people (individuals, communities, and society), organizations, and systems/ecosystems. Risk management can enable AI developers and users to understand impacts and account for the inherent limitations and uncertainties in their models and systems, which in turn can improve overall system performance and trustworthiness and the likelihood that AI technologies will be used in ways that are beneficial.\n\nThe AI RMF is designed to address new risks as they emerge. This flexibility is particularly important where impacts are not easily foreseeable and applications are evolving. While some AI risks and benefits are well-known, it can be challenging to assess negative impacts and the degree of harms. Figure 1 provides examples of potential harms that can be related to AI systems.\n\nAI risk management efforts should consider that humans may assume that AI systems work - and work well - in all settings. For example, whether correct or not, AI systems are often perceived as being more objective than humans or as offering greater capabilities than general software.\n\nFig. 1. Examples of potential harms related to AI systems. Trustworthy AI systems and their responsible use can mitigate negative risks and contribute to benefits for people, organizations, and ecosystems.\n\n<!-- image -->\n\n## 1.2 Challenges for AI Risk Management\n\nSeveral challenges are described below. They should be taken into account when managing risks in pursuit of AI trustworthiness.\n\n## 1.2.1 Risk Measurement\n\nAI risks or failures that are not well-defined or adequately understood are difficult to measure quantitatively or qualitatively. The inability to appropriately measure AI risks does not imply that an AI system necessarily poses either a high or low risk. Some risk measurement challenges include:\n\nRisks related to third-party software, hardware, and data: Third-party data or systems can accelerate research and development and facilitate technology transition. They also may complicate risk measurement. Risk can emerge both from third-party data, software or hardware itself and how it is used. Risk metrics or methodologies used by the organization developing the AI system may not align with the risk metrics or methodologies uses by the organization deploying or operating the system. Also, the organization developing the AI system may not be transparent about the risk metrics or methodologies it used. Risk measurement and management can be complicated by how customers use or integrate thirdparty data or systems into AI products or services, particularly without sufficient internal governance structures and technical safeguards. Regardless, all parties and AI actors should manage risk in the AI systems they develop, deploy, or use as standalone or integrated components.\n\nTracking emergent risks: Organizations' risk management efforts will be enhanced by identifying and tracking emergent risks and considering techniques for measuring them.\n\nAI system impact assessment approaches can help AI actors understand potential impacts or harms within specific contexts.\n\nAvailability of reliable metrics: The current lack of consensus on robust and verifiable measurement methods for risk and trustworthiness, and applicability to different AI use cases, is an AI risk measurement challenge. Potential pitfalls when seeking to measure negative risk or harms include the reality that development of metrics is often an institutional endeavor and may inadvertently reflect factors unrelated to the underlying impact. In addition, measurement approaches can be oversimplified, gamed, lack critical nuance, become relied upon in unexpected ways, or fail to account for differences in affected groups and contexts.\n\nApproaches for measuring impacts on a population work best if they recognize that contexts matter, that harms may affect varied groups or sub-groups differently, and that communities or other sub-groups who may be harmed are not always direct users of a system.\n\nRisk at different stages of the AI lifecycle: Measuring risk at an earlier stage in the AI lifecycle may yield different results than measuring risk at a later stage; some risks may be latent at a given point in time and may increase as AI systems adapt and evolve. Furthermore, different AI actors across the AI lifecycle can have different risk perspectives. For example, an AI developer who makes AI software available, such as pre-trained models, can have a different risk perspective than an AI actor who is responsible for deploying that pre-trained model in a specific use case. Such deployers may not recognize that their particular uses could entail risks which differ from those perceived by the initial developer. All involved AI actors share responsibilities for designing, developing, and deploying a trustworthy AI system that is fit for purpose.\n\nRisk in real-world settings: While measuring AI risks in a laboratory or a controlled environment may yield important insights pre-deployment, these measurements may differ from risks that emerge in operational, real-world settings.\n\nInscrutability: Inscrutable AI systems can complicate risk measurement. Inscrutability can be a result of the opaque nature of AI systems (limited explainability or interpretability), lack of transparency or documentation in AI system development or deployment, or inherent uncertainties in AI systems.\n\nHuman baseline: Risk management of AI systems that are intended to augment or replace human activity, for example decision making, requires some form of baseline metrics for comparison. This is difficult to systematize since AI systems carry out different tasks - and perform tasks differently - than humans.\n\n## 1.2.2 Risk Tolerance\n\nWhile the AI RMF can be used to prioritize risk, it does not prescribe risk tolerance. Risk tolerance refers to the organization's or AI actor's (see Appendix A) readiness to bear the risk in order to achieve its objectives. Risk tolerance can be influenced by legal or regulatory requirements (Adapted from: ISO GUIDE 73). Risk tolerance and the level of risk that is acceptable to organizations or society are highly contextual and application and use-case specific. Risk tolerances can be influenced by policies and norms established by AI system owners, organizations, industries, communities, or policy makers. Risk tolerances are likely to change over time as AI systems, policies, and norms evolve. Different organizations may have varied risk tolerances due to their particular organizational priorities and resource considerations.\n\nEmerging knowledge and methods to better inform harm/cost-benefit tradeoffs will continue to be developed and debated by businesses, governments, academia, and civil society. To the extent that challenges for specifying AI risk tolerances remain unresolved, there may be contexts where a risk management framework is not yet readily applicable for mitigating negative AI risks.\n\nThe Framework is intended to be flexible and to augment existing risk practices which should align with applicable laws, regulations, and norms. Organizations should follow existing regulations and guidelines for risk criteria, tolerance, and response established by organizational, domain, discipline, sector, or professional requirements. Some sectors or industries may have established definitions of harm or established documentation, reporting, and disclosure requirements. Within sectors, risk management may depend on existing guidelines for specific applications and use case settings. Where established guidelines do not exist, organizations should define reasonable risk tolerance. Once tolerance is defined, this AI RMF can be used to manage risks and to document risk management processes.\n\n## 1.2.3 Risk Prioritization\n\nAttempting to eliminate negative risk entirely can be counterproductive in practice because not all incidents and failures can be eliminated. Unrealistic expectations about risk may lead organizations to allocate resources in a manner that makes risk triage inefficient or impractical or wastes scarce resources. A risk management culture can help organizations recognize that not all AI risks are the same, and resources can be allocated purposefully. Actionable risk management efforts lay out clear guidelines for assessing trustworthiness of each AI system an organization develops or deploys. Policies and resources should be prioritized based on the assessed risk level and potential impact of an AI system. The extent to which an AI system may be customized or tailored to the specific context of use by the AI deployer can be a contributing factor.\n\nWhen applying the AI RMF, risks which the organization determines to be highest for the AI systems within a given context of use call for the most urgent prioritization and most thorough risk management process. In cases where an AI system presents unacceptable negative risk levels - such as where significant negative impacts are imminent, severe harms are actually occurring, or catastrophic risks are present - development and deployment should cease in a safe manner until risks can be sufficiently managed. If an AI system's development, deployment, and use cases are found to be low-risk in a specific context, that may suggest potentially lower prioritization.\n\nRisk prioritization may differ between AI systems that are designed or deployed to directly interact with humans as compared to AI systems that are not. Higher initial prioritization may be called for in settings where the AI system is trained on large datasets comprised of sensitive or protected data such as personally identifiable information, or where the outputs of the AI systems have direct or indirect impact on humans. AI systems designed to interact only with computational systems and trained on non-sensitive datasets (for example, data collected from the physical environment) may call for lower initial prioritization. Nonetheless, regularly assessing and prioritizing risk based on context remains important because non-human-facing AI systems can have downstream safety or social implications.\n\nResidual risk - defined as risk remaining after risk treatment (Source: ISO GUIDE 73) directly impacts end users or affected individuals and communities. Documenting residual risks will call for the system provider to fully consider the risks of deploying the AI product and will inform end users about potential negative impacts of interacting with the system.\n\n## 1.2.4 Organizational Integration and Management of Risk\n\nAI risks should not be considered in isolation. Different AI actors have different responsibilities and awareness depending on their roles in the lifecycle. For example, organizations developing an AI system often will not have information about how the system may be used. AI risk management should be integrated and incorporated into broader enterprise risk management strategies and processes. Treating AI risks along with other critical risks, such as cybersecurity and privacy, will yield a more integrated outcome and organizational efficiencies.\n\nThe AI RMF may be utilized along with related guidance and frameworks for managing AI system risks or broader enterprise risks. Some risks related to AI systems are common across other types of software development and deployment. Examples of overlapping risks include: privacy concerns related to the use of underlying data to train AI systems; the energy and environmental implications associated with resource-heavy computing demands; security concerns related to the confidentiality, integrity, and availability of the system and its training and output data; and general security of the underlying software and hardware for AI systems.\n\nOrganizations need to establish and maintain the appropriate accountability mechanisms, roles and responsibilities, culture, and incentive structures for risk management to be effective. Use of the AI RMF alone will not lead to these changes or provide the appropriate incentives. Effective risk management is realized through organizational commitment at senior levels and may require cultural change within an organization or industry. In addition, small to medium-sized organizations managing AI risks or implementing the AI RMF may face different challenges than large organizations, depending on their capabilities and resources.\n\n## 2. Audience\n\nIdentifying and managing AI risks and potential impacts - both positive and negative - requires a broad set of perspectives and actors across the AI lifecycle. Ideally, AI actors will represent a diversity of experience, expertise, and backgrounds and comprise demographically and disciplinarily diverse teams. The AI RMF is intended to be used by AI actors across the AI lifecycle and dimensions.\n\nThe OECD has developed a framework for classifying AI lifecycle activities according to five key socio-technical dimensions, each with properties relevant for AI policy and governance, including risk management [OECD (2022) OECD Framework for the Classification of AI systems - OECD Digital Economy Papers]. Figure 2 shows these dimensions, slightly modified by NIST for purposes of this framework. The NIST modification highlights the importance of test, evaluation, verification, and validation (TEVV) processes throughout an AI lifecycle and generalizes the operational context of an AI system.\n\nAI dimensions displayed in Figure 2 are the Application Context, Data and Input, AI Model, and Task and Output. AI actors involved in these dimensions who perform or manage the design, development, deployment, evaluation, and use of AI systems and drive AI risk management efforts are the primary AI RMF audience.\n\nRepresentative AI actors across the lifecycle dimensions are listed in Figure 3 and described in detail in Appendix A. Within the AI RMF, all AI actors work together to manage risks and achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific expertise are integrated throughout the AI lifecycle and are especially likely to benefit from the Framework. Performed regularly, TEVV tasks can provide insights relative to technical, societal, legal, and ethical standards or norms, and can assist with anticipating impacts and assessing and tracking emergent risks. As a regular process within an AI lifecycle, TEVV allows for both mid-course remediation and post-hoc risk management.\n\nThe People &amp; Planet dimension at the center of Figure 2 represents human rights and the broader well-being of society and the planet. The AI actors in this dimension comprise a separate AI RMF audience who informs the primary audience. These AI actors may include trade associations, standards developing organizations, researchers, advocacy groups,\n\nFig. 2. Lifecycle and Key Dimensions of an AI System. Modified from OECD (2022) OECD Framework for the Classification of AI systems - OECD Digital Economy Papers. The two inner circles show AI systems' key dimensions and the outer circle shows AI lifecycle stages. Ideally, risk management efforts start with the Plan and Design function in the application context and are performed throughout the AI system lifecycle. See Figure 3 for representative AI actors.\n\n<!-- image -->\n\nenvironmental groups, civil society organizations, end users, and potentially impacted individuals and communities. These actors can:\n\n- assist in providing context and understanding potential and actual impacts;\n- be a source of formal or quasi-formal norms and guidance for AI risk management;\n- designate boundaries for AI operation (technical, societal, legal, and ethical); and\n- promote discussion of the tradeoffs needed to balance societal values and priorities related to civil liberties and rights, equity, the environment and the planet, and the economy.\n\nSuccessful risk management depends upon a sense of collective responsibility among AI actors shown in Figure 3. The AI RMF functions, described in Section 5, require diverse perspectives, disciplines, professions, and experiences. Diverse teams contribute to more open sharing of ideas and assumptions about the purposes and functions of technology making these implicit aspects more explicit. This broader collective perspective creates opportunities for surfacing problems and identifying existing and emergent risks.\n\nFig. 3. AI actors across AI lifecycle stages. See Appendix A for detailed descriptions of AI actor tasks, including details about testing, evaluation, verification, and validation tasks. Note that AI actors in the AI Model dimension (Figure 2) are separated as a best practice, with those building and using the models separated from those verifying and validating the models.\n\nPage 11\n\n## 3. AI Risks and Trustworthiness\n\nFor AI systems to be trustworthy, they often need to be responsive to a multiplicity of criteria that are of value to interested parties. Approaches which enhance AI trustworthiness can reduce negative AI risks. This Framework articulates the following characteristics of trustworthy AI and offers guidance for addressing them. Characteristics of trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. Creating trustworthy AI requires balancing each of these characteristics based on the AI system's context of use. While all characteristics are socio-technical system attributes, accountability and transparency also relate to the processes and activities internal to an AI system and its external setting. Neglecting these characteristics can increase the probability and magnitude of negative consequences.\n\nFig. 4. Characteristics of trustworthy AI systems. Valid &amp; Reliable is a necessary condition of trustworthiness and is shown as the base for other trustworthiness characteristics. Accountable &amp; Transparent is shown as a vertical box because it relates to all other characteristics.\n\n<!-- image -->\n\nTrustworthiness characteristics (shown in Figure 4) are inextricably tied to social and organizational behavior, the datasets used by AI systems, selection of AI models and algorithms and the decisions made by those who build them, and the interactions with the humans who provide insight from and oversight of such systems. Human judgment should be employed when deciding on the specific metrics related to AI trustworthiness characteristics and the precise threshold values for those metrics.\n\nAddressing AI trustworthiness characteristics individually will not ensure AI system trustworthiness; tradeoffs are usually involved, rarely do all characteristics apply in every setting, and some will be more or less important in any given situation. Ultimately, trustworthiness is a social concept that ranges across a spectrum and is only as strong as its weakest characteristics.\n\nWhen managing AI risks, organizations can face difficult decisions in balancing these characteristics. For example, in certain scenarios tradeoffs may emerge between optimizing for interpretability and achieving privacy. In other cases, organizations might face a tradeoff between predictive accuracy and interpretability. Or, under certain conditions such as data sparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions\n\nabout fairness and other values in certain domains. Dealing with tradeoffs requires taking into account the decision-making context. These analyses can highlight the existence and extent of tradeoffs between different measures, but they do not answer questions about how to navigate the tradeoff. Those depend on the values at play in the relevant context and should be resolved in a manner that is both transparent and appropriately justifiable.\n\nThere are multiple approaches for enhancing contextual awareness in the AI lifecycle. For example, subject matter experts can assist in the evaluation of TEVV findings and work with product and deployment teams to align TEVV parameters to requirements and deployment conditions. When properly resourced, increasing the breadth and diversity of input from interested parties and relevant AI actors throughout the AI lifecycle can enhance opportunities for informing contextually sensitive evaluations, and for identifying AI system benefits and positive impacts. These practices can increase the likelihood that risks arising in social contexts are managed appropriately.\n\nUnderstanding and treatment of trustworthiness characteristics depends on an AI actor's particular role within the AI lifecycle. For any given AI system, an AI designer or developer may have a different perception of the characteristics than the deployer.\n\nTrustworthiness characteristics explained in this document influence each other. Highly secure but unfair systems, accurate but opaque and uninterpretable systems, and inaccurate but secure, privacy-enhanced, and transparent systems are all undesirable. A comprehensive approach to risk management calls for balancing tradeoffs among the trustworthiness characteristics. It is the joint responsibility of all AI actors to determine whether AI technology is an appropriate or necessary tool for a given context or purpose, and how to use it responsibly. The decision to commission or deploy an AI system should be based on a contextual assessment of trustworthiness characteristics and the relative risks, impacts, costs, and benefits, and informed by a broad set of interested parties.\n\n## 3.1 Valid and Reliable\n\nValidation is the 'confirmation, through the provision of objective evidence, that the requirements for a specific intended use or application have been fulfilled' (Source: ISO 9000:2015). Deployment of AI systems which are inaccurate, unreliable, or poorly generalized to data and settings beyond their training creates and increases negative AI risks and reduces trustworthiness.\n\nReliability is defined in the same standard as the 'ability of an item to perform as required, without failure, for a given time interval, under given conditions' (Source: ISO/IEC TS 5723:2022). Reliability is a goal for overall correctness of AI system operation under the conditions of expected use and over a given period of time, including the entire lifetime of the system.\n\nAccuracy and robustness contribute to the validity and trustworthiness of AI systems, and can be in tension with one another in AI systems.\n\nAccuracy is defined by ISO/IEC TS 5723:2022 as 'closeness of results of observations, computations, or estimates to the true values or the values accepted as being true.' Measures of accuracy should consider computational-centric measures (e.g., false positive and false negative rates), human-AI teaming, and demonstrate external validity (generalizable beyond the training conditions). Accuracy measurements should always be paired with clearly defined and realistic test sets - that are representative of conditions of expected use - and details about test methodology; these should be included in associated documentation. Accuracy measurements may include disaggregation of results for different data segments.\n\nRobustness or generalizability is defined as the 'ability of a system to maintain its level of performance under a variety of circumstances' (Source: ISO/IEC TS 5723:2022). Robustness is a goal for appropriate system functionality in a broad set of conditions and circumstances, including uses of AI systems not initially anticipated. Robustness requires not only that the system perform exactly as it does under expected uses, but also that it should perform in ways that minimize potential harms to people if it is operating in an unexpected setting.\n\nValidity and reliability for deployed AI systems are often assessed by ongoing testing or monitoring that confirms a system is performing as intended. Measurement of validity, accuracy, robustness, and reliability contribute to trustworthiness and should take into consideration that certain types of failures can cause greater harm. AI risk management efforts should prioritize the minimization of potential negative impacts, and may need to include human intervention in cases where the AI system cannot detect or correct errors.\n\n## 3.2 Safe\n\nAI systems should 'not under defined conditions, lead to a state in which human life, health, property, or the environment is endangered' (Source: ISO/IEC TS 5723:2022). Safe operation of AI systems is improved through:\n\n- responsible design, development, and deployment practices;\n- clear information to deployers on responsible use of the system;\n- responsible decision-making by deployers and end users; and\n- explanations and documentation of risks based on empirical evidence of incidents.\n\nDifferent types of safety risks may require tailored AI risk management approaches based on context and the severity of potential risks presented. Safety risks that pose a potential risk of serious injury or death call for the most urgent prioritization and most thorough risk management process.\n\nEmploying safety considerations during the lifecycle and starting as early as possible with planning and design can prevent failures or conditions that can render a system dangerous. Other practical approaches for AI safety often relate to rigorous simulation and in-domain testing, real-time monitoring, and the ability to shut down, modify, or have human intervention into systems that deviate from intended or expected functionality.\n\nAI safety risk management approaches should take cues from efforts and guidelines for safety in fields such as transportation and healthcare, and align with existing sector- or application-specific guidelines or standards.\n\n## 3.3 Secure and Resilient\n\nAI systems, as well as the ecosystems in which they are deployed, may be said to be resilient if they can withstand unexpected adverse events or unexpected changes in their environment or use - or if they can maintain their functions and structure in the face of internal and external change and degrade safely and gracefully when this is necessary (Adapted from: ISO/IEC TS 5723:2022). Common security concerns relate to adversarial examples, data poisoning, and the exfiltration of models, training data, or other intellectual property through AI system endpoints. AI systems that can maintain confidentiality, integrity, and availability through protection mechanisms that prevent unauthorized access and use may be said to be secure . Guidelines in the NIST Cybersecurity Framework and Risk Management Framework are among those which are applicable here.\n\nSecurity and resilience are related but distinct characteristics. While resilience is the ability to return to normal function after an unexpected adverse event, security includes resilience but also encompasses protocols to avoid, protect against, respond to, or recover from attacks. Resilience relates to robustness and goes beyond the provenance of the data to encompass unexpected or adversarial use (or abuse or misuse) of the model or data.\n\n## 3.4 Accountable and Transparent\n\nTrustworthy AI depends upon accountability. Accountability presupposes transparency. Transparency reflects the extent to which information about an AI system and its outputs is available to individuals interacting with such a system - regardless of whether they are even aware that they are doing so. Meaningful transparency provides access to appropriate levels of information based on the stage of the AI lifecycle and tailored to the role or knowledge of AI actors or individuals interacting with or using the AI system. By promoting higher levels of understanding, transparency increases confidence in the AI system.\n\nThis characteristic's scope spans from design decisions and training data to model training, the structure of the model, its intended use cases, and how and when deployment, post-deployment, or end user decisions were made and by whom. Transparency is often necessary for actionable redress related to AI system outputs that are incorrect or otherwise lead to negative impacts. Transparency should consider human-AI interaction: for exam-\n\nple, how a human operator or user is notified when a potential or actual adverse outcome caused by an AI system is detected. A transparent system is not necessarily an accurate, privacy-enhanced, secure, or fair system. However, it is difficult to determine whether an opaque system possesses such characteristics, and to do so over time as complex systems evolve.\n\nThe role of AI actors should be considered when seeking accountability for the outcomes of AI systems. The relationship between risk and accountability associated with AI and technological systems more broadly differs across cultural, legal, sectoral, and societal contexts. When consequences are severe, such as when life and liberty are at stake, AI developers and deployers should consider proportionally and proactively adjusting their transparency and accountability practices. Maintaining organizational practices and governing structures for harm reduction, like risk management, can help lead to more accountable systems.\n\nMeasures to enhance transparency and accountability should also consider the impact of these efforts on the implementing entity, including the level of necessary resources and the need to safeguard proprietary information.\n\nMaintaining the provenance of training data and supporting attribution of the AI system's decisions to subsets of training data can assist with both transparency and accountability. Training data may also be subject to copyright and should follow applicable intellectual property rights laws.\n\nAs transparency tools for AI systems and related documentation continue to evolve, developers of AI systems are encouraged to test different types of transparency tools in cooperation with AI deployers to ensure that AI systems are used as intended.\n\n## 3.5 Explainable and Interpretable\n\nExplainability refers to a representation of the mechanisms underlying AI systems' operation, whereas interpretability refers to the meaning of AI systems' output in the context of their designed functional purposes. Together, explainability and interpretability assist those operating or overseeing an AI system, as well as users of an AI system, to gain deeper insights into the functionality and trustworthiness of the system, including its outputs. The underlying assumption is that perceptions of negative risk stem from a lack of ability to make sense of, or contextualize, system output appropriately. Explainable and interpretable AI systems offer information that will help end users understand the purposes and potential impact of an AI system.\n\nRisk from lack of explainability may be managed by describing how AI systems function, with descriptions tailored to individual differences such as the user's role, knowledge, and skill level. Explainable systems can be debugged and monitored more easily, and they lend themselves to more thorough documentation, audit, and governance.\n\nRisks to interpretability often can be addressed by communicating a description of why an AI system made a particular prediction or recommendation. (See 'Four Principles of Explainable Artificial Intelligence' and 'Psychological Foundations of Explainability and Interpretability in Artificial Intelligence' found here.)\n\nTransparency, explainability, and interpretability are distinct characteristics that support each other. Transparency can answer the question of 'what happened' in the system. Explainability can answer the question of 'how' a decision was made in the system. Interpretability can answer the question of 'why' a decision was made by the system and its meaning or context to the user.\n\n## 3.6 Privacy-Enhanced\n\nPrivacy refers generally to the norms and practices that help to safeguard human autonomy, identity, and dignity. These norms and practices typically address freedom from intrusion, limiting observation, or individuals' agency to consent to disclosure or control of facets of their identities (e.g., body, data, reputation). (See The NIST Privacy Framework: A Tool for Improving Privacy through Enterprise Risk Management.)\n\nPrivacy values such as anonymity, confidentiality, and control generally should guide choices for AI system design, development, and deployment. Privacy-related risks may influence security, bias, and transparency and come with tradeoffs with these other characteristics. Like safety and security, specific technical features of an AI system may promote or reduce privacy. AI systems can also present new risks to privacy by allowing inference to identify individuals or previously private information about individuals.\n\nPrivacy-enhancing technologies ('PETs') for AI, as well as data minimizing methods such as de-identification and aggregation for certain model outputs, can support design for privacy-enhanced AI systems. Under certain conditions such as data sparsity, privacyenhancing techniques can result in a loss in accuracy, affecting decisions about fairness and other values in certain domains.\n\n## 3.7 Fair - with Harmful Bias Managed\n\nFairness in AI includes concerns for equality and equity by addressing issues such as harmful bias and discrimination. Standards of fairness can be complex and difficult to define because perceptions of fairness differ among cultures and may shift depending on application. Organizations' risk management efforts will be enhanced by recognizing and considering these differences. Systems in which harmful biases are mitigated are not necessarily fair. For example, systems in which predictions are somewhat balanced across demographic groups may still be inaccessible to individuals with disabilities or affected by the digital divide or may exacerbate existing disparities or systemic biases.\n\nBias is broader than demographic balance and data representativeness. NIST has identified three major categories of AI bias to be considered and managed: systemic, computational and statistical, and human-cognitive. Each of these can occur in the absence of prejudice, partiality, or discriminatory intent. Systemic bias can be present in AI datasets, the organizational norms, practices, and processes across the AI lifecycle, and the broader society that uses AI systems. Computational and statistical biases can be present in AI datasets and algorithmic processes, and often stem from systematic errors due to non-representative samples. Human-cognitive biases relate to how an individual or group perceives AI system information to make a decision or fill in missing information, or how humans think about purposes and functions of an AI system. Human-cognitive biases are omnipresent in decision-making processes across the AI lifecycle and system use, including the design, implementation, operation, and maintenance of AI.\n\nBias exists in many forms and can become ingrained in the automated systems that help make decisions about our lives. While bias is not always a negative phenomenon, AI systems can potentially increase the speed and scale of biases and perpetuate and amplify harms to individuals, groups, communities, organizations, and society. Bias is tightly associated with the concepts of transparency as well as fairness in society. (For more information about bias, including the three categories, see NIST Special Publication 1270, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.)\n\n## 4. Effectiveness of the AI RMF\n\nEvaluations of AI RMF effectiveness - including ways to measure bottom-line improvements in the trustworthiness of AI systems - will be part of future NIST activities, in conjunction with the AI community.\n\nOrganizations and other users of the Framework are encouraged to periodically evaluate whether the AI RMF has improved their ability to manage AI risks, including but not limited to their policies, processes, practices, implementation plans, indicators, measurements, and expected outcomes. NIST intends to work collaboratively with others to develop metrics, methodologies, and goals for evaluating the AI RMF's effectiveness, and to broadly share results and supporting information. Framework users are expected to benefit from:\n\n- enhanced processes for governing, mapping, measuring, and managing AI risk, and clearly documenting outcomes;\n- improved awareness of the relationships and tradeoffs among trustworthiness characteristics, socio-technical approaches, and AI risks;\n- explicit processes for making go/no-go system commissioning and deployment decisions;\n- established policies, processes, practices, and procedures for improving organizational accountability efforts related to AI system risks;\n- enhanced organizational culture which prioritizes the identification and management of AI system risks and potential impacts to individuals, communities, organizations, and society;\n- better information sharing within and across organizations about risks, decisionmaking processes, responsibilities, common pitfalls, TEVV practices, and approaches for continuous improvement;\n- greater contextual knowledge for increased awareness of downstream risks;\n- strengthened engagement with interested parties and relevant AI actors; and\n- augmented capacity for TEVV of AI systems and associated risks.\n\n## Part 2: Core and Profiles\n\n## 5. AI RMF Core\n\nThe AI RMF Core provides outcomes and actions that enable dialogue, understanding, and activities to manage AI risks and responsibly develop trustworthy AI systems. As illustrated in Figure 5, the Core is composed of four functions: GOVERN, MAP, MEASURE, and MANAGE . Each of these high-level functions is broken down into categories and subcategories. Categories and subcategories are subdivided into specific actions and outcomes. Actions do not constitute a checklist, nor are they necessarily an ordered set of steps.\n\nFig. 5. Functions organize AI risk management activities at their highest level to govern, map, measure, and manage AI risks. Governance is designed to be a cross-cutting function to inform and be infused throughout the other three functions.\n\n<!-- image -->\n\nRisk management should be continuous, timely, and performed throughout the AI system lifecycle dimensions. AI RMF Core functions should be carried out in a way that reflects diverse and multidisciplinary perspectives, potentially including the views of AI actors outside the organization. Having a diverse team contributes to more open sharing of ideas and assumptions about purposes and functions of the technology being designed, developed,\n\ndeployed, or evaluated - which can create opportunities to surface problems and identify existing and emergent risks.\n\nAn online companion resource to the AI RMF, the NIST AI RMF Playbook, is available to help organizations navigate the AI RMF and achieve its outcomes through suggested tactical actions they can apply within their own contexts. Like the AI RMF, the Playbook is voluntary and organizations can utilize the suggestions according to their needs and interests. Playbook users can create tailored guidance selected from suggested material for their own use and contribute their suggestions for sharing with the broader community. Along with the AI RMF, the Playbook is part of the NIST Trustworthy and Responsible AI Resource Center.\n\nFramework users may apply these functions as best suits their needs for managing AI risks based on their resources and capabilities. Some organizations may choose to select from among the categories and subcategories; others may choose and have the capacity to apply all categories and subcategories. Assuming a governance structure is in place, functions may be performed in any order across the AI lifecycle as deemed to add value by a user of the framework. After instituting the outcomes in GOVERN , most users of the AI RMF would start with the MAP function and continue to MEASURE or MANAGE . However users integrate the functions, the process should be iterative, with cross-referencing between functions as necessary. Similarly, there are categories and subcategories with elements that apply to multiple functions, or that logically should take place before certain subcategory decisions.\n\n## 5.1 Govern\n\nThe GOVERN function:\n\n- cultivates and implements a culture of risk management within organizations designing, developing, deploying, evaluating, or acquiring AI systems;\n- outlines processes, documents, and organizational schemes that anticipate, identify, and manage the risks a system can pose, including to users and others across society - and procedures to achieve those outcomes;\n- incorporates processes to assess potential impacts;\n- provides a structure by which AI risk management functions can align with organizational principles, policies, and strategic priorities;\n- connects technical aspects of AI system design and development to organizational values and principles, and enables organizational practices and competencies for the individuals involved in acquiring, training, deploying, and monitoring such systems; and\n- addresses full product lifecycle and associated processes, including legal and other issues concerning use of third-party software or hardware systems and data.\n\nGOVERN is a cross-cutting function that is infused throughout AI risk management and enables the other functions of the process. Aspects of GOVERN , especially those related to compliance or evaluation, should be integrated into each of the other functions. Attention to governance is a continual and intrinsic requirement for effective AI risk management over an AI system's lifespan and the organization's hierarchy.\n\nStrong governance can drive and enhance internal practices and norms to facilitate organizational risk culture. Governing authorities can determine the overarching policies that direct an organization's mission, goals, values, culture, and risk tolerance. Senior leadership sets the tone for risk management within an organization, and with it, organizational culture. Management aligns the technical aspects of AI risk management to policies and operations. Documentation can enhance transparency, improve human review processes, and bolster accountability in AI system teams.\n\nAfter putting in place the structures, systems, processes, and teams described in the GOVERN function, organizations should benefit from a purpose-driven culture focused on risk understanding and management. It is incumbent on Framework users to continue to execute the GOVERN function as knowledge, cultures, and needs or expectations from AI actors evolve over time.\n\nPractices related to governing AI risks are described in the NIST AI RMF Playbook. Table 1 lists the GOVERN function's categories and subcategories.\n\nTable 1: Categories and subcategories for the GOVERN function.\n\n| Categories                                                                                                                                                                                           | Subcategories                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| GOVERN 1: Policies, processes, procedures, and practices across the organization related to the mapping, measuring, and managing of AI risks are in place, transparent, and implemented effectively. | GOVERN 1.1: Legal and regulatory requirements involving AI are understood, managed, and documented. GOVERN 1.2: The characteristics of trustworthy AI are inte- grated into organizational policies, processes, procedures, and practices. GOVERN 1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based on the organization's risk tolerance. GOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities. |\n\nContinued on next page\n\nNIST AI 100-1\n\nTable 1: Categories and subcategories for the GOVERN function. (Continued)\n\n| Categories                                                                                                                                                                            | Subcategories                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                                                                                                                                                                       | GOVERN 1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned and or- ganizational roles and responsibilities clearly defined, including determining the frequency of periodic review. GOVERN 1.6: Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities. GOVERN 1.7: Processes and procedures are in place for decom- missioning and phasing out AI systems safely and in a man- ner that does not increase risks or decrease the organization's trustworthiness.                     |\n| GOVERN 2: Accountability structures are in place so that the appropriate teams and individuals are empowered, responsible, and trained for mapping, measuring, and managing AI risks. | GOVERN 2.1: Roles and responsibilities and lines of communi- cation related to mapping, measuring, and managing AI risks are documented and are clear to individuals and teams throughout the organization. GOVERN 2.2: The organization's personnel and partners receive AI risk management training to enable them to perform their du- ties and responsibilities consistent with related policies, proce- dures, and agreements. GOVERN 2.3: Executive leadership of the organization takes re- sponsibility for decisions about risks associated with AI system development and deployment. |\n| GOVERN 3: Workforce diversity, equity, inclusion, and accessibility processes are prioritized in the mapping, measuring, and managing of AI risks throughout the lifecycle.           | GOVERN 3.1: Decision-making related to mapping, measuring, and managing AI risks throughout the lifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, expe- rience, expertise, and backgrounds). GOVERN 3.2: Policies and procedures are in place to define and differentiate roles and responsibilities for human-AI configura- tions and oversight of AI systems.                                                                                                                                                                                            |\n| GOVERN 4: Organizational teams are committed to a culture                                                                                                                             | GOVERN 4.1: Organizational policies and practices are in place to foster a critical thinking and safety-first mindset in the design, development, deployment, and uses of AI systems to minimize potential negative impacts.                                                                                                                                                                                                                                                                                                                                                                    |\n\nContinued on next page\n\nTable 1: Categories and subcategories for the GOVERN function. (Continued)\n\n| Categories                                                                                                                                                | Subcategories                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| that considers and communicates AI risk.                                                                                                                  | GOVERN 4.2: Organizational teams document the risks and po- tential impacts of the AI technology they design, develop, deploy, evaluate, and use, and they communicate about the impacts more broadly. GOVERN 4.3: Organizational practices are in place to enable AI testing, identification of incidents, and information sharing.                                                                                                                                                       |\n| GOVERN 5: Processes are in place for robust engagement with relevant AI actors.                                                                           | GOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks. GOVERN 5.2: Mechanisms are established to enable the team that developed or deployed AI systems to regularly incorporate adjudicated feedback from relevant AI actors into system design and implementation. |\n| GOVERN 6: Policies and procedures are in place to address AI risks and benefits arising from third-party software and data and other supply chain issues. | GOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of in- fringement of a third-party's intellectual property or other rights. GOVERN 6.2: Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be high-risk.                                                                                                                                          |\n\n## 5.2 Map\n\nThe MAP function establishes the context to frame risks related to an AI system. The AI lifecycle consists of many interdependent activities involving a diverse set of actors (See Figure 3). In practice, AI actors in charge of one part of the process often do not have full visibility or control over other parts and their associated contexts. The interdependencies between these activities, and among the relevant AI actors, can make it difficult to reliably anticipate impacts of AI systems. For example, early decisions in identifying purposes and objectives of an AI system can alter its behavior and capabilities, and the dynamics of deployment setting (such as end users or impacted individuals) can shape the impacts of AI system decisions. As a result, the best intentions within one dimension of the AI lifecycle can be undermined via interactions with decisions and conditions in other, later activities.\n\nThis complexity and varying levels of visibility can introduce uncertainty into risk management practices. Anticipating, assessing, and otherwise addressing potential sources of negative risk can mitigate this uncertainty and enhance the integrity of the decision process.\n\nThe information gathered while carrying out the MAP function enables negative risk prevention and informs decisions for processes such as model management, as well as an initial decision about appropriateness or the need for an AI solution. Outcomes in the MAP function are the basis for the MEASURE and MANAGE functions. Without contextual knowledge, and awareness of risks within the identified contexts, risk management is difficult to perform. The MAP function is intended to enhance an organization's ability to identify risks and broader contributing factors.\n\nImplementation of this function is enhanced by incorporating perspectives from a diverse internal team and engagement with those external to the team that developed or deployed the AI system. Engagement with external collaborators, end users, potentially impacted communities, and others may vary based on the risk level of a particular AI system, the makeup of the internal team, and organizational policies. Gathering such broad perspectives can help organizations proactively prevent negative risks and develop more trustworthy AI systems by:\n\n- improving their capacity for understanding contexts;\n- checking their assumptions about context of use;\n- enabling recognition of when systems are not functional within or out of their intended context;\n- identifying positive and beneficial uses of their existing AI systems;\n- improving understanding of limitations in AI and ML processes;\n- identifying constraints in real-world applications that may lead to negative impacts;\n- identifying known and foreseeable negative impacts related to intended use of AI systems; and\n- anticipating risks of the use of AI systems beyond intended use.\n\nAfter completing the MAP function, Framework users should have sufficient contextual knowledge about AI system impacts to inform an initial go/no-go decision about whether to design, develop, or deploy an AI system. If a decision is made to proceed, organizations should utilize the MEASURE and MANAGE functions along with policies and procedures put into place in the GOVERN function to assist in AI risk management efforts. It is incumbent on Framework users to continue applying the MAP function to AI systems as context, capabilities, risks, benefits, and potential impacts evolve over time.\n\nPractices related to mapping AI risks are described in the NIST AI RMF Playbook. Table 2 lists the MAP function's categories and subcategories.\n\nTable 2: Categories and subcategories for the MAP function.\n\n| Categories                                           | Subcategories                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n|------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| MAP 1: Context is established and understood.        | MAP 1.1: Intended purposes, potentially beneficial uses, context- specific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and docu- mented. Considerations include: the specific set or types of users along with their expectations; potential positive and negative im- pacts of system uses to individuals, communities, organizations, society, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or product AI lifecycle; and related TEVV and system metrics. MAP 1.2: Interdisciplinary AI actors, competencies, skills, and capacities for establishing context reflect demographic diversity and broad domain and user experience expertise, and their par- ticipation is documented. Opportunities for interdisciplinary col- laboration are prioritized. MAP 1.3: The organization's mission and relevant goals for AI technology are understood and documented. MAP 1.4: The business value or context of business use has been clearly defined or - in the case of assessing existing AI systems - re-evaluated. MAP 1.5: Organizational risk tolerances are determined and documented. MAP 1.6: System requirements (e.g., 'the system shall respect the privacy of its users') are elicited from and understood by rel- evant AI actors. Design decisions take socio-technical implica- |\n| MAP 2: Categorization of the AI system is performed. | MAP 2.1: The specific tasks and methods used to implement the tasks that the AI system will support are defined (e.g., classifiers, generative models, recommenders). MAP 2.2: Information about the AI system's knowledge limits and how system output may be utilized and overseen by humans is documented. Documentation provides sufficient information to assist relevant AI actors when making decisions and taking subsequent actions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n\nContinued on next page\n\nTable 2: Categories and subcategories for the MAP function. (Continued)\n\n| Categories                                                                                                                          | Subcategories                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n|-------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                                                                                                                     | MAP 2.3: Scientific integrity and TEVV considerations are iden- tified and documented, including those related to experimental design, data collection and selection (e.g., availability, repre- sentativeness, suitability), system trustworthiness, and construct validation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| MAP 3: AI capabilities, targeted usage, goals, and expected benefits and costs compared with appropriate benchmarks are understood. | MAP 3.1: Potential benefits of intended AI system functionality and performance are examined and documented. MAP 3.2: Potential costs, including non-monetary costs, which result from expected or realized AI errors or system functionality and trustworthiness - as connected to organizational risk toler- ance - are examined and documented. MAP 3.3: Targeted application scope is specified and docu- mented based on the system's capability, established context, and AI system categorization. MAP 3.4: Processes for operator and practitioner proficiency with AI system performance and trustworthiness - and relevant technical standards and certifications - are defined, assessed, and documented. MAP 3.5: Processes for human oversight are defined, assessed, and documented in accordance with organizational policies from the GOVERN function. |\n| MAP 4: Risks and benefits are mapped for all components of the AI system including third-party software and data.                   | MAP 4.1: Approaches for mapping AI technology and legal risks of its components - including the use of third-party data or soft- ware - are in place, followed, and documented, as are risks of in- fringement of a third party's intellectual property or other rights. MAP 4.2: Internal risk controls for components of the AI sys- tem, including third-party AI technologies, are identified and documented.                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| MAP 5: Impacts to individuals, groups, communities, organizations, and society are characterized.                                   | MAP 5.1: Likelihood and magnitude of each identified impact (both potentially beneficial and harmful) based on expected use, past uses of AI systems in similar contexts, public incident re- ports, feedback from those external to the team that developed or deployed the AI system, or other data are identified and documented.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n\nContinued on next page\n\nTable 2: Categories and subcategories for the MAP function. (Continued)\n\n| Categories   | Subcategories                                                                                                                                                                                          |\n|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|              | MAP 5.2: Practices and personnel for supporting regular en- gagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented. |\n\n## 5.3 Measure\n\nThe MEASURE function employs quantitative, qualitative, or mixed-method tools, techniques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related impacts. It uses knowledge relevant to AI risks identified in the MAP function and informs the MANAGE function. AI systems should be tested before their deployment and regularly while in operation. AI risk measurements include documenting aspects of systems' functionality and trustworthiness.\n\nMeasuring AI risks includes tracking metrics for trustworthy characteristics, social impact, and human-AI configurations. Processes developed or adopted in the MEASURE function should include rigorous software testing and performance assessment methodologies with associated measures of uncertainty, comparisons to performance benchmarks, and formalized reporting and documentation of results. Processes for independent review can improve the effectiveness of testing and can mitigate internal biases and potential conflicts of interest.\n\nWhere tradeoffs among the trustworthy characteristics arise, measurement provides a traceable basis to inform management decisions. Options may include recalibration, impact mitigation, or removal of the system from design, development, production, or use, as well as a range of compensating, detective, deterrent, directive, and recovery controls.\n\nAfter completing the MEASURE function, objective, repeatable, or scalable test, evaluation, verification, and validation (TEVV) processes including metrics, methods, and methodologies are in place, followed, and documented. Metrics and measurement methodologies should adhere to scientific, legal, and ethical norms and be carried out in an open and transparent process. New types of measurement, qualitative and quantitative, may need to be developed. The degree to which each measurement type provides unique and meaningful information to the assessment of AI risks should be considered. Framework users will enhance their capacity to comprehensively evaluate system trustworthiness, identify and track existing and emergent risks, and verify efficacy of the metrics. Measurement outcomes will be utilized in the MANAGE function to assist risk monitoring and response efforts. It is incumbent on Framework users to continue applying the MEASURE function to AI systems as knowledge, methodologies, risks, and impacts evolve over time.\n\nPractices related to measuring AI risks are described in the NIST AI RMF Playbook. Table 3 lists the MEASURE function's categories and subcategories.\n\nTable 3: Categories and subcategories for the MEASURE function.\n\n| Categories                                                             | Subcategories                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| MEASURE 1: Appropriate methods and metrics are identified and applied. | MEASURE 1.1: Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for imple- mentation starting with the most significant AI risks. The risks or trustworthiness characteristics that will not - or cannot - be measured are properly documented. MEASURE 1.2: Appropriateness of AI metrics and effectiveness of existing controls are regularly assessed and updated, including reports of errors and potential impacts on affected communities. MEASURE 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are in- volved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance. |\n| MEASURE 2: AI systems are evaluated for trustworthy characteristics.   | MEASURE 2.1: Test sets, metrics, and details about the tools used during TEVV are documented. MEASURE 2.2: Evaluations involving human subjects meet ap- plicable requirements (including human subject protection) and are representative of the relevant population. MEASURE 2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented. MEASURE 2.4: The functionality and behavior of the AI sys- tem and its components - as identified in the MAP function - are monitored when in production. MEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability be- yond the conditions under which the technology was developed are documented.                                                |\n\nContinued on next page\n\nTable 3: Categories and subcategories for the MEASURE function. (Continued)\n\n| Categories                                                                     | Subcategories                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n|--------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                                                                | MEASURE 2.6: The AI system is evaluated regularly for safety risks - as identified in the MAP function. The AI system to be de- ployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics re- flect system reliability and robustness, real-time monitoring, and response times for AI system failures. MEASURE 2.7: AI system security and resilience - as identified in the MAP function - are evaluated and documented. MEASURE 2.8: Risks associated with transparency and account- ability - as identified in the MAP function - are examined and documented. MEASURE 2.9: The AI model is explained, validated, and docu- mented, and AI system output is interpreted within its context - as identified in the MAP function - to inform responsible use and governance. MEASURE 2.10: Privacy risk of the AI system - as identified in the MAP function - is examined and documented. MEASURE 2.11: Fairness and bias - as identified in the MAP function - are evaluated and results are documented. MEASURE 2.12: Environmental impact and sustainability of AI model training and management activities - as identified in the MAP function - are assessed and documented. MEASURE 2.13: Effectiveness of the employed TEVV met- rics and processes in the MEASURE function are evaluated and |\n| MEASURE 3: Mechanisms for tracking identified AI risks over time are in place. | MEASURE 3.1: Approaches, personnel, and documentation are in place to regularly identify and track existing, unanticipated, and emergent AI risks based on factors such as intended and ac- tual performance in deployed contexts. MEASURE 3.2: Risk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n\nContinued on next page\n\nTable 3: Categories and subcategories for the MEASURE function. (Continued)\n\n| Categories                                                                  | Subcategories                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n|-----------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                                                             | MEASURE 3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into AI system evaluation metrics.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| MEASURE 4: Feedback about efficacy of measurement is gathered and assessed. | MEASURE 4.1: Measurement approaches for identifying AI risks are connected to deployment context(s) and informed through consultation with domain experts and other end users. Ap- proaches are documented. MEASURE 4.2: Measurement results regarding AI system trust- worthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI ac- tors to validate whether the system is performing consistently as intended. Results are documented. MEASURE 4.3: Measurable performance improvements or de- clines based on consultations with relevant AI actors, in- cluding affected communities, and field data about context- relevant risks and trustworthiness characteristics are identified and documented. |\n\n## 5.4 Manage\n\nThe MANAGE function entails allocating risk resources to mapped and measured risks on a regular basis and as defined by the GOVERN function. Risk treatment comprises plans to respond to, recover from, and communicate about incidents or events.\n\nContextual information gleaned from expert consultation and input from relevant AI actors - established in GOVERN and carried out in MAP - is utilized in this function to decrease the likelihood of system failures and negative impacts. Systematic documentation practices established in GOVERN and utilized in MAP and MEASURE bolster AI risk management efforts and increase transparency and accountability. Processes for assessing emergent risks are in place, along with mechanisms for continual improvement.\n\nAfter completing the MANAGE function, plans for prioritizing risk and regular monitoring and improvement will be in place. Framework users will have enhanced capacity to manage the risks of deployed AI systems and to allocate risk management resources based on assessed and prioritized risks. It is incumbent on Framework users to continue to apply the MANAGE function to deployed AI systems as methods, contexts, risks, and needs or expectations from relevant AI actors evolve over time.\n\nPractices related to managing AI risks are described in the NIST AI RMF Playbook. Table 4 lists the MANAGE function's categories and subcategories.\n\nTable 4: Categories and subcategories for the MANAGE function.\n\n| Categories                                                                                                                                                                | Subcategories                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| MANAGE 1: AI risks based on assessments and other analytical output from the MAP and MEASURE functions are prioritized, responded to, and managed.                        | MANAGE 1.1: A determination is made as to whether the AI system achieves its intended purposes and stated objectives and whether its development or deployment should proceed. MANAGE 1.2: Treatment of documented AI risks is prioritized based on impact, likelihood, and available resources or methods. MANAGE 1.3: Responses to the AI risks deemed high priority, as identified by the MAP function, are developed, planned, and doc- umented. Risk response options can include mitigating, transfer- ring, avoiding, or accepting. MANAGE 1.4: Negative residual risks (defined as the sum of all unmitigated risks) to both downstream acquirers of AI systems and end users are documented. |\n| MANAGE 2: Strategies to maximize AI benefits and minimize negative impacts are planned, prepared, implemented, documented, and informed by input from relevant AI actors. | MANAGE 2.1: Resources required to manage AI risks are taken into account - along with viable non-AI alternative systems, ap- proaches, or methods - to reduce the magnitude or likelihood of potential impacts. MANAGE 2.2: Mechanisms are in place and applied to sustain the value of deployed AI systems. MANAGE 2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identified. MANAGE 2.4: Mechanisms are in place and applied, and respon- sibilities are assigned and understood, to supersede, disengage, or deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use.                                      |\n| MANAGE 3: AI risks and benefits from third-party entities are managed.                                                                                                    | MANAGE 3.1: AI risks and benefits from third-party resources are regularly monitored, and risk controls are applied and documented. MANAGE 3.2: Pre-trained models which are used for develop- ment are monitored as part of AI system regular monitoring and maintenance.                                                                                                                                                                                                                                                                                                                                                                                                                            |\n\nContinued on next page\n\nTable 4: Categories and subcategories for the MANAGE function. (Continued)\n\n| Categories                                                                                                                                                           | Subcategories                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| MANAGE 4: Risk treatments, including response and recovery, and communication plans for the identified and measured AI risks are documented and monitored regularly. | MANAGE 4.1: Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and eval- uating input from users and other relevant AI actors, appeal and override, decommissioning, incident response, recovery, and change management. MANAGE 4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular engage- ment with interested parties, including relevant AI actors. MANAGE 4.3: Incidents and errors are communicated to relevant AI actors, including affected communities. Processes for track- ing, responding to, and recovering from incidents and errors are followed and documented. |\n\n## 6. AI RMF Profiles\n\nAI RMF use-case profiles are implementations of the AI RMF functions, categories, and subcategories for a specific setting or application based on the requirements, risk tolerance, and resources of the Framework user: for example, an AI RMF hiring profile or an AI RMF fair housing profile . Profiles may illustrate and offer insights into how risk can be managed at various stages of the AI lifecycle or in specific sector, technology, or end-use applications. AI RMF profiles assist organizations in deciding how they might best manage AI risk that is well-aligned with their goals, considers legal/regulatory requirements and best practices, and reflects risk management priorities.\n\nAI RMF temporal profiles are descriptions of either the current state or the desired, target state of specific AI risk management activities within a given sector, industry, organization, or application context. An AI RMF Current Profile indicates how AI is currently being managed and the related risks in terms of current outcomes. A Target Profile indicates the outcomes needed to achieve the desired or target AI risk management goals.\n\nComparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk management objectives. Action plans can be developed to address these gaps to fulfill outcomes in a given category or subcategory. Prioritization of gap mitigation is driven by the user's needs and risk management processes. This risk-based approach also enables Framework users to compare their approaches with other approaches and to gauge the resources needed (e.g., staffing, funding) to achieve AI risk management goals in a costeffective, prioritized manner.\n\nAI RMF cross-sectoral profiles cover risks of models or applications that can be used across use cases or sectors. Cross-sectoral profiles can also cover how to govern, map, measure, and manage risks for activities or business processes common across sectors such as the use of large language models, cloud-based services or acquisition.\n\nThis Framework does not prescribe profile templates, allowing for flexibility in implementation.\n\n## Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3\n\nAI Design tasks are performed during the Application Context and Data and Input phases of the AI lifecycle in Figure 2. AI Design actors create the concept and objectives of AI systems and are responsible for the planning, design, and data collection and processing tasks of the AI system so that the AI system is lawful and fit-for-purpose. Tasks include articulating and documenting the system's concept and objectives, underlying assumptions, context, and requirements; gathering and cleaning data; and documenting the metadata and characteristics of the dataset. AI actors in this category include data scientists, domain experts, socio-cultural analysts, experts in the field of diversity, equity, inclusion, and accessibility, members of impacted communities, human factors experts (e.g., UX/UI design), governance experts, data engineers, data providers, system funders, product managers, third-party entities, evaluators, and legal and privacy governance.\n\nAI Development tasks are performed during the AI Model phase of the lifecycle in Figure 2. AI Development actors provide the initial infrastructure of AI systems and are responsible for model building and interpretation tasks, which involve the creation, selection, calibration, training, and/or testing of models or algorithms. AI actors in this category include machine learning experts, data scientists, developers, third-party entities, legal and privacy governance experts, and experts in the socio-cultural and contextual factors associated with the deployment setting.\n\nAI Deployment tasks are performed during the Task and Output phase of the lifecycle in Figure 2. AI Deployment actors are responsible for contextual decisions relating to how the AI system is used to assure deployment of the system into production. Related tasks include piloting the system, checking compatibility with legacy systems, ensuring regulatory compliance, managing organizational change, and evaluating user experience. AI actors in this category include system integrators, software developers, end users, operators and practitioners, evaluators, and domain experts with expertise in human factors, socio-cultural analysis, and governance.\n\nOperation and Monitoring tasks are performed in the Application Context/Operate and Monitor phase of the lifecycle in Figure 2. These tasks are carried out by AI actors who are responsible for operating the AI system and working with others to regularly assess system output and impacts. AI actors in this category include system operators, domain experts, AI designers, users who interpret or incorporate the output of AI systems, product developers, evaluators and auditors, compliance experts, organizational management, and members of the research community.\n\nTest, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout the AI lifecycle. They are carried out by AI actors who examine the AI system or its components, or detect and remediate problems. Ideally, AI actors carrying out verification\n\nand validation tasks are distinct from those who perform test and evaluation actions. Tasks can be incorporated into a phase as early as design, where tests are planned in accordance with the design requirement.\n\n- TEVV tasks for design, planning, and data may center on internal and external validation of assumptions for system design, data collection, and measurements relative to the intended context of deployment or application.\n- TEVV tasks for development (i.e., model building) include model validation and assessment.\n- TEVV tasks for deployment include system validation and integration in production, with testing, and recalibration for systems and process integration, user experience, and compliance with existing legal, regulatory, and ethical specifications.\n- TEVV tasks for operations involve ongoing monitoring for periodic updates, testing, and subject matter expert (SME) recalibration of models, the tracking of incidents or errors reported and their management, the detection of emergent properties and related impacts, and processes for redress and response.\n\nHuman Factors tasks and activities are found throughout the dimensions of the AI lifecycle. They include human-centered design practices and methodologies, promoting the active involvement of end users and other interested parties and relevant AI actors, incorporating context-specific norms and values in system design, evaluating and adapting end user experiences, and broad integration of humans and human dynamics in all phases of the AI lifecycle. Human factors professionals provide multidisciplinary skills and perspectives to understand context of use, inform interdisciplinary and demographic diversity, engage in consultative processes, design and evaluate user experience, perform human-centered evaluation and testing, and inform impact assessments.\n\nDomain Expert tasks involve input from multidisciplinary practitioners or scholars who provide knowledge or expertise in - and about - an industry sector, economic sector, context, or application area where an AI system is being used. AI actors who are domain experts can provide essential guidance for AI system design and development, and interpret outputs in support of work performed by TEVV and AI impact assessment teams.\n\nAI Impact Assessment tasks include assessing and evaluating requirements for AI system accountability, combating harmful bias, examining impacts of AI systems, product safety, liability, and security, among others. AI actors such as impact assessors and evaluators provide technical, human factor, socio-cultural, and legal expertise.\n\nProcurement tasks are conducted by AI actors with financial, legal, or policy management authority for acquisition of AI models, products, or services from a third-party developer, vendor, or contractor.\n\nGovernance and Oversight tasks are assumed by AI actors with management, fiduciary, and legal authority and responsibility for the organization in which an AI system is de-\n\nsigned, developed, and/or deployed. Key AI actors responsible for AI governance include organizational management, senior leadership, and the Board of Directors. These actors are parties that are concerned with the impact and sustainability of the organization as a whole.\n\n## Additional AI Actors\n\nThird-party entities include providers, developers, vendors, and evaluators of data, algorithms, models, and/or systems and related services for another organization or the organization's customers or clients. Third-party entities are responsible for AI design and development tasks, in whole or in part. By definition, they are external to the design, development, or deployment team of the organization that acquires its technologies or services. The technologies acquired from third-party entities may be complex or opaque, and risk tolerances may not align with the deploying or operating organization.\n\nEnd users of an AI system are the individuals or groups that use the system for specific purposes. These individuals or groups interact with an AI system in a specific context. End users can range in competency from AI experts to first-time technology end users.\n\nAffected individuals/communities encompass all individuals, groups, communities, or organizations directly or indirectly affected by AI systems or decisions based on the output of AI systems. These individuals do not necessarily interact with the deployed system or application.\n\nOther AI actors may provide formal or quasi-formal norms or guidance for specifying and managing AI risks. They can include trade associations, standards developing organizations, advocacy groups, researchers, environmental groups, and civil society organizations .\n\nThe general public is most likely to directly experience positive and negative impacts of AI technologies. They may provide the motivation for actions taken by the AI actors. This group can include individuals, communities, and consumers associated with the context in which an AI system is developed or deployed.\n\n## Appendix B: How AI Risks Differ from Traditional Software Risks\n\nAs with traditional software, risks from AI-based technology can be bigger than an enterprise, span organizations, and lead to societal impacts. AI systems also bring a set of risks that are not comprehensively addressed by current risk frameworks and approaches. Some AI system features that present risks also can be beneficial. For example, pre-trained models and transfer learning can advance research and increase accuracy and resilience when compared to other models and approaches. Identifying contextual factors in the MAP function will assist AI actors in determining the level of risk and potential management efforts.\n\nCompared to traditional software, AI-specific risks that are new or increased include the following:\n\n- The data used for building an AI system may not be a true or appropriate representation of the context or intended use of the AI system, and the ground truth may either not exist or not be available. Additionally, harmful bias and other data quality issues can affect AI system trustworthiness, which could lead to negative impacts.\n- AI system dependency and reliance on data for training tasks, combined with increased volume and complexity typically associated with such data.\n- Intentional or unintentional changes during training may fundamentally alter AI system performance.\n- Datasets used to train AI systems may become detached from their original and intended context or may become stale or outdated relative to deployment context.\n- AI system scale and complexity (many systems contain billions or even trillions of decision points) housed within more traditional software applications.\n- Use of pre-trained models that can advance research and improve performance can also increase levels of statistical uncertainty and cause issues with bias management, scientific validity, and reproducibility.\n- Higher degree of difficulty in predicting failure modes for emergent properties of large-scale pre-trained models.\n- Privacy risk due to enhanced data aggregation capability for AI systems.\n- AI systems may require more frequent maintenance and triggers for conducting corrective maintenance due to data, model, or concept drift.\n- Increased opacity and concerns about reproducibility.\n- Underdeveloped software testing standards and inability to document AI-based practices to the standard expected of traditionally engineered software for all but the simplest of cases.\n- Difficulty in performing regular AI-based software testing, or determining what to test, since AI systems are not subject to the same controls as traditional code development.\n\n- Computational costs for developing AI systems and their impact on the environment and planet.\n- Inability to predict or detect the side effects of AI-based systems beyond statistical measures.\n\nPrivacy and cybersecurity risk management considerations and approaches are applicable in the design, development, deployment, evaluation, and use of AI systems. Privacy and cybersecurity risks are also considered as part of broader enterprise risk management considerations, which may incorporate AI risks. As part of the effort to address AI trustworthiness characteristics such as 'Secure and Resilient' and 'Privacy-Enhanced,' organizations may consider leveraging available standards and guidance that provide broad guidance to organizations to reduce security and privacy risks, such as, but not limited to, the NIST Cybersecurity Framework, the NIST Privacy Framework, the NIST Risk Management Framework, and the Secure Software Development Framework. These frameworks have some features in common with the AI RMF. Like most risk management approaches, they are outcome-based rather than prescriptive and are often structured around a Core set of functions, categories, and subcategories. While there are significant differences between these frameworks based on the domain addressed - and because AI risk management calls for addressing many other types of risks - frameworks like those mentioned above may inform security and privacy considerations in the MAP , MEASURE , and MANAGE functions of the AI RMF.\n\nAt the same time, guidance available before publication of this AI RMF does not comprehensively address many AI system risks. For example, existing frameworks and guidance are unable to:\n\n- adequately manage the problem of harmful bias in AI systems;\n- confront the challenging risks related to generative AI;\n- comprehensively address security concerns related to evasion, model extraction, membership inference, availability, or other machine learning attacks;\n- account for the complex attack surface of AI systems or other security abuses enabled by AI systems; and\n- consider risks associated with third-party AI technologies, transfer learning, and offlabel use where AI systems may be trained for decision-making outside an organization's security controls or trained in one domain and then 'fine-tuned' for another.\n\nBoth AI and traditional software technologies and systems are subject to rapid innovation. Technology advances should be monitored and deployed to take advantage of those developments and work towards a future of AI that is both trustworthy and responsible.\n\n## Appendix C: AI Risk Management and Human-AI Interaction\n\nOrganizations that design, develop, or deploy AI systems for use in operational settings may enhance their AI risk management by understanding current limitations of humanAI interaction. The AI RMF provides opportunities to clearly define and differentiate the various human roles and responsibilities when using, interacting with, or managing AI systems.\n\nMany of the data-driven approaches that AI systems rely on attempt to convert or represent individual and social observational and decision-making practices into measurable quantities. Representing complex human phenomena with mathematical models can come at the cost of removing necessary context. This loss of context may in turn make it difficult to understand individual and societal impacts that are key to AI risk management efforts.\n\nIssues that merit further consideration and research include:\n\n1. Human roles and responsibilities in decision making and overseeing AI systems need to be clearly defined and differentiated. Human-AI configurations can span from fully autonomous to fully manual. AI systems can autonomously make decisions, defer decision making to a human expert, or be used by a human decision maker as an additional opinion. Some AI systems may not require human oversight, such as models used to improve video compression. Other systems may specifically require human oversight.\n2. Decisions that go into the design, development, deployment, evaluation, and use of AI systems reflect systemic and human cognitive biases. AI actors bring their cognitive biases, both individual and group, into the process. Biases can stem from end-user decision-making tasks and be introduced across the AI lifecycle via human assumptions, expectations, and decisions during design and modeling tasks. These biases, which are not necessarily always harmful, may be exacerbated by AI system opacity and the resulting lack of transparency. Systemic biases at the organizational level can influence how teams are structured and who controls the decision-making processes throughout the AI lifecycle. These biases can also influence downstream decisions by end users, decision makers, and policy makers and may lead to negative impacts.\n3. Human-AI interaction results vary. Under certain conditions - for example, in perceptual-based judgment tasks - the AI part of the human-AI interaction can amplify human biases, leading to more biased decisions than the AI or human alone. When these variations are judiciously taken into account in organizing human-AI teams, however, they can result in complementarity and improved overall performance.\n\n4. Presenting AI system information to humans is complex. Humans perceive and derive meaning from AI system output and explanations in different ways, reflecting different individual preferences, traits, and skills.\n\nThe GOVERN function provides organizations with the opportunity to clarify and define the roles and responsibilities for the humans in the Human-AI team configurations and those who are overseeing the AI system performance. The GOVERN function also creates mechanisms for organizations to make their decision-making processes more explicit, to help counter systemic biases.\n\nThe MAP function suggests opportunities to define and document processes for operator and practitioner proficiency with AI system performance and trustworthiness concepts, and to define relevant technical standards and certifications. Implementing MAP function categories and subcategories may help organizations improve their internal competency for analyzing context, identifying procedural and system limitations, exploring and examining impacts of AI-based systems in the real world, and evaluating decision-making processes throughout the AI lifecycle.\n\nThe GOVERN and MAP functions describe the importance of interdisciplinarity and demographically diverse teams and utilizing feedback from potentially impacted individuals and communities. AI actors called out in the AI RMF who perform human factors tasks and activities can assist technical teams by anchoring in design and development practices to user intentions and representatives of the broader AI community, and societal values. These actors further help to incorporate context-specific norms and values in system design and evaluate end user experiences - in conjunction with AI systems.\n\nAI risk management approaches for human-AI configurations will be augmented by ongoing research and evaluation. For example, the degree to which humans are empowered and incentivized to challenge AI system output requires further studies. Data about the frequency and rationale with which humans overrule AI system output in deployed systems may be useful to collect and analyze.\n\n## Appendix D: Attributes of the AI RMF\n\nNIST described several key attributes of the AI RMF when work on the Framework first began. These attributes have remained intact and were used to guide the AI RMF's development. They are provided here as a reference.\n\nThe AI RMF strives to:\n\n1. Be risk-based, resource-efficient, pro-innovation, and voluntary.\n2. Be consensus-driven and developed and regularly updated through an open, transparent process. All stakeholders should have the opportunity to contribute to the AI RMF's development.\n3. Use clear and plain language that is understandable by a broad audience, including senior executives, government officials, non-governmental organization leadership, and those who are not AI professionals - while still of sufficient technical depth to be useful to practitioners. The AI RMF should allow for communication of AI risks across an organization, between organizations, with customers, and to the public at large.\n4. Provide common language and understanding to manage AI risks. The AI RMF should offer taxonomy, terminology, definitions, metrics, and characterizations for AI risk.\n5. Be easily usable and fit well with other aspects of risk management. Use of the Framework should be intuitive and readily adaptable as part of an organization's broader risk management strategy and processes. It should be consistent or aligned with other approaches to managing AI risks.\n6. Be useful to a wide range of perspectives, sectors, and technology domains. The AI RMF should be universally applicable to any AI technology and to context-specific use cases.\n7. Be outcome-focused and non-prescriptive. The Framework should provide a catalog of outcomes and approaches rather than prescribe one-size-fits-all requirements.\n8. Take advantage of and foster greater awareness of existing standards, guidelines, best practices, methodologies, and tools for managing AI risks - as well as illustrate the need for additional, improved resources.\n9. Be law- and regulation-agnostic. The Framework should support organizations' abilities to operate under applicable domestic and international legal or regulatory regimes.\n10. Be a living document. The AI RMF should be readily updated as technology, understanding, and approaches to AI trustworthiness and uses of AI change and as stakeholders learn from implementing AI risk management generally and this framework in particular.\n\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T13:34:15Z", "sha256": "8840ff2438cf0965c4cebfb1adba5e66883bf261d7051583a01a972083813983", "meta": {"file_name": "AI Risk Management Framework - NIST.pdf", "file_size": 1946127, "mtime": 1766930197, "docling_errors": []}}
{"doc_id": "pdf-pdfs-ai-risk-management-framework-playbook-nist-1a939802ec83", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Risk Management Framework Playbook - NIST.pdf", "title": "AI Risk Management Framework Playbook - NIST", "text": "AI RMF AI RMF PLAYBOOK PLAYBOOK\n\n<!-- image -->\n\n- Table of Contents GOVERN ...........................................................................................................................................................................  4 GOVERN 1.1 .....................................................................................................................................................................................4 GOVERN 1.2 .....................................................................................................................................................................................5 GOVERN 1.3 .....................................................................................................................................................................................7 GOVERN 1.4 .....................................................................................................................................................................................9 GOVERN 1.5 ..................................................................................................................................................................................  11 GOVERN 1.6 ..................................................................................................................................................................................  12 GOVERN 1.7 ..................................................................................................................................................................................  13 GOVERN 2.1 ..................................................................................................................................................................................  15 GOVERN 2.2 ..................................................................................................................................................................................  17 GOVERN 2.3 ..................................................................................................................................................................................  18 GOVERN 3.1 ..................................................................................................................................................................................  19 GOVERN 3.2 ..................................................................................................................................................................................  21 GOVERN 4.1 ..................................................................................................................................................................................  23 GOVERN 4.2 ..................................................................................................................................................................................  24 GOVERN 4.3 ..................................................................................................................................................................................  27 GOVERN 5.1 ..................................................................................................................................................................................  28 GOVERN 5.2 ..................................................................................................................................................................................  30 GOVERN 6.1 ..................................................................................................................................................................................  32 GOVERN 6.2 ..................................................................................................................................................................................  33 MANAGE........................................................................................................................................................................  35 MANAGE 1.1 .................................................................................................................................................................................  35 MANAGE 1.2 .................................................................................................................................................................................  36 MANAGE 1.3 .................................................................................................................................................................................  37 MANAGE 1.4 .................................................................................................................................................................................  39 MANAGE 2.1 .................................................................................................................................................................................  40 MANAGE 2.2 .................................................................................................................................................................................  42 MANAGE 2.3 .................................................................................................................................................................................  48 MANAGE 2.4 .................................................................................................................................................................................  49 MANAGE 3.1 .................................................................................................................................................................................  51 MANAGE 3.2 .................................................................................................................................................................................  52 MANAGE 4.1 .................................................................................................................................................................................  53 MANAGE 4.2 .................................................................................................................................................................................  54 MANAGE 4.3 .................................................................................................................................................................................  56 MAP ................................................................................................................................................................................  58 MAP 1.1...........................................................................................................................................................................................  58 MAP 1.2...........................................................................................................................................................................................  62 MAP 1.3...........................................................................................................................................................................................  63 MAP 1.4...........................................................................................................................................................................................  65 MAP 1.5...........................................................................................................................................................................................  66 MAP 1.6...........................................................................................................................................................................................  68 MAP 2.1...........................................................................................................................................................................................  70\n\n## MAP 2.2...........................................................................................................................................................................................  71 MAP 2.3...........................................................................................................................................................................................  74 MAP 3.1...........................................................................................................................................................................................  77 MAP 3.2...........................................................................................................................................................................................  79 MAP 3.3...........................................................................................................................................................................................  80 MAP 3.4...........................................................................................................................................................................................  82 MAP 3.5...........................................................................................................................................................................................  84 MAP 4.1...........................................................................................................................................................................................  86 MAP 4.2...........................................................................................................................................................................................  88 MAP 5.1...........................................................................................................................................................................................  89 MAP 5.2...........................................................................................................................................................................................  90 MEASURE......................................................................................................................................................................  93 MEASURE 1.1  ...............................................................................................................................................................................  93 MEASURE 1.2  ...............................................................................................................................................................................  95 MEASURE 1.3  ...............................................................................................................................................................................  96 MEASURE 2.1  ...............................................................................................................................................................................  98 MEASURE 2.2  ...............................................................................................................................................................................  99 MEASURE 2.3  ............................................................................................................................................................................  102 MEASURE 2.4  ............................................................................................................................................................................  104 MEASURE 2.5  ............................................................................................................................................................................  106 MEASURE 2.6  ............................................................................................................................................................................  108 MEASURE 2.7  ............................................................................................................................................................................  110 MEASURE 2.8  ............................................................................................................................................................................  112 MEASURE 2.9  ............................................................................................................................................................................  115 MEASURE 2.10 .........................................................................................................................................................................  118 MEASURE 2.11 .........................................................................................................................................................................  121 MEASURE 2.12 .........................................................................................................................................................................  126 MEASURE 2.13 .........................................................................................................................................................................  128 MEASURE 3.1  ............................................................................................................................................................................  129 MEASURE 3.2  ............................................................................................................................................................................  131 MEASURE 3.3  ............................................................................................................................................................................  132 MEASURE 4.1  ............................................................................................................................................................................  134 MEASURE 4.2  ............................................................................................................................................................................  137 MEASURE 4.3  ............................................................................................................................................................................  140\n\n## About the Playbook FORWARD\n\nThe Playbook provides suggested actions for achieving the outcomes laid out in the AI Risk Management Framework (AI RMF) Core (Tables 1 - 4 in AI RMF 1.0). Suggestions are aligned to each sub-category within the four AI RMF functions (Govern, Map, Measure, Manage).\n\nThe Playbook is neither a checklist nor set of steps to be followed in its entirety.\n\nPlaybook suggestions are voluntary. Organizations may utilize this information by borrowing as many - or as few - suggestions as apply to their industry use case or interests.\n\nGovern\n\nMap\n\nMeasure\n\nManage\n\n## GOVERN\n\n<!-- image -->\n\n## Govern\n\nPolicies, processes, procedures and practices across the organization related to the mapping, measuring and managing of AI risks are in place, transparent, and implemented effectively.\n\n## GOVERN 1.1\n\nLegal and regulatory requirements involving AI are understood, managed, and documented.\n\n## About\n\nAI systems may be subject to specific applicable legal and regulatory requirements. Some legal requirements can mandate (e.g., nondiscrimination, data privacy and security controls) documentation, disclosure, and increased AI system transparency. These requirements are complex and may not be applicable or differ across applications and contexts. For example, AI system testing processes for bias measurement, such as disparate impact, are not applied uniformly within the legal context. Disparate impact is broadly defined as a facially neutral policy or practice that disproportionately harms a group based on a protected trait. Notably, some modeling algorithms or debiasing techniques that rely on demographic information, could also come into tension with legal prohibitions on disparate treatment (i.e., intentional discrimination). Additionally, some intended users of AI systems may not have consistent or reliable access to fundamental internet technologies (a phenomenon widely described as the 'digital divide') or may experience difficulties interacting with AI systems due to disabilities or impairments. Such factors may mean different communities experience bias or other negative impacts when trying to access AI systems. Failure to address such design issues may pose legal risks, for example in employment related activities affecting persons with disabilities.\n\n## Suggested Actions\n\n- Maintain awareness of the applicable legal and regulatory considerations and requirements specific to industry, sector, and business purpose, as well as the application context of the deployed AI system.\n- Align risk management efforts with applicable legal standards.\n- Maintain policies for training (and re-training) organizational staff about necessary legal or regulatory considerations that may impact AI-related design, development and deployment activities.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- To what extent has the entity defined and documented the regulatory environment -including minimum requirements in laws and regulations?\n\n- Has the system been reviewed for its compliance to applicable laws, regulations, standards, and guidance?\n- To what extent has the entity defined and documented the regulatory environment -including applicable requirements in laws and regulations?\n- Has the system been reviewed for its compliance to relevant applicable laws, regulations, standards, and guidance?\n\n## AI Transparency Resources\n\nGAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities. References Andrew Smith, \"Using Artificial Intelligence and Algorithms,\" FTC Business Blog (2020). Rebecca Kelly Slaughter, \"Algorithms and Economic Justice,\" ISP Digital Future Whitepaper &amp; YJoLT Special Publication (2021). Patrick Hall, Benjamin Cox, Steven Dickerson, Arjun Ravi Kannan, Raghu Kulkarni, and Nicholas Schmidt, \"A United States fair lending perspective on machine learning,\" Frontiers in Artificial Intelligence 4 (2021). AI Hiring Tools and the Law, Partnership on Employment &amp; Accessible Technology (PEAT, peatworks.org).\n\n## GOVERN 1.2\n\nThe characteristics of trustworthy AI are integrated into organizational policies, processes, and procedures.\n\n## About\n\nPolicies, processes, and procedures are central components of effective AI risk management and fundamental to individual and organizational accountability. All stakeholders benefit from policies, processes, and procedures which require preventing harm by design and default. Organizational policies and procedures will vary based on available resources and risk profiles, but can help systematize AI actor roles and responsibilities throughout the AI lifecycle. Without such policies, risk management can be subjective across the organization, and exacerbate rather than minimize risks over time.  Policies, or summaries thereof, are understandable to relevant AI actors. Policies reflect an understanding of the underlying metrics, measurements, and tests that are necessary to support policy and AI system design, development, deployment and use. Lack of clear information about responsibilities and chains of command will limit the effectiveness of risk management.\n\n## Suggested Actions\n\nOrganizational AI risk management policies should be designed to:\n\n- Define key terms and concepts related to AI systems and the scope of their purposes and intended uses.\n- Connect AI governance to existing organizational governance and risk controls.\n- Align to broader data governance policies and practices, particularly the use of sensitive or otherwise risky data.\n- Detail standards for experimental design, data quality, and model training.\n- Outline and document risk mapping and measurement processes and standards.\n- Detail model testing and validation processes.\n- Detail review processes for legal and risk functions.\n- Establish the frequency of and detail for monitoring, auditing and review processes.\n- Outline change management requirements.\n- Outline processes for internal and external stakeholder engagement.\n- Establish whistleblower policies to facilitate reporting of serious AI system concerns.\n- Detail and test incident response plans.\n- Verify that formal AI risk management policies align to existing legal standards, and industry best practices and norms.\n- Establish AI risk management policies that broadly align to AI system trustworthy characteristics.\n- Verify that formal AI risk management policies include currently deployed and thirdparty AI systems.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent do these policies foster public trust and confidence in the use of the AI system?\n- What policies has the entity developed to ensure the use of the AI system is consistent with its stated values and principles?\n- What policies and documentation has the entity developed to encourage the use of its AI system as intended?\n- To what extent are the model outputs consistent with the entity's values and principles to foster public trust and equity?\n\nAI Transparency Resources GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities. References Off. Comptroller Currency, Comptroller's Handbook: Model Risk Management (Aug. 2021). GAO, 'Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities,' GAO@100 (GAO -21-519SP), June 2021. NIST, \"U.S. Leadership in AI: A Plan for Federal Engagement in Developing Technical Standards and Related Tools\".\n\nLipton, Zachary and McAuley, Julian and Chouldechova, Alexandra, Does mitigating ML's impact disparity require treatment disparity? Advances in Neural Information Processing Systems, 2018. Jessica Newman (2023) 'A Taxonomy of Trustworthiness for Artificial Intelligence: Connecting Properties of Trustworthiness with Risk Management and the AI Lifecycle,' UC Berkeley Center for Long-Term Cybersecurity. Emily Hadley (2022). Prioritizing Policies for Furthering Responsible Artificial Intelligence in the United States. 2022 IEEE International Conference on Big Data (Big Data), 5029-5038. SAS Institute, 'The SASÂ® Data Governance Framework: A Blueprint for Success'. ISO, 'Information technology Reference Model of Data Management, ' ISO/IEC TR 10032:200. 'Play 5: Create a formal policy,' Partnership on Employment &amp; Accessible Technology (PEAT, peatworks.org). \"National Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity. Kaitlin R. Boeckl and Naomi B. Lefkovitz. \"NIST Privacy Framework: A Tool for Improving Privacy Through Enterprise Risk Management, Version 1.0.\" National Institute of Standards and Technology (NIST), January 16, 2020. 'plainlanguage.gov Home,' The U.S. Government.\n\n## GOVERN 1.3\n\nProcesses and procedures are in place to determine the needed level of risk management activities based on the organization's risk tolerance.\n\n## About\n\nRisk management resources are finite in any organization. Adequate AI governance policies delineate the mapping, measurement, and prioritization of risks to allocate resources toward the most material issues for an AI system to ensure effective risk management. Policies may specify systematic processes for assigning mapped and measured risks to standardized risk scales. AI risk tolerances  range from negligible to critical -from, respectively, almost no risk to risks that can result in irredeemable human, reputational, financial, or environmental losses. Risk tolerance rating policies consider different sources of risk, (e.g., financial, operational, safety and wellbeing, business, reputational, or model risks). A typical risk measurement approach entails the multiplication, or qualitative combination, of measured or estimated impact and likelihood of impacts into a risk score (risk â‰ˆ impact x likelihood). This score is then placed on a risk scale. Scales for risk may be qualitative, such as redamber-green (RAG), or may entail simulations or econometric approaches. Impact\n\nassessments are a common tool for understanding the severity of mapped risks. In the most fulsome AI risk management approaches, all models are assigned to a risk level.\n\n## Suggested Actions\n\n- Establish policies to define mechanisms for measuring or understanding an AI system's potential impacts, e.g., via regular impact assessments at key stages in the AI lifecycle, connected to system impacts and frequency of system updates.\n- Establish policies to define mechanisms for measuring or understanding the likelihood of an AI system's impacts and their magnitude at key stages in the AI lifecycle.\n- Establish policies that define assessment scales for measuring potential AI system impact. Scales may be qualitative, such as red-amber-green (RAG), or may entail simulations or econometric approaches.\n- Establish policies for assigning an overall risk measurement approach for an AI system, or its important components, e.g., via multiplication or combination of a mapped risk's impact and likelihood (risk â‰ˆ impact x likelihood).\n- Establish policies to assign systems to uniform risk scales that are valid across the organization's AI portfolio (e.g. documentation templates), and acknowledge risk tolerance and risk levels may change over the lifecycle of an AI system.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- How do system performance metrics inform risk tolerance decisions?\n- What policies has the entity developed to ensure the use of the AI system is consistent with organizational risk tolerance?\n- How do the entity's data security and privacy assessments inform risk tolerance decisions?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n\nReferences Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). The Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, 2019). Brenda Boultwood, How to Develop an Enterprise Risk-Rating Approach (Aug. 26, 2021). Global Association of Risk Professionals (garp.org). Accessed Jan. 4, 2023. 63: Enterprise Risk Management: Selected Agencies' Experiences Illustrate Good\n\nGAO-17Practices in Managing Risk.\n\n## GOVERN 1.4\n\nThe risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities.\n\n## About\n\nClear policies and procedures relating to documentation and transparency facilitate and enhance efforts  to communicate roles and responsibilities for the Map, Measure and Manage functions across the AI lifecycle. Standardized documentation can help organizations systematically integrate AI risk management processes and enhance accountability efforts. For example, by adding their contact information to a work product document, AI actors can improve communication, increase ownership of work products, and potentially enhance consideration of product quality. Documentation may generate downstream benefits related to improved system replicability and robustness. Proper documentation storage and access procedures allow for quick retrieval of critical information during a negative incident. Explainable machine learning efforts (models and explanatory methods) may bolster technical documentation practices by introducing additional information for review and interpretation by AI Actors.\n\n## Suggested Actions\n\n- Establish and regularly review documentation policies that, among others, address information related to:\n- AI actors contact informations\n- Business justification\n- Scope and usages\n- Expected and potential risks and impacts\n- Assumptions and limitations\n- Description and characterization of training data\n- Algorithmic methodology\n- Evaluated alternative approaches\n- Description of output data\n- Testing and validation results (including explanatory visualizations and information)\n- Down- and up-stream dependencies\n- Plans for deployment, monitoring, and change management\n- Stakeholder engagement plans\n- Verify documentation policies for AI systems are standardized across the organization and remain current.\n- Establish policies for a model documentation inventory system and regularly review its completeness, usability, and efficacy.\n- Establish mechanisms to regularly review the efficacy of risk management processes.\n- Identify AI actors responsible for evaluating efficacy of risk management processes and approaches, and for course-correction based on results.\n\n- Establish policies and processes regarding public disclosure of the use of AI and risk management material such as impact assessments, audits, model documentation and validation and testing results.\n- Document and review the use and efficacy of different types of transparency tools and follow industry standards at the time a model is in use.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?\n- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?\n- How will the appropriate performance metrics, such as accuracy, of the AI be monitored after the AI is deployed? How much distributional shift or model drift from baseline performance is acceptable?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020.\n\n## References\n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011). Off. Comptroller Currency, Comptroller's Handbook: Model Risk Management (Aug. 2021). Margaret Mitchell et al., 'Model Cards for Model Reporting.' Proceedings of 2019 FATML Conference. Timnit Gebru et al., 'Datasheets for Datasets,' Communications of the ACM 64, No. 12, 2021. Emily M. Bender, Batya Friedman,  Angelina McMillan-Major (2022). A Guide for Writing Data Statements for Natural Language Processing. University of Washington. Accessed July 14, 2022. M. Arnold, R. K. E. Bellamy, M. Hind, et al. FactSheets: Increasing trust in AI services through supplier's declarations of conformity. IBM Journal of Research and Development 63, 4/5 (July-September 2019), 6:1-6:13. Navdeep Gill, Abhishek Mathur, Marcos V. Conde (2022). A Brief Overview of AI Governance for Responsible Machine Learning Systems. ArXiv, abs/2211.13130. Creating AI FactSheets. Bulletin of the IEEE Computer Society Technical Committee on Data\n\nJohn Richards, David Piorkowski, Michael Hind, et al. A Human-Centered Methodology for Engineering.\n\nChristoph Molnar, Interpretable Machine Learning, lulu.com. David A. Broniatowski. 2021. Psychological Foundations of Explainability and Interpretability in Artificial Intelligence. National Institute of Standards and Technology (NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD. OECD (2022), 'OECD Framework for the Classification of AI systems', OECD Digital Economy Papers, No. 323, OECD Publishing, Paris.\n\n## GOVERN 1.5\n\nOngoing monitoring and periodic review of the risk management process and its outcomes are planned, organizational roles and responsibilities are clearly defined, including determining the frequency of periodic review.\n\n## About\n\nAI systems are dynamic and may perform in unexpected ways once deployed or after deployment. Continuous monitoring is a risk management process for tracking unexpected issues and performance changes, in real-time or at a specific frequency, across the AI system lifecycle. Incident response and 'appeal and override' are commonly used processes in information technology management. These processes enable real-time flagging of potential incidents, and human adjudication of system outcomes. Establishing and maintaining incident response plans can reduce the likelihood of additive impacts during an AI incident. Smaller organizations which may not have fulsome governance programs, can utilize incident response plans for addressing system failures, abuse or misuse.\n\n## Suggested Actions\n\n- Establish policies to allocate appropriate resources and capacity for assessing impacts of AI systems on individuals, communities and society.\n- Establish policies and procedures for monitoring and addressing AI system performance and trustworthiness, including bias and security problems, across the lifecycle of the system.\n- Establish policies for AI system incident response, or confirm that existing incident response policies apply to AI systems.\n- Establish policies to define organizational functions and personnel responsible for AI system monitoring and incident response activities.\n- Establish mechanisms to enable the sharing of feedback from impacted individuals or communities about negative impacts from AI systems.\n- Establish mechanisms to provide recourse for impacted individuals or communities to contest problematic AI system outcomes.\n- Establish opt-out mechanisms.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent does the system/entity consistently measure progress towards stated goals and objectives?\n- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?\n- Did your organization address usability problems and test whether user interfaces served their intended purposes?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- WEF Model AI Governance Framework Assessment 2020.\n\n## References\n\nNational Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity.\n\nNational Institute of Standards and Technology. (2012). Computer Security Incident Handling Guide. NIST Special Publication 800-61 Revision 2.\n\n## GOVERN 1.6\n\nMechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities.\n\n## About\n\nAn AI system inventory is an organized database of artifacts relating to an AI system or model. It may include system documentation, incident response plans, data dictionaries, links to implementation software or source code, names and contact information for relevant AI actors, or other information that may be helpful for model or system maintenance and incident response purposes. AI system inventories also enable a holistic view of organizational AI assets. A serviceable AI system inventory may allow for the quick resolution of:\n\n- specific queries for single models, such as  'when was this model last refreshed?'\n- highlevel queries across all models, such as, 'how many models are currently deployed within our organization?' or 'how many users are impacted by our models?'\n\nAI system inventories are a common element of traditional model risk management approaches and can provide technical, business and risk management benefits. Typically inventories capture all organizational models or systems, as partial inventories may not provide the value of a full inventory.\n\n## Suggested Actions\n\n- Establish policies that define the creation and maintenance of AI system inventories.\n- Establish policies that define a specific individual or team that is responsible for maintaining the inventory.\n- Establish policies that define which models or systems are inventoried, with preference to inventorying all models or systems, or minimally, to high risk models or systems, or systems deployed in high-stakes settings.\n- Establish policies that define model or system attributes to be inventoried, e.g, documentation, links to source code, incident response plans, data dictionaries, AI actor contact information.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Who is responsible for documenting and maintaining the AI system inventory details?\n- What processes exist for data generation, acquisition/collection, ingestion, staging/storage, transformations, security, maintenance, and dissemination?\n- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Intel.gov: AI Ethics Framework for Intelligence Community - 2020.\n\nReferences 'A risk -based integrity level schema', in IEEE 1012, IEEE Standard for System, Software, and Hardware Verification and Validation. See Annex B. Off. Comptroller Currency, Comptroller's Handbook: Model Risk Management (Aug. 2021). See 'Model Inventory,' pg. 26. -source system for Machine Learning model versioning,\n\nVertaAI, 'ModelDB: An open metadata, and experiment management.' Accessed Jan. 5, 2023.\n\n## GOVERN 1.7\n\nProcesses and procedures are in place for decommissioning and phasing out of AI systems safely and in a manner that does not increase risks or decrease the organization's trustworthiness.\n\n## About\n\nIrregular or indiscriminate termination or deletion of models or AI systems may be inappropriate and increase organizational risk. For example, AI systems may be subject to regulatory requirements or implicated in future security or legal investigations. To maintain trust, organizations may consider establishing policies and processes for the systematic and deliberate decommissioning of AI systems. Typically, such policies consider user and\n\ncommunity concerns, risks in dependent and linked systems, and security, legal or regulatory concerns. Decommissioned models or systems may be stored in a model inventory along with active models,  for an established length  of time.\n\n## Suggested Actions\n\n- Establish policies for decommissioning AI systems. Such policies typically address:\n- User and community concerns, and reputational risks.\n- Business continuity and financial risks.\n- Up and downstream system dependencies.\n- Regulatory requirements (e.g., data retention).\n- Potential future legal, regulatory, security or forensic investigations.\n- Migration to the replacement system, if appropriate.\n- Establish policies that delineate where and for how long decommissioned systems, models and related artifacts are stored.\n- Establish practices to track accountability and consider how decommission and other adaptations or changes in system deployment contribute to downstream impacts for individuals, groups and communities.\n- Establish policies that address ancillary data or artifacts that must be preserved for fulsome understanding or execution of the decommissioned AI system, e.g., predictions, explanations, intermediate input feature representations, usernames and passwords, etc.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- What processes exist for data generation, acquisition/collection, ingestion, staging/storage, transformations, security, maintenance, and dissemination?\n- To what extent do these policies foster public trust and confidence in the use of the AI system?\n- If anyone believes that the AI no longer meets this ethical framework, who will be responsible for receiving the concern and as appropriate investigating and remediating the issue? Do they have authority to modify, limit, or stop the use of the AI?\n- If it relates to people, were there any ethical review applications/reviews/approvals? (e.g. Institutional Review Board applications)\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Intel.gov: AI Ethics Framework for Intelligence Community - 2020.\n- Datasheets for Datasets.\n\nReferences Michelle De Mooy, Joseph Jerome and Vijay Kasschau, 'Should It Stay or Should It Go? The Legal, Policy and Technical Landscape Around Data Deletion,' Center for Democracy and Technology, 2017. Burcu Baykurt, \"Algorithmic accountability in US cities: Transparency, impact, and political economy.\" Big Data &amp; Society 9, no. 2 (2022): 20539517221115426. Upol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. 'The Algorithmic Imprint.' Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (2022). 'Information System Decommissioning Guide,' Bureau of Land Management, 2011.\n\n## GOVERN 2.1\n\nRoles and responsibilities and lines of communication related to mapping, measuring, and managing AI risks are documented and are clear to individuals and teams throughout the organization.\n\n## About\n\nThe development of a risk-aware organizational culture starts with defining responsibilities. For example, under some risk management structures, professionals carrying out test and evaluation  tasks are independent from AI system developers and report through risk management functions or directly to executives.  This kind of structure may help counter implicit biases such as groupthink or sunk cost fallacy and bolster risk management functions, so efforts are not  easily bypassed or ignored. Instilling a culture where AI system design and implementation decisions can be questioned and coursecorrected by empowered AI actors can enhance organizations' abilities to anticipate and effectively manage risks before they become ingrained.\n\n## Suggested Actions\n\n- Establish policies that define the AI risk management roles and responsibilities for positions directly and indirectly related to AI systems, including, but not limited to\n- Boards of directors or advisory committees\n- Senior management\n- AI audit functions\n- Product management\n- Project management\n- AI design\n- AI development\n- Human-AI interaction\n- AI testing and evaluation\n- AI acquisition and procurement\n\n- Impact assessment functions\n- Oversight functions\n- Establish policies that promote regular communication among AI actors participating in AI risk management efforts.\n- Establish policies that separate management of AI system development functions from AI system testing functions, to enable independent course-correction of AI systems.\n- Establish policies to identify, increase the transparency of, and prevent conflicts of interest in AI risk management efforts.\n- Establish policies to counteract confirmation bias and market incentives that may hinder AI risk management efforts.\n- Establish policies that incentivize AI actors to collaborate with existing legal, oversight, compliance, or enterprise risk functions in their AI risk management activities.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?\n- Who is ultimately responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?\n- Are the responsibilities of the personnel involved in the various AI governance processes clearly defined?\n- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?\n- Did your organization implement accountability-based practices in data management and protection (e.g. the PDPA and OECD Privacy Principles)?\n\n## AI Transparency Resources\n\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n\n## References Andrew Smith, 'Using Artificial Intelligence and Algorithms,' FTC Business Blog (Apr. 8, 2020).\n\nOff. Superintendent Fin. Inst. Canada, Enterprise-Wide Model Risk Management for DepositTaking Institutions, E-23 (Sept. 2017). Bd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter\n\n11-7 (Apr. 4, 2011).\n\nOff. Comptroller Currency, Comptroller's Handbook: Model Risk Management (Aug. 2021).\n\nISO, 'Information Technology Artificial Intelligence -Guidelines for AI applications,' ISO/IEC CD 5339. See Section 6, 'Stakeholders' perspectives and AI application framework.'\n\n## GOVERN 2.2\n\nThe organization's personnel and partners receive AI risk management training to enable them to perform their duties and responsibilities consistent with related policies, procedures, and agreements.\n\n## About\n\nTo enhance AI risk management adoption and effectiveness, organizations are encouraged to identify and integrate appropriate training curricula into enterprise learning requirements. Through regular training, AI actors can maintain awareness of:\n\n- AI risk management goals and their role in achieving them.\n- Organizational policies, applicable laws and regulations, and industry best practices and norms.\n\nSee [MAP 3.4]() and [3.5]() for additional relevant information.\n\n## Suggested Actions\n\n- Establish policies for personnel addressing ongoing education about:\n- Applicable laws and regulations for AI systems.\n- Potential negative impacts that may arise from AI systems.\n- Organizational AI policies.\n- Trustworthy AI characteristics.\n- Ensure that trainings are suitable across AI actor sub-groups - for AI actors carrying out technical tasks (e.g., developers, operators, etc.) as compared to AI actors in oversight roles (e.g., legal, compliance, audit,  etc.).\n- Ensure that trainings comprehensively address technical and socio-technical aspects of AI risk management.\n- Verify that organizational AI policies include mechanisms for internal AI personnel to acknowledge and commit to their roles and responsibilities.\n- Verify that organizational policies address change management and include mechanisms to communicate and acknowledge substantial AI system changes.\n- Define paths along internal and external chains of accountability to escalate risk concerns.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Are the relevant staff dealing with AI systems properly trained to interpret AI model output and decisions as well as to detect and manage bias in data?\n\n- How does the entity determine the necessary skills and experience needed to design, develop, deploy, assess, and monitor the AI system?\n- How does the entity assess whether personnel have the necessary skills, training, resources, and domain knowledge to fulfill their assigned responsibilities?\n- What efforts has the entity undertaken to recruit, develop, and retain a workforce with backgrounds, experience, and perspectives that reflect the community impacted by the AI system?\n\n## AI Transparency Resources\n\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n\n## References\n\nOff. Comptroller Currency, Comptroller's Handbook: Model Risk Management (Aug. 2021).\n\n'Developing Staff Trainings for Equitable AI,' Partnership on Employment &amp; Accessible Technology (PEAT, peatworks.org).\n\n## GOVERN 2.3\n\nExecutive leadership of the organization takes responsibility for decisions about risks associated with AI system development and deployment.\n\n## About\n\nSenior leadership and members of the C-Suite in organizations that maintain an AI portfolio, should maintain awareness of AI risks, affirm the organizational appetite for such risks, and be responsible for managing those risks.. Accountability ensures that a specific team and individual is responsible for AI risk management efforts. Some organizations grant authority and resources (human and budgetary) to a designated officer who ensures adequate performance of the institution's AI portfolio (e.g. predictive modeling, machine learning).\n\n## Suggested Actions\n\n- Organizational management can:\n- Declare risk tolerances for developing or using AI systems.\n- Support AI risk management efforts, and play an active role in such efforts.\n- Integrate a risk and harm prevention mindset throughout the AI lifecycle as part of organizational culture\n- Support competent risk management executives.\n- Delegate the power, resources, and authorization to perform risk management to each appropriate level throughout the management chain.\n\n- Organizations can establish board committees for AI risk management and oversight functions and integrate those functions within the organization's broader enterprise risk management approaches.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Did your organization's board and/or senior management sponsor, support and participate in your organization's AI governance?\n- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?\n- Do AI solutions provide sufficient information to assist the personnel to make an informed decision and take actions accordingly?\n- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?\n\n## AI Transparency Resources\n\n- WEF Companion to the Model AI Governance Framework- 2020.\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n\n## References\n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)\n\nOff. Superintendent Fin. Inst. Canada, Enterprise-Wide Model Risk Management for DepositTaking Institutions, E-23 (Sept. 2017).\n\n## GOVERN 3.1\n\nDecision-makings related to mapping, measuring, and managing AI risks throughout the lifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, experience, expertise, and backgrounds).\n\n## About\n\nA diverse team that includes AI actors with diversity of experience, disciplines, and backgrounds to enhance organizational capacity and capability for anticipating risks is better equipped to carry out risk management. Consultation with external personnel may be necessary when internal teams lack a diverse range of lived experiences or disciplinary expertise. To extend the benefits of diversity, equity, and inclusion to both the users and AI actors, it is recommended that teams are composed of a diverse group of individuals who reflect a range of backgrounds, perspectives and expertise. Without commitment from senior leadership, beneficial aspects of team diversity and inclusion can be overridden by unstated organizational incentives that inadvertently conflict with the broader values of a diverse workforce.\n\n## Suggested Actions\n\nOrganizational management can:\n\n- Define policies and hiring practices at the outset that promote interdisciplinary roles, competencies, skills, and capacity for AI efforts.\n- Define policies and hiring practices that lead to demographic and domain expertise diversity; empower staff with necessary resources and support, and facilitate the contribution of staff feedback and concerns without fear of reprisal.\n- Establish policies that facilitate inclusivity and the integration of new insights into existing practice.\n- Seek external expertise to supplement organizational diversity, equity, inclusion, and accessibility where internal expertise is lacking.\n- Establish policies that incentivize AI actors to collaborate with existing nondiscrimination, accessibility and accommodation, and human resource functions, employee resource group (ERGs), and diversity, equity, inclusion, and accessibility (DEIA) initiatives.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- Are the relevant staff dealing with AI systems properly trained to interpret AI model output and decisions as well as to detect and manage bias in data?\n- Entities include diverse perspectives from technical and non-technical communities throughout the AI life cycle to anticipate and mitigate unintended consequences including potential bias and discrimination.\n- Stakeholder involvement: Include diverse perspectives from a community of stakeholders throughout the AI life cycle to mitigate risks.\n- Strategies to incorporate diverse perspectives include establishing collaborative processes and multidisciplinary teams that involve subject matter experts in data science, software development, civil liberties, privacy and security, legal counsel, and risk management.\n- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?\n\n## AI Transparency Resources\n\n- WEF Model AI Governance Framework Assessment 2020.\n- Datasheets for Datasets.\n\nReferences\n\nDylan Walsh, 'How can human\n\nMgmt. Rev., 2021.\n\nMichael Li, 'To Build Less -Biased AI, Hire a More Diverse Team,' Harvard Bus. Rev., 2020.\n\n-\n\ncentered AI fight bias in machines and people?' MIT Sloan\n\nBo Cowgill et al., 'Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing AI Ethics,' 2020.\n\nNaomi Ellemers, Floortje Rink, 'Diversity in work groups,' Current opinion in psychology, vol. 11, pp. 49 -53, 2016. innovation orientation: The relationship and consequences for innovativeness and\n\nKatrin Talke, SÃ¸ren Salomo, Alexander Kock, 'Top management team diversity and strategic performance,' Journal of Product Innovation Management, vol. 28, pp. 819832, 2011. Sarah Myers West, Meredith Whittaker, and Kate Crawford,, 'Discriminating Systems: Gender, Race, and Power in AI,' AI Now Institute, Tech. Rep., 2019. Sina Fazelpour, Maria De-Arteaga, Diversity in sociotechnical machine learning systems. Big Data &amp; Society. January 2022. doi:10.1177/20539517221082027 Mary L. Cummings and Songpo Li, 2021a. Sources of subjectivity in machine learning models. ACM Journal of Data and Information Quality, 13(2), 1 -9 'Staffing for Equitable AI: Roles &amp; Responsibilities,' Partnership on Employment &amp;\n\nAccessible  Technology (PEAT, peatworks.org). Accessed Jan. 6, 2023.\n\n## GOVERN 3.2\n\nPolicies and procedures are in place to define and differentiate roles and responsibilities for human-AI configurations and oversight of AI systems.\n\n## About\n\nIdentifying and managing AI risks and impacts are enhanced when a broad set of perspectives and actors across the AI lifecycle, including technical, legal, compliance, social science, and human factors expertise is engaged. AI actors include those who operate, use, or interact with AI systems for downstream tasks, or monitor AI system performance. Effective risk management efforts include:\n\n- clear definitions and differentiation of the various human roles and responsibilities for AI system oversight and governance\n- recognizing and clarifying differences between AI system overseers and those using or interacting with AI systems.\n\n## Suggested Actions\n\n- Establish policies and procedures that define and differentiate the various human roles and responsibilities when using, interacting with, or monitoring AI systems.\n- Establish procedures for capturing and tracking risk information related to human-AI configurations and associated outcomes.\n- Establish policies for the development of proficiency standards for AI actors carrying out system operation tasks and system oversight tasks.\n\n- Establish specified risk management training protocols for AI actors carrying out system operation tasks and system oversight tasks.\n- Establish policies and procedures regarding AI actor roles, and responsibilities for human oversight of deployed systems.\n- Establish policies and procedures defining  human-AI configurations (configurations where AI systems are explicitly designated and treated as team members in primarily human teams) in relation to organizational risk tolerances, and associated documentation.\n- Establish policies to enhance the explanation, interpretation, and overall transparency of AI systems.\n- Establish policies for managing risks regarding known difficulties in human-AI configurations, human-AI teaming, and AI system user experience and user interactions (UI/UX).\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n- To what extent has the entity documented the appropriate level of human involvement in AI-augmented decision-making?\n- How will the accountable human(s) address changes in accuracy and precision due to either an adversary's attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI?\n- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?\n- How does the entity assess whether personnel have the necessary skills, training, resources, and domain knowledge to fulfill their assigned responsibilities?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Intel.gov: AI Ethics Framework for Intelligence Community - 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n\nReferences\n\nMadeleine Clare Elish, \"Moral Crumple Zones: Cautionary tales in human-robot interaction,\" Engaging Science, Technology, and Society, Vol. 5, 2019.\n\n'Human -AI Teaming: State-Of-TheArt and Research Needs,' National Academies of Sciences, Engineering, and Medicine, 2022.\n\nBen Green, \"The Flaws Of Policies Requiring Human Oversight Of Government Algorithms,\" Computer Law &amp; Security Review 45 (2022).\n\nDavid A. Broniatowski. 2021. Psychological Foundations of Explainability and\n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology (NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD. Off. Comptroller Currency, Comptroller's Handbook: Model Risk Management (Aug. 2021).\n\n## GOVERN 4.1\n\nOrganizational policies, and practices are in place to foster a critical thinking and safety-first mindset in the design, development, deployment, and uses of AI systems to minimize negative impacts.\n\n## About\n\nA risk culture and accompanying practices can help organizations effectively triage the most critical risks. Organizations in some industries implement three (or more) 'lines of defense,' where separate teams are held accountable for different aspects of the system lifecycle, such as development, risk management, and auditing. While a traditional threelines approach may be impractical for smaller organizations, leadership can commit to cultivating a strong risk culture through other means. For example, 'effective challenge,' is a culture- based practice that encourages critical thinking and questioning of important design and implementation decisions by experts with the authority and stature to make such changes. Red-teaming is another risk measurement and management approach. This practice consists of adversarial testing of AI systems under stress conditions to seek out failure modes or vulnerabilities in the system. Red-teams are composed of external experts or personnel who are independent from internal AI actors.\n\n## Suggested Actions\n\n- Establish policies that require inclusion of oversight functions (legal, compliance, risk management) from the outset of the system design process.\n- Establish policies that promote effective challenge of AI system design, implementation, and deployment decisions, via mechanisms such as the three lines of defense, model audits, or red-teaming -to minimize workplace risks such as groupthink.\n- Establish policies that incentivize safety-first mindset and general critical thinking and review at an organizational and procedural level.\n- Establish whistleblower protections for insiders who report on perceived serious problems with AI systems.\n- Establish policies to integrate a harm and risk prevention mindset throughout the AI lifecycle.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent has the entity documented the AI system's development, testing methodology, metrics, and performance outcomes?\n\n- Are organizational information sharing practices widely followed and transparent, such that related past failed designs can be avoided?\n- Are training manuals and other resources for carrying out incident response documented and available?\n- Are processes for operator reporting of incidents and near-misses documented and available?\n- How might revealing mismatches between claimed and actual system performance help users understand limitations and anticipate risks and impacts?'\n\n## AI Transparency Resources\n\n- Datasheets for Datasets.\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- WEF Model AI Governance Framework Assessment 2020.\n\n## References\n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)\n\n## Patrick Hall, Navdeep Gill, and Benjamin Cox, 'Responsible Machine Learning,' O'Reilly Media, 2020.\n\nOff. Superintendent Fin. Inst. Canada, Enterprise-Wide Model Risk Management for DepositTaking Institutions, E-23 (Sept. 2017).\n\nGAO, 'Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities,' GAO@100 (GAO -21-519SP), June 2021.\n\nDonald Sull, Stefano Turconi, and Charles Sull, 'When It Comes to Culture, Does Your Company Walk the Talk?' MIT Sloan Mgmt. Rev., 2020.\n\n## Kathy Baxter, AI Ethics Maturity Model, Salesforce.\n\nUpol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, and Hal DaumÃ©. 2024. Seamful XAI: Operationalizing Seamful Design in Explainable AI. Proc. ACM Hum.-Comput. Interact. 8, CSCW1, Article 119. https://doi.org/10.1145/3637396\n\n## GOVERN 4.2\n\nOrganizational teams document the risks and potential impacts of the AI technology they design, develop, deploy, evaluate and use, and communicate about the impacts more broadly.\n\n## About\n\nImpact assessments are one approach for driving responsible technology development practices. And, within a specific use case, these assessments can provide a high-level structure for organizations to frame risks of a given algorithm or deployment. Impact\n\nassessments can also serve as a mechanism for organizations to articulate risks and generate documentation for managing and oversight activities when harms do arise.\n\n## Impact assessments may:\n\n- be applied at the beginning of a process but also iteratively and regularly since goals and outcomes can evolve over time.\n- include perspectives from AI actors, including operators, users, and potentially impacted communities (including historically marginalized communities, those with disabilities, and individuals impacted by the digital divide),\n- assist in 'go/no -go' decisions for an AI system.\n- consider conflicts of interest, or undue influence, related to the organizational team being assessed.\n\nSee the MAP function playbook guidance for more information relating to impact assessments.\n\n## Suggested Actions\n\n- Establish impact assessment policies and processes for AI systems used by the organization.\n- Align organizational impact assessment activities with relevant regulatory or legal requirements.\n- Verify that impact assessment activities are appropriate to evaluate the potential negative impact of a system and how quickly a system changes, and that assessments are applied on a regular basis.\n- Utilize impact assessments to inform broader evaluations of AI system risk.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?\n- How has the entity documented the AI system's data provenance, including sources, origins, transformations, augmentations, labels, dependencies, constraints, and metadata?\n- To what extent has the entity clearly defined technical specifications and requirements for the AI system?\n- To what extent has the entity documented and communicated the AI system's development, testing methodology, metrics, and performance outcomes?\n- Have you documented and explained that machine errors may differ from human errors?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Datasheets for Datasets.\n\nReferences Dillon Reisman, Jason Schultz, Kate Crawford, Meredith Whittaker, 'Algorithmic Impact\n\nAssessments: A Practical Framework For Public Agency Accountability,' AI Now Institute, 2018. H.R. 2231, 116th Cong. (2019). BSA The Software Alliance (2021) Confronting Bias: BSA's Framework to Build Trust in AI. Anthony M. Barrett, Dan Hendrycks, Jessica Newman and Brandie Nonnecke. Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks. ArXiv abs/2206.08966 (2022) https://arxiv.org/abs/2206.08966 David Wright, 'Making Privacy Impact Assessments More Effective.\" The Information Society 29, 2013. Konstantinia Charitoudi and Andrew Blyth. A Socio-Technical Approach to Cyber Risk Management and Impact Assessment. Journal of Information Security 4, 1 (2013), 33-41. Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, &amp; Jacob Metcalf. 2021. 'Assembling Accountability: Algorithmic Impact Assessment for the Public Interest'. Microsoft. Responsible AI Impact Assessment Template. 2022. Microsoft. Responsible AI Impact Assessment Guide. 2022. Microsoft. Foundations of assessing harm. 2022. Mauritz Kop, 'AI Impact Assessment &amp; Code of Conduct,' Futurium, May 2019. Dillon Reisman, Jason Schultz, Kate Crawford, and Meredith Whittaker, 'Algorithmic Impact Assessments: A Practical Framework For Public Agency Accountability,' AI Now, Apr. 2018. Andrew D. Selbst, 'An Institutional View Of Algorithmic Impact Assessments,' Harvard Journal of Law &amp; Technology, vol. 35, no. 1, 2021 Ada Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. Accessed July 14, 2022. Kathy Baxter, AI Ethics Maturity Model, Salesforce Ravit Dotan, Borhane Blili-Hamelin, Ravi Madhavan, Jeanna Matthews, Joshua Scarpino, &amp; Carol Anderson. (2024). A Flexible Maturity Model for AI Governance Based on the NIST AI Risk Management Framework [Technical Report]. IEEE. https://ieeeusa.org/product/aflexible-maturity-model-for-ai-governance\n\n## GOVERN 4.3\n\nOrganizational practices are in place to enable AI testing, identification of incidents, and information sharing.\n\n## About\n\nIdentifying AI system limitations, detecting and tracking negative impacts and incidents, and sharing information about these issues with appropriate AI actors will improve risk management. Issues such as concept drift, AI bias and discrimination, shortcut learning or underspecification are difficult to identify using current standard AI testing processes. Organizations can institute in-house use and testing policies and procedures to identify and manage such issues. Efforts can take the form of pre-alpha or pre-beta testing, or deploying internally developed systems or products within the organization. Testing may entail limited and controlled in-house, or publicly available, AI system testbeds, and accessibility of AI system interfaces and outputs. Without policies and procedures that enable consistent testing practices, risk management efforts may be bypassed or ignored, exacerbating risks or leading to inconsistent risk management activities.\n\nInformation sharing about impacts or incidents detected during testing or deployment can:\n\n- draw attention to AI system risks, failures, abuses or misuses,\n- allow organizations to benefit from insights based on a wide range of AI applications and implementations, and\n- allow organizations to be more proactive in avoiding known failure modes.\n\nOrganizations may consider sharing incident information with the AI Incident Database, the AIAAIC, users, impacted communities, or with traditional cyber vulnerability databases, such as the MITRE CVE list.\n\n## Suggested Actions\n\n- Establish policies and procedures to facilitate and equip AI system testing.\n- Establish organizational commitment to identifying AI system limitations and sharing of insights about limitations within appropriate AI actor groups.\n- Establish policies for reporting and documenting incident response.\n- Establish policies and processes regarding public disclosure of incidents and information sharing.\n- Establish guidelines for incident handling related to AI system risks and performance.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Did your organization address usability problems and test whether user interfaces served their intended purposes? Consulting the community or end users at the earliest\n\n- stages of development to ensure there is transparency on the technology used and how it is deployed.\n- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?\n- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n\n## AI Transparency Resources\n\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n\nReferences Sean McGregor, 'Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database,' arXiv:2011.08512 [cs], Nov. 2020, arXiv:2011.08512. Christopher Johnson, Mark Badger, David Waltermire, Julie Snyder, and Clem Skorupka, 'Guide to cyber threat information sharing,' National Institute of Standards and Technology, NIST Special Publication 800-150, Nov 2016. Mengyi Wei, Zhixuan Zhou (2022). AI Ethics Issues in Real World: Evidence from AI Incident Database. ArXiv, abs/2206.07635. BSA The Software Alliance (2021) Confronting Bias: BSA's Framework to Build Trust in AI. 'Using Combined Expertise to Evaluate Web Accessibility,' W3C Web Accessibility Initiative.\n\n## GOVERN 5.1\n\nOrganizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks.\n\n## About\n\nBeyond internal and laboratory-based system testing, organizational policies and practices may consider AI system fitness-for-purpose related to the intended context of use. Participatory stakeholder engagement is one type of qualitative activity to help AI actors answer questions such as whether to pursue a project or how to design with impact in mind. This type of feedback, with domain expert input, can also assist AI actors to identify emergent scenarios and risks in certain AI applications. The consideration of when and how to convene a group and the kinds of individuals, groups, or community organizations to include is an iterative process connected to the system's purpose and its level of risk. Other factors relate to how to collaboratively and respectfully capture stakeholder feedback and insight that is useful, without being a solely perfunctory exercise.\n\nThese activities are best carried out by personnel with expertise in participatory practices, qualitative methods, and translation of contextual feedback for technical audiences. Participatory engagement is not a one-time exercise and is best carried out from the very beginning of AI system commissioning through the end of the lifecycle. Organizations can consider how to incorporate engagement when beginning a project and as part of their monitoring of systems. Engagement is often utilized as a consultative practice, but this perspective may inadvertently lead to 'participation washing.' Organizational transparency about the purpose and goal of the engagement can help mitigate that possibility. Organizations may also consider targeted consultation with subject matter experts as a complement to participatory findings. Experts may assist internal staff in identifying and conceptualizing potential negative impacts that were previously not considered.\n\n## Suggested Actions\n\n- Establish AI risk management policies that explicitly address mechanisms for collecting, evaluating, and incorporating stakeholder and user feedback that could include:\n- Recourse mechanisms for faulty AI system outputs.\n- Bug bounties.\n- Human-centered design.\n- User-interaction and experience research.\n- Participatory stakeholder engagement with individuals and communities that may experience negative impacts.\n- Verify that stakeholder feedback is considered and addressed, including environmental concerns, and across the entire population of intended users, including historically excluded populations, people with disabilities, older people, and those with limited access to the internet and other basic technologies.\n- Clarify the organization's principles as they apply to AI systems considering those which have been proposed publicly -to inform external stakeholders of the organization's values. Consider publishing or adopting AI principles.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?\n- How easily accessible and current is the information available to external stakeholders?\n- What was done to mitigate or reduce the potential for harm?\n- Stakeholder involvement: Include diverse perspectives from a community of stakeholders throughout the AI life cycle to mitigate risks.\n\n## AI Transparency Resources\n\n- Datasheets for Datasets.\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019.\n- Stakeholders in Explainable AI, Sep. 2018.\n\nReferences ISO, 'Ergonomics of human -system interaction -Part 210: Human-centered design for interactive systems,' ISO 9241 -210:2019 (2nd ed.), July 2019. Rumman Chowdhury and Jutta Williams, \"Introducing Twitter's first algorithmic bias bounty challenge,\" Leonard Haas and Sebastian GieÃŸler, 'In the realm of paper tigers exploring the failings of AI ethics guidelines,' AlgorithmWatch, 2020. Josh Kenway, Camille Francois, Dr. Sasha Costanza-Chock, Inioluwa Deborah Raji, &amp; Dr. Joy Buolamwini. 2022. Bug Bounties for Algorithmic Harms? Algorithmic Justice League. Accessed July 14, 2022. Microsoft Community Jury , Azure Application Architecture Guide.\n\n'Definition of independent verification and validation (IV&amp;V)', in IEEE 1012, IEEE Standard for System, Software, and Hardware Verification and Validation. Annex C,\n\n## GOVERN 5.2\n\nMechanisms are established to enable AI actors to regularly incorporate adjudicated feedback from relevant AI actors into system design and implementation.\n\n## About\n\nOrganizational policies and procedures that equip AI actors with the processes, knowledge, and expertise needed to inform collaborative decisions about system deployment improve risk management. These decisions are closely tied to AI systems and organizational risk tolerance. Risk tolerance, established by organizational leadership, reflects the level and type of risk the organization will accept while conducting its mission and carrying out its strategy. When risks arise, resources are allocated based on the assessed risk of a given AI system. Organizations typically apply a risk tolerance approach where higher risk systems receive larger allocations of risk management resources and lower risk systems receive less resources.\n\n## Suggested Actions\n\n- Explicitly acknowledge that AI systems, and the use of AI, present inherent costs and risks along with potential benefits.\n\n- Define reasonable risk tolerances for AI systems informed by laws, regulation, best practices, or industry standards.\n- Establish policies that ensure all relevant AI actors are provided with meaningful opportunities to provide feedback on system design and implementation.\n- Establish policies that define how to assign AI systems to established risk tolerance levels by combining system impact assessments with the likelihood that an impact occurs. Such assessment often entails some combination of:\n- Econometric evaluations of impacts and impact likelihoods to assess AI system risk.\n- Red-amber-green (RAG) scales for impact severity and likelihood to assess AI system risk.\n- Establishment of policies for allocating risk management resources along established risk tolerance levels, with higher-risk systems receiving more risk management resources and oversight.\n- Establishment of policies for approval, conditional approval, and disapproval of the design, implementation, and deployment of AI systems.\n- Establish policies facilitating the early decommissioning of AI systems that surpass an organization's ability to reasonably mitigate risks.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Who is ultimately responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?\n- Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?\n- Who is accountable for the ethical considerations during all stages of the AI lifecycle?\n- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?\n- Does the AI solution provide sufficient information to assist the personnel to make an informed decision and take actions accordingly?\n\n## AI Transparency Resources\n\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n- Stakeholders in Explainable AI, Sep. 2018.\n- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019.\n\n## References\n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)\n\nOff. Comptroller Currency, Comptroller's Handbook: Model Risk Management (Aug. 2021).\n\nThe Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, 2019). Retrieved on July 12, 2022.\n\n## GOVERN 6.1\n\nPolicies and procedures are in place that address AI risks associated with third-party entities, including risks of infringement of a third party's intellectual property or other rights.\n\n## About\n\nRisk measurement and management can be complicated by how customers use or integrate third-party data or systems into AI products or services, particularly without sufficient internal governance structures and technical safeguards. Organizations usually engage multiple third parties for external expertise, data, software packages (both open source and commercial), and software and hardware platforms across the AI lifecycle. This engagement has beneficial uses and can increase complexities of risk management efforts. Organizational approaches to managing third-party (positive and negative) risk may be governance approaches to third-party AI systems and data as they would for internal resources -including open source software, publicly available data, and commercially tailored to the resources, risk profile, and use case for each system. Organizations can apply available models.\n\n## Suggested Actions\n\n- Collaboratively establish policies that address third-party AI systems and data.\n- Establish policies related to:\n- Transparency into third-party system functions, including knowledge about training data, training and inference algorithms, and assumptions and limitations.\n- Thorough testing of third-party AI systems. (See MEASURE for more detail)\n- Requirements for clear and complete instructions for third-party system usage.\n- Evaluate policies for third-party technology.\n- Establish policies that address supply chain, full product lifecycle and associated processes, including legal, ethical, and other issues concerning procurement and use of third-party software or hardware systems and data.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Did you establish mechanisms that facilitate the AI system's auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system's processes, outcomes, positive and negative impact)?\n\n- If a third party created the AI, how will you ensure a level of explainability or interpretability?\n- Did you ensure that the AI system can be audited by independent third parties?\n- Did you establish a process for third parties (e.g. suppliers, end users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?\n- To what extent does the plan specifically address risks associated with acquisition, procurement of packaged software from vendors, cybersecurity controls, computational infrastructure, data, data science, deployment mechanics, and system failure?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020.\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019.\n- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI 2019.\n\n## References\n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)\n\n'Proposed Interagency Guidance on Third -Party Relationships: Risk Management,' 2021.\n\nOff. Comptroller Currency, Comptroller's Handbook: Model Risk Management (Aug. 2021).\n\n## GOVERN 6.2\n\nContingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be high-risk.\n\n## About\n\nTo mitigate the potential harms of third-party system failures, organizations may implement policies and procedures that include redundancies for covering third-party functions.\n\n## Suggested Actions\n\n- Establish policies for handling third-party system failures to include consideration of redundancy mechanisms for vital third-party AI systems.\n- Verify that incident response plans address third-party AI systems.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent does the plan specifically address risks associated with acquisition, procurement of packaged software from vendors, cybersecurity controls, computational infrastructure, data, data science, deployment mechanics, and system failure?\n- Did you establish a process for third parties (e.g. suppliers, end users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?\n- If your organization obtained datasets from a third party, did your organization assess and manage the risks of using such datasets?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019.\n\n## References\n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)\n\n'Proposed Interagency Guidance on Third -Party Relationships: Risk Management,' 2021.\n\nOff. Comptroller Currency, Comptroller's Handbook: Model Risk Management (Aug. 2021).\n\n## MANAGE\n\n<!-- image -->\n\n## Manage\n\nAI risks based on assessments and other analytical output from the Map and Measure functions are prioritized, responded to, and managed.\n\n## MANAGE 1.1\n\nA determination is made as to whether the AI system achieves its intended purpose and stated objectives and whether its development or deployment should proceed.\n\n## About\n\nAI systems may not necessarily be the right solution for a given business task or problem. A standard risk management practice is to formally weigh an AI system's negative risks against its benefits, and to determine if the AI system is an  appropriate solution. Tradeoffs among trustworthiness characteristics -such as deciding to deploy a system based on system performance vs system transparency -may require regular assessment throughout the AI lifecycle.\n\n## Suggested Actions\n\n- Consider trustworthiness characteristics when evaluating AI systems' negative risks and benefits.\n- Utilize TEVV outputs from map and measure functions when considering risk treatment.\n- Regularly track and monitor negative risks and benefits throughout the AI system lifecycle including in post-deployment monitoring.\n- Regularly assess and document system performance relative to trustworthiness characteristics and tradeoffs between negative risks and opportunities.\n- Evaluate tradeoffs in connection with real-world use cases and impacts and as enumerated in Map function outcomes.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- How do the technical specifications and requirements align with the AI system's goals and objectives?\n- To what extent are the metrics consistent with system goals, objectives, and constraints, including ethical and compliance considerations?\n- What goals and objectives does the entity expect to achieve by designing, developing, and/or deploying the AI system?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF Companion to the Model AI Governance Framework -Implementation and SelfAssessment Guide for Organizations\n\nReferences Arvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. Assembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, 2021). Fraser, Henry L and Bello y Villarino, Jose-Miguel, Where Residual Risks Reside: A Comparative Approach to Art 9(4) of the European Union's Proposed AI Regulation (September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), Microsoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT*\n\n'20). Association for Computing Machinery, New York, NY, USA, 695.\n\n## MANAGE 1.2\n\nTreatment of documented AI risks is prioritized based on impact, likelihood, or available resources or methods.\n\n## About\n\nRisk refers to the composite measure of an event's probability of occurring and the magnitude (or degree) of the consequences of the corresponding events. The impacts, or consequences, of AI systems can be positive, negative, or both and can result in opportunities or risks. Organizational risk tolerances are often informed by several internal and external factors, including existing industry practices, organizational values, and legal or regulatory requirements. Since risk management resources are often limited, organizations usually assign them based on risk tolerance. AI risks that are deemed more serious receive more oversight attention and risk management resources.\n\n## Suggested Actions\n\n- Assign risk management resources relative to established risk tolerance. AI systems with lower risk tolerances receive greater oversight, mitigation and management resources.\n- Document AI risk tolerance determination practices and resource decisions.\n- Regularly review risk tolerances and re-calibrate, as needed, in accordance with information from AI system monitoring and assessment .\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?\n- What assessments has the entity conducted on data security and privacy impacts associated with the AI system?\n- Does your organization have an existing governance structure that can be leveraged to oversee the organization's use of AI?\n\n## AI Transparency Resources\n\n- WEF Companion to the Model AI Governance Framework -Implementation and SelfAssessment Guide for Organizations\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n\nReferences Arvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. Assembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, 2021). Fraser, Henry L and Bello y Villarino, Jose-Miguel, Where Residual Risks Reside: A Comparative Approach to Art 9(4) of the European Union's Proposed AI Regulation (September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), Microsoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 695.\n\n## MANAGE 1.3\n\nResponses to the AI risks deemed high priority as identified by the Map function, are developed, planned, and documented. Risk response options can include mitigating, transferring, avoiding, or accepting.\n\n## About\n\nOutcomes from GOVERN-1, MAP-5 and MEASURE-2, can be used to address and document identified risks based on established risk tolerances. Organizations can follow existing regulations and guidelines for risk criteria, tolerances and responses established by organizational, domain, discipline, sector, or professional requirements. In lieu of such guidance, organizations can develop risk response plans based on strategies such as accepted model risk management, enterprise risk management, and information sharing and disclosure practices.\n\n## Suggested Actions\n\n- Observe regulatory and established organizational, sector, discipline, or professional standards and requirements for applying risk tolerances within the organization.\n- Document procedures for acting on AI system risks related to trustworthiness characteristics.\n- Prioritize risks involving physical safety, legal liabilities, regulatory compliance, and negative impacts on individuals, groups, or society.\n- Identify risk response plans and resources and organizational teams for carrying out response functions.\n- Store risk management and system documentation in an organized, secure repository that is accessible by relevant AI Actors and appropriate personnel.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Has the system been reviewed to ensure the AI system complies with relevant laws, regulations, standards, and guidance?\n- To what extent has the entity defined and documented the regulatory environment -including minimum requirements in laws and regulations?\n- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Datasheets for Datasets.\n\nReferences Arvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011).\n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. Assembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, 2021). Fraser, Henry L and Bello y Villarino, Jose-Miguel, Where Residual Risks Reside: A Comparative Approach to Art 9(4) of the European Union's Proposed AI Regulation (September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), Microsoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT*\n\n'20). Association for Computing Machinery, New York, NY, USA, 695.\n\n## MANAGE 1.4\n\nNegative residual risks (defined as the sum of all unmitigated risks) to both downstream acquirers of AI systems and end users are documented.\n\n## About\n\nOrganizations may choose to accept or transfer some of the documented risks  from MAP and MANAGE 1.3 and 2.1.  Such risks, known as residual risk, may affect downstream AI actors such as those engaged in system procurement or use. Transparent monitoring and managing residual risks enables cost benefit analysis and the examination of potential values of AI systems versus its potential negative impacts.\n\n## Suggested Actions\n\n- Document residual risks within risk response plans, denoting risks that have been accepted, transferred, or subject to minimal mitigation.\n- Establish procedures for disclosing residual risks to relevant downstream AI actors .\n- Inform relevant downstream AI actors of requirements for safe operation, known limitations, and suggested warning labels as identified in MAP 3.4.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?\n- Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?\n- How will updates/revisions be documented and communicated? How often and by whom?\n- How easily accessible and current is the information available to external stakeholders?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- Datasheets for Datasets.\n\nReferences Arvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. Assembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, 2021). Fraser, Henry L and Bello y Villarino, Jose-Miguel, Where Residual Risks Reside: A Comparative Approach to Art 9(4) of the European Union's Proposed AI Regulation (September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), Microsoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 695.\n\n## MANAGE 2.1\n\nResources required to manage AI risks are taken into account, along with viable non-AI alternative systems, approaches, or methods -to reduce the magnitude or likelihood of potential impacts.\n\n## About\n\nOrganizational risk response may entail identifying and analyzing alternative approaches, methods, processes or systems, and balancing tradeoffs between trustworthiness characteristics and how they relate to organizational principles and societal values. Analysis of these tradeoffs is informed by consulting with interdisciplinary organizational teams, independent domain experts, and engaging with individuals or community groups. These processes require sufficient resource allocation.\n\n## Suggested Actions\n\n- Plan and implement risk management practices in accordance with established organizational risk tolerances.\n- Verify risk management teams are resourced to carry out functions, including\n\n- Establishing processes for considering methods that are not automated; semiautomated; or other procedural alternatives for AI functions.\n- Enhance AI system transparency mechanisms for AI teams.\n- Enable exploration of AI system limitations by AI teams.\n- Identify, assess, and catalog past failed designs and negative impacts or outcomes to avoid known failure modes.\n- Identify resource allocation approaches for managing risks in systems:\n- deemed high-risk,\n- that self-update (adaptive, online, reinforcement self-supervised learning or similar),\n- trained without access to ground truth (unsupervised, semi-supervised, learning or similar),\n- with high uncertainty or where risk management is insufficient.\n- Regularly seek and integrate external expertise and perspectives to supplement organizational diversity (e.g. demographic, disciplinary), equity, inclusion, and accessibility where internal capacity is lacking.\n- Enable and encourage regular, open communication and feedback among AI actors and internal or external stakeholders related to system design or deployment decisions.\n- Prepare and document plans for continuous monitoring and feedback mechanisms.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Are mechanisms in place to evaluate whether internal teams are empowered and resourced to effectively carry out risk management functions?\n- How will user and other forms of stakeholder engagement be integrated into risk management processes?\n\n## AI Transparency Resources\n\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- Datasheets for Datasets.\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n\nReferences Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk\n\nManagement. (April 4, 2011).\n\nDavid Wright. 2013. Making Privacy Impact Assessments More Effective. The Information Society, 29 (Oct 2013), 307-315.\n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 220 -229. Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets.\n\narXiv:1803.09010.\n\n## MANAGE 2.2\n\nMechanisms are in place and applied to sustain the value of deployed AI systems.\n\n## About\n\nSystem performance and trustworthiness may evolve and shift over time, once an AI system is deployed and put into operation. This phenomenon, generally known as drift, can degrade the value of the AI system to the organization and increase the likelihood of negative impacts.  Regular monitoring of AI systems' performance and trustworthiness enhances organizations' ability to detect and respond to drift, and thus sustain an AI system's value once deployed. Processes and mechanisms for regular monitoring address system functionality and behavior - as well as impacts and alignment with the values and norms within the specific context of use. For example, considerations regarding impacts on personal or public safety or privacy may include limiting high speeds when operating autonomous vehicles or restricting illicit content recommendations for minors. Regular monitoring activities can enable organizations to systematically and proactively identify emergent risks and respond according to established protocols and metrics. Options for organizational responses include 1) avoiding the risk, 2)accepting the risk, 3) mitigating the risk, or 4) transferring the risk. Each of these actions require planning and resources. Organizations are encouraged to establish risk management protocols with consideration of the trustworthiness characteristics, the deployment context, and real world impacts.\n\n## Suggested Actions\n\n- Establish risk controls considering trustworthiness characteristics, including:\n- Data management, quality, and privacy (e.g. minimization, rectification or deletion requests) controls as part of organizational data governance policies.\n- Machine learning and end-point security countermeasures (e.g., robust models, differential privacy, authentication, throttling).\n- Business rules that augment, limit or restrict AI system outputs within certain contexts\n- Utilizing domain expertise related to deployment context for continuous improvement and TEVV across the AI lifecycle.\n- Development and regular tracking of human-AI teaming configurations.\n\n- Model assessment and test, evaluation, validation and verification (TEVV) protocols.\n- Use of standardized documentation and transparency mechanisms.\n- Software quality assurance practices across AI lifecycle.\n- Mechanisms to explore system limitations and avoid past failed designs or deployments.\n- Establish mechanisms to capture feedback from system end users and potentially impacted groups while system is in deployment.\n- stablish mechanisms to capture feedback from system end users and potentially impacted groups about how changes in system deployment (e.g.,  introducing new technology, decommissioning algorithms and models, adapting system, model or algorithm) may create negative impacts that are not visible along the AI lifecycle.\n- Review insurance policies, warranties, or contracts for legal or oversight requirements for risk transfer procedures.\n- Document risk tolerance decisions and risk acceptance procedures.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n- Could the AI system expose people to harm or negative impacts? What was done to mitigate or reduce the potential for harm?\n- How will the accountable human(s) address changes in accuracy and precision due to either an adversary's attempts to disrupt the AI or unrelated changes in the operational or business environment?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\n## References\n\n## Safety, Validity and Reliability Risk Management Approaches and Resources AI Incident Database. 2022. AI Incident Database.\n\nAIAAIC Repository. 2022. AI, algorithmic and automation incidents collected, dissected, examined, and divulged.\n\nAlexander D'Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification Presents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395.\n\nAndrew L. Beam, Arjun K. Manrai, Marzyeh Ghassemi. 2020. Challenges to the Reproducibility of Machine Learning Models in Health Care. Jama 323, 4 (January 6, 2020), 305-306.\n\nAnthony M. Barrett, Dan Hendrycks, Jessica Newman et al. 2022. Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks. arXiv:2206.08966.\n\nDebugging Machine Learning Models, In Proceedings of ICLR 2019 Workshop, May 6, 2019, New Orleans, Louisiana.\n\nJessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing,\n\nExploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363.\n\nJoelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, et al. 2020. Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program) arXiv:2003.12206.\n\nKirstie Whitaker. 2017. Showing your working: a how to guide to reproducible research.\n\n(August 2017).\n\n[LINK](https://github.com/WhitakerLab/ReproducibleResearch/blob/master/PRESENTA TIONS/Whitaker\\_ICON\\_August2017.pdf),\n\nNetflix. Chaos Monkey.\n\nPeter Henderson, Riashat Islam, Philip Bachman, et al. 2018. Deep reinforcement learning that matters. Proceedings of the AAAI Conference on Artificial Intelligence. 32, 1 (Apr. 2018).\n\nSuchi Saria, Adarsh Subbaswamy. 2019. Tutorial: Safe and Reliable Machine Learning. arXiv:1904.07204.\n\nKang, Daniel, Deepti Raghavan, Peter Bailis, and Matei Zaharia. \"Model assertions for monitoring and improving ML models.\" Proceedings of Machine Learning and Systems 2 (2020): 481-496.\n\n## Managing Risk Bias\n\nNational Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et al. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.\n\n## Bias Testing and Remediation Approaches\n\nAlekh Agarwal, Alina Beygelzimer, Miroslav DudÃ­k, et al. 2018. A Reductions Approach to Fair Classification. arXiv:1803.02453.\n\nBrian Hu Zhang, Blake Lemoine, Margaret Mitchell. 2018. Mitigating Unwanted Biases with Adversarial Learning. arXiv:1801.07593.\n\nDrago PleÄko, Nicolas Bennett, Nicolai Meinshausen. 2021. Fairadapt: Causal Reasoning for Fair Data Pre-processing. arXiv:2110.10200.\n\nFaisal Kamiran, Toon Calders. 2012. Data Preprocessing Techniques for Classification without Discrimination. Knowledge and Information Systems 33 (2012), 1 -33.\n\nFaisal Kamiran; Asim Karim; Xiangliang Zhang. 2012. Decision Theory for DiscriminationAware Classification. In Proceedings of the 2012 IEEE 12th International Conference on Data Mining, December 10-13, 2012, Brussels, Belgium. IEEE, 924-929.\n\nFlavio P. Calmon, Dennis Wei, Karthikeyan Natesan Ramamurthy, et al. 2017. Optimized Data Pre-Processing for Discrimination Prevention. arXiv:1704.03354.\n\nGeoff Pleiss, Manish Raghavan, Felix Wu, et al. 2017. On Fairness and Calibration. arXiv:1709.02012.\n\nL. Elisa Celis, Lingxiao Huang, Vijay Keswani, et al. 2020. Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees. arXiv:1806.06055.\n\nMichael Feldman, Sorelle Friedler, John Moeller, et al. 2014. Certifying and Removing Disparate Impact. arXiv:1412.3756.\n\nMichael Kearns, Seth Neel, Aaron Roth, et al. 2017. Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness. arXiv:1711.05144.\n\nMichael Kearns, Seth Neel, Aaron Roth, et al. 2018. An Empirical Study of Rich Subgroup Fairness for Machine Learning. arXiv:1808.08166.\n\nMoritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of Opportunity in Supervised Learning. In Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS 2016), 2016, Barcelona, Spain.\n\nRich Zemel, Yu Wu, Kevin Swersky, et al. 2013. Learning Fair Representations. In Proceedings of the 30th International Conference on Machine Learning 2013, PMLR 28, 3, 325-333.\n\nToshihiro Kamishima, Shotaro Akaho, Hideki Asoh &amp; Jun Sakuma. 2012. Fairness-Aware Classifier with Prejudice Remover Regularizer. In Peter A. Flach, Tijl De Bie, Nello Cristianini (eds) Machine Learning and Knowledge Discovery in Databases. European Conference ECML PKDD 2012, Proceedings Part II, September 24-28, 2012, Bristol, UK. Lecture Notes in Computer Science 7524. Springer, Berlin, Heidelberg.\n\nSecurity and Resilience Resources\n\nFTC Start With Security Guidelines. 2015.\n\nGary McGraw et al. 2022. BIML Interactive Machine Learning Risk Framework. Berryville Institute for Machine Learning.\n\nIlia Shumailov, Yiren Zhao, Daniel Bates, et al. 2021. Sponge Examples: Energy-Latency Attacks on Neural Networks. arXiv:2006.03463.\n\nMarco Barreno, Blaine Nelson, Anthony D. Joseph, et al. 2010. The Security of Machine\n\nLearning. Machine Learning 81 (2010), 121-148.\n\nMatt Fredrikson, Somesh Jha, Thomas Ristenpart. 2015. Model Inversion Attacks that\n\nExploit Confidence Information and Basic Countermeasures. In Proceedings of the 22nd\n\nACM SIGSAC Conference on Computer and Communications Security (CCS '15), October\n\n2015. Association for Computing Machinery, New York, NY, USA, 1322\n\n-\n\n1333.\n\nNational Institute for Standards and Technology (NIST). 2022. Cybersecurity Framework.\n\nNicolas Papernot. 2018. A Marauder's Map of Security and Privacy in Machine Learning. arXiv:1811.01134.\n\nReza Shokri, Marco Stronati, Congzheng Song, et al. 2017. Membership Inference Attacks against Machine Learning Models. arXiv:1610.05820.\n\nAdversarial Threat Matrix (MITRE). 2021.\n\nInterpretability and Explainability Approaches\n\nChaofan Chen, Oscar Li, Chaofan Tao, et al. 2019. This Looks Like That: Deep Learning for Interpretable Image Recognition. arXiv:1806.10574.\n\nCynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. arXiv:1811.10154.\n\nDaniel W. Apley, Jingyu Zhu. 2019. Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models. arXiv:1612.08468.\n\nDavid A. Broniatowski. 2021. Psychological Foundations of Explainability and\n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology\n\n(NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD.\n\nForough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. arXiv:1802.07810.\n\nHongyu Yang, Cynthia Rudin, Margo Seltzer. 2017. Scalable Bayesian Rule Lists. arXiv:1602.08610.\n\nP. Jonathon Phillips, Carina A. Hahn, Peter C. Fontana, et al. 2021. Four Principles of Explainable Artificial Intelligence. National Institute of Standards and Technology (NIST) IR 8312. National Institute of Standards and Technology, Gaithersburg, MD.\n\nScott Lundberg, Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. arXiv:1705.07874.\n\nSusanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021). pairwise interactions. In Proceedings of the 19th ACM SIGKDD international conference on\n\nYin Lou, Rich Caruana, Johannes Gehrke, et al. 2013. Accurate intelligible models with Knowledge discovery and data mining (KDD '13), August 2013. Association for Computing Machinery, New York, NY, USA, 623 -631.\n\n## Post-Decommission\n\nUpol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. 'The Algorithmic Imprint.' Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (2022).\n\n## Privacy Resources\n\nNational Institute for Standards and Technology (NIST). 2022. Privacy Framework.\n\n## Data Governance\n\nMarijn Janssen, Paul Brous, Elsa Estevez, Luis S. Barbosa, Tomasz Janowski, Data governance: Organizing data for trustworthy Artificial Intelligence, Government Information Quarterly, Volume 37, Issue 3, 2020, 101493, ISSN 0740-624X.\n\n## Software Resources\n\n- PiML (explainable models, performance assessment)\n- Interpret (explainable models)\n- Iml (explainable models)\n- Drifter library (performance assessment)\n- Manifold library (performance assessment)\n- SALib library (performance assessment)\n- What-If Tool (performance assessment)\n- MLextend (performance assessment)\n\n- AI Fairness 360:\n\nâ€¢\n\n- Python (bias testing and mitigation)\n- R (bias testing and mitigation)\n- Adversarial-robustness-toolbox (ML security)\n- Robustness (ML security)\n- tensorflow/privacy (ML security)\n- NIST De-identification Tools (Privacy and ML security)\n- Dvc (MLops, deployment)\n- Gigantum (MLops, deployment)\n- Mlflow (MLops, deployment)\n- Mlmd (MLops, deployment)\n- Modeldb (MLops, deployment)\n\n## MANAGE 2.3\n\nProcedures are followed to respond to and recover from a previously unknown risk when it is identified.\n\n## About\n\nAI systems -like any technology -can demonstrate non-functionality or failure or unexpected and unusual behavior. They also can be subject to attacks, incidents, or other misuse or abuse -which their sources are not always known apriori. Organizations can establish, document, communicate and maintain treatment procedures to recognize and counter, mitigate and manage risks that were not previously identified.\n\n## Suggested Actions\n\n- Protocols, resources, and metrics  are in place for continual monitoring of AI systems' performance, trustworthiness, and alignment with contextual norms and values\n- Establish and regularly review treatment and response plans for incidents, negative impacts, or outcomes.\n- Establish and maintain procedures to regularly monitor system components for drift, decontextualization, or other AI system behavior factors,\n- Establish and maintain procedures for capturing feedback about negative impacts.\n- Verify contingency processes to handle any negative impacts associated with missioncritical AI systems, and to deactivate systems.\n- Enable preventive and post-hoc exploration of AI system limitations by relevant AI actor groups.\n- Decommission systems that exceed risk tolerances.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?\n- Are the responsibilities of the personnel involved in the various AI governance processes clearly defined? (Including responsibilities to decommission the AI system.)\n- What processes exist for data generation, acquisition/collection, ingestion, staging/storage, transformations, security, maintenance, and dissemination?\n- How will the appropriate performance metrics, such as accuracy, of the AI be monitored after the AI is deployed?\n\n## AI Transparency Resources\n\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF - Companion to the Model AI Governance Framework -Implementation and SelfAssessment Guide for Organizations.\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n\nReferences AI Incident Database. 2022. AI Incident Database. AIAAIC Repository. 2022. AI, algorithmic and automation incidents collected, dissected, examined, and divulged. Andrew Burt and Patrick Hall. 2018. What to Do When AI Fails. O'Reilly Media, Inc. (May 18, 2020). Retrieved October 17, 2022. National Institute for Standards and Technology (NIST). 2022. Cybersecurity Framework. SANS Institute. 2022. Security Consensus Operational Readiness Evaluation (SCORE) Security Checklist [or Advanced Persistent Threat (APT) Handling Checklist]. Suchi Saria, Adarsh Subbaswamy. 2019. Tutorial: Safe and Reliable Machine Learning.\n\narXiv:1904.07204.\n\n## MANAGE 2.4\n\nMechanisms are in place and applied, responsibilities are assigned and understood to supersede, disengage, or deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use.\n\n## About\n\nPerformance inconsistent with intended use does not always increase risk or lead to negative impacts. Rigorous TEVV practices are useful for protecting against negative impacts regardless of intended use. When negative impacts do arise, superseding (bypassing), disengaging, or deactivating/decommissioning a model, AI system component(s), or the entire AI system may be necessary, such as when:\n\n- a system reaches the end of its lifetime\n- detected or identified risks exceed tolerance thresholds\n- adequate system mitigation actions are beyond the organization's capacity\n- feasible system mitigation actions do not meet regulatory, legal, norms or standards.\n- impending risk is detected during continual monitoring, for which feasible mitigation cannot be identified or implemented in a timely fashion.\n\nSafely removing AI systems from operation, either temporarily or permanently, under these scenarios requires standard protocols that minimize operational disruption and downstream negative impacts. Protocols can involve redundant or backup systems that are developed in alignment with established system governance policies (see GOVERN 1.7), regulatory compliance, legal frameworks, business requirements and norms and l standards within the application context of use. Decision thresholds and metrics for actions to bypass or deactivate system components are part of continual monitoring procedures. Incidents that result in a bypass/deactivate decision require documentation and review to understand root causes, impacts, and potential opportunities for mitigation and redeployment. Organizations are encouraged to develop risk and change management\n\nprotocols that consider and anticipate upstream and downstream consequences of both temporary and/or permanent decommissioning, and provide contingency options.\n\n## Suggested Actions\n\n- Regularly review established procedures for AI system bypass actions, including plans for redundant or backup systems to ensure continuity of operational and/or business functionality.\n- Regularly review Identify system incident thresholds for activating bypass or deactivation responses.\n- Apply change management processes to understand the upstream and downstream consequences of bypassing or deactivating an AI system or AI system components.\n- Apply protocols, resources and metrics for decisions to supersede, bypass or deactivate AI systems or AI system components.\n- Preserve materials for forensic, regulatory, and legal review.\n- Conduct internal root cause analysis and process reviews of bypass or deactivation events.\n- Decommission and preserve system components that cannot be updated to meet criteria for redeployment.\n- Establish criteria for redeploying updated system components, in consideration of trustworthy characteristics\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?\n- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?\n- What testing, if any, has the entity conducted on the AI system to identify errors and limitations (i.e. adversarial or stress testing)?\n- To what extent does the entity have established procedures for retiring the AI system, if it is no longer needed?\n- How did the entity use assessments and/or evaluations to determine if the system can be scaled up, continue, or be decommissioned?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n\nReferences Decommissioning Template. Application Lifecycle And Supporting Docs. Cloud and Infrastructure Community of Practice.\n\nDevelop a Decommission Plan. M3 Playbook. Office of Shared Services and Solutions and Performance Improvement. General Services Administration.\n\n## MANAGE 3.1\n\nAI risks and benefits from third-party resources are regularly monitored, and risk controls are applied and documented.\n\n## About\n\nAI systems may depend on external resources and associated processes, including thirdparty data, software or hardware systems. Third parties' supplying organizations with components and services, including tools, software, and expertise for AI system design, development, deployment or use can improve efficiency and scalability. It can also increase complexity and opacity, and, in-turn, risk. Documenting third-party technologies, personnel, and resources that were employed can help manage risks. Focusing first and foremost on risks involving physical safety, legal liabilities, regulatory compliance, and negative impacts on individuals, groups, or society is recommended.\n\n## Suggested Actions\n\n- Have legal requirements been addressed?\n- Apply organizational risk tolerance to third-party AI systems.\n- Apply and document organizational risk management plans and practices to third-party AI technology, personnel, or other resources.\n- Identify and maintain documentation for third-party AI systems and components.\n- Establish testing, evaluation, validation and verification processes for third-party AI systems which address the needs for transparency without exposing proprietary algorithms .\n- Establish processes to identify beneficial use and risk indicators in third-party systems or components, such as inconsistent software release schedule, sparse documentation, and incomplete software change management (e.g., lack of forward or backward compatibility).\n- Organizations can establish processes for third parties to report known and potential vulnerabilities, risks or biases in supplied resources.\n- Verify contingency processes for handling negative impacts associated with missioncritical third-party AI systems.\n- Monitor third-party AI systems for potential negative impacts and risks associated with trustworthiness characteristics.\n- Decommission third-party systems that exceed risk tolerances.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- If a third party created the AI system or some of its components, how will you ensure a level of explainability or interpretability? Is there documentation?\n\n- If your organization obtained datasets from a third party, did your organization assess and manage the risks of using such datasets?\n- Did you establish a process for third parties (e.g. suppliers, end users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?\n- Have legal requirements been addressed?\n\n## AI Transparency Resources\n\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF - Companion to the Model AI Governance Framework -Implementation and SelfAssessment Guide for Organizations.\n- Datasheets for Datasets.\n\nReferences Office of the Comptroller of the Currency. 2021. Proposed Interagency Guidance on ThirdParty Relationships: Risk Management. July 12, 2021.\n\n## MANAGE 3.2\n\nPre-trained models which are used for development are monitored as part of AI system regular  monitoring and maintenance.\n\n## About\n\nA common approach in AI development is transfer learning, whereby an existing pretrained model is adapted for use in a different, but related application. AI actors in development tasks often use pre-trained models from third-party entities for tasks such as image classification, language prediction, and entity recognition, because the resources to build such models may not be readily available to most organizations. Pre-trained models are typically trained to address various classification or prediction problems, using exceedingly large datasets and computationally intensive resources. The use of pre-trained models can make it difficult to anticipate negative system outcomes or impacts. Lack of documentation or transparency tools increases the difficulty and general complexity when deploying pre-trained models and hinders root cause analyses.\n\n## Suggested Actions\n\n- Identify pre-trained models within AI system inventory for risk tracking.\n- Establish processes to independently and continually monitor performance and trustworthiness  of pre-trained models, and as part of third-party risk tracking.\n- Monitor performance and trustworthiness of AI system components connected to pretrained models, and as part of third-party risk tracking.\n- Identify, document and remediate risks arising from AI system components and pretrained models per organizational risk management procedures, and as part of thirdparty risk tracking.\n- Decommission AI system components and pre-trained models which exceed risk tolerances, and as part of third-party risk tracking.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- How has the entity documented the AI system's data provenance, including sources, origins, transformations, augmentations, labels, dependencies, constraints, and metadata?\n- Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet?\n- How does the entity ensure that the data collected are adequate, relevant, and not excessive in relation to the intended purpose?\n- If the dataset becomes obsolete how will this be communicated?\n\n## AI Transparency Resources\n\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF - Companion to the Model AI Governance Framework -Implementation and SelfAssessment Guide for Organizations.\n- Datasheets for Datasets.\n\nReferences Larysa Visengeriyeva et al. 'Awesome MLOps,' GitHub. Accessed January 9, 2023.\n\n## MANAGE 4.1\n\nPost-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating input from users and other relevant AI actors, appeal and override, decommissioning, incident response, recovery, and change management.\n\n## About\n\nAI system performance and trustworthiness can change due to a variety of factors. Regular AI system monitoring can help deployers identify performance degradations, adversarial attacks, unexpected and unusual behavior, near-misses, and impacts. Including pre- and post-deployment external feedback about AI system performance can enhance organizational awareness about positive and negative impacts, and reduce the time to respond to risks and harms.\n\n## Suggested Actions\n\n- Establish and maintain procedures to monitor AI system performance for risks and negative and positive impacts associated with trustworthiness characteristics.\n- Perform post-deployment TEVV tasks to evaluate AI system validity and reliability, bias and fairness, privacy, and security and resilience.\n- Evaluate AI system trustworthiness in conditions similar to deployment context of use, and prior to deployment.\n- Establish and implement red-teaming exercises at a prescribed cadence, and evaluate their efficacy.\n\n- Establish procedures for tracking dataset modifications such as data deletion or rectification requests.\n- Establish mechanisms for regular communication and feedback between relevant AI actors and internal or external stakeholders to capture information about system performance, trustworthiness and impact.\n- Share information about errors, near-misses, and attack patterns with incident databases, other organizations with similar systems, and system users and stakeholders.\n- Respond to and document detected or reported negative impacts or issues in AI system performance and trustworthiness.\n- Decommission systems that exceed establish risk tolerances.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- To what extent has the entity documented the postdeployment AI system's testing methodology, metrics, and performance outcomes?\n- How easily accessible and current is the information available to external stakeholders?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities,\n- Datasheets for Datasets.\n\n## References\n\nNavdeep Gill, Patrick Hall, Kim Montgomery, and Nicholas Schmidt. \"A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrimination Testing.\" Information 11, no. 3 (2020): 137.\n\n## MANAGE 4.2\n\nMeasurable activities for continual improvements are integrated into AI system updates and include regular engagement with interested parties, including relevant AI actors.\n\n## About\n\nRegular monitoring processes enable system updates to enhance performance and functionality in accordance with regulatory and legal frameworks, and organizational and contextual values and norms. These processes also facilitate analyses of root causes, system degradation, drift, near-misses, and failures, and incident response and documentation. AI actors across the lifecycle have many opportunities to capture and incorporate external feedback about system performance, limitations, and impacts, and implement continuous improvements. Improvements may not always be to model pipeline or system processes, and may instead be based on metrics beyond accuracy or other quality performance measures. In these cases, improvements may entail adaptations to business or organizational procedures or practices. Organizations are encouraged to develop\n\nimprovements that will maintain traceability and transparency for developers, end users, auditors, and relevant AI actors.\n\n## Suggested Actions\n\n- Integrate trustworthiness characteristics into protocols and metrics used for continual improvement.\n- Establish processes for evaluating and integrating feedback into AI system improvements.\n- Assess and evaluate alignment of proposed improvements with relevant regulatory and legal frameworks\n- Assess and evaluate alignment of proposed improvements connected to the values and norms within the context of use.\n- Document the basis for decisions made relative to tradeoffs between trustworthy characteristics, system risks, and system opportunities\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- How will user and other forms of stakeholder engagement be integrated into the model development process and regular performance review once deployed?\n- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n- To what extent has the entity defined and documented the regulatory environment -including minimum requirements in laws and regulations?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities,\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\n## References\n\nYen, Po-Yin, et al. \"Development and Evaluation of Socio-Technical Metrics to Inform HIT Adaptation.\"\n\nCarayon, Pascale, and Megan E. Salwei. \"Moving toward a sociotechnical systems approach to continuous health information technology design: the path forward for improving electronic health record usability and reducing clinician burnout.\" Journal of the American Medical Informatics Association 28.5 (2021): 1026-1028.\n\nMishra, Deepa, et al. \"Organizational capabilities that enable big data and predictive analytics diffusion and organizational performance: A resource-based perspective.\" Management Decision (2018).\n\n## MANAGE 4.3\n\nIncidents and errors are communicated to relevant AI actors including affected communities. Processes for tracking, responding to, and recovering from incidents and errors are followed and documented.\n\n## About\n\nRegularly documenting an accurate and transparent account of identified and reported errors can enhance AI risk management activities., Examples include:\n\n- how errors were identified,\n- incidents related to the error,\n- whether the error has been repaired, and\n- how repairs can be distributed to all impacted stakeholders and users.\n\n## Suggested Actions\n\n- Establish procedures to regularly share information about errors, incidents and negative impacts with relevant stakeholders, operators, practitioners and users, and impacted parties.\n- Maintain a database of reported errors, near-misses, incidents and negative impacts including date reported, number of reports, assessment of impact and severity, and responses.\n- Maintain a database of system changes, reason for change, and details of how the change was made, tested and deployed.\n- Maintain version history information and metadata to enable continuous improvement processes.\n- Verify that relevant AI actors responsible for identifying complex or emergent risks are properly resourced and empowered.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- What corrective actions has the entity taken to enhance the quality, accuracy, reliability, and representativeness of the data?\n- To what extent does the entity communicate its AI strategic goals and objectives to the community of stakeholders? How easily accessible and current is the information available to external stakeholders?\n- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities,\n\nReferences\n\nWei, M., &amp; Zhou, Z. (2022). AI Ethics Issues in Real World: Evidence from AI Incident Database. ArXiv, abs/2206.07635. McGregor, Sean. \"Preventing repeated real world AI failures by cataloging incidents: The AI incident database.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 17. 2021. Macrae, Carl. \"Learning from the failure of autonomous and intelligent systems: Accidents, safety, and sociotechnical sources of risk.\" Risk analysis 42.9 (2022): 1999-2025.\n\n## MAP\n\n<!-- image -->\n\n## Map\n\nContext is established and understood.\n\n## MAP 1.1\n\nIntended purpose, potentially beneficial uses, context-specific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and documented. Considerations include: specific set or types of users along with their expectations; potential positive and negative impacts of system uses to individuals, communities, organizations, society, and the planet; assumptions and related limitations about AI system purposes; uses and risks across the development or product AI lifecycle; TEVV and system metrics.\n\n## About\n\nHighly accurate and optimized systems can cause harm. Relatedly, organizations should expect broadly deployed AI tools to be reused, repurposed, and potentially misused regardless of intentions. AI actors can work collaboratively, and with external parties such as community groups, to help delineate the bounds of acceptable deployment, consider preferable alternatives, and identify principles and strategies to manage likely risks. Context mapping is the first step in this effort, and may include examination of the following:\n\n- intended purpose and impact of system use.\n- concept of operations.\n- intended, prospective, and actual deployment setting.\n- requirements for system deployment and operation.\n- end user and operator expectations.\n- specific set or types of end users.\n- potential negative impacts to individuals, groups, communities, organizations, and society -or context-specific impacts such as legal requirements or impacts to the environment.\n- unanticipated, downstream, or other unknown contextual factors.\n- how AI system changes connect to impacts.\n\nThese types of processes can assist AI actors in understanding how limitations, constraints, and other realities associated with the deployment and use of AI technology can create impacts once they are deployed or operate in the real world. When coupled with the enhanced organizational culture resulting from the established policies and procedures in the Govern function, the Map function can provide opportunities to foster and instill new perspectives, activities, and skills for approaching risks and impacts. Context mapping also includes discussion and consideration of non-AI or  non-technology alternatives especially as related to whether the given context is narrow enough to manage\n\nAI and its potential negative impacts. Non-AI alternatives may include capturing and evaluating information using semi-autonomous or mostly-manual methods.\n\n## Suggested Actions\n\n- Maintain awareness of industry, technical, and applicable legal standards.\n- Examine trustworthiness of AI system design and consider, non-AI solutions\n- Consider intended AI system design tasks along with unanticipated purposes in collaboration with human factors and socio-technical domain experts.\n- Define and document the task, purpose, minimum functionality, and benefits of the AI system to inform considerations about whether the utility of the project or its lack of.\n- Identify whether there are non-AI or non-technology alternatives that will lead to more trustworthy outcomes.\n- Examine how changes in system performance affect downstream events such as decision-making (e.g: changes in an AI model objective function create what types of impacts in how many candidates do/do not get a job interview).\n- Determine actions to map and track post-decommissioning stages of AI deployment and potential negative or positive impacts to individuals, groups and communities.\n- Determine the end user and organizational requirements, including business and technical requirements.\n- Determine and delineate the expected and acceptable AI system context of use, including:\n- social norms\n- Impacted individuals, groups, and communities\n- potential positive and negative impacts to individuals, groups, communities, organizations, and society\n- operational environment\n- Perform context analysis related to time frame, safety concerns, geographic area, physical environment, ecosystems, social environment, and cultural norms within the intended setting (or conditions that closely approximate the intended setting.\n- Gain and maintain awareness about evaluating scientific claims related to AI system performance and benefits before launching into system design.\n- Identify human-AI interaction and/or roles, such as whether the application will support or replace human decision making.\n- Plan for risks related to human-AI configurations, and document requirements, roles, and responsibilities for human oversight of deployed systems.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent is the output of each component appropriate for the operational context?\n\n- Which AI actors are responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?\n- Which AI actors are responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?\n- Who is the person(s) accountable for the ethical considerations across the AI lifecycle?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities,\n- 'Stakeholders in Explainable AI,' Sep. 2018.\n- \"Microsoft Responsible AI Standard, v2\".\n\n## References\n\n## Socio-technical systems\n\nAndrew D. Selbst, danah boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAccT'19). Association for Computing Machinery, New York, NY, USA, 59 -68.\n\n## Problem formulation\n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702.\n\nSamir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAccT'19). Association for Computing Machinery, New York, NY, USA, 39 -48.\n\n## Context mapping\n\nEmilio GÃ³mez-GonzÃ¡lez and Emilia GÃ³mez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission).\n\nSarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676.\n\nSocial Impact Lab. 2017. Framework for Context Analysis of Technologies in Social Change\n\nProjects (Draft v2.0).\n\nSolon Barocas, Asia J. Biega, Margarita Boyarskaya, et al. 2021. Responsible computing during COVID-19 and beyond. Commun. ACM 64, 7 (July 2021), 30 -32.\n\n## Identification of harms\n\nHarini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002.\n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416.\n\nMicrosoft. Foundations of assessing harm. 2022.\n\nUnderstanding and documenting limitations in ML\n\nAlexander D'Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification\n\nPresents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395.\n\nArvind Narayanan. \"How to Recognize AI Snake Oil.\" Arthur Miller Lecture on Science and\n\nEthics (2019).\n\nJessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing,\n\nExploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363.\n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model\n\nReporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency\n\n(FAT* '19). Association for Computing Machinery, New York, NY, USA, 220\n\n-229.\n\nMatthew Arnold, Rachel K. E. Bellamy, Michael Hind, et al. 2019. FactSheets: Increasing\n\nTrust in AI Services through Supplier's Declarations of Conformity. arXiv:1808.07261.\n\nMatthew J. Salganik, Ian Lundberg, Alexander T. Kindel, Caitlin E. Ahearn, Khaled Al-\n\nGhoneim, Abdullah Almaatouq, Drew M. Altschul et al. \"Measuring the Predictability of Life\n\nOutcomes with a Scientific Mass Collaboration.\" Proceedings of the National Academy of\n\nSciences 117, No. 15 (2020): 8398-8403.\n\nMichael A. Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co-\n\nDesigning Checklists to Understand Organizational Challenges and Opportunities around\n\nFairness in AI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing\n\nSystems (CHI '20). Association for Computing Machinery, New York, NY, USA, 1-\n\n14.\n\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. arXiv:1803.09010.\n\nBender, E. M., Friedman, B. &amp; McMillan-Major, A.,  (2022). A Guide for Writing Data\n\nStatements for Natural Language Processing. University of Washington.  Accessed July 14,\n\n2022.\n\nMeta AI. System Cards, a new resource for understanding how AI systems work, 2021.\n\nWhen not to deploy\n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy.\n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT*\n\n'20). Association for Computing Machinery, New York, NY, USA, 695.\n\nPost-decommission\n\nUpol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. 'The Algorithmic Imprint.'\n\nProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency\n\n(2022).\n\n## Statistical balance\n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453.\n\n## Assessment of science in AI\n\nArvind Narayanan. How to recognize AI snake oil.\n\nEmily M. Bender. 2022. On NYT Magazine on AI: Resist the Urge to be Impressed. (April 17, 2022).\n\n## MAP 1.2\n\nInter-disciplinary AI actors, competencies, skills and capacities for establishing context reflect demographic diversity and broad domain and user experience expertise, and their participation is documented. Opportunities for interdisciplinary collaboration are prioritized.\n\n## About\n\nSuccessfully mapping context requires a team of AI actors with a diversity of experience, expertise, abilities and backgrounds, and with the resources and independence to engage in critical inquiry. Having a diverse team contributes to more  broad and open sharing of ideas and assumptions about the purpose and function of the technology being designed and developed -making these implicit aspects more explicit. The benefit of a diverse staff in managing AI risks is not the beliefs or presumed beliefs of individual workers, but the behavior that results from a collective perspective. An environment which fosters critical inquiry creates opportunities to surface problems and identify existing and emergent risks.\n\n## Suggested Actions\n\n- Establish interdisciplinary teams to reflect a wide range of skills, competencies, and capabilities for AI efforts. Verify that team membership includes demographic diversity, broad domain expertise, and lived experiences. Document team composition.\n- Create and empower interdisciplinary expert teams to capture, learn, and engage the interdependencies of deployed AI systems and related terminologies and concepts from disciplines outside of AI practice such as law, sociology, psychology, anthropology, public policy, systems design, and engineering.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent do the teams responsible for developing and maintaining the AI system reflect diverse opinions, backgrounds, experiences, and perspectives?\n\n- Did the entity document the demographics of those involved in the design and development of the AI system to capture and communicate potential biases inherent to the development process, according to forum participants?\n- What specific perspectives did stakeholders share, and how were they integrated across the design, development, deployment, assessment, and monitoring of the AI system?\n- To what extent has the entity addressed stakeholder perspectives on the potential negative impacts of the AI system on end users and impacted populations?\n- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n- Did your organization address usability problems and test whether user interfaces served their intended purposes? Consulting the community or end users at the earliest stages of development to ensure there is transparency on the technology used and how it is deployed.\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019.\n\nReferences\n\nSina Fazelpour and Maria De-Arteaga. 2022. Diversity in sociotechnical machine learning systems. Big Data &amp; Society 9, 1 (Jan. 2022).\n\nMicrosoft Community Jury , Azure Application Architecture Guide.\n\nFernando Delgado, Stephen Yang, Michael Madaio, Qian Yang. (2021). Stakeholder Participation in AI: Beyond \"Add Diverse Stakeholders and Stir\".\n\nKush Varshney, Tina Park, Inioluwa Deborah Raji, Gaurush Hiranandani, Narasimhan Harikrishna, Oluwasanmi Koyejo, Brianna Richardson, and Min Kyung Lee. Participatory specification of trustworthy machine learning, 2021.\n\nDonald Martin, Vinodkumar Prabhakaran, Jill A. Kuhlberg, Andrew Smart and William S. Isaac. 'Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics', ArXiv abs/2005.07572 (2020).\n\n## MAP 1.3\n\nThe organization's mission and relevant goals for the AI technology are understood and documented.\n\n## About\n\nDefining and documenting the specific business purpose of an AI system in a broader context of societal values helps teams to evaluate risks and increases the clarity of 'go/no -go' decisions about whether to deploy. Trustworthy AI technologies may present a demonstrable business benefit beyond implicit or explicit costs, provide added value, and don't lead to wasted resources. Organizations can feel confident in performing risk avoidance if the implicit or explicit risks outweigh the advantages of AI systems,  and  not implementing an AI solution whose risks surpass potential benefits. For example, making AI systems more equitable can result in better managed risk, and can help enhance consideration of the business value of making inclusively designed, accessible and more equitable AI systems.\n\n## Suggested Actions\n\n- Build transparent practices into AI system development processes.\n- Review the documented system purpose from a socio-technical perspective and in consideration of societal values.\n- Determine possible misalignment between societal values and stated organizational principles and code of ethics.\n- Flag latent incentives that may contribute to negative impacts.\n- Evaluate AI system purpose in consideration of potential risks, societal values, and stated organizational principles.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- How does the AI system help the entity meet its goals and objectives?\n- How do the technical specifications and requirements align with the AI system's goals and objectives?\n- To what extent is the output appropriate for the operational context?\n\n## AI Transparency Resources\n\n- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI -2019, [LINK](https://altai.insight-centre.org/),\n- Including Insights from the Comptroller General's Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities, 2021,\n\nReferences M.S. Ackerman (2000). The Intellectual Challenge of CSCW: The Gap Between Social Requirements and Technical Feasibility. Human -Computer Interaction, 15, 179 - 203.\n\n437\n\nMcKane Andrus, Sarah Dean, Thomas Gilbert,  Nathan Lambert, Tom Zick (2021). AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks. Abeba Birhane, Pratyusha Kalluri, Dallas Card, et al. 2022. The Values Encoded in Machine Learning Research. arXiv:2106.15590. Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). Iason Gabriel, Artificial Intelligence, Values, and Alignment. Minds &amp; Machines 30, 411 -(2020). PEAT 'Business Case for Equitable AI'.\n\n## MAP 1.4\n\nThe business value or context of business use has been clearly defined or -in the case of assessing existing AI systems -re-evaluated.\n\n## About\n\nSocio-technical AI risks emerge from the interplay between technical development decisions and how a system is used, who operates it, and the social context into which it is deployed. Addressing these risks is complex and requires a commitment to understanding how contextual factors may interact with AI lifecycle actions. One such contextual factor is how organizational mission and identified system purpose create incentives within AI system design, development, and deployment tasks that may result in positive and negative impacts. By establishing comprehensive and explicit enumeration of AI systems' context of of business use and expectations, organizations can identify and manage these types of risks.\n\n## Suggested Actions\n\n- Document business value or context of business use\n- Reconcile documented concerns about the system's purpose within the business context of use  compared to the organization's stated values, mission statements, social responsibility commitments, and AI principles.\n- Reconsider the design, implementation strategy, or deployment of AI systems with potential impacts that do not reflect institutional values.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- What goals and objectives does the entity expect to achieve by designing, developing, and/or deploying the AI system?\n- To what extent are the system outputs consistent with the entity's values and principles to foster public trust and equity?\n- To what extent are the metrics consistent with system goals, objectives, and constraints, including ethical and compliance considerations?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020.\n- WEF Model AI Governance Framework Assessment 2020.\n\nReferences Algorithm Watch. AI Ethics Guidelines Global Inventory. Ethical OS toolkit. Emanuel Moss and Jacob Metcalf. 2020. Ethics Owners: A New Model of Organizational Responsibility in Data-Driven Technology Companies. Data &amp; Society Research Institute. Future of Life Institute. Asilomar AI Principles. Leonard Haas, Sebastian GieÃŸler, and Veronika Thiel. 2020. In the realm of paper tigers\n\n-exploring the failings of AI ethics guidelines. (April 28, 2020).\n\n## MAP 1.5\n\nOrganizational risk tolerances are determined and documented.\n\n## About\n\nRisk tolerance reflects the level and type of risk the organization is willing to accept while conducting its mission and carrying out its strategy. Organizations can follow existing regulations and guidelines for risk criteria, tolerance and response established by organizational, domain, discipline, sector, or professional requirements. Some sectors or industries may have established definitions of harm or may have established documentation, reporting, and disclosure requirements. Within sectors, risk management may depend on existing guidelines for specific applications and use case settings. Where established guidelines do not exist, organizations will want to define reasonable risk tolerance in consideration of different sources of risk (e.g., financial, operational, safety and wellbeing, business, reputational, and model risks) and different levels of risk (e.g., from negligible to critical). Risk tolerances inform and support decisions about whether to continue with development or deployment termed 'go/no -go'. Go/no -go decisions related to AI system risks can take stakeholder feedback into account, but remain independent from stakeholders' vested financial or reputational interests. If mapping risk is prohibitively difficult, a \"no-go\" decision may be considered for the specific system.\n\n## Suggested Actions\n\n- Utilize existing regulations and guidelines for risk criteria, tolerance and response established by organizational, domain, discipline, sector, or professional requirements.\n\n- Establish risk tolerance levels for AI systems and allocate the appropriate oversight resources to each level.\n- Establish risk criteria in consideration of different sources of risk, (e.g., financial, operational, safety and wellbeing, business, reputational, and model risks) and different levels of risk (e.g., from negligible to critical).\n- Identify maximum allowable risk tolerance above which the system will not be deployed, or will need to be prematurely decommissioned, within the contextual or application setting.\n- Articulate and analyze tradeoffs across trustworthiness characteristics as relevant to proposed context of use.  When tradeoffs arise, document them and plan for traceable actions (e.g.: impact mitigation, removal of system from development or use) to inform management decisions.\n- Review uses of AI systems for 'off -label' purposes, especially in settings that organizations have deemed as high-risk. Document decisions, risk-related trade-offs, and system limitations.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- Which existing regulations and guidelines apply, and the entity has followed, in the development of system risk tolerances?\n- What criteria and assumptions has the entity utilized when developing system risk tolerances?\n- How has the entity identified maximum allowable risk tolerance?\n- What conditions and purposes are considered 'off -label' for system use?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n\nReferences Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). The Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, 2019). Brenda Boultwood, How to Develop an Enterprise Risk-Rating Approach (Aug. 26, 2021). Global Association of Risk Professionals (garp.org). Accessed Jan. 4, 2023. Virginia Eubanks, 1972-, Automating Inequality: How High-tech Tools Profile, Police, and\n\nPunish the Poor. New York, NY, St. Martin's Press, 2018.\n\nGAO-1763: Enterprise Risk Management: Selected Agencies' Experiences Illustrate Good Practices in Managing Risk. NIST Risk Management Framework.\n\n## MAP 1.6\n\nSystem requirements (e.g., 'the system shall respect the privacy of its users') are elicited from and understood by relevant AI actors.  Design decisions take socio-technical implications into account to address AI risks.\n\n## About\n\nAI system development requirements may outpace documentation processes for traditional software. When written requirements are unavailable or incomplete, AI actors may inadvertently overlook business and stakeholder needs, over-rely on implicit human biases such as confirmation bias and groupthink, and maintain exclusive focus on computational requirements. Eliciting system requirements, designing for end users, and considering societal impacts early in the design phase is a priority that can enhance AI systems' trustworthiness.\n\n## Suggested Actions\n\n- Proactively incorporate trustworthy characteristics into system requirements.\n- Establish mechanisms for regular communication and feedback between relevant AI actors and internal or external stakeholders related to system design or deployment decisions.\n- Develop and standardize practices to assess potential impacts at all stages of the AI lifecycle, and in collaboration with interdisciplinary experts, actors external to the team that developed or deployed the AI system, and potentially impacted communities .\n- Include potentially impacted groups, communities and external entities (e.g. civil society organizations, research institutes, local community groups, and trade associations) in the formulation of priorities, definitions and outcomes during impact assessment activities.\n- Conduct qualitative interviews with end user(s) to regularly evaluate expectations and design plans related to Human-AI configurations and tasks.\n- Analyze dependencies between contextual factors and system requirements. List potential impacts that may arise from not fully considering the importance of trustworthiness characteristics in any decision making.\n- Follow responsible design techniques in tasks such as software engineering, product management, and participatory engagement. Some examples for eliciting and documenting stakeholder requirements include product requirement documents (PRDs), user stories, user interaction/user experience (UI/UX) research, systems engineering, ethnography and related field methods.\n\n- Conduct user research to understand individuals, groups and communities that will be impacted by the AI, their values &amp; context, and the role of systemic and historical biases. Integrate learnings into decisions about data selection and representation.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n- To what extent is this information sufficient and appropriate to promote transparency? Promote transparency by enabling external stakeholders to access information on the design, operation, and limitations of the AI system.\n- To what extent has relevant information been disclosed regarding the use of AI systems, such as (a) what the system is for, (b) what it is not for, (c) how it was designed, and (d) what its limitations are? (Documentation and external communication can offer a way for entities to provide transparency.)\n- How will the relevant AI actor(s) address changes in accuracy and precision due to either an adversary's attempts to disrupt the AI system or unrelated changes in the operational/business environment, which may impact the accuracy of the AI system?\n- What metrics has the entity developed to measure performance of the AI system?\n- What justifications, if any, has the entity provided for the assumptions, boundaries, and limitations of the AI system?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Stakeholders in Explainable AI, Sep. 2018.\n- High-Level Expert Group on Artificial Intelligence set up by the European Commission, Ethics Guidelines for Trustworthy AI.\n\nReferences National Academies of Sciences, Engineering, and Medicine 2022. Fostering Responsible Computing Research: Foundations and Practices. Washington, DC: The National Academies Press. Abeba Birhane,  William S. Isaac, Vinodkumar Prabhakaran, Mark Diaz, Madeleine Clare Elish, Iason Gabriel and Shakir Mohamed. 'Power to the People? Opportunities and Challenges for Participatory AI.' Equity and Access in Algorithms, Mechanisms, and Optimization (2022). Amit K. Chopra, Fabiano Dalpiaz, F. BaÅŸak Aydemir, et al. 2014. Protos: Foundations for engineering innovative sociotechnical systems. In 2014 IEEE 22nd International\n\nRequirements Engineering Conference (RE) (2014), 53-62.\n\nAndrew D. Selbst, danah boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 59 -68. Gordon Baxter and Ian Sommerville. 2011. Socio-technical systems: From design methods to systems engineering. Interacting with Computers, 23, 1 (Jan. 2011), 4 -17. Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. Yilin Huang, Giacomo Poderi, Sanja Å Ä‡epanoviÄ‡, et al. 2019. Embedding Internet -of-Things in Large-Scale Socio-technical Systems: A Community-Oriented Design in Future Smart Grids. In The Internet of Things for Smart Urban Ecosystems (2019), 125-150. Springer, Cham. Victor Udoewa, (2022). An introduction to radical participatory design: decolonising participatory design processes. Design Science. 8. 10.1017/dsj.2022.24.\n\n## MAP 2.1\n\nThe specific task, and methods used to implement the task, that the AI system will support is defined (e.g., classifiers, generative models, recommenders).\n\n## About\n\nAI actors define the technical learning or decision-making task(s) an AI system is designed to accomplish, or the benefits that the system will provide. The clearer and narrower the task definition, the easier it is to map its benefits and risks, leading to more fulsome risk management.\n\n## Suggested Actions\n\n- Define and document AI system's existing and potential learning task(s) along with known assumptions and limitations.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent has the entity clearly defined technical specifications and requirements for the AI system?\n- To what extent has the entity documented the AI system's development, testing methodology, metrics, and performance outcomes?\n- How do the technical specifications and requirements align with the AI system's goals and objectives?\n- Did your organization implement accountability-based practices in data management and protection (e.g. the PDPA and OECD Privacy Principles)?\n- How are outputs marked to clearly show that they came from an AI?\n\n## AI Transparency Resources\n\n- Datasheets for Datasets.\n\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n- ATARC Model Transparency Assessment (WD) -2020.\n- Transparency in Artificial Intelligence - S. Larsson and F. Heintz -2020.\n\nReferences Leong, Brenda (2020). The Spectrum of Artificial Intelligence - An Infographic Tool. Future of Privacy Forum. Brownlee, Jason (2020). A Tour of Machine Learning Algorithms. Machine Learning\n\nMastery.\n\n## MAP 2.2\n\nInformation about the AI system's knowledge limits and how system output may be utilized and overseen by humans is documented. Documentation provides sufficient information to assist relevant AI actors when making informed decisions and taking subsequent actions.\n\n## About\n\nAn AI lifecycle consists of many interdependent activities involving a diverse set of actors that often do not have full visibility or control over other parts of the lifecycle and its associated contexts or risks. The interdependencies between these activities, and among the relevant AI actors and organizations, can make it difficult to reliably anticipate potential impacts of AI systems. For example, early decisions in identifying the purpose and objective of an AI system can alter its behavior and capabilities, and the dynamics of deployment setting (such as end users or impacted individuals) can shape the positive or negative impacts of AI system decisions. As a result, the best intentions within one dimension of the AI lifecycle can be undermined via interactions with decisions and conditions in other, later activities. This complexity and varying levels of visibility can introduce uncertainty. And, once deployed and in use, AI systems may sometimes perform poorly, manifest unanticipated negative impacts, or violate legal or ethical norms. These risks and incidents can result from a variety of factors. For example, downstream decisions can be influenced by end user over-trust or under-trust, and other complexities related to AI-supported decision-making. Anticipating, articulating, assessing and documenting AI systems' knowledge limits and how system output may be utilized and overseen by humans can help mitigate the uncertainty associated with the realities of AI system deployments. Rigorous design processes include defining system knowledge limits, which are confirmed and refined based on TEVV processes.\n\n## Suggested Actions\n\n- Document settings, environments and conditions that are outside the AI system's intended use.\n\n- Design for end user workflows and toolsets, concept of operations, and explainability and interpretability criteria in conjunction with end user(s) and associated qualitative feedback.\n- Plan and test human-AI configurations under close to real-world conditions and document results.\n- Follow stakeholder feedback processes to determine whether a system achieved its documented purpose within a given use context, and whether end users can correctly comprehend system outputs or results.\n- Document dependencies on upstream data and other AI systems, including if the specified system is an upstream dependency for another AI system or other data.\n- Document connections the AI system or data will have to external networks (including the internet), financial markets, and critical infrastructure that have potential for negative externalities. Identify and document negative impacts as part of considering the broader risk thresholds and subsequent go/no-go deployment as well as postdeployment decommissioning decisions.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- Does the AI system provide sufficient information to assist the personnel to make an informed decision and take actions accordingly?\n- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n- Based on the assessment, did your organization implement the appropriate level of human involvement in AI-augmented decision-making?\n\n## AI Transparency Resources\n\n- Datasheets for Datasets.\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n- ATARC Model Transparency Assessment (WD) -2020.\n- Transparency in Artificial Intelligence - S. Larsson and F. Heintz -2020.\n\nReferences Context of use International Standards Organization (ISO). 2019. ISO 9241-210:2019 Ergonomics of human-system interaction -Part 210: Human-centred design for interactive systems. National Institute of Standards and Technology (NIST), Mary Theofanos, Yee-Yin Choong, et al. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: Ensuring Successful Systems for First Responders.\n\nHuman-AI interaction Committee on Human-System Integration Research Topics for the 711th Human Performance Wing of the Air Force Research Laboratory and the National Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming: State-of-the-Art and Research Needs. Washington, D.C. National Academies Press.\n\nHuman Readiness Level Scale in the System Development Process, American National Standards Institute and Human Factors and Ergonomics Society, ANSI/HFES 400-2021\n\nMicrosoft Responsible AI Standard, v2.\n\nSaar Alon-Barkat, Madalina Busuioc, Human -AI Interactions in Public Sector Decision Making: 'Automation Bias' and 'Selective Adherence' to Algorithmic Advice, Journal of Public Administration Research and Theory, 2022;, muac007.\n\nZana BuÃ§inca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages.\n\nMary L. Cummings. 2006 Automation and accountability in decision support system interface design.The Journal of Technology Studies 32(1): 23 -31.\n\nEngstrom, D. F., Ho, D. E., Sharkey, C. M., &amp; CuÃ©llar, M. F. (2020). Government by algorithm: Artificial intelligence in federal administrative agencies. NYU School of Law, Public Law Research Paper, (20-54).\n\nSusanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021).\n\nBen Green. 2021. The Flaws of Policies Requiring Human Oversight of Government Algorithms. Computer Law &amp; Security Review 45 (26 Apr. 2021).\n\nBen Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to A.I. Harm. (June 15, 2021).\n\nGrgiÄ‡ -HlaÄa, N., Engel, C., &amp; Gummadi, K. P. (2019). Human decision making with machine assistance: An experiment on bailing and jailing. Proceedings of the ACM on HumanComputer Interaction, 3(CSCW), 1-25.\n\nForough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 237, 1 -52.\n\nC. J. Smith (2019). Designing trustworthy AI: A human-machine teaming framework to guide development. arXiv preprint arXiv:1910.03515.\n\nT. Warden, P. Carayon, EM  et al. The National Academies Board on Human System Integration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2019;63(1):631-635. doi:10.1177/1071181319631100.\n\n## MAP 2.3\n\nScientific integrity and TEVV considerations are identified and documented, including those related to experimental design, data collection and selection (e.g., availability, representativeness, suitability), system trustworthiness, and construct validation.\n\n## About\n\nStandard testing and evaluation protocols provide a basis to confirm assurance in a system that it is operating as designed and claimed. AI systems' complexities create challenges for traditional testing and evaluation methodologies, which tend to be designed for static or isolated system performance.  Opportunities for risk continue well beyond design and deployment, into system operation and application of system-enabled decisions. Testing and evaluation methodologies and metrics therefore address a continuum of activities. TEVV is enhanced when key metrics for performance, safety, and reliability are interpreted in a socio-technical context and not confined to the boundaries of the AI system pipeline. Other challenges for managing AI risks relate to dependence on large scale datasets, which can impact data quality and validity concerns. The difficulty of finding the 'right' data may lead AI actors to select datasets based more on accessibility and availability than on suitability for operationalizing the phenomenon that the AI system intends to support or inform. Such decisions could contribute to an environment where the data used in processes is not fully representative of the populations or phenomena that are being modeled, introducing downstream risks.  Practices such as dataset reuse may also lead to disconnect from the social contexts and time periods of their creation.  This contributes to issues of validity of the underlying dataset for providing proxies, measures, or predictors within the model.\n\n## Suggested Actions\n\n- Identify and document experiment design and statistical techniques that are valid for testing complex socio-technical systems like AI, which involve human factors, emergent properties, and dynamic context(s) of use.\n- Develop and apply TEVV protocols for models, system and its subcomponents, deployment, and operation.\n- Demonstrate and document that AI system performance and validation metrics are interpretable and unambiguous for downstream decision making tasks, and take sociotechnical factors such as context of use into consideration.\n- Identify and document assumptions,  techniques, and metrics used for testing and evaluation throughout the AI lifecycle including experimental design techniques for data collection, selection, and management practices in accordance with data governance policies established in GOVERN.\n\n- Identify testing modules that can be incorporated throughout the AI lifecycle, and verify that processes enable corroboration by independent evaluators.\n- Establish mechanisms for regular communication and feedback among relevant AI actors and internal or external stakeholders related to the validity of design and deployment assumptions.\n- Establish mechanisms for regular communication and feedback between relevant AI actors and internal or external stakeholders related to the development of TEVV approaches throughout the lifecycle to detect and assess potentially harmful impacts\n- Document assumptions made and techniques used in data selection, curation, preparation and analysis, including:\n- identification of constructs and proxy targets,\n- development of  indices -especially those operationalizing concepts that are inherently unobservable (e.g. 'hireability,' 'criminality.' 'lendability').\n- Map adherence to policies that address data and construct validity, bias, privacy and security for AI systems and verify documentation, oversight, and processes.\n- Identify and document transparent methods (e.g. causal discovery methods) for inferring causal relationships between constructs being modeled and dataset attributes or proxies.\n- Identify and document processes to understand and trace test and training data lineage and its metadata resources for mapping risks.\n- Document known limitations, risk mitigation efforts associated with, and methods used for, training data collection, selection, labeling, cleaning, and analysis (e.g. treatment of missing, spurious, or outlier data; biased estimators).\n- Establish and document practices to check for capabilities that are in excess of those that are planned for, such as emergent properties, and to revisit prior risk management steps in light of any new capabilities.\n- Establish processes to test and verify that design assumptions about the set of deployment contexts continue to be accurate and sufficiently complete.\n- Work with domain experts and other external AI actors to:\n- Gain and maintain contextual awareness and knowledge about how human behavior, organizational factors and dynamics, and society influence, and are represented in, datasets, processes, models, and system output.\n- Identify participatory approaches for responsible Human-AI configurations and oversight tasks, taking into account sources of cognitive bias.\n- Identify techniques to manage and mitigate sources of bias (systemic, computational, human- cognitive) in computational models and systems, and the assumptions and decisions in their development..\n- Investigate and document potential negative impacts due related to the full product lifecycle and associated processes that may conflict with organizational values and principles.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Are there any known errors, sources of noise, or redundancies in the data?\n- Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame\n- What is the variable selection and evaluation process?\n- How was the data collected? Who was involved in the data collection process? If the dataset relates to people (e.g., their attributes) or was generated by people, were they informed about the data collection? (e.g., datasets that collect writing, photos, interactions, transactions, etc.)\n- As time passes and conditions change, is the training data still representative of the operational environment?\n- Why was the dataset created? (e.g., were there specific tasks in mind, or a specific gap that needed to be filled?)\n- How does the entity ensure that the data collected are adequate, relevant, and not excessive in relation to the intended purpose?\n\n## AI Transparency Resources\n\n- Datasheets for Datasets.\n- WEF Model AI Governance Framework Assessment 2020.\n- WEF Companion to the Model AI Governance Framework- 2020.\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- ATARC Model Transparency Assessment (WD) -2020.\n- Transparency in Artificial Intelligence - S. Larsson and F. Heintz -2020.\n\n## References\n\nChallenges with dataset selection Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries. Front. Big Data 2, 13 (11 July 2019). Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. Catherine D'Ignazio and Lauren F. Klein. 2020. Data Feminism. The MIT Press, Cambridge, MA. Miceli, M., &amp; Posada, J. (2022). The Data-Production Dispositif. ArXiv, abs/2205.11963. Barbara Plank. 2016. What to do about non-standard (or non-canonical) language in NLP.\n\narXiv:1608.07836.\n\n## Dataset and test, evaluation, validation and verification (TEVV) processes in AI system development\n\nNational Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et al. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.\n\nInioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, et al. 2021. AI and the Everything in the Whole Wide World Benchmark. arXiv:2111.15366.\n\n## Statistical balance\n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453.\n\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345.\n\nSolon Barocas, Anhong Guo, Ece Kamar, et al. 2021. Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs. Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, USA, 368 -378.\n\n## Measurement and evaluation\n\nAbigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21). Association for Computing Machinery, New York, NY, USA, 375 -385.\n\nBen Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in Machine Learning Practice. arXiv:2205.05256.\n\nLaura Freeman, \"Test and evaluation for artificial intelligence.\" Insight 23.1 (2020): 27-30.\n\n## Existing frameworks\n\nNational Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity.\n\nKaitlin R. Boeckl and Naomi B. Lefkovitz. \"NIST Privacy Framework: A Tool for Improving Privacy Through Enterprise Risk Management, Version 1.0.\" National Institute of Standards and Technology (NIST), January 16, 2020.\n\n## MAP 3.1\n\nPotential benefits of intended AI system functionality and performance are examined and documented.\n\n## About\n\nAI systems have enormous potential to improve quality of life, enhance economic prosperity and security costs. Organizations are encouraged to define and document system purpose and utility, and its potential positive impacts and benefits beyond current known performance benchmarks. It is encouraged that risk management and assessment of benefits and impacts include processes for regular and meaningful communication with potentially affected groups and communities. These stakeholders can provide valuable input related to systems' benefits and possible limitations. Organizations may differ in the types and number of stakeholders with which they engage. Other approaches such as human-centered design (HCD) and value-sensitive design (VSD) can help AI teams to engage broadly with individuals and communities. This type of engagement can enable AI teams to learn about how a given technology may cause positive\n\nor negative impacts, that were not originally considered or intended.\n\n## Suggested Actions\n\n- Utilize participatory approaches and engage with system end users to understand and document  AI systems' potential benefits,  efficacy and interpretability of AI task output.\n- Maintain awareness and documentation of the individuals, groups, or communities who make up the system's internal and external stakeholders.\n- Verify that appropriate skills and practices are available in-house for carrying out participatory activities such as eliciting, capturing, and synthesizing user, operator and external feedback, and translating it for AI design and development functions.\n- Establish mechanisms for regular communication and feedback between relevant AI actors and internal or external stakeholders related to system design or deployment decisions.\n- Consider performance to human baseline metrics or other standard benchmarks.\n- Incorporate feedback from end users, and potentially impacted individuals and communities about perceived system benefits .\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Have the benefits of the AI system been communicated to end users?\n- Have the appropriate training material and disclaimers about how to adequately use the AI system been provided to end users?\n- Has your organization implemented a risk management system to address risks involved in deploying the identified AI system (e.g. personnel risk or changes to commercial objectives)?\n\n## AI Transparency Resources\n\n- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020.\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n\n- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI -2019. [LINK](https://altai.insight-centre.org/),\n\nReferences Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. Samir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 39 -48. Vincent T. Covello. 2021. Stakeholder Engagement and Empowerment. In Communicating in Risk, Crisis, and High Stress Situations (Vincent T. Covello, ed.), 87-109. Yilin Huang, Giacomo Poderi, Sanja Å Ä‡epanoviÄ‡, et al. 2019. Embedding Internet -of-Things in Large-Scale Socio-technical Systems: A Community-Oriented Design in Future Smart Grids. In The Internet of Things for Smart Urban Ecosystems (2019), 125-150. Springer, Cham. Eloise Taysom and Nathan Crilly. 2017. Resilience in Sociotechnical Systems: The Perspectives of Multiple Stakeholders. She Ji: The Journal of Design, Economics, and\n\nInnovation, 3, 3 (2017), 165-182, ISSN 2405-8726.\n\n## MAP 3.2\n\nPotential costs, including non-monetary costs, which result from expected or realized AI errors or system functionality and trustworthiness - as connected to organizational risk tolerance - are examined and documented.\n\n## About\n\nAnticipating negative impacts of AI systems is a difficult task. Negative impacts can be due to many factors, such as system non-functionality or use outside of its operational limits, and may range from minor annoyance to serious injury, financial losses, or regulatory enforcement actions. AI actors can work with a broad set of stakeholders to improve their capacity for understanding  systems' potential impacts and subsequently -systems' risks.\n\n## Suggested Actions\n\n- Perform context analysis to map potential negative impacts arising from not integrating trustworthiness characteristics. When negative impacts are not direct or obvious, AI actors can engage with stakeholders external to the team that developed or deployed the AI system, and potentially impacted communities, to examine and document:\n- Who could be harmed?\n- What could be harmed?\n- When could harm arise?\n- How could harm arise?\n\n- Identify and implement procedures for regularly evaluating the qualitative and quantitative costs of internal and external AI system failures. Develop actions to prevent, detect, and/or correct potential risks and related impacts. Regularly evaluate failure costs to inform go/no-go deployment decisions throughout the AI system lifecycle.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- To what extent does the system/entity consistently measure progress towards stated goals and objectives?\n- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n- Have you documented and explained that machine errors may differ from human errors?\n\n## AI Transparency Resources\n\n- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020.\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI -2019. [LINK](https://altai.insight-centre.org/),\n\n## References\n\nAbagayle Lee Blank. 2019. Computer vision machine learning and future-oriented ethics.\n\nHonors Project. Seattle Pacific University (SPU), Seattle, WA.\n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416.\n\n## Jeff Patton. 2014. User Story Mapping. O'Reilly, Sebastopol, CA.\n\nMargarita Boenig-Liptsin, Anissa Tanweer &amp; Ari Edmundson (2022) Data Science Ethos Lifecycle: Interplay of ethical thinking and data science practice, Journal of Statistics and Data Science Education, DOI: 10.1080/26939169.2022.2089411 J. Cohen, D. S. Katz, M. Barker, N. Chue Hong, R. Haines and C. Jay, \"The Four Pillars of\n\nResearch Software Engineering,\" in IEEE Software, vol. 38, no. 1, pp. 97-105, Jan.-Feb. 2021, doi: 10.1109/MS.2020.2973362.\n\nNational Academies of Sciences, Engineering, and Medicine 2022. Fostering Responsible Computing Research: Foundations and Practices. Washington, DC: The National Academies Press.\n\n## MAP 3.3\n\nTargeted application scope is specified and documented based on the system's capability, established context, and AI system categorization.\n\n## About\n\nSystems that function in a narrow scope tend to enable better mapping, measurement, and management of risks in the learning or decision-making tasks and the system context. A narrow application scope also helps ease TEVV functions and related resources within an organization. For example, large language models or open-ended chatbot systems that interact with the public on the internet have a large number of risks that may be difficult to map, measure, and manage due to the variability from both the decision-making task and the operational context. Instead, a task-specific chatbot utilizing templated responses that follow a defined 'user journey' is a scope that can be more easily mapped, measured and managed.\n\n## Suggested Actions\n\n- Consider narrowing contexts for system deployment, including factors related to:\n- How outcomes may directly or indirectly affect users, groups, communities and the environment.\n- Length of time the system is deployed in between re-trainings.\n- Geographical regions in which the system operates.\n- Dynamics related to community standards or likelihood of system misuse or abuses (either purposeful or unanticipated).\n- How AI system features and capabilities can be utilized within other applications, or in place of other existing processes.\n- Engage AI actors from legal and procurement functions when specifying target application scope.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent has the entity clearly defined technical specifications and requirements for the AI system?\n- How do the technical specifications and requirements align with the AI system's goals and objectives?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI -2019. [LINK](https://altai.insight-centre.org/),\n\n## References\n\nMark J. Van der Laan and Sherri Rose (2018). Targeted Learning in Data Science. Cham: Springer International Publishing, 2018.\n\nAlice Zheng. 2015. Evaluating Machine Learning Models (2015). O'Reilly.\n\nBrenda Leong and Patrick Hall (2021). 5 things lawyers should know about artificial intelligence. ABA Journal. UK Centre for Data Ethics and Innovation, 'The roadmap to an effective AI assurance\n\n## ecosystem'.\n\n## MAP 3.4\n\nProcesses for operator and practitioner proficiency with AI system performance and trustworthiness -and relevant technical standards and certifications -are defined, assessed and documented.\n\n## About\n\nHuman-AI configurations can span from fully autonomous to fully manual. AI systems can autonomously make decisions, defer decision-making to a human expert, or be used by a human decision-maker as an additional opinion. In some scenarios, professionals with expertise in a specific domain work in conjunction with an AI system towards a specific end goal -for example, a decision about another individual(s). Depending on the purpose of the system, the expert may interact with the AI system but is rarely part of the design or development of the system itself. These experts are not necessarily familiar with machine learning, data science, computer science, or other fields traditionally associated with AI design or development and - depending on the application - will likely not require such familiarity. For example, for AI systems that are deployed in health care delivery the experts are the physicians and bring their expertise about medicine -not data science, data modeling and engineering, or other computational factors. The challenge in these settings is not educating the end user about AI system capabilities, but rather leveraging, and not replacing, practitioner domain expertise. Questions remain about how to configure humans and automation for managing AI risks. Risk management is enhanced when organizations that design, develop or deploy AI systems for use by professional operators and practitioners:\n\n- are aware of these knowledge limitations and strive to identify risks in human-AI interactions and configurations across all contexts, and the potential resulting impacts,\n- define and differentiate the various human roles and responsibilities when using or interacting with AI systems, and\n- determine proficiency standards for AI system operation in proposed context of use, as enumerated in MAP-1 and established in GOVERN-3.2.\n\n## Suggested Actions\n\n- Identify and declare AI system features and capabilities that may affect downstream AI actors' decision -making in deployment and operational settings for example how system features and capabilities may activate known risks in various human-AI configurations, such as selective adherence.\n- Identify skills and proficiency requirements for operators, practitioners and other domain experts that interact with AI systems,Develop AI system operational\n\n- documentation for AI actors in deployed and operational environments, including information about known risks, mitigation criteria, and trustworthy characteristics enumerated in Map-1.\n- Define and develop training materials for proposed end users, practitioners and operators about AI system use and known limitations.\n- Define and develop certification procedures for operating AI systems within defined contexts of use, and information about what exceeds operational boundaries.\n- Include operators, practitioners and end users in AI system prototyping and testing activities to help inform operational boundaries and acceptable performance. Conduct testing activities under scenarios similar to deployment conditions.\n- Verify model output provided to AI system operators, practitioners and end users is interactive, and specified to context and user requirements defined in MAP-1.\n- Verify AI system output is interpretable and unambiguous for downstream decision making tasks.\n- Design AI system explanation complexity to match the level of problem and context complexity.\n- Verify that design principles are in place for safe operation by AI actors in decisionmaking environments.\n- Develop approaches to track human-AI configurations, operator, and practitioner outcomes for integration into continual improvement.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- What policies has the entity developed to ensure the use of the AI system is consistent with its stated values and principles?\n- How will the accountable human(s) address changes in accuracy and precision due to either an adversary's attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI?\n- How does the entity assess whether personnel have the necessary skills, training, resources, and domain knowledge to fulfill their assigned responsibilities?\n- Are the relevant staff dealing with AI systems properly trained to interpret AI model output and decisions as well as to detect and manage bias in data?\n- What metrics has the entity developed to measure performance of various components?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- WEF Companion to the Model AI Governance Framework- 2020.\n\n## References\n\nNational Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming:\n\nState-of-the-Art and Research Needs. Washington, DC: The National Academies Press.\n\nHuman Readiness Level Scale in the System Development Process, American National Standards Institute and Human Factors and Ergonomics Society, ANSI/HFES 400-2021. Human-Machine Teaming Systems Engineering Guide. P McDermott, C Dominguez, N Kasdaglis, M Ryan, I Trahan, A Nelson. MITRE Corporation, 2018. Saar Alon-Barkat, Madalina Busuioc, Human -AI Interactions in Public Sector Decision Making: 'Automation Bias' and 'Selective Adherence' to Algorithmic Advice, Journal of Public Administration Research and Theory, 2022;, muac007. Breana M. Carter-Browne, Susannah B. F. Paletz, Susan G. Campbell , Melissa J. Carraway, Sarah H. Vahlkamp, Jana Schwartz , Polly O'Rourke, 'There is No 'AI' in Teams: A Multidisciplinary Framework for AIs to Work in Human Teams; Applied Research Laboratory for Intelligence and Security (ARLIS) Report, June 2021. R Crootof, ME Kaminski, and WN Price II.  Humans in the Loop (March 25, 2022). Vanderbilt Law Review, Forthcoming 2023, U of Colorado Law Legal Studies Research Paper No. 22-10, U of Michigan Public Law Research Paper No. 22-011. S Mo Jones-Jang, Yong Jin Park, How do people react to AI failure? Automation bias, algorithmic aversion, and perceived controllability, Journal of Computer-Mediated Communication, Volume 28, Issue 1, January 2023, zmac029. A Knack, R Carter and A Babuta, \"Human-Machine Teaming in Intelligence Analysis: Requirements for developing trust in machine learning systems,\" CETaS Research Reports (December 2022). SD Ramchurn, S Stein , NR Jennings. Trustworthy human-AI partnerships. iScience. 2021;24(8):102891. Published 2021 Jul 24. doi:10.1016/j.isci.2021.102891. M. Veale, M. Van Kleek, and R. Binns, 'Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector DecisionMaking,' in Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems CHI '18. Montreal QC, Canada: ACM Press, 2018, pp. 1 -14.\n\n## MAP 3.5\n\nProcesses for human oversight are defined, assessed, and documented in accordance with organizational policies from GOVERN function.\n\n## About\n\nAs AI systems have evolved in accuracy and precision, computational systems have moved from being used purely for decision support -or for explicit use by and under the control of a human operator -to automated decision making with limited input from humans. Computational decision support systems augment another, typically human, system in making decisions.These types of configurations increase the likelihood of outputs being produced with little human involvement.\n\nDefining and differentiating various human roles and responsibilities for AI systems' governance,  and differentiating AI system overseers and those using or interacting with AI systems can enhance AI risk management activities. In critical systems, high-stakes settings, and systems deemed high-risk it is of vital importance to evaluate risks and effectiveness of oversight procedures before an AI system is deployed. Ultimately, AI system oversight is a shared responsibility, and attempts to properly authorize or govern oversight practices will not be effective without organizational buy-in and accountability mechanisms, for example those suggested in the GOVERN function.\n\n## Suggested Actions\n\n- Identify and document AI systems' features and capabilities that require human oversight, in relation to operational and societal contexts, trustworthy characteristics, and risks identified in MAP-1.\n- Establish practices for AI systems' oversight in accordance with policies developed in GOVERN-1.\n- Define and develop training materials for relevant AI Actors about AI system performance, context of use, known limitations and negative impacts, and suggested warning labels.\n- Include relevant AI Actors in AI system prototyping and testing activities. Conduct testing activities under scenarios similar to deployment conditions.\n- Evaluate AI system oversight practices for validity and reliability. When oversight practices undergo extensive updates or adaptations, retest, evaluate results, and course correct as necessary.\n- Verify that model documents contain interpretable descriptions of system mechanisms, enabling oversight personnel to make informed, risk-based decisions about system risks.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?\n- How does the entity assess whether personnel have the necessary skills, training, resources, and domain knowledge to fulfill their assigned responsibilities?\n- Are the relevant staff dealing with AI systems properly trained to interpret AI model output and decisions as well as to detect and manage bias in data?\n- To what extent has the entity documented the AI system's development, testing methodology, metrics, and performance outcomes?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n\nReferences Ben Green, 'The Flaws of Policies Requiring Human Oversight of Government Algorithms,' SSRN Journal, 2021. Luciano Cavalcante Siebert, Maria Luce Lupetti,  Evgeni Aizenberg, Niek Beckers, Arkady Zgonnikov, Herman Veluwenkamp, David Abbink, Elisa Giaccardi, Geert-Jan Houben, Catholijn Jonker, Jeroen van den Hoven, Deborah Forster, &amp; Reginald Lagendijk (2021). Meaningful human control: actionable properties for AI system development. AI and Ethics. Mary Cummings, (2014). Automation and Accountability in Decision Support System Interface Design. The Journal of Technology Studies. 32. 10.21061/jots.v32i1.a.4. Madeleine Elish, M. (2016). Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction (WeRobot 2016). SSRN Electronic Journal. 10.2139/ssrn.2757236. R Crootof, ME Kaminski, and WN Price II.  Humans in the Loop (March 25, 2022). Vanderbilt Law Review, Forthcoming 2023, U of Colorado Law Legal Studies Research Paper No. 22-10, U of Michigan Public Law Research Paper No. 22-011. [LINK](https://ssrn.com/abstract=4066781), Bogdana Rakova, Jingying Yang, Henriette Cramer, &amp; Rumman Chowdhury (2020). Where\n\nResponsible AI meets Reality. Proceedings of the ACM on Human-Computer Interaction, 5, 1 - 23.\n\n## MAP 4.1\n\nApproaches for mapping AI technology and legal risks of its components -including the use of third-party data or software -are in place, followed, and documented, as are risks of infringement of a thirdparty's intellectual property or other rights.\n\n## About\n\nTechnologies and personnel from third-parties are another potential sources of risk to consider during AI risk management activities. Such risks may be difficult to map since risk priorities or tolerances may not be the same as the deployer organization. For example, the use of pre-trained models, which tend to rely on large uncurated dataset or often have undisclosed origins, has raised concerns about privacy, bias, and unanticipated effects along with possible introduction of increased levels of statistical uncertainty, difficulty with reproducibility, and issues with scientific validity.\n\n## Suggested Actions\n\n- Review audit reports, testing results, product roadmaps, warranties, terms of service, end user license agreements, contracts, and other documentation related to third-party entities to assist in value assessment and risk management activities.\n- Review third-party software release schedules and software change management plans (hotfixes, patches, updates, forward- and backward- compatibility guarantees) for irregularities that may contribute to AI system risks.\n\n- Inventory third-party material (hardware, open-source software, foundation models, open source data, proprietary software, proprietary data, etc.) required for system implementation and maintenance.\n- Review redundancies related to third-party technology and personnel to assess potential risks due to lack of adequate support.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Did you establish a process for third parties (e.g. suppliers, end users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?\n- If your organization obtained datasets from a third party, did your organization assess and manage the risks of using such datasets?\n- How will the results be independently verified?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020.\n- WEF Model AI Governance Framework Assessment 2020.\n\n## References\n\n## Language  models\n\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ . In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21). Association for Computing Machinery, New York, NY, USA, 610 -623.\n\nJulia Kreutzer, Isaac Caswell, Lisa Wang, et al. 2022. Quality at a Glance: An Audit of WebCrawled Multilingual Datasets. Transactions of the Association for Computational Linguistics 10 (2022), 50 -72.\n\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, et al. 2022. Taxonomy of Risks posed by Language Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). Association for Computing Machinery, New York, NY, USA, 214 -229.\n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk\n\nManagement, Version 1.0, August 2021.\n\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. 2021. On the Opportunities and Risks of Foundation Models. arXiv:2108.07258.\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\n\nOriol Vinyals, Percy Liang, Jeff Dean, William Fedus. 'Emergent Abilities of Large Language Models.' ArXiv abs/2206.07682 (2022).\n\n## MAP 4.2\n\nInternal risk controls for components of the AI system including third-party AI technologies are identified and documented.\n\n## About\n\nIn the course of their work, AI actors often utilize open-source, or otherwise freely available, third-party technologies -some of which may have privacy, bias, and security risks. Organizations may consider internal risk controls for these technology sources and build up practices for evaluating third-party material prior to deployment.\n\n## Suggested Actions\n\n- Track third-parties preventing or hampering risk-mapping as indications of increased risk.\n- Supply resources such as model documentation templates and software safelists to assist in third-party technology inventory and approval activities.\n- Review third-party material (including data and models) for risks related to bias, data privacy, and security vulnerabilities.\n- Apply traditional technology risk controls -such as procurement, security, and data privacy controls -to all acquired third-party technologies.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- Can the AI system be audited by independent third parties?\n- To what extent do these policies foster public trust and confidence in the use of the AI system?\n- Are mechanisms established to facilitate the AI system's auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system's processes, outcomes, positive and negative impact)?\n\n## AI Transparency Resources\n\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020.\n- WEF Model AI Governance Framework Assessment 2020.\n- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI 2019. [LINK](https://altai.insight-centre.org/),\n\nReferences\n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. Retrieved on July 7, 2022. Proposed Interagency Guidance on Third-Party Relationships: Risk Management, 2021.\n\nKang, D., Raghavan, D., Bailis, P.D., &amp; Zaharia, M.A. (2020). Model Assertions for Monitoring and Improving ML Models. ArXiv, abs/2003.01668.\n\n## MAP 5.1\n\nLikelihood and magnitude of each identified impact (both potentially beneficial and harmful) based on expected use, past uses of AI systems in similar contexts, public incident reports, feedback from those external to the team that developed or deployed the AI system, or other data are identified and documented.\n\n## About\n\nAI actors can evaluate, document and triage the likelihood of AI system impacts identified in Map 5.1 Likelihood estimates may then be assessed and judged for go/no-go decisions about deploying an AI system. If an organization decides to proceed with deploying the system, the likelihood and magnitude estimates can be used to assign TEVV resources appropriate for the risk level.\n\n## Suggested Actions\n\n- Establish assessment scales for measuring AI systems' impact. Scales may be qualitative, such as red-amber-green (RAG), or may entail simulations or econometric approaches. Document and apply scales uniformly across the organization's AI portfolio.\n- Apply TEVV regularly at key stages in the AI lifecycle, connected to system impacts and frequency of system updates.\n- Identify and document  likelihood and magnitude of system benefits and negative impacts in relation to trustworthiness characteristics.\n- Establish processes for red teaming to identify and connect system limitations to AI lifecycle stage(s) and potential downstream impacts\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Which population(s) does the AI system impact?\n- What assessments has the entity conducted on trustworthiness characteristics for example data security and privacy impacts associated with the AI system?\n- Can the AI system be tested by independent third parties?\n\n## AI Transparency Resources\n\n- Datasheets for Datasets.\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019.\n- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020.\n- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI 2019. [LINK](https://altai.insight-centre.org/),\n\nReferences Emilio GÃ³mez-GonzÃ¡lez and Emilia GÃ³mez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). Artificial Intelligence Incident Database. 2022. Anthony M. Barrett, Dan Hendrycks, Jessica Newman and Brandie Nonnecke. 'Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks\". ArXiv abs/2206.08966 (2022) Ganguli, D., et al. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. arXiv. https://arxiv.org/abs/2209.07858 Upol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, and Hal DaumÃ©. 2024. Seamful XAI: Operationalizing Seamful Design in Explainable AI. Proc. ACM Hum.-Comput. Interact. 8,\n\nCSCW1, Article 119. https://doi.org/10.1145/3637396\n\n## MAP 5.2\n\nPractices and personnel for supporting regular engagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented.\n\n## About\n\nAI systems are socio-technical in nature and can have positive, neutral, or negative implications that extend beyond their stated purpose. Negative impacts can be wideranging and affect individuals, groups, communities, organizations, and society, as well as the environment and national security. Organizations can create a baseline for system monitoring to increase opportunities for detecting emergent risks. After an AI system is deployed, engaging different stakeholder groups -who may be aware of, or experience, benefits or negative impacts that are unknown to AI actors involved in the design, development and deployment activities -allows organizations to understand and monitor system benefits and potential negative impacts more readily.\n\n## Suggested Actions\n\n- Establish and document stakeholder engagement processes at the earliest stages of system formulation to identify potential impacts from the AI system on individuals, groups, communities, organizations, and society.\n- Employ methods such as value sensitive design (VSD) to identify misalignments between organizational and societal values, and system implementation and impact.\n- Identify approaches to engage, capture, and incorporate input from system end users and other key stakeholders to assist with continuous monitoring for potential impacts and emergent risks.\n\n- Incorporate quantitative, qualitative, and mixed methods in the assessment and documentation of potential impacts to individuals, groups, communities, organizations, and society.\n- Identify a team (internal or external) that is independent of AI design and development functions to assess AI system benefits, positive and negative impacts and their likelihood and magnitude.\n- Evaluate and document stakeholder feedback to assess potential impacts for actionable insights regarding trustworthiness characteristics and changes in design approaches and principles.\n- Develop TEVV procedures that incorporate socio-technical elements and methods and plan to normalize across organizational culture. Regularly review and refine TEVV processes.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- If the AI system relates to people, does it unfairly advantage or disadvantage a particular social group? In what ways? How was this managed?\n- If the AI system relates to other ethically protected groups, have appropriate obligations been met? (e.g., medical data might include information collected from animals)\n- If the AI system relates to people, could this dataset expose people to harm or legal action? (e.g., financial social or otherwise) What was done to mitigate or reduce the potential for harm?\n\n## AI Transparency Resources\n\n- Datasheets for Datasets.\n- GAO-21-519SP: AI Accountability Framework for Federal Agencies &amp; Other Entities.\n- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019.\n- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020.\n- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI 2019. [LINK](https://altai.insight-centre.org/),\n\nReferences Susanne Vernim, Harald Bauer, Erwin Rauch, et al. 2022. A value sensitive design approach for designing AI-based worker assistance systems in manufacturing. Procedia Comput. Sci. 200, C (2022), 505 -516. Harini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. Konstantinia Charitoudi and Andrew Blyth. A Socio-Technical Approach to Cyber Risk\n\nManagement and Impact Assessment. Journal of Information Security 4, 1 (2013), 33-41.\n\nRaji, I.D., Smart, A., White, R.N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., &amp; Barnes, P. (2020). Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, &amp; Jacob Metcalf. 2021. Assemlbing Accountability: Algorithmic Impact Assessment for the Public Interest. Data &amp; Society. Accessed 7/14/2022 at Shari Trewin (2018). AI Fairness for People with Disabilities: Point of View. ArXiv, abs/1811.10670. Ada Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. Accessed July 14, 2022. Microsoft Responsible AI Impact Assessment Template. 2022. Accessed July 14, 2022. Microsoft Responsible AI Impact Assessment Guide. 2022. Accessed July 14, 2022. Microsoft Responsible AI Standard, v2. Microsoft Research AI Fairness Checklist.\n\nPEAT AI &amp; Disability Inclusion Toolkit -Risks of Bias and Discrimination in AI Hiring Tools.\n\n## MEASURE\n\n<!-- image -->\n\n## Measure\n\nAppropriate methods and metrics are identified and applied.\n\n## MEASURE 1.1\n\nApproaches and metrics for measurement of AI risks enumerated during the Map function are selected for implementation starting with the most significant AI risks. The risks or trustworthiness characteristics that will not -or cannot -be measured are properly documented.\n\n## About\n\nThe development and utility of trustworthy AI systems depends on reliable measurements and evaluations of underlying technologies and their use. Compared with traditional software systems, AI technologies bring new failure modes, inherent dependence on training data and methods which directly tie to data quality and representativeness. Additionally, AI systems are inherently socio-technical in nature, meaning they are influenced by societal dynamics and human behavior. AI risks -and benefits -can emerge from the interplay of technical aspects combined with societal factors related to how a system is used, its interactions with other AI systems, who operates it, and the social context in which it is deployed. In other words, What should be measured depends on the purpose, audience, and needs of the evaluations. These two factors influence selection of approaches and metrics for measurement of AI risks enumerated during the Map function. The AI landscape is evolving and so are the methods and metrics for AI measurement. The evolution of metrics is key to maintaining efficacy of the measures.\n\n## Suggested Actions\n\n- Establish approaches for detecting, tracking and measuring known risks, errors, incidents or negative impacts.\n- Identify testing procedures and metrics to demonstrate whether or not the system is fit for purpose and functioning as claimed.\n- Identify testing procedures and metrics to demonstrate AI system trustworthiness\n- Define acceptable limits for system performance (e.g. distribution of errors), and include course correction suggestions if/when the system performs beyond acceptable limits.\n- Define metrics for, and regularly assess, AI actor competency for effective system operation,\n- Identify transparency metrics to assess whether stakeholders have access to necessary information about system design, development, deployment, use, and evaluation.\n- Utilize accountability metrics to determine whether AI designers, developers, and deployers maintain clear and transparent lines of responsibility and are open to inquiries.\n- Document metric selection criteria and include considered but unused metrics.\n\n- Monitor AI system external inputs including training data, models developed for other contexts, system components reused from other contexts, and third-party tools and resources.\n- Report metrics to inform assessments of system generalizability and reliability.\n- Assess and  document pre- vs post-deployment system performance. Include existing and emergent  risks.\n- Document risks or trustworthiness characteristics identified in the Map function that will not be measured, including justification for non- measurement.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- How will the appropriate performance metrics, such as accuracy, of the AI be monitored after the AI is deployed?\n- What corrective actions has the entity taken to enhance the quality, accuracy, reliability, and representativeness of the data?\n- Are there recommended data splits or evaluation measures? (e.g., training, development, testing; accuracy/AUC)\n- Did your organization address usability problems and test whether user interfaces served their intended purposes?\n- What testing, if any, has the entity conducted on the AI system to identify errors and limitations (i.e. manual vs automated, adversarial and stress testing)?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- Datasheets for Datasets.\n\nReferences Sara R. Jordan. 'Designing Artificial Intelligence Review Boards: Creating Risk Metrics for Review of AI.' 2019 IEEE International Symposium on Technology and Society (ISTAS), 2019. IEEE. 'IEEE -1012-2016: IEEE Standard for System, Software, and Hardware Verification and Validation.' IEEE Standards Association. ACM Technology Policy Council. 'Statement on Principles for Responsible Algorithmic Systems.' Association for Computing Machinery (ACM), October 26, 2022. Perez, E., et al. (2022). Discovering Language Model Behaviors with Model-Written Evaluations. arXiv. https://arxiv.org/abs/2212.09251 Ganguli, D., et al. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling\n\nBehaviors, and Lessons Learned. arXiv. https://arxiv.org/abs/2209.07858\n\nDavid Piorkowski, Michael Hind, and John Richards. \"Quantitative AI Risk Assessments: Opportunities and Challenges.\" arXiv preprint, submitted January 11, 2023. Daniel Schiff, Aladdin Ayesh, Laura Musikanski, and John C. Havens. 'IEEE 7010: A New Standard for Assessing the WellBeing Implications of Artificial Intelligence.' 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2020.\n\n## MEASURE 1.2\n\nAppropriateness of AI metrics and effectiveness of existing controls is regularly assessed and updated including reports of errors and impacts on affected communities.\n\n## About\n\nDifferent AI tasks, such as neural networks or natural language processing, benefit from different evaluation techniques. Use-case and particular settings in which the AI system is used also affects appropriateness of the evaluation techniques.  Changes in the operational settings, data drift, model drift are among factors that suggest regularly assessing and updating appropriateness of AI metrics and their effectiveness can enhance reliability of AI system measurements.\n\n## Suggested Actions\n\n- Assess external validity of all measurements (e.g., the degree to which measurements taken in one context can generalize to other contexts).\n- Assess effectiveness of existing metrics and controls on a regular basis throughout the AI system lifecycle.\n- Document reports of errors, incidents and negative impacts and assess sufficiency and efficacy of existing metrics for repairs, and upgrades\n- Develop new metrics when existing metrics are insufficient or ineffective for implementing repairs and upgrades.\n- Develop and utilize metrics to monitor, characterize and track external inputs, including any third-party tools.\n- Determine frequency and scope for sharing metrics and related information with stakeholders and impacted communities.\n- Utilize stakeholder feedback processes established in the Map function to capture, act upon and share feedback from end users and potentially impacted communities.\n- Collect and report software quality metrics such as rates of bug occurrence and severity, time to response, and time to repair (See Manage 4.3).\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- What metrics has the entity developed to measure performance of the AI system?\n- To what extent do the metrics provide accurate and useful measure of performance?\n- What corrective actions has the entity taken to enhance the quality, accuracy, reliability, and representativeness of the data?\n\n- How will the accuracy or appropriate performance metrics be assessed?\n- What is the justification for the metrics selected?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\nReferences ACM Technology Policy Council. 'Statement on Principles for Responsible Algorithmic Systems.' Association for Computing Machinery (ACM), October 26, 2022. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer-Verlag, 2009. Harini Suresh and John Guttag. 'A Framework for Understanding Sources of Harm Throughout the Machine Learning Life Cycle.' Equity and Access in Algorithms, Mechanisms, and Optimization, October 2021. Christopher M. Bishop. Pattern Recognition and Machine Learning. New York: Springer, 2006. Evaluations of AI Systems: Choices, Considerations, and Tradeoffs.' Proceedings of the 2021\n\nSolon Barocas, Anhong Guo, Ece Kamar, Jacquelyn Krones, Meredith Ringel Morris, Jennifer Wortman Vaughan, W. Duncan Wadsworth, and Hanna Wallach. 'Designing Disaggregated AAAI/ACM Conference on AI, Ethics, and Society, July 2021, 368 -78.\n\n## MEASURE 1.3\n\nInternal experts who did not serve as front-line developers for the system and/or independent assessors are involved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance.\n\n## About\n\nThe current AI systems are brittle, the failure modes are not well described, and the systems are dependent on the context in which they were developed and do not transfer well outside of the training environment. A reliance on local evaluations will be necessary along with a continuous monitoring of these systems. Measurements that extend beyond classical measures (which average across test cases) or expand to focus on pockets of failures where there are potentially significant costs can improve the reliability of risk management activities. Feedback from affected communities about how AI systems are being used can make AI evaluation purposeful. Involving internal experts who did not serve as front-line developers for the system and/or independent assessors regular assessments of AI systems helps a fulsome characterization of AI systems' performance and trustworthiness .\n\n## Suggested Actions\n\n- Evaluate TEVV processes regarding incentives to identify risks and impacts.\n- Utilize separate testing teams established in the Govern function (2.1 and 4.1) to enable independent decisions and course-correction for AI systems. Track processes and measure and document change in performance.\n- Plan and evaluate AI system prototypes with end user populations early and continuously in the AI lifecycle. Document test outcomes and course correct.\n- Assess independence and stature of TEVV and oversight AI actors, to ensure they have the required levels of independence and resources to perform assurance, compliance, and feedback tasks effectively\n- Evaluate interdisciplinary and demographically diverse internal team established in Map 1.2\n- Evaluate effectiveness of external stakeholder feedback mechanisms, specifically related to processes for eliciting, evaluating and integrating input from diverse groups.\n- Evaluate effectiveness of external stakeholder feedback mechanisms for enhancing AI actor visibility and decision making regarding AI system risks and trustworthy characteristics.\n- Identify and utilize participatory approaches for assessing impacts that may arise from changes in system deployment (e.g.,  introducing new technology, decommissioning algorithms and models, adapting system, model or algorithm)\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?\n- How easily accessible and current is the information available to external stakeholders?\n- To what extent does the entity communicate its AI strategic goals and objectives to the community of stakeholders?\n- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n- To what extent is this information sufficient and appropriate to promote transparency? Do external stakeholders have access to information on the design, operation, and limitations of the AI system?\n- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\nReferences Board of Governors of the Federal Reserve System. 'SR 11 -7: Guidance on Model Risk Management.' April 4, 2011. 'Definition of independent verification and validation (IV&amp;V)', in IEEE 1012, IEEE Standard for System, Software, and Hardware Verification and Validation. Annex C, Mona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. 'Participation Is Not a Design Fix for Machine Learning.' Equity and Access in Algorithms, Mechanisms, and Optimization, October 2022. Rediet Abebe and Kira Goldner. 'Mechanism Design for Social Good.' AI Matters 4, no. 3 (October 2018): 27 -34. Upol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. 'The Algorithmic Imprint.' Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency\n\n(2022).\n\n## MEASURE 2.1\n\nTest sets, metrics, and details about the tools used during test, evaluation, validation, and verification (TEVV) are documented.\n\n## About\n\nDocumenting measurement approaches, test sets, metrics, processes and materials used, and associated details builds foundation upon which to build a valid, reliable measurement process.  Documentation enables repeatability and consistency, and can enhance AI risk management decisions.\n\n## Suggested Actions\n\n- Leverage existing industry best practices for transparency and documentation of all possible aspects of measurements. Examples include: data sheet for data sets, model cards\n- Regularly assess the effectiveness of tools used to document measurement approaches, test sets, metrics, processes and materials used\n- Update the tools as needed\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?\n- To what extent has the entity documented the AI system's development, testing methodology, metrics, and performance outcomes?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020.\n\nReferences Emily M. Bender and Batya Friedman. 'Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science.' Transactions of the Association for Computational Linguistics 6 (2018): 587 -604. Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 'Model Cards for Model Reporting.' FAT *19: Proceedings of the Conference on Fairness, Accountability, and Transparency, January 2019, 220 -29. IEEE Computer Society. 'Software Engineering Body of Knowledge Version 3: IEEE Computer Society.' IEEE Computer Society. IEEE. 'IEEE -1012-2016: IEEE Standard for System, Software, and Hardware Verification and Validation.' IEEE Standards Association. Board of Governors of the Federal Reserve System. 'SR 11 -7: Guidance on Model Risk Management.' April 4, 2011. Abigail Z. Jacobs and Hanna Wallach. 'Measurement and Fairness.' FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, 375 -85. Jeanna Matthews, Bruce Hedin, Marc Canellas. Trustworthy Evidence for Trustworthy Technology: An Overview of Evidence for Assessing the Trustworthiness of Autonomous and Intelligent Systems. IEEE-USA, September 29 2022. Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 'Hard Choices in Artificial Intelligence.' Artificial Intelligence 300 (November 2021).\n\n## MEASURE 2.2\n\nEvaluations involving human subjects meet applicable requirements (including human subject protection) and are representative of the relevant population.\n\n## About\n\nMeasurement and evaluation of AI systems often involves testing with human subjects or using data captured from human subjects. Protection of human subjects is required by law when carrying out federally funded research, and is a domain specific requirement for some disciplines. Standard human subjects protection procedures include protecting the welfare\n\nand interests of human subjects, designing  evaluations to minimize risks to subjects, and completion of mandatory training regarding legal requirements and expectations. Evaluations of AI system performance that utilize human subjects or human subject data should reflect the population within the context of use. AI system activities utilizing nonrepresentative data may lead to inaccurate assessments or negative and harmful outcomes. It is often difficult -and sometimes impossible, to collect data or perform evaluation tasks that reflect the full operational purview of an AI system. Methods for collecting, annotating, or using these data can also contribute to the challenge. To counteract these challenges, organizations can connect human subjects data collection, and dataset practices, to AI system contexts and purposes and do so in close collaboration with AI Actors from the relevant domains.\n\n## Suggested Actions\n\n- Follow human subjects research requirements as established by organizational and disciplinary requirements, including informed consent and compensation, during dataset collection activities.\n- Analyze differences between intended and actual population of users or data subjects, including likelihood for errors, incidents or negative impacts.\n- Utilize disaggregated evaluation methods (e.g. by race, age, gender, ethnicity, ability, region) to improve AI system performance when deployed in real world settings.\n- Establish thresholds and alert procedures for dataset representativeness within the context of use.\n- Construct datasets in close collaboration with experts with knowledge of the context of use.\n- Follow intellectual property and privacy rights related to datasets and their use, including for the subjects represented in the data.\n- Evaluate data representativeness through\n- investigating known failure modes,\n- assessing data quality and diverse sourcing,\n- applying public benchmarks,\n- traditional bias testing,\n- chaos engineering,\n- stakeholder feedback\n- Use informed consent for individuals providing data used in system testing and evaluation.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?\n\n- How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?\n- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?\n- To what extent has the entity identified and mitigated potential bias -statistical, contextual, and historical -in the data?\n- If it relates to people, were they told what the dataset would be used for and did they consent? What community norms exist for data collected from human communications? If consent was obtained, how? Were the people provided with any mechanism to revoke their consent in the future or for certain uses?\n- If human subjects were used in the development or testing of the AI system, what protections were put in place to promote their safety and wellbeing?.\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020.\n- Datasheets for Datasets.\n\nReferences United States Department of Health, Education, and Welfare's National Commission for the II. United States Department of Health and Human Services Office for Human Research\n\nProtection of Human Subjects of Biomedical and Behavioral Research. The Belmont Report: Ethical Principles and Guidelines for the Protection of Human Subjects of Research. Volume Protections. April 18, 1979. Office for Human Research Protections (OHRP). '45 CFR 46.' United States Department of Health and Human Services Office for Human Research Protections, March 10, 2021. Office for Human Research Protections (OHRP). 'Human Subject Regulations Decision Chart.' United States Department of Health and Human Services Office for Human Research Protections, June 30, 2020. Jacob Metcalf and Kate Crawford. 'Where Are Human Subjects in Big Data Research? The Emerging Ethics Divide.' Big Data and Society 3, no. 1 (2016). Boaz Shmueli, Jan Fell, Soumya Ray, and Lun-Wei Ku. \"Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing.\" arXiv preprint, submitted April 20, 2021. Divyansh Kaushik, Zachary C. Lipton, and Alex John London. \"Resolving the Human Subjects Status of Machine Learning's Crowdworkers.\" arXiv preprint, submitted June 8, 2022.\n\nOffice for Human Research Protections (OHRP). 'International Compilation of Human\n\nResearch Standards.' United States Department of Health and Human Services Office for Human Research Protections, February 7, 2022. National Institutes of Health. 'Definition of Human Subjects Research.' NIH Central Resource for Grants and Funding Information, January 13, 2020. Joy Buolamwini and Timnit Gebru. 'Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.' Proceedings of the 1st Conference on Fairness, Accountability and Transparency in PMLR 81 (2018): 77 -91. Eun Seo Jo and Timnit Gebru. 'Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning.' FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, January 2020, 306 -16. Marco Gerardi, Katarzyna Barud, Marie-Catherine Wagner, Nikolaus Forgo, Francesca Fallucchi, Noemi Scarpato, Fiorella Guadagni, and Fabio Massimo Zanzotto. \"Active Informed Consent to Boost the Application of Machine Learning in Medicine.\" arXiv preprint, submitted September 27, 2022. Shari Trewin. \"AI Fairness for People with Disabilities: Point of View.\" arXiv preprint, submitted November 26, 2018. Andrea Brennen, Ryan Ashley, Ricardo Calix, JJ Ben-Joseph, George Sieniawski, Mona Gogia, and BNH.AI. AI Assurance Audit of RoBERTa, an Open source, Pretrained Large Language Model. IQT Labs, December 2022.\n\n## MEASURE 2.3\n\nAI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented.\n\n## About\n\nThe current risk and impact environment suggests AI system performance estimates are insufficient and require a deeper understanding of deployment context of use. Computationally focused performance testing and evaluation schemes are restricted to test data sets and in silico techniques. These approaches do not directly evaluate risks and impacts in real world environments and can only predict what might create impact based on an approximation of expected AI use. To properly manage risks, more direct information is necessary to understand how and under what conditions deployed AI creates impacts, who is most likely to be impacted, and what that experience is like.\n\n## Suggested Actions\n\n- Conduct regular and sustained engagement with potentially impacted communities\n- Maintain a demographically diverse and multidisciplinary and collaborative internal team\n\n- Regularly test and evaluate systems in non-optimized conditions, and in collaboration with AI actors in user interaction and user experience (UI/UX) roles.\n- Evaluate feedback from stakeholder engagement activities, in collaboration with human factors and socio-technical experts.\n- Collaborate with socio-technical, human factors, and UI/UX experts to identify notable characteristics in context of use that can be translated into system testing scenarios.\n- Measure AI systems prior to deployment in conditions similar to expected scenarios.\n- Measure and document performance criteria such as validity (false positive rate, false negative rate, etc.) and efficiency (training times, prediction latency, etc.) related to ground truth within the deployment context of use.\n- Measure assurance criteria such as AI actor competency and experience.\n- Document differences between measurement setting and the deployment environment(s).\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- What experiments were initially run on this dataset? To what extent have experiments on the AI system been documented?\n- To what extent does the system/entity consistently measure progress towards stated goals and objectives?\n- How will the appropriate performance metrics, such as accuracy, of the AI be monitored after the AI is deployed? How much distributional shift or model drift from baseline performance is acceptable?\n- As time passes and conditions change, is the training data still representative of the operational environment?\n- What testing, if any, has the entity conducted on theAI system to identify errors and limitations (i.e.adversarial or stress testing)?\n\n## AI Transparency Resources\n\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020.\n- Datasheets for Datasets.\n\nReferences Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer-Verlag, 2009.\n\nJessica Zosa Forde, A. Feder Cooper, Kweku Kwegyir-Aggrey, Chris De Sa, and Michael Littman. \"Model Selection's Disparate Impact in Real-World Deep Learning Applications.\" arXiv preprint, submitted September 7, 2021.\n\nInioluwa Deborah Raji, I. Elizabeth Kumar, Aaron Horowitz, and Andrew Selbst. 'The Fallacy of AI Functionality.' FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, June 2022, 959 -72. Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. 'Data and Its (Dis)Contents: A Survey of Dataset Development and Use in Machine Learning Research.' Patterns 2, no. 11 (2021): 100336. Christopher M. Bishop. Pattern Recognition and Machine Learning. New York: Springer, 2006. Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. \"A Comprehensive Study on Deep Learning Bug Characteristics.\" arXiv preprint, submitted June 3, 2019. Swaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva, Chris Bryan, and Chitta Baral. \"DQI: Measuring Data Quality in NLP.\" arXiv preprint, submitted May 2, 2020. Doug Wielenga. \"Paper 073-2007: Identifying and Overcoming Common Data Mining Mistakes.\" SAS Global Forum 2007: Data Mining and Predictive Modeling, SAS Institute,\n\n2007.\n\n## Software Resources\n\n- Drifter library (performance assessment)\n- Manifold library (performance assessment)\n- MLextend library (performance assessment)\n- PiML library (explainable models, performance assessment)\n- SALib library (performance assessment)\n- What-If Tool (performance assessment)\n\n## MEASURE 2.4\n\nThe functionality and behavior of the AI system and its components -as identified in the MAP function -are monitored when in production.\n\n## About\n\nAI systems may encounter new issues and risks while in production as the environment evolves over time. This effect, often referred to as 'drift', means AI systems no longer meet the assumptions and limitations of the original design. Regular monitoring allows AI Actors to monitor the functionality and behavior of the AI system and its components -as identified in the MAP function - and enhance the speed and efficacy of necessary system interventions.\n\n## Suggested Actions\n\n- Monitor and document how metrics and performance indicators observed in production differ from the same metrics collected during pre-deployment testing. When differences are observed, consider error propagation and feedback loop risks.\n\n- Utilize hypothesis testing or human domain expertise to measure monitored distribution differences in new input or output data relative to test environments\n- Monitor for anomalies using approaches such as control limits, confidence intervals, integrity constraints and ML algorithms. When anomalies are observed, consider error propagation and feedback loop risks.\n- Verify alerts are in place for when distributions in new input data or generated predictions observed in production differ from pre-deployment test outcomes, or when anomalies are detected.\n- Assess the accuracy and quality of generated outputs against new collected groundtruth information as it becomes available.\n- Utilize human review to track processing of unexpected data and reliability of generated outputs; warn system users when outputs may be unreliable. Verify that human overseers responsible for these processes have clearly defined responsibilities and training for specified tasks.\n- Collect uses cases from the operational environment for system testing and monitoring activities in accordance with organizational policies and regulatory or disciplinary requirements (e.g. informed consent, institutional review board approval, human research protections),\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent is the output of each component appropriate for the operational context?\n- What justifications, if any, has the entity provided for the assumptions, boundaries, and limitations of the AI system?\n- How will the appropriate performance metrics, such as accuracy, of the AI be monitored after the AI is deployed?\n- As time passes and conditions change, is the training data still representative of the operational environment?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\nReferences\n\nLuca Piano, Fabio Garcea, Valentina Gatteschi, Fabrizio Lamberti, and Lia Morra. 'Detecting Drift in Deep Learning: A Methodology Primer.' IT Professional 24, no. 5 (2022): 5360. Larysa Visengeriyeva, et al. 'Awesome MLOps.' GitHub.\n\n## MEASURE 2.5\n\nThe AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the conditions under which the technology was developed are documented.\n\n## About\n\nAn AI system that is not validated or that fails validation may be inaccurate or unreliable or may generalize poorly to data and settings beyond its training, creating and increasing AI risks and reducing trustworthiness. AI Actors can improve system validity by creating processes for exploring and documenting system limitations. This includes broad consideration of purposes and uses for which the system was not designed. Validation risks include the use of proxies or other indicators that are often constructed by AI development teams to operationalize phenomena that are either not directly observable or measurable (e.g, fairness, hireability, honesty, propensity to commit a crime). Teams can mitigate these risks by demonstrating that the indicator is measuring the concept it claims to measure (also known as construct validity). Without this and other types of validation, various negative properties or impacts may go undetected, including the presence of confounding variables, potential spurious correlations, or error propagation and its potential impact on other interconnected systems.\n\n## Suggested Actions\n\n- Define the operating conditions and socio-technical context under which the AI system will be validated.\n- Define and document processes to establish the system's operational conditions and limits.\n- Establish or identify, and document approaches to measure forms of validity, including:\n- construct validity (the test  is measuring the concept it claims to measure)\n- internal validity (relationship being tested is not influenced by other factors or variables)\n- external validity (results are generalizable beyond the training condition)\n- the use of experimental design principles and statistical analyses and modeling.\n- Assess and document system variance. Standard approaches include confidence intervals, standard deviation, standard error, bootstrapping, or cross-validation.\n- Establish or identify, and document robustness measures.\n- Establish or identify, and document reliability measures.\n- Establish practices to specify and document the assumptions underlying measurement models to ensure proxies accurately reflect the concept being measured.\n- Utilize standard software testing approaches (e.g. unit, integration, functional and chaos testing, computer-generated test cases, etc.)\n- Utilize standard statistical methods to test bias, inferential associations, correlation, and covariance in adopted measurement models.\n\n- Utilize standard statistical methods to test variance and reliability of system outcomes.\n- Monitor operating conditions for system performance outside of defined limits.\n- Identify TEVV approaches for exploring AI system limitations, including testing scenarios that differ from the operational environment. Consult experts with knowledge of specific context of use.\n- Define post-alert actions. Possible actions may include:\n- alerting other relevant AI actors before action,\n- requesting subsequent human review of action,\n- alerting downstream users and stakeholder that the system is operating outside it's defined validity limits,\n- tracking and mitigating possible error propagation\n- action logging\n- Log input data and relevant system configuration information whenever there is an attempt to use the system beyond its well-defined range of system validity.\n- Modify the system over time to extend its range of system validity to new operating conditions.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- What testing, if any, has the entity conducted on theAI system to identify errors and limitations (i.e.adversarial or stress testing)?\n- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?\n- How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?\n- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?\n- What goals and objectives does the entity expect to achieve by designing, developing, and/or deploying the AI system?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n\n## References Abigail Z. Jacobs and Hanna Wallach. 'Measurement and Fairness.' FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, 375 -85. Debugging Machine Learning Models. Proceedings of ICLR 2019 Workshop, May 6, 2019,\n\nNew Orleans, Louisiana.\n\nPatrick Hall. 'Strategies for Model Debugging.' Towards Data Science, November 8, 2019. Suchi Saria and Adarsh Subbaswamy. \"Tutorial: Safe and Reliable Machine Learning.\" arXiv preprint, submitted April 15, 2019. Google Developers. 'Overview of Debugging ML Models.' Google Developers Machine Learning Foundational Courses, n.d. R. Mohanani, I. Salman, B. Turhan, P. RodrÃ­guez and P. Ralph, \"Cognitive Biases in Software Engineering: A Systematic Mapping Study,\" in IEEE Transactions on Software Engineering, vol. 46, no. 12, pp. 1318-1339, Dec. 2020,\n\n## Software Resources\n\n- Drifter library (performance assessment)\n- Manifold library (performance assessment)\n- MLextend library (performance assessment)\n- PiML library (explainable models, performance assessment)\n- SALib library (performance assessment)\n- What-If Tool (performance assessment)\n\n## MEASURE 2.6\n\nAI system is evaluated regularly for safety risks -as identified in the MAP function. The AI system to be deployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics implicate system reliability and robustness, real-time monitoring, and response times for AI system failures.\n\n## About\n\nMany AI systems are being introduced into settings such as transportation, manufacturing or security, where failures may give rise to various physical or environmental harms. AI systems that may endanger human life, health, property or the environment are tested thoroughly prior to  deployment, and are regularly evaluated to confirm the system is safe during normal operations, and in settings beyond its proposed use and knowledge limits. Measuring activities for safety often relate to exhaustive testing in development and deployment contexts, understanding the limits of a system's reliable, robust, and safe behavior, and real-time monitoring of various aspects of system performance. These activities are typically conducted along with other risk mapping, management, and governance tasks such as avoiding past failed designs, establishing and rehearsing incident response plans that enable quick responses to system problems, the instantiation of redundant functionality to cover failures, and transparent and accountable governance. System safety incidents or failures are frequently reported to be related to organizational dynamics and culture. Independent auditors may bring important independent perspectives for reviewing evidence of AI system safety.\n\n## Suggested Actions\n\n- Thoroughly measure system performance in development and deployment contexts, and under stress conditions.\n- Employ test data assessments and simulations before proceeding to production testing. Track multiple performance quality and error metrics.\n- Stress-test system performance under likely scenarios (e.g., concept drift, high load) and beyond known limitations, in consultation with domain experts.\n- Test the system under conditions similar to those related to past known incidents or near-misses and measure system performance and safety characteristics\n- Apply chaos engineering approaches to test systems in extreme conditions and gauge unexpected responses.\n- Document the range of conditions under which the system has been tested and demonstrated to fail safely.\n- Measure and monitor system performance in real-time  to enable rapid response when AI system incidents are detected.\n- Collect pertinent safety statistics (e.g., out-of-range performance, incident response times, system down time, injuries, etc.) in anticipation of potential information sharing with impacted communities or as required by AI system oversight personnel.\n- Align measurement to the goal of continuous improvement. Seek to increase the range of conditions under which the system is able to fail safely through system modifications in response to in-production testing and events.\n- Document, practice and measure incident response plans for AI system incidents, including measuring response and down times.\n- Compare documented safety testing and monitoring information with established risk tolerances on an on-going basis.\n- Consult MANAGE for detailed information related to managing safety risks.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- What testing, if any, has the entity conducted on the AI system to identify errors and limitations (i.e.adversarial or stress testing)?\n- To what extent has the entity documented the AI system's development, testing methodology, metrics, and performance outcomes?\n- Did you establish mechanisms that facilitate the AI system's auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system's processes, outcomes, positive and negative impact)?\n- Did you ensure that the AI system can be audited by independent third parties?\n- Did you establish a process for third parties (e.g. suppliers, end-users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\nReferences AI Incident Database. 2022. AIAAIC Repository. 2022. Netflix. Chaos Monkey. IBM. 'IBM's Principles of Chaos Engineering.' IBM, n.d. Suchi Saria and Adarsh Subbaswamy. \"Tutorial: Safe and Reliable Machine Learning.\" arXiv preprint, submitted April 15, 2019. Daniel Kang, Deepti Raghavan, Peter Bailis, and Matei Zaharia. \"Model assertions for monitoring and improving ML models.\" Proceedings of Machine Learning and Systems 2 (2020): 481-496.\n\n## Larysa Visengeriyeva, et al. 'Awesome MLOps.' GitHub.\n\nMcGregor, S., Paeth, K., &amp; Lam, K.T. (2022). Indexing AI Risks with Incidents, Issues, and Variants. ArXiv, abs/2211.10384.\n\n## MEASURE 2.7\n\nAI system security and resilience -as identified in the MAP function -are evaluated and documented.\n\n## About\n\nAI systems, as well as the ecosystems in which they are deployed, may be said to be resilient if they can withstand unexpected adverse events or unexpected changes in their environment or use -or if they can maintain their functions and structure in the face of internal and external change and degrade safely and gracefully when this is necessary. Common security concerns relate to adversarial examples, data poisoning, and the exfiltration of models, training data, or other intellectual property through AI system endpoints. AI systems that can maintain confidentiality, integrity, and availability through protection mechanisms that prevent unauthorized access and use may be said to be secure. Security and resilience are related but distinct characteristics. While resilience is the ability to return to normal function after an unexpected adverse event, security includes resilience but also encompasses protocols to avoid, protect against, respond to, or recover\n\nfrom attacks. Resilience relates to robustness and encompasses unexpected or adversarial use (or abuse or misuse) of the model or data.\n\n## Suggested Actions\n\n- Establish and track AI system security tests and metrics (e.g.,  red-teaming activities, frequency and rate of anomalous events, system down-time, incident response times, time-to-bypass, etc.).\n- Use red-team exercises to actively test the system under adversarial or stress conditions, measure system response, assess failure modes or determine if system can return to normal function after an unexpected adverse event.\n- Document red-team exercise results as part of continuous improvement efforts, including the range of security test conditions and results.\n- Use red-teaming exercises to evaluate potential mismatches between claimed and actual system performance.\n- Use countermeasures (e.g, authentication, throttling, differential privacy, robust ML approaches) to increase the range of security conditions under which the system is able to return to normal function.\n- Modify system security procedures and countermeasures to increase robustness and resilience to attacks in response to testing and events experienced in production.\n- Verify that information about errors and attack patterns is shared with incident databases, other organizations with similar systems, and system users and stakeholders (MANAGE-4.1).\n- Develop and maintain information sharing practices with AI actors from other organizations to learn from common attacks.\n- Verify that third party AI resources and personnel undergo security audits and screenings. Risk indicators may include failure of third parties to provide relevant security information.\n- Utilize watermarking technologies as a deterrent to data and model extraction attacks.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent does the plan specifically address risks associated with acquisition, procurement of packaged software from vendors, cybersecurity controls, computational infrastructure, data, data science, deployment mechanics, and system failure?\n- What assessments has the entity conducted on data security and privacy impacts associated with the AI system?\n- What processes exist for data generation, acquisition/collection, security, maintenance, and dissemination?\n- What testing, if any, has the entity conducted on the AI system to identify errors and limitations (i.e. adversarial or stress testing)?\n- If a third party created the AI, how will you ensure a level of explainability or interpretability?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\nReferences\n\nMatthew P. Barrett. 'Framework for Improving Critical Infrastructure Cybersecurity Version 1.1.' National Institute of Standards and Technology (NIST), April 16, 2018.\n\nNicolas Papernot. \"A Marauder's Map of Security and Privacy in Machine Learning.\" arXiv preprint, submitted on November 3, 2018.\n\nGary McGraw, Harold Figueroa, Victor Shepardson, and Richie Bonett. 'BIML Interactive Machine Learning Risk Framework.' Berryville Institute of Machine Learning (BIML), 2022.\n\nMitre Corporation. 'Mitre/Advmlthreatmatrix: Adversarial Threat Landscape for AI Systems.' GitHub, 2023.\n\n## National Institute of Standards and Technology (NIST). 'Cybersecurity Framework.' NIST, 2023.\n\nUpol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, and Hal DaumÃ©. 2024. Seamful XAI: Operationalizing Seamful Design in Explainable AI. Proc. ACM Hum.-Comput. Interact. 8, CSCW1, Article 119. https://doi.org/10.1145/3637396\n\n## Software Resources\n\n- adversarial-robustness-toolbox\n- counterfit\n- foolbox\n- ml\\_privacy\\_meter\n- robustness\n- tensorflow/privacy\n- projectGuardRail\n\n## MEASURE 2.8\n\nRisks associated with transparency and accountability -as identified in the MAP function -are examined and documented.\n\n## About\n\nTransparency enables meaningful visibility into entire AI pipelines, workflows, processes or organizations and decreases information asymmetry between AI developers and operators and other AI Actors and impacted communities. Transparency is a central element of effective AI risk management that enables insight into how an AI system is working, and the ability to address risks if and when they emerge. The ability for system users, individuals, or impacted communities to seek redress for incorrect or problematic AI system outcomes is\n\none control for transparency and accountability. Higher level recourse processes are typically enabled by lower level implementation efforts directed at explainability and interpretability functionality. See Measure 2.9. Transparency and accountability across organizations and processes is crucial to reducing AI risks. Accountable leadership -whether individuals or groups -and transparent roles, responsibilities, and lines of communication foster and incentivize quality assurance and risk management activities within organizations. Lack of transparency complicates measurement of trustworthiness and whether AI systems or organizations are subject to effects of various individual and group biases and design blindspots and could lead to diminished user, organizational and community trust, and decreased overall system value. Enstating accountable and transparent organizational structures along with documenting system risks can enable system improvement and risk management efforts, allowing AI actors along the lifecycle to identify errors, suggest improvements, and figure out new ways to contextualize and generalize AI system features and outcomes.\n\n## Suggested Actions\n\n- Instrument the system for measurement and tracking, e.g., by maintaining histories, audit logs and other information that can be used by AI actors to review and evaluate possible sources of error, bias, or vulnerability.\n- Calibrate controls for users in close collaboration with experts in user interaction and user experience (UI/UX), human computer interaction (HCI), and/or human-AI teaming.\n- Test provided explanations for calibration with different audiences including operators, end users, decision makers and decision subjects (individuals for whom decisions are being made), and to enable recourse for consequential system decisions that affect end users or subjects.\n- Measure and document human oversight of AI systems:\n- Document the degree of oversight that is provided by specified AI actors regarding AI system output.\n- Maintain statistics about downstream actions by end users and operators such as system overrides.\n- Maintain statistics about and document reported errors or complaints, time to respond, and response types.\n- Maintain and report statistics about adjudication activities.\n- Track, document, and measure organizational accountability regarding AI systems via policy exceptions and escalations, and document 'go' and 'no/go' decisions made by accountable parties.\n- Track and audit the effectiveness of organizational mechanisms related to AI risk management, including:\n\n- Lines of communication between AI actors, executive leadership, users and impacted communities.\n- Roles and responsibilities for AI actors and executive leadership.\n- Organizational accountability roles, e.g., chief model risk officers, AI oversight committees, responsible or ethical AI directors, etc.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?\n- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?\n- Who is accountable for the ethical considerations during all stages of the AI lifecycle?\n- Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?\n- Are the responsibilities of the personnel involved in the various AI governance processes clearly defined?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\nReferences National Academies of Sciences, Engineering, and Medicine. Human-AI Teaming: State-ofthe-Art and Research Needs. 2022. Inioluwa Deborah Raji and Jingying Yang. \"ABOUT ML: Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles.\" arXiv preprint, submitted January 8, 2020. Andrew Smith. \"Using Artificial Intelligence and Algorithms.\" Federal Trade Commission Business Blog, April 8, 2020. Board of Governors of the Federal Reserve System. 'SR 11 -7: Guidance on Model Risk Management.' April 4, 2011. Joshua A. Kroll. 'Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems.' FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 1, 2021, 758 -71. Jennifer Cobbe, Michelle Seng Lee, and Jatinder Singh. 'Reviewable Automated Decision -Making: A Framework for Accountable Algorithmic Systems.' FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 1, 2021, 598 -609.\n\n## MEASURE 2.9\n\nThe AI model is explained, validated, and documented, and  AI system output is interpreted within its context -as identified in the MAP function -and to inform responsible use and governance.\n\n## About\n\nExplainability and interpretability assist those operating or overseeing an AI system, as well as users of an AI system, to gain deeper insights into the functionality and trustworthiness of the system, including its outputs. Explainable and interpretable AI systems offer information that help end users understand the purposes and potential impact of an AI system. Risk from lack of explainability may be managed by describing how AI systems function, with descriptions tailored to individual differences such as the user's role, knowledge, and skill level. Explainable systems can be debugged and monitored more easily, and they lend themselves to more thorough documentation, audit, and governance. Risks to interpretability often can be addressed by communicating a description of why an AI system made a particular prediction or recommendation. Transparency, explainability, and interpretability are distinct characteristics that support each other. Transparency can answer the question of 'what happened'. Explainability can answer the question of 'how' a decision was made in the system. Interpretability can answer the question of 'why' a decision was made by the system and its meaning or context to the user.\n\n## Suggested Actions\n\n- Verify systems are developed to produce explainable models, post-hoc explanations and audit logs.\n- When possible or available, utilize approaches that are inherently explainable, such as traditional and penalized generalized linear models , decision trees, nearest-neighbor and prototype-based approaches, rule-based models, generalized additive models , explainable boosting machines  and neural additive models.\n- Test explanation methods and resulting explanations prior to deployment to gain feedback from relevant AI actors, end users, and potentially impacted individuals or groups about whether explanations are accurate, clear, and understandable.\n- Document AI model details including model type  (e.g., convolutional neural network, reinforcement learning, decision tree, random forest, etc.) data features, training algorithms, proposed uses, decision thresholds, training data, evaluation data, and ethical considerations.\n- Establish, document, and report performance and error metrics across demographic groups and other segments relevant to the deployment context.\n\n- Explain systems using a variety of methods, e.g., visualizations, model extraction, feature importance, and others. Since explanations may not accurately summarize complex systems, test explanations according to properties such as fidelity, consistency, robustness, and interpretability.\n- Assess the characteristics of system explanations according to properties such as fidelity (local and global), ambiguity, interpretability, interactivity, consistency, and resilience to attack/manipulation.\n- Test the quality of system explanations with end-users and other groups.\n- Secure model development processes to avoid vulnerability to external manipulation such as gaming explanation processes.\n- Test for changes in models over time, including for models that adjust in response to production data.\n- Use transparency tools such as data statements and model cards to document explanatory and validation information.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Given the purpose of the AI, what level of explainability or interpretability is required for how the AI made its determination?\n- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?\n- How has the entity documented the AI system's data provenance, including sources, origins, transformations, augmentations, labels, dependencies, constraints, and metadata?\n- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020.\n\nReferences Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin. \"This Looks Like That: Deep Learning for Interpretable Image Recognition.\" arXiv preprint, submitted December 28, 2019. Cynthia Rudin. \"Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.\" arXiv preprint, submitted September 22, 2019.\n\nDavid A. Broniatowski. \"NISTIR 8367 Psychological Foundations of Explainability and\n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology\n\n(NIST), 2021.\n\nAlejandro Barredo Arrieta, Natalia DÃ­az-RodrÃ­guez, Javier Del Ser, Adrien Bennetot, Siham\n\nTabik, Alberto Barbado, Salvador Garcia, et al. 'Explainable Artificial Intelligence (XAI):\n\nConcepts, Taxonomies, Opportunities, and Challenges Toward Responsible AI.' Information\n\nFusion 58 (June 2020): 82\n\n-\n\n115.\n\nZana BuÃ§inca, Phoebe Lin, Krzysztof Z. Gajos, and Elena L. Glassman. 'Proxy Tasks and\n\nSubjective Measures Can Be Misleading in Evaluating Explainable AI Systems.' IUI '20:\n\nProceedings of the 25th International Conference on Intelligent User Interfaces, March 17,\n\n2020, 454\n\n-\n\n64.\n\nP. Jonathon Phillips, Carina A. Hahn, Peter C. Fontana, Amy N. Yates, Kristen Greene, David\n\nA. Broniatowski, and Mark A. Przybocki. \"NISTIR 8312 Four Principles of Explainable\n\nArtificial Intelligence.\" National Institute of Standards and Technology (NIST), September\n\n2021.\n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben\n\nHutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 'Model Cards for\n\nModel Reporting.' FAT *19: Proceedings of the Conference on Fairness, Accountability, and\n\nTransparency, January 2019, 220\n\n-\n\n29.\n\nKe Yang, Julia Stoyanovich, Abolfazl Asudeh, Bill Howe, HV Jagadish, and Gerome Miklau. 'A\n\nNutritional Label for Rankings.' SIGMOD '18: Proceedings of the 2018 International\n\nConference on Management of Data, May 27, 2018, 1773\n\n-\n\n76.\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"'Why Should I Trust You?': Explaining the Predictions of Any Classifier.\" arXiv preprint, submitted August 9, 2016.\n\nScott M. Lundberg and Su-In Lee. \"A unified approach to interpreting model predictions.\" NIPS'17: Proceedings of the 31st International Conference on Neural Information Processing Systems, December 4, 2017, 4768-4777.\n\nDylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 'Fooling\n\nLIME and SHAP: Adversarial Attacks on Post Hoc Explanation Methods.' AIES '20:\n\nProceedings of the AAAI/ACM Conference on AI, Ethics, and Society, February 7, 2020, 180\n\n86.\n\nDavid Alvarez-Melis and Tommi S. Jaakkola. \"Towards robust interpretability with self- explaining neural networks.\" NIPS'18: Proceedings of the 32nd International Conference on\n\nNeural Information Processing Systems, December 3, 2018, 7786-7795.\n\nFinRegLab, Laura Biattner, and Jann Spiess. \"Machine Learning Explainability &amp; Fairness: Insights from Consumer Lending.\" FinRegLab, April 2022.\n\n-\n\nMiguel Ferreira, Muhammad Bilal Zafar, and Krishna P. Gummadi. \"The Case for Temporal Transparency: Detecting Policy Change Events in Black-Box Decision Making Systems.\" arXiv preprint, submitted October 31, 2016. Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. \"Interpretable &amp;\n\nExplorable Approximations of Black Box Models.\" arXiv preprint, July 4, 2017.\n\n## Software Resources\n\n- SHAP\n- LIME\n- Interpret\n- PiML\n- Iml\n- Dalex\n\n## MEASURE 2.10\n\nPrivacy risk of the AI system -as identified in the MAP function -is examined and documented.\n\n## About\n\nPrivacy refers generally to the norms and practices that help to safeguard human autonomy, identity, and dignity. These norms and practices typically address freedom from intrusion, limiting observation, or individuals' agency to consent to disclosure or control of facets of their identities (e.g., body, data, reputation). Privacy values such as anonymity, confidentiality, and control generally should guide choices for AI system design, development, and deployment. Privacy-related risks may influence security, bias, and transparency and come with tradeoffs with these other characteristics. Like safety and security, specific technical features of an AI system may promote or reduce privacy. AI systems can also present new risks to privacy by allowing inference to identify individuals or previously private information about individuals. Privacyenhancing technologies ('PETs') for AI, as well as data minimizing methods such as de-identification and aggregation for certain model outputs, can support design for privacyenhanced AI systems. Under certain conditions such as data sparsity, privacy enhancing techniques can result in a loss in accuracy, impacting decisions about fairness and other\n\nvalues in certain domains.\n\n## Suggested Actions\n\n- Specify privacy-related values, frameworks, and attributes that are applicable in the context of use through direct engagement with end users and potentially impacted groups and communities.\n- Document collection, use, management, and disclosure of personally sensitive information in datasets, in accordance with privacy and data governance policies\n\n- Quantify privacy-level data aspects such as the ability to identify individuals or groups (e.g. k-anonymity metrics, l-diversity, t-closeness).\n- Establish and document protocols (authorization, duration, type) and access controls for training sets or production data containing personally sensitive information, in accordance with privacy and data governance policies.\n- Monitor internal queries to production data for detecting patterns that isolate personal records.\n- Monitor PSI disclosures and inference of sensitive or legally protected attributes\n- Assess the risk of manipulation from overly customized content. Evaluate information presented to representative users at various points along axes of difference between individuals (e.g. individuals of different ages, genders, races, political affiliation, etc.).\n- Use privacy-enhancing techniques such as differential privacy,  when publicly sharing dataset information.\n- Collaborate with privacy experts, AI end users and operators, and other domain experts to determine optimal differential privacy metrics within contexts of use.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Did your organization implement accountability-based practices in data management and protection (e.g. the PDPA and OECD Privacy Principles)?\n- What assessments has the entity conducted on data security and privacy impacts associated with the AI system?\n- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?\n- Does the dataset contain information that might be considered sensitive or confidential? (e.g., personally identifying information)\n- If it relates to people, could this dataset expose people to harm or legal action? (e.g., financial, social or otherwise) What was done to mitigate or reduce the potential for harm?\n\n## AI Transparency Resources\n\n- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020. (\n- Datasheets for Datasets.\n\n## References\n\nKaitlin R. Boeckl and Naomi B. Lefkovitz. \"NIST Privacy Framework: A Tool for Improving\n\nPrivacy Through Enterprise Risk Management, Version 1.0.\" National Institute of Standards and Technology (NIST), January 16, 2020.\n\nLatanya Sweeney. 'K -Anonymity: A Model for Protecting Privacy.' International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 10, no. 5 (2002): 557 -70.\n\nAshwin Machanavajjhala, Johannes Gehrke, Daniel Kifer, and Muthuramakrishnan\n\nVenkitasubramaniam. 'L\n\n-Diversity: Privacy beyond K-\n\nConference on Data Engineering (ICDE'06), 2006.\n\nNinghui Li, Tiancheng Li, and Suresh Venkatasubramanian. \"CERIAS Tech Report 2007-78 tCloseness: Privacy Beyond k-Anonymity and -Diversity.\" Center for Education and Research, Information Assurance and Security, Purdue University, 2001.\n\nJ. Domingo-Ferrer and J. Soria-Comas. \"From t-closeness to differential privacy and vice versa in data anonymization.\" arXiv preprint, submitted December 21, 2015.\n\nJoseph Near, David Darais, and Kaitlin Boeckly. \"Differential Privacy for Privacy-Preserving Data Analysis: An Introduction to our Blog Series.\" National Institute of Standards and Technology (NIST), July 27, 2020.\n\nCynthia Dwork. 'Differential Privacy.' Automata, Languages and Programming, 2006, 112.\n\nZhanglong Ji, Zachary C. Lipton, and Charles Elkan. \"Differential Privacy and Machine\n\nLearning: a Survey and Review.\" arXiv preprint, submitted December 24,2014.\n\nMichael B. Hawes. \"Implementing Differential Privacy: Seven Lessons From the 2020 United States Census.\" Harvard Data Science Review 2, no. 2 (2020).\n\nHarvard University Privacy Tools Project. 'Differential Privacy.' Harvard University, n.d.\n\nJohn M. Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Micah Heineck, Christine Heiss, Robert Johns, Daniel Kifer, Philip Leclerc, Ashwin Machanavajjhala, Brett Moran, William Matthew Spence Sexton and Pavel Zhuravlev. \"The 2020 Census Disclosure Avoidance System TopDown Algorithm.\" United States Census Bureau, April 7, 2022.\n\nNicolas Papernot and Abhradeep Guha Thakurta. \"How to deploy machine learning with differential privacy.\" National Institute of Standards and Technology (NIST), December 21, 2021.\n\nClaire McKay Bowen. \"Utility Metrics for Differential Privacy: No One-Size-Fits-All.\" National Institute of Standards and Technology (NIST), November 29, 2021.\n\nHelen Nissenbaum. \"Contextual Integrity Up and Down the Data Food Chain.\" Theoretical Inquiries in Law 20, L. 221 (2019): 221-256.\n\nSebastian Benthall, Seda GÃ¼rses, and Helen Nissenbaum. 'Contextual Integrity through the Lens of Computer Science.' Foundations and Trends in Privacy and Security 2, no. 1 (December 22, 2017): 1 -69.\n\nAnonymity.' 22nd International\n\nJenifer Sunrise Winter and Elizabeth Davidson. 'Big Data Governance of Personal Health Information and Challenges to Contextual Integrity.' The Information Society: An International Journal 35, no. 1 (2019): 36 -51.\n\n## MEASURE 2.11\n\nFairness and bias -as identified in the MAP function -is evaluated and results are documented.\n\n## About\n\nFairness in AI includes concerns for equality and equity by addressing issues such as harmful bias and discrimination. Standards of fairness can be complex and difficult to define because perceptions of fairness differ among cultures and may shift depending on application. Organizations' risk management efforts will be enhanced by recognizing and considering these differences. Systems in which harmful biases are mitigated are not necessarily fair. For example, systems in which predictions are somewhat balanced across demographic groups may still be inaccessible to individuals with disabilities or affected by the digital divide or may exacerbate existing disparities or systemic biases. Bias is broader than demographic balance and data representativeness. NIST has identified three major categories of AI bias to be considered and managed: systemic, computational and statistical, and human-cognitive. Each of these can occur in the absence of prejudice, partiality, or discriminatory intent.\n\n- Systemic bias can be present in AI datasets, the organizational norms, practices, and processes across the AI lifecycle, and the broader society that uses AI systems.\n- Computational and statistical biases can be present in AI datasets and algorithmic processes, and often stem from systematic errors due to non-representative samples.\n- Human-cognitive biases relate to how an individual or group perceives AI system information to make a decision or fill in missing information, or how humans think about purposes and functions of an AI system. Human-cognitive biases are omnipresent in decision-making processes across the AI lifecycle and system use, including the design, implementation, operation, and maintenance of AI.\n\nBias exists in many forms and can become ingrained in the automated systems that help make decisions about our lives. While bias is not always a negative phenomenon, AI systems can potentially increase the speed and scale of biases and perpetuate and amplify harms to individuals, groups, communities, organizations, and society.\n\n## Suggested Actions\n\n- Conduct fairness assessments to manage computational and statistical forms of bias which include the following steps:\n- Identify types of harms, including allocational, representational, quality of service, stereotyping, or erasure\n- Identify across, within, and intersecting groups that might be harmed\n\n- Quantify harms using both a general fairness metric, if appropriate (e.g. demographic parity, equalized odds, equal opportunity, statistical hypothesis tests), and custom, context-specific metrics developed in collaboration with affected communities\n- Analyze quantified harms for contextually significant differences across groups, within groups, and among intersecting groups\n- Refine identification of within-group and intersectional group disparities.\n- Evaluate underlying data distributions and employ sensitivity analysis during the analysis of quantified harms.\n- Evaluate quality  metrics including false positive rates and false negative rates.\n- Consider biases affecting small groups, within-group or intersectional communities, or single individuals.\n- Understand and consider sources of bias in training and TEVV data:\n- Differences in distributions of outcomes across and within groups, including intersecting groups.\n- Completeness, representativeness and balance of data sources.\n- Identify input data features that may serve as proxies for demographic group membership (i.e., credit score, ZIP code) or otherwise give rise to emergent bias within AI systems.\n- Forms of systemic bias in images, text (or word embeddings), audio or other complex or unstructured data.\n- Leverage impact assessments to identify and classify system impacts and harms to end users, other individuals, and groups with input from potentially impacted communities.\n- Identify the classes of individuals, groups, or environmental ecosystems which might be impacted through direct engagement with potentially impacted communities.\n- Evaluate systems in regards to disability inclusion, including consideration of disability status in bias testing, and discriminatory screen out processes that may arise from noninclusive design or deployment decisions.\n- Develop objective functions in consideration of systemic biases, in-group/out-group dynamics.\n- Use context-specific fairness metrics to examine how system performance varies across groups, within groups, and/or for intersecting groups. Metrics may include statistical parity, error-rate equality, statistical parity difference, equal opportunity difference, average absolute odds difference, standardized mean difference, percentage point differences.\n- Customize fairness metrics to specific context of use to examine how system performance and potential harms vary within contextual norms.\n- Define acceptable levels of difference in performance in accordance with established organizational governance policies, business requirements, regulatory compliance, legal frameworks, and ethical standards within the context of use\n\n- Define the actions to be taken if disparity levels rise above acceptable levels.\n- Identify groups within the expected population that may require disaggregated analysis, in collaboration with impacted communities.\n- Leverage experts with knowledge in the specific context of use to investigate substantial measurement differences and identify root causes for those differences.\n- Monitor system outputs for performance or bias issues that exceed established tolerance levels.\n- Ensure periodic model updates; test and recalibrate with updated and more representative data to stay within acceptable levels of difference.\n- Apply pre-processing data transformations to address factors related to demographic balance and data representativeness.\n- Apply in-processing to balance model performance quality with bias considerations.\n- Apply post-processing mathematical/computational techniques to model results in close collaboration with impact assessors, socio-technical experts, and other AI actors with expertise in the context of use.\n- Apply model selection approaches with transparent and deliberate consideration of bias management and other trustworthy characteristics.\n- Collect and share information about differences in outcomes for the identified groups.\n- Consider mediations to mitigate differences, especially those that can be traced to past patterns of unfair or biased human decision making.\n- Utilize human-centered design practices to generate deeper focus on societal impacts and counter human-cognitive biases within the AI lifecycle.\n- Evaluate practices along the lifecycle to identify potential sources of human-cognitive bias such as availability, observational, and confirmation bias, and to make implicit decision making processes more explicit and open to investigation.\n- Work with human factors experts to evaluate biases in the presentation of system output to end users, operators and practitioners.\n- Utilize processes to enhance contextual awareness, such as diverse internal staff and stakeholder engagement.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?\n- If it relates to people, does it unfairly advantage or disadvantage a particular social group? In what ways? How was this mitigated?\n- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?\n- How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?\n- To what extent has the entity identified and mitigated potential bias -statistical, contextual, and historical -in the data?\n\n- Were adversarial machine learning approaches considered or used for measuring bias (e.g.: prompt engineering, adversarial models)\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020.\n- Datasheets for Datasets.\n\nReferences\n\nAli Hasan, Shea Brown, Jovana Davidovic, Benjamin Lange, and Mitt Regan. 'Algorithmic Bias and Risk Assessments: Lessons from Practice.' Digital Society 1 (2022). Richard N. Landers and Tara S. Behrend. 'Auditing the AI Auditors: A Framework for Evaluating Fairness and Bias in High Stakes AI Predictive Models.' American Psychologist 78, no. 1 (2023): 36 -49. Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 'A Survey on Bias and Fairness in Machine Learning.' ACM Computing Surveys 54, no. 6 (July 2021): 1 -35. Michele Loi and Christoph Heitz. 'Is Calibration a Fairness Requirement?' FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, June 2022, 2026 -34. Shea Brown, Ryan Carrier, Merve Hickok, and Adam Leon Smith. 'Bias Mitigation in Data Sets.' SocArXiv, July 8, 2021. Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, Andrew Burt, and Patrick Hall. \"NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.\" National Institute of Standards and Technology (NIST), 2022. Microsoft Research. 'AI Fairness Checklist.' Microsoft, February 7, 2022. Samir Passi and Solon Barocas. 'Problem Formulation and Fairness.' FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency, January 2019, 39 -48. Jade S. Franklin, Karan Bhanot, Mohamed Ghalwash, Kristin P. Bennett, Jamie McCusker, and Deborah L. McGuinness. 'An Ontology for Fairness Metrics.' AIES '22: Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, July 2022, 265 -75. Zhang, B., Lemoine, B., &amp; Mitchell, M. (2018). Mitigating Unwanted Biases with Adversarial Learning. Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. https://arxiv.org/pdf/1801.07593.pdf\n\nGanguli, D., et al. (2023). The Capacity for Moral Self-Correction in Large Language Models. arXiv. https://arxiv.org/abs/2302.07459\n\nArvind Narayanan. 'Tl;DS -21 Fairness Definition and Their Politics by Arvind Narayanan.' Dora's world, July 19, 2019.\n\nBen Green. 'Escaping the Impossibility of Fairness: From Formal to Substantive Algorithmic Fairness.' Philosophy and Technology 35, no. 90 (October 8, 2022).\n\nAlexandra Chouldechova. 'Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.' Big Data 5, no. 2 (June 1, 2017): 15363.\n\nSina Fazelpour and Zachary C. Lipton. 'Algorithmic Fairness from a Non -Ideal Perspective.' AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, February 7, 2020, 57 -63.\n\nHemank Lamba, Kit T. Rodolfa, and Rayid Ghani. 'An Empirical Comparison of Bias Reduction Methods on Real-World Problems in HighStakes Policy Settings.' ACM SIGKDD Explorations Newsletter 23, no. 1 (May 29, 2021): 69 -85.\n\nISO. 'ISO/IEC TR 24027:2021 Information technology Artificial intelligence (AI) -Bias in AI systems and AI aided decision making.' ISO Standards, November 2021.\n\nShari Trewin. \"AI Fairness for People with Disabilities: Point of View.\" arXiv preprint, submitted November 26, 2018.\n\nMathWorks. 'Explore Fairness Metrics for Credit Scoring Model.' MATLAB &amp; Simulink, 2023.\n\nAbigail Z. Jacobs and Hanna Wallach. 'Measurement and Fairness.' FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, 375 -85.\n\nTolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. \"Quantifying and Reducing Stereotypes in Word Embeddings.\" arXiv preprint, submitted June 20, 2016.\n\nAylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 'Semantics Derived Automatically from Language Corpora Contain HumanLike Biases.' Science 356, no. 6334 (April 14, 2017): 183 -86.\n\nSina Fazelpour and Maria DeArteaga. 'Diversity in Sociotechnical Machine Learning Systems.' Big Data and Society 9, no. 1 (2022).\n\nFairlearn. 'Fairness in Machine Learning.' Fairlearn 0.8.0 Documentation, n.d.\n\nSafiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. New York, NY: New York University Press, 2018.\n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 'Dissecting\n\nRacial Bias in an Algorithm Used to Manage the Health of Populations.' Science 366, no. 6464 (October 25, 2019): 447 -53. Alekh Agarwal, Alina Beygelzimer, Miroslav DudÃ­k, John Langford, and Hanna Wallach. \"A Reductions Approach to Fair Classification.\" arXiv preprint, submitted July 16, 2018. Moritz Hardt, Eric Price, and Nathan Srebro. \"Equality of Opportunity in Supervised Learning.\" arXiv preprint, submitted October 7, 2016. Alekh Agarwal, Miroslav Dudik, Zhiwei Steven Wu. \"Fair Regression: Quantitative Definitions and Reduction-Based Algorithms.\" Proceedings of the 36th International Conference on Machine Learning, PMLR 97:120-129, 2019. Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 'Fairness and Abstraction in Sociotechnical Systems.' FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency, January 29, 2019, 59 -68. Matthew Kay, Cynthia Matuszek, and Sean A. Munson. 'Unequal Representation and Gender Stereotypes in Image Search Results for Occupations.' CHI '15: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, April 18, 2015, 3819 -28.\n\n## Software Resources\n\nÂ· aequitas - AI Fairness 360: Â· Python Â· R Â· algofairness Â· fairlearn Â· fairml Â· fairmodels Â· fairness Â· solas-ai-disparity Â· tensorflow/fairness-indicators Â· Themis\n\n## MEASURE 2.12\n\nEnvironmental impact and sustainability of AI model training and management activities -as identified in the MAP function -are assessed and documented.\n\n## About\n\nLarge-scale, high-performance computational resources used by AI systems for training and operation can contribute to environmental impacts.  Direct negative impacts to the environment from these processes are related to energy consumption, water consumption,\n\nand greenhouse gas (GHG) emissions. The OECD has identified metrics for each type of negative direct impact. Indirect negative impacts to the environment reflect the complexity of interactions between human behavior, socio-economic systems, and the environment and can include induced consumption and 'rebound effects', where efficiency gains are offset by accelerated resource consumption. Other AI related environmental impacts can arise from the production of computational equipment and networks (e.g. mining and extraction of raw materials), transporting hardware, and electronic waste recycling or disposal.\n\n## Suggested Actions\n\n- Include environmental impact indicators in AI system design and development plans, including reducing consumption and improving efficiencies.\n- Identify and implement key indicators of AI system energy and water consumption and efficiency, and/or GHG emissions.\n- Establish measurable baselines for sustainable AI system operation in accordance with organizational policies, regulatory compliance, legal frameworks, and environmental protection and sustainability norms.\n- Assess tradeoffs between AI system performance and sustainable operations in accordance with organizational principles and policies, regulatory compliance, legal frameworks, and environmental protection and sustainability norms.\n- Identify and establish acceptable resource consumption and efficiency, and GHG emissions levels, along with actions to be taken if indicators rise above acceptable levels.\n- Estimate AI system emissions levels throughout the AI lifecycle via carbon calculators or similar process.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- Are greenhouse gas emissions, and energy and water consumption and efficiency tracked within the organization?\n- Are deployed AI systems evaluated for potential upstream and downstream environmental impacts (e.g., increased consumption, increased emissions, etc.)?\n- Could deployed AI systems cause environmental incidents, e.g., air or water pollution incidents, toxic spills, fires or explosions?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- Datasheets for Datasets.\n\nReferences Organisation for Economic Co-operation and Development (OECD). \"Measuring the environmental impacts of artificial intelligence compute and applications: The AI footprint.' OECD Digital Economy Papers, No. 341, OECD Publishing, Paris. Victor Schmidt, Alexandra Luccioni, Alexandre Lacoste, and Thomas Dandres. 'Machine Learning CO2 Impact Calculator.' ML CO2 Impact, n.d. Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. \"Quantifying the Carbon Emissions of Machine Learning.\" arXiv preprint, submitted November 4, 2019. Matthew Hutson. 'Measuring AI's Carbon Footprint: New Tools Track and Reduce Emissions from Machine Learning.' IEEE Spectrum, November 22, 2022. Association for Computing Machinery (ACM). \"TechBriefs: Computing and Climate Change.\" ACM Technology Policy Council, November 2021. Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 'Green AI.' Communications of the ACM 63, no. 12 (December 2020): 54 -63.\n\n## MEASURE 2.13\n\nEffectiveness of the employed TEVV metrics and processes in the MEASURE function are evaluated and documented.\n\n## About\n\nThe development of metrics is a process often considered to be objective but, as a human and organization driven endeavor, can reflect implicit and systemic biases, and may inadvertently reflect factors unrelated to the target function. Measurement approaches can be oversimplified, gamed, lack critical nuance, become used and relied upon in unexpected ways, fail to account for differences in affected groups and contexts. Revisiting the metrics chosen in Measure 2.1 through 2.12 in a process of continual improvement can help AI actors to evaluate and document metric effectiveness and make necessary course corrections.\n\n## Suggested Actions\n\n- Review selected system metrics and associated TEVV processes to determine if they are able to sustain system improvements, including the identification and removal of errors.\n- Regularly evaluate system metrics for utility, and consider descriptive approaches in place of overly complex methods.\n- Review selected system metrics for acceptability within the end user and impacted community of interest.\n- Assess effectiveness of metrics for identifying and measuring risks.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent does the system/entity consistently measure progress towards stated goals and objectives?\n- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?\n- What corrective actions has the entity taken to enhance the quality, accuracy, reliability, and representativeness of the data?\n- To what extent are the model outputs consistent with the entity's values and principles to foster public trust and equity?\n- How will the accuracy or appropriate performance metrics be assessed?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\nReferences Arvind Narayanan. \"The limits of the quantitative approach to discrimination.\" 2022 James Baldwin lecture, Princeton University, October 11, 2022. Devansh Saxena, Karla BadilloUrquiola, Pamela J. Wisniewski, and Shion Guha. 'A Human -Centered Review of Algorithms Used within the U.S. Child Welfare System.' CHI '20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, April 23, 2020, 1 -15. Rachel Thomas and David Uminsky. 'Reliance on Metrics Is a Fundamental Challenge for AI.' Patterns 3, no. 5 (May 13, 2022): 100476. Momin M. Malik. \"A Hierarchy of Limitations in Machine Learning.\" arXiv preprint, submitted February 29, 2020.\n\n## MEASURE 3.1\n\nApproaches, personnel, and documentation are in place to regularly identify and track existing, unanticipated, and emergent AI risks based on factors such as intended and actual performance in deployed contexts.\n\n## About\n\nFor trustworthy AI systems, regular system monitoring is carried out in accordance with organizational governance policies, AI actor roles and responsibilities, and within a culture of continual improvement. If and when emergent or complex risks arise, it may be necessary to adapt internal risk management procedures, such as regular monitoring, to stay on course. Documentation, resources, and training are part of an overall strategy to\n\nsupport AI actors as they investigate and respond to AI system errors, incidents or negative impacts.\n\n## Suggested Actions\n\n- Compare AI system risks with:\n- simpler or traditional models\n- human baseline performance\n- other manual performance benchmarks\n- Compare end user and community feedback about deployed AI systems to internal measures of system performance.\n- Assess effectiveness of metrics for identifying and measuring emergent risks.\n- Measure error response times and track response quality.\n- Elicit and track feedback from AI actors in user support roles about the type of metrics, explanations and other system information required for fulsome resolution of system issues. Consider:\n- Instances where explanations are insufficient for investigating possible error sources or identifying responses.\n- System metrics, including system logs and explanations, for identifying and diagnosing sources of system error.\n- Elicit and track feedback from AI actors in incident response and support roles about the adequacy of staffing and resources to perform their duties in an effective and timely manner.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?\n- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n- What metrics has the entity developed to measure performance of the AI system, including error logging?\n- To what extent do the metrics provide accurate and useful measure of performance?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF Companion to the Model AI Governance Framework -Implementation and SelfAssessment Guide for Organizations\n\nReferences ISO. \"ISO 9241-210:2019 Ergonomics of human-system interaction -Part 210: Humancentred design for interactive systems.\" 2nd ed. ISO Standards, July 2019. Larysa Visengeriyeva, et al. 'Awesome MLOps.' GitHub.\n\n## MEASURE 3.2\n\nRisk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available.\n\n## About\n\nRisks identified in the Map function may be complex, emerge over time, or difficult to measure. Systematic methods for risk tracking, including novel measurement approaches, can be established as part of regular monitoring and improvement processes.\n\n## Suggested Actions\n\n- Establish processes for tracking emergent risks that may not be measurable with current approaches. Some processes may include:\n- Recourse mechanisms for faulty AI system outputs.\n- Bug bounties.\n- Human-centered design approaches.\n- User-interaction and experience research.\n- Participatory stakeholder engagement with affected or potentially impacted individuals and communities.\n- Identify AI actors responsible for tracking emergent risks and inventory methods.\n- Determine and document the rate of occurrence and severity level for complex or difficult-to-measure risks when:\n- Prioritizing new measurement approaches for deployment tasks.\n- Allocating AI system risk management resources.\n- Evaluating AI system improvements.\n- Making go/no-go decisions for subsequent system iterations.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- Who is ultimately responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?\n- Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?\n- To what extent does the entity communicate its AI strategic goals and objectives to the community of stakeholders?\n\n- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?\n- If anyone believes that the AI no longer meets this ethical framework, who will be responsible for receiving the concern and as appropriate investigating and remediating the issue? Do they have authority to modify, limit, or stop the use of the AI?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\nReferences\n\nISO. \"ISO 9241-210:2019 Ergonomics of human-system interaction -Part 210: Humancentred design for interactive systems.\" 2nd ed. ISO Standards, July 2019.\n\nMark C. Paulk, Bill Curtis, Mary Beth Chrissis, and Charles V. Weber. 'Capability Maturity Model, Version 1.1.' IEEE Software 10, no. 4 (1993): 1827.\n\nJeff Patton, Peter Economy, Martin Fowler, Alan Cooper, and Marty Cagan. User Story Mapping: Discover the Whole Story, Build the Right Product. O'Reilly, 2014.\n\nRumman Chowdhury and Jutta Williams. \"Introducing Twitter's first algorithmic bias bounty challenge.\" Twitter Engineering Blog, July 30, 2021.\n\nHackerOne. \"Twitter Algorithmic Bias.\" HackerOne, August 8, 2021.\n\nJosh Kenway, Camille FranÃ§ois, Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. \"Bug Bounties for Algorithmic Harms?\" Algorithmic Justice League, January 2022.\n\nMicrosoft. 'Community Jury.' Microsoft Learn's Azure Application Architecture Guide, 2023.\n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. \"Overcoming Failures of Imagination in AI Infused System Development and Deployment.\" arXiv preprint, submitted December 10, 2020.\n\n## MEASURE 3.3\n\nFeedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into AI system evaluation metrics.\n\n## About\n\nAssessing impact is a two-way effort. Many AI system outcomes and impacts may not be visible or recognizable to AI actors across the development and deployment dimensions of the AI lifecycle, and may require direct feedback about system outcomes from the perspective of end users and impacted groups.\n\nFeedback can be collected indirectly, via systems that are mechanized to collect errors and other feedback from end users and operators Metrics and insights developed in this sub-category feed into Manage 4.1 and 4.2.\n\n## Suggested Actions\n\n- Measure efficacy of end user and operator error reporting processes.\n- Categorize and analyze type and rate of end user appeal requests and results.\n- Measure feedback activity participation rates and awareness of feedback activity availability.\n- Utilize feedback to analyze measurement approaches and determine subsequent courses of action.\n- Evaluate measurement approaches to determine efficacy for enhancing organizational understanding of real world impacts.\n- Analyze end user and community feedback in close collaboration with domain experts.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n- Did your organization address usability problems and test whether user interfaces served their intended purposes?\n- How easily accessible and current is the information available to external stakeholders?\n- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- WEF Companion to the Model AI Governance Framework -Implementation and SelfAssessment Guide for Organizations\n\nReferences Sasha Costanza-Chock. Design Justice: Community-Led Practices to Build the Worlds We Need. Cambridge: The MIT Press, 2020. David G. Robinson. Voices in the Code: A Story About People, Their Values, and the Algorithm They Made. New York: Russell Sage Foundation, 2022. Fernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder Participation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted November 1, 2021.\n\n-\n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. 'Human Centered Design of Artificial Intelligence.' In Handbook of Human Factors and Ergonomics, edited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085 -1106. John Wiley &amp; Sons, 2021. Ben Shneiderman. Human-Centered AI. Oxford: Oxford University Press, 2022 Batya Friedman, David G. Hendry, and Alan Borning. 'A Survey of Value Sensitive Design Methods.' Foundations and Trends in Human -Computer Interaction 11, no. 2 (November 22, 2017): 63 -125. Batya Friedman, Peter H. Kahn, Jr., and Alan Borning. \"Value Sensitive Design: Theory and Methods.\" University of Washington Department of Computer Science &amp; Engineering Technical Report 02-12-01, December 2002. Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. 'Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.' SSRN, July 8, 2021. Alexandra Reeve Givens, and Meredith Ringel Morris. 'Centering Disability Perspectives in Algorithmic Fairness, Accountability, &amp; Transparency.' FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, January 27, 2020, 684-84.\n\n## MEASURE 4.1\n\nMeasurement approaches for identifying AI risks are connected to deployment context(s) and informed through consultation with domain experts and other end users. Approaches are documented.\n\n## About\n\nAI Actors carrying out TEVV tasks may have difficulty evaluating impacts within the system context of use. AI system risks and impacts are often best described by end users and others who may be affected by output and subsequent decisions. AI Actors can elicit feedback from impacted individuals and communities via participatory engagement processes established in Govern 5.1 and 5.2, and carried out in Map 1.6, 5.1, and 5.2. Activities described in the Measure function enable AI actors to evaluate feedback from impacted individuals and communities. To increase awareness of insights, feedback can be evaluated in close collaboration with AI actors responsible for impact assessment, humanfactors, and governance and oversight tasks, as well as with other socio-technical domain experts and researchers. To gain broader expertise for interpreting evaluation outcomes, organizations may consider collaborating with advocacy groups and civil society organizations. Insights based on this type of analysis can inform TEVV-based decisions about metrics and related courses of action.\n\n## Suggested Actions\n\n- Support mechanisms for capturing feedback from system end users (including domain experts, operators, and practitioners). Successful approaches are:\n- conducted in settings where end users are able to openly share their doubts and insights about AI system output, and in connection to their specific context of use (including setting and task-specific lines of inquiry)\n- developed and implemented by human-factors and socio-technical domain experts and researchers\n- designed to ensure control of interviewer and end user subjectivity and biases\n- Identify and document approaches\n- for evaluating and integrating elicited feedback from system end users\n- in collaboration with human-factors and socio-technical domain experts,\n- to actively inform a process of continual improvement.\n- Evaluate feedback from end users alongside evaluated feedback from impacted communities (MEASURE 3.3).\n- Utilize end user feedback to investigate how selected metrics and measurement approaches interact with organizational and operational contexts.\n- Analyze and document system-internal measurement processes in comparison to collected end user feedback.\n- Identify and implement approaches to measure effectiveness and satisfaction with end user elicitation techniques, and document results.\n\n## Transparency &amp; Documentation Organizations can document the following\n\n- Did your organization address usability problems and test whether user interfaces served their intended purposes?\n- How will user and peer engagement be integrated into the model development process and periodic performance review once deployed?\n- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n- WEF Companion to the Model AI Governance Framework -Implementation and SelfAssessment Guide for Organizations\n\nReferences\n\nBatya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with Moral Imagination. Cambridge, MA: The MIT Press, 2019.\n\nBatya Friedman, David G. Hendry, and Alan Borning. 'A Survey of Value Sensitive Design Methods.' Foundations and Trends in Human -Computer Interaction 11, no. 2 (November 22, 2017): 63 -125.\n\nSteven Umbrello, and Ibo van de Poel. 'Mapping Value Sensitive Design onto AI for Social Good Principles.' AI and Ethics 1, no. 3 (February 1, 2021): 28396.\n\nKaren Boyd. 'Designing Up with Value -Sensitive Design: Building a Field Guide for Ethical ML Development.' FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, June 20, 2022, 2069 -82.\n\nJanet Davis and Lisa P. Nathan. 'Value Sensitive Design: Applications, Adaptations, and Critiques.' In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van den Hoven, Pieter E. Vermaas, and Ibo van de Poel,  January 1, 2015, 11 -40.\n\nBen Shneiderman. Human-Centered AI. Oxford: Oxford University Press, 2022.\n\nShneiderman, Ben. 'Human\n\n-\n\nCentered AI.' Issues in Science and Technology 37, no. 2\n\n(2021): 56\n\n-\n\n61.\n\nShneiderman, Ben. 'Tutorial: Human\n\n-\n\nCentered AI: Reliable, Safe and Trustworthy.' IUI '21\n\nCompanion: 26th International Conference on Intelligent User Interfaces - Companion, April\n\n14, 2021, 7\n\n-\n\n8.\n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. 'Human\n\n-\n\nCentered Design of Artificial Intelligence.' In Handbook of Human Factors and Ergonomics, edited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085\n\nSons, 2021.\n\nCaitlin Thompson. 'Who's Homeless Enough for Housing? In San Francisco, an Algorithm Decides.' Coda, September 21, 2021.\n\nJohn Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. 'Algorithmic Decision -\n\nMaking and the Control Problem.' Minds and Machines 29, no. 4 (December 11, 2019): 555-\n\n78.\n\nFry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton &amp; Company, 2018.\n\nSasha Costanza-Chock. Design Justice: Community-Led Practices to Build the Worlds We Need. Cambridge: The MIT Press, 2020.\n\nDavid G. Robinson. Voices in the Code: A Story About People, Their Values, and the Algorithm They Made. New York: Russell Sage Foundation, 2022.\n\n-\n\n1106. John Wiley &amp;\n\nDiane Hart, Gabi DiercksO'Brien, and Adrian Powell. 'Exploring Stakeholder Engagement in Impact Evaluation Planning in Educational Development Work.' Evaluation 15, no. 3\n\n(2009): 285 -306. Asit Bhattacharyya and Lorne Cummings. 'Measuring Corporate Environmental Performance -Stakeholder Engagement Evaluation.' Business Strategy and the Environment 24, no. 5 (2013): 309 -25. Hendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. 'A Modified Stakeholder Participation Assessment Framework for Design Thinking in Health Innovation.' Healthcare 6, no. 3 (September 2018): 19196. Fernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder Participation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted November 1, 2021. Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. 'Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.' SSRN, July 8, 2021. Alexandra Reeve Givens, and Meredith Ringel Morris. 'Centering Disability Perspectives in Algorithmic Fairness, Accountability, &amp; Transparency.' FAT* '20: Proceedings of the 2020\n\nConference on Fairness, Accountability, and Transparency, January 27, 2020, 684-84.\n\n## MEASURE 4.2\n\nMeasurement results regarding AI system trustworthiness in deployment context(s) and across AI lifecycle are informed by input from domain experts and other relevant AI actors to validate whether the system is performing consistently as intended. Results are documented.\n\n## About\n\nFeedback captured from relevant AI Actors can be evaluated in combination with output from Measure 2.5 to 2.11 to determine if the AI system is performing within pre-defined operational limits for validity and reliability, safety, security and resilience, privacy, bias and fairness, explainability and interpretability, and transparency and accountability. This feedback provides an additional layer of insight about AI system performance, including potential misuse or reuse outside of intended settings. Insights based on this type of analysis can inform TEVV-based decisions about metrics and related courses of action.\n\n## Suggested Actions\n\n- Integrate feedback from end users, operators, and affected individuals and communities from Map function as inputs to assess AI system trustworthiness characteristics. Ensure both positive and negative feedback is being assessed.\n\n- Evaluate feedback in connection with AI system trustworthiness characteristics from Measure 2.5 to 2.11.\n- Evaluate feedback regarding end user satisfaction with, and confidence in, AI system performance including whether output is considered valid and reliable, and explainable and interpretable.\n- Identify mechanisms to confirm/support AI system output (e.g. recommendations), and end user perspectives about that output.\n- Measure frequency of AI systems' override decisions, evaluate and document results, and feed insights back into continual improvement processes.\n- Consult AI actors in impact assessment, human factors and socio-technical tasks to assist with analysis and interpretation of results.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent does the system/entity consistently measure progress towards stated goals and objectives?\n- What policies has the entity developed to ensure the use of the AI system is consistent with its stated values and principles?\n- To what extent are the model outputs consistent with the entity's values and principles to foster public trust and equity?\n- Given the purpose of the AI, what level of explainability or interpretability is required for how the AI made its determination?\n- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\nReferences Batya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with Moral Imagination. Cambridge, MA: The MIT Press, 2019. Batya Friedman, David G. Hendry, and Alan Borning. 'A Survey of Value Sensitive Design Methods.' Foundations and Trends in Human -Computer Interaction 11, no. 2 (November 22, 2017): 63 -125. Steven Umbrello, and Ibo van de Poel. 'Mapping Value Sensitive Design onto AI for Social\n\nGood Principles.' AI and Ethics 1, no. 3 (February 1, 2021): 28396.\n\nKaren Boyd. 'Designing Up with Value -Sensitive Design: Building a Field Guide for Ethical ML Development.' FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, June 20, 2022, 2069 -82.\n\nJanet Davis and Lisa P. Nathan. 'Value Sensitive Design: Applications, Adaptations, and\n\nCritiques.' In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van den Hoven, Pieter E. Vermaas, and Ibo van de Poel,  January 1, 2015, 11 -40. Ben Shneiderman. Human-Centered AI. Oxford: Oxford University Press, 2022. Shneiderman, Ben. 'Human -Centered AI.' Issues in Science and Technology 37, no. 2 (2021): 56 -61. Shneiderman, Ben. 'Tutorial: Human -Centered AI: Reliable, Safe and Trustworthy.' IUI '21 Companion: 26th International Conference on Intelligent User Interfaces - Companion, April 14, 2021, 7 -8. George Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. 'Human -Centered Design of Artificial Intelligence.' In Handbook of Human Factors and Ergonomics, edited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085 -1106. John Wiley &amp; Sons, 2021. Caitlin Thompson. 'Who's Homeless Enough for Housing? In San Francisco, an Algorithm Decides.' Coda, September 21, 2021. John Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. 'Algorithmic Decision -Making and the Control Problem.' Minds and Machines 29, no. 4 (December 11, 2019): 55578. Fry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton &amp; Company, 2018. Sasha Costanza-Chock. Design Justice: Community-Led Practices to Build the Worlds We Need. Cambridge: The MIT Press, 2020. David G. Robinson. Voices in the Code: A Story About People, Their Values, and the Algorithm They Made. New York: Russell Sage Foundation, 2022. Diane Hart, Gabi DiercksO'Brien, and Adrian Powell. 'Exploring Stakeholder Engagement in Impact Evaluation Planning in Educational Development Work.' Evaluation 15, no. 3 (2009): 285 -306. Asit Bhattacharyya and Lorne Cummings. 'Measuring Corporate Environmental Performance -Stakeholder Engagement Evaluation.' Business Strategy and the Environment 24, no. 5 (2013): 309 -25. Hendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. 'A Modified Stakeholder Participation Assessment Framework for Design Thinking in Health Innovation.' Healthcare 6, no. 3 (September 2018): 19196.\n\nFernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder\n\nParticipation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted November 1, 2021. Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. 'Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.' SSRN, July 8, 2021. Alexandra Reeve Givens, and Meredith Ringel Morris. 'Centering Disability Perspectives in\n\nAlgorithmic Fairness, Accountability, &amp; Transparency.' FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, January 27, 2020, 684-84.\n\n## MEASURE 4.3\n\nMeasurable performance improvements or declines based on consultations with relevant AI actors including affected communities, and field data about context-relevant risks and trustworthiness characteristics, are identified and documented.\n\n## About\n\nTEVV activities conducted throughout the AI system lifecycle can provide baseline quantitative measures for trustworthy characteristics. When combined with results from Measure 2.5 to 2.11 and Measure 4.1 and 4.2, TEVV actors can maintain a comprehensive view of system performance. These measures can be augmented through participatory engagement with potentially impacted communities or other forms of stakeholder elicitation about AI systems' impacts. These sources of information can allow AI actors to explore potential adjustments to system components, adapt operating conditions, or institute performance improvements.\n\n## Suggested Actions\n\n- Develop baseline quantitative measures for trustworthy characteristics.\n- Delimit and characterize baseline operation values and states.\n- Utilize qualitative approaches to augment and complement quantitative baseline measures, in close coordination with impact assessment, human factors and sociotechnical AI actors.\n- Monitor and assess measurements as part of continual improvement to identify potential system adjustments or modifications\n- Perform and document sensitivity analysis to characterize actual and expected variance in performance after applying system or procedural updates.\n- Document decisions related to the sensitivity analysis and record expected influence on system performance and identified risks.\n\n## Transparency &amp; Documentation\n\n## Organizations can document the following\n\n- To what extent are the model outputs consistent with the entity's values and principles to foster public trust and equity?\n\n- How were sensitive variables (e.g., demographic and socioeconomic categories) that may be subject to regulatory compliance specifically selected or not selected for modeling purposes?\n- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?\n- How will the accountable human(s) address changes in accuracy and precision due to either an adversary's attempts to disrupt the AI or unrelated changes in the operational/business environment?\n- How will user and peer engagement be integrated into the model development process and periodic performance review once deployed?\n\n## AI Transparency Resources\n\n- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies &amp; Other Entities.\n- Artificial Intelligence Ethics Framework For The Intelligence Community.\n\nReferences\n\nBatya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with\n\nMoral Imagination. Cambridge, MA: The MIT Press, 2019.\n\nBatya Friedman, David G. Hendry, and Alan Borning. 'A Survey of Value Sensitive Design Methods.' Foundations and Trends in Human -Computer Interaction 11, no. 2 (November 22, 2017): 63 -125.\n\nSteven Umbrello, and Ibo van de Poel. 'Mapping Value Sensitive Design onto AI for Social Good Principles.' AI and Ethics 1, no. 3 (February 1, 2021): 28396.\n\nKaren Boyd. 'Designing Up with Value -Sensitive Design: Building a Field Guide for Ethical ML Development.' FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, June 20, 2022, 2069 -82.\n\nJanet Davis and Lisa P. Nathan. 'Value Sensitive Design: Applications, Adaptations, and Critiques.' In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van den Hoven, Pieter E. Vermaas, and Ibo van de Poel,  January 1, 2015, 11 -40.\n\n## Ben Shneiderman. Human-Centered AI. Oxford: Oxford University Press, 2022.\n\nShneiderman, Ben. 'Human -Centered AI.' Issues in Science and Technology 37, no. 2 (2021): 56 -61.\n\nShneiderman, Ben. 'Tutorial: Human -Centered AI: Reliable, Safe and Trustworthy.' IUI '21 Companion: 26th International Conference on Intelligent User Interfaces - Companion, April 14, 2021, 7 -8.\n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. 'Human\n\n-\n\nCentered Design of Artificial Intelligence.' In Handbook of Human Factors and Ergonomics, edited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085\n\nSons, 2021.\n\nCaitlin Thompson. 'Who's Homeless Enough for Housing? In San Francisco, an Algorithm Decides.' Coda, September 21, 2021.\n\nJohn Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. 'Algorithmic Decision\n\n-\n\nMaking and the Control Problem.' Minds and Machines 29, no. 4 (December 11, 2019): 555-\n\n78.\n\nFry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton &amp; Company, 2018.\n\nSasha Costanza-Chock. Design Justice: Community-Led Practices to Build the Worlds We Need. Cambridge: The MIT Press, 2020.\n\nDavid G. Robinson. Voices in the Code: A Story About People, Their Values, and the Algorithm They Made. New York: Russell Sage Foundation, 2022.\n\nDiane Hart, Gabi DiercksO'Brien, and Adrian Powell. 'Exploring Stakeholder Engagement in Impact Evaluation Planning in Educational Development Work.' Evaluation 15, no. 3 (2009): 285 -306.\n\nAsit Bhattacharyya and Lorne Cummings. 'Measuring Corporate Environmental Performance -Stakeholder Engagement Evaluation.' Business Strategy and the Environment 24, no. 5 (2013): 309 -25.\n\nHendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. 'A Modified\n\nStakeholder Participation Assessment Framework for Design Thinking in Health\n\nInnovation.' Healthcare 6, no. 3 (September 2018): 191-\n\n96.\n\nFernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder\n\nParticipation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted\n\nNovember 1, 2021.\n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf.\n\n'Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.' SSRN,\n\nJuly 8, 2021.\n\nAlexandra Reeve Givens, and Meredith Ringel Morris. 'Centering Disability Perspectives in Algorithmic Fairness, Accountability, &amp; Transparency.' FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, January 27, 2020, 684-84.\n\n-\n\n1106. John Wiley &amp;", "fetched_at_utc": "2026-02-09T13:38:20Z", "sha256": "1a939802ec83291aa7579a1777bcbadde0982cdb7805d0d3a82472463b46b42a", "meta": {"file_name": "AI Risk Management Framework Playbook - NIST.pdf", "file_size": 2882270, "mtime": 1766930270, "docling_errors": []}}
{"doc_id": "pdf-pdfs-ai-security-concerns-in-a-nutshell-62a50d12c363", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Security Concerns in a Nutshell.pdf", "title": "AI Security Concerns in a Nutshell", "text": "<!-- image -->\n\n## AI SECURITY CONCERNS IN A NUTSHELL\n\n<!-- image -->\n\n## Document history\n\n|   Version | Date       | Editor   | Description   |\n|-----------|------------|----------|---------------|\n|         1 | 09.03.2023 | TK24     | First Release |\n\nFederal Office for Information Security P.O. Box 20 03 63 53133 Bonn E-Mail:  ki-kontakt@bsi.bund.de Internet: https://www.bsi.bund.de Â© Federal Office for Information Security 2023\n\n## Table of Contents\n\n| 1                                                                                                                                                                                                                  | Introduction............................................................................................................................................................................................4          |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2                                                                                                                                                                                                                  | General Measures for IT Security of AI-Systems...................................................................................................................5                                                 |\n| 3                                                                                                                                                                                                                  | Evasion Attacks......................................................................................................................................................................................6             |\n| 3.1                                                                                                                                                                                                                | Construction of Adversarial Examples............................................................................................................................6                                                  |\n| 3.2                                                                                                                                                                                                                | Evasion Attacks in Transfer Learning ..............................................................................................................................6                                               |\n| 3.3                                                                                                                                                                                                                | Defending against Evasion Attacks...................................................................................................................................7                                              |\n| 4                                                                                                                                                                                                                  | Information Extraction Attacks.....................................................................................................................................................8                               |\n| 4.1                                                                                                                                                                                                                | Model Stealing Attacks............................................................................................................................................................8                                |\n| 4.2                                                                                                                                                                                                                | Membership Inference Attacks...........................................................................................................................................8                                           |\n| 4.3                                                                                                                                                                                                                | Attribute Inference Attacks...................................................................................................................................................8                                    |\n| 4.4                                                                                                                                                                                                                | Model Inversion Attacks.........................................................................................................................................................8                                  |\n| 4.5                                                                                                                                                                                                                | Defending against Information Extraction Attacks..................................................................................................9                                                                |\n| 5                                                                                                                                                                                                                  | Poisoning and Backdoor Attacks...................................................................................................................................................9                                 |\n| 5.1                                                                                                                                                                                                                | Poisoning Attacks ......................................................................................................................................................................9                          |\n| 5.2                                                                                                                                                                                                                | Backdoor Attacks.....................................................................................................................................................................10                            |\n| 5.3                                                                                                                                                                                                                | Defending against Poisoning and Backdoor Attacks..............................................................................................10                                                                   |\n| 6                                                                                                                                                                                                                  | Limitations.............................................................................................................................................................................................11         |\n| Bibliography....................................................................................................................................................................................................12 | Bibliography....................................................................................................................................................................................................12 |\n\n## 1 Introduction\n\nThis guideline introduces developers to the most relevant attacks on machine learning systems and potential complementary defences. It does not claim to be comprehensive and can only offer a first introduction to the topic.\n\nIn many applications, machine learning models use sensitive information as training data or make decisions that affect people in critical areas, like autonomous driving, cancer detection, and biometric authentication. The possible impact of attacks increase as machine learning is used more and more in critical applications. Attacks that either aim at extracting data from the models or manipulating their decisions are threats that need to be considered during a risk assessment. Using pre-trained models or publicly available datasets from external sources lowers the resources needed for developing AI systems, but may also enable a variety of attacks. The datasets or models could be prepared maliciously to induce a specific behaviour during deployment, unknown to the AI developer. Furthermore, overfitting, a state in which a model has memorized the training data and does not generalize well to previously unseen data, can increase the chances of extracting private information from models or facilitate more effective evasion attacks.\n\nApart from malicious attacks on machine learning models, a lack of comprehension of their decisionmaking process poses a threat. The models could be learning spurious correlations from faulty or insufficient training data. Therefore, it is helpful to understand their decision process before deploying them to real-world use cases. The following chapters introduce three broad categories of possible attacks: Evasion Attacks, Information Extraction Attacks and Backdoor Attacks. Additionally a set of possible first defences for each category is introduced.\n\n## 2 General Measures for IT Security of AI-Systems\n\nAI systems exhibit some unique characteristics that give rise to novel attacks, which are treated extensively in the following sections. AI systems are IT systems, meaning classical measures can be applied to increase IT security. Moreover, AI systems, in practice, do not operate in isolation but are embedded in a more extensive IT system consisting of various components. They can introduce additional layers of defence, e.g., by making side conditions unfavourable for attackers, beyond the level of the AI system itself (which is the last line of defence).\n\nClassical IT security measures address a wide array of topics. In-depth recommendations by the BSI can be found in the IT-Grundschutz [1]. One important measure is the documentation of all relevant facts and developer choices during the system's development and the way the system operates. Log files should be used to monitor the system's operation and should be regularly checked for anomalies. The responsibilities within the development process and the subsequent operation should be clearly distributed, and emergency plans should be in place.\n\nIn addition, technical protection measures on various levels should be applied. This includes classical network security, e.g., by using firewalls. To thwart attacks, it is also essential to protect the input and output of the AI system from tampering, using measures on the hardware, operating system, and software level (in particular, installing security patches as soon as possible) as appropriate for the respective threat level. Access control should be used for the AI system during development and inference time. Furthermore, access rights should be bound to authentication at an appropriate level of assurance.\n\nApart from generic classical measures, other general measures can also help address AI-specific threats. A possible safeguard for the AI system development process is to mandate background checks of the (core) developers. Another measure is to document and protect important information cryptographically for the whole AI life cycle. This can include the used data sets, pre-processing steps, pre-trained models, and the training procedure itself. Cryptographic protection can be applied using hash functions and digital signatures, which allow for verifying that no tampering has occurred at intermediate steps [2]. The amount of effort required for documentation and protection can vary greatly and should be appropriate for the use case.\n\nThe robustness of the outputs of the AI system can be increased and its susceptibility to attacks be reduced by operating multiple AI systems using different architectures or different training data redundantly. Further information may also be gleaned from other sources and allow for detecting attacks. For example, biometric fakes can be detected using additional sensors in biometric authentication. In cases where this is feasible, an additional layer of human supervision - constantly present or acting on request in cases of ambiguity - can also improve security.\n\nAttacks that aim to extract information via queries to the model can be hampered by supplying only relevant information, ignoring invalid queries, or imposing and enforcing limits on the number of allowed queries.\n\n## 3 Evasion Attacks\n\nWithin an evasion attack, an attacker aims to cause a misclassification during the inference phase of a machine learning model. The attacker constructs a malicious input, which is typically close to a benign sample, to conceal the attack. These inputs, denoted as adversarial examples, are generated by adding a perturbation to the input that fools the model or reduces its accuracy. Evasion attacks can be separated into targeted attacks, where the attacker forces the model to predict the desired target value, and untargeted attacks that cause a general reduction in model accuracy or prediction confidence. Evasion attacks can take place in the physical or digital world. For example, certain patterns could cause an automated car to mistake traffic signs, or a biometric camera system to mistake somebody's identity. These patterns or perturbations may not be perceptible to humans.\n\nIn the following, a brief overview of methods to create such attacks and possible defences are outlined. The article provides key ideas instead of covering all existing methods and details. For a more in-depth reading, we refer an interested reader to the study [3] or other up-to-date research surveys on the topic.\n\n## 3.1 Construction of Adversarial Examples\n\nA popular approach to creating adversarial examples is the Fast Gradient Method (FGM) [4], which creates adversarial examples by relying on the model's gradient. The method needs white-box access, which means access to the model, including its structure, internal weights and gradients. For the attack, a perturbation pattern is calculated from the gradient of the loss function with respect to the input. It is scaled by Epsilon ðœ–ðœ– , which describes the amount of perturbation, and added to the original sample, creating an adversarial one. The adversarial sample increases the result of the cost function for the correct label, which can result in a completely different prediction while staying visually close to the original sample. Depending on the magnitude of ðœ–ðœ– , the manipulated images are more or less noticeable to the human observer. In the case of image recognition tasks, the larger the epsilon, the easier it is for a human observer to spot the perturbation.\n\nFigure 1: A perturbation of epsilon = 0.2 is added to inputs, creating adversarial examples.\n\n<!-- image -->\n\nFigure 1 shows a perturbation of ðœ–ðœ– = 0.2 added to a sample. As a result, the model's prediction confidence decreases, and some samples are misclassified. Apart from white-box attacks like FGM, there exist black-box attacks that require only access to the model, meaning the attacker can only query the model as an oracle for confidence scores or output labels, see e.g. [5].\n\n## 3.2 Evasion Attacks in Transfer Learning\n\nTransfer learning describes a technique in which (parts of) an existing machine learning model, called the teacher model, are retrained for a different target domain. The resulting model is called the student model.\n\nThe retraining might require only a small training data set, and the computational effort might be modest in comparison to a model trained from scratch.\n\nRegarding evasion attacks, the main concern is that evasion attacks on the teacher model might also be applicable to a student model. If the teacher model is openly available, it could be misused for this purpose by an attacker.\n\n## 3.3 Defending against Evasion Attacks\n\nGiven a concrete task, a risk analysis should be performed to determine the criticality and applicability of evasion attacks. In the following, several defence methods are outlined. It is encouraged to simulate concrete attacks on your system to check the vulnerability to attacks and effectiveness of selected defence mechanisms.\n\n## Adversarial Retraining\n\nAdversarial retraining consists of iteratively generating adversarial examples and repeatedly training the model on them. As a result, the robustness of the model against the selected attack methods increases.\n\n## Generalization\n\nUsing a diverse and qualitative training data set is a good way to reduce the susceptibility of the model to certain adversarial examples. If the AI's decision barriers enclose the known class too closely it may be easy to sample visually close inputs, which are detected as different class [6]. Additionally, random transformations within the bounds of the natural feature distribution, like omitting input pixels (dropout), tilting, compression, or filters can be used to increase the size and variety of the training data.\n\n## Defending against Adversarial Attacks based on a teacher model\n\nThe success of adversarial attacks transferred from a teacher model to a student model may be reduced by lowering the similarity between the teacher and the student model [7]. For this purpose, the weights in the different layers of the student model need to be changed. A disadvantage is the computing time required for the adjustment. This procedure can be applied without affecting classification accuracy significantly. However, black-box attacks on the student model are still possible [7].\n\n## 4 Information Extraction Attacks\n\nInformation extraction attacks, which are also referred to as privacy or reconstruction attacks, summarize all attacks that aim at reconstructing the model or information from its training data. They include model stealing attacks, attribute inference attacks, membership inference attacks, and model inversion attacks. Information extraction attacks often require prior knowledge about the training dataset or access to its publicly available parts.\n\n## 4.1 Model Stealing Attacks\n\nFor organizations who invested significant resources in the development of a commercial AI model, model stealing is a threat. Attackers can try to steal the model's architecture or reconstruct it by querying the original model and feeding the answers back into their own shadow model. Model stealing can serve as a stepping stone for other attacks, e.g. generating transferable adversarial attacks based on the shadow model.\n\n## 4.2 Membership Inference Attacks\n\nIn membership inference attacks, the attacker tries to determine whether a data sample was part of a model's training data. From a privacy perspective, determining the membership of an individual's data in a dataset or restoring its attributes can be sensitive [8]. The attack utilizes differences in model behaviour on new input data and data used for training. One possibility to implement such an attack is to train an attack model to recognize such differences [8]. For this purpose, the attacker requires at least black-box access to the predicted label, e.g. API access. For some attacks, background knowledge about the population from which the target model's training dataset was drawn is required [8].\n\n## 4.3 Attribute Inference Attacks\n\nIn attribute inference attacks, the attacker seeks to breach the confidentiality of the model's training data by determining the value of a sensitive attribute associated with a specific individual or identity in the training data [9, 10]. The attacker requires access to the model and a publicly available part of the victim's dataset. Such attack methods utilize the statistical correlation of sensitive (non-public attributes) and non-sensitive (public) attributes as well as the general distribution of attributes [11]. An example for an attribute inference attack in general, is to infer sensitive attributes, e.g. the home address of a user, by using publicly available information in social networks [12]. Although the sensitive attribute might not be publicly available directly, it might be deduced combining different sources of public knowledge.\n\n## 4.4 Model Inversion Attacks\n\nModel inversion attacks aim to recover features that characterize classes from the training data. As a result, the attacker can create a representative sample for a class, which is not from the training set but shows features of the class it represents. Attacks based on generative adversarial networks (GANs) typically require only black-box access to the model, which makes the target architectures irrelevant [9]. However, attacks based on 'DeepInversion' [13, 14] require black-box access to the batch normalization layers of a neural network, which contain the average and variance of the activations. Therefore, they are architecturedependent. The basic idea of each attack version is to search for input features, which maximize the model's output probability for the attacked class. By gaining knowledge of the distribution of input features, the attacker is able to narrow down the search space for high-dimensional input features In GAN-based attacks, the attacker can train a GAN with a surrogate training set that shares a similar feature distribution with the actual training data. As a result, the GAN generates high-probability samples (Figure 2) for a chosen class [9]. A possible attack scenario could be to recover a person's face only by having access to the outputs of a classifier trained to recognize this person. As the GAN-based attack only needs surrogate training data with e.g., sample faces, knowledge of the victim's face is not required for the attack.\n\nFigure 2: A horse from the CIFAR10 dataset on the left vs. an artificial one created by a GAN trained on a surrogate dataset on the right.\n\n<!-- image -->\n\n## 4.5 Defending against Information Extraction Attacks\n\nIt is encouraged to simulate concrete attacks on your system to check the vulnerability to attacks and effectiveness of selected defence mechanisms.\n\n## Decrease Model Output\n\nAs many information extraction attacks use model confidence scores as the basis for an attack, reducing the scope of the model's output values or their precision might increase the effort for attackers [15]. However, as for example seen in the case of membership inference attacks, there might be attack methods just relying on class labels circumventing such a measure.\n\n## Data Sanitization\n\nRemoving all the sensitive parts of the data before using it for training makes it impossible for intruders to extract the data from the trained model.\n\n## Avoid Overfitting\n\nPrivacy attacks benefit from the overfitting of a model. Consequently, good model generalization mitigates the risk of successful privacy attacks. This might be achieved by a large and diverse training set as well as techniques such as regularization, dropout, or dataset condensation [16]. However, effectiveness of the used methods might depend on the concrete setting at hand.\n\n## Differential Privacy\n\nDifferential privacy is a concept that helps to describe and quantify privacy in the processing of data. It demands, 'Nothing about an individual should be learnable from the database that cannot be learned without access to the database' [17]. Differential privacy is often measured by a parameter Îµ , with lower values corresponding to greater privacy. Given a concrete application, the correct choice of Îµ is difficult to determine because there is a trade-off between privacy and the accuracy of the algorithm or model that uses the database. Finding suitable parameters might be costly in terms of computational effort. A model trained with differential private data might still be susceptible to attribute inference attacks since DP does not explicitly aim to protect attribute privacy [9].\n\n## 5 Poisoning and Backdoor Attacks\n\n## 5.1 Poisoning Attacks\n\nThe attack goals of data poisoning are the malfunctioning or performance degradation of machine learning models [18]. Therefore, the adversary manipulates the training dataset used by a machine learning model.\n\nA computationally inexpensive poisoning attack consists of flipping the label of an input to the desired class. Subsequently, the poisoned samples are injected into the training set and used during model training. This attack method may be effective with only a small number of poisoned samples. However, the flipped training sample labels might be discoverable by manual inspection of the training dataset [19].\n\n## 5.2 Backdoor Attacks\n\nBackdoor attacks are targeted poisoning attacks. A backdoor attack aims at creating a predetermined response to a trigger in an input while maintaining the system's performance in its absence. In the image domain, attack triggers can take the form of patterns or hard-to-see projections onto the input images [18].\n\nFigure 3: The trigger (left) is placed in the training set in a picture of a bird labelled as a cat. A model trained with these triggered examples is likely to classify pictures containing the trigger as cats instead of birds during inference.\n\n<!-- image -->\n\nThe trigger is implanted in a subset of training data. The subset is labelled with the adversary's chosen class. The key idea is that the model learns to connect the trigger with a class determined by the adversary (Figure 3). An attack is successful when a backdoored model behaves normally when encountering benign images but predicts the adversary's chosen label when presented with triggered images [19]. The success rate of backdoor attacks depends on the model's architecture, the number and rate of triggered images, and the trigger patterns chosen by the attacker. A trigger with a high success rate in a model does not necessarily negatively influence the overall model performance on benign inputs. Therefore, backdoored models are hard to detect by inspecting their performance alone [18]. Research shows that backdoor attacks can be successful with only a small number of triggered training samples. In addition, when using pretrained models from public sources, it should be noted that through transfer learning (3.2), risks like built-in backdoors could also be transferred.\n\n## 5.3 Defending against Poisoning and Backdoor Attacks\n\nIt is encouraged to simulate concrete attacks on your system to check the vulnerability to attacks and effectiveness of selected defence mechanisms.\n\n## Use Trusted Sources\n\nDepending on the security requirements for the use case, it is essential to make adequate efforts to ensure that the supply chain and the sources of training data, models, and code are known and trustworthy. Publicly available models could contain backdoors.\n\n## Search for Triggers\n\nThe triggers used for backdoors rely on logical shortcuts between the target class and the input. To find a shortcut, one must determine the minimal input change required to shift the model's prediction. If such a change is minimal, a backdoor may have been found [20]. Another method for the image domain is to randomly mask parts of an input image and examine how the model's prediction changes. If the input image contains a trigger, masking it will change the model's prediction [21].\n\n## Retraining\n\nRetraining a model with benign training samples, if available, reduces the probability of backdoors being successful [18]. The degree of success depends on the size and quality of the clean dataset [22]. Research suggests that even with a small retraining dataset, the vulnerability of a model to backdoor attacks significantly drops, while its accuracy may be slightly reduced [22].\n\n## Network Pruning\n\nFor network pruning, benign data samples are fed into the trained neural network, and their average activation is measured. Neurons without a high level of activation can be trimmed without substantially\n\nreducing the model's accuracy. In the process, potential backdoors can be removed as well. Similar to retraining, the complete success of the measure cannot be guaranteed [20].\n\n## Autoencoder Detection\n\nAn autoencoder is trained with a benign dataset whose feature distribution is close to the training dataset. As a result, the trained autoencoder may be able to detect manipulated data samples that lie outside of the learned distribution [22].\n\n## Regularization\n\nRegularization can lower the success rate of backdoor attacks without significantly degrading the baseline performance on benign inputs [18].\n\n## 6 Limitations\n\nThe introduced defences can help counter attacks on machine learning models but can also adversely affect other aspects of the model. They often require more computational time. Moreover, an increased attack resilience can lower the general performance of the model. It is advisable to balance attack resilience and performance, as well as other relevant aspects, based on the expected risk of the overall AI system. Adaptive attacks on machine learning models might circumvent existing defence methods. However, the named defence methods can increase the attack effort, be it through higher computational costs or a larger attack budget needed.\n\nFor further reading on attacks on machine learning, we refer the reader to the study [3] or other up-to-date publications like [23] and [24].\n\n## Bibliography\n\n- [1]   Bundesamt fÃ¼r Sicherheit in der Informationstechnik, 'IT-Grundschutz-Kompendium,' Bonn, Germany, 2022.\n- [2]   C. Berghoff, 'Protecting the integrity of the training procedure of neural networks,' Bundesamt fÃ¼r Sicherheit in der Informationstechnik, Bonn, Germany, 2020.\n- [3]   Federal Office for Information Security, 'Security of AI-Systems: Fundamentals,' Bonn, Germany, 2022.\n- [4]   I. J. Goodfellow, J. Shlens und C. Szegedy, 'Explaining and Harnessing Adversarial Examples,' in 3rd International Conference on Learning Representations , San Diego, CA, USA, 2015.\n- [5]   J. Chen, M. I. Jordan und M. J. Wainwright, 'HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,' in IEEE Symposium on Security and Privacy , San Francisco, CA, USA, 2020.\n- [6]   D. Stutz, M. Hein und B. Schiele, 'Disentangling Adversarial Robustness and Generalization,' in IEEE Conference on Computer Vision and Pattern Recognition , Long Beach, CA, USA, 2019.\n- [7]   B. Wang, Y. Yao, B. Viswanath, H. Zheng und B. Y. Zhao, 'With Great Training Comes Great Vulnerability: Practical Attacks against Transfer Learning,' in 27th USENIX Security Symposium, USENIX Security , Baltimore, MD, USA, 2018.\n- [8]   R. Shokri, M. Stronati, C. Song und V. Shmatikov, 'Membership Inference Attacks Against Machine Learning Models,' in IEEE Symposium on Security and Privacy , San Jose, CA, USA, 2017.\n- [9]   Y. Zhang, R. Jia, H. Pei, W. Wang, B. Li und D. Song, 'The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks,' in IEEE/CVF: Conference on Computer Vision and Pattern Recognition , Seattle, WA, USA, 2020.\n- [10]   B. Z. H. Zhao, A. Agrawal, C. Coburn, H. J. Asghar, R. Bhaskar, M. A. Kaafar, D. Webb und P. Dickinson, 'On the (In)Feasibility of Attribute Inference Attacks on Machine Learning Models,' in IEEE European Symposium on Security and Privacy, EuroS&amp;P , Vienna, Austria, 2021.\n- [11]   S. Mehnaz, S. V. Dibbo, E. Kabir, N. Li und E. Bertino, 'Are Your Sensitive Attributes Private? Novel Model Inversion Attribute Inference Attacks on Classification Models,' in 31st USENIX Security Symposium , Boston, MA, USA, 2022.\n- [12]   N. Z. Gong und B. Liu, 'Attribute Inference Attacks in Online Social Networks,' ACM Trans. Priv. Secur. 21, pp. 3:1--3:30, 2018.\n- [13]   H. Yin, P. Molchanov, J. M. Alvarez und Z. Li, 'Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion,' in Conference on Computer Vision and Pattern Recognition , Seattle, WA, USA, 2020.\n- [14]   A. Chawla, H. Yin, P. Molchanov und J. Alvarez, 'Data-free Knowledge Distillation for Object Detection,' in Winter Conference on Applications of Computer Vision , Waikoloa, HI, USA, 2021.\n- [15]   M. Fredrikson, S. Jha und T. Ristenpart, 'Model inversion attacks that exploit confidence information and basic countermeasures,' in Proceedings of the 22nd ACM Conference on Computer and Communications Security , Denver, CO, USA, 2015.\n- [16]   T. Dong, B. Zhao und L. Lyu, 'Privacy for Free: How does Dataset Condensation Help Privacy?,' in Proceedings of the 39th International Conference on Machine Learning , Baltimore, MD, USA, 2022.\n\n- [17]   T. Dalenius, 'Towards a Methodology for Statistical Disclosure Control,' Statistik Tidskrift 15, p. 429444, 1977.\n- [18]   L. Truong, C. Jones, B. Hutchinson, A. August, B. Praggastis, R. Jasper, N. Nichols und A. Tuor, 'Systematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers,' in IEEE/CVF Conference on Computer Vision and Pattern Recognition , Seattle, WA, USA, 2020.\n- [19]   X. Chen, C. Liu, B. Li, K. Lu und D. Song, 'Targeted Backdoor Attacks on Deep Learning,' CoRR, 2017.\n- [20]   B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng und B. Y. Zhao, 'Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks,' in IEEE Symposium on Security and Privacy , San Francisco, CA, USA, 2019.\n- [21]   S. Udeshi, S. Peng, G. Woo, L. Loh, L. Rawshan und S. Chattopadhyay, 'Model Agnostic Defence Against Backdoor Attacks in Machine Learning,' in IEEE Transactions on Reliability , 2022.\n- [22]   Y. Liu, Y. Xie und A. Srivastava, 'Neural Trojans,' in IEEE International Conference on Computer Design , Boston, MA, USA, 2017.\n- [23]   NCSA, 'AI systems: develop them securely,' 15 02 2023. [Online]. Available: https://english.aivd.nl/latest/news/2023/02/15/ai-systems-develop-them-securely.\n- [24]   A. Malatras, I. Agrafiotis und M. Adamczyk, 'Securing machine learning algorithms,' ENISA, 2021.", "fetched_at_utc": "2026-02-09T13:38:43Z", "sha256": "62a50d12c3634a10c1dbbd3d843a5fb830da6382fde05d8323a162335a1f67b2", "meta": {"file_name": "AI Security Concerns in a Nutshell.pdf", "file_size": 440346, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-artificial-intelligence-systems-and-the-gdpr-belgium-662050b41ff0", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Artificial Intelligence Systems and the GDPR - Belgium.pdf", "title": "Artificial Intelligence Systems and the GDPR - Belgium", "text": "(Original version -version December 2024)\n\n## Data Protection Authority of Belgium General Secretariat\n\n## Artificial Intelligence Systems and the GDPR A Data Protection Perspective\n\n<!-- image -->\n\n## (Original version -version December 2024)\n\n| EXECUTIVESUMMARY....................................................................................................................3                                                                                                                                                |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| OBJECTIVEOFTHISINFORMATIONBROCHURE....................................................................4                                                                                                                                                                              |\n| AUDIENCEFORTHISINFORMATIONBROCHURE...................................................................5                                                                                                                                                                               |\n| WHATISANAISYSTEM?...............................................................................................................6                                                                                                                                                    |\n| GDPR&AIACTREQUIREMENTS..................................................................................................8                                                                                                                                                            |\n| LAWFUL, FAIR , ANDTRANSPARENTPROCESSING ........................................................................................................8                                                                                                                                    |\n| PURPOSELIMITATIONANDDATAMINIMISATION .........................................................................................................9                                                                                                                                      |\n| DATAACCURACYANDUP-TO-DATENESS.......................................................................................................................9                                                                                                                                |\n| STORAGELIMITATION ............................................................................................................................................................9                                                                                                      |\n| AUTOMATEDDECISION - MAKING ......................................................................................................................................10                                                                                                                  |\n| SECURITYOF PROCESSING .................................................................................................................................................11                                                                                                            |\n| DATA SUBJECT RIGHTS .......................................................................................................................................................13                                                                                                        |\n| ACCOUNTABILITY ..................................................................................................................................................................14                                                                                                  |\n| MAKINGCOMPLIANCESTRAIGHTFORWARD:USERSTORIESFORAISYSTEMSIN                                                                                                                                                                                                                            |\n| LIGHT OFGDPRANDAIACTREQUIREMENTS......................................................................16                                                                                                                                                                             |\n| REQUIREMENTSOFPURPOSELIMITATIONANDDATAMINIMIZATION .................................................................17                                                                                                                                                               |\n| REQUIREMENTSOFDATAACCURACYANDUP-TO-DATENESS ..............................................................................18 REQUIREMENTOFSECUREPROCESSING .......................................................................................................................19 |\n| REQUIREMENTOF ( THEABILITYOFDEMONSTRATING ) ACCOUNTABILITY .......................................................20                                                                                                                                                                 |\n| REFERENCES.....................................................................................................................................21                                                                                                                                    |\n\n## Executive summary\n\nThis  information  brochure  outlines  the  complex  interplay  between  the  General  Data Protection Regulation (GDPR) i  and the Artificial Intelligence (AI) Act ii in the context of AI system development. The document emphasizes the importance of aligning AI systems processing  personal  data  with  data  protection  principles  while  addressing  the  unique challenges posed by AI technologies.\n\nKey points include:\n\n- GDPR and AI Act alignment: the brochure highlights the complementary nature of the GDPR and AI Act in ensuring lawful, fair, and transparent processing of personal data in AI systems.\n- AI system definition: the document provides a clear definition of AI systems and offers illustrative examples to clarify the concept.\n- data protection principles: the brochure delves into core GDPR principles such as lawfulness, fairness, transparency, purpose limitation, data minimization, accuracy, storage limitation, and data subject rights in the context of AI systems.\n- accountability:  the  importance  of  accountability  is  emphasized,  with  specific requirements outlined for both the GDPR and AI Act.\n- security: the document highlights the need for robust technical and organizational measures, to protect personal data processed by AI systems.\n- human oversight: The crucial role of human oversight in AI system development and operation is emphasized, particularly for high-risk AI systems.\n\nBy  providing  insights  into  the  legal  framework  and  practical  guidance,  this  information brochure  aims  to  empower  legal  professionals,  data  protection  officers,  technical stakeholders,  including  controllers  and  processors,  to  understand  and  comply  with  the GDPR and AI Act requirements when developing and deploying AI systems.\n\n## Objective of this information brochure\n\nThe  General  Secretariat  of  the  Belgian  Data  Protection  Authority  monitors  social, economic, and technological developments that impact the protection of personal data iii .\n\nIn  recent  years,  AI  technologies  have  experienced  exponential  growth,  revolutionizing various industries  and significantly  impacting  the  way  data  is  collected,  processed,  and utilized.  However,  this  rapid  advancement  has  brought  about  complex  challenges regarding data privacy, transparency, and accountability.\n\nIn this context, the General Secretariat of the Belgian Data Protection Authority publishes this information brochure to provide insights on data protection and the development and implementation of AI systems.\n\nUnderstanding and adhering to the GDPR principles and provisions is crucial for ensuring that AI systems operate ethically, responsibly, and in compliance with legal standards. This information brochure aims to elucidate the GDPR requirements specifically applicable to AI  systems  that  process  personal  data,  offering  more  clarity  and  useful  insights  to stakeholders involved in the development, implementation, and (internal) regulation of AI technologies.\n\nIn addition to the GDPR, the Artificial Intelligence Act (AI Act), which entered into force on 1st of August 2024, will also significantly impact the regulation of AI system development and use. This information brochure will also address the requirements of the AI Act.\n\nThe  examples  included  in  this  brochure  serve  a  purely  pedagogical  purpose,  they  are sometimes hypothetical and do not take into account certain exceptions iv and imperfections v  in the regulation.\n\n## Audience for this information brochure\n\nThis information brochure is intended for a diverse audience comprising legal professionals, Data Protection Officers (DPOs), and individuals with technical backgrounds including  business  analysts,  architects,  and  developers.  It  also  targets  controllers  and processors  involved  in  the  development  and  deployment  of  AI  systems.  Given  the intersection of legal and technical considerations inherent in the application of the GDPR to  AI  systems,  this  information  brochure  seeks  to  bridge  the  gap  between  legal requirements and technical implementation.\n\nLegal professionals and DPOs play a crucial role in ensuring organizational compliance with GDPR obligations, specifically those relevant to AI systems that process personal data. By providing insights into GDPR requirements specific to AI, this information brochure equips legal professionals and DPOs with useful knowledge to navigate the complexities of AIrelated data processing activities, assess risks, and implement appropriate measures.\n\nAt  the  same  time,  individuals  with  technical  backgrounds  such  as  business  analysts, architects, and developers are integral to the design, development, and deployment of AI systems. Recognizing their pivotal role, this information brochure aims to elucidate GDPR requirements in a manner accessible to technical stakeholders.\n\nConcrete  examples  are  incorporated  into  the  text  to  illustrate  how  GDPR  principles translate  into  practical  considerations  during  the  lifecycle  of  AI  projects.  By  offering relatively simple and actionable insights, this information brochure empowers professionals  with  various  backgrounds  to  design  AI  systems  that  are  compliant  with GDPR obligations, embed data protection-by-design principles, and mitigate potential legal and ethical risks.\n\n## What is an AI system ?\n\nThe term \"AI system\" encompasses a wide range of interpretations.\n\nThis information brochure will not delve into the intricacies and nuances that distinguish these various definitions.\n\nInstead, we will begin by examining the definition of an AI system as outlined in the AI Act vi :\n\nFor the purposes of this Regulation, the following definitions apply: such as predictions, content, recommendations, or decisions that can influence physical\n\n(1) 'AI system' means a machine -based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs or virtual environments;\n\n## In other terms :\n\nAn AI system is a computer system specifically designed to analyze data, identify patterns, and use that knowledge to make informed decisions or predictions.\n\nIn some cases, AI systems can learn from data and adapt over time. This learning capability allows them to improve their performance, identify complex patterns across different data sets, and make more accurate or nuanced decisions.\n\n## Examples of AI systems in everyday life:\n\nSpam  filters  in  email:  spam  filters  analyze  incoming  emails  and  identify  patterns  that distinguish spam messages from legitimate emails. Over time, as people mark emails as spam or not spam, the AI system can learn and improve its filtering accuracy. This is an example of an AI system that meets the criteria of an AI system :\n\n- machine-based system: it's a computer program.\n- analyzes data: it analyzes the content of emails.\n- identifies patterns: it identifies patterns in emails that suggest spam.\n- makes decisions: it decides whether to categorize an email as spam or not.\n\nRecommendation  systems  on  streaming  services:  movie  streaming  services  utilize  AI systems to generate recommendations for users. These systems analyze a user's past viewing habits, along with the habits of similar users, to recommend content they might be interested in. This is another example of an AI system :\n\n- machine-based system: it's a computer program.\n- analyzes data: it analyzes a user's viewing/listening history.\n- identifies patterns: it identifies patterns in user preferences and those of similar users.\n- makes recommendations: it recommends content based on the identified patterns.\n\nVirtual assistants: virtual assistants respond to voice commands and complete tasks like setting  alarms,  playing  music,  or  controlling  smart  home  devices.  These  systems  use speech recognition and natural language processing to understand user requests and take action. This is again an example of an AI system :\n\n- machine-based system: it's a computer program.\n- analyzes data: it analyzes user voice commands.\n- identifies patterns: it identifies patterns in speech to understand user requests.\n- makes  decisions: it decides how  to respond  to  commands  based  on  its understanding.\n- may exhibit adaptiveness: some virtual assistants can learn user preferences and adapt their responses over time.\n\nAI-powered medical imaging analysis: many hospitals and healthcare providers are utilizing AI systems to assist doctors in analyzing medical images, such as X-rays, CT scans, and MRIs. These systems are trained on vast datasets of labeled medical images, allowing them to identify patterns and potential abnormalities.\n\n- machine-based system: it's a computer program.\n- analyzes data: it analyzes the digital medical images.\n- identifies  patterns:  it  identifies  patterns  in  the  images  that  might  indicate  the presence of a disease or abnormality.\n- supports decision-making: the system highlights potential areas of concern in the images, which can help doctors make more informed diagnoses.\n\n## GDPR &amp; AI Act requirements\n\n## Lawful, fair, and transparent processing\n\nThe GDPR requires lawfulness, fairness and transparency.\n\nLeveraging GDPR lawfulness of processing :  The  GDPR establishes six legal bases for processing personal data in Article 6 (consent, contract, legal obligation, vital interests, public interest, and legitimate interests vii ). These same legal bases remain applicable for AI systems that process personal data under the AI Act.\n\nProhibited AI Systems : the AI Act introduces additional prohibitions beyond the GDPR for certain high-risk AI systems. While the GDPR focuses on protecting personal data through various principles, the AI Act directly prohibits specific types of high-risk AI applications. Here are some examples:\n\n- Social  scoring  systems:  these  systems  assign  a  score  to  individuals  based  on various factors, potentially leading to discrimination and limitations on opportunities.\n- AI  systems  for  real  time  remote  biometric  identification  for  the  purpose  of  law enforcement  in  public  places  (with  limited  exceptions):  these  systems  raise concerns  about  privacy,  freedom  of  movement,  and  potential  misuse  for  mass surveillance.\n\n## Fairness:\n\n- While the AI Act doesn't have a dedicated section titled 'fairness', it builds upon the  GDPR's  principle  of  fair  processing  (art.  5.1.a)  as  the  AI  Act  focuses  on mitigating bias and discrimination in the development, deployment, and use of AI systems.\n\n## Transparency:\n\n- the AI Act requires a baseline level of transparency for certain AI systems. This means users should be informed that they're interacting with an AI system. For instance,  a  chatbot  could  begin  an  interaction  with  a  message  like  \"Hello,  I  am Nelson, a chatbot . How can I assist you today?\"\n- the  AI  Act  requires  a  higher  transparency  level  for  high-risk  AI  systems.  This includes providing clear and accessible information about how data is used in these systems, particularly regarding the decision-making process. Users should understand the factors influencing AI-based decisions and how potential bias is mitigated.\n\n## Purpose limitation and data minimisation\n\nThe  GDPR requires purpose limitation (art.  5.1.b)  and  data  minimisation  (art.  5.1.c).  This means personal data must be collected for specific and legitimate purposes, and limited to what is necessary for those purposes. These principles ensure that AI systems don't use personal data for purposes beyond their intended function or collect excessive data. The AI Act strengthens the principle of purpose limitation -from the GDPR -for high-risk AI systems by emphasizing the need for a well-defined and documented intended purpose.\n\nHypothetical example :  A loan approval AI system of a financial institution, in addition to standard identification data and credit bureau information, also utilizes geolocation data (e.g., past locations visited) and social media data (e.g., friends' profiles and interests) of a data subject. This extensive data collection, including geolocation and social media data, raises concerns about the system's compliance with the GDPR.\n\n## Data accuracy and up-to-dateness\n\nThe GDPR requires personal data to be accurate and, where necessary, kept up-to-date (art. 5.1.d).  Organizations must take reasonable steps to ensure this. The AI Act builds upon this principle by requiring high-risk AI systems to use high-quality and unbiased data to prevent discriminatory outcomes.\n\nHypothetical  example :  a  financial  institution  develops  an  AI  system  to  automate  loan approvals. The system analyzes various data points about loan applicants, including credit history,  income,  and  demographics  (postal  code).  However,  the  training  data  for  the  AI system unknowingly reflects historical biases : the data stems from a period when loans were more readily granted in wealthier neighborhoods (with a higher average income). The AI system perpetuates these biases as loan applicants from lower-income neighborhoods might be systematically denied loans, even if they are financially qualified. This results in a discriminatory outcome, and might raise serious concerns about the system's compliance with the AI Act.\n\n## Storage limitation\n\nThe GDPR requires personal data to be stored only for as long as necessary to achieve the purposes  for  which  it  was  collected  (art.  5.1.e).  The  AI  Act  doesn't  explicitly  introduce another or an extra requirement on storage limitation for high-risk AI systems.\n\n## Automated decision-making\n\nThe GDPR and the AI Act both address the importance of human involvement in automated decision-making processes that impact individuals. However, they differ in their focus:\n\n- The  GDPR  grants  individuals  the  right  not  to  be  subject  solely  to  automated processing for decisions that produce legal effects concerning them (art. 22). This means data subjects have the right to request a reconsideration of an automated decision  by  a  human  decision-maker.  This  functions  as  an  individual  right  to challenge decisions perceived as unfair or inaccurate.\n- The AI Act strengthens the focus on human involvement by requiring meaningful human oversight throughout the development, deployment, and use of high-risk AI systems. This acts as a governance measure to ensure responsible AI development and  use.  Human  oversight  under  the  AI  Act  encompasses  a  broader  range  of activities than just reconsideration of individual decisions. It includes, for example, reviewing  the  AI  system's  training  data  and  algorithms  for  potential  biases, monitoring the system's performance, and intervening in critical decision-making pathways.\n\nIn essence, the GDPR empowers individuals to object to solely automated decisions, while the  AI  Act  requires  proactive  human  oversight  for  high-risk  AI  systems  to  safeguard against potential biases and ensure responsible development and use of such systems.\n\nHypothetical example :  a  government agency uses an AI system to assess eligibility for social welfare benefits based on income, employment status, and family situation.\n\nFollowing  the  GDPR,  individuals  have  the  right  not  to  be  subject  solely  to  automated processing for social welfare benefits eligibility (art. 22). This means they can request  a reconsideration of an automated decision by a human decision-maker.\n\nFollowing  the  AI  Act,  this  AI  system  is  classified  as  an  high-risk  system  (as  it  has  a significant  impact  on  individuals'  livelihoods).  This  requires  the  government  agency  to implement human oversight throughout the development, deployment, and use of the AI system.\n\n## Security of Processing\n\nBoth  the  GDPR  and  the  AI  Act  emphasize  the  importance  of  securing  personal  data throughout  its  processing  lifecycle.  However,  AI  systems  introduce  specific  risks  that require additional security measures beyond traditional data protection practices.\n\nThe  GDPR  requires  organizations  to  implement  technical  and  organizational  measures (TOMs) that are appropriate to the risk associated with their data processing activities. This involves conducting risk assessments to identify potential threats and vulnerabilities. The selected  TOMs  should  mitigate  these  risks  and  ensure  a  baseline  level  of  security  for personal data.\n\nThe AI Act builds upon this foundation by mandating robust security measures for highrisk  AI  systems.  This  is  because  AI  systems  introduce  specific  risks  that  go  beyond traditional data processing, such as:\n\n- potential bias in training data: biased training data can lead to biased decisions by the AI system, impacting individuals unfairly.\n- manipulation by unauthorized individuals: for example, a hacker could potentially manipulate the AI system's training data to influence its decisions in a harmful way. Imagine a system trained to approve loan applications being tricked into rejecting qualified applicants based on irrelevant factors.\n\nTo address these unique risks, the AI Act emphasizes proactive measures such as:\n\n- identifying and planning for potential problems: This involves brainstorming what could go wrong with the AI system and how likely it is to happen (risk assessment). This is a core practice under both the GDPR and AI Act.\n- continuous  monitoring  and  testing:  This  involves  regularly  evaluating  the  AI system's performance for several aspects including:\n- o security flaws: identifying vulnerabilities in the system's code or design that could be exploited by attackers.\n- o bias: checking for potential biases in the system's training data or decisionmaking processes.\n- human  oversight:  the  AI  Act  emphasizes  the  importance  of  meaningful  human oversight  throughout  the  development,  deployment,  and  use  of  high-risk  AI systems. This ensures that humans  are  involved in critical decisions and\n\nunderstand the system's vulnerabilities. Human oversight under the AI Act goes beyond just security processes and encompasses various aspects, such as:\n\n- o reviewing training data and algorithms for potential biases.\n- o monitoring the system's performance for fairness, accuracy, and potential unintended behaviour.\n- o intervening  in  critical  decision-making  pathways,  especially  when  they could significantly impact individuals.\n\nExample : AI-powered Lung Cancer Diagnosis System.\n\nAn AI system used by a hospital to diagnose lung cancer exemplifies a high-risk AI system due to several factors:\n\n- highly sensitive data: it processes highly sensitive personal data, including patients' health information (lungs) and diagnoses (special category data under article 9 of the GDPR) ;\n- data breach impact: a data breach could expose critical health information about patients,  potentially  leading  to  privacy  violations  and  reputational  harm  for  the hospital ;\n- life-altering  decisions:  the  system's  output  directly  impacts  patients'  lives.  A diagnosis based on inaccurate or compromised  data could have serious consequences for their health and well-being.\n\nBoth the GDPR and the AI Act emphasize the importance of security measures for data processing activities, especially those involving sensitive data.\n\n- the GDPR establishes a foundation for data security: It requires organizations to implement appropriate technical and organizational measures (TOMs) to protect personal data based on a risk assessment. For health data, these measures would be particularly strong due to its sensitive nature. Examples under the GDPR could include:\n- o data encryption: encrypting patient data at rest and in transit ensures its confidentiality even if a breach occurs ;\n- o access controls: implementing strict access controls limits who can access and modify patient data ;\n- o penetration testing: regularly conducting penetration tests helps identify and address vulnerabilities in the system's security posture ;\n- o logging and auditing: maintaining detailed logs of system activity allows for monitoring and investigation of any suspicious behavior.\n- The AI Act builds upon this foundation for high-risk AI systems: recognizing the specific risks of AI, the AI Act mandates robust security measures.  These might\n\ninclude additional measures tailored to the specific vulnerabilities of the AI system, such  as  data  validation  and  quality  assurance  :  the  AI  Act  emphasizes  the importance  of  ensuring  the  quality  and  integrity  of  the  data  used  to  train  and operate the AI system. This could involve techniques for:\n\n- o data provenance: tracking the origin of data to identify potential sources of bias or manipulation in the training data, such as incorrect X-ray labeling.\n- o anomaly detection: identifying and flagging unusual patterns in the training data that might indicate malicious tampering, such as a sudden influx of Xrays with unrealistic characteristics.\n- o human  review  of  high-risk  data  points:  Having  healthcare  professionals review critical X-rays before they are used to train the AI system, especially those  that  show  unusual  features  or  could  significantly  impact  patient outcomes.\n\nBy implementing these security measures the hospital can mitigate the risks associated with  the  AI-powered  lung  cancer  diagnosis  system  and  ensure  patient  privacy,  data security, and ultimately, the best possible patient outcomes.\n\n## Data Subject Rights\n\nThe GDPR grants natural persons data subject rights, empowering them to control their personal  data  and  how  it's  used.  These  rights  include  access  (seeing  what  data  is processed, art. 15), rectification (correcting inaccurate data and completing data, art. 16), erasure (requesting data deletion, art. 17), restriction of processing (limiting how data is used, art. 18), and data portability (transferring data to another service, art. 20).\n\nTo effectively exercise these rights, natural persons need to understand how their data is being  processed.  The  AI  Act  reinforces  this  by  emphasizing  the  importance  of  clear explanations  about  how  data  is  used  in  certain  AI  systems.  With  this  transparency, individuals  can  make  informed  decisions  about  their  data  and  utilize  their  data  subject rights more effectively.\n\nExample : an AI system used to determine  premiums for life insurance assigns a relatively high premium to a particular customer (data subject). The AI Act entitles this customer to a  clear  explanation  of  how  their  premium  is  calculated.  For  example,  the  insurer  (data controller)  could  explain  that  various  data  points  were  used,  such  as  medical  problems customers have faced in the past. This information, in turn, allows the customer to exercise their data subject rights under the GDPR, such as the right to rectification (correction of inaccurate personal data or completion of personal data).\n\n## Accountability\n\nThe  GDPR  requires  (organizations  to  demonstrate)  accountability  for  personal  data processing through several measures, such as :\n\n- Transparent processing: individuals must understand how their data is collected, used, stored and shared (f.e. by a clear and concise data protection statement, by data subject access rights , â€¦) . This transparency allows them to see if their data is being handled lawfully and fairly ;\n- Policies and procedures for handling personal data: documented policies ensure consistent data handling practices across the organization ;\n- Documented  legal  basis  for  processing:  for  each  data  processing activity, organizations need documented proof of the lawful justification (consent, contract, legitimate interest, etc.) ;\n- Keeping different records (like the Register Of Processing Activities (ROPA), data subject requests, data breaches)  is required: maintaining accurate  records demonstrates a commitment to accountability and allows organizations to prove compliance during audits or investigations ;\n- Security measures: implementing and correctly maintaining appropriate technical and  organizational  measures  (TOMs)  to  protect  personal  data  is  crucial  for demonstrating accountability ;\n- A Data Protection Impact Assessment (DPIAs) is required in some cases: these are mandatory when processing high-risk data or implementing new technologies ;\n- A  Data  Protection  Officer  (DPO)  is  required  in  some  cases:  f.e.  governmental organizations, regardless of their core activities, are required to have a DPO.\n\nWhile  the  AI  Act  doesn't  have  a  dedicated  section  on  demonstrating  accountability,  it builds upon the GDPR's principles. The AI Act requires organizations to implement :\n\n- a  two-step  risk  management  approach  for  AI  systems.  First,  there's  an  initial classification process that categorizes the risk the AI poses to individuals (ranging from minimal to high).\n\nFor high-risk systems, a more in-depth risk assessment is required in some cases. This dives deeper into the specific risks and identifies potential harms associated with  the  AI  system,  and  is  also  called  a  FRIA  (Fundamental  Rights  Impact Assessment) ;\n\n## (Original version -version December 2024)\n\n- clear documentation of the design and implementation of AI systems ;\n- processes dealing with human oversight in high-risk AI systems. This could involve human intervention or approval for critical decisions made by the AI system ;\n- a  formal  incident reporting  process for reporting incidents related to AI system malfunctions or unintended behaviour.\n\n## Making compliance straightforward: user stories for AI systems in light of GDPR and AI Act requirements\n\nTranslating regulatory requirements into technical specifications for AI systems presents significant  challenges.  This  document  focuses  on  using  user  stories  to  bridge  the  gap between legal obligations and system development.\n\nUser  stories  offer  a  practical  approach  to  understanding  and  addressing  regulatory requirements in the context of AI system design. By adopting a user-centric perspective, organizations can effectively translate legal obligations into actionable steps.\n\nThis document uses a life insurance premium calculation system as an example to illustrate the application of user stories in the AI domain.\n\n## Requirements of lawful, fair, and transparent processing\n\n## User story: ensuring lawfulness -correct legal basis\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we need to conduct a thorough legal basis assessment to determine the most appropriate legal justification for collecting and using customer data in our AI system. This is important to comply with the GDPR principle of lawfulness.\n\n## User story: e nsuring lawfulness - prohibited data\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we need to ensure our system complies with the GDPR and AI Act prohibitions on processing certain types of personal data. This includes special categories of personal data such as racial or ethnic origin, political opinions, religious beliefs,  health, etc. This is important to comply with the GDPR's protection of sensitive personal data and the AI Act's emphasis on preventing discriminatory outcomes.\n\n## User story: ensuring fairness\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we need to ensure fair and non-discriminatory processing of customer data. This is important to comply with the GDPR principle of fairness and the specific AI Act's focus on preventing biased outcomes that could disadvantage certain groups.\n\nThe life insurance company can achieve fairness by:\n\n- data source review: analyze the data sources used to train the AI system to identify and mitigate potential biases based on factors like  postal code, gender, age ,  â€¦ . Ensure these factors are used in a way that is relevant and necessary for premium calculations, avoiding any discriminatory outcomes.\n- fairness testing: regularly test the AI system for potential biases in its outputs. This might involve comparing life insurance premium calculations for similar customer profiles to identify any unexplainable disparities.\n- human oversight: implement a human review process for high-impact decisions made by the AI system, such as significant  life insurance premium increases or even policy denials.\n\n## User story: ensuring transparency\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums,  we  need  to  be  transparent  about  how  our  customers'  data  is  used.    This  is important to comply with the general GDPR principle of transparency and the specific AI Act's focus on transparency for high -risk AI systems.\n\nThe life insurance company can achieve transparency by :\n\n- a  data  protection  statement  :  clearly  explain  in  the  company's  data  protection statement how customer data is collected, used, and stored in the AI system for premium calculations.\n- easy-to-understand explanations : provide customer-friendly explanations of the AI premium calculations process. This could involve using simple language, visuals, or FAQs to demystify the AI's role in determining life insurance premiums.\n- right to access information : implement mechanisms for customers to easily access information about the data points used in their specific premium calculations.\n\n## Requirements of purpose limitation and data minimization\n\n## User story : ensuring purpose limitation\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we need to ensure that the data we collect from our customers is limited to what is strictly necessary for the accurate premium calculations. This is important to comply with the principle of purpose limitation under the GDPR.\n\n## User story : ensuring data minimization\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we need to implement a data minimization strategy to ensure we only collect and  use  the  minimum  amount  of  customer  data  necessary  for  the  accurate  premium calculations. This is important to comply with the principle of data minimization under the GDPR.\n\n## Requirements of data accuracy and up-to-dateness\n\n## User story : ensuring data accuracy and up-to-dateness\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we need to implement processes to ensure the accuracy and up-to-dateness of customer data used in the system. This is important to comply with the principle of data accuracy under the GDPR.\n\nThe life insurance company can achieve accuracy and up-to-dateness of customer data by:\n\n- data verification mechanisms:  offer customers easy-to-use mechanisms to verify and  update  their  personal  data  within  the  life  insurance  system.  This  could  be through an online portal, mobile app, or dedicated phone line.\n- regular data refresh: establish procedures for regularly refreshing customer data used in the AI system. This might involve requesting customers to update their information periodically or integrating with external data sources to automatically update relevant data points.\n- data  quality  alerts:  implement  alerts  for  missing  or  potentially  inaccurate  data points in customer profiles. This allows the company to proactively reach out to customers and request updates.\n- clearly communicate to customers their right to rectification under the GDPR. This right  allows  them  to  request  corrections  of  any  inaccurate  personal  data  or completion of missing data used in the premium calculations system.\n\n## User story : ensuring use of unbiased data\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we need to ensure that the data used to train and operate the system is of free from bias. This is important to comply with the specific AI Act's focus on preventing biased outcomes that could disadvantage certain groups.\n\nThe life insurance company can achieve unbiased data for fair AI premium calculations by :\n\n- data source evaluation:  Analyze the sources of data used to train the AI system. Identify potential biases based on factors like socioeconomic background in the data collection process.\n- regular  monitoring  and  bias  testing:    Continuously  monitor  the  AI  system's performance for potential biases in its  outputs.  Conduct  regular  bias  testing  to identify and address any discriminatory outcomes in premium calculations.\n- human oversight: implement a human review process for high-impact decisions made by the AI system, such as significant  life insurance premium increases or even policy denials. This allows human intervention to prevent biased out comes.\n- transparency with customers:  Inform customers in the data protection statement about the company's commitment to using high-quality, unbiased data in the AI system.\n\n## Requirement of secure processing\n\n## User story : implementing appropriate security measures for life insurance AI\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we need to conduct a thorough risk assessment to identify potential threats and vulnerabilities that could impact our customer data. This assessment will consider various factors, including the type of data (health data vs. basic customer information), processing activities,  and  potential  impact  of  a  security  breach.  Based  on  this  assessment,  we  will implement appropriate technical and organizational measures (TOMs) to mitigate these risks and ensure the security of our customer data. This is important to comply with the requirement of security of the processing under the GDPR.\n\nExamples of TOMs may include:\n\n- data  encryption:  encrypting  customer  data  at  rest  and  in  transit  to  protect confidentiality ;\n- access controls: implementing strict access controls to limit who can access and modify customer data ;\n- regular penetration testing: conducting penetration tests to identify and address vulnerabilities in the system's security posture ;\n- logging and auditing: maintaining detailed logs of system activity for monitoring and investigation of any suspicious behavior.\n\n## User story : implementing specific security measures for life insurance AI\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we recognize that AI systems introduce specific risks beyond traditional data processing.  These  risks might  include  potential  bias  in  training  data  or  manipulation  by unauthorized  actors.  To  address  these  specific  risks  we  will  implement  additional measures in conjunction with the baseline  GDPR-compliant TOMs. This is important to comply with the requirement of security of the processing under the AI Act.\n\nExamples of these additional measures may include:\n\n- data  validation  and  quality  assurance:  implementing  processes  to  ensure  the quality and integrity of the data used to train and operate the AI system. This could involve  data  provenance  tracking  and  anomaly  detection  to  identify  potential biases or manipulation attempts.\n- human oversight: establishing a framework for human oversight throughout the AI system's  lifecycle.  This  could  involve  human  review  of  high-risk  data  points, monitoring the system's performance for fairness and accuracy, and intervening in critical decision-making pathways.\n\n## Requirement of (the ability of demonstrating) accountability\n\n## User story : documenting the legal basis\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we need to have a clear and concise record of the legal basis for collecting and using customer data in the AI system.  This is important to comply with the GDPR principle of (demonstrating) accountability (also in the context of audits or investigations).\n\n## User story : conducting a Fundamental Rights Impact Assessment (FRIA)\n\nAs  a  life  insurance  company  implementing  an  AI  system  to  calculate  life  insurance premiums, we need to develop and maintain a comprehensive FRIA (Fundamental Rights Impact Assessment) to proactively identify and mitigate potential risks associated with this AI system. This is important to comply with the AI Act's requirements for high-risk AI systems and promote fair and non-discriminatory premium calculations for our customers. This obligation is in addition to the GDPR rules on data protection impact assessment.\n\n*     *     *\n\n## References\n\n0  This paper also utilized spelling and grammar checking, and a large language model, as a tool for refining and correcting initial text sections.\n\ni  Regulation (EU)2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the  free  movement  of  such  data,  and  repealing  Directive  95/46/EC  (General  Data Protection Regulation), Official Journal of the European Union L 119/1, 4.5.2016, p. 1 -88.\n\nii  Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying  down  harmonised  rules  on  artificial  intelligence  (Artificial  Intelligence  Act), Official Journal of the European Union L 199/1, 12.7.2024, p. 1 -120.\n\niii Art. 20, Â§1, 1Â°, Data Protection Authority Act of 3 December 2017, amended by the Act of 25 December 2023.\n\niv  In  the  examples  discussing  high-risk  AI  systems  listed  in  Annex  III  of  the  AI  act,  the possible exceptions referred to in section 6.3 of the AI act are not taken into account.\n\nv In the examples addressing life insurance, possible gaps in the legal basis (see decision 109/2024 of the Litigation Chamber of the Belgian DPA) are not taken into account.\n\nvi Artificial Intelligence Act, Article 3 (1)\n\nvii For more information on the legal basis 'legitimate interest', see the following opinion of the European Data Protection Board: Opinion 28/2024 on certain data protection aspects related to the processing of personal data in the context of AI models (https://www.edpb.europa.eu/news/news/2024/edpb-opinion-ai-models-gdpr-principlessupport-responsible-ai\\_en)", "fetched_at_utc": "2026-02-09T13:39:21Z", "sha256": "662050b41ff0c5e8450d015a5784366a42a37b375a0994c8e99b1c924f395675", "meta": {"file_name": "Artificial Intelligence Systems and the GDPR - Belgium.pdf", "file_size": 489694, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-assessing-high-risk-ai-systems-under-the-eu-ai-act-from-legal-requirements--a2b99c97c526", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Assessing High-Risk AI Systems under the EU AI Act - From Legal Requirements to Technical Verification.pdf", "title": "Assessing High-Risk AI Systems under the EU AI Act - From Legal Requirements to Technical Verification", "text": "## Assessing High-Risk AI Systems under the EU AI Act: From Legal Requirements to Technical Verification\n\nALESSIO BUSCEMI 1 , Luxembourg Institute of Science and Technology (LIST), Luxembourg TOM DECKENBRUNNEN 1 , LIST, University of Luxembourg, Luxembourg FAHRIA KABIR 2 , KATERYNA MISHCHENKO 2 , and NISHAT MOWLA 2 , Research Institutes of Sweden\n\n(RISE), Sweden\n\nThe implementation of the AI Act requires practical mechanisms to verify compliance with legal obligations, yet concrete and operational mappings from high-level requirements to verifiable assessment activities remain limited, contributing to uneven readiness across Member States. This paper presents a structured mapping that translates high-level AI Act requirements into concrete, implementable verification activities applicable across the AI lifecycle. The mapping is derived through a systematic process in which legal requirements are decomposed into operational sub-requirements and grounded in authoritative standards and recognised practices. From this basis, verification activities are identified and characterised along two dimensions: the type of verification performed and the lifecycle target to which it applies. By making explicit the link between regulatory intent and technical and organisational assurance practices, the proposed mapping reduces interpretive uncertainty and provides a reusable reference for consistent, technology-agnostic compliance verification under the AI Act.\n\n## 1 Introduction\n\nProgress in AI has created substantial opportunities while also introducing significant technical, organisational, and regulatory challenges, particularly as AI systems are increasingly deployed in critical domains where failures may have serious societal consequences [19, 50]. In response, regulatory efforts worldwide are intensifying to promote trustworthiness in AI development and deployment [33, 41, 46]. Within this context, the EU Artificial Intelligence Act (AI Act) [13] represents one of the most comprehensive regulatory responses to date, establishing a risk-based framework that mandates ex ante verification and ongoing oversight for high-risk AI systems. Despite this progress, a persistent gap remains between high-level legal requirements and the concrete verification activities needed to demonstrate compliance in practice. This gap reflects the absence of explicit and shared mappings between regulatory obligations and implementable assurance practices, resulting in fragmented compliance efforts across stakeholders and Member States and limiting comparability and regulatory learning. In practice, organisations often struggle to translate abstract legal obligations into verifiable controls, tests, and documentation artefacts that can be applied consistently across the AI lifecycle.\n\nThese challenges are compounded by three interrelated forms of uncertainty. Interpretive uncertainty concerns the contextual meaning and scope of legal obligations; operational uncertainty relates to how such obligations can be instantiated across heterogeneous development and deployment practices; and procedural uncertainty arises from evolving standards, guidance documents, and the gradual designation of notified bodies [17, 24, 25, 38, 42, 48]. These uncertainties are further amplified by the emergent behaviour of modern generative AI systems, whose properties arise from complex interactions rather than explicit programming [32, 36]. As a horizontal regulation relying on harmonised standards, post-market monitoring, and iterative guidance, the AI Act embodies a model of regulatory learning in which compliance expectations are progressively refined through implementation experience and verification outcomes\n\nAuthors' Contact Information: Alessio Buscemi 1 , Luxembourg Institute of Science and Technology (LIST), Luxembourg; Tom Deckenbrunnen 1 , LIST, University of Luxembourg, Luxembourg; Fahria Kabir 2 ; Kateryna Mishchenko 2 ; Nishat Mowla 2 , Research Institutes of Sweden (RISE), Sweden, 1{name}. {surname}@list.lu, 2{name}.{surname}@ri.se.\n\nManuscript submitted to ACM\n\n[38, 43, 52]. Such learning depends on the availability of structured, comparable, and traceable verification evidence, which remains fragmented across actors, disciplines, and use cases.\n\nThis paper addresses the need for an explicit and operational mapping between AI Act requirements and concrete verification activities that can be executed, documented, and compared across the AI lifecycle. The objective is to operationalise compliance by making the link between regulatory intent and technical and organisational assurance practices explicit, reproducible, and auditable. The focus is on high-risk AI systems as defined in Article 6 and Annex III, which concern application domains where failures may adversely affect health, safety, or fundamental rights. Compliance must be demonstrated through conformity verification procedures under Articles 43 and 44, either via internal control procedures (Annex VI) or third-party assessment by notified bodies (Annex VII), both requiring verifiable evidence spanning design, development, deployment, and post-market monitoring.\n\nTranslating these obligations into concrete assessment activities remains challenging. Many provisions of the AI Act are inherently context-dependent, requiring case-specific interpretation grounded in the system's intended use, operational environment, and evolving risk profile. Requirements related to data governance, robustness, accuracy etc. encompass multiple dimensions, each necessitating distinct validation and testing methodologies, while the lifecycleoriented nature of the Act entails continuous monitoring and post-deployment evaluation rather than reliance on point-in-time certification alone. Existing initiatives provide partial foundations for AI verification, but none offers a complete and operational mapping between legal requirements and verification activities. The NIST AI Risk Management Framework [40] provides high-level guidance but is not aligned with EU-specific legal obligations, while ISO/IEC standards such as ISO/IEC 42001 [34] establish process-oriented management requirements without specifying AI Act-specific verification procedures. European standardisation efforts within CEN-CENELEC JTC 21 [21] aim to develop harmonised standards granting presumption of conformity under Article 40, but this work remains ongoing.\n\nAgainst this background, the central contribution of this paper is a structured and operational mapping that translates high-level AI Act requirements into concrete verification activities applicable across the AI lifecycle. The mapping is constructed through a systematic decomposition of legal obligations into operational sub-requirements and is grounded in authoritative standards and recognised engineering and governance practices. To ensure consistency and comparability, verification activities are characterised along two dimensions capturing both verification type and lifecycle target. The applicability of the mapping is illustrated through a real-world case study involving a high-risk AI system in the automotive domain. Within this scope, the paper addresses three guiding questions that structure the construction of the proposed mapping: (i) how high-level legal obligations can be decomposed into concrete verification activities; (ii) which verification dimensions are required to ensure comparability and traceability across heterogeneous actors; and (iii) how a shared verification structure can support communication and regulatory learning across use cases and sectors. While related efforts exist (e.g. [30]), the proposed mapping provides a higher level of granularity and operationalisability. It is not intended to be prescriptive or exhaustive, but to serve as a reusable reference that can evolve alongside regulatory guidance, standardisation efforts, and advances in evaluation techniques, thereby supporting more consistent conformity verification and evidence-based regulatory learning over time.\n\n## 2 Methodology\n\nThis section describes the methodology used to construct a structured mapping between high-level AI Act requirements and concrete, implementable verification activities. Figure 1 provides an overview of the resulting compliance verification structure and the main methodological building blocks. It illustrates how high-level legal requirements are progressively\n\ndecomposed, grounded, and translated into concrete verification activities, which are then situated within a structured verification space defined along two core dimensions.\n\nFig. 1. Overview of the methodology.\n\n<!-- image -->\n\n## 2.1 Normative inputs and requirement structuring\n\nThe starting point of the methodology consists in identifying and structuring a set of high-level requirements derived from the obligations established by the AI Act. Given the heterogeneity and dispersion of these obligations across the regulation, an explicit structuring step is required to organise them into a coherent form suitable for systematic verification. For this purpose, we use the seven principles of Trustworthy AI defined by the European Commission's High-Level Expert Group in the Ethics Guidelines for Trustworthy AI [31] as an organising abstraction. Although these principles predate the AI Act and are not legally binding, they have strongly influenced the development of European AI governance and are reflected throughout the regulation, both as explicit obligations (e.g. transparency, human oversight) and as distributed requirements embedded in provisions on governance, documentation, and system performance. Their role in the methodology is therefore structural: they serve as a stable taxonomy for grouping and analysing legally binding requirements, not as an independent source of obligations.\n\nTo account for the more prescriptive and organisational character of the AI Act, this abstraction is complemented with four additional requirement categories corresponding to obligations that are central to conformity assessment for high-risk systems: quality management, risk management, technical documentation, and record keeping. These categories capture lifecycle governance and accountability mechanisms that are explicitly mandated by the regulation but are only partially represented in principle-based formulations. Together, the resulting eleven high-level requirements provide a structured normative representation of the obligations imposed by the AI Act for high-risk systems. Each requirement corresponds to a coherent cluster of legally binding provisions, organised in a form suitable for systematic verification and without introducing additional normative content beyond the regulation itself.\n\nEach high-level requirement is subsequently decomposed into more granular operational components. These components correspond either to obligations explicitly stated in the AI Act or to requirements that can be reasonably derived from its provisions in order to enable verification. This decomposition does not aim to provide an exhaustive or definitive interpretation of the law, but to articulate traceable operational interpretations that can be subjected to assessment. The methodology explicitly allows for alternative decompositions depending on context, sector, and risk profile, and is designed to accommodate such variability rather than eliminate it.\n\n## 2.2 Authoritative grounding\n\nTo ensure both normative alignment and technical soundness, the operational interpretation of requirements is grounded in authoritative sources that specify or exemplify how high-level obligations are implemented and assessed in practice.\n\nThese sources are predominantly non-binding, complemented where appropriate by binding instruments whose established operational mechanisms are widely reused in compliance and assurance activities. In this respect, the GDPR [28] is included not as a source of normative extension of the AI Act, but because it provides mature and widely operationalised mechanisms-such as risk-based assessment, documentation practices, and accountability procedures-that directly inform how several AI Act obligations are interpreted and verified in practice. The AI Act explicitly operates in complementarity with existing data protection law, and high-risk AI systems in the European context frequently involve personal data processing throughout development, deployment, and monitoring. As a result, verification activities related to data governance, transparency and information provision, record keeping, and affected-person rights often rely on GDPR-derived concepts, artefacts, and procedures, such as information duties, accountability documentation, and impact-assessment practices. For this reason, the GDPR is treated here as an authoritative operational reference within the grounding layer.\n\nBeyond this exception, operational grounding relies on non-binding but authoritative sources that provide reusable mechanisms for implementing and assessing regulatory obligations in practice. These include the EU General Purpose AI Code of Practice [22], ISACA's Advanced Audit in AI Official Review Manual [35], UNESCO's Recommendation on the Ethics of AI [47], the Web Content Accessibility Guidelines (WCAG) 2.2 [51], and a broad range of ISO and IEC standards relevant to AI governance, risk management, quality management, cybersecurity, and data quality (e.g. [1-12, 14, 34]). These sources were selected because they translate high-level principles into implementable governance processes, controls, and audit practices, thereby bridging normative requirements and operational verification.\n\nWhere regulatory, guidance, and standardisation sources did not sufficiently specify technical verification procedures, the analysis was extended to the scientific literature. Highly cited and influential works in AI governance, robustness testing, risk management, and human-system interaction were considered (e.g. [18, 20, 29, 37, 39, 44, 45]). Selection criteria combined citation impact, venue relevance, and explicit connection to measurable or testable aspects of AI trustworthiness. This layered grounding process ensures that each operational requirement and associated verification activity can be traced back to authoritative legal, standardisation, or scientific sources.\n\n## 2.3 Verification dimensions\n\nFig. 2. Verification space defined by verification type and verification target.\n\n<!-- image -->\n\nConcrete verification activities are organised within a structured verification space defined by two orthogonal dimensions: the type of verification and the target of verification within the AI lifecycle. Together, these dimensions provide a common operational language for identifying, organising, and comparing verification activities across heterogeneous systems and stakeholders. The first dimension concerns the type of verification . Two broad categories are distinguished: controls and testing. Controls refer to process-based assurance mechanisms, including governance structures, documentation practices, quality management procedures, and organisational safeguards that ensure responsible\n\nManuscript submitted to ACM\n\nsystem development and operation. Testing refers to empirical evaluations that assess system behaviour, performance, or properties under specified conditions. Controls and testing are complementary rather than hierarchical: controls establish accountability and traceability, while testing provides empirical evidence of compliance. Both may be applied at any stage of the AI lifecycle. The second dimension concerns the target of verification , corresponding to the lifecycle component or artefact being evaluated. Numerous software and AI lifecycle models provide fine-grained decompositions of development and deployment phases (e.g. [23, 49]). However, such models vary significantly across standards, engineering practices, and organisational contexts, and their level of detail often presupposes technical expertise. For the purpose of compliance verification, which involves regulators, auditors, legal experts, risk managers, and developers alike, a higher level of abstraction is required to support immediate interpretability and shared understanding.\n\nAccordingly, the methodology adopts four high-level verification targets that act as stable and intuitive aggregation buckets rather than exhaustive lifecycle stages. The data target addresses issues related to data acquisition, composition, representativeness, and quality. The model target concerns the AI system's architecture, training procedures, explainability, robustness, and performance characteristics. The processes target encompasses organisational and procedural elements such as risk management, quality assurance, documentation, traceability, and lifecycle governance. The final product target focuses on the behaviour of the deployed system, including its interaction with users, integration into sociotechnical contexts, and observable outputs and impacts. This abstraction is intentional: it sacrifices fine-grained lifecycle specificity in favour of clarity, stability, and cross-disciplinary usability. Each target is sufficiently broad to accommodate different development methodologies and technical implementations, while remaining concrete enough to support systematic verification and evidence collection.\n\nWhile analytically distinct, these targets may overlap in practice. Certain verification activities, such as model monitoring or data versioning, span multiple targets simultaneously. The methodology treats these boundaries as pragmatic rather than absolute, allowing verification activities to address multiple concerns where appropriate. Within this verification space, each high-level requirement is associated with a set of control and testing activities consistent with its legal intent and operational scope. This structure enables evaluators to systematically identify relevant verification activities, organise evidence coherently, and ensure coverage across governance and empirical dimensions. By making explicit how regulatory requirements translate into concrete verification actions, the methodology supports comparability, traceability, and communication across heterogeneous stakeholders and use cases.\n\n## 3 Mapping\n\nIn this section, we present the mapping between the high-level requirements of the AI Act defined in Section 2.1 and the mechanisms that support their implementation and verification. Each subsection ( R1-R11 ) introduces a brief explanation of the corresponding requirement, followed by the actual mapping presented in the form of a table. These tables identify the relevant legal provisions, associated authoritative references, and the concrete methods through which compliance can be assessed or demonstrated. To ensure conceptual clarity and consistency across requirements, each method is classified along the two analytical dimensions described in Section 2.3. Type distinguishes between C (Controls) and T (Tests) , while Target specifies the primary layer of the AI lifecycle to which the method applies: D (Data) , M(Models) , P (Processes) , and FP (Final Product) .\n\n## 3.1 R1: Human Agency and Oversight\n\nArticle 14 of the AI Act establishes the principle of human agency and oversight as a cornerstone of trustworthy AI. It requires that AI systems be designed and implemented so that they operate under meaningful human control and Manuscript submitted to ACM\n\nthat ultimate responsibility remains with human operators. In practice, this means that individuals supervising the system must understand its capabilities and limitations, be able to intervene when necessary, and override its decisions to prevent or mitigate risks.\n\n| Requirement                | References                            | Methods                                                                                                                                | Type   | Target   |\n|----------------------------|---------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|--------|----------|\n| R1.1: Human in the loop    | Sheridan et al. [45]                  | M1.1: Manual override - Confirm that a human operator can take control of the AI system at any time during operation.                  | C      | P, FP    |\n| R1.1: Human in the loop    | Sheridan et al. [45]                  | M1.2: Supervisory control -Assess whether humanoperators are able to supervise the AI system and intervene when necessary.             | C      | P, FP    |\n| R1.1: Human in the loop    | ISO/IEC 42001:2023 (Ann.A.3.2)        | M1.3: Strategic governance - Ensure that organisational oversight mechanisms for AI deployment are defined and documented.             | C      | P        |\n| R1.2: User Autonomy        | GDPR (Art.7); An- dreotta et al. [16] | M1.4: Informed consent - Check that users are provided with clear information about AI usage and are able to give or withhold consent. | C      | FP       |\n| R1.2: User Autonomy        | ISACA manual (Â§ 3.2)                  | M1.5: User preferences - Review whether the system allows adaptation of its behaviour based on user-set preferences.                   | C      | FP       |\n| R1.2: User Autonomy        | ISACA manual (Â§ 3.2)                  | M1.6: Opt-in/out - Confirm that users can enable or disable AI functionalities according to their choices.                             | C      | FP       |\n| R1.3: Oversight Mechanisms | ISACA manual (Â§ 2.11.1)               | M1.7: Audit trails - Ensure that system actions and decisions are recorded in tamper-evident logs for traceability.                    | C      | P        |\n\n## 3.2 R2: Technical Robustness and Safety\n\nTechnical robustness and safety under the AI Act (Article 15) concern the system's ability to operate reliably under normal and adverse conditions, to resist manipulation or degradation, and to recover safely from failures. These requirements ensure that AI systems are resilient to risks arising from data drift, adversarial attacks, or component malfunctions, and that their performance remains consistent throughout the lifecycle. They also mandate mechanisms for fallback and uncertainty management, so that any deviation from intended operation can be detected, controlled, and mitigated in a way that preserves both functionality and safety.\n\n| Requirement                      | References                                    | Methods                                                                                                                                     | Type   | Target   |\n|----------------------------------|-----------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|--------|----------|\n| R2.1: Resilience & Re- liability | ISACA manual (Â§ 1.4.4)                        | M2.1: Stress testing - Assess whether the system has been exposed to extreme loads or edge cases to identify potential failure points.      | T      | P        |\n| R2.1: Resilience & Re- liability | ISACA manual (Â§ 2.8.6)                        | M2.2: Drift detection - Confirm that mechanisms are in place to monitor data distribution changes and evaluate behaviour under such shifts. | T      | D, P     |\n| R2.1: Resilience & Re- liability | Basiri et al. [18]                            | M2.3: Chaos engineering -Determine whether resilience has been tested through controlled fault injection in production-like environments.   | T      | P        |\n| R2.1: Resilience & Re- liability | ISACA manual (Â§ 2.9.4)                        | M2.4: Fail-over architecture - Ensure redundant components exist to maintain functionality in case of failure.                              | C      | P, FP    |\n| R2.2: Robustness to Attacks      | Brundage et al. [20]; ISACA manual (Â§ 2.13.3) | M2.5: Red teaming -Examine whether adversarial testing is conducted to uncover vulnerabilities.                                             | T      | P, FP    |\n| R2.3: Fallback & Fail- safe      | HLEG Guidelines                               | M2.6: Human fallback - Check that the system includes a mechanism to transfer control to a human in the event of failure.                   | C      | P, FP    |\n| R2.4: Accuracy & Un- certainty   | ISACA manual (Â§ 1.4.3)                        | M2.7: Model calibration - Review whether confidence scores are appropriately aligned with model performance.                                | T      | M        |\n\n## 3.3 R3: Privacy and Data Governance\n\nArticle 10 of the AI Act establishes specific obligations concerning data governance and data quality for high-risk AI systems, requiring that datasets be relevant, representative, free of errors, and complete to the extent possible. These obligations are AI-specific and apply irrespective of whether personal data is involved. Where high-risk AI systems Manuscript submitted to ACM\n\nrely on personal data, the operational interpretation and verification of Article 10 requirements necessarily draw on established data protection mechanisms defined under the GDPR, in particular Articles 5, 25, and 32, which address purpose limitation, data protection by design, and security of processing. In such cases, GDPR concepts and safeguards provide concrete, widely operationalised mechanisms through which certain data governance obligations of the AI Act can be implemented and assessed in practice. Accordingly, compliance with Article 10 typically involves a combination of organisational and technical measures, such as encryption, pseudonymisation, access controls, and accountability procedures, which are already well established in data protection practice. Effective data governance thus supports compliance with AI Act requirements while also contributing to the technical robustness and trustworthiness of AI systems across their lifecycle.\n\n| Requirement                | References                                 | Methods                                                                                                                | Type   | Target   |\n|----------------------------|--------------------------------------------|------------------------------------------------------------------------------------------------------------------------|--------|----------|\n| R3.1: Data Minimisa- tion  | GDPR (Art. 5.1.b-c)                        | M3.1: Purpose limitation - Verify that only data strictly necessary for a defined processing purpose is collected.     | C      | D, P     |\n| R3.1: Data Minimisa- tion  | GDPR (Art. 5.1.b)                          | M3.2: Scope restriction - Assess whether processing is confined to explicitly defined contexts.                        | C      | D, P     |\n| R3.2: Privacy- Enhancing   | GDPR (Art. 25.1-2; Art. 32.1.a)            | M3.3: Pseudonymisation - Check whether mechanisms are in place to pseudonymise personal data.                          | C      | D        |\n| R3.3: Access & Secu- rity  | GDPR (Art. 32.1.b-d)                       | M3.4: Role-based access control - Ensure access rights are granted based on organisational roles.                      | C      | P        |\n| R3.3: Access & Secu- rity  | GDPR (Art. 32.1.a); ISACA manual (Â§ 2.7.3) | M3.5: Encryption - Check that data is encrypted in storage and transmission.                                           | T      | D, P     |\n| R3.4: Data Prove- nance    | ISACA manual (Â§ 2.7.5)                     | M3.6: Lineage tracking - Confirm that data transformations are logged to ensure traceability.                          | C      | D, P     |\n| R3.4: Data Prove- nance    | ISACA manual (Â§ 3.1.2)                     | M3.7: Metadata versioning - Verify dataset versions and schema changes are tracked.                                    | C      | D, P     |\n| R3.5: Consent Man- agement | GDPR (Art. 7.1-2)                          | M3.8: Consent tracking - Inspect whether user consents are explicitly obtained, logged, and tied to specific purposes. | C      | P, FP    |\n| R3.5: Consent Man- agement | GDPR (Art. 7.3)                            | M3.9: Granular revocation - Confirm that users can revoke permissions without affecting unrelated consents.            | C      | FP, P    |\n\n## 3.4 R4: Transparency\n\nTransparency is one of the most fundamental requirements for trustworthy and accountable AI. Under the AI Act, specifically in Art. 13, AI systems must be designed and documented in a way that allows users, regulators, and auditors to understand their capabilities, limitations, and decision-making logic. Transparency encompasses both technical and communicative aspects: it includes model explainability, data and process traceability, disclosure of AI usage to end users, and interpretability of outputs in accessible language. Together, these mechanisms ensure that AI operations remain observable and interpretable across all stages of their lifecycle, thereby fostering accountability and enabling meaningful human oversight.\n\n| Requirement                     | References                   | Methods                                                                                                                                                                                                           | Type   | Target   |\n|---------------------------------|------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|----------|\n| R4.1: Explainability            | ISACA manual (Â§ 3.2)         | M4.1: Local interpretability - Verify that the system provides human- understandable explanations of model outputs using recognised local explanation techniques (e.g., feature attribution or surrogate models). | T      | D,M,FP   |\n| R4.1: Explainability            | Mundhenk et al. [39]         | M4.2: Saliency maps - Check if visualisation tools highlight relevant input re- gions.                                                                                                                            | T      | D,M,FP   |\n| R4.2: Model & Data Traceability | GPAI CoP (Sec. Transparency) | M4.3: Model cards - Ensure that standardised documentation is available and maintained.                                                                                                                           | C      | M, P     |\n\nManuscript submitted to ACM\n\n|                                      | GPAI CoP (Sec. Transparency)   | M4.4: Datasheets - Assess whether datasheets describing dataset origin and composition are accessible.   | C   | D, P   |\n|--------------------------------------|--------------------------------|----------------------------------------------------------------------------------------------------------|-----|--------|\n| R4.3: Disclosure of AI Use           | ISO/IEC 22989 (Â§5.15)          | M4.5: AI identity notices -Verify that users are clearly informed when interacting with an AI system.    | C   | FP     |\n| R4.3: Disclosure of AI Use           | GPAI CoP; ISACA manual (Â§ 3.2) | M4.6: Disclaimers -Checkwhether limitations and assumptions are transparently disclosed.                 | C   | FP     |\n| R4.3: Disclosure of AI Use           | ISO/IEC 23894 (Â§6.1)           | M4.7: Risk communication - Assess whether the system communicates known risks and uncertainties.         | C   | FP     |\n| R4.4: Interpretability for End Users | ISO/IEC 22989 (Â§5.15)          | M4.8: Understandable language - Confirm that technical terms are explained using plain language.         | C   | FP     |\n\n## 3.5 R5: Diversity, Non-discrimination and Fairness\n\nDiversity, non-discrimination, and fairness are central to ensuring that AI systems respect fundamental rights and avoid reinforcing structural biases. Articles 10, and 27 of the AI Act require that systems be designed, trained, and deployed in a way that prevents discriminatory outcomes and ensures inclusive access. This includes assessing data representativeness, applying fairness metrics across protected groups, and incorporating participatory design practices that reflect societal diversity. Fairness thus spans both technical and organisational domains, linking data quality, model evaluation, and stakeholder engagement to the ethical and legal accountability of AI systems.\n\n| Requirement                    | References           | Methods                                                                                                        | Type   | Target   |\n|--------------------------------|----------------------|----------------------------------------------------------------------------------------------------------------|--------|----------|\n| R5.1: Fairness in Data         | ISACA manual (Â§ 3.2) | M5.1: Bias detection -Evaluate whether datasets are examined for representation gaps.                          | T      | D, P     |\n| R5.1: Fairness in Data         | ISACA manual (Â§ 3.2) | M5.2: Sampling strategies - Confirm that sampling ensures balanced representa- tion.                           | C      | D, P     |\n| R5.2: Fairness in Al- gorithms | ISACA manual (Â§ 3.2) | M5.3: Protected-class fairness metrics - Verify that fairness metrics are applied across protected groups.     | T      | D, M, P  |\n| R5.3: Inclusive De- sign       | WCAG 2.2             | M5.4: Accessibility - Check that the system supports assistive technologies.                                   | C      | FP       |\n| R5.4: Stakeholder En- gagement | Schuler et al. [44]  | M5.5: Participatory design - Assess whether affected users and communities are involved in design and testing. | C      | FP, P    |\n\n## 3.6 R6: Societal and Environmental Well-being\n\nSocietal and environmental well-being extend the notion of trustworthy AI beyond individual rights, addressing collective and long-term impacts. Articles 27 and 40 of the AI Act require that systems be aligned with broader ethical and sustainability goals, including environmental protection, energy efficiency, and respect for human dignity. This involves monitoring resource consumption, integrating human rights principles into design, and ensuring that ethical oversight bodies are established to guide responsible innovation. These requirements embed sustainability and societal benefit as integral dimensions of AI governance.\n\n| Requirement                | References                                | Methods                                                                                                            | Type   | Target   |\n|----------------------------|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------|--------|----------|\n| R6.1: Environmental Impact | UNESCO (Rec. 84); ISACA manual (Â§ 1.17.7) | M6.1: Energy-use tracking - Verify that the system monitors computational energy consumption and carbon footprint. | T      | P, FP    |\n| R6.2: Ethical Align- ment  | Leslie et al. [37]                        | M6.2: Rights-based system -Assess whether system design aligns with funda- mental rights.                          | C      | P        |\n|                            | UNESCO (Rec. 58)                          | M6.3: Independent ethics committee - Verify that an independent ethics commit- tee or officer is appointed.        | C      | P        |\n\nManuscript submitted to ACM\n\n## 3.7 R7: Accountability\n\nAccountability ensures that responsibility for AI-related outcomes is clearly defined and traceable across the system's lifecycle. Articles 12 and 17 of the AI Act establish the duty to maintain documentation, assign responsibilities, and ensure that auditability and redress mechanisms are in place. Accountability mechanisms link technical and organisational controls to transparency, enabling effective oversight and redress when harm occurs. They require clear responsibility allocation, secure logging, version-controlled documentation, and channels for incident reporting, thereby operationalising the principle that accountability cannot be delegated to the machine.\n\n| Requirement                     | References                                                | Methods                                                                                                     | Type   | Target   |\n|---------------------------------|-----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|--------|----------|\n| R7.1: Responsibility Assignment | ISACA manual (Â§ 3.2)                                      | M7.1: RASCI charts - Confirm that responsibility matrices define roles for each lifecycle activity.         | C      | P        |\n| R7.2: Auditability              | ISO/IEC 42001:2023 (Ann.B.6.2.8); ISACA manual (Â§ 2.11.1) | M7.2: Logging - Verify that detailed, tamper-evident logs of system operations are maintained and reviewed. | C      | P        |\n| R7.2: Auditability              | ISO/IEC 42001:2023 (Ann.B.8.3)                            | M7.3: External reporting interfaces - Ensure that secure portals are available for third-party auditors.    | C      | P        |\n| R7.3: Incident Report- ing      | Floridi et al. [29]                                       | M7.4: Redress systems - Check whether users can report issues or contest AI- driven outcomes.               | C      | FP, P    |\n| R7.4: Documentation Integrity   | ISO/IEC 42001:2023 (Ann.B.6.2.5); ISACA manual (Â§ 2.11.1) | M7.5: Version-controlled records - Assess whether documentation is versioned and traceable over time.       | C      | P        |\n\n## 3.8 R8: Quality Management\n\nArticle 17 of the AI Act requires providers of high-risk AI systems to establish, implement, document, and maintain a quality management system (QMS). The QMS ensures that the organisation applies consistent procedures for design, testing, data management, and post-market monitoring. It provides a structured approach for achieving and maintaining conformity with the requirements of the AI Act. Quality management thereby supports traceability, continuous improvement, and long-term compliance across the AI system lifecycle.\n\n| Requirement                       | References    | Methods                                                                                                                                                                                                                                                                                                                                                                                                                        | Type   | Target   |\n|-----------------------------------|---------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|----------|\n| T8.1: Quality policy & objectives | ISO 9001:2015 | M8.1: Quality Policy Declaration - Verify that top management has issued a formal, documented policy defining quality objectives and regulatory compliance commitments for high-risk AI systems under a Quality Management System (QMS). The QMS would determine, implement, and control processes needed for quality management, and maintain documented information to support operation and provide evidence of conformity. | C      | P        |\n| T8.2: Documented QMS procedures   | ISO 9001:2015 | M8.2: Control of documented information - Check that all lifecycle stages (design, development, validation, post-market) have controlled and versioned documented information stored in a document management system.                                                                                                                                                                                                          | C      | P        |\n| T8.3: Internal quality audits     | ISO 9001:2015 | M8.3: Internal Auditing - Plan and perform periodic audits to verify conformity of processes and compliance with the Regulation; record results and corrective actions.                                                                                                                                                                                                                                                        | C      | P        |\n| T8.4: Continuous im- provement    | ISO 9001:2015 | M8.4: Continuous Improvement - Use audit results, performance data, and feed- back to identify non-conformities and improvement opportunities; update pro- cesses and objectives accordingly.                                                                                                                                                                                                                                  | C      | P        |\n\nManuscript submitted to ACM\n\n## 3.9 R9: Risk Management\n\nArticle 9 of the AI Act establishes the obligation to implement a continuous and systematic risk management process throughout the lifecycle of high-risk AI systems. This process covers the identification, analysis, evaluation, and control of risks, as well as post-market monitoring and review. It must ensure that residual risks are acceptable relative to the intended use and that appropriate mitigation measures are applied. The methodology used here aligns with a set of horizontal, cross-sectoral standards, including ISO/IEC 23894 (AI risk management), ISO/IEC 42001 (AI management system), ISO 31000 and ISO 31010 (General risk management principles and techniques), ISO/IEC 22989 (AI concepts and terminology) as well as data- and robustness-related standards such as ISO/IEC 5259 series, ISO 8000 series, ISO/IEC TR 24027, ISO/IEC 24029-1/-2, and ISO/IEC 24970. In the future, when harmonised standards under the AI Act, particularly prEN 18228 (AI Risk Management) and prEN 18286 (AI Quality Management System) will be finalised, they will provide the solid structure for conformity verification and presumption of conformity once a system is categorised as high-risk.\n\nHowever, it is essential to underline that none of these standards, both international ISO/IEC standards and future harmonised European standards, determine whether an AI system is classified as high-risk under Article 6. Classification is governed exclusively by the legal criteria of the AI Act, particularly in Annex III. Standards are used in this report solely to support structured analysis, documentation and evidence generation, and they become fully applicable only after an AI system has been classified as high-risk. In addition, the methodology deliberately excludes sector-specific or application-specific standards, such as functional-safety standards (e.g., IEC 61508 or ISO 13849), medical-device standards (e.g., ISO 14971) or automotive safety standards (e.g., ISO 26262). These frameworks are highly relevant within their respective regulatory regimes but are not part of the horizontal, cross-sector risk-classification process defined by the AI Act and therefore are not included in the present analysis.\n\n| Requirement                             | References                                                                                                                                       | Methods                                                                                                    | Type   | Target   |\n|-----------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|--------|----------|\n| T9.1: System Defini- tion               | ISO/IEC 42001 (Â§Â§ 4.3, 6.1.2); ISO/IEC 23894 (Â§ 6.3); ISO/IEC 22989                                                                              | M9.1 Description of AI system - define intended purpose for Art 6 mapping and record system de- tails      | C      | P        |\n| T9.2: Risk identifica- tion             | ISO/IEC 42001 (Â§Â§ 6.1.2, 7.5); ISO/IEC 23894 (Â§Â§ 5.4, 6.4); ISO 31000; ISO 31010; ISO/IEC 5259- 1; ISO 8000-8; ISO/IEC TR 24027                  | M9.2 Structured risk identification relevant to clas- sification (Art. 6) and risk management (Art. 9)     | C      | P        |\n| T9.3: Risk analysis                     | ISO/IEC 23894 (Â§Â§ 6.4, 6.5); ISO/IEC 42001; ISO 31000; ISO 31010; ISO/IEC 24029-2                                                                | M9.3 Core classification step - apply AI Act Art. 6: Annex I & Annex III gates using structured analy- sis | C      | P        |\n| T9.4: Risk evaluation & prioritisation  | ISO/IEC 23894 (Â§Â§ 6.4, 6.5); ISO/IEC 42001; ISO 31000; ISO 31010                                                                                 | M9.4 Finalize classification decision;record justifi- cation and prioritise risks                          | C      | P        |\n| T9.5: Risk control con- siderations     | ISO/IEC 23894 (Â§Â§ 5.5-5.7); ISO/IEC 42001; ISO 31000; ISO/IEC 24029-1/-2; ISO/IEC TR 24027; ISO/IEC 5259; ISO 8000; ISO/IEC 27001; ISO/IEC 27005 | M9.5 Outline potential risk areas aligned with AI Act essential requirements                               | C      | FP       |\n| T9.6: PMM: Planning                     | ISO/IEC 42001 (Â§Â§ 8.4, 9.1, 7.5); ISO/IEC 23894 (Â§Â§ 5.3, 5.7); ISO 31000; ISO/IEC 24970; ISO/IEC 27001; ISO/IEC 27005                            | M9.6 Establishing PMMplan with monitoring, log- ging and reporting                                         | C      | P        |\n| T9.7: PMM: Data col- lection & analysis | ISO/IEC 42001 (Â§Â§ 9.1, 9.3, 7.5); ISO/IEC 23894 (Â§Â§ 5.5-5.7); ISO 31000; ISO/IEC 24970; ISO/IEC 27001; ISO/IEC 27005                             | M9.7 Continuous verification and early detection of anomalies or new hazards                               | T      | FP       |\n| T9.8: PMM: Review & CAPA                | ISO/IEC 42001 (Â§Â§ 8.3, 10, 7.5); ISO/IEC 23894 (Â§Â§ 5.6, 5.8); ISO 31000                                                                          | M9.8 Perform CAPA, update system, maintain con- tinuous compliance                                         | C      | P        |\n| T9.9: Incident report- ing              | ISO/IEC 42001 (Â§Â§ 8.4, 9.1, 7.5); ISO/IEC 23894 (Â§Â§ 5.7, 5.8); ISO/IEC 24970; ISO/IEC 27035                                                      | M9.9 Compliant incident reporting; documenta- tion; integrate lessons learned                              | C      | P        |\n\nManuscript submitted to ACM\n\n## 3.10 R10: Technical Documentation\n\nArticle 11 and Annex IV of the AI Act require providers of high-risk AI systems to prepare and maintain comprehensive technical documentation demonstrating compliance with the regulation. This documentation must provide sufficient detail to assess the system's conformity, including its intended purpose, design specifications, data sources, testing methods, and risk management processes. Proper documentation ensures transparency, enables traceability of design choices, and supports both internal governance and external conformity verification.\n\n| Requirement                                   | References                                                                                                                    | Methods                                                                                                                     | Type   | Target   |\n|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|--------|----------|\n| R10.1: System overview & intended purpose     | ISO/IEC 42001 (Â§Â§ 4.3, 6.1.2, 7.5); ISO/IEC 23894 (Â§ 6.3); ISO/IEC 22989                                                      | M10.1 System overview and intended purpose doc- umented in alignment with Annex IV and QMS requirements.                    | C      | FP       |\n| R10.2: Design specifi- cations & architecture | ISO/IEC 42001(Â§Â§ 8.1-8.3, 7.5); ISO/IEC 23894 (Â§Â§ 6.4-6.5)                                                                    | M10.2 Architecture and design specifications cap- tured with complete traceability to risks and re- quirements.             | C      | D&M      |\n| R10.3: Dataset descrip- tion & provenance     | ISO/IEC 42001 (Â§Â§ 7.5, 8.3); ISO/IEC 5259-1; ISO 8000-8; ISO/IEC TR 24027                                                     | M10.3 Dataset provenance, quality attributes and limitations documented in compliance with Art. 10-11 and Annex IV.         | C      | D&M      |\n| R10.4: Performance metrics & validation       | ISO/IEC 42001 (Â§Â§ 8.3, 9.1, 7.5); ISO/IEC 23894 (Â§Â§ 6.4-6.5); ISO/IEC 24029-1/-2; ISO/IEC 24970; ISO/IEC 27001; ISO/IEC 27005 | M10.4 Performance, robustness and validation ev- idence compiled and traceable to requirements, risks and intended purpose. | T      | FP       |\n| R10.5: Compliance ev- idence mapping          | ISO/IEC 42001 (Â§Â§ 6.1, 8.1--8.4, 9.1-9.3, 10); ISO/IEC 23894 (Â§Â§ 5.3-5.8); ISO/IEC 27001; ISO/IEC 27005                       | M10.5 Structured compliance mapping demon- strating how Annex IV and Article 17 obligations are satisfied.                  | C      | P        |\n\n## 3.11 R11: Record-keeping\n\nArticle 18 of the AI Act mandates that providers retain automatically generated logs and relevant records to ensure traceability and accountability throughout the AI lifecycle. Record-keeping supports post-market monitoring, facilitates incident investigation, and provides evidence for conformity verifications. It must include documentation of model versions, dataset lineage, and data retention or deletion policies, consistent with organisational governance and data protection requirements, including GDPR.\n\n| Requirement                          | References                                                                                                  | Methods                                                                                                                                                                                      | Type   | Target   |\n|--------------------------------------|-------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|----------|\n| R11.1: Operational logs              | ISO/IEC 42001 (Â§Â§ 7.5, 8.3, 9.1); ISO/IEC 24970                                                             | M11.1 Operational logging system in place with records supporting Art. 18 record-keeping and PMM requirements.                                                                               | C      | FP       |\n| R11.2: Version control records       | ISO/IEC 42001 (Â§Â§ 7.5, 8.3)                                                                                 | M11.2 Version control ensures full traceability and reproducibility of AI system states over time.                                                                                           | C      | D&M      |\n| R11.3: Model &dataset lineage        | ISO/IEC 42001 (Â§Â§ 7.5, 8.3); ISO/IEC 23894 (Â§Â§ 5.3-5.8)                                                     | M11.3 Full lineage enables auditing, incident anal- ysis, risk traceability and evidence-based updates.                                                                                      | C      | D&M      |\n| R11.4: Retention & deletion policies | ISO/IEC 42001 (Â§Â§ 7.5, 8.3); ISO/IEC 23894 (Â§Â§ 5.3-5.8); ISO/IEC 27001; ISO/IEC 27005; GDPR (Art. 5.1.c, e) | M11.4 Retention/deletion policies implemented to ensure compliance, minimise risk and maintain only necessary records, in line with GDPR data minimisation and storage limitation principles | C      | P        |\n\n## 4 Case Study\n\nThis section illustrates how the mapping proposed in Section 3 can be applied in practice through a real, ongoing highrisk AI use case currently being analysed and tested in collaboration with Scania, a Swedish automotive manufacturer. While the mapping is generic and technology-agnostic, this example shows how it can structure assurance activities Manuscript submitted to ACM\n\nacross the AI lifecycle, from design-time verification to validation and documentation. Table ?? presents the results of this mapping process; for the sake of brevity, it is reported only partially and serves as an illustrative example. The use case concerns an AI-based system for detecting cyberattacks within connected vehicles in an industrial setting at Scania. Modern vehicles rely on internal communication networks that enable real-time data exchange between electronic components. Protecting these networks is critical, as cyberattacks may compromise both vehicle safety and system reliability. To address these risks, automotive manufacturers increasingly deploy Intrusion Detection Systems (IDS) that monitor in-vehicle communications and identify abnormal or potentially harmful behaviour. In this setting, the IDS employs AI techniques to analyse in-vehicle network traffic and detect patterns associated with known or emerging attacks. The system processes network data, extracts relevant features, and uses machine-learning models to distinguish normal operation from suspicious activity. To support transparency and engineering oversight, explainability mechanisms are integrated to allow engineers to understand why specific activities are flagged as intrusions. Designed for real-time deployment in vehicle control units or central gateways, this use case demonstrates the application of the proposed mapping in a complex, safety-critical automotive context. The mapping enables structured and traceable verification of data handling, model behaviour, explainability, and documentation, providing evidence to support safety, cybersecurity, and regulatory requirements without intervening in system development.\n\n| Req.                                          | Sub-req.                      | Method                        | Instantiation in the Use Case                                                                                                                                                                                  |\n|-----------------------------------------------|-------------------------------|-------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| R1: Human Agency & Oversight (Art. 14)        | T1.1 Human-in-the-loop        | M1.1 Manual over- ride        | Engineers and operators can override IDS decisions and reclassify labels during development.                                                                                                                   |\n| R1: Human Agency & Oversight (Art. 14)        | T1.1 Human-in-the-loop        | M1.2 Supervisory control      | IDS outputs are logged and verified by safety and security engineers before integration into final decisions.                                                                                                  |\n| R1: Human Agency & Oversight (Art. 14)        | T1.1 Human-in-the-loop        | M1.3 Strategic gov- ernance   | Ensure a documented organisational oversight structure: define who in the team (e.g., vehicle OEM cybersecurity lead) is responsible for authorising deployment and change management of the IDS.              |\n| R1: Human Agency & Oversight (Art. 14)        | T1.2 User Autonomy            | M1.4 Informed con- sent       | Data collection from vehicles was conducted under user consent for research and model development.                                                                                                             |\n| R1: Human Agency & Oversight (Art. 14)        | T1.2 User Autonomy            | M1.5 User prefer- ences       | Provide vehicle end-users or clients (e.g., fleet operators) the ability to config- ure IDS alert sensitivity or disable non-critical automated responses (while still keeping core safety monitoring active). |\n| R1: Human Agency & Oversight (Art. 14)        | T1.3 Oversight Mechanisms     | M1.7 Audit trails             | The IDS decisions, rule activations, and system overrides are logged for auditabil- ity.                                                                                                                       |\n| R2: Technical Robustness & Safety (Art. 15)   | T2.1 Resilience & Reliability | M2.1 Stress testing           | Baseline models were evaluated under high-throughput simulated traffic condi- tions.                                                                                                                           |\n| R2: Technical Robustness & Safety (Art. 15)   | T2.1 Resilience & Reliability | M2.2 Drift detection          | Monitor for changes over time in the in-vehicle network traffic (e.g., ECU up- dates, different firmware) and detect when feature distributions deviate; trigger retraining or model review.                   |\n| R2: Technical Robustness & Safety (Art. 15)   | T2.1 Resilience & Reliability | M2.4 Fail-over archi- tecture | Ensure the IDS has a fallback mode: if the neuro-symbolic model fails or confidence drops, revert to a simpler rule-based guard or human-supervised monitoring.                                                |\n| R2: Technical Robustness & Safety (Art. 15)   | T2.2 Robustness to Attacks    | M2.5 Red teaming              | The system was tested with synthetic adversarial attacks, including crafted PTP manipulations.                                                                                                                 |\n| R2: Technical Robustness & Safety (Art. 15)   | T2.3 Fallback & Fail-safe     | M2.6 Human fall- back         | Low-confidence detections trigger alerts requiring human verification.                                                                                                                                         |\n| R2: Technical Robustness & Safety (Art. 15)   | T2.4 Accuracy & Uncertainty   | M2.7 Model calibra- tion      | Performance metrics include confidence intervals; rule activation thresholds are tunable.                                                                                                                      |\n| R3: Privacy & Data Governance (Art. 10; GDPR) | T3.1 Data Minimisation        | M3.1 Purpose limita- tion     | Only relevant features for detection (e.g., timestamp, ID, payload length) are processed.                                                                                                                      |\n| R3: Privacy & Data Governance (Art. 10; GDPR) | T3.2 Privacy-Enhancing        | M3.3 Pseudonymisa- tion       | When capturing vehicle network traffic for training, any identifiable driver or ve- hicle identity data is pseudonymised or removed, retaining only features necessary for intrusion detection.                |\n\nManuscript submitted to ACM\n\n| Legal Require- ment (AI Act)                                             | Test / Control Type                        | Method                                   | Instantiation in the Use Case                                                                                                                                                                                                                              |\n|--------------------------------------------------------------------------|--------------------------------------------|------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                                                          | T3.3 Access & Security                     | M3.4 Role-based ac- cess control         | Raw data is accessible only to authorised research engineers; results are pseudonymised.                                                                                                                                                                   |\n| R4: Transparency (Art. 13, 50)                                           | T4.1 Explainability                        | M4.1 SHAP / M4.2 LIME                    | SHAP is used to validate feature contributions; LNN rules provide symbolic reasoning.                                                                                                                                                                      |\n| R4: Transparency (Art. 13, 50)                                           | T4.2 Model & Data Traceability             | M4.4 Model cards                         | All models used are accompanied by documentation on training data, assumptions, and risks.                                                                                                                                                                 |\n| R4: Transparency (Art. 13, 50)                                           | T4.2 Model & Data Traceability             | M4.5 Datasheets                          | Provide a datasheet for the dataset used in the IDS: describe origin (e.g., vehicles from the partner), format, attack types covered, limitations, and preprocessing steps to support traceability in audits.                                              |\n| R4: Transparency (Art. 13, 50)                                           | T4.3 Disclosure of AI Use                  | M4.6 AI identity no- tice                | In future deployments, HMI interfaces will indicate when AI-based IDS is active.                                                                                                                                                                           |\n| R5: Fairness & Non- Discrimination (Art. 9, 10)                          | T5.1 Fairness in Data                      | M5.1 Data bias de- tection               | Attack data were balanced to ensure representative detection performance.                                                                                                                                                                                  |\n| R5: Fairness & Non- Discrimination (Art. 9, 10)                          | T5.4 Stakeholder Engagement                | M5.5 Participatory design review         | Security experts and domain engineers reviewed and refined rule design and evaluation criteria.                                                                                                                                                            |\n| R7: Accountability (Art. 12, 17)                                         | T7.1 Responsibility Assignment             | M7.1 Responsibility- matrix verification | Roles for data handling, model design, and validation are clearly documented.                                                                                                                                                                              |\n| R7: Accountability (Art. 12, 17)                                         | T7.2 Auditability                          | M7.2 Logging                             | All detection events and rule-based decisions are logged and version controlled.                                                                                                                                                                           |\n| R7: Accountability (Art. 12, 17)                                         | T7.3 Incident Reporting                    | M7.4 Redress sys- tems                   | False positives or missed detections can be flagged through internal validation interfaces.                                                                                                                                                                |\n| R7: Accountability (Art. 12, 17)                                         | T7.4 Documentation Integrity               | M7.5 Version- controlled records         | Maintain version control for IDS models, rule sets, training datasets, and change logs (who changed what, when, and why) to support traceability and post-incident investigation.                                                                          |\n| R8: Quality Management (Art. 17)                                         | T8.1 Quality policy & objectives           | M8.1 Quality Policy Declaration          | The company's cybersecurity and safety management define a documented QMS policy for AI-based IDS development, specifying objectives on detection accuracy, explainability, robustness, and regulatory alignment; relevant engineers receive QMS training. |\n| R8: Quality Management (Art. 17)                                         | T8.2 Documented QMS procedures             | M8.2 Control of documented information   | IDS development artifacts (model architectures, LNN rule sets, preprocessing pipelines, datasets, validation reports) are placed under controlled documentation with restricted access and traceability.                                                   |\n| R8: Quality Management (Art. 17)                                         | T8.3 Internal quality audits               | M8.3 Internal audit- ing                 | Periodic internal audits review model lineage logs, robustness testing outcomes, fairness checks, and documentation completeness; non-conformities trigger cor- rective actions.                                                                           |\n| R8: Quality Management (Art. 17)                                         | T8.4 Continuous improvement                | M8.4 Continuous improvement              | Findings from audits and monitoring are used to update models, rules, thresholds, and documentation throughout the lifecycle.                                                                                                                              |\n| R9: Risk classification, management, and post marketing (Art. 6, Art. 9) | T9.1 System                                | M9.1 Description of                      | The automotive AI-based IDS is defined as a system intended to detect cyber intrusions targeting in-vehicle networks and connected services.                                                                                                               |\n| R9: Risk classification, management, and post marketing (Art. 6, Art. 9) | Definition T9.2 Risk                       | AI system                                | Structured risk identification places the automotive IDS as preliminarily high-risk                                                                                                                                                                        |\n| R9: Risk classification, management, and post marketing (Art. 6, Art. 9) | identification                             | M9.2 Structured risk identification      | due to safety-critical vehicle functions.                                                                                                                                                                                                                  |\n| R9: Risk classification, management, and post marketing (Art. 6, Art. 9) | T9.3 Risk analysis                         | M9.3 Core classifica- tion step          | The IDS is assessed against AIA Annex I & III, with critical-infrastructure and safety-component gating confirming high-risk applicability.                                                                                                                |\n| R9: Risk classification, management, and post marketing (Art. 6, Art. 9) | T9.5 Risk control measures                 | M9.5 Risk control strategy               | Risk controls are established, covering safety, robustness, explainability, human oversight, and documentation through git platform in automotive contexts.                                                                                                |\n| R9: Risk classification, management, and post marketing (Art. 6, Art. 9) | T9.6 PMM planning                          | M9.6 PMM system established              | A post-market monitoring plan is defined to track performance, drift, and safety-relevant anomalies across vehicle fleets.                                                                                                                                 |\n| R10: Technical Documentation (Art. 11; Annex IV)                         | T10.1 System overview & intended purpose   | M10.1 Annex-IV compliant descrip- tion   | Provide a structured description of the IDS: architecture, model pipeline, LNN rule types, deployment targets (ECUs, gateways), and intended function (early anomaly detection).                                                                           |\n| R10: Technical Documentation (Art. 11; Annex IV)                         | T10.2 Design specifications & architecture | M10.2 Architecture traceability          | Document the full dataflow: CAN/Ethernet packet capture â†’ preprocessing â†’ feature extraction â†’ ML/LNN inference â†’ explainability layer â†’ alert routing, with traceability from risks to design decisions.                                                  |\n| R10: Technical Documentation (Art. 11; Annex IV)                         | T10.3 Dataset description & provenance     | M10.3 Dataset provenance                 | Record dataset sources (vehicle captures), attack types, collection conditions, preprocessing steps, firmware versions when relevant, and known limitations (e.g., limited coverage of rare timing attacks).                                               |\n\nManuscript submitted to ACM\n\n| Legal Require- ment (AI Act)      | Test / Control Type                    | Method                        | Instantiation in the Use Case                                                                                                                                                             |\n|-----------------------------------|----------------------------------------|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                   | T10.4 Performance metrics & validation | M10.4 Validation ev- idence   | Provide detection rate, latency under high traffic, robustness under noise, calibra- tion curves, and interpretability validation using SHAP and LNN reasoning.                           |\n|                                   | T10.5 Compliance evidence mapping      | M10.5 Mapping to obligations  | Map each AI Act requirement to evidence: oversight logs, stress-test results, explainability artefacts, model cards, dataset datasheets, QMS records, and PMM outputs.                    |\n| R11: Record-keeping (Art. 12, 18) | T11.1 Operational logs                 | M11.1 Operational logging     | Maintain logs of detections, LNN rule activations, SHAP explanations shown for review, fallback events, and red-team outcomes; store logs securely in automotive backend systems.         |\n| R11: Record-keeping (Art. 12, 18) | T11.2 Version control records          | M11.2 Version con- trol       | Track versions of ML models, LNN logic rules, preprocessing code, and dataset snapshots in a versioned repository with approvals and traceable change history.                            |\n| R11: Record-keeping (Art. 12, 18) | T11.3 Model & dataset lineage          | M11.3 Lineage docu- mentation | Maintain lineage links from raw CAN/Ethernet captures to preprocessing scripts, training datasets, trained model versions, and deployed artefacts to support re- producibility and audit. |\n| R11: Record-keeping (Art. 12, 18) | T11.4 Retention & deletion policies    | M11.4 Reten- tion/deletion    | Define retention periods for logs, dataset snapshots, and models; ensure GDPR compliance when identifiable metadata appears; enforce secure deletion mecha- nisms.                        |\n\n## 5 Discussion\n\nRather than functioning solely as an analytical construct, the proposed mapping is intended to support the institutional ecosystem through which AI Act compliance is operationalised in practice. By linking legal requirements to identifiable and verifiable assessment activities, the mapping provides a common reference structure that can be reused across provider self-assessment, regulatory sandbox experimentation, and formal conformity assessment. This addresses a central implementation risk of the AI Act: that identical legal obligations may be translated into materially different verification practices across organisations, sectors, and Member States.\n\nFor providers, the mapping enables structured self-assessment as part of continuous monitoring and post-market obligations. By making explicit which verification activities correspond to which legal requirements, it supports systematic internal evaluations over time, including reassessment following model updates, data drift, or changes in deployment context. This reduces reliance on implicit or ad hoc interpretations of compliance and facilitates the generation of traceable evidence that can be reused across internal governance, regulatory reporting, and external review. In regulatory sandboxes established under Article 57, the mapping can function as a shared interpretive layer between developers and competent authorities. By clarifying how experimental testing activities relate to specific AI Act obligations, it supports the structuring of testing plans, documentation practices, and exit reports, while preserving the flexibility required for innovation-oriented assessment. This role is particularly relevant in a cross-border context, where divergent sandbox practices risk emerging in the absence of a common operational reference. European-funded initiatives such as AI Factories [26] and Testing and Experimentation Facilities (TEFs) [15] are expected to play an increasingly central role in supporting the testing and validation of AI systems, including in the context of regulatory sandboxes, as reflected in recent policy and implementation guidance [27]. In this setting, the availability of a shared mapping between legal requirements and verification activities becomes critical. These infrastructures are not merely technical providers, but intermediaries that translate regulatory expectations into concrete testing capabilities. A common mapping helps ensure that testing performed within such initiatives is interpretable, comparable, and reusable across regulatory and institutional contexts, rather than being tied to local or project-specific interpretations.\n\nMore broadly, the mapping contributes to convergence in verification methodologies across the EU by offering a reusable, technology-agnostic operational vocabulary. While enforcement remains decentralised and sectoral specificities\n\nmust be respected, convergence on underlying verification logic is essential to avoid fragmentation of compliance practices. The mapping is therefore intentionally designed to be extensible: it can be refined, extended, and instantiated by different actors, including competent authorities, notified bodies, EU-funded infrastructures, and sectoral initiatives. Such extensions can introduce domain-specific metrics or procedures while preserving traceability to the original legal requirements. In this sense, the mapping is best understood as an intermediate coordination layer between legal text and assessment practice. It does not prescribe how verification must be performed, but provides a common structure within which diverse practices can evolve in a coherent and interoperable manner. Encouraging its extension and sectoral instantiation is therefore not a limitation, but a necessary condition for achieving both regulatory consistency and practical relevance under the AI Act.\n\nFinally, while this work is grounded in the European regulatory context and explicitly targets the implementation of the AI Act, the underlying challenge it addresses is not unique to Europe. Many jurisdictions are currently developing or refining AI-specific regulatory frameworks that similarly rely on high-level, principle-driven obligations whose practical verification remains underspecified. In this respect, the approach adopted in this paper is intended to be transferable. The notion of an explicit mapping between legal requirements and verifiable assessment activities can be adapted to other regulatory settings, supporting regulatory learning, comparability, and institutional capacity-building beyond the EU. The authors therefore encourage the development of analogous mappings in other jurisdictions and see value in future cross-jurisdictional dialogue on verification methodologies as AI regulation continues to evolve globally.\n\n## 6 Conclusion\n\nThis paper introduced an explicit and structured mapping between high-level obligations under the EU AI Act and concrete, verifiable assessment activities. Starting from 11 high-level requirements used as a normative basis, the mapping decomposes these obligations into 48 operational sub-requirements and associates them with 66 verification activities. By making these relationships explicit, the mapping bridges the gap between regulatory intent and assessable technical and organisational evidence across the AI lifecycle.\n\nA central contribution of this work is not merely the enumeration of verification activities, but the articulation of a reusable mapping logic. By organising verification activities along two orthogonal dimensions, i.e. the type of verification performed and the lifecycle target to which it applies, the mapping makes visible how compliance with the AI Act is constructed in practice. It shows that conformity emerges from structured combinations of procedural controls and empirical testing, rather than from isolated checks or single metrics. By formalising these relationships, the mapping directly reduces key sources of uncertainty in AI Act implementation. Interpretive uncertainty is addressed by clarifying how abstract legal obligations can be decomposed into operational elements grounded in recognised practices. Operational uncertainty is reduced by identifying verification activities that are implementable, repeatable, and auditable across different organisational and technical contexts. Procedural uncertainty is mitigated by enabling consistent documentation, traceability, and comparison of evidence across systems, actors, and regulatory settings.\n\nThe value of the mapping extends beyond individual assessments. It provides a common reference that can be reused across provider self-assessment, regulatory sandboxes, conformity assessment, and post-market monitoring, supporting convergence in verification practices across Member States while preserving contextual flexibility. In doing so, it aligns with the AI Act's governance model, which relies on continuous oversight and regulatory learning rather than one-off certification. The mapping is intentionally technology-agnostic and non-prescriptive. Its purpose is not to fix verification practices in advance, but to offer a stable coordination layer between legal text and assessment practice that can be extended, refined, and instantiated by different actors, including sectoral initiatives and EU-funded testing\n\ninfrastructures. In this sense, the contribution of this work lies in establishing a shared operational grammar for AI Act compliance verification, one that enables consistency without rigidity and innovation without fragmentation.\n\n## Acknowledgement\n\nThis work is supported by the EU project Citcom.AI and Vinnova INTERSTICE project (reference number: 2024-00661).\n\nThis work is also supported by the Swedish AI Factory and the Luxembourg AI Factory.\n\n## References\n\n- [1] 2015. Data quality - Part 8: Information and data quality: Concepts and measuring. ISO 8000-8:2015.\n- [2] 2015. ISO 9001:2015 - Quality management systems - Requirements. https://www.iso.org/standard/62085.html Latest version published in 2015.\n- [3] 2018. Risk management - Guidelines. ISO 31000:2018.\n- [4] 2019. Risk management - Risk assessment techniques. IEC 31010:2019.\n- [5] 2021. Information technology - Artificial intelligence (AI) - Bias in AI systems and AI-aided decision making. ISO/IEC TR 24027:2021.\n- [6] 2022. Information security, cybersecurity and privacy protection - Information security management systems - Requirements. ISO/IEC 27001:2022.\n- [7] 2022. Information security, cybersecurity and privacy protection - Information security risk management. ISO/IEC 27005:2022.\n- [8] 2022. Information technology - Artificial intelligence - Artificial intelligence concepts and terminology. ISO/IEC 22989:2022.\n- [9] 2023. Artificial intelligence (AI) - Assessment of the robustness of neural networks - Part 2: Methodology for the use of formal methods. ISO/IEC 24029-2:2023.\n- [10] 2023. Information security, cybersecurity and privacy protection - Information security incident management - Part 1: Principles of incident management. ISO/IEC 27035-1:2023.\n- [11] 2023. Information technology - Artificial intelligence - Guidance on risk management. https://www.iso.org/standard/77304.html Available at: https://www.iso.org/standard/77304.html.\n- [12] 2024. Artificial intelligence - Data quality for analytics and machine learning (ML) - Part 1: Overview, terminology and examples. ISO/IEC 5259-1:2024.\n- [13] 2024. Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts. https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32024R1689. OJ L 2024/1689, 12 July 2024.\n- [14] 2025. Artificial intelligence - AI System Logging.\n- [15] 2025. Sectorial AI Testing and Experimentation Facilities under the Digital Europe Programme. https://digital-strategy.ec.europa.eu/en/policies/ testing-and-experimentation-facilities.\n- [16] Adam J Andreotta, Nin Kirkham, and Marco Rizzi. 2022. AI, big data, and the future of consent. Ai &amp; Society 37, 4 (2022), 1715-1728.\n- [17] Julien Arnal. 2024. AI at Risk in the EU: It's Not Regulation, It's Implementation. European Journal of Risk Regulation (2024). https://www.cambridge.org/core/journals/european-journal-of-risk-regulation/article/ai-at-risk-in-the-eu-its-not-regulation-itsimplementation/A9FD120F3EACE2C083048ABCBF96C0F6\n- [18] Ali Basiri, Casey Rosenthal, Nora Jones, Andrew Hodges, and Cole Mickens. 2016. Chaos Engineering. IEEE Software 33, 3 (2016), 35-41.\n- [19] Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, et al. 2024. Managing extreme AI risks amid rapid progress. Science 384, 6698 (2024), 842-845.\n- [20] Miles Brundage and et al. 2020. Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims. arXiv preprint arXiv:2004.07213 (2020).\n- [21] CEN-CENELEC Joint Technical Committee 21. 2025. European AI Standardization | CEN-CENELEC JTC 21. https://jtc21.eu.\n- [22] Chairs and Vice-Chairs of the General-Purpose AI Code of Practice. 2025. General-Purpose AI Code of Practice, Third Draft . Technical Draft Draft 3. European AI Office / European Commission, Brussels, Belgium. https://digital-strategy.ec.europa.eu/en/library/third-draft-general-purpose-aicode-practice-published-written-independent-experts Third draft published for consultation; feedback invited until 30 March 2025, final version expected in May 2025. .\n- [23] Daswin De Silva and Damminda Alahakoon. 2022. An artificial intelligence life cycle: From conception to production. Patterns 3, 6 (2022).\n- [24] Deloitte. 2024. EU AI Act Survey: Uncertainty in Implementation. Deloitte Legal Research (2024). https://www.deloitte.com/dl/en/services/legal/ research/umfrage-eu-ai-act-2024.html\n- [25] Mario Draghi. 2024. EU Competitiveness Report (Draghi Report). https://sciencebusiness.net/news/ai/eu-losing-narrative-battle-over-ai-act-saysun-adviser\n- [26] European Commission. 2025. AI Factories - Shaping Europe's Digital Future. https://digital-strategy.ec.europa.eu/en/policies/ai-factories.\n- [27] European Commission. 2025. Draft -Implementing Act on AI regulatory sandboxes under the Artificial Intelligence Act. DraftImplementingActAIregulatorysandboxes.\n- [28] European Union. 2016. Regulation (EU) 2016/679 (General Data Protection Regulation). Official Journal of the EU, L119.\n\n- [29] Luciano Floridi, Josh Cowls, and et al. 2018. AI4People: An Ethical Framework for a Good AI Society. Minds and Machines 28, 4 (2018), 689-707.\n- [30] Julio Hernandez, Delaram Golpayegani, and Dave Lewis. 2025. An open knowledge graph-based approach for mapping concepts and requirements between the eu ai act and international standards. AI and Ethics (2025), 1-12.\n- [31] High-Level Expert Group on Artificial Intelligence. 2019. Ethics Guidelines for Trustworthy AI. https://digital-strategy.ec.europa.eu/en/library/ethicsguidelines-trustworthy-ai Accessed: 2025-05-25.\n- [32] Ari Holtzman, Peter West, and Luke Zettlemoyer. 2025. Generative Models as a Complex Systems Science: How Can We Make Sense of Large Language Model Behavior? Journal of Social Computing 6, 2 (June 2025), 75-94. doi:10.23919/JSC.2025.0009\n- [33] Ken Huang, Aditi Joshi, Sandy Dun, and Nick Hamilton. 2024. AI regulations. (2024), 61-98.\n- [34] International Organization for Standardization and International Electrotechnical Commission. 2023. ISO/IEC 42001:2023 - Artificial intelligence Management system. https://www.iso.org/standard/81230.html. First AI Management System Standard, supporting transparency, fairness, and accountability.\n- [35] ISACA. 2025. AAIA Official Review Manual . ISACA, Rolling Meadows, IL. Print version, 182 pages; first released May 19, 2025.\n- [36] Noam Kolt, Michal Shur-Ofry, and Reuven Cohen. 2025. Lessons from Complex Systems Science for AI Governance. Patterns 6, 8 (Aug. 2025), 101341. doi:10.1016/j.patter.2025.101341\n- [37] David Leslie, Christopher Burr, Mhairi Aitken, Josh Cowls, Michael Katell, and Morgan Briggs. 2020. Human Rights, Democracy and the Rule of Law in the Age of Artificial Intelligence. https://search.coe.int/cm/Pages/result\\_details.aspx?ObjectID=09000016809c4bd1\n- [38] Dave Lewis, Maria Lasek-Markey, Donya Golpayegani, and Harshvardhan J. Pandit. 2025. Mapping the Regulatory Learning Space for the EU AI Act. arXiv preprint arXiv:2503.05787. https://arxiv.org/abs/2503.05787\n- [39] T. Nathan Mundhenk, Barry Y. Chen, and Gerald Friedland. 2020. Efficient Saliency Maps for Explainable AI. arXiv:1911.11293 [cs.CV] https: //arxiv.org/abs/1911.11293\n- [40] National Institute of Standards and Technology (NIST). 2025. AI Risk Management Framework (AI RMF). https://www.nist.gov/itl/ai-riskmanagement-framework.\n- [41] Claudio Novelli, Federico Casolari, Antonino Rotolo, Mariarosaria Taddeo, and Luciano Floridi. 2024. Taking AI risks seriously: a new assessment model for the AI Act. Ai &amp; Society 39, 5 (2024), 2493-2497.\n- [42] DLA Piper. 2025. The European Commission Considers Pause on AI Act's Entry into Application. AI Outlook Report (2025). https://www.dlapiper. com/en/insights/publications/ai-outlook/2025/the-european-commission-considers-pause-on-ai-act-entry-into-application\n- [43] Thibault Schrepel. 2025. Adaptive Regulation. social science research network:5416454 doi:10.2139/ssrn.5416454\n- [44] Douglas Schuler and Aki Namioka. 1993. Participatory design: Principles and practices . CRC press.\n- [45] Thomas B Sheridan. 1992. Telerobotics, automation, and human supervisory control . MIT press.\n- [46] Nathalie A Smuha. 2021. From a 'race to AI'to a 'race to AI regulation': regulatory competition for artificial intelligence. Law, Innovation and Technology 13, 1 (2021), 57-84.\n- [47] UNESCO. 2021. Recommendation on the Ethics of Artificial Intelligence. https://unesdoc.unesco.org/ark:/48223/pf0000381137. Adopted on 23 November 2021 by the General Conference of UNESCO at its 41st session.\n- [48] unknown. 2025. Regulating Uncertainty: Governing General-Purpose AI Models and Systemic Risk. European Journal of Risk Regulation (2025). https://resolve.cambridge.org/core/journals/european-journal-of-risk-regulation/article/regulating-uncertainty-governing-generalpurposeai-models-and-systemic-risk/7EEFE1D8421A43A98CE91F7C697DE538\n- [49] Lei Wang, Zhengchao Liu, Ang Liu, and Fei Tao. 2021. Artificial intelligence in product lifecycle management. The International Journal of Advanced Manufacturing Technology 114, 3 (2021), 771-796.\n- [50] Yue Wang and Sai Ho Chung. 2022. Artificial intelligence in safety-critical systems: a systematic review. Industrial Management &amp; Data Systems 122, 2 (2022), 442-470.\n- [51] World Wide Web Consortium (W3C). 2023. Web Content Accessibility Guidelines (WCAG) 2.2 . Technical Report. World Wide Web Consortium. https://www.w3.org/TR/WCAG22/ W3C Recommendation.\n- [52] Bishoy Zaki. 2025. Conceptualising Organisational Policy Learning: Triggers, Processes, Outcomes, and Implications for Policy and Governance Change. Australian Journal of Public Administration (Nov. 2025). doi:10.1111/1467-8500.70031", "fetched_at_utc": "2026-02-09T13:40:38Z", "sha256": "a2b99c97c526dd5047337c944fd67504f8a1b44cbb763b82ddd7b1e2d5858b28", "meta": {"file_name": "Assessing High-Risk AI Systems under the EU AI Act - From Legal Requirements to Technical Verification.pdf", "file_size": 624377, "mtime": 1770643057, "docling_errors": []}}
{"doc_id": "pdf-pdfs-bsi-eu-ai-act-whitepaper-final-2-9-24-2ca62ebf0cb4", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\BSI_EU_AI_Act_Whitepaper_Final_2_9_24.pdf", "title": "BSI_EU_AI_Act_Whitepaper_Final_2_9_24", "text": "## Artificial Intelligence Act\n\nWhat AI providers and   deployers needÂ to know.\n\n<!-- image -->\n\n## Disclaimer\n\nThe AI Act text used for the analysis is the text voted in the Plenary of the European Parliament on the 13th of Mar 2024   (P9\\_ TA(2024)0138 Artificial Intelligence Act). This paper is BSI's interpretation of the AI Act and is currently not legally binding.\n\nArtificial Intelligence Act (AI Act)\n\nWhat AI providers and deployers needÂ to know.\n\nAuthors: Alex Zaretsky * , Daniela Seneca * , Inma PÃ©rez * ,\n\nSarah Mathew * , Aris Tzavaras ** .\n\n* Regulatory Lead, Artificial Intelligence Notified Body, BSI group.\n\n** Head of Artificial Intelligence Notified Body, BSI group.\n\n<!-- image -->\n\n<!-- image -->\n\nWe did not wake up to a world where Artificial Intelligence (AI) was just born. The genesis of AI as an idea is evident from ancient times in the form of myths. 1 However, the term 'AI' more recently has been attributed to John McCarthy of the Massachusetts Institute of Technology (MIT) who first suggested the concept at a 1956 conference at Dartmouth College.\n\nThe technological evolution permitted AI to become accessible, both in terms of computation power as well as in terms of the tooling and availability of digital data for facilitating development of AI systems.\n\nSince 1956, AI has shown significant progress in performing 'narrow' tasks, in most cases, better than the average human and, in some, better than experts. A landmark victory of AI's progress became clear when the Deep Blue expert system played chess against the world champion Garry Kasparov in the 90s. 2\n\nNow, why is there an increasing global concern to regulate and/or control AI? The short answer to this question is that we may lose to an opponent we created. Society cannot afford to leave AI unregulated as this could lead to the misuse of this technology. AI also needs vast amounts of data to become increasingly intelligent, and there is the risk that fundamental rights would be violated. For example, an algorithm that processes profiles to evaluate candidates for a job position, may be biased against people of a certain ethnicity, limiting exposure of their profiles for opportunities. These algorithmic biases have serious real-world implications. In this context, to prevent any form of manipulation or biased outcome, several regions are leaning towards AI regulation.\n\n1 Stanford researcher examines earliest concepts of artificial intelligence, robots in ancient myths\n\n2\n\n20 Years after Deep Blue: How AI Has Advanced Since Conquering Chess\n\n<!-- image -->\n\nLet's not forget about the geopolitics linked to regulating AI. The AI industry is growing at an extremely rapid pace and AI has become of strategic importance for governments across the world. Countries are competing to win the 'AI race' and those able to successfully lead on AI innovation will be well positioned in global affairs.\n\nRegulating a technology sector is not something new. Typical examples are regulations bestowed on the pharmaceutical and medical industries, as well as on more abstract technologies like those processing our digital data. 3  Justifications behind market regulation includes market acceleration and harmonization as well as protecting consumers from the negative effects of such technologies. In the context of AI systems, harms may arise from the deployment of AI and its after effects. Therefore, such justifications can be used to limit AI. 4\n\nReasons to regulate AI may differ across regions. The European Union's (EU) approach, for instance, has been characterized by its focus on the protection of human rights - or as it is called in Europe, fundamental rights. Those rights are enshrined in the Charter of Fundamental Rights of the EU and in other binding legislation such as the General Data Protection Regulation (GDPR). As technology is becoming an ever more central part of citizens' life, the EU understands that trust, is a prerequisite for AI uptake in Europe. We use, as consumers, products and services coming from 'unfamiliar sources' and we need to have the assurance that those products are safe, trustworthy, and ethical for us to consume. Regulations set the\n\n<!-- image -->\n\n<!-- image -->\n\nbasic 'rules of the game', ensuring consumers that unfamiliar sources deliver a product or service that obeys those rules. In most cases, legislation goes beyond the first entry of a product to the market, they additionally dictate the need for monitoring the product while in use and to deliver feedback to relevant authorities and action when something goes wrong; this is known as Post Market Surveillance (PMS).\n\nThe use of AI comes with more sophisticated and nuanced challenges: some philosophical, some practical. Due to the increasing concerns about the adverse impact that AI systems may have on individuals, EU lawmakers have the challenge of ensuring that AI is used for good. But what is ''good AI''? This quest is relatively new, however, defining what is 'good' has been a long-standing question with different answers depending on the moral theory that you consider. 5  For this reason, since human rights are universally recognized, the EU decided to take a human-centric based approach to AI governance 6  and, in April 2019, the European Commission published its conceptualization of ''good AI'': the Ethics Guidelines for Trustworthy AI. 7  The non-binding nature of these guidelines were criticized, however, in 2021, the EU Commission published the proposal for an Artificial Intelligence Act (AI Act) that largely codifies the ethics requirements proposed by the High-Level Expert Group on AI in its Guidelines. 8  Since then, the final text has gained political agreement and has been voted by the EU Parliament in March 2024.\n\n3 'Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation)', OJ L 119 (2016).\n\n4 Chesterman, Simon, From Ethics to Law: Why, When, and How to Regulate AI (April 29, 2023). Forthcoming in The Handbook of the Ethics of AI edited by David J. Gunkel (Edward Elgar Publishing Ltd.), NUS Law Working Paper No. 2023/014\n\n5 Smuha, Nathalie A., Beyond a Human Rights-based approach to AI Governance: Promise, Pitfalls, Plea (February 1, 2020). Published in Philosophy &amp; Technology, 2020\n\n6 Idem from footnote 4, p.5\n\n7 High-Level Expert Group on AI, 'Ethics Guidelines for Trustworthy AI', 8 April 2019\n\n8 European Parliament, 'EU AI Act: first regulation on artificial intelligence'\n\nThe AI Act is a 'horizontal' legislation as it does not target a specific industry sector but rather any industry that uses AI. The AI Act sets requirements that products must comply with, as well as obligations for all parties involved (economic operators). The horizontal nature of this legislation is envisioned to 'build on' sectorial legislations, regulating only the AI aspects of those products. Furthermore, because the AI Act is technology agnostic, it does not prescribe specific rules for specific types of AI techniques, with the exemption of General-Purpose AI systems (GPAI).\n\nTo determine if the upcoming legislation is applicable to one's product, one must first define whether their product is or uses AI. Unfortunately, the definition of AI has been the 'holy grail' of the last decades, as there is no globally acceptable definition of 'intelligence.' The above-mentioned 'AI race' has forced governments as well as supranational and intergovernmental organizations to attempt to find a common definition of AI.\n\nFor instance in the AI Act, the EU ultimately suggested a definition aligned to the Organization for Economic Co-operation and Development (OECD)'s AI definition, 9  ensuring the text 'distinguish[es] it from simpler traditional software systems or programming approaches and should not cover systems that are based on the rules defined solely by natural persons to automatically execute operations.' 10\n\nThe AI Act defines an AI system in Article 3 as: ' a machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. '\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nThere are two key actors caught under the scope of the AI Act which are providers 11  and deployers 12 of AI systems. Moreover, certain obligations have been introduced for importers, distributors, product manufacturers and authorized representatives of providers. The AI Act states 13  that the regulation applies to providers and deployers of AI systems established in the EU, as well as externally, to any provider and deployer of AI systems outside the EU, if the output of the AI system is used within the EU. What 'output' means is not defined under the AI Act, however, the definition of AI system refers to outputs in the form of content (generative AI systems) including text, video, or image 14  and predictions, recommendations, or decisions that can influence physical or virtual environments. 15 The key thing to ask here is whether the impact of the AI system occurs within the EU, regardless of where the provider and deployer is established.\n\nSimilar to the GDPR, one of the most important consequences of the AI Act extraterritorial scope is that it will impose significant obligations on non-EU businesses, even if they do not have a legal presence in the EU. The rationale behind this approach is linked to the EU's growing concern on how authoritarian governments use AI and its potential impact on the rights and freedoms of individuals. Consequently, the AI Act aims to level the playing field and make the AI Act applicable in a non-discriminatory manner.\n\nFurthermore, it is important to mention that the AI Act will potentially become another example of the phenomenon called the 'Brussels Effect,' a concept originally coined by Anu Bradford, 16  a professor at Columbia University. The 'Brussels effect' refers to 'the EU's unilateral power to regulate global markets.'\n\n11 Article 3(3) of P9\\_TA(2024)0138 AI Act defines providers as 'a natural or legal person, public authority, agency or other body that develops an AI system or a general purpose AI model or that has an AI system or a general purpose AI model developed and places them on the market or puts the system into service under its own name or trademark, whether for payment or free of charge'\n\n12 Article 3(4) of AI Act defines deployers as 'any natural or legal person, public authority, agency or other body using an AI system under its authority except where the AI system is used in the course of a personal non-professional activity'\n\n13 See Article 2 of AI Act\n\n14 Throughout the text we see that the AIA includes image, audio or video as example of AI-generated content.\n\n15 See Article 3 and Recital 12 of AI Act\n\n16 Bradford, Anu. (2020). The Brussels Effect: How the European Union Rules the World.\n\n<!-- image -->\n\nSome may argue that the upcoming AI Act Regulation will hamper AI innovation in Europe as it is more stringent in comparison to other more flexible countries that encourage self-regulation and voluntary commitments. However, for economic operators, it is more beneficial to adopt a uniform global standard rather than adhering to multiple, including laxer, regulatory standards, as this brings legal certainty. This would be the case for those organizations operating globally, who have multiple production locations where it is not legally, technically, or even economically viable, for the company to comply with multiple regulatory regimes. 17  This business approach would explain why so many large non-EU companies follow the GDPR and many other EU environmental regulations across their global operations.\n\nLet's not also forget that the EU was the first mover when it comes to regulating AI. It is true that due to the slow pace of the EU legislative process, there have been delays in the AI Act negotiations and other jurisdictions have adopted comprehensive regulation for parts of the AI ecosystem before the EU. 18  For example, China has been one of the first countries to implement AI regulations, including new rules on the use of recommendation algorithms. 19  Despite this, the AI Act has been the first draft to be published and this may be the reason why other jurisdictions have sped up their legislative processes.\n\n<!-- image -->\n\n<!-- image -->\n\nTaking the success of the GDPR adoption as an example, the EU plans to promote its blueprint on AI globally. 20  It is fair to assume that the AI Act's human-centered approach, strong focus on ethics, transparency and fundamental rights, will serve as an inspiration to like-minded countries. Especially after seeing over 100 countries today with GDPR-like data privacy rules. 21\n\n17 Bradford, Anu, The Brussels Effect (2012). Northwestern University Law Review, Vol. 107, No. 1, 2012, Columbia Law and Economics Working Paper No. 533\n\n18 Siegmann, Charlotte, Anderljung, Markus, The Brussels effect and Artificial Intelligence: How EU regulation will impact the global AI market (2022). Centre for the Governance of AI. P.39\n\n19 See translation of the text here Translation: Internet Information Service Algorithmic Recommendation Management Provisions - Effective March 1, 2022 (stanford.edu)\n\n20\n\nSiegmann, Charlotte, Anderljung, Markus, The Brussels effect and Artificial Intelligence: How EU regulation will impact the global AI market (2022). Centre for the Governance of AI. P.3\n\n21 See interview to Bradford What is the Brussels Effect, and what does it mean for global regulation? (microsoft.com)\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nTo sum up, the AI Act will have an extraterritorial impact on AI providers and deployers in non-EU jurisdictions, if their AI systems and/or outputs are used within the EU.\n\n<!-- image -->\n\nMoreover, its existing extraterritorial scope has the potential to become the gold standard when it comes to AI governance. Experience has proved that European values have a broad appeal, and it is fair to assume that the AI Act will be globally widespread.\n\n<!-- image -->\n\n<!-- image -->\n\nCompanies operating globally may be glad to follow only one set of rules, even if they are more stringent. However, the extraterritorial scope of the AI Act might raise compliance challenges, especially to those AI providers established in third countries if they are not aware that the output of their AI system will be used in the EU.\n\nTherefore, all relevant operators need to understand which role they play along the AI value chain and properly determine the scope of their AI systems to see if they fall within the scope of the AI Act.\n\n<!-- image -->\n\nAs we have seen in this whitepaper, the AI Act's definition of AI is close to the one proposed by the OECD and this seems to be an advantage as it maintains a semantic alignment with international partners. The EU believes that this definition gives a clear criterion for differentiating AI systems from traditional software, thus ensuring a proportionate regulatory approach. However, this definition has drawn much criticism for still remaining too broad. In any case, it is important to understand that not all AI technologies defined as an AI system under the AI Act will be subject to obligations, however one must consider the degree of risk they pose to the health, safety, and fundamental rights of individuals.\n\nThe EU believes that a risk-based approach is important to help ensure that the regulatory intervention is proportionate. 22 To that end, the AI Act distinguishes between AI systems posing (i) unacceptable risk, (ii) high risk, (iii) limited risk, and (iv) low or minimal risk.\n\nThe Commission judges the level of risk by the likelihood that the system may harm the health and safety of specific individuals, and/or potentially violate their fundamental rights. The obligations imposed on such systems range from prohibitions to the voluntary codes of conduct.\n\nThe AI Act proposes prohibitions on AI applications that pose 'unacceptable risks' to people's safety, health and rights. 23\n\n22 European Commission, White Paper on Artificial Intelligence - A European approach to excellence and trust, COM(2020) 65 final, 2020. P. 17\n\n23 See Article 5 of AI Act\n\n<!-- image -->\n\nThe AI Act considers these practices to be harmful and abusive and should be prohibited because they contradict Union values. 24  Accordingly, these systems would be prohibited to be placed on the market, put into service, or used in the EU:\n\n- 1 AI systems that deploy harmful manipulative 'subliminal techniques.' 25\n- Example: 'An inaudible sound is played in truck drivers' cabins to push them to drive longer than healthy and safe. AI is used to find the frequency maximising this effect on drivers.' 26\n- 2 AI systems that exploit specific vulnerable groups (due to their age, physical or mental disabilities). 27\n- Example: 'A doll with an integrated voice assistant encourages a minor to engage in progressively dangerous behaviour or challenges in the guise of a fun or cool game.' 28\n- 3 AI systems used for social scoring purposes for public and private purposes- in particular, to classify the reliability of people based on their social behaviour or personality traits. 29\n- Example: 'An AI system identifies at-risk children in need of social care based on insignificant or irrelevant social 'misbehaviour' of parents, e.g. missing a doctor's appointment or divorce.' 30\n- 4 Biometric categorization of natural persons based on biometric data to deduce or infer their race, political opinions, trade union, membership, religious or philosophical beliefs, sex life or sexual orientation. 31\n- Example: 'AI systems that infer 'criminality' based on data about people's facial structure or biological characteristics, for example, the colour of the skin.'\n\n24 See recital 28 of AI Act\n\n25 See Article 5(1)(a) of AI Act\n\n26 For the sake of clarity, the Commission has presented some examples of the above prohibitions. Some argue that these are borderline fantastical, however , being AI such an innovative technology, who knows where it will take us. See https://cor .europa.eu/en/events/Documents/SEDEC/FINAL%20PDF%20AI%20Presentatiofor%20COR%20Sedec%20Committee%20 meeting%2023%2006%2021.pdf\n\n27 See Article 5(1)(b) of AI Act\n\n28  See https://cor .europa.eu/en/events/Documents/SEDEC/FINAL%20PDF%20AI%20Presentatiofor%20COR%20Sedec%20Committee%20meeting%2023%2006%2021.pdf\n\n29 See Article 5(1)(c) of AI Act\n\n30  See https://cor .europa.eu/en/events/Documents/SEDEC/FINAL%20PDF%20AI%20 Presentatiofor%20COR%20Sedec%20Committee%20meeting%2023%2006%2021.pdf\n\n31 See Article 5(1)(g) of AI Act\n\n32 See Article 5(1)(h) of AI Act\n\n33 See Article 5(1)(d) of AI Act\n\n34 See Recital 42 of AI Act\n\n<!-- image -->\n\n<!-- image -->\n\n- 5 Real-time remote biometric identification in publicly accessible spaces by law enforcement. 32\n- Example: 'All faces captured live in a public space by video cameras checked, in real time, against a database to identify a criminal in the crowd.'\n- 6 Individual predictive policing; except for law enforcement if based on objective and verifiable facts. 33\n- Example: 'AI-predicted behaviour based solely on their profiling, personality traits or characteristics, such as nationality, place of birth, place of residence, number of children, debt, their type of car, without a reasonable suspicion of that person being involved in a criminal activity based on objective verifiable facts and without human assessment thereof.' 34\n\n<!-- image -->\n\n- 7 Emotion recognition in the workplace and education institutions, unless for medical or safety reasons (i.e. AI systems used in detecting the state of fatigue of professional pilots or drivers for the purpose of preventing accidents). 35\n- Example : 'AI recruitment tools that assess a candidate's emotional state or truthfulness through analysis of facial expressions, voice modulation, or body language during interviews.'\n- 8 AI systems using indiscriminate scraping of biometric data from the internet or CCTV footage to create facial recognition databases. 36\n- Example: 'AI system that collects facial images from social media without any specific targeting or consent, amassing a vast database of faces.'\n\nHowever, it is important to mention that the use of real-time remote biometric identification in point 5 above has some exceptions related to the safety of society as a whole. In particular, the use of real-time remote biometric identification systems (such as facial recognition) in public spaces for law enforcement purposes will be allowed when the use of such systems can be justified by 'three exhaustively listed and narrowly defined situations.' These narrowly\n\n35 See Article 5(1)(f) of AI Act\n\n36 See Article 5(1)(e) of AI Act\n\n37 See Article 5(1)(h)(i) of AI Act\n\n38 See Article 5(1)(h)(ii) of AI Act\n\n39 The list of the 16 crimes in Annex II of AI Act: Terrorism; Trafficking in human beings; Sexual exploitation of children and child sexual abuse material; Illicit trafficking in narcotic drugs and psychotropic substances; Illicit trafficking in weapons, munitions and explosives; Murder; Grievous bodily injury; Illicit trade in human organs and tissue; Illicit trafficking in nuclear or radioactive materials; Kidnapping, illegal restraint and hostage-taking; Crimes within the jurisdiction of the International Criminal Court; Unlawful seizure of aircraft/ships; Rape; Environmental crime; Organised or armed robbery; Sabotage, participation in a criminal organisation involved in one or more crimes listed above.\n\n40 See Article 5(1)(h)(iii) of AI Act\n\n41 See Article 26 (10) of AI Act\n\n<!-- image -->\n\ndefined exceptions cover a rather broad range of situations:\n\n- a 'targeted search for specific victims of abduction, trafficking in human beings or sexual exploitation of human beings, as well as searching for missing persons.' 37\n- the 'prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons or (...) of a terrorist attack.' 38\n- the 'localisation or identification' of a person suspected of having committed a crime 39  with a maximum sentence of at least 4 years that would allow for the issuing of a European Arrest Warrant. 40\n\nTherefore, if the above is fulfilled, real-time biometric identification by law enforcement authorities may be permitted, if it is accompanied by safeguards for fundamental rights, including the ex-ante involvement of judicial authorities and prior fundamental rights impact assessment (unless in duly justified situations of urgency).\n\nThese safeguards will also be mandatory for the usage of AI systems for post remote biometric identification of persons under investigation. 41 An example of this would be the use of biometric surveillance to analyse footage during a protest to\n\nidentify an individual that has committed a crime.\n\nIt is important to state that the above prohibition does not ban actors from using remote biometric identification for non-law enforcement purposes.\n\nThis means that private entities may use such systems (e.g. marketplaces, public transport and even schools) if they go through a third-party conformity assessment or comply with harmonized European standards that are to be published later on.\n\nMoving on to the next category under the AI Act, 'high-risk AI systems' are those systems that create adverse impact on people's health and safety or their fundamental rights in a number of defined applications, products and sectors. This is the main focus of the regulation.\n\nBefore going into more detail, it is important to clarify that AI systems can be used on a standalone basis or as a component of a product, irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). 42\n\nThe high-risk regime is based on the intended purpose of the AI system, in line with the New Legislative Framework (NLF), a common EU approach to the regulation of certain products such as machinery, lifts, medical devices, personal protective equipment and toys. 43\n\nThe AI Act distinguishes between two categories of high-risk AI systems:\n\n- 1 AI systems that are products or safety components of products covered by certain Union health and safety harmonization legislation (such as toys, machinery, lifts, or medical devices) and are required to undergo a third party conformity assessment. 44\n- 2 Stand-alone AI systems deployed in eight specific areas: 45\n- a. biometric identification, categorization and emotion recognition (outside prohibited categories); Example: AI systems used for facial recognition.\n- b. management and operation of critical infrastructure; Example: AI systems used in road traffic, the supply of water, gas, heating, and electricity.\n\n42 See Recital 12 of AI Act\n\n43  See New legislative framework - European Commission (europa.eu)\n\n44 See Annex I of AI Act with the list of NLF legislation. Annex I (B) is older-style product safety legislation where Title XII introduces new AI Act-related considerations for future delegated acts in those areas.\n\n45 See the list in AI Act\n\n46 See recital 33 of AI Act\n\n47 See Articles 7 and AI Act\n\n48 See Article 6 (3) of AI Act\n\n49 See Article 6 (3) of AI Act\n\n50 See Article 6 (3) of AI Act\n\n<!-- image -->\n\n- c.    educational and vocational training; Example: AI systems used in evaluating students on tests required for university admission.\n- d.    employment, worker management and access to self-employment; Example: AI systems used to place targeted job advertisements, to analyse and filter job applications, and to evaluate candidates.\n- e.    access to and enjoyment of essential services and benefits; Example: AI systems used for creditworthiness evaluation of natural persons.\n- f. law enforcement; Example: AI systems used for detection, investigation, or prosecution of criminal offenses.\n- g.    migration, asylum and border management; Example: AI systems that identify a person who, during an identity check, either refuses to be identified or is unable to state or prove his or her identity . 46\n- h.    administration of justice and democracy; Example: AI systems aimed at helping analyse and interpret facts regarding judicial authority.\n\nThe above list of high-risk AI systems may be updated over time as the EU Commission may modify or add additional use cases if they pose similar risks to the uses currently on the list. However, it can remove areas if they do no longer pose a significant risk to health, safety and fundamental rights. 47\n\nA stand-alone AI system can be classified as high-risk if it falls under any of the eight specific areas. However, as always, the AI Act has exceptions. If the system does not pose significant harm to the health, safety, or fundamental rights of people, including not materially influencing the outcome of decision-making, it may be exempt. This would be the case of the following systems: 48\n\n- Intended to perform a narrow procedural task. 49\n- Intended to improve the result of a previously completed human activity. 50\n\n- Intended to detect decision-making patterns or deviations from prior decision-making patterns. 51\n- Intended to perform a task that is only prepara tory to an assessment relevant for the purpose of the high-risk use cases in Annex III. 52\n\nIt is important to note that an AI system will always be considered at a minimum high-risk if the AI system performs profiling of natural persons. 53\n\nThe next type of AI system risk level is 'limited risk'. These are systems that have special disclosure obligations due to their particular interaction with humans given that they may pose the risk of manipulation. These include AI systems that generate or manipulate image, audio or video content (i.e. deep fakes 54 ), AI systems that are intended to interact with people (e.g. chatbots), and AI-powered emotion recognition systems and biometric categorization systems.\n\nWith this inclusion, the AI Act ensures that EU customers make informed decisions as they are aware that they are interacting with a machine. 55\n\nFinally, the last category is 'minimal risk'. These AI systems do not fit in any of the other categories and present only low or minimal risk. They can be developed and used within the EU without conforming to any additional legal requirements. However, the AI Act envisages the creation of codes of conduct to encourage providers of non high-risk AI systems to voluntarily apply the mandatory requirements for high-risk AI systems. These codes of conduct should be based on clear objectives and key performance indicators to measure the achievement of those objectives. 56  This could include elements around inclusiveness, fairness, transparency, confidentiality, and environmental sustainability.\n\n51 See Article 6 (3) of AI Act\n\n52 See Article 6 (3) of AI Act\n\n53 See Article 6 (3) of AI Act\n\n54 See Article 3(6) of of AI Act\n\n55 See Article 50 of AI Act\n\n56 See Recital 165 of AI Act\n\n57 See Article 95 of AI Act\n\n<!-- image -->\n\nThe Commission and Member States will encourage the creation and voluntary compliance with these codes. 57\n\nIt is important to underline that existing EU law, such as the GDPR, still applies when the use of AI systems falls within the scope of that law, no matter if classified as no risk under the AI Act.\n\n<!-- image -->\n\n<!-- image -->\n\nGiven the horizontal nature of the AI Act, all AI systems across sectors are subject to the same risk-assessment criteria and legal requirements.\n\nThe AI Act interrelates with other EU legal instruments, for example rules on data protection, privacy, civil liability or sectorial law such as the Machinery Regulation or the Medical Devices Regulation (MDR). This horizontal approach prevents companies from 'shopping around' between sectors and thus ensuring that all players conform to the same legal requirements. This approach might seem preferable as it is uniform, stable, and fair across industries. However, it is important to say that, if this approach is not properly addressed, it may lead to conflicting obligations and procedures for AI providers.\n\n## The AI Act draws on the New Legal Framework (NLF) regime, 58  designed to improve the EU\n\ninternal market, and increase the quality of conformity assessment of certain products such as medical devices, machinery, or toys. As described by Veale and Zuiderveen Borgesius, 59 'under NLF\n\n58  New legislative framework, European Commission\n\n59 Veale, Michael and Zuiderveen Borgesius, Frederik, Demystifying the Draft EU Artificial Intelligence Act (July 31, 2021). Computer Law Review International (2021) 22(4) 97-112, P.6\n\n60 Union harmonization legislation refers to Union legislation that harmonizes the conditions for the marketing of product. This list can be found here and also in Annex II of AI Act\n\n61 See Recital 64 of P9\\_TA(2024)0138 Artificial Intelligence Act\n\n62 Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (MDR).\n\n<!-- image -->\n\nregimes, a manufacturer must undertake pre-marketing controls undertaken to establish products' safety and performance, through conformity assessment to certain essential requirements laid out in law. Manufacturers then mark conforming products with 'CE'; marked products enjoy EU freedom of movement.'\n\nThe AI Act acknowledges that a single AI system may be affected by different Union Harmonization legislation. 60 61  For example, a medical device product incorporating AI might present risks not addressed by the Medical Devices Regulation (MDR). 62  This calls for a simultaneous and complementary application of several EU laws.\n\nThe NLF legal acts are built on the legal concept that whenever a matter is regulated by two rules, the more specific one should be applied first. 63  With this, it is ensured that products incorporating AI are not subject to a double regulatory burden. This was the intention of the Commission when it proposed the AI Act:\n\n'To achieve those objectives, this proposal presents a balanced and proportionate horizontal regulatory approach to AI that is limited to the minimum necessary requirements to address the risks and problems linked to AI, without unduly constraining or hindering technological development or otherwise disproportionately increasing the cost of placing AI solutions on the market.' 64\n\nIf we take AI-enabled medical devices as an example, the AI Act tries to ensure consistency, avoid duplications, and minimize additional burdens associated with the cumulative application of the AI Act and MDR. It allows AI providers to integrate the necessary measures to comply with the AI Act into the procedures and documents already required under MDR. 65  In practice, this means that the AI-enabled medical device manufacturer would be allowed to integrate the testing and reporting processes, information and documentation required under the AI Act into the already existing documentation and procedures required under the MDR. This is because the MDR is considered the more specific rule and, therefore, will take precedence.\n\nHowever, as previously mentioned, there are certain tensions and inconsistencies that make it difficult to apply the AI Act in conjunction with other Union harmonization laws.\n\nFor example, as seen in the previous section, the AI Act categorizes as high-risk AI systems those that are products or safety components of products already covered by Union harmonization legislation. For AI-enabled medical devices both, the AI Act and MDR, would be applicable to the same product. The problem here is that there is no definition of 'safety component' under the MDR and it is not clear for a medical device what a 'safety component' is. 66\n\nAdditionally, the AI Act interplays not only with Union Harmonization law, but also with other horizontal legislation, such as the GDPR. 67\n\nThe AI Act makes several references to the GDPR throughout the text and assures that it is without prejudice and complements the GDPR. Accordingly, both regulations apply side by side. In practice, this means that all AI systems must strictly adhere to the GDPR if they use personal data belonging to EU citizens, or plan to be deployed for usage within the EU. However, again there are some tensions when it comes to an AI system processing personal data.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFor example, the AI Act states that it does not provide legal grounds for processing personal data and refers back to the GDPR to find a justifiable ground for this processing. 68  However, it does not give clarity on how to apply the GDPR's requirements for collecting and processing personal data.\n\nIn this context, it is difficult to find a legal ground for the processing of personal data by Large Language Models (LLMs) as there is no controller/data subject relationship (e.g. contract), nor can the data subject expect their data to be used as training data for an app, and there is no possibility for the data subject to object to such processing (e.g. no explicit consent). In some cases, AI can handle personal data based on the justified grounds of 'legitimate interest.' 69  Nevertheless, this needs to be balanced to ensure that the data subject's rights are not compromised.\n\nIt is expected that the Commission will issue guidelines clarifying how to train AI models without violating personal data protection rules,\n\n68 See Recital 63 of AI Act\n\n69 See Article 6(1)(f) Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (GDPR)\n\n70 Regulation (EU) 2019/881 on ENISA and on information and communications technology cybersecurity certification and repealing Regulation (EU) No 526/2013 (Cybersecurity Act).\n\n71 See Art 42(2) of AI Act\n\n72 Established in 2004 and strengthened by the EU Cybersecurity Act, the European Union Agency for Cybersecurity, ENISA contributes to EU cyber policy, enhances the trustworthiness of ICT products, services and processes with cybersecurity certification schemes.\n\n73 ENISA: AI Cybersecurity Challenges - Threat Landscape for Artificial Intelligence. Artificial Intelligence Cybersecurity Challenges - ENISA (europa.eu)\n\n74 See Article 40 of AI Act\n\n75 See Article 41 of AI Act\n\n76  Regulation 2022/0272 (CRA).\n\n77 See Article 8 of Regulation 2022/0272 (CRA).\n\n78 Essential cybersecurity requirements are in Article 10 and Annex I of Regulation 2022/0272 (CRA).\n\n<!-- image -->\n\nincluding the Data Act, Data Governance Act and the Copyright Directive. Regarding the GDPR, legal grounds for the processing of personal data might require a significant rethink for AI systems.\n\nOn the cybersecurity side, the AI Act also overlaps with the EU Cybersecurity Act 70 , including a presumption of conformity in Article 42(2) of the AI Act. The clause acknowledges that high-risk AI systems that have been certified under a cybersecurity scheme created according to the process provided by the Cybersecurity Act 'shall be presumed to be in compliance with the cybersecurity requirements set out in Article 15 of this Regulation.' 71 Article 15 and recital 49 of the AI Act state that high-risk AI systems should perform consistently throughout their lifecycle and meet an appropriate level of accuracy, robustness and cybersecurity in accordance with the generally acknowledged state of the art.\n\nIt is fair to say that Article 15 of the AI Act is very general and does not cover the entirety of potential cyberthreats to AI-powered systems such as those identified by ENISA 72 in its 'Artificial intelligence and Cybersecurity Challenges' report. 73 Therefore, if AI developers want to ensure the highest level of cybersecurity for their AI system, they will need to either rely on the available cybersecurity schemes or, most likely, apply the harmonized standards 74  or common specifications defined by the Commission, 75 which are not available at this time.\n\nOn another note, but still within the EU cybersecurity framework, the AI Act will also interplay with the new Cyber Resilience Act (CRA). 76 Like the AI Act, this regulation is expected to enter into force in 2024. In the CRA, 77  we find a presumption of conformity with the AI Act. It states that products with digital elements classified as high-risk AI systems under the AI Act should comply with the essential cybersecurity requirements set out under the CRA. 78\n\n<!-- image -->\n\nIf those high-risk AI systems fulfil the CRA's cybersecurity essential requirements, they should be deemed compliant with the cybersecurity requirements set out in Article 15 of the AI Act, as long as those requirements are covered by the EU declaration of conformity issued under the CRA. 79\n\nMoreover, the CRA clearly states that the AI Act is the reference act and that the AI Act's conformity assessment procedures are the ones to be followed. In addition, the CRA clarifies that AI Notified Bodies under the AI Act can also control the conformity of high-risk AI systems with the CRA essential requirements. 80  However, there is an exception to this: if a high-risk AI system also falls under the CRA's scope as a 'critical product with digital elements' and to which internal control of the AI Act applies, then the conformity assessment to follow is the one under the CRA insofar as the essential cybersecurity requirements are concerned. The other aspects of the product can still follow the AI Act's internal control procedure. The reason behind this is that 'critical products with digital elements' create greater cybersecurity risks and,\n\n79 See Recital 77 of AI Act\n\n80 See Article 8(2) of Regulation 2022/0272 (CRA).\n\n<!-- image -->\n\ntherefore, the conformity assessment should always involve a third-party conformity assessment body.\n\nFinally, something important to mention is that high-risk AI providers also need to comply with accessibility requirements, including the EU directives 2016/2102 and 2019/882. The AI Act intends to ensure equal access to technology for all persons with disabilities. Therefore, AI providers will need to ensure compliance with these requirements by design.\n\nAll the above points have shown that, before placing in the EU market or putting into service a high-risk AI system, AI providers will need to consider multiple horizontal and sector-specific laws if they want to guarantee a holistic compliance of their products to EU law. Despite the contradictions and overlaps between the AI Act and other horizontal and sectorial laws, it is expected that the Commission will perform an in-depth gap analysis where it will provide clarification about the relationship between those laws.\n\n## Requirements &amp; Obligations\n\n<!-- image -->\n\nFor AI systems falling under the high-risk classification, there are stringent requirements. The legal act does not specify how to fulfill its requirements at technical level - therefore, the AI Act will be supported by a series of technical specifications produced by European Standardization Organizations (ESOs) 81  following a mandate by the Commission. The standards will translate the AI Act's requirements into actionable steps. Although these standards are not mandatory, AI providers that follow harmonized standards adopted by CEN/ CENELEC will benefit from the 'presumption of conformity' with the AI Act. There are still a greater number of AI-specific standards under development.\n\nThe AI Act lists requirements for high-risk AI systems in Chapter III, Section 2 (articles 9 to 15). Compliance with the requirements should take into account the AI system's intended purpose and what is generally acknowledged as State of the Art (SotA). Table 1 provides a high level summary of Chapter III requirements.\n\nSotA is not a well-defined term, neither in the AI Act nor under other relevant NLF legislations. However, we can find multiple references to the SotA in the AI Act - harmonized standards, common specifications, technical standards, and codes of practice. 82\n\n81  ESO are the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC) and the European Telecommunications Standards Institute (ETSI).\n\n82 See Articles 50(2), 96 &amp; Recitals 116, 121 of AI Act\n\n<!-- image -->\n\n## Table 1: High-risk AI systems requirements 83\n\n## Requirement\n\n## Risk management (RM) system\n\n## Data and data governance\n\n83 See chapter III, Section 2 of AI Act\n\n<!-- image -->\n\n## Summary\n\n## The RM system planned and run throughout the entire lifecycle shall comprise of:\n\n- Identification and analysis of the known and foreseeable risks to health, safety, or fundamental rights.\n- Evaluation of risks, including the analysis of data gathered from the post-market monitoring system.\n- Adoption of appropriate and targeted RM measures.\n\nMost appropriate RM measures shall reduce risks as far as technically feasible, with a view to minimizing risks effectively so that each residual risk as well as the overall residual risk are acceptable, considering the context and the deployer's technical knowledge, experience, education, and training. Specific considerations are made for vulnerable persons and those under the age of 18.\n\n## Data governance practices shall concern:\n\n- Design choices.\n- Data collection processes and the original purpose of data collection.\n- Data-preparation processing operations such as annotation, labelling, cleaning, updating, enrichment, and aggregation.\n- Relevant assumptions on information that the data are supposed to measure.\n- Prior assessment of the availability, quantity, and suitability of the needed datasets.\n- Biases affecting health and safety or leading to discrimination.\n- Being free of errors and complete, to the best extent possible.\n\n## Training, validation and testing data sets shall:\n\n- To the best extent possible free of errors and complete, and having the appropriate statistical properties at the level of individual data sets (or a combination thereof), including in regards to the persons or groups on which the system is intended to be used.\n- Take into account the intended purpose, the characteristics, or the elements that are particular to the specific geographical, behavioral, or functional setting within which the system is intended to be used.\n\n## Requirement\n\n## Technical documentation\n\n## Record-keeping\n\n## Transparency and provision of information to deployers\n\n<!-- image -->\n\n## Summary\n\n## Technical documentation shall:\n\n- Be drawn up before placing on the market or put into service and shall be kept up-to date.\n- Provide national competent authorities and notified bodies with all the necessary information in a clear and comprehensive form to assess the compliance of the AI system with requirements.\n- Contain, at a minimum, the elements set out in Annex IV (amendable by Commission's delegated acts).\n\nIn the case of small and medium-sized enterprises (SMEs), including start-ups, any equivalent documentation should meet the same objectives, unless deemed inappropriate by the competent authority.\n\nWhere a high-risk AI system is placed on the market or put into service, one single technical document shall be drawn up containing all the information required under those legal acts listed in Annex I.\n\nAnnex IV describes the required content of the technical documentation.\n\n## AI systems shall technically allow for the automatic recording of events ('logs') over their lifecycle. Logging capabilities shall enable:\n\n- The recording of events relevant for identification of situations that may result in risks to health or safety or fundamental rights of persons.\n- Post-market monitoring.\n- Monitoring of the operations.\n- Recording of each use period of the system.\n- The reference database against which input data has been checked by the system.\n- The input data for which the search has led to a match.\n- The identification of the natural persons involved in the verification of results.\n\nAI systems shall be designed and developed to ensure sufficiently transparent operation, achieving compliance with the relevant obligations, and enabling deployers to understand and use the system appropriately. It shall be accompanied by instructions for use (IFU) in an appropriate digital format or otherwise including concise, complete, correct, and clear information, which is relevant, accessible, and comprehensible to deployers.\n\n@2024 BSI. All rights reserved.\n\n## Requirement\n\n## Summary\n\n| Human oversight   | AI systems shall be designed to be effectively overseen by natural persons, aiming at preventing or minimizing the risks to health, safety, or fundamental rights. Human oversight shall be ensured through 'pre' built-in measures (by the provider) and 'post' measures (identified by the provider, but implemented by the user), and be enabled to understand capacities and limitations, automation bias, the system's output, as well a the ability to override or reverse the output and interrupt the system. For remote biometric identification systems, two natural persons   |\n|-------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\n## Accuracy, robustness and cybersecurity\n\nAI systems shall be designed and developed to achieve an appropriate level of accuracy, robustness, and cybersecurity, and to perform consistently in those respects throughout their lifecycle.\n\nLevels of accuracy and accuracy metrics shall be declared in instructions for use.\n\nHigh-risk AI systems shall be resilient as regards errors, faults, or inconsistencies due to their interaction with natural persons or other systems.\n\nThe robustness may be achieved through technical redundancy solutions (backup or fail-safe plans). Learning AI systems shall be developed to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations ('feedback loops').\n\nAppropriate cybersecurity solutions to address AI specific vulnerabilities shall include measures to prevent and control for attacks trying to manipulate the training dataset ('data poisoning'), inputs designed to cause the model to make a mistake ('adversarial examples'), or model flaws.\n\nThe AI Act goes further than other NLF legislations, as it details obligations not only for economic operators (e.g., providers, importers, distributors), but also obligations for deployers, providers, and deployers of certain AI systems (Chapter IV), providers of GPAI models (Chapter V, Section 2) and GPAI models with systemic risk (Chapter V, Section 3). These obligations are detailed in Table 2.\n\n<!-- image -->\n\n## Table 2: AI Act obligations\n\n| Obligations per economic operators                                 | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n|--------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Obligations of providers of high-risk AI systems 84                | Providers shall ensure compliance with AI Act requirements, indicate in the AI system the provider information, comply with Article 17 (QMS), keep Article 18 documentation available for 10 years, keep automatic generated logs for at least six months, follow appropriate conformity assessment procedures, comply with registration obligations (Article 49), affix CE and draw up an EU declaration of conformity, ensure compliance with accessibility requirements, and investigate and inform non-conformities and corrective actions to appropriate stakeholders.                                                                      |\n| Authorized representatives of providers of high-risk AI systems 85 | European Union providers shall mandate authorized representatives for specific tasks - namely, an EU declaration of conformity and technical documentation verification, keeping the prementioned plus the issued certificate and providing contact details for 10 years, provide national competent authorities with documentation and access to logs, cooperating to reduce/mitigate risks, and where applicable complying with registration obligations.                                                                                                                                                                                      |\n| Obligations of importers 86                                        | Importers shall ensure their systems are in conformity with the AI Act, verifying that a conformity assessment according to Article 43 has been carried out, technical documentation, CE marking, EU declaration and IFUs are in place, and that the authorized representative is assigned. Packaging or documentations should indicate the importer's details. Importers should cooperate with national competent authorities and keep the relevant documentation for 10 years.                                                                                                                                                                 |\n| Obligations of distributors 87                                     | Distributors shall verify the required CE marking, EU declaration of conformity and instruction of use, and that provider and importer have complied with their obligations. They shall inform providers or importers of risks to the health or safety or to fundamental rights of persons, take the corrective actions necessary to bring system into conformity or ensure provider, importer or, operator do, and inform national competent authorities of the non- compliance or any corrective actions taken. Distributors shall cooperate and provide national competent authorities, upon reasoned request, information and documentation. |\n\n<!-- image -->\n\n| Obligations per economic operators                                            | Summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Obligations of deployers of high-risk AI systems 88                           | Deployers shall use the system in accordance with the instructions of use, assign human oversight to natural persons with the necessary competence, training, and authority, and ensure input data is relevant and sufficiently representative. They shall inform the provider or distributor in case of risks or serious incidents and interrupt the use of the system, fulfill rules on internal governance arrangements, processes, and mechanisms pursuant to the relevant financial service legislation in case of financial institutions, and keep logs for at least six months. They are required to comply with the registration obligations, carry out data protection impact assessment (GDPR) and cooperate with national competent authorities. |\n| Transparency obligations for providers and deployers of certain AI systems 89 | AI systems intended to interact directly with natural persons, should be designed to inform interaction with AI, unless this is obvious. Obligations are not applicable to AI systems authorized by law to detect, prevent, investigate, or prosecute criminal offences. AI systems, generating synthetic audio, image, video, or text content shall ensure outputs are marked as AI generated or manipulated. Deployers of an emotion recognition or biometric categorization system, excluding systems permitted by law to detect, prevent, or investigate criminal offences shall inform the natural person on the exposure to the system.                                                                                                               |\n| Obligations for providers of general-purpose AI models 90                     | Providers of GPAI models, other than those which are free & open license, should draw and maintain technical documentation according to Annex XI, and draw up information/documentation to be provided to other AI system providers intending to integrate the GPAI model into their system.                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Obligations for providers of general-purpose AI models with systemic risk 91  | In addition to GPAI providers obligations, systemic risk GPAI providers need to perform model evaluation including adversarial testing, assessing & mitigating systemic risks at the EU level, reporting appropriate incidents, and ensurng cybersecurity protection of the model and the physical infrastructure.                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n\nAny distributor, importer, deployer or other third-party that makes a substantial modification of a high-risk AI system OR changes the intended purpose of a non-high risk AI turning it into a high risk one, it will be considered the provider and will be subject to the AI Act providers' obligations. 92\n\n88 See Article 26 of AI Act\n\n89 See Article 50 of AI Act\n\n90 See Article 53 of AI Act\n\n91 See Article 55 of AI Act\n\n92 See Article 25 of AI Act\n\n<!-- image -->\n\n<!-- image -->\n\nThe AI Act's regulatory framework introduces a structured ecosystem of entities entrusted with the assessment, certification, and oversight of AI systems. This framework aims to harmonize approaches, ensuring the safe and ethical deployment of AI systems. To unpack this regulatory landscape, we explain and clarifying the definitions of the involved stakeholders:\n\n<!-- image -->\n\nConformity Assessment Body (CAB): 93  A separate legal entity that performs third-party conformity assessment activities including testing, certification, and inspection. The primary objective of a CAB is to ascertain that AI systems meet requirements of the relevant applicable standards.\n\nNotified Body (NB): 94  A specialized form of CABs, NBs undergo formal notification in accordance with the EU AI Act and relevant EU harmonization legislation. Their tasks include conducting conformity assessment activities for high-risk AI systems, adhering to organizational, quality management, resource, process, and cybersecurity requirements. NBs maintain independence from evaluated AI system providers, ensuring impartiality and confidentiality. Articles 31 to 34 of the EU AI Act delineate specific obligations and operational criteria for NBs.\n\nNotifying Authority (NA): 95  Designated within each Member State, NAs manage the procedural framework for assessing, designating, and notifying CABs, alongside ongoing supervision. Operating under a mandate to prevent conflicts of interest, NAs uphold principles of objectivity and impartiality. They are structured to separate decision-making from assessment activities, explicitly prohibiting any commercial or competitive offerings. NAs ensure their personnel are highly qualified in relevant fields, including information technologies, artificial intelligence, and law.\n\nMarket Surveillance Authority (MSA): 96  Designated as the national authority responsible for overseeing market activities to ensure compliance with legal requirements, particularly for high-risk AI systems. They enforce the regulation by monitoring, identifying non-compliance, and oversight of the corrective actions implemented by the AI providers to protect public interests, health, safety, and fundamental rights.\n\nNational Competent Authority (NCA): 97  It includes the NA and the MSA. It represents the authoritative entities designated by EU member states to oversee the regulation and compliance of AI systems within their jurisdictions, focusing on ensuring the safety, security, and rights compliance of AI technologies.\n\nArtificial Intelligence Office (AIO): 98  An office within the European Commission tasked with monitoring and supervising AI systems, general-purpose AI models, and AI governance. The AIO plays a central role in fostering a coherent regulatory framework for AI across the EU, ensuring compliance with legislative mandates, facilitating enforcement, and overseeing AI governance to safeguard public interest and uphold standards.\n\nEuropean Artificial Intelligence Board (AIB): 99  An advisory and coordinating body established to support the consistent and effective application of the AI Act across the EU. It functions to enhance cooperation among national competent authorities tasked with the Regulation's enforcement, share technical and regulatory expertise, and promote best practices among Member States.\n\n93 See Article 3 (21) of AI Act\n\n94 See Article 3 (22) of AI Act\n\n95 See Article 3 (19) of AI Act\n\n96 See Article 3 (26) of AI Act\n\n97 See Article 3 (48) of AI Act\n\n98 See Article 3 (47) of AI Act\n\n99 See Article 65 of AI Act\n\n<!-- image -->\n\n## The AI Act lists the responsibilities of Notified Bodies (NBs): 100\n\n<!-- image -->\n\n## 01. Conformity Assessment 101 :\n\n- NBs, or CABs acting on their behalf, impartially evaluate the quality management system (AI management system) 102  implemented by the provider of a high-risk AI system and the technical documentation 103 , submitted by that provider. These assessments aim to verify compliance with the AI Act's harmonized standards and common specifications, focusing on applicable requirements. Each assessment is completed with an audit report 104  detailing the outcomes, identifying areas of compliance, and highlighting non-compliant areas (so-called 'non-conformities') with the AI Act's regulatory framework.\n- NBs are granted necessary and relevant access to the training, validation, and testing datasets utilized by AI systems, potentially through application programming interfaces (APIs) or other mechanisms facilitating remote access, underpinned by adequate security safeguards. NBs are also granted access to the training and trained models of the AI system, including relevant parameters (e.g., weights, architecture), after alternative conformity verification methods and finding them inadequate. NBs must conduct direct testing if they find the tests provided by the high-risk AI system provider inadequate. 105\n\n<!-- image -->\n\n## 02. Issuance of Certificates: 106\n\n- Upon successful conformity assessment, NBs issue certificates to AI system providers, signifying compliance with the AI Act and associated requirements.\n\n100  See Articles 31, 34 of AI Act\n\n101  See Recitals 50,78,86, 123, 125 and Articles 45, 46,57 Section 7 of AI Act\n\n102  See Recital 173, Article 43 and Annex VII, Section 5 of AI Act\n\n103  See Recitals 66,173, Articles 11, 43 Annex VII, Section 4.3 of AI Act\n\n104  See Annex VII, Section 5.3 of AI Act\n\n105  See Annex VII, Sections 4.3-4.5 of AI Act\n\n106  See Article 44 and Annex VII of AI Act\n\n107  See Annex VII, Section 5 of AI Act\n\n108  See Annex VII, Sections 3.4 and 4.7 of AI Act\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## 03. Continuous Monitoring and Surveillance: 107\n\n- Following a successful conformity assessment, NBs undertake ongoing surveillance of EU conformity certificates issued to AI system providers. This surveillance focuses on ensuring continual compliance with the AI Act and includes regular audits of quality management system to verify continuous compliance with applicable regulatory and technical requirements (harmonized standards, common specifications, and industry practices, in the absence of harmonized standards and common specifications).\n- NBs must be informed by providers of high-risk AI systems of any proposed changes to the AI management system or AI system itself 108  that could impact compliance (and/or intended purpose) and NBs review the relevant documentation to approve changes where compliance is maintained.\n\n<!-- image -->\n\n## 04. Cooperation among Notified Bodies:\n\n- NBs communicate with other NBs regarding any refusals, suspensions, or withdrawals of quality management system or EU technical documentation approvals. Additionally, upon request, the NB must provide information regarding quality system approvals it has issued. 109\n\n<!-- image -->\n\n## 05. Documentation and Reporting:\n\n- NBs maintain records and associated evidence of all assessments, decisions, and certifications, which are made accessible to NCAs for oversight purposes. 110\n\nThe key responsibilities of NCAs 111  within the regulatory framework under the AI Act entail diverse roles essential for ensuring effective governance and oversight of the national-level implementation of the AI Act:\n\n<!-- image -->\n\n## 01. Regulatory Oversight:\n\n- NCAs can request and review documentation from providers of non-high risk AI systems and general-purpose AI models (GPAI), ensuring transparency and compliance with AI Act regulatory requirements. 112  NCAs also request technical documentation from providers of high-risks AI systems (when the NB is not involved). 113\n- NCAs may support the provision of high-quality data for AI system training, which aims to support innovations, as well as the transparency and reliability of the data framework. 114\n\n109  See Article 45, of AI Act\n\n110  See Articles 45 AI Act\n\n111  See Article 28 of AI Act\n\n112  See Article 6, Section 6 and Article 53, Section 1 AI Act\n\n113  See Article 11, Section 1 AI Act\n\n114  See Recital 68 of AI Act\n\n115  See Article 70 of AI Act\n\n116  See Article 30 of AI Act 117  See Article 57 of AI Act\n\n118  See Article 70 of AI Act\n\n<!-- image -->\n\n<!-- image -->\n\n- NCAs serve as primary points of contact, facilitating communication, regulatory, and technical compliance. 115\n- NCAs support the European Commission and exchange information between NCAs. 116\n\n<!-- image -->\n\n## 02. Support for Innovations:\n\n- NCAs establish national-level AI regulatory sandboxes to foster innovation and enable testing of AI systems and inform the AIO about the progress. Collaboration between NCAs, the AIO, and other relevant authorities facilitates knowledge exchange and best practices dissemination. 117\n- NCAs can offer SMEs and startups guidance on the AI Act, aligned with the Board and Commission position. For AI systems under other EU laws, relevant authorities must be consulted. 118\n\n<!-- image -->\n\n## 03. Enforcement Actions: 119\n\n- NCAs (specifically MSA) are responsible for oversight of corrective actions implemented by the AI model providers to address incidents with involved AI systems, issuing warnings, imposing fines, and mandating compliance measures to uphold regulatory integrity. NCAs also act as the single point of contact.\n\n<!-- image -->\n\n## 04. Designation, Monitoring, and Coordination of Notified Bodies (NBs): 120\n\n- NCAs designate NBs and ensure their qualifications comply with the AI Act's requirements for conducting conformity assessments. NCAs monitor and audit NBs, ensuring their compliance with the AI Act's requirements, which include the maintenance of competence, impartiality, and other legal, regulatory, and technical requirements applicable to the NBs 121.\n\nThe essential duties of the AIO within the regulatory framework established by the AI Act and its official website 122  are outlined as follows:\n\n<!-- image -->\n\n## 01. Central Coordination, Governance, and Oversight of GPAI:\n\n- The AIO serves as the central coordinating body among EU member states, providing guidance and support for consistent AI Act implementation. It facilitates best practice exchange and ensures alignment across the EU. The AIO leverages its expertise in establishing EU-level advisory bodies (such as the Artificial Intelligence Board) fostering collaboration to ensure coherent AI Act application across all Member States. 123\n\n119  See Recital 153, Article 89 of AI Act\n\n120  See Article 38 of AI Act\n\n121  See Article 28 of AI Act\n\n122  See https://digital-strategy.ec.europa.eu/en/policies/ai-office\n\n123  See Recital 148 of AI Act\n\n124  See Articles 88, 91, 92 of the EU AI Act\n\n<!-- image -->\n\n<!-- image -->\n\n- The AIO supervises GPAI (requirements for providers, authorized representatives, deployers) to ensure compliance with the AI Act, standards, and associated requirements. It develops tools, methodologies, and benchmarks for evaluating the capabilities and reach of GPAI models, classifying models with systemic risks. The AIO also provides oversight of corrective actions taken by GPAI provider in case of non-compliance. The AIO has the power to request documentation and information from the GPAI provider and its authorized representatives, conduct evaluation of GPAI-models and request measures (including to restrict making GPAI available on the market, or to withdraw or recall the model). It also monitors fulfilment of obligations by the providers of GPAI. 124\n\n- The AIO provides coordination support for joint investigations in case of market surveillance with involvement of specific categories of high-risk AI system(s), supervises and monitors the compliance of GPAI, as well as taking necessary action(s) to monitor effective implementation and continuing compliance for providers of GPAI models with the AI Act. 125\n\n<!-- image -->\n\n## 02. Policy Development:\n\n- The AIO provides standardized templates for the areas required by the AI Act (e.g., summary of content used for training of the GPAI, summary of the content used codes of practice, questionnaire for deployers). 126\n- The AIO advises on best practices, facilitates access to AI testing environments, and promotes innovative ecosystems to boost EU competitiveness. 127\n- The AIO ensures the regulatory framework adapts to technological advancements and societal needs by engaging with diverse stakeholders, including AI developers, SMEs, and experts. It fosters continuous dialogue to inform policy formulation and develop codes of practice aligned with the AI Act. 128\n\n<!-- image -->\n\n## 03. Cooperation with stakeholders:\n\n- The AIO collaborates with a diverse range of stakeholders, such as NCAs, providers of GPAI, scientific panels (including independent experts), institutions, the European Artificial Intelligence Board, and the European Centre for Algorithmic Transparency (ECAT), to gather and share technical and regulatory expertise, including knowledge gathered from the establishment, running, and oversight of AI sandbox. 129\n\n125  See Recitals 112,114, 160, 161, 162, 164 and Article 75 of AI Act\n\n126  See Recitals 107,108 and Articles 25, 27, 50, 56, 95 of AI Act\n\n127  See Recitals 116, 117, 179 of AI Act\n\n128  See Recitals 113, 151 and Article 90 of AI Act\n\n129  See Recital 111, 116, 163 and Articles 57, 68 of AI Act\n\n130  See Articles 65 and 66 of AI Act\n\n131  See Article 74, Section 11 of AI Act\n\n<!-- image -->\n\nThe European Artificial Intelligence Board 130 plays a   major role in robust oversight and the application of the AI Act by the Commission and Member states. It achieves this through several key activities:\n\n- 1 Facilitating coordination among NCAs responsible for enforcing the AI Act and endorsing joint market surveillance activities. 131\n- 2 Maintaining technical expertise and best practices across EU Member States, as well as contributing to a cohesive understanding and application of the AI Act.\n- 3 Delivering strategic advice on the regulation's enforcement, focusing on GPAI models and aiming to standardize approaches and interpretations.\n- 4 Emphasizing the importance of consistent conformity assessments, the effective use of regulatory sandboxes, and the value of testing AI systems in real-world scenarios.\n- 5 Playing a critical role in advising on the regulation's implementation and offering recommendations on various fronts including codes of conduct, the evolution of AI standards, and the integration of emerging technologies.\n- 6 Engaging in broad educational efforts to boost AI literacy and fostering the public's awareness of AI's benefits and risks, while facilitating crosssectoral and international cooperation to enhance the regulation's global relevance and effectiveness.\n\n<!-- image -->\n\nThe development process of the AI Act has been characterized by significant complexity and anticipation over the past several months, as this period was fraught by a series of strategic discussions, uncertainties, and widespread speculations regarding the outcome. Nevertheless, the legislative journey succeeded with the final political agreement and the adoption of the Act on 13th March 2024. This result indicates a procedural timeline which is anticipated to take approximately up to two months for the formal publication of the legislation in the Official Journal of the EU. The legislation's entry into force,\n\n<!-- image -->\n\nthe significant milestone for the new AI Act, will be twenty days after its publication in the Official Journal.\n\nTo operate within the timelines in practical perspective, we analysed associated key milestones and relevant actions to be completed. The results of this analysis are presented in Table 3 .\n\n## Table 3 - Key timelines of the AI Act implementation\n\n| Timeline                                            | Relevant Action   | Relevant Action                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|-----------------------------------------------------|-------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 13 March 2024                                       | â€¢                 | Adoption on the EU Artificial Intelligence Act in the European Parliament's plenary.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| 22-25 April 2024                                    | â€¢                 | Approval of the AI Act corrigenda in the plenary of the parliament                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Entry into force 132 1 August 2024                  | â€¢                 | 20 days after publication in the Official Journal of the EU.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| Entry into force + Three months 133 2 November 2024 | â€¢                 | Member States must list, publish, and keep up to date list of public authorities and/or bodies.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Entry into force + Six months 134 2 February 2025   | â€¢ â€¢               | Date of application for prohibited AI systems to be available on the market (Chapter II) Date of application of general provisions (Chapter I)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n| Entry into force + Nine months 135 2 May 2025       | â€¢                 | Readiness of codes of practices to be published by the AIO.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Entry into force + 12 months 2 August 2025          | â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ â€¢ | Chapter III (High-Risk AI Systems) Section 4, Chapter V (GPAI Models), Chapter VII, and Chapter XII, except Article 101. 136 If the AI Office finds the code of practice insufficient or unfinalized 12 months after Entry into Force, the Commission, via implementing acts, may establish common rules for obligations in Articles 53 and 55, aligning with the examination process of Article 98(2). 137 Member states to have implemented rules on penalties, including administrative fines. 138 Readiness of NBs and governance structure, including conformity assessments. 139 If no code of practice is finalized within 12 months after entry into force, or if deemed inadequate by the AIO, the Commission may issue implementing acts for Articles 53 and 55 obligations, following Article 98(2)'s examination procedure. 140 Member States must inform the Commission of NAs and MSAs, including their tasks, and provide publicly accessible information on how NCAs (in a form of single point contact) can be contacted. 141 The Commission will provide guidance for high-risk AI system providers to comply with obligations of reporting of serious incidents to MSAs. 142 Member States must inform the Commission about their NCA's financial and human resources, assessing their sufficiency, with this being repeated every two years afterwards. The Commission will share this data with the AI Board for analysis and potential advice. 143 Following the entry into force and until the delegated powers in Article 97 expire, the Commission is tasked with annually evaluating the necessity for updates to Annex III's list and Article 5's catalogue of banned AI practices. The outcomes of these assessments will be systematically presented to both the European Parliament and the Council. 144 |\n\n137  See Article 56, Section 9 of AI Act\n\n138  See Recital 179 of AI Act\n\n139  See Recital 179 of AI Act\n\n140  See Article 56 of AI Act\n\n141  See Article 70, 59 of AI Act\n\n142  See Article 73 of AI Act\n\n143  See Article 70 of AI Act\n\n144  See Article 112 of AI Act\n\n<!-- image -->\n\n| Timeline                                       | Relevant Action                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Entry into force + 18 months 2 February 2026   | â€¢ The Commission, as well as the consulting AI Board, are to provide guidelines for the AI Act's entry, including examples of high/non-high risk AI use cases as required by Article 96. 145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Entry into force + 24 months 2 August 2026     | â€¢ Application of the AI Act, which includes obligations on high-risk AI systems specifically listed in An- nex III, inlcuding AI systems in biometrics, critical infrastructure, education, employment, access to essential public services, law enforcement, immigration, and the administration of justice to comply with the AI Act. It applies when significant design changes occur from that timeframe, aligning with Article 5 under Article 113(3)(a). 146 â€¢ Member States are mandated to create at least one national AI regulatory sandbox. Implementation of this requirement can be a collaborative effort among different Member States. The EU Commis- sion offers support for sandbox development and operation. States may also join existing sandboxes, provided they ensure equivalent national coverage. 147                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n| Entry into force + 36 months 2 August 2027     | â€¢ Applicability of Article 6(1) (classification rules for Annex I Union harmonization legislation) and corre- lated obligations of the AI Act. 148 â€¢ GPAI model providers with products placed on the EU market 12 months prior to the AI Act's entry into force are required to undertake essential measures to meet the relevant obligations of GPAI model providers. 149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n| Entry into force + 48 months 150 2 August 2028 | â€¢ Every 48 months starting from the AI Act's entry into force, the Commission will analyse and inform the European Parliament and Council on potential amendments to Annex III, Article 50's AI system transparency requirements, and improvements to supervisory and governance frameworks. â€¢ Every 48 months from the AI Act's entry into force, the Commission will report its evaluation to the Parliament and Council, focusing on enforcement structure and the potential for an EU agency to address gaps. Amendments may be proposed based on these insights. All reports shall be made publicly available. â€¢ The Commission will assess the AI Office's performance, examining if it possesses adequate powers and competences for its duties, and if needed enhancing its role and enforcement capabilities, along with increasing its resources. The evaluation report will be submitted to the European Parliament and Council. â€¢ Every 48 months from the AI Act's entry into force, the Commission reports on the advancement of standardisation in energy-efficient development of general-purpose models. This includes assessing the necessity for additional measures, which are potentially binding. The final evaluation report will be submitted to the European Parliament and the Council and made publicly available. â€¢ The Commission is mandated to assess the influence and efficacy of voluntary codes of conduct every three years afterwards. These evaluations are aimed to optimize adoption of Chapter II, Section 2's requirements for providers of AI systems not classified as high-risk and to explore the potential for integrating additional mandates, notably concerning environmental sustainability. 151 |\n| Entry into force + 60 months 152 2 August 2029 | â€¢ The Commission is tasked with conducting a review of the AI Act, at intervals of four years and to report the findings to the European Parliament and the Council. An annual assessment is to be provided by the Commission to evaluate potential revisions to the lists of high-risk AI systems and prohibited practices.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Entry into force + 72 months 2 August 2030     | â€¢ Providers and deployers of high-risk AI systems designated for public authority use shall comply with AI Act's requirements. 153                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n\n<!-- image -->\n\n## Timeline\n\n## 31 December 2030\n\nEntry into force + 84 months 155\n\n2 August 2031\n\n## Relevant Action\n\n- AI systems integrated within large-scale IT frameworks, as specified in Annex X, and operational 36 months before the AI Act's entry into force must comply with the AI Act. Evaluations of these largescale IT systems, mandated by the legal acts in Annex X, will incorporate the AI Act's requirements, especially when these acts undergo revisions or updates. 154\n- The Commission is required to execute an assessment of its enforcement. This analysis will be reported to the European Parliament, the Council, and the European Economic and Social Committee, reflecting on the initial years of the AI Act's application. Based on findings, if and when necessary, the final report shall be accompanied by a proposal for AI Act's amendment regarding the structure of enforcement and changes needed to be implemented by the EU agency to resolve any identified negative findings.\n\n<!-- image -->\n\n154  See Article 111 of AI Act\n\n155  See Article 112, Section 13 of AI Act\n\n<!-- image -->\n\n<!-- image -->\n\nThe AI Act introduces the term 'substantial modification', referring to a change to an AI system already placed on the market or put into service which is not foreseen in the initial conformity, and may affect compliance or modify its intended purpose. 156  The AI Act also introduces the term 'predetermined changes', a term used to describe predefined changes subject to an initial conformity assessment of AI systems which are not static but continue to learn or evolve following placement on the market. 157\n\nFor high-risk AI systems that have been placed on the market prior to the application of the AI Act, the AI Act applies only if following the AI Act's application date there are significant changes in the design or intended purpose. Furthermore, the AI Act makes no distinction for the prementioned purpose between the terms 'significant change' and 'substantial modifications'. 158\n\n156  See Article 3(23) of AI Act\n\n157  See Recital 128 and Article 43(4) of AI Act\n\n158  See Recital 177 of AI Act\n\n159  See Recital 84 of AI Act\n\n<!-- image -->\n\nThe AI Act makes an exemption for AI systems which are components of the large-scale IT systems and high-risk AI systems intended to be used by public authorities - compliance of those systems with the AI Act requirements is required by end of 2030, or by six years after the entry into force.\n\nHowever, it is not immediately clear when a high-risk AI system falls under another EU legislation - as in the case of Annex I section A products - which legislation prevails in terms of substantial modifications. The answer to this question can be found in the recitals of the AI Act. 159 If a change is not considered significant under a more specific EU legislation (e.g., Regulation 2017/745 MDR), then the change should not trigger substantial modification under the relevant clauses of the AI Act.\n\n<!-- image -->\n\nThe AI Act brings scrutiny also to sectors that were not previously subject to regulation. Due to its horizontal nature and levels of risk, the AI Act has different obligations for AI providers and deployers to ensure conformity of AI systems. The principal mechanisms of compliance within the AI Act are:\n\n## The use of a quality management system\n\n(QMS): 160  Although the recently published standard ISO/IEC 42001:2023 Information Technology Artificial Intelligence Management System (AIMS) is not yet harmonized with the AI Act, it is expected to be the reference standard for conformity with the relevant requirements. The AIMS should cover a strategy for compliance, processes on design, development, data governance, testing and validation of AI systems, risk management, post-market monitoring, and incident reporting. Providers and deployers of high-risk AI systems are obliged to use AIMS, with this also being an obligation for AI providers that are required to follow conformity routes stated in Annex VI (internal control) and Annex VII (assessment of QMS and Technical Documentation). When AI systems are subject to obligations for a QMS under other sectorial EU legislations, AIMS aspects may be covered as part of the sectorial QMS standard.\n\n160  See Article 17 of AI Act\n\n161  See Articles 11 and 53 of AI Act\n\n162  See Articles 8 to 14 of AI Act\n\n<!-- image -->\n\nThe creation and maintenance of technical documentation: 161  High-risk AI systems that may follow the conformity routes of Annex VI (internal control) and Annex VII (assessment of QMS and technical documentation (TD)), and providers of general-purpose AI models, will require putting in place TD for assessing compliance with the AI Act Chapter III, Section 2 requirements. Annex XI (TD for providers of general-purpose AI models, Article 53(1))) and Annex IV (TD referred to in Article 11(1)), describe the content of the TD. TD will need to be drawn up before placing the AI system on the market. When the AI system is subject to obligations of TD under other sectorial EU legislations, AI Act aspects may be covered as part of the sectorial TD.\n\n## Conformity with high-risk AI system\n\nrequirements: 162  Chapter III, Section 2, lists highrisk AI systems requirements. Although the ISO/IEC JTC 1/SC 42 committee has already published multiple standards, none has as of yet been harmonized with the AI Act. The legislation clearly states (Article 40) that conformity with the AI Act's harmonized standards is a presumption of conformity with AI Act requirements.\n\nTransparency obligations: 163  Providers of AI systems, including general-purpose AI systems, as well as deployers of certain AI systems, need to comply with transparency obligations set in Chapter IV, Article 50.\n\nSandboxes: 164  Sandboxes are established by members states, competent authorities. Sandboxes are controlled environments to facilitate development, training, testing, and validation of innovative AI systems. AI systems might use sandboxes prior to being placed on the market or put into service. The output (exit reports) of the sandboxes may be used to demonstrate compliance with the AI Act, as part of documentation provided for the conformity assessment process, or provided to relevant market surveillance authorities.\n\n## Routes to conformity: 165\n\n- When a high-risk AI system is subject to other sectorial EU legislations (Annex I, section A) the provider shall follow the relevant conformity assessment procedure as required under those legal acts. Requirements set out in the AI Act apply to these AI systems, and Notified Bodies may request datasets and the AI model for carrying out additional testing (Annex VII).\n- Internal control (Annex VI) is available as a conformity assessment route to Annex III highrisk AI systems, however, Annex III point 1 AI systems (Biometric identification) may opt for (or be forced to follow) Annex VII conformity assessments which require the Notified Body's involvement for the assessment of AIMS and TD.\n\n163  See Article 50 of AI Act\n\n164  See Article 57 of AI Act\n\n165  See Article 43 of AI Act\n\n166  See Article 51 and 56 of AI Act\n\n167  See Article 56 of AI Act\n\n<!-- image -->\n\nGeneral-purpose AI (GPAI) models: The main oversight authority for GPAIs is the AI Office. Article 53 states obligations for GPAI providers and Annex XI describes technical documentation requirements for GPAIs models. Obligations are not applicable for free and open-license providers. There are additional obligations for GPAI models with systemic risks, including model performance evaluation, mitigation measures of systemic risks, and cybersecurity protection. 166 Compliance with codes of practice 167 will be considered sufficient for GPAIs systemic risk obligations, until harmonized standards are released.\n\n<!-- image -->\n\n<!-- image -->\n\nThe AI Act is en route to the Official Journal of the European Union, and the world is watching. What started out as conjecture has evolved into an emerging and impactful industry, not without its risks. It is evident in the legislative text that not one person or one group of persons can manage this new landscape on its own. It will take the collective effort of providers, deployers, the Commission, the AI Office, the AI Board, member states, National Competent Authorities, the public, and others to ensure the predictability and deployment of these systems is effectively controlled.\n\nThe momentum of the AI Act has left many wondering what the next steps are to prepare themselves for its impact. We anticipate that this paper will support organizations to take the firsts steps to determine if and how AI systems or models affect their organization, identifying the organization's role and obligations, supporting understanding of AI systems classification, and shedding\n\n168  See Recital 178 of AI Act\n\n<!-- image -->\n\nlight on the requirements need to be fulfilled, as well as the methods those requirements are met. Organizations should be able to understand conformity routes for their products, allowing them to proactively seek accredited or designated organizations under the schemes of interest to consolidate assessments where possible.\n\nThe AI Act encourages   providers of high-risk AI systems to start the compliance journey on a voluntary basis during the transitional period. 168  BSI shares this view; all stakeholders will face a steep learning curve. BSI deeply values the AI Act and the approach adopted by the community engaged in the development of this regulation, recognizing that the publishing of the AI Act by legislators and achieving compliance are not end goals. Instead, they represent a journey of ongoing enhancement of the regulatory framework, evolving in tandem with technological advancements.", "fetched_at_utc": "2026-02-09T13:41:56Z", "sha256": "2ca62ebf0cb48ac1e67dcc5a16ad9724b4b3d85cb79413ffe9d7336fc436781c", "meta": {"file_name": "BSI_EU_AI_Act_Whitepaper_Final_2_9_24.pdf", "file_size": 4255354, "mtime": 1769012293, "docling_errors": []}}
{"doc_id": "pdf-pdfs-debunking-10-common-eu-ai-act-misconceptions-part-1-oliver-patel-7dd2f38489b3", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel.pdf", "title": "Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable, and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement, and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nThe EU AI Act entered into force in August 2024 and its first provisions became applicable in February 2025.\n\nDue to a combination of factors, such as the complexity and novelty of the law, and the lack of guidance and standards, some unhelpful misconceptions have taken hold.\n\nThis two-part series on Enterprise AI Governance presents and debunks 10 common misconceptions about the AI Act, providing a detailed explanation for each one. The first 5 are covered in part 1 and the second 5 will be covered in part 2.\n\n## The ten misconceptions are:\n\n1.  The EU AI Act has a two-year grace period and applies in full from August 2026.\n\n2.  All open-source AI systems and models are exempt from the EU AI Act.\n3.  High-risk AI models are explicitly regulated under the EU AI Act.\n4.  Emotion recognition is prohibited under the EU AI Act.\n5.  Facial recognition is prohibited under the EU AI Act.\n6.  Transparency is required for 'limited risk' AI systems.\n7.  Third-party conformity assessments are required for all high-risk AI systems.\n8.  Fundamental rights impact assessments are required for all high-risk AI systems.\n9.  All high-risk AI systems must be registered in the public EU-wide database.\n10.  Deployers do not need to register their use of high-risk AI systems.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n## Misconception 1: The EU AI Act has a two-year grace period and applies in full from August 2026\n\nIt is commonly remarked that the EU AI Act has a two-year grace period, which allows organisations to prepare to be compliant. Although there are grace periods for compliance, they vary in length, and certain provisions are already applicable today.\n\nIn fairness, Article 113 does state that the AI Act ' shall apply from 2 August 2026 '. However, given the number of exceptions to this, simply claiming there is a 'two-year grace period' is misleading.\n\nThe reality is that different provisions become applicable at different times. Although most provisions apply from August 2026, the provisions on AI literacy and prohibited AI practices became applicable in February 2025, and the obligations for providers of new general-purpose AI models become applicable in August 2025.\n\nHere is a breakdown of when the most significant provisions apply:\n\n- 2 February 2025: prohibition of specific AI practices became applicable.\n- 2 February 2025: AI literacy provisions (for deployers and providers of AI systems) became applicable.\n- 2 August 2025: obligations for providers of 'new' general-purpose AI models become applicable (i.e., general-purpose AI models placed on the market from 2 August 2025 onwards).\n\n- 2 August 2026: many of the AI Act's provisions become applicable, including obligations and requirements for high-risk AI systems listed in Annex III.\n- 2 August 2027: obligations and requirements for high-risk AI systems which are products, or safety components of products, regulated by specific EU product safety laws (listed in Annex I) become applicable.\n- 2 August 2027: obligations for providers of 'old' general-purpose AI models become applicable (i.e., general-purpose AI models placed on the market before 2 August 2025).\n\nAlthough the provisions on prohibited AI practices became applicable earlier this year, meaningful enforcement will come later. This is because the applicability of the penalty and governance regime, including the deadline for member states to designate their AI regulators, lands on 2 August 2025. This creates a unique situation where there is a 6-month lag between important provisions becoming applicable and the regulatory enforcement structure and regime being operational.\n\n## Misconception 2: All open-source AI systems and models are exempt from the EU AI Act\n\nAlthough there are broad exemptions for open-source AI systems and models, there are also several important ways in which the AI Act regulates them.\n\nFor example, high-risk AI systems which are open-source are still classified as high-risk, and providers of general-purpose AI models which are open-source must adhere to specific obligations (which are trimmed down in some cases).\n\nArticle 2(12) states that the AI Act ' does not apply to AI systems released under free and open-source licenses, unless they are placed on the market or put into service as highrisk AI systems or as an AI system that falls under Article 5 or Article 50'. This has the following meaning:\n\n- Providers and deployers of high-risk AI systems which are open-source must adhere to all the obligations and requirements for high-risk AI systems, despite their AI system's open-source nature.\n- The reference to Article 5 means that prohibited AI practices are prohibited, irrespective of whether or not they leverage open-source AI systems.\n- Finally, providers and deployers of open-source AI systems which interact with individuals, generate synthetic content and deep fakes, or perform emotion recognition or biometric categorisation, must adhere to the transparency obligations outlined in Article 50.\n\nArticle 53(2) refers to open-source AI models as ' models that are released under a free and open-source licence that allows for the access, usage, modification, and distribution of the model, and whose parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available'.\n\nProviders of open-source general-purpose AI models must adhere to a limited set of obligations, such as publishing a summary of the model's training data and implementing a copyright compliance policy.\n\nHowever, providers of these models do not have to adhere to the obligations to produce and make available technical documentation about their general-purpose AI model. Nor do they have to appoint an authorised representative in the EU if they are established in a third country.\n\nFor providers of open-source general-purpose AI models with systemic risk, the full and extensive set of obligations for general-purpose AI models with systemic risk applies. This includes all the above obligations, as well as performing model evaluations, systemic risk assessment and mitigation, and ensuring adequate cybersecurity protection.\n\nIn practice, this will mean that despite the broad exemptions for open-source AI, it is\n\nlikely that providers of some of the most advanced and widely used open-source AI models will have extensive compliance obligations.\n\n## Misconception 3: High-risk AI models are explicitly regulated under the EU AI Act\n\nThe AI Act regulates the following types of AI, with explicit and targeted provisions:\n\n- prohibited AI practices;\n- high-risk AI systems;\n- general-purpose AI models;\n- general-purpose AI models with systemic risk; and\n- certain AI systems which require transparency.\n\nThe AI Act does not explicitly refer to 'high-risk AI models' as a regulated category, nor are there specific provisions relating directly to them. Moreover, there are no specific provisions relating to AI models which are not general-purpose AI models.\n\nThis means that unless an AI model is general-purpose, or part of an AI system or practice which is high-risk, transparency requiring, or prohibited, it is not in scope of\n\nthe AI Act.\n\nHowever, in most scenarios, an AI model (or AI models) will be one of the most important components of a broader AI system, which could either be high-risk or used for a prohibited practice. Therefore, AI models are regulated in this 'indirect' but consequential sense.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nImage credit: This helpful infographic from the team at Digiphile highlights what is and what is not regulated under the AI Act.\n\nInterestingly, the AI Act does not include a legal definition of the term 'AI model'.\n\nArticle 3 lists 68 different definitions, for terms like 'AI system', 'training data', and 'general-purpose AI model' (see below). Despite there being a definition of an important and common type of AI model (i.e., a general-purpose one), there is no definition of an 'AI model' itself, which is a broader and arguably more important and foundational concept.\n\nIn my view, given the centrality of AI models to high-risk AI systems, general-purpose AI models, virtually any type of prohibited AI practice, and the wider field of AI, it would have been helpful for the AI Act to include an official legal definition of an AI model.\n\nFor context, the AI Act defines an AI system as: 'a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments' .\n\nA general-purpose AI model is defined as: ' an AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market '.\n\nThe concept of an AI model has been defined in other sources.\n\nNIST defines an AI model as: ' a component of an information system that implements AI technology and uses computational, statistical, or machine-learning techniques to produce outputs from a given set of inputs '.\n\nIn its paper on the updated definition of an AI system, the OECD refers to an AI model as 'a core component of an AI system used to make inferences from inputs to produce outputs'.\n\n<!-- image -->\n\nImage credit: OECD visual on the definition of an AI system, which highlights that an AI model is an essential component of an AI system.\n\n## Misconception 4: Emotion recognition is prohibited under the EU AI Act\n\nArticle 5(1) stipulates that the following AI practice is prohibited : placing on the market, putting into service, or using ' AI systems to infer emotions of a natural person in the areas of workplace and education institutions, except where the use of the AI system is intended to be put in place or into the market for medical or safety reasons'.\n\nThis means that the use of AI-enabled emotion recognition is only prohibited in specific, pre-defined contexts; namely, the workplace and educational institutions.\n\nIt also means that AI-enabled emotion recognition could potentially be lawfully used in workplace and educational settings, if it can be demonstrated that this supports medical or safety objectives.\n\nFor example, using AI to infer and predict the emotional state of a pilot while they are flying a plane, for the sole purpose of determining the pilot's future bonus or compensation package, would almost certainly be prohibited. However, if the AI system is used solely to initiate safety-critical interventions, which could prevent potentially harmful incidents, then this would likely be permitted under the AI Act.\n\nFurthermore, AI systems used for emotion recognition in settings other than the workplace and educational institutions are classified as high-risk, not prohibited. This is clarified in Annex III(1), which lists high-risk AI systems and includes ' AI systems intended to be used for emotion recognition '.\n\nThis means that providers and deployers developing, making available, or using emotion recognition systems in contexts other than the workplace and educational institutions, as well as emotion recognition systems for safety or medical reasons in the two aforementioned settings, must adhere to the obligations and requirements for high-risk AI systems.\n\nTo understand exactly what constitutes an emotion recognition system (that could either be high-risk or prohibited, depending on the context), we need to consult the definition provided in Article 3(39): '' an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data '.\n\nIn practice, this implies that unless it is based on biometric data (e.g., voice or facial expressions), using AI for emotional recognition would not be classified as high-risk or prohibited, and thus may not even be in scope of the AI Act.\n\nFor example, if I copy and paste a colleague's email into a generative AI tool, in advance of an important meeting, and ask it to predict their emotional state at the time of writing, this would most likely not be considered a prohibited AI practice entailing a hefty penalty, given the lack of biometric data being processed. However, if this was done via an AI assessing a video recording of a meeting in which their camera was on, the situation could be quite different.\n\nEuropean Commission guidelines on the topic have confirmed that an AI system inferring emotions from written text (i.e., content/sentiment analysis) is not prohibited as it does not constitute emotion recognition, because it is not based on biometric data.\n\n## Misconception 5: Facial recognition is prohibited under the EU AI Act\n\nSimilarly, there is no blanket prohibition on the development and use of facial recognition systems, which are already widely deployed in society.\n\nHowever, there are prohibitions on the ways in which law enforcement can use facial recognition and similar technologies to perform remote biometric identification of people in real-time.\n\nAlso, if facial recognition technology was used for a prohibited AI practice, this would still be prohibited. However, this does not amount to a blanket prohibition.\n\nConcretely, law enforcement use of 'real-time' remote biometric identification systems in public (e.g., facial recognition used to identify and stop flagged people in public) is prohibited, apart from in specific and narrowly defined scenarios.\n\nAcceptable scenarios for law enforcement use of such technology includes searching for victims of serious crime, preventing imminent threats to life (e.g., terrorist attacks), and locating suspects or perpetrators of serious crimes (e.g., murder).\n\nBefore AI is used in this way, independent judicial or administrative authorisation must be granted. Also, the use can only occur for a limited time period, with safeguards to protect privacy and fundamental rights.\n\nThis became a totemic issue during the AI Act trilogue negotiations and legislative process. The European Parliament's initial AI Act proposals called for an outright ban on the use of real-time biometric identification systems in public, like facial recognition. This ban would have applied to law enforcement authorities and any other organisation, with no exceptions. The Council (EU member states) were never going to accept an outright prohibition and a compromise was brokered.\n\nSome types of AI-enabled facial recognition, which are permitted, would be classified as a high-risk AI system.\n\nAnnex III(1) clarifies that 'remote biometric identification systems', 'AI systems intended to be used for biometric categorisation', and 'AI systems intended to be used for emotion recognition' are high-risk AI systems.\n\nIt is also conceivable that a facial recognition system or component could be used as part of any other high-risk AI system listed in the AI Act.\n\nHowever, it is also conceivable that a facial recognition system or component could be\n\npart of an AI system which is not high-risk, nor in scope of the AI Act. This is because AI systems used for 'biometric verification', which perform the sole function of confirming or authenticating that an individual is who they claim to be, are not highrisk.\n\nA facial recognition system used to unlock your phone would not be classified as highrisk, as it is merely confirming that you are who you claim to be (i.e., the owner of the phone). This is in contrast to live facial recognition cameras used by police to identify potential suspects from a crowd, as their facial data is being compared to a larger reference database of faces, with the goal of establishing a match.\n\nFinally, as per Article 5(e), it is prohibited to use AI systems to conduct untargeted scraping of facial images, from the internet or CCTV footage, with the goal of creating or expanding databases which are used to develop or operate facial recognition systems. However, this does not prohibit the use of facial recognition in a broader sense, merely a specific data scraping practice.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.", "fetched_at_utc": "2026-02-09T13:42:27Z", "sha256": "7dd2f38489b3bb5f0a5c817f6d61894886addedaa96c9dae00a3e5c50e156852", "meta": {"file_name": "Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel.pdf", "file_size": 1601603, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-debunking-10-common-eu-ai-act-misconceptions-part-2-oliver-patel-8e29920044d1", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel.pdf", "title": "Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable, and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement, and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nThis two-part series on Enterprise AI Governance presents and debunks 10 common misconceptions about the AI Act, providing a detailed explanation for each one. The first 5 were covered in part 1 and the second 5 are covered in this edition.\n\n## The ten misconceptions are:\n\n1.  The EU AI Act has a two-year grace period and applies in full from August 2026.\n2.  All open-source AI systems and models are exempt from the EU AI Act.\n3.  High-risk AI models are explicitly regulated under the EU AI Act.\n4.  Emotion recognition is prohibited under the EU AI Act.\n5.  Facial recognition is prohibited under the EU AI Act.\n6.  Transparency is required for 'limited risk' AI systems.\n\n7.  Third-party conformity assessments are required for all high-risk AI systems.\n8.  Fundamental rights impact assessments are required for all high-risk AI systems.\n9.  All high-risk AI systems must be registered in the public EU-wide database.\n10.  Deployers do not need to register their use of high-risk AI systems.\n\n<!-- image -->\n\nðŸ§‘ðŸŽ“ If you want to dive much deeper, register interest for my EU AI Act Compliance Bootcamp here . This will be an exclusive and intimate masterclass for AI governance leaders, breaking down how to implement AI Act compliance in an enterprise setting. More information will be shared later in the year.\n\nNote: if you already registered interest for my AI Usage Policy Bootcamp, you do not need to register here again. Stay tuned for further info.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\nMisconception 6. Transparency is required for 'limited risk' AI systems.\n\nThis is an important example, because the AI Act does not use the term 'limited risk' to refer to the AI systems for which transparency is required. Therefore, it does not make sense to use this term. It would be like using completely different and inappropriate terminology when referring to 'high-risk AI systems' or 'prohibited AI practices'.\n\nWhat the AI Act actually says is that for certain AI systems, there are transparency obligations for providers and deployers. Therefore, more accurate terminology than 'limited risk' is 'transparency requiring AI systems'. It may not roll off the tongue as smoothly, but it does the job.\n\nAll the AI Act risk pyramid images which are widely circulated (including sometimes by EU departments), which use the term 'limited risk' instead of 'transparency requiring' (or something of that nature) are arguably fuelling this misconception.\n\nThis misconception has taken hold due to a lack of precision regarding the language which is used to talk about the AI Act. This is problematic, because in a complex legal text like this, there are many different terms which, although they may sound conceptually similar, mean different things and can give rise to very different realworld consequences.\n\nTo accurately and effectively understand, interpret, and comply with a law of this complexity, precision and care with language is key.\n\nArticle 50 outlines the transparency obligations for certain AI systems. The AI systems are:\n\n- AI systems intended to interact directly with people;\n- AI systems that generate synthetic image, audio, video or text content;\n- emotion recognition systems;\n- biometric identification systems; and\n- AI systems which generate or manipulate deep fake content.\n\nProviders and deployers of these AI systems are obliged to implement additional transparency measures, such as notification and disclosure, which are detailed in Article 50.\n\nThese 'transparency requiring AI systems' can simultaneously be high-risk AI systems, or AI systems which are not high-risk. In the former scenario, all applicable obligations and requirements for high-risk AI systems would also apply. In the latter scenario, only the transparency obligations in Article 50 would apply.\n\nThis is another reason why the 'classic' yet misleading AI Act risk pyramid doesn't\n\nwork: it falsely implies that the obligations for high-risk AI systems and 'transparency requiring AI systems' are mutually exclusive.\n\nThis point has been observed and elaborated on by other analysts, such as Aleksandr Tiulkanov, who has produced two helpful infographics to help conclusively debunk this misconception.\n\nSee below for the two original images, which have not been modified, and were posted on Aleksandr's article 'EU AI Act: Getting the Basics Straight'.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nMisconception 7: Third-party conformity assessments are required for all high-risk AI systems\n\nUnder the AI Act, providers are obliged to ensure that their high-risk AI systems undergo a conformity assessment before they are placed on the market or put into service.\n\nA conformity assessment is an established and longstanding practice for many products regulated by EU product safety laws.\n\nThe AI Act defines a 'conformity assessment' as ' the process of demonstrating whether the requirements set out in Chapter III, Section 2 relating to a high-risk AI system have been fulfilled'. This means the focus of the conformity assessment is largely on the following requirements:\n\n- risk management system,\n- data and data governance;\n- technical documentation;\n- record-keeping;\n- transparency and provision of information to deployers;\n- human oversight; and\n- accuracy, robustness and cybersecurity.\n\nOnce the conformity assessment is completed, the provider must produce the 'declaration of conformity', which can be inspected by other parties, like importers or distributors.\n\nHowever, not all high-risk AI systems are obliged to undergo the same conformity assessment procedure. The key distinction is between the two main categories of highrisk AI system:\n\n1.  AI systems which are products, or safety components of products, regulated by one of the EU product safety laws listed in Annex I; and\n2.  AI systems listed in Annex III.\n\nFor the first category of high-risk AI system, the requirement to undergo a third-party conformity assessment is already stipulated in the existing laws which are referenced in Annex I. Moreover, the AI systems covered by these existing laws are not classified as high-risk under the AI Act unless the relevant product is already required to undergo a third-party conformity assessment.\n\nTo avoid burdensome and duplicative compliance work, the AI Act does not create a new and additional conformity assessment regime for these AI systems.\n\nWhat it says is that the existing conformity assessment procedure, performed by a third-party (i.e., a notified body) must continue to be followed, but that it must also\n\nnow consider and include the new requirements outlined in the AI Act. This necessarily entails an update to, and augmentation of, those existing third-party conformity assessment procedures.\n\nHowever, for the second category of high-risk AI systems (i.e., those listed in Annex III), there is no obligation for a third-party conformity assessment. Rather, providers must perform a 'self-assessment', which does not involve a third-party (i.e., a notified body).\n\nArticle 43(2) refers to this as the ' conformity assessment procedure based on internal control '. Annex VI provides further information about how providers must perform this self-assessment.\n\nEU legislators opted for this approach due to the lack of AI certification expertise and maturity in the wider market, which was deemed an impediment to notified bodies performing conformity assessments across all these domains, at least for now. However, Recital 125 suggests that in future, as the market matures, these conformity assessments may also be performed by notified bodies.\n\nThe only partial exception to this is high-risk AI systems used for biometrics, including:\n\n- remote biometric identification;\n- biometric categorisation; and\n\n- emotion recognition.\n\nIn certain scenarios, a third-party conformity is required for these high-risk AI systems. For example, if the provider has not fully applied an official technical standard to demonstrate compliance, or if such a standard does not exist, then the conformity assessment must be performed by a notified body. The procedure for this is outlined in Annex VII.\n\n## Misconception 8: Fundamental rights impact assessments are required for all high-risk AI systems\n\nPerforming a fundamental rights impact assessment is an important obligation which applies to certain deployers of specific high-risk AI systems.\n\nWhere applicable, a fundamental rights impact assessment must be performed by the deployer prior to using the AI system.\n\nThe fundamental rights impact assessment is a formal exercise where deployers must consider, describe, and document how and when they will use the AI system, which people or groups will be impacted by it, the specific harms likely to impact those\n\npeople, how human oversight will be implemented, and what will be done by the deployer if any risks materialise or harm arises.\n\nIt does not apply to providers, nor does it apply to all deployers or all high-risk AI systems.\n\nThe obligation only applies to the following deployers:\n\n- bodies governed by public law;\n- private entities providing public services;\n- deployers of AI systems used for life and health insurance risk assessment and pricing; and\n- deployers of AI systems used for creditworthiness evaluation and credit score assessment.\n\nFor the deployers which are governed by public law, or private entities providing public services, they must only perform a fundamental rights impact assessment before using one of the high-risk AI systems in Annex III. However, this excludes AI systems used for safety management and operation of critical infrastructure, for which no fundamental rights impact assessment is required.\n\nIn practice, this means that most businesses will not have to perform a fundamental rights impact assessment prior to using a high-risk AI system. This is either because it does not apply to them as a deployer, or it does not apply to the high-risk AI system they are using, or both.\n\nHowever, for some financial services and insurance firms, as well as companies which provide AI-driven public services in domains like welfare, education, and border control, this will become an important part of their AI governance and compliance work.\n\nOnce the fundamental rights impact assessment has been completed, the deployer must notify the relevant regulator and, if applicable, summarise its findings in their registration entry in the EU database for high-risk AI systems ( see misconception 10 below ).\n\nWhilst providers of all high-risk AI systems are required to perform risk assessments and implement risk management measures (see Article 9)-which includes considering the potential risk to fundamental rights-this is not the same as a dedicated fundamental rights impact assessment.\n\n## Misconception 9: All high-risk AI systems must be registered in the public EU-wide database\n\nArticle 71 of the AI Act mandates the European Commission to establish and maintain an EU database for high-risk AI systems. It must be ' publicly accessible, free of charge, and easy to navigate '.\n\nProviders of certain high-risk AI systems are obliged to register information about their AI systems and their organisation in this database. They must do this before placing the high-risk AI system on the market or putting it into service.\n\nTo dispel this misconception, there are four important points you should understand, each of which are explained below:\n\n1.  The registration obligation does not apply to all high-risk AI systems.\n2.  The registration obligation applies to some AI systems which are not technically high-risk.\n3.  Not all information in the database will be available to the public.\n4.  Some high-risk AI systems must be registered in a national database, instead of the EU database.\n1. The registration obligation does not apply to all high-risk AI systems\n\nThe registration obligation, outlined in Article 49, only applies to high-risk AI systems listed in Annex III. This includes AI systems used for recruitment, health insurance pricing, and educational admissions.\n\nHowever, this excludes AI systems which are products, or safety components of products, regulated by one of the EU safety laws listed in Annex I. This means that AI systems used in critical, regulated domains, like medical devices, vehicles, and aviation do not need to be registered in the EU's public database.\n\nThere are various other ways in which the AI Act compliance obligations and requirements differ across these two main categories of high-risk AI system, such as with respect to conformity assessments ( as outlined in misconception 7 above ).\n\n2. The registration obligation applies to some AI systems which are not technically highrisk\n\nArticle 6(3) specifies an important caveat for the classification of high-risk AI systems. It states that high-risk AI systems listed in Annex III are not high-risk if they do not ' pose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing the outcome of decision making '.\n\nFour potential conditions are provided. If one of these is met, then the AI system does\n\nnot pose this type of risk and is thus not classified as high-risk. For example, one condition is that the AI system performs only a ' narrow procedural task '.\n\nInterestingly, even where a provider legitimately determines-as per the Article 6(3) exception procedure-that their AI system is not high-risk, they must still register that AI system in the EU's public database.\n\nThe logic of this is to promote transparency regarding how providers are determining and documenting that AI systems are not high-risk, despite being used in sensitive (Annex III) domains.\n\nThis could lead to a potentially vast number of AI systems needing to be registered and many organisations being unaware that they are obliged to do so.\n\n3. Not all information in the database will be available to the public\n\nFor certain high-risk AI systems listed in Annex III, including AI systems used for law enforcement, migration, asylum, and border control management, the information will be registered and stored in a ' secure non-public section ' of the EU database.\n\nThis includes lawful AI systems used for remote biometric identification, biometric categorisation, and emotion recognition, in the context of law enforcement, migration, asylum, and border control management.\n\nThese providers are obliged to register less information than the providers of the other high-risk AI systems, and that information can only be viewed by the European Commission and the specific member state regulators who have been designated to lead on AI Act enforcement for those sensitive sectors.\n\nTherefore, many of the AI systems which are the most sensitive, and arguably pose the greatest risk to fundamental rights, will not be publicly registered.\n\n4. Some high-risk AI systems must be registered in a national database, instead of the EU database.\n\nProviders of high-risk AI systems that are used as safety components for the operation and management of critical infrastructure, like water and energy, are obliged to register their AI systems, and themselves, at the 'national level'.\n\nThis means that the registration of these AI systems will be in different databasesmaintained by member state regulators and/or governments-separate from the public database maintained by the European Commission.\n\nThe AI Act does not reveal much about these national level databases. However, there is no provision which states that they must be public. This signals an acknowledgement of the sensitivity and secrecy of these domains, because of their importance to national security and economic stability.\n\n## Misconception 10: Deployers do not need to register their use of highrisk AI systems\n\nCertain deployers of specific high-risk AI systems are also obliged to register information about their organisation, and the high-risk AI system they are using, in the EU database. They must do this before using the AI system.\n\nThis obligation only applies to deployers that are ' public authorities, EU institutions, bodies, offices or agencies or persons acting on their behalf '.\n\nThe focus is therefore on public sector organisations in the EU using high-risk AI, as opposed to private sector and commercial AI usage.\n\nThese organisations must register themselves, and select which high-risk AI system they are using, in the EU database. It will not be possible to do this if the provider has not already registered the high-risk AI system. Therefore, this is something which should be checked by the deployer, as part of procurement and partnership due diligence.\n\nThis also means that the deployer registration obligation only applies to the high-risk AI systems which providers are obliged to register. This includes the high-risk AI\n\nsystems used for law enforcement, migration, asylum and border control management, however the deployer information will also be registered the ' secure non-public section ' of the EU database, which only the European Commission and certain public authorities can access.\n\nThere is no explicit provision which stipulates that deployers must register their use of AI systems used for critical infrastructure safety management and operation, which providers must register at the national level instead of the EU-wide database.\n\nThere is also nothing which indicates that deployers are obliged to register their use of AI systems which are not high-risk, but have nonetheless been registered as they fell under the scope of an Article 6(3) derogation.\n\nFinally, there are certain scenarios when deployers can become providers of high-risk AI systems, either intentionally or inadvertently. For example, if they make a 'substantial modification' to the AI system and it remains high-risk. In these scenarios, the new provider would be required to register the high-risk AI system.\n\n<!-- image -->\n\nðŸ§‘ðŸŽ“ If you found this post useful, register interest for my EU AI Act Compliance Bootcamp here . This will be an exclusive and intimate masterclass for AI governance leaders, breaking down how to implement AI Act compliance in an enterprise setting.", "fetched_at_utc": "2026-02-09T13:43:00Z", "sha256": "8e29920044d1345515d021988025ec4a2940ac6a897659cecffdc7b92db2ef37", "meta": {"file_name": "Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel.pdf", "file_size": 1467187, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-eu-ai-act-simplification-oliver-patel-3edb2b6586f2", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\EU AI Act simplification - Oliver Patel.pdf", "title": "EU AI Act simplification - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance and author of the forthcoming book, Fundamentals of AI Governance (2026).\n\nOn Wednesday 19 November 2025, the European Commission will publish its Digital Omnibus Package-a sweeping proposal to reform the EU's digital legislative framework. The EU AI Act is in sharp focus, alongside the GDPR and a host of other flagship EU laws. This article explains how we got here, what's at stake, and the potential changes being discussed in Brussels. It will be followed by in-depth analysis of the proposed changes, once they are published.\n\n## How did we get here?\n\nThe EU has consistently sought to shape global regulatory standards on digital technology, data, and AI. The EU has been open about its policy objectives of protecting citizens and promoting global regulatory convergence towards its high standards, in domains like data protection and privacy. And in some ways, it has succeeded.\n\nWith GDPR, for example, there has been a demonstrable 'Brussels effect'. Many countries worldwide have enacted data protection laws of a similar flavour and\n\ncorporations have prioritised aligning their internal frameworks accordingly. Despite the lack of U.S. federal privacy law (which renders the U.S. a global outlier), almost all major American enterprises are significantly impacted by the GDPR and have implemented dedicated privacy compliance programmes.\n\nHowever, as EU laws covering digital governance have proliferated, the picture has become increasingly complex and convoluted. Keeping track of the EU's regulations and directives covering the digital sphere is no mean feat.\n\nCEPS and Kai Zenner's dataset of EU digital sector legislation, updated in July 2025, lists 101 different laws , including the EU AI Act and the GDPR. The authors remark that there has been an ' absolute explosion ' of EU digital laws and that 'even the best experts struggle to keep up with this torrent of legal and policy instruments'. To complement the 101 laws, there exists a far greater number of regulatory bodies covering digital issues.\n\nEnterprises require large teams of people to make sense of these developments and determine how to be compliant, not least because obtaining deep expertise in just one legislative area takes years. In response, enterprises have also established a range of somewhat separate yet overlapping digital governance capabilities to address key legal obligations and requirements. This includes implementing enterprise AI governance, which has become a core priority in recent years due to both the EU AI\n\nAct and the surge in AI adoption. But you didn't need me to tell you thatâ€¦\n\nFor AI governance professionals, this regulatory complexity is especially acute, as AI systems frequently trigger multiple regulatory requirements simultaneously-from GDPR's rules on automated decision-making and use of sensitive personal data, to the AI Act's requirements for high-risk and transparency-requiring AI systems, cybersecurity mandates, copyright and intellectual property, and existing sectoral laws.\n\nIn recent months, discussion regarding simplification of the EU's digital rulebook has intensified-both within the EU and externally. It is widely reported that EU officials and member states have faced sustained lobbying from U.S. industry and the Trump administration. There are even reports of President Trump considering imposing tariffs and sanctions on the EU and its officials, due to the perceived impact of EU digital laws on U.S. companies and citizens.\n\nThis pressure has been accompanied by increasingly vocal calls from segments of European industry that EU digital regulations need to be reformed, streamlined, simplified, to reduce compliance burdens. For example, in July 2025, dozens of companies-including industry heavyweights like Mercedes Benz, Deutsche Bank, and L'Oreal-signed an open letter to the EU urging for a two-year delay to the EU AI Act's key provisions on general-purpose AI (GPAI) models and high-risk AI systems.\n\nThis is coming to a head on Wednesday 19 November, as the European Commission is set to publish its 'Digital Omnibus Package' proposal. This will outline, for the first time, the comprehensive set of legislative amendments the Commission proposes, to simplify the EU's digital rulebook.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n## What was the Draghi report?\n\nAlthough this situation has evolved over several years, the September 2024 publication of the 'Draghi report' on The Future of European Competitiveness was an important milestone in this story. Mario Draghi is the former president of the European Central Bank and former prime minister of Italy. This independent report, commissioned by the EU, is shaping aspects of the European Commission's policy agenda.\n\nThe report presents the EU's economic challenges, focusing on diminished competitiveness, weakening productivity, and slowing growth. Draghi frames the economic outlook as an 'existential challenge' for the EU, arguing that it will be unable\n\nto finance its social model, achieve its environmental ambitions, and deliver the prosperous and fair society that represents its raison d'Ãªtre if it fails to make radical changes.\n\nThe Draghi report outlines three action areas to 'reignite growth'. These are:\n\n1.  Closing the innovation gap with the U.S. and China, especially in advanced technologies.\n2.  Forging a joint plan for decarbonisation and competitiveness.\n3.  Increasing security and reducing dependencies.\n\nIt is the first action area which is most relevant for our focus-EU AI Act simplification. The Draghi report argues that the EU's complex digital regulatory environment impedes innovation and growth. It elevates 'reducing the regulatory burden' as a core priority for transforming the EU's economic prospects.\n\nExamining the 'innovation gap' between the U.S. and China on the one hand and the EU on the other, the report claims that innovative European companies attempting to scale up are 'hindered at every stage by inconsistent and restrictive regulations'. This is why, the report argues, European tech entrepreneurs routinely seek to grow their businesses in the U.S. The report also cites that 55% of SMEs flag 'regulatory obstacles and administrative burden' as their greatest challenge, arguing that these 'regulatory\n\nburdens' are particularly damaging for digital sector SMEs.\n\nThe report criticises the 'precautionary approach' taken by EU digital laws, including the EU AI Act. It specifically calls out the compute threshold for determining whether a general-purpose AI (GPAI) model poses 'systemic risk', noting that various frontier AI models already exceed the threshold, despite the EU AI Act's nascency. It also highlights the increasing difficulty that companies face in navigating the various overlapping laws relevant for AI and the hundreds of regulatory bodies across the EU responsible for digital governance.\n\nIt is important to note that a vast array of other impediments and challenges are highlighted-from inadequate research talent pipelines to low investment in innovation commercialisation-as contributing factors to the EU's competitiveness challenge. Therefore, my intention is not to claim that the Draghi report is all about digital 'regulatory burdens', as that would be misleading. It would be equally misleading to claim the Digital Omnibus Package results solely from the Draghi report. However, the complexity of the EU's digital legislative framework has undeniably become a totemic issue, which the Draghi report shone a very bright light on.\n\nNext, we turn our attention to the European Commission's Digital Omnibus Package. Will it deliver the changes that Draghi seeks? And what could this mean for the EU AI Act?\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n## What is the EU's Digital Omnibus Package?\n\nThe Digital Omnibus Package is the European Commission's much anticipated proposal to reform the EU's digital legislative framework. It is due to be announced and published on Wednesday 19 November. This follows public consultations earlier in the year.\n\nThe proposal will feature meaningful amendments to some of the EU's flagship laws and perhaps even the repeal of specific instruments. The Digital Omnibus Package is expected to focus on the EU AI Act, the GDPR, the ePrivacy Directive, the Data Act, and the NIS2 Directive-horizontally cutting across the core digital governance domains of AI, data protection and privacy, and cyber security.\n\nThe purpose of the Digital Omnibus Package is to simplify and streamline the EU's digital legislative framework, to ease compliance burdens and cut costs for organisations (particularly startups and SMEs), promote the growth and\n\ncompetitiveness of European companies, and boost innovation. It is inevitable that the core threads from the Draghi report will be woven into the European Commission's proposal.\n\nBefore engaging in any further analysis of the imminent proposal, several caveats are required.\n\nFirst, the current discussion, including this article, is based on leaked documents, media reporting, speculation, and rumours. Brussels is a famously leaky city, so this is nothing new. However, until the European Commission officially publishes its proposal, we do not know exactly what the proposal consists of. At the time of writing, nothing official has been published.\n\nSecond, the Digital Omnibus Package that will be published later this week merely represents the European Commission's initial proposal on this controversial set of issues. Therefore, even when we have the proposal, all we will have is the official starting position of one of the EU's institutions. To amend EU laws in this way will require extensive trilogue negotiations and approval from the European Parliament and EU member states via the Council of the EU (the Council).\n\nThird, the aforementioned trilogue negotiations-and the process of amending and repealing a suite of EU laws in this way-are bound to be lengthy, complex, and\n\nfraught with drama. Although the precise legislative instrument to operationalise the Digital Omnibus Package is yet to be confirmed, it is most likely to be an EU regulation (which is the same legal instrument as laws like the GDPR and the EU AI Act).\n\nTo amend existing EU laws via a new regulation, the EU's 'ordinary legislative procedure' must be followed. This necessitates dual approval from both the European Parliament and the Council, following trilogue negotiations where both institutions have several opportunities to amend and update the legislative proposal. On average, it takes the EU 19 months to agree new laws, from the initial Commission proposal to formal adoption.\n\nPlainly speaking, what all this means is that:\n\n- We don't yet have an official proposal from the European Commission, merely leaks and media speculation.\n- When we do, it will be subject to lengthy and fraught trilogue negotiations, the outcome of which is impossible to predict.\n- However, what can be predicted with a degree of certainty is that there will be a substantial difference between the European Commission's initial legislative proposal and the final text that is voted on.\n\n- Following this, formal approval will be required from both the European Parliament and the Council to pass any regulation that meaningfully amends existing EU laws.\n- It is impossible to predict what the final legislative text will consist of and how long the negotiation and approval process will take.\n- Finally, there is no guarantee that any legislative changes will be approvedalthough this does seem unlikely given the various points made above.\n\n## How could the EU AI Act change?\n\nCaveats aside, the final part of this article will highlight some of the potential changes that are reported to be on the cards for the EU AI Act.\n\nAlthough based primarily on leaks and tip-offs, recent media reporting nonetheless shines a light on what is being discussed in the EU's corridors of power. Below is a list of the various potential EU AI Act changes that have been reported. A special shout out for the reporting from MLex's Luca Bertuzzi (who is a must follow for AI governance practitioners), which this list is largely based on:\n\n- Delaying the applicable date for the obligations for providers and deployers of transparency-requiring AI systems.\n- Delaying the applicable date for the obligations for providers and deployers of high-risk AI systems.\n- These obligations apply from 2 August 2026, but this enforcement could reportedly be delayed for one year. This is partly due to delays in finalising technical standards that organisations can use to comply with these provisions.\n- Scrapping the AI literacy obligation for organisations and shifting it to governments, regulators, and EU institutions.\n- Centralising enforcement powers with the European Commission's AI Office.\n- This could be done by designating the AI Office as the regulatory authority responsible for supervising AI systems based on GPAI models. This is partly driven by concerns regarding readiness and capacity of EU member state regulators (some of which have not yet been designated), as well as the complexity firms could face in engaging with many different regulatory bodies.\n- Introducing smaller and more proportionate compliance penalties for 'small midcaps' (defined as companies that employ up to 750 people and have an annual turnover of under â‚¬150 million).\n\n- This would complement and expand the scope of the existing proportionality for compliance penalties for SMEs.\n- Removing the obligation for providers to register, in the EU's public database, AI systems that are used in a high-risk domain which they have deemed and demonstrated are not high-risk due to the nature of the use.\n\nIt will be interesting to see which (if any) of these potential changes survive in the Digital Omnibus Package that the European Commission publishes on Wednesday. Given the speculative nature of this, I will not provide any further analysis on these potential changes in this article.\n\nOne final point to note is that the European Commission does have powers to amend or change aspects of the EU AI Act, via instruments called delegated acts and implementing acts. For example, the European Commission can modify the compute threshold for GPAI models with systemic risk via a delegated act.\n\nWhere these powers exist, the European Commission is able to drive changes without the need for the ordinary legislative procedure and formal approval from the European Parliament and Council. Although, there is always a degree of oversight from these institutions, who retain veto powers. However, the EU AI Act 'simplification' changes discussed above extend far beyond the scope of the Commission's powers to drive changes via delegated and implementing acts.", "fetched_at_utc": "2026-02-09T13:43:23Z", "sha256": "3edb2b6586f21735bcdf9dd073ce2f7012159744eb349f5f8b8f9fd3ab09cde0", "meta": {"file_name": "EU AI Act simplification - Oliver Patel.pdf", "file_size": 682333, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-european-union-artificial-intelligence-act-bird-bird-778dc0538cbf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\European Union Artificial Intelligence Act - Bird & Bird.pdf", "title": "European Union Artificial Intelligence Act - Bird & Bird", "text": "## European Union Artificial Intelligence Act: a guide\n\n7 April 2025\n\n## Contents\n\n<!-- image -->\n\n- OVERVIEW, KEY CONCEPTS &amp; TIMING OF IMPLEMENTATION 1\n\nOverview Key concepts\n\nTimeline\n\n- MATERIAL AND TERRITORIAL SCOPE 2\n\n<!-- image -->\n\nMaterial scope Territorial scope Exclusions\n\nRelationship with other regulatory frameworks\n\n- PROHIBITED AI PRACTICES 3\n\n<!-- image -->\n\nProhibited AI practices To whom do the prohibitions apply? Enforcement and Fines\n\n- HIGH-RISK AI SYSTEMS 4\n\nClassification of an AI system as a high-risk AI system\n\nObligations for providers of high-risk AI systems\n\nHarmonised standards and conformity assessment procedure for providers of high-risk AI systems\n\nObligations for deployers of high-risk AI systems Obligations for other parties in connection with high-risk AI systems\n\n- GENERAL-PURPOSE AI MODELS 5\n\nBackground and relevance of general-purpose AI models of personal data\n\nTerminology and general-purpose AI value chain Obligations for providers of general-purpose AI models\n\nGeneral-purpose AI models with systemic risk\n\n<!-- image -->\n\nLegal 500 for Artificial Intelligence\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- TRANSPARENCY OBLIGATIONS 6\n\nGeneral transparency obligations\n\nTransparency obligations for high-risk AI systems Timing and format\n\nTransparency obligations at the national level and codes of practice\n\nRelationship with other regulatory frameworks\n\n- REGULATORY SANDBOXES 7\n\nAI regulatory sandboxes Real-world testing of AI systems\n\n- ENFORCEMENT &amp; GOVERNANCE 8\n\n<!-- image -->\n\nOverview Post-marking obligations Market surveillance authorities Procedures for enforcement Authorities protecting fundamental rights General-purpose AI models Penalties Remedies for third parties Governance\n\n- AI ACT: WHAT'S NEXT 9\n\nAI Act application deadlines Delegated Acts Implementing Acts Commission Guidelines Codes of conduct and practice Standards Liability\n\n- OUR GLOBAL CONTRIBUTORS 10\n\nDistinguished for our client satisfaction\n\n<!-- image -->\n\n## Overview, key concepts &amp; timing of implementation\n\n## Overview\n\nThe European Union (EU) stands as a pioneer in the regulation of artificial intelligence (AI), setting a global benchmark with its proactive approach to ensuring ethical and responsible AI development. Indeed, it seems we may witness a new Brussels effect, reminiscent of the influence wielded by the GDPR. The EU's comprehensive and precautionary framework prioritises transparency, accountability, and human rights.\n\nThe AI Act applies beyond the borders of the EU - many of its provisions apply regardless of whether the providers are established or located within the EU or in a third country. The AI Act applies to any provider or entity responsible for deploying an AI system if ' the output produced by the system is intended to be used ' in the EU. Foreign suppliers must appoint an authorised representative in the Union to ensure compliance with the Act's provisions. However, the AI Act does not apply to public authorities of third countries or to international organisations under police and judicial cooperation agreements with the Union, nor to AI systems placed on the market for military defence or national security purposes. This broad scope aims to ensure comprehensive regulation of AI systems and their uses.\n\n## What you can expect from this guide\n\n- This chapter provides an overview of the whole AI Act, its key concepts and the dates from when its provisions will apply.\n- Chapter 2 looks at the territorial and material scope of the AI Act.\n- Chapters 3, 4, 5 and 6 address the requirements the AI Act imposes on different types of AI - prohibited practices; high risk systems; general purpose AI; and AI where greater transparency is needed.\n- Chapter 7 explains the AI Act's arrangements for testing AI in regulatory sandboxes. Chapter 8 looks at governance and enforcement.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n4\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- Chapter 9 summarises the numerous further measures that have to follow the adoption of the AI Act.\n- Last, Chapter 10 includes all the contributors to this guide.\n\n## A risk-focused approach\n\nThe EU approach to AI regulation is characterised by its risk-based framework. This regulation adopts a technology-neutral perspective, categorising AI systems based on their risk level, ranging from minimal to high risk. This system ensures that higher-risk AI applications, particularly those that can significantly impact fundamental rights, are either prohibited or subjected to stricter requirements and oversight.\n\nThe EU places a strong emphasis on promoting the development and use of responsible AI. The AI Act mandates strict measures for data security and user privacy, ensuring that AI systems are designed and deployed with these considerations at the forefront. This includes rigorous requirements for how data is handled and protected, ensuring that users' personal information remains secure.\n\nAdditionally, the AI Act requires comprehensive risk assessments for AI systems. These assessments help identify and mitigate potential risks associated with AI technologies, fostering transparency and accountability among AI providers. By making these evaluations mandatory, the EU ensures that AI developers thoroughly understand and address the implications of their technologies.\n\nThis proactive approach aims to build public trust in AI technologies by protecting users' rights and well-being. By prioritising data security, privacy, and risk management, the EU seeks to reassure the public that AI can be used safely and ethically. This focus on responsible development helps to promote broader acceptance and integration\n\nof AI technologies, ultimately benefiting society as a whole. The AI Act has been developed not only to create laws for AI systems, but also to establish an ethical framework for their use, to ensure that organisations consider the impact of their AI systems on people, other businesses, the environment and many other aspects of our lives.\n\n## Ethics at the heart of the AI Act\n\nThe AI Act explicitly builds on the Ethical Guidelines on Trustworthy AI, which were published by the European Commission in 2019. While these guidelines remain non-binding, many of their principles have been directly incorporated into the AI Act. The best example of this approach is that in many of its provisions, the AI Act refers directly to the fundamental rights enshrined in the Charter of Fundamental Rights of the European Union. For example, high-risk AI systems are those that have a significant harmful impact on the health, safety and fundamental rights of persons in the Union.\n\nThe proper application of the AI Act will in many cases require an analysis of the risks to fundamental rights, which includes both legal and ethical issues. It can therefore be said that ethics has been embedded into the AI Act.\n\n## Governance\n\nThe European Union adopts a decentralised supervision model, promoting collaboration with various national authorities. The AI Act establishes the European Artificial Intelligence Office (the AI Office) as an independent entity, serving as the central authority on AI expertise across the EU, and playing a crucial role in implementing of the legal framework. This office will encourage the development of trustworthy AI and support international collaboration. The European Artificial Intelligence Board will be composed of one representative per Member State and the European Data Protection Supervisor shall participate as observer.\n\nThe AI Office aims to promote and facilitate the creation, review, and adaptation of codes of good practice, considering international approaches. To ensure these codes reflect the current state of the art and incorporate diverse perspectives, the AI Office will collaborate with relevant national authorities and may consult with civil society organisations, stakeholders, and experts, including scientific experts.\n\n<!-- image -->\n\n## Key concepts\n\nAI systems (see also Chapter 2)\n\nMost of the AI Act applies to 'AI systems' , which the Act defines as ' a machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments '.\n\nIt is worth noting that the AI Act does not define 'artificial intelligence' , but only the term 'artificial intelligence system' . The definition of an AI system is intentionally consistent with the OECD definition of an AI system. The definition does not mention any specific technology or currently known approaches to artificial intelligence systems. With the rapidly evolving nature of AI, this prevents the AI Act from becoming obsolete due to technological developments.\n\nA key element of this definition is the AI system's ability to 'infer' . This should allow for a clear distinction between AI systems and traditional software. If a computer program operates according to rules defined in advance by the programmers, it is not an AI system; if a system is built using techniques that allow the program to create rules of its own based on input data or data sets provided to the program, then it is an AI system. The definition of an AI system is discussed further in guidelines published by the Commission on 6 February 2025.\n\n## Obligations across the supply chain\n\n(see also Chapter 2)\n\nThe AI Act applies to all participants in the supply chain, starting with the 'provider' and also encompassing the 'importer' , 'distributor' and 'deployer' of the system. Most responsibilities lie with the provider, and next with the deployer.\n\nAn importer, distributor or deployer may become a provider of the high-risk AI system if they have put their name or trademark on the system. They may become a provider of a high-risk system (see page 5) if they make substantial modifications to, or modify the intended purpose of the AI system, which renders the system high-risk.\n\n## Risk approach to classification of AI systems\n\nThe AI Act defines risk as ' the combination of the probability of harm occurring and the severity of that harm.'\n\nThe risk-based classification of AI systems is a fundamental aspect of the AI Act, focusing on the potential harm to health, safety, and fundamental human rights that an AI system may cause. This approach categorises AI systems into four distinct risk levels:\n\n1. Unacceptable risk: AI systems that pose such significant risks are unacceptable and therefore prohibited.\n2. High risk: High-risk AI systems are subject to stringent regulatory requirements.\n3. Limited risk: AI systems in this category pose a limited risk, but have specific transparency obligations.\n4. Minimal or no risk: AI systems that pose minimal or no risk have no regulatory restrictions under the AI Act.\n\n## Unacceptable risk: prohibited practices\n\n(see also Chapter 3)\n\nThe AI Act contains a list of prohibited AI practices, which should be understood as a prohibition on placing on the market, putting into service, or using an AI system that employs any of these practices. The list prohibits:\n\n- using subliminal techniques or purposefully manipulative or deceptive techniques to materially distort behaviour, leading to significant harm;\n- exploiting vulnerabilities of an individual or group due to their specific characteristics, leading to significant harm;\n- social scoring systems i.e. evaluating or classifying of an individual or group based on their social behaviour or personal characteristics, leading to detrimental or unfavourable treatment;\n- evaluating a person's likelihood of committing a criminal offence, based solely on profiling or personal characteristics; except when used to support human assessment based\n\n<!-- image -->\n\non objective and verifiable facts linked to a criminal activity;\n\n- facial recognition databases based on untargeted scraping from the internet or CCTV;\n- inferring emotions in workplaces or educational institutions, except for medical or safety reasons;\n- biometric categorisation systems that categorise a person based on their sensitive data, except for labelling or filtering lawfully acquired biometric datasets such as images in the area of law enforcement;\n- real-time remote biometric identification systems in publicly available spaces for law enforcement purposes, except in narrowly defined circumstances.\n\nIn some cases, the AI Act contains exceptions that allow these 'prohibited' practices to be used in certain situations. A good example is real-time biometric identification, where the Regulation allows its use in exceptional circumstances. The application of these exceptions requires notifications or prior authorisations. The Commission published guidelines on prohibited AI practices on 4 February 2025.\n\n## High-risk AI systems (see also Chapter 4)\n\nThe extensive regulation of high-risk AI systems constitutes a major part of the AI Act. AI systems are identified as high-risk AI systems if they have a significant harmful impact on the health, safety and fundamental rights of persons in the Union. There are two categories of high-risk AI systems which are regulated differently:\n\n- AI systems intended to be used as a product or a safety component of a product which is covered by EU harmonisation legislation, such as civil aviation, vehicle security, marine equipment, radio equipment, toys, lifts, pressure equipment, medical devices, personal protective equipment (listed in Annex I to the AI Act).\n- AI systems listed in Annex III, such as AI used in education, employment, credit scoring, law enforcement, migration, remote biometric identification systems, and AI systems used as a safety component in critical infrastructure. This list can be amended by the Commission.\n\nThe first category of high-risk systems is covered by both the harmonisation legislation and the AI Act.\n\nProviders have an option of integrating the requirements of the AI Act into the procedures required under the respective Union harmonisation legislation listed in Section A of Annex I. In addition, only selected provisions of the AI Act apply to highrisk AI systems in relation to products covered by Union harmonisation legislation listed in Section B of Annex I (such as aviation equipment).\n\nPractical assistance in  the classification of highrisk AI systems will be provided no later than 2 February 2026 by the Commission, to include a comprehensive list of practical examples of use cases of high-risk and non-high-risk AI systems.\n\n## Exceptions to the qualification of high-risk AI system\n\nIf a high-risk AI system listed in Annex III does not pose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing the outcome of decision making, it will not be treated as a high-risk AI system.\n\nSuch situations may only arise in four cases where the AI system is intended to:\n\n- perform a narrow procedural task;\n- improve the result of a previously completed human activity;\n- detect decision-making patterns or deviations from prior decision-making patterns, and is not meant to replace or influence the previously completed human assessment without proper human review; or\n- perform a preparatory task to an assessment relevant for the purposes of the use cases listed in Annex III.\n\nIf, however, the AI system performs profiling of natural persons, it is always considered a highrisk AI system and cannot fall into one of the above exceptions.\n\nThis exemption is likely to play an important role in practice, as it allows avoiding the obligations and costs associated with placing a high-risk AI system on the market. One of the options is, for example, to carve out those parts of an AI system that can take advantage of this exemption to limit the scope of the high-risk AI system.\n\nHowever, even if a provider relies on the exemption, its assessment of the system must be documented, and the system must still be\n\n<!-- image -->\n\nregistered in the EU database for high-risk systems before it is placed on the market or put into service.\n\n## Extensive obligations for high-risk AI systems\n\nThe requirements that must be met by providers of high-risk AI systems are strict. These requirements include, in particular, the need to document every stage of the development of the AI system, to meet obligations regarding the use of high-quality data for training, to produce system documentation that provides users with full information about the nature and purpose of the system, or to ensure the accuracy, robustness and cybersecurity of the systems. High-risk AI systems will also have to be registered in an EU database, which will be publicly available.\n\n## Obligations across the supply chain of AI systems\n\nThe AI Act imposes obligations on all participants in the supply chain of a high-risk system throughout its life cycle. The responsibilities are not only those of the 'provider', but also those of the 'importer', 'distributor' and 'deployer' of the system, although most of the responsibilities lie with the provider and the deployer.\n\nThe primary duty of the importer and distributor is to verify that the high-risk AI system being imported or distributed meets the requirements of the AI Act. Moreover, an importer, distributor or deployer may become a provider of the high-risk AI system if they have put their name or trademark on the system, made substantial modifications or they have modified the intended purpose of the AI system, which renders the system high-risk.\n\n## General-purpose AI models (see also Chapter 5)\n\nThe distinction between AI models and AI systems is crucial for the application of the AI Act. AI models are essential components of AI systems, but they do not constitute AI systems on their own. AI models require the addition of other components, such as a user interface, to become AI systems. The AI Act mostly regulates AI systems, not models. However, it does contain rules on general-purpose AI models.\n\nThe AI Act provides rules for all general-purpose AI models and additional rules for generalpurpose AI models that pose systemic risks. They apply in the following situations:\n\n- where the provider of a general-purpose AI model integrates its own model into its own AI system that is made available on the market or put into service;\n- where the provider of a general-purpose AI model only offers its own model to providers of AI systems.\n\nThe distinction may be particularly important in cases where a general-purpose AI model of one provider is used in a general-purpose AI system of a second provider, which in turn is integrated into another AI system with a more specific purpose, built by a third provider.\n\n## Transparency obligations (see also Chapter 6)\n\nThe AI Act includes transparency obligations for four types of AI systems:\n\n- AI systems designed to interact directly with natural persons;\n- AI systems, including general-purpose AI systems, that generate synthetic audio, image, video or text content;\n- emotion recognition or biometric categorisation systems;\n- AI systems that generate or manipulate images, audio or video that are deepfakes.\n\nIn all these cases, the user must be informed about the use of the AI system. There are also more detailed obligations, for example to mark the output in a machine-readable way so that it can be identified as artificially generated or manipulated.\n\n## Complex supervision and enforcement structure (see also Chapter 8)\n\nThe AI Act provides for a complex, multi-level structure for overseeing implementation. It includes both national and EU level entities. At each level there will be several types of bodies, such as notifying authorities and notified bodies, conformity assessment bodies, the European AI Board, the AI Office, national competent authorities and market surveillance authorities.\n\nThese authorities will not only control compliance, but also support the market by,\n\n<!-- image -->\n\namong other things, developing codes of conduct, organising AI regulatory sandboxes and providing support for SMEs and start-ups.\n\n## Role of technical standards, codes of practice and guidelines (see also Chapters 7, 8 and 9)\n\nThe AI Act requires providers of high-risk AI systems to affix a European Conformity (CE) marking. The CE marking will show compliance with the requirements of the AI Act. For the mark to be issued, providers will have to apply harmonised technical standards. In addition, high-risk AI systems or general-purpose AI models which are in conformity with harmonised standards shall be presumed to be in conformity with the requirements of the AI Act to the extent that those standards cover those requirements or obligations. Consequently, the rather general provisions of the AI Act will be complemented by technical standards that will provide the concrete forms of compliance with the AI Act. Thus, we can expect that the CE marking and technical standards will play very important role in practical application of the AI Act.\n\nCodes of practice should also form an important role. If they are not prepared by market participants, the Commission may provide the common rules within implementing acts. The Commission can also, by way of an implementing act, approve a code of practice and give it a general validity within the Union. In addition, the Commission has the obligation to develop several guidelines on the practical implementation of the Regulation.\n\nThe AI Act can therefore be seen as just a framework for more detailed obligation that will result from many further documents and legal acts.\n\n## Enforcement (see also Chapter 8)\n\nThe AI Act stipulates significant penalties for non-compliance, which vary depending on the nature of the violation and the size of the entity involved. Actions that may incur high penalties include:\n\n- non-compliance with the rules on prohibited AI practices outlined in article 5. Offenders in such cases may face administrative fines of up to â‚¬35,000,000 or up to 7% of annual worldwide turnover, whichever is higher, for undertakings.\n\n- violations related to data, data governance, and transparency: AI systems found in breach of these provisions could be fined up to â‚¬20 million or 4% of annual global turnover.\n- failure to comply with any of the provisions set out in article 99 (e.g. relating to high-risk AI systems), will be subject to administrative fines of up to â‚¬15 million or, if the offender is a company, up to 3% of its global turnover in the preceding financial year, whichever is higher.\n\nThese penalties underscore the importance of complying with the AI Act's regulations. It is essential for companies to fully grasp these penalties and ensure that their AI systems meet the Act's requirements.\n\n## Timeline\n\nThe AI Act becomes applicable on a staggered basis. There are also transitional arrangements for AI systems that had been placed on the market or put into service before certain dates. The AI Act applies to all operators of high-risk AI systems that have been placed on the market or put into service before 2 August 2026, unless those systems are subsequently subject to significant change in design (in which case, the provisions would apply in full with respect to the redesigned system). The relevant dates of application are set out below.\n\n| 12 July 2024     | The AI Act was published in the Official Journal of the EU, triggering the dates for specific provisions in the Regulation becoming applicable.                                                                                                      |\n|------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2 February 2025  | Prohibited practices ban applies (Chapter II). AI literacy rules apply (article 4).                                                                                                                                                                  |\n| 2 May 2025       | Codes of practice for general-purpose AI must be ready (article 56 (9)).                                                                                                                                                                             |\n| 2 August 2025    | National authorities designated (Chapter III Section 4). Obligations for General-purpose AI (GPAI) (Chapter V). Governance (at EU and national level) (Chapter VII). Confidentiality and penalties (other than in relation to gen-AI) (Chapter XII). |\n| 2 August 2026    | Start of application of all other provisions of the EU AI Act (unless a later date applies below).                                                                                                                                                   |\n| 2 August 2027    | High-risk categories listed in Annex I. General-purpose AI models placed on the market before 2 August 2025 (article 111).                                                                                                                           |\n| 2 August 2030    | High-risk AI systems (other than those listed below), which have been placed on the market or put into service before 2 August 2026 and which are intended to be used by public authorities (article 111).                                           |\n| 31 December 2030 | Components of large-scale IT systems listed in Annex X, which have been placed on the market or put into service before 2 August 2027 (article 111).                                                                                                 |\n\n<!-- image -->\n\n## Material and territorial scope\n\n<!-- image -->\n\n- The AI Act covers AI systems, general-purpose AI models and prohibited AI practices.\n- Obligations can be imposed on six categories of economic actors: providers, importers, distributors, product manufacturers, authorised representatives and deployers.\n- Economic operators involved with high-risk AI systems have significant obligations. Providers and deployers of certain categories of AI systems are also subject to transparency obligations.\n- Providers of general-purpose AI models are subject to obligations.\n- The AI Act applies when an AI system or general-purpose AI model is placed on the EU market, put into service in the EU, imported into or distributed in the EU. It also applies where an AI system is used by a deployer who has their place of establishment or is in the EU.\n- Providers and deployers of AI systems who fall within scope of the AI Act are subject to AI literacy requirements from 2 February 2025.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nDetermine whether you, your suppliers or your customers will be an operator falling within the material and territorial scope of the AI Act.\n\n<!-- image -->\n\n<!-- image -->\n\n- If you or your supply chain fall within the scope of the AI Act, check whether any AI systems or AI models fall within one or more of the regulated categories.\n- If you are a provider or deployer of AI systems within the scope of the AI Act, ensure you have taken steps to comply with the Act's AI literacy requirements.\n\n## Material scope\n\nThe AI Act primarily provides harmonised rules for the placing on the market, the putting into service, and the use of AI systems. It imposes an extensive set of obligations on 'high-risk' AI systems and transparency obligations on certain AI systems. It also prohibits certain AI practices and regulates the supply of general-purpose AI models in the EU.\n\nThe AI Act also sets out rules for market monitoring, market surveillance, governance and enforcement, which includes administrative fines, as well as measures to support innovation, with a particular focus on small and medium enterprises, such as through the operation of AI sandboxes. It also establishes two new bodies: (i) the European Artificial Intelligence Board which is tasked with advising and assisting the European Commission and EU Member States to facilitate the consistent and effective application of the AI Act; and (ii) the AI Office, which has been established within the European Commission and is tasked with implementing the AI Act, fostering the development and use of trustworthy AI and promoting international cooperation.\n\n## Regulated persons: Operators\n\nThe AI Act imposes obligations on six categories of entities: providers, deployers, importers, distributors, product manufacturers and authorised representatives - the term 'operator' is used to describe all of them. There will always be a provider for an AI system or a generalpurpose AI model. Whether there will also be\n\nThe regulated operators under the AI Act are:\n\n| Operator                                                    |                         | Role                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|-------------------------------------------------------------|-------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Relevant for both AI systems and general- purpose AI models | Provider (article 3(3)) | Develops an AI system or a general-purpose AI model or has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge. Although the definition of 'placing on the market' refers to the EU market, a person can still be deemed a provider regulated by the AI Act even if they do not place an AI system on the EU market, where the output of the AI system is used in the EU. See 'Territorial Scope' further below. A provider can be a natural or legal person, public authority, agency or other body. EU institutions, bodies, offices and agencies may also act as a provider of an AI system. |\n\n<!-- image -->\n\nother operators will depend on the way in which the AI system or general-purpose AI model is being supplied and deployed.  Most operators are defined with reference to three key terms adapted from the EU product legislation referenced in Annex I of the AI Act: 'making available' , 'placing on the market' and 'putting into service' .\n\n'making available' is the supply of an AI system or a general-purpose AI model for distribution or use on the EU market in the course of a commercial activity, whether in return for payment or free of charge;\n\n'placing on the market' is the first making available of an AI system or a generalpurpose AI model on the EU market; and\n\n'putting into service' is the supply of an AI system for first use directly to the deployer or for own use in the EU for its intended purposes.\n\nThe term 'use' is not defined in the AI Act. In essence, 'use' would be perceived by reference to the key characteristic of an AI system which is to infer, from inputs it receives, how to generate outputs. These three terms are discussed in section 2.3 of the Commission's Guidelines on prohibited AI practices, which provides illustrative examples of each activity in the context of the restrictions on prohibited practices.\n\n|                              |                                          | It is also possible to become a provider where an AI system has already been placed on the market or put into service in the EU by another provider, by taking one of the steps set out in article 25(1) (a)-(c). See further below, under 'High-risk AI systems' .                                                                                                                                                                                                                                                                                                                                                                                              |\n|------------------------------|------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                              | Authorised representative (article 3(5)) | An EU-established natural or legal person appointed by a provider established outside the EU to act as their authorised representative. The role includes ensuring that the documentation required by the AI Act is available to the competent authorities and co-operating with those authorities. See article 22 (for high-risk AI systems) and article 54 (for general- purpose AI models).                                                                                                                                                                                                                                                                   |\n| Relevant for AI systems only | Deployer (article 3(4))                  | Uses an AI system under its authority (excluding use in the course of personal, non-professional activity). A deployer can be a natural or legal person, public authority, agency or other body. EU institutions, bodies, offices and agencies may also act as a deployer of an AI system.                                                                                                                                                                                                                                                                                                                                                                       |\n|                              | Importer (article 3(6))                  | Natural or legal person located or established in the EU that places an AI system bearing the name or trademark of a person not established in the EU on the EU market.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n|                              | Distributor (article 3(7))               | Natural or legal person in the supply chain, other than the provider or the importer, that makes an AI system available on the EU market.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|                              | Product manufacturer (article 25(3))     | In certain circumstances, a product manufacturer will be considered the 'provider' of a high-risk AI system where: this is a safety component of a product covered by the AI Act (by virtue of being subject to the EU product safety legislation referenced in Section A of Annex I), and the manufacturer places the AI system on the EU market or puts it into service in the EU together with that product and under its own name or trademark. The term 'product manufacturer' is not defined in the AI Act - but Recital 87 clarifies that this is the 'manufacturer' defined under the EU product safety legislation referenced in Annex I to the AI Act. |\n\n## Indirect obligations under the AI Act\n\nThe AI Act imposes indirect obligations on component suppliers to providers of high-risk AI systems. Those supplying AI systems, tools, services, components, or processes that are used or integrated in a high-risk AI system are required to enter into a written agreement with the provider of the high-risk AI system and to enable the latter to comply with its obligations under the AI Act (article 25(4)). This obligation does not apply to third parties who make such tools, services, processes or components (other than general-purpose AI models) accessible to the public under a free and open-source licence.\n\n<!-- image -->\n\n## Rights granted by the AI Act\n\nUnlike the GDPR, which provides a comprehensive set of rights to individuals, the rights under the AI Act are limited. The AI Act only confers a right to explanation of individual decision-making on affected persons located in the EU (article 86). Affected persons are those who are subject to a decision which has a legal or similarly significant effect on them and which is based on the output of one of the high-risk AI systems identified in Annex III. The wording used here is similar to that used under the automated decision-making provisions of the GDPR (article 22 GDPR); the scope of the two provisions however is not identical.\n\n## Regulated subject matter: AI systems\n\nAn AI system is defined broadly in article 3(1) as: ' a machine-based system that is designed to operate with varying levels of autonomy, and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments' .\n\nThis definition is intended to align with the definition used by the OECD AI Principles. A key characteristic of AI systems is their capability to infer, i.e. to obtain outputs and to derive models or algorithms, or both, from inputs or data. Instead, traditional software, which executes operations based solely on rules defined by natural persons is not, on its own, considered an AI system.\n\nIn February 2025, the Commission published guidelines for this definition. These guidelines provide further explanations for each aspect of the definition, with a clear emphasis on the 'ability to infer.' In a positive sense, the guidelines outline various machine learning approaches that enable this ability. At the same time, they list systems - particularly those primarily based on mathematical or statistical methods - that do not possess this ability and should therefore not fall within the scope of the AI Act. A noteworthy negative example is 'logistic regression,' which is widely used in the financial sector.\n\nAn AI system can be used on a standalone basis or as a component of a product, irrespective of whether the AI system is physically integrated into the product or serves the product's functionality without being integrated into it.\n\nUnder the AI Act, AI systems fall into the following categories:\n\n- high-risk AI systems;\n- AI systems with transparency risks; and\n- all other AI systems.\n\nAn AI system can also form part of a prohibited AI practice. This can be because of certain features of that AI system or because of the way the AI system would be used.\n\n<!-- image -->\n\n## High-risk AI systems\n\nSection III of the AI Act regulates high-risk AI systems. These are AI systems that pose a significant risk of harm to the health, safety and fundamental rights of persons in the EU. An AI system may be classified as high-risk in two ways:\n\n- Article 6(1): The AI system is used as a safety component in a product that is regulated by certain EU product safety legislation (the Union harmonisation legislation listed in Annex I of the AI Act) and is subject to the conformity assessment procedure with a third-party conformity assessment body under such legislation, or constitutes on its own such a product (e.g. an AI system which is used for medical diagnostic purposes will itself be a regulated medical device); or\n- Article 6(2): The AI system falls within one of the eight categories set out in Annex III of the AI Act - unless the provider can demonstrate and document that such AI system does not pose a significant risk of harm.\n\nMost of the obligations regarding high-risk AI systems fall on providers (which includes product manufacturers as we describe further above), whilst a more limited set of obligations is imposed on deployers, on importers and distributors, and where relevant, authorised representatives.\n\nSee Chapter 4 of this guide for more details.\n\n## AI systems with transparency risks\n\nThe AI Act imposes certain transparency obligations on:\n\n- providers of AI systems intended to interact directly with natural persons (article 50(1));\n- providers of AI systems generating synthetic audio, image, video or text content (article 50(2));\n- deployers of an emotion recognition system or a biometric categorisation system (article 50(3)); and\n- deployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake (article 50(4)).\n\nSee Chapter 6 of this guide for more details.\n\n## All other AI systems\n\nAll other types of AI systems, which do not fall under the above categories and are not used for prohibited AI practices are not subject to direct legal obligations under the AI Act. Voluntary codes of conduct may be drawn up in future covering this broader category of AI systems and those deploying them (article 95). Providers and deployers may choose to adhere to these codes of conduct.\n\nAside from rules relating to specific categories of AI systems, those qualifying as the provider or deployer of any AI system under the AI Act are required to take AI literacy measures to ensure that their staff and other persons dealing with the operation and use of AI systems on their behalf, have a sufficient level of knowledge, skills and understanding regarding the deployment of AI systems, their opportunities and risks (article 4). This obligation aims to foster the development, operation and use of AI in a trustworthy manner in the EU - however, it is worth noting that this provision refers to voluntary codes of conduct and that administrative fines are not foreseen for failure to comply with the AI literacy obligation.\n\n## Regulated subject matter: Prohibited AI practices\n\nThe AI Act prohibits the placing on the market, putting into service and use of AI systems that have certain prohibited features and/or are intended to be used for certain prohibited purposes, e.g. AI systems that create or expand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage. These practices are deemed to be particularly harmful and abusive and contradict EU values and fundamental rights. The prohibited AI practices are listed in article 5 of the AI Act. This list does not affect the prohibitions of AI practices that infringe other EU law (such as data protection, non-discrimination, consumer protection and competition law).\n\nSee Chapter 3 of this guide for more detail.\n\n<!-- image -->\n\n## Regulated subject matter: general-purpose AI models\n\nA general-purpose AI model is defined in article 3(63) as: ' an AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market '\n\nThe AI Act does not provide a definition of an ' AI model '; recital 97 notes that although AI models are essential components of AI systems, they do not constitute AI systems on their own and require further components, such as a user interface, to become AI systems. The characteristics of general-purpose AI models are discussed further in recitals 98 and 99.\n\nThe AI Act regulates general-purpose AI models and imposes additional obligations for generalpurpose AI models with systemic risks. The rules apply to providers of general-purpose AI models, once these models are placed on the market: this can be done in various ways, such as through libraries, APIs, as a direct download or as a physical copy.\n\nRecital 97 suggests that the rules on generalpurpose AI models can also apply when these models are integrated into or form part of an AI system. When the provider of a generalpurpose AI model integrates its own model into its own AI system that is made available in the market or put into service, then recital 97 suggests that model will be viewed as being placed on the market and the general-purpose AI model provisions will apply, in addition to those regarding AI systems.\n\n<!-- image -->\n\nMaterial Scope article 1 recitals 1-3, 6-8\n\nThose who integrate third party generalpurpose AI models into their own AI systems are considered 'downstream providers' and are granted certain rights under the AI Act. However, the AI Act appears to envisage that a provider who fine-tunes a third party general-purpose AI model and integrates that fine-tuned model into their own AI system (or otherwise places a finetuned general-purpose AI model on the market or puts it into service) will be considered the provider of this with respect to that fine-tuning only (see recital109).\n\nSee Chapter 5 of this guide for more detail.\n\n## Territorial scope\n\n## AI System provisions\n\nThe AI Act is intended to have a broad jurisdictional scope for its AI system provisions: these are engaged when an AI system, either on its own or as part of a product covered by the EU product safety legislation in Annex I, is:\n\n- placed on the EU market, put into service in the EU, imported into or distributed in the EU; or\n- used by a deployer who has their place of establishment or is located in the EU.\n\nThe first point applies applies irrespective of where the provider of the AI system is established. The concept of ' establishment ' is not defined in the AI Act. It is expected that this would be interpretated broadly, similar to the use of this term under other EU legislation, such as the GDPR.\n\nIn addition to those cases, the AI system\n\n<!-- image -->\n\nprovisions also apply if the output produced by an AI system outside the EU is used in the EU. In that case, the non-EU established/ located providers and deployers will also be caught by the scope of the AI Act. Recital 22 clarifies that in those instances the AI Act will apply even though the relevant AI systems are not placed on the market, put into service or used in the EU.\n\n## Prohibited AI Practices\n\nThe AI Act's provisions relating to prohibited AI practices apply to the placing on the EU market, putting into service in the EU and use of the relevant AI practices set out in Article 5. As we saw above, the definitions of 'placing on the market' and 'putting into service' refer to the EU market. The AI Act itself does not specify what a prohibited 'use' would entail. The Commission's Guidelines on prohibited AI practices suggest that use 'should be understood in a broad manner to cover the use or deployment of the system at any moment of its lifecycle after having been placed on the market or put into service' and further that use 'may also cover the integration of the AI system in the services and processes of the person(s) making use of the AI system, including as part of more complex systems, processes or infrastructure.'\n\n## General-purpose AI Models\n\nThe AI Act's general-purpose AI model provisions will be engaged where a provider of a generalpurpose AI model places it on the market in the EU or puts it into service in the EU - irrespective of where the provider is located or established.\n\n<!-- image -->\n\narticle 2\n\n## Exclusions\n\nCertain activities are entirely outside the AI Act's scope. The AI Act does not apply to:\n\n- areas outside the scope of EU law (e.g. activities concerning national security). This is the case irrespective of the type of entity entrusted under national legislation with carrying out the exempted activities. Given the very broad competences of the EU, as set out in the TFEU, this provision will have very limited scope of application in practice;\n- AI systems placed on the market, put into service, or used with or without modification - or where their output is used in the EU, exclusively for military, defence or national security purposes, regardless of the type of entity carrying out those activities. An AI system placed on the market or put into service for an excluded purpose (military, defence or national security) and one or more non-excluded purposes (e.g. civilian purposes or law enforcement) is subject to the AI Act and providers of those systems should ensure compliance with the AI Act;\n- public authorities in a third country or international organisations that use AI systems in the framework of international cooperation or agreements for law enforcement and judicial cooperation with the EU or EU member states, provided that such a third country or international organisation provides adequate safeguards for the protection of fundamental rights and freedoms of individuals. The national authorities and EU institutions, bodies, offices and agencies making use of those outputs remain subject to EU law;\n- AI systems and models, including their output, specifically developed and put into service for the sole purpose of scientific research and development;\n- research, testing or development of AI systems or models prior to their being placed on the market or put into service, excluding though testing in real world conditions;\n- deployers who are individuals and use the AI system in the course of a purely personal, non-professional activity. This is similar to the GDPR's 'household exemption' - whilst providers of those AI systems continue to be subject to the AI Act; and\n- AI systems released under free and opensource licences, unless they are placed on the market or put into service as high-risk AI systems, as a prohibited AI system or as a system that is covered by the Act's transparency obligations.\n\n<!-- image -->\n\n## Relationship with other regulatory frameworks\n\n- As a Regulation, the AI Act is directly applicable in EU Member States without the need for implementing legislation. EU Member States are prevented from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by the AI Act. This is only provided for in limited circumstances: for example, EU member states may introduce more restrictive laws on the use of remote biometric identification systems - some of which constitute prohibited AI practices (article 5(5)) and the use of post-remote biometric identification systems, which constitute high-risk AI systems (article 26(10)).\n- The AI Act's provisions on high-risk AI systems are built around the New Legislative Framework for EU products. This is a legislative package that sets out rules for the placing of products on the EU market, enhances market surveillance rules and rules for conformity assessments and CE marking. It also establishes a common legal framework for industrial products in the form of a toolbox of measures for use in future legislation. The AI Act specifies how these tools set out in the New Legislative Framework should apply in the context of AI systems.\n- In parallel, the AI Act complements Union harmonisation legislation - this is the set of EU product safety legislation on the basis of which certain AI systems are to be classified as high-risk.\n- The obligations of the AI Act apply in addition to and without prejudice to the obligations under GDPR, the e-Privacy Directive and the Law Enforcement Directive.\n\n## Prohibited AI Practices\n\n<!-- image -->\n\n- Article 5 lists eight prohibited practices which are deemed to pose an unacceptable level of risk.\n- Prohibitions come into effect on 2 February 2025.\n- The prohibited practices are:\n- -Subliminal, manipulative, or deceptive techniques\n- -Techniques exploiting vulnerable groups in each case which materially distorts behaviour and risks significant harm\n- -Social scoring in certain use cases\n- -Predicting criminality based on profiling\n- -Scraping the web or CCTV for facial recognition databases\n- -Inferences of emotions at workplaces or schools\n- -Biometric categorisation to infer race, political opinion, trade union membership, religious or political beliefs, sex life or sexual orientation\n- -Real-time remote biometric identification in public spaces for law enforcement purposes.\n- Many of the prohibitions have exceptions case by case analysis is needed.\n- The list is not final: it will be re-assessed annually.\n- Non-compliance sanctioned by fines up to â‚¬35 million or 7% of total worldwide annual turnover for the proceeding financial year (whichever is higher).\n- The prohibitions are operator-agnostic and apply irrespective of the role of the actor (i.e. whether provider, deployer, distributor or importer).\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n4\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nCheck the AI systems you use to see if they fall under the prohibited category.\n\n<!-- image -->\n\n<!-- image -->\n\n- Check for updates to this list annually as the list of prohibited practices may change over time.\n\nConsider whether any exceptions apply. The prohibited practices are not absolute; many have exceptions.\n\n## Prohibited AI practices\n\nThe AI Act relies on a risk-based approach, so different requirements apply in accordance with the level of risk. This chapter concentrates on prohibited practices i.e. those which conflict with the values of the European Union and are a clear threat to fundamental rights such as freedom, equality and privacy. The prohibitions are an attempt by law makers to respond to transparency and ethics concerns and to guarantee the protection of human rights.\n\nThe prohibited practices are listed exhaustively in article 5 (and are further explained in recitals 28 - 45 of the Act and by guidelines issued by the Commission on 4 February 2025) and provide a clear framework for what AI can and cannot do within the EU. The prohibitions in Article 5 apply from 2 February 2025 and are therefore the first provisions to come into force, highlighting their importance.The list of prohibited practices in article 5 is exhaustive, but not final. The Commission will assess the need for amendment of the list of prohibited practices annually (article 112) and can submit findings to the European Parliament and Council. So, there may be variations to the list of prohibited practices in due course.\n\nThere are currently eight prohibited practices, which focus on practices that materially distort peoples' behaviour, or raise concerns in democratic societies. Special attention has been given to biometric identification systems. However, there are detailed exceptions to many of the prohibitions and each practice should be considered on a case-by-case basis.\n\n## Article 5(1)(a) Subliminal, manipulative or deceptive techniques\n\nThe first prohibition concerns AI systems deploying subliminal, manipulative or deceptive techniques in cases where:\n\n- the techniques either aim to, or actually have, the effect of materially distorting the behaviour of an individual or a group;\n- by appreciably impairing the ability of individuals to make informed decisions; and\n- causing them to take decisions they would not otherwise have taken, and that either cause or are reasonably likely to cause them significant harm.\n\n<!-- image -->\n\nThe techniques expressly mentioned in recital 29 involve: deployment of subliminal components such as audio, image, video stimuli that persons cannot perceive, or other manipulative or deceptive techniques that subvert or impair a person's autonomy, decision-making, or free choice, in ways so that people are not consciously aware of those techniques or, where they are aware of them, can still be deceived or are not able to control or resist them. The reference in recital 29 to machine-brain interfaces having the capability to materially distort human behaviour in a significantly harmful manner may also be the Act's attempt to regulate tools that employ neural data which is currently under discussion in other jurisdictions such as Colorado, California, and Chile.\n\nFor an AI system to be prohibited, there needs to be a causal link between the deceptive techniques and the significant harm caused. The threshold of 'significant' harm was added in the legislative process and makes clear that not all dark patterns would fall under this provision.\n\nThe provision is open for interpretation and, in particular, the word 'deceptive' will lead to further discussions. According to the Commission's guidelines, deceptive techniques could cover presenting false or misleading information with the objective or effect of misleading individuals, if the other requirements of the first prohibition are met.\n\n## Article 5(1)(b) Exploitation of vulnerabilities\n\nThe second category of prohibited AI practices aims to protect vulnerable people. There are three groups: vulnerability due to age, disability, or due to specific social or economic situations.\n\nAn AI system is only prohibited if it has the objective or the effect of materially distorting the behaviour of an individual and does so in a manner that causes or is likely to cause someone significant harm.\n\nAn exploitation from a socio-economic perspective does not exist, according to the Commission guidelines on prohibited practices, if the situation may be experienced by any person irrespective of their socio-economic situation (e.g. grievances or loneliness). In such case, however, an exploitation may be covered under Article 5(1)(a) AI Act.\n\nAI systems that inadvertently impact sociodisadvantaged groups due to biased training data do not automatically exploit vulnerabilities, as there is no intentional targeting. However, under the Commission guidelines on prohibited practices, if AI providers or deployers are aware that their systems unlawfully discriminate against socio-economically disadvantaged persons and foresee significant harm without taking corrective action, they may still be considered to exploit these vulnerabilities.\n\nAn exploitation of a person's economic situation could exist in cases where an AI system is used to find persons in poverty to exploit their weaknesses economically. Organisations using AI systems for marketing and sales should make sure they test their systems against this requirement.\n\nThe concept of significant harm is common to both subliminal techniques and exploitation of vulnerable groups. In the legislative process, requirements that the harm needed to be physical or psychological were dropped. It seems that a broad approach is intended to be taken to the concept of harm, although recital 29 still gives the examples of important adverse impacts on physical and psychological health, alongside financial interests. The recital also notes that harms can be accumulated over time.\n\nThis prohibition is not intended to affect lawful medical treatment (e.g. psychological treatment of a mental disease carried out with consent). Recital 29 also implicitly recognises that advertising and some other commercial practices inherently depend on nudging - and states that the intent is not to prohibit common, legitimate and lawful commercial practices, particularly in the field of advertising. Consent can play a crucial role in these scenarios. In persuasive interactions, individuals are aware of the influence attempt and can make choices freely and autonomously.\n\n## Article 5(1)(c) Social scoring\n\nThe third prohibition concerns so-called social scoring, i.e. classifying individuals or groups over a period based on their social behaviour, or known, inferred, or predicted personal characteristics. Social scoring is prohibited in two cases:\n\n- if it leads to unfavourable treatment in social contexts that are unrelated to the context in which the data was originally generated; and\n- if this leads to unfavourable treatment of individuals or groups that is unjustified or disproportionate to their social behaviour or its gravity.\n\n<!-- image -->\n\nSocial scoring is used by several governments around the world. The government in the Netherlands stepped down in 2021 due to a flawed risk-scoring algorithm, which lead to unjustified accusation of fraud for welfare benefits based on personal characteristics and behaviour. The algorithm in that case targeted minorities and people based on their economic situation. Whilst governments might be the first example that comes to mind when thinking about social scoring, the provision is wider and encompasses all social scoring systems in public or private contexts. Many algorithms inherently depend on behavioural scores. However, the AI Act only prohibits those scoring systems resulting in unfavourable treatment in unrelated social contexts. This key restriction targets the consequences of social scoring, preventing unjust outcomes, or discrimination of individuals or groups.\n\nThe social scoring prohibition under the AI Act therefore depends on the context the data has been obtained from and the context the data is being used. As the Commission guidelines on prohibited practices illustrate, lawful activities, like credit and risk scoring in financial services, are permitted if they improve service quality or prevent fraud. Conversely, an insurance company using spending and other financial data from a bank to set life insurance premiums is provided as an example of unlawful social scoring.\n\n## Article 5(1)(d) Profiling for criminal risk assessment\n\nThe fourth prohibition is placing on the market, putting into service, or using AI systems that assess or predict the likelihood of a person committing criminal offences based solely on profiling or on assessing the personality traits and characteristics of a person. There is an exception for AI systems used to support human assessment of involvement of a person in a criminal activity, which is based on objective and verifiable facts directly linked to a criminal activity - i.e. detection tools which are factual and supplement, but do not supplant, human decision making. This prohibition aims to avoid the scenario whereby people are treated as guilty for crimes they have not (yet) committed - as illustrated in the film Minority Report . It is tied to human dignity as laid down in article 1 of the Charter of Fundamental Rights.\n\nThe Commission guidelines on prohibited practices emphasise that the prohibition can extend to private entities if they act with public authority or assist law enforcement. For instance, a private company analysing data for law enforcement might face prohibition if specific criteria are met.\n\nThe Commission guidelines also suggest that retrospective human assessments of AI system evaluations can fall outside the scope under certain conditions. This is informed by CJEU case law, which underscores the importance of human review to ensure that AI-driven decisions are based on objective criteria and are nondiscriminatory, thus extending beyond the initial exemption in the AI Act.\n\n## Article 5(1)(e) Facial recognition databases\n\nThe fifth prohibited practice is the placing on the market, putting into service for the specific purpose, or use of AI systems to create or expand facial recognition databases through untargeted scraping of facial images from the internet or CCTV footage. Recital 43 considers this practice to add to the feeling of mass surveillance and that it can lead to gross violations of fundamental rights, including the right to privacy. This may be a response to the investigations by supervisory authorities into Clearview AI.\n\nThe Commission guidelines on prohibited practices regarding facial recognition databases clarify several key points. Such databases can be temporary, centralised, or decentralised, and they fall under Article 5(1)(e), if they can be used for facial recognition, regardless of their primary purpose. Targeted scraping, such as collecting images of specific individuals or using reverse image searches, is allowed, but combining it with untargeted scraping is prohibited. The prohibition does not cover untargeted scraping of other biometric data, like voice samples, or databases not used for recognition, such as those for AI model training without identifying individuals.\n\n## Article 5(1)(f) Inference of emotions in working life and education\n\nThe sixth prohibited practice is the placing on the market, putting into service for this specific purpose, or use of AI systems to infer emotions in workplace or schools, except for safety or medical reasons such as systems intended for therapeutical use. The guidelines clarify that the definition of both the school and workplace\n\n<!-- image -->\n\nshould be interpreted widely and in the case of workplace use they should also cover the selection and hiring phases of recruitment. The exception for the safety or medical reasons on the other hand is to be interpreted narrowly. For example, systems measuring burnout or depression in the workplace would not be exempted.\n\nRecital 18 distinguishes between emotions or intentions such as happiness, sadness, anger etc. It explains that the notion does not include physical states, such as pain or fatigue (so, systems used in detecting the state of fatigue of professional pilots or drivers for the purpose of preventing accidents would not be affected). It also does not include detection of readily apparent expressions such as a frown or a smile, or gestures such as the movement of hands, arms or head, or characteristics of a person's voice, such as a raised voice or whispering. However, the guidelines still do not clarify the meaning of 'intention' which are also covered by the definition of emotion recognition systems.\n\nThe AI Act has a defined term of 'emotion recognition system' , which means an ' AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of biometric data '.\n\nCuriously, article 5(1)(f) does not use this term, and refers to any use of AI systems to infer emotions (i.e. without the requirement that this should be derived from biometric data). However, the Commission's guidelines clarified that Article 5(1)(f) should be read as referring to the emotion recognition systems as the defined term under the Act. They further clarified that nonbiometric emotion recognition systems (e.g. text-based) are not prohibited provided they are not used in conjunction with biometric data such as keystroke analysis. The Act references the inaccuracy of biometric emotion recognition systems and their intrusive nature in settings where there is an imbalance of power (such as workplace and schools) as the reason for the prohibition in such settings. However, the AI Act does not explain why it considers non-biometric emotion recognition systems as less intrusive or more accurate than biometric systems.\n\n## Article 5(1)(g) Biometric categorisation\n\nThe seventh prohibition is on the use of biometric categorisation systems that categorise individuals based on their biometric data to deduce or infer certain (not all) special category data under the GDPR, namely: race, political opinions, trade union membership, religious or political beliefs, sex life or sexual orientation.\n\nSpecial category data under the GDPR that are not covered in the prohibition are inferences of ethnic origin, health, and genetic data. However, inferring such types of data would likely fall under the high-risk category under Annex III. Additionally, the prohibition does not cover labelling or filtering of lawfully acquired biometric datasets or categorising of biometric data by law enforcement (e.g. sorting of images according to hair colour or eye colour by law enforcement to search for suspects). However as recital 54 suggests that AI systems intended to be used for biometric categorisation according to sensitive attributes or special category data under the GDPR, in so far as they are not prohibited the AI Act, should be classified as high-risk and the guidelines also state that most AI systems that fall under an exception from a prohibition listed in Article 5 AI Act will qualify as high-risk this would suggest that the exempted labelling and filtering systems would fall under the high-risk category.\n\nRecital 16 clarifies that biometric categorisation systems do not include purely ancillary features which are linked to another commercial service, where the feature cannot, for objective technical reasons, be used without the main service, and where this is not a circumvention mechanism to evade AI Act rules (e.g. retail try before you buy filters, or social media filters).\n\nThe guidelines also clarify that the scope of biometric categorisation excludes categorisation according to clothes or accessories, such as scarfs or crosses, or social media activity.\n\n## Article 5(1)(h) Real-time remote biometric identification in public spaces\n\nThe eighth and last prohibition is the use of real-time remote biometric identification systems ('RBI') in publicly accessible spaces for law enforcement purposes. RBI systems are AI systems for the purpose of identifying natural persons, without their involvement, typically at a\n\n<!-- image -->\n\ndistance, by comparing biometric data with that contained in a reference database. Real-time systems include those where there is a short delay in the comparison. The AI Act does not define how much time amounts to 'significant delay' . However, the guidelines suggest that this would likely be the case for when the person is likely to have left the place where the biometric data was taken and not allow for a quick reaction from the law enforcement.\n\nBiometric systems used for verification (i.e. confirming that someone is who they claim to be, to access a service, a device, or to have security access to premises) are distinguished from RBI and so not covered by this prohibition (recital 15). The guidelines clarify that the distinction between the identification and verification comes from the active involvement of the individual in the process which may have minor impact on fundamental rights of natural persons. For active involvement, however, it is not sufficient that persons are informed about the presence of cameras, but they need to step actively and consciously in front of a camera that is installed in a way fostering active participation.\n\nThe AI Act allows (but does not require) member states to permit use of RBI for law enforcement purposes in limited situations where the use of RBI is strictly necessary for:\n\n- targeted searches for specific victims of abduction, human trafficking, or sexual exploitation as well as searching for missing persons;\n- the prevention of a specific, substantial, and imminent threat to the life or physical safety, or a genuine and present or foreseeable threat of terrorist attack; or\n- the localisation or identification of a person suspected of having committed a criminal offence, conducting a criminal investigation, prosecution or executing a criminal penalty for serious offences - being those referred to in Annex II and punishable in the Member State concerned by a prison sentence for a maximum period of at least four years.\n\nThe exemptions only permit RBI used to confirm the identity of the specifically targeted individual. In addition, use of RBI should consider the nature of the situation, in particular the seriousness, probability, and scale of the harm that would be caused if the system were not used, against the consequences of use on the rights and freedoms of the persons concerned.\n\nFurther, protections include the need to complete a fundamental rights assessment, registration of the system in an EU database in line with article 49, and prior authorisation of each use case by judicial or administrative authority (subject to urgency measures). In addition, each use of RBI in publicly accessible spaces must be notified to the relevant market surveillance authority and the national data protection authority. The national authorities must then report to the European Commission which, in turn, prepares an annual state of the nation report on usage of RBI in accordance with these provisions.\n\n## To whom do the prohibitions apply?\n\nAs set out in Chapter 2, the AI Act distinguishes between different actors involved in AI systems, attributing specific responsibilities based on their role in relation to the AI model or system. This method ensures that those who have the most influence over the development and implementation of AI technologies adhere to the highest standards.\n\nHowever, the rules on prohibited practices are operator-agnostic. In other words, they apply universally, independent of the specific role of the actor (i.e. whether they are involved in the provision, development, deployment, distribution, or use of AI systems engaging in prohibited practices).\n\nThis wide-ranging application highlights the Act's dedication to stopping practices that could infringe on fundamental rights or present intolerable risks, emphasising a comprehensive approach to regulation that covers all types of interaction with harmful AI technologies.\n\n<!-- image -->\n\n## Enforcement and fines\n\nWhen a practice is prohibited, the AI system in question may not be used in the EU. In the case of an infringement, competent authorities may issue a fine of up to 7% of the total worldwide annual turnover of the offender for the preceding financial year or 35 million EUR, whichever is higher.\n\nNational market surveillance authorities will be responsible for ensuring compliance with the AI Act's provisions regarding prohibited AI systems. They will report to the European Commission annually about use of prohibited practices that occurred during the year and about the measures they have taken.\n\n<!-- image -->\n\n| Subliminal, manipulative or deceptive techniques           | article 5(1)(a)   | recitals 28 & 29   |\n|------------------------------------------------------------|-------------------|--------------------|\n| Exploitation of vulnerabilities                            | article 5(1)(b)   | recitals 28 & 29   |\n| Social scoring                                             | article 5(1)(c)   | recital 31         |\n| Profiling for criminal risk assessment                     | article 5(1)(d)   | recital 42         |\n| Facial recognition database                                | article 5(1)(e)   | recital 43         |\n| Inference of emotions in working life and education        | article 5(1)(f)   | recitals 44 - 45   |\n| Biometric categorisation                                   | article 5(1)(g)   | recital 30         |\n| Real-time remote biometric identification in public spaces | article 5(1)(h)   | recitals 32 - 41   |\n\n## Other useful resources\n\n- Commission guidelines on prohibited artificial intelligence practices established by Regulation (EU 2024/1689 (AI Act)\n- ETHICS GUIDELINES FOR TRUSTWORTHY AI: High-Level Expert Group on Artificial Intelligence (2019)\n- EDPB Guidelines on Processing Personal Data Through Video Devices\n- EDPB Guidelines on Use of Facial Recognition Technology In The Area of Law Enforcement\n- EDPB Guidelines on Automated Decision Making and Profiling\n- EDPB-EDPS Joint Opinion On The Proposal For The Artificial Intelligence Act\n- EDPB guidelines on Deceptive Design Patterns in Social Media\n- Guidelines on dark patterns from the Finnish Market Authority\n\n<!-- image -->\n\n## High-risk AI systems\n\n<!-- image -->\n\n- AI systems fall within the scope of 'high-risk' if they are intended to be used as:\n- -products, or safety components of products, which must undergo third-party conformity assessment pursuant to the legislation covered by Annex I; or\n- -for one of the purposes described in Annex III.\n- Providers, deployers, importers, distributors and suppliers to providers of high-risk AI systems have obligations under the AI Act. Market parties can have multiple roles in parallel and need to comply with multiple sets of obligations simultaneously.\n- Providers of high-risk AI systems have the heaviest compliance burden and need to carry out a conformity assessment before the system can be placed on the market or put into service.\n- It's possible to become the provider of a high-risk AI system (e.g. by placing your own name/trademark on the system, making a substantial modification, or using the system for different purposes than intended by the original provider).\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nDetermine whether the AI system falls within the scope of high-risk as meant in article 6, in conjunction with Annexes I and III.\n\n<!-- image -->\n\nDetermine your role in the value chain (provider, deployer, importer, distributor, or third-party supplier) and review the corresponding obligations.\n\n## Classification of an AI system as a high-risk AI system\n\nThe main part of the AI Act regulates high-risk AI systems. These are AI systems that can have a significant harmful impact on the health, safety and fundamental rights of persons in the EU. There are two main categories of highrisk AI systems:\n\n- a. systems which are intended to be used as safety components of products or systems, or which are themselves products or systems, falling within the scope of Union harmonisation legislation listed in Annex I, if required to undergo a third-party conformity assessment pursuant to this legislation; and\n- b. systems whose intended purpose falls within the scope of the use cases set out in Annex III of the AI Act.\n\n## Category A: Annex I systems\n\nRegarding the first category (a), the product safety legislation listed in Annex I covers the following categories:\n\n- machinery\n- toys\n- recreational craft and personal watercraft\n- lifts/elevators\n- equipment and protective systems for potentially explosive atmospheres\n- radio equipment\n- pressure equipment\n- cableway installations\n- personal protective equipment\n- appliances burning gaseous fuels, medical devices\n- in vitro diagnostic medical devices\n- civil aviation\n- 2/3-wheel vehicles\n- agricultural and forestry vehicles\n- marine equipment\n- rail systems\n- motor vehicles and their trailers\n- unmanned aircraft\n\n<!-- image -->\n\nNote that the legislation in Annex I covers the categories above, but can also cover related products. For example, the Machinery Regulation covers lifting accessories and removable mechanical transmission devices as well as machinery. It's also the core regulation for robotics, another steadily growing area of AI adoption for which the AI Act and its high-risk requirements will become highly relevant.\n\nSafety components fulfil a safety function for a product, where their failure or malfunction would endanger the health and safety of persons or property. You should make an assessment pursuant to the applicable product safety regulation in Annex I to see whether the AI system would have to undergo thirdparty conformity assessment pursuant to that legislation. For example, in the Medical Device Regulation, medical devices in class IIa and higher are subject to the third-party conformity procedure. If an AI-system qualifies as a safety component of such a medical device, or if it constitutes such a medical device itself, it is a high-risk AI system pursuant to the AI Act.\n\nSome of the legislation covered in Annex I also uses terms such as 'high-risk' and 'medium-risk' . However, these categories are independent from the classification as high risk under the AI Act. For example, under applicable product safety legislation a product can be classed as 'mediumrisk' , but if the product has to  to undergo thirdparty conformity assessment, then an AI system that is a safety component of that product, or that itself constitutes such a product, will be high-risk under the AI Act.\n\n## Category B: Annex III systems\n\nThe stand-alone list of high-risk systems currently contains:\n\n- Biometrics: remote biometric identification of individuals, biometric categorisation of individuals and/or emotion recognition of individuals.\n\n- Management and operation of critical infrastructure: to directly protect physical integrity or health and safety of individuals and property in relation to the management and operation of critical digital infrastructure (e.g., internet exchange points, DNS services, TLD registries, cloud computing services, data centres, content delivery networks, trust service providers, electronic communication networks or services), or in the supply of water, gas, heating, or electricity.\n- Education and vocational training : decisionmaking in education and vocational training (e.g. selection, evaluation, assessment and monitoring of students or individuals applying to be students).\n- Recruitment and HR : decision-making in recruitment and HR (e.g. selection, evaluation, assessment, promotion, termination, task allocation and monitoring of employees and/ or other workers and/or applicants).\n- Essential services: evaluating the (continued) eligibility of individuals for public assistance benefits (e.g. healthcare services, social security allowances, disability benefits); evaluating creditworthiness of individuals or establishing their credit score (with the exception of the detection of financial fraud); risk assessment and pricing in relation to individuals in the case of life and health insurance; and evaluating and classifying emergency calls or making decisions in relation to dispatching or prioritisation of the dispatching of emergency first response services (e.g. police, firefighters, medical aid); and emergency healthcare patient triage.\n- Crime analytics: assessment by/on behalf of/ in support of law enforcement authorities: (i) of the risk of individuals of becoming a victim or (re-)offender; (ii) of personality traits and characteristics; (iii) of past criminal behaviour of individuals or groups; or (iv) consisting of profiling of persons, in the course of the detection, investigation or prosecution of criminal offences.\n- Evidence gathering and evaluation: evaluation of reliability of evidence during the investigation or prosecution of criminal offences, or in the course of applications for asylum, visa or residence permits, or with regard to associated complaints; use of polygraphs or similar tools by/on behalf of/ in support of law enforcement authorities or authorities conducting migration, asylum and/ or border control.\n- Immigrant identification, migration risk and migration application assessment: detecting, recognising or identifying individuals (with the exception of verification of travel documents) in the context of migration, asylum or border control management; assessment of risk (e.g. security risk, risk of irregular migration or health risk) posed by individuals who intend to enter or have entered the territory of an EU country and examination of applications for asylum, visa or residence permits and for associated complaints.\n- Administration of justice: assisting judicial authorities or alternative dispute resolution institutions in researching and interpreting facts and the law and in applying the law to facts.\n- Democratic processes: influencing the outcome of an election or referendum or voting behaviour of individuals.\n\n<!-- image -->\n\nNote that Annex III may be amended by the Commission (article 7).\n\nThe intended purpose is defined in article 3(12) as: ' the use for which an AI system is intended by the provider, including the specific context and conditions of use, as specified in the information supplied by the provider in the instructions for use, promotional or sales materials and statements, as well as in the technical documentation.'\n\n## Exceptions: not sufficiently high-risk\n\nArticle 6(3) provides that AI systems whose intended purpose falls within the scope of Annex III, so that they would (absent this provision be high-risk) shall nonetheless not be considered as high-risk if they do not pose a significant risk of harm to the health, safety or fundamental rights of natural persons. The article mentions four criteria. The exemption can be relied upon if one or more of these criteria are fulfilled (article 6(3) and recital 53):\n\n- the AI system is intended to perform a narrow procedural task;\n- -Example: a system which transforms unstructured data into structured data or a system which detects duplicates of documents\n\n- the AI system is intended to improve the result of a previously completed human activity;\n- -Example: a system which improves the professional tone or academic style of language used in already drafted documents\n- the AI system is intended to detect decisionmaking patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment, without proper human review; or\n- -Example: a system which  checks  flags inconsistencies or anomalies in the grades applied by a teacher, when compared with an existing grading pattern for that teacher\n- the AI system is intended to perform a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III\n- -Example: a system for translating documents.\n\nThe exception does not apply if the AI system involves profiling of natural persons within the meaning of article 4(4) of Regulation (EU) 2016/679 (GDPR) or article 3 (4) of Directive (EU) 2016/680 (Data Protection Enforcement Directive) or article 3, (5) of Regulation (EU) 2018/1725 (Data Protection for EU institutions) (recital 53).\n\nCompanies deciding to make use of this exemption should note that they carry the burden of proof as to whether the system is high-risk. The assessment under  article 6(3) must be documented before the system is placed on the market or put into service and the system must be registered (articles 49(2) and 6(4)). Providers of such systems must provide this documentation to national competent authorities on request.\n\nThe Commission will provide guidelines specifying the practical implementation of article 6, including a comprehensive list of practical examples of high-risk and non-high-risk use cases of AI systems (article 6(5)). It may also adopt delegated acts adding to or modifying the criteria for article 6(3). The guidelines are expected to be published within six months after entry into force of the AI Act.\n\n## Obligations for providers of high-risk AI systems\n\nThe AI Act provides a detailed list of obligations for providers and deployers of high-risk AI systems as follows in Chapter III, Sections 2, 3 and 4:\n\n## Obligations for providers on high-risk AI systems\n\n| Requirements of Section 2                | Ensure compliance with requirements of Section 2 (see below).                                                                                                                              |\n|------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Name of provider and contact information | Indicate on the system (or, if not possible, on its packaging or accompanying documentation) the name of the provider or its brand and its contact information.                            |\n| Quality management system                | Have a quality management system complying with article 17. (Article 17 provides a detailed list of aspects of the system to be documented through policies, procedures and instructions). |\n\nKeep the documentation referred to in article 18. The documentation will include:\n\n- technical documentation (article 11)\n- documentation concerning the quality management system (article 17)\n- documentation concerning changes approved by notified bodies, where applicable\n- decisions and other documents issued by notified bodies, where applicable\n- the EU declaration of conformity (article 47).\n\n<!-- image -->\n\n## Documentation\n\n## Logs\n\n## Conformity Assessment\n\n## Declaration of conformity\n\n## CE marking\n\n## Registration obligation\n\n## Corrective actions / provision of information\n\n## Demonstration of conformity\n\n## Accessibility requirements\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nIf the system is under their control, keep logs automatically generated by the system (article 19).\n\nSuch logs must be kept for a period appropriate to the intended purpose of the high-risk AI system. The period should be at least six months (unless any personal data protection provisions state otherwise).\n\nEnsure that the system undergoes the relevant conformity assessment procedure in article 43, prior to being placed on the market or put into service (see below).\n\nDraw up an EU declaration of conformity (article 47). See below.\n\nAffix the CE marking to the high-risk AI system (or, if not possible, on its packaging or accompanying documentation).\n\nThe CE marking will confirm the conformity of the high-risk AI system with the AI Act as per article 48.\n\nSee below.\n\nComply with EU Database registration obligations (article 49(I)). See below.\n\nIn cases where the system is not in conformity with the AI Act, take the necessary corrective actions, or withdraw, disable, or recall it.\n\nWhere the system presents a risk to safety, or the fundamental rights of persons, inform the competent market surveillance authorities and, where applicable, the notified body that issued a certificate for that system (article 79).\n\nUpon a reasoned request of a national competent authority, demonstrate the conformity of the system with the requirements set out in Section 2 (see above), providing all necessary information and documentation.\n\nThe duties relating to cooperation with competent authorities are set out in more detail in article 21.\n\nAny information shared with a national competent authority shall be treated as confidential.\n\nEnsure the high-risk AI system complies with accessibility requirements in accordance with:\n\n- Directive (EU) 2016/2102 (on the accessibility of the websites and mobile applications of public sector bodies); and\n- Directive (EU) 2019/882 (on the accessibility requirements for products and services).\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Harmonised standards and conformity assessment procedure for providers of high-risk AI systems\n\n## Harmonised standards\n\nHarmonised standards will be published in the Official Journal of the European Union. If the AI system complies with these standards, there will be a presumption of conformity with the requirements for high-risk AI systems in Chapter III, Section 2 (article 40(1).\n\nHarmonised standards are highly relevant in practice. Under traditional product safety laws, 'manufacturers' usually follow them to demonstrate compliance with product safety law requirements. This will be similar under the AI Act.\n\nThe European Commission issued a (draft) standardisation request in accordance with article 40(2) to standardisation bodies CEN/CELENEC, requesting these bodies to draft harmonised standards covering the requirements of Chapter III, Section 2 by 30 April 2025.\n\n## Conformity assessment procedure\n\nThe conformity assessment procedure for high-risk AI systems under article 43 requires providers to demonstrate compliance with the requirements for high-risk AI systems in Section 2 of Chapter III (overview below).\n\n## Annex III high-risk AI systems\n\nHere, the AI Act outlines two primary procedures for conformity assessment. Most providers of high-risk AI systems in Annex III (i.e. those referred to in points 2 to 8 of Annex III),  must follow the internal control procedure specified in Annex VI, without involving a notified body. Providers of high-risk AI systems listed in point 1 of Annex III (biometrics), who have applied harmonised standards or common specifications, as referenced in articles 40 and 41 must also follow the internal control procedure sufficient. However, for providers of high-risk biometric systems who have not done this theinvolvement of a notified body is required.\n\n## Annex I high-risk AI systems\n\nIf a high-risk AI system falls under Union harmonisation legislation listed in Section A of Annex I, the conformity assessment procedures from those legal acts apply. The high-risk AI system requirements of Section 2 in Chapter III are integrated into this assessment, and specific provisions of Annex VII also apply. Notified bodies under these legal acts must comply with certain requirements of the AI Act, to ensure consistent oversight.\n\n## New conformity assessments for substantial modifications\n\nSubstantial modifications to high-risk AI systems necessitate a new conformity assessment. However, changes that form part of the system's predetermined learning process do not count as substantial modifications.\n\n## Requirements for high-risk AI systems\n\n## Focus on Articles 8-15; requirements for high-risk AI systems\n\nCompliance with the requirements (Article 8)\n\nRisk management (Article 9)\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n4\n\nArticle 8 emphasises that high-risk AI systems must meet technical and organisational requirements (articles 9-15) throughout their life cycle, considering the intended use and the status of the technology. It's crucial to prioritise requirements impacting humans and if suitable trade-offs are not found, the AI system should not be deployed.\n\nArticle 9 requires providers to establish a risk management system. This is an ongoing process to identify, analyse, and mitigate foreseeable risks, including designing risk reduction measures, implementing controls, and providing user information and training. The measures taken must be documented and high-risk AI systems tested at appropriate stages to ensure consistent performance.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n| Data governance (Article 10)                                    | Robust data governance is a critical component of the technical and organisational requirements for high-risk AI systems. High-quality, representative, and to the best extent possible error-free and complete training, validation, and testing datasets are required to ensure proper functioning and safety of the system. Providers must also take measures to mitigate biases in datasets that could lead to prohibited discrimination, including by processing special categories of personal data under specific conditions. Certified third-party services can be employed for data integrity verification and to demonstrate compliance with the AI Act's data governance requirements.   |\n|-----------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Technical documentation and record keeping (Articles 11 and 12) | Articles 11 and 12 necessitate detailed technical documentation and record-keeping logs throughout the system's lifecycle. Providers must prepare this before deployment and regularly update it. It should cover all aspects of the system, including its characteristics, algorithms, data, training, testing, validation, and risk management. High-risk AI systems should also automatically record usage logs to provide traceability and identify potential risks or needed modifications.                                                                                                                                                                                                    |\n| Transparency and provision of information (Article 13)          | Article 13 mandates clear, comprehensive instructions for deployers of high-risk AI systems. These instructions should enable deployers to understand and use the system's outputs correctly. The system's decision- making must be understandable, and details on its identity, characteristics, limitations, purpose, accuracy, risks, capabilities, oversight, maintenance, and expected lifespan must be provided. All documentation should be tailored to the needs and knowledge level of the intended deployers.                                                                                                                                                                             |\n| Human oversight (Article 14)                                    | Human oversight measures must prevent or minimise risks to health, safety, and rights. These measures must be proportionate to the system's risks and level of autonomy. Human operators should also be able to override the system if necessary. Oversight can be achieved through: â€¢ Built-in system constraints and responsiveness to human operators. â€¢ Provider-identified measures for deployers to help them make informed, autonomous decisions. â€¢ Oversight approaches can include human-in-the-loop, human-on-the- loop, or human-in-command, depending on the application's risks.                                                                                                       |\n| Accuracy, robustness and cybersecurity (Article 15)             | Article 15 mandates that high-risk AI systems must achieve suitable accuracy, robustness, and cybersecurity levels. Accuracy measures include minimising prediction errors, robustness measures ensure systems can handle errors and inconsistencies. Lastly, cybersecurity measures shall protect against unauthorised system alterations in which case compliance can be demonstrated through the EU Cyber Resilience Act for relevant AI systems subject to the EU Cyber Resilience Act.                                                                                                                                                                                                         |\n\n<!-- image -->\n\n## Obligations for deployers of high-risk AI systems\n\nThe AI Act provides for obligations for deployers of high-risk AI systems (article 26):\n\n## Technical and organisational measures\n\nDeployers must take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems.\n\n## Human oversight\n\nDeployers must assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support.\n\n## Input data\n\nWhere the deployer exercises control over input data, that deployer must ensure that the input data is relevant and sufficiently representative. In other words, this principle states the deployer's responsibility as to the quality of the input data.\n\n## Monitoring high-risk AI system\n\nDeployers must monitor the operation of the highrisk AI system based on the instructions for use.\n\nDeployers must inform providers in accordance with article 72 relating to post-marketing activities. If the deployer identifies a  risk  per article 79(1) it will immediately inform the provider, and then the importer or distributor and the relevant market surveillance authorities and suspend the use of that system. If a serious incident is identified, deployers must also immediately inform the provider, and then the importer or distributor and the relevant market surveillance authorities of that incident.\n\n## Logs\n\nDeployers of high-risk AI systems must keep logs automatically generated by that high-risk AI system where these  logs are under their control, for a period appropriate to the intended purpose of the high-risk AI system. This period is at least six months, unless provided otherwise in applicable Union or national law, in particular on the protection of personal data.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Information to the workers' representatives\n\nDeployers who are employers must inform workers' representatives and the affected workers that they will be subject to the use of the high-risk AI system.\n\n## Public authority deployers\n\nDeployers of high-risk AI systems who are public authorities, or Union institutions, bodies, offices or agencies must comply with the EU Database registration obligations under article 49.\n\n## Data protection impact assessment\n\nIf deployers of high-risk AI systems are required to perform a data protection impact assessment under article 35 of Regulation (EU) 2016/679 (GDPR) or article 27 of Directive (EU) 2016/680 (Data Protection Enforcement Directive), they must make use of the information provided by the provider under article 13 of the AI Act.\n\n## Investigation for criminal offences - highrisk AI system for post-remote biometric identification\n\nWithout prejudice to Directive (EU) 2016/680 (Data Protection Enforcement Directive), in the framework of an investigation for the targeted search of a person suspected or convicted of having committed a criminal offence, someone who wishes to  deploy a high-risk AI system for post-remote biometric identification must request an authorisation for this use, ex-ante, or without undue delay and no later than 48 hours, from a judicial authority or an administrative authority.\n\n## Fundamental rights impact assessment for high-risk AI systems\n\nPrior to deploying a high-risk AI system referred to in article 6(2) (i.e. high-risk AI systems detailed in Annex III of AI Act), deployers that are:\n\n- I. bodies governed by public law, or\n- II. private entities providing public services, and in each case are\n- III. deployers of high-risk AI systems intended to be used\n\n- a. to evaluate the creditworthiness of natural persons or establish their credit score (apart from AI systems used for the purpose of detecting financial fraud), and\n- b. for risk assessment and pricing in relation to natural persons in the case of life and health insurance must perform an assessment of the impact of the use of the system on fundamental rights (FRIA) . There is an exception for high-risk AI systems relating to critical infrastructure.\n\nThe assessment consists of:\n\n- a description of the deployer's processes in which the high-risk AI system will be used in line with its intended purpose;\n- a description of the time period within which, and the frequency with which, each high-risk AI system is intended to be used;\n- the categories of natural persons and groups likely to be affected by its use in the specific context;\n- the specific risks of harm likely to have an impact on the categories of natural persons or groups of persons identified pursuant to point above, considering the information given by the provider pursuant to article 13;\n- a description of the implementation of human oversight measures, according to the instructions for use; and\n- the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal governance and complaint mechanisms.\n\n<!-- image -->\n\n## Obligations for other parties in connection with high-risk AI systems\n\nMost obligations regarding high-risk systems in the AI Act are directed at providers and deployers. However, there are also a limited set of obligations for other parties: namely, importers and distributors of high-risk AI systems, and suppliers of any systems, tools, services, components or processes which are used or integrated in high-risk AI systems. Examples of services by suppliers include model (re)training, testing and evaluation and integration into software (recital 88). The obligations do not apply to suppliers that offer the relevant product or service under a free and open-source licence (article 25(4)). Additionally, it is possible for parties other than the original provider of an AI system to be assigned the role of provider of a high-risk AI system by the AI Act.\n\n## Obligations for importers, distributors and suppliers\n\nArticles 23, 24 and 25 set out the obligations for importers, distributors and suppliers:\n\n## Importers (article 23)\n\nVerification: before placing the system on the market, verifying that the provider has genuinely:\n\n- carried out the conformity assessment procedure;\n- drawn up the technical documentation;\n- affixed the CE marking and has attached the EU declaration of conformity; and\n- appointed an authorised representative.\n\nRisk flagging: inform the provider, the authorised representative and the market surveillance authority when the system presents a risk 1 to health, safety or fundamental rights of persons.\n\nCare: ensure that storage or transport conditions do not jeopardise compliance with the requirements in Section 2.\n\n## Distributors (article 24)\n\nVerification: before making the system available on the market, verifying that:\n\n- it bears the CE marking;\n- it is accompanied by a copy of the EU declaration of conformity and instructions for use; and\n- the provider and the importer, as applicable, have complied with their respective obligations.\n\nRisk flagging : not make the system available when the distributor considers or has reason to consider that the system is not in conformity with the requirements set out in Section 2, until the system has been brought into conformity, and where the system presents a risk to health, safety or fundamental rights of persons, immediately inform the provider or the importer of the system and the competent authorities, giving details, in particular, of the non-compliance and of any corrective actions taken.\n\nCare: ensure that storage or transport conditions do not jeopardise compliance with the requirements in Section 2.\n\n1.  Risk here means: 'having the potential to affect adversely health and safety of persons in general, health and safety (...) to a degree which goes beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the product concerned, including the duration of use and, where applicable, its putting into service, installation and maintenance requirements' (article 79(1) AI Act in conjunction with Article 3(19) of Regulation (EU) 2019/1020 (Market surveillance regulation).\n\n<!-- image -->\n\n## Suppliers (article 25)\n\nProvide assistance: by written agreement, specifying the necessary information, capabilities, technical access and other assistance based on the generally acknowledged state of the art, in order to enable the provider of the high-risk AI system to fully comply with their obligation.\n\nThe AI Office/Commission may also develop and recommend voluntary model contractual terms between providers of high-risk AI systems and their third-party suppliers (article 25(4)) and recital 90).\n\n## Importers (article 23)\n\nCooperation with authorities: upon a reasoned request, provide competent authorities with all necessary information/ documentation, including technical documentation, to demonstrate conformity of the system and cooperate with these authorities in any action they take in relation to the system.\n\nRecord keeping: keep, for a period of ten years after the system has been placed on the market/put into service, a copy of: the certificate issued by the notified body (in the event of third-party conformity assessment), the instructions for use and the EU declaration of conformity.\n\nContact details: indicate name, registered trade name or registered trademark and the address at which the importer can be contacted on the system and its packaging or accompanying documentation.\n\n## Distributors (article 24)\n\nCooperation with authorities: upon a reasoned request, provide competent authorities with all necessary information/documentation regarding their obligations in the rows above to demonstrate the conformity of that system, and cooperate with these authorities in any action they take in relation to the system.\n\nCorrective actions: take the corrective actions necessary to bring the system into conformity, where the distributor considers or has reason to consider the system not to be in conformity with the requirements set out in Section 2, or withdraw or recall the system, or ensure that the provider, the importer or any relevant operator, as appropriate, takes those corrective actions.\n\n<!-- image -->\n\n## Suppliers (article 25)\n\n## Becoming a provider of someone else's (high-risk) AI system\n\nArticle 25(1) provides that a person will be considered the provider of a high-risk AI system, even if that person was not originally the provider of the AI system, when that person:\n\n- places their name or trademark on a highrisk AI system which is already placed on the market or put into service;\n- makes a substantial modification 2  to an existing high-risk AI system in such a way that it remains high-risk; and/or\n- modifies the intended purpose of an AI system of an AI system which is not currently high-risk so that it becomes high-risk.\n\nIf any of these three situations occur, the original provider will no longer be considered the provider of the (new or newly used) AI-system. One situation which often occurs in practice that could lead to such switching of provider roles is the deployment of a general-purpose AI system by a deployer in a way that falls within the high-risk category as set out in article 6 (and Annexes I and III). As such, if a person deploys  a general-purpose AI system in a high-risk way, that deployer assumes the responsibilities of a provider.\n\nThe new provider will assume all the obligations of a provider of a high-risk AI system. The original provider is obliged to closely cooperate with the new provider and make available the necessary information and provide reasonably expected technical access and other assistance to the new provider to bring the system into conformity with the AI Act (article 25(2)). If, however, that original provider had 'clearly specified' that the AI system was not to be changed into a high-risk AI system (article 25(2)) or 'expressly excluded the change of the AI system into a high-risk AI system' (recital 86), for example by prohibiting deployment for high-risk purposes in the applicable contract(s), then that original provider is not obligated to do\n\n2.  A 'substantial modification' is defined in article 3(23) as 'a change to an AI system after its placing on the market or putting into service which is not foreseen or planned in the initial conformity assessment carried out by the provider and as a result of which the compliance of the AI system with the requirements set out in Chapter III, Section 2 is affected or results in a modification to the intended purpose for which the AI system has been assessed' . The Commission will provide further guidelines on the practical implementation of the provisions related to substantial modification (Article 96(1)(c)). Recital 84 also provides that provisions established in certain Union harmonisation legislation based on the New Legislative Framework, such as the Medical Device Regulation, should continue to apply. For example, article 16(2) of the Medical Device Regulation provides that certain changes should not be modifications of a device that could affect its compliance with the applicable requirements, and these provisions should continue to apply to high-risk AI systems which are medical devices within the meaning of the Medical Device Regulation.\n\n<!-- image -->\n\nthis. If high-risk deployment is not prohibited, then the co-operation obligation applies, but is without prejudice to the need to observe and protect intellectual property rights, confidential business information and trade secrets (article 25(5)). As such, the original provider does not have to help to the extent that it compromises their own intellectual property rights or trade secrets (recital 88).\n\nThe Commission will provide guidelines on the application of the requirements and obligations referred to in this article 25 (article 96(1)(a)).\n\n<!-- image -->\n\n|                                                            | Where can I find this?     | Where can I find this?                      |\n|------------------------------------------------------------|----------------------------|---------------------------------------------|\n| Scope of high-risk systems                                 | Scope of high-risk systems | article 6, Annexes I and III recitals 46-63 |\n| Requirements for providers of high-risk AI systems         | articles 8-22, 47-49       | 43, recitals 64-83, 123-128, 147, 131       |\n| Requirements for deployers of high-risk AI systems article | 26,                        | 27 recitals 91-96                           |\n| Requirements for importers of high-risk AI systems         | article 23                 | recitals 83                                 |\n| Requirements for distributors of high-risk AI systems      | article 24                 | recitals 83                                 |\n| Requirement for third-party suppliers to high-risk systems | article 25                 | recitals 83-90                              |\n| Standards                                                  | article 40,                | 41 recital 121                              |\n| Conformity assessment procedure                            | article 28                 | recital 149                                 |\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## General-purpose AI models\n\n<!-- image -->\n\n- General-purpose AI models are versatile AI components demonstrating immense generality in the tasks they can handle, particularly encompassing current generative AI models.\n- Fine-tuning and modification of generalpurpose AI models may result in new generalpurpose AI models.\n- Providers of general-purpose AI models are tasked with a number of transparency obligations both towards the AI Office and competent authorities as well as towards AI systems providers intending to integrate their AI systems with general-purpose AI models.\n- General purpose AI models that pose systemic risks, i.e., the most versatile and powerful models to date, are under heightened evaluation, transparency, security, risk assessment and incident management obligations. The classification procedure for general-purpose AI models with systemic risk should be a key area of focus for generalpurpose AI models providers.\n- The development and publication of codes of practice will help general-purpose AI models providers identify specific technical and organisational measures to implement in order to comply with their obligations.\n- Provisions regarding general-purpose AI models will apply from 2 August 2025.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n4\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFamiliarise yourself with the concepts of general-purpose AI models, generalpurpose AI systems, AI systems, and high-risk AI systems - and their relation to each other. This understanding is crucial for assessing which systems your company uses or markets and for making informed legal evaluations.\n\n- For providers of general-purpose AI models: undertake a thorough governance review and make necessary adjustments to ensure compliance - the obligations for providers of general-purpose AI models are among the strictest in the AI Act.\n- For providers of general-purpose AI models: conduct a comprehensive legal IP assessment - regulations for generalpurpose AI models are heavily intertwined with IP laws, particularly regarding the copyright policy and the various training data obligations.\n- For providers of general-purpose AI models: continuously and closely monitor the thresholds for 'systemic risk,' as these may be adjusted over time via delegated acts.\n- For providers of general-purpose AI models: keep an eye out for the development and publication of codes of practice, which will include specific and technical details on how to comply with the obligations for general-purpose model providers in practice. Sign up to our Connected newsletter and keep up with the latest developments here!\n\n<!-- image -->\n\n## Background and relevance of general-purpose AI models\n\nOne of the most prominent debates in the legislative process of the AI Act revolved around the regulation of general-purpose AI. The first draft of the AI Act (the Commission's proposal of April 2021) was based on the understanding that each AI system is created for a specific purpose, and that this purpose can be associated with a specific risk potential. This classification did not have in mind foundation models which are trained on broad data such that it can be applied across a wide range of use cases. These AI models did not fit into the risk-based scheme of the first draft of the AI Act. The categorisation had to be expanded to include a new category that took into account the specific capabilities and dangers of such models. In the summer of 2023, the 'foundation model' (later renamed general-purpose AI) was added to the then-current draft of the AI Act.\n\nThe AI Act's chapter on the regulation of generalpurpose AI models holds significant importance for two main reasons:\n\n- firstly, it addresses generative AI, a subset of AI that is currently opening up the most intriguing new opportunities in the business environment and encompasses the majority of corporate use cases; and\n- secondly, the requirements for generalpurpose AI under the AI Act, alongside those for high-risk AI systems, are the most demanding in the AI Act, necessitating the utmost diligence in corporate implementation.\n\nThis significance is only somewhat diminished by the fact that all requirements are directed solely at providers, not deployers.\n\n<!-- image -->\n\n## Terminology and general-purpose AI value chain\n\n## General-purpose AI models and generalpurpose AI systems\n\nArticle 3(63) outlines the characteristics of a general-purpose AI model, emphasising its versatility and competence across various tasks.\n\nRecital 98 highlights two key indicators:\n\n1. having at least a billion parameters; and\n2. being trained with a large amount of data using self-supervision.\n\nThese models are distinguished by their ability to integrate into and function within diverse downstream systems or applications. Typically, general-purpose AI models undergo extensive training with large datasets, often utilising methods like self-supervision at scale. Recital 99 further specifies that large generative AI Models, such as LLMs or Diffusion Models, are typical examples of general-purpose AI models.\n\nRecital 97 clarifies that while general-purpose AI models are crucial components of AI systems, they are not AI systems themselves. Additional elements, such as user interfaces, are needed to transform general-purpose AI models into fully operational AI systems. A general-purpose AI system is an AI system built upon a generalpurpose AI model, maintaining its versatility across various tasks (article 3(66) and recital 100). To clarify with an example, a system that solely performs translations would likely not qualify as a general-purpose AI system.\n\n## General-purpose AI systems and high-risk AI systems\n\nRecital 85 emphasises that general-purpose AI systems, due to their versatility, may function as high-risk AI systems or as components within them. Providers of general-purpose AI systems must collaborate closely with providers of highrisk AI systems to ensure compliance with the AI Act and to distribute responsibilities fairly along the AI value chain (see Chapter 4 for more on high-risk systems).\n\n## Modification and fine-tuning of generalpurpose AI models\n\nModifying or fine-tuning a general-purpose AI model, where a new specialised training data set is fed into the model to achieve better performance for specific tasks, does not transform it into a general-purpose AI system; it remains an abstract model without an interface. Instead, such actions create a modified or fine-tuned general-purpose AI model. Recital 97 and recital 109 specify that a provider who modifies or fine-tunes a general-purpose AI model has limited obligations related only to the changes made, including providing technical documentation or a summary of the training data used.\n\n## Obligations for providers of general-purpose AI models\n\nA provider of a general-purpose AI model that places such a model on the market, or integrates it with its own AI system and places it on the market or puts it into service, is obliged to:\n\n- a. prepare and maintain up-to-date technical documentation containing i.a. a description of the model and information on its development process (including training, testing and validation) for the purpose of making it available to the AI Office and competent authorities (article 53(1)(a)) -aÂ list of the minimum information required is provided in Annex XI;\n- b. prepare, maintain up-to-date and make certain information and documentation available to downstream AI systems providers (i.e. those who wish to integrate their AI systems with the general-purpose AI model) so that they can understand the model's characteristics and comply with their own obligations (article 53(1)(b)) - aÂ list of the minimum information required is provided in Annex XII; providers are allowed to balance the information they share against their need to protect confidential business information and trade secrets;\n- c. establish a policy to comply with the EU regulations on copyright and related rights (article 53(1)(c)), taking into account, i.a., the right to opt-out of text and data mining as provided for in article 4(3) of Directive (EU) 2019/790 -on copyright and related rights\n4. in the Digital Single Market (the AI Act does not specify other matters that have to be addressed in the policy);\n- d. prepare and publicly share a comprehensive summary on the data used for training the model (article 53(1)(d)) - the AI Office is tasked with providing a template for this purpose; as the recital 107 explains the summary should allow interested parties to exercise their rights by, for example, listing main data collections, databases or data archives used;\n- e. cooperate with the relevant authorities when they exercise the powers granted to them under AI Act (article 53(3)); and\n- f. if the provider is established outside the EU: appoint an authorised representative in the EU (article 54(1)).\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nIf a provider releases a general-purpose AI model under a free and open-source licence and makes relevant information publicly available, it is not obliged to fulfil the requirements listed in a-b and f above - unless the general-purpose AI model is qualified as presenting a systemic risk (article 53(2) and article 54(6)).\n\n## General-purpose AI models with systemic risk\n\n## Qualification criteria\n\nThe AI Act introduces specific heightened obligations for general-purpose AI models presenting 'systemic risks' , e.g. reasonably foreseeable negative effects relating to major accidents, disruption of critical sectors, serious consequences to public health and safety, public and economic security, democratic processes, the dissemination of false or discriminatory content, etc. (recital 110).\n\nAccording to article 51(1) of the AI Act, a general-purpose AI model is classified as a general-purpose AI model with systemic risk if it meets one of these two conditions: (a) it has ' high impact capabilities ' evaluated on the basis of technical tools and methodologies, or (b) is designated by the Commission as having capabilities or impact equivalent to those set out in point (a) having regard to the criteria set out in Annex XIII of the AI Act. These criteria notably\n\ninclude the number of parameters of the model, the quality or size of the data set, the amount of computation used for training, the model's impact on the European market, the number of registered users the EU.\n\nIn addition, a model is presumed to have ' high impact capabilities ' if it is trained with more than 10^25 floating point operations, i.e., massive computing powers (article 51(2)). At the time of this Guide, only a handful of Large Language Models seem to meet this threshold.\n\nArticle 52 of the AI Act sets out the classification procedure. Most notably, providers of generalpurpose AI models which meet the systemic risk classification conditions must notify the Commission without delay, and at the latest within two weeks after that requirement is met or it becomes known that it will be met. Providers may present arguments to demonstrate that their models do not pose systemic risks despite meeting the requirements. Should such arguments be rejected by the Commission, the concerned models will be considered as presenting\n\n<!-- image -->\n\nsystemic risks. Upon 'reasoned request' of a provider, the Commission may decide to reassess the classification (article 52(5)).\n\nA list of general-purpose AI models with systemic risk will be published and updated by the Commission (article 52(6).\n\n## Obligations for providers of general-purpose AI models with systemic risk\n\nIn addition to the general requirements applicable to all general-purpose AI models providers, the AI Act imposes additional heightened obligations on providers of generalpurpose AI models with systemic risk (articles 53(1) and 55(1)). These obligations apply prior to the models' placing on the market and throughout their entire lifecycle, and relate to:\n\n- models evaluation;\n- assessment and mitigation of systemic risks;\n- incident management and reporting;\n- increased level of cybersecurity protection; and\n- extended technical documentation.\n\n## Transparency\n\n<!-- image -->\n\nThe AI Act classifies AI systems by risk level, with increased transparency demands for high-risk categories. Transparency is required for high-risk AI systems before they are placed on the market or put into service. See Chapter 4 of this guide for more details regarding the transparency requirements for high-risk AI systems.\n\nAdditionally, the AI Act mandates transparency requirements under article 50 for specific types of products, requiring that adequate information be provided to individuals, by either providers or deployers.\n\n- Disclaimers: providers of AI systems intended to interact directly with individuals' need to design and develop them, so that the individuals will be informed about the fact that they are interacting with an AI system.\n- Marking requirement: providers of AI systems must mark AI-generated content (audio, images, videos, text) in a way that distinguishes it from humangenerated content.\n- Deepfake marking: AI-generated content (images, audio, video) that resembles real entities and could mislead people into believing it is authentic must be labelled.\n- Emotion recognition system/ biometric categorisation system: deployers of AIsystems should make individuals aware of the operation of these systems.\n\nThe AI Act's transparency obligations collate with the other regulatory framework in the EU. In particular, there is some overlap between the transparency requirements of the GDPR and the AI Act, although the latter is more technical in nature.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## For providers\n\n<!-- image -->\n\nImplement Marking: ensure AI-generated content is marked in a machine-readable format.\n\n<!-- image -->\n\nImplement Disclaimers: ensure proper disclaimers are added to AI systems intended to interact directly with individuals.\n\n## For deployers\n\n<!-- image -->\n\nDeepfakes: label as 'Deepfake' in a clear and distinguishable manner to disclose their artificial creation or manipulation.\n\n<!-- image -->\n\nEmotion recognition system/ biometric categorisation system: make individuals aware that such a system is operating.\n\n## General transparency obligations\n\nThe AI Act acknowledges the importance of transparency in the use of AI systems. Individuals should be enabled to understand the AI system's design and use, and there should be accountability for decisions made by companies and public authorities. Transparency is also essential for creating public trust in AI systems and ensuring their responsible deployment.\n\nTransparency also enhances the broader concept of 'AI literacy', developing awareness about the opportunities and risks of AI and the possible harm it can cause. Such awareness should especially be developed amongst:\n\n- individuals concerned, giving them a better understanding of their rights in the context of AI, and\n- deployers, allowing them to deploy AI systems in an informed way.\n\nProviders, and in certain circumstances deployers as well, have their own transparency requirements. The AI Act classifies AI systems by risk level, with increasing transparency demands for higher risk categories.\n\nThe transparency requirements for specific types of products are described below.\n\n## Provider Obligations:\n\n## Chatbots (article 50(1) AI Act)\n\nArticle 50(1) of the AI Act mandates that providers of AI systems need to ensure that such systems intended to interact directly with individuals are designed and developed such that the individuals concerned are informed that they are interacting with an AI system.\n\n- Target audience: when implementing that obligation, the provider should identify not only the intended but also the broader potential target audience to whom the disclaimer may be displayed. The characteristics of individuals belonging to vulnerable groups due to their age or disability should be taken into account, to the extent the AI system is also intended to interact with those groups. The intended or potential target audience has a significant impact on accessibility considerations.\n- Form: in practice, providers can design a disclaimer in different forms (e.g. as an avatar, icon or interface), as long as it provides clear information that the individual is interacting with an AI system.\n\n<!-- image -->\n\n## Exemptions\n\n- Obvious cases: if, considering the circumstances and the context of the AI system use, it is obvious for an individual who is reasonably well-informed, observant and circumspect that they are interacting with an AI system, then the system is exempt from this transparency requirement.\n- Legal use: AI systems that are permitted by law for use in detecting, preventing, investigating, or prosecuting criminal activities, that are subject to appropriate safeguards for the rights and freedoms of third parties, are also exempt from these transparency requirements, unless those systems are available for the public to report a criminal offence.\n\n## Marking of AI-Generated Content (article 50(2) AI Act)\n\nArticle 50(2) of the AI Act mandates that providers of AI systems, including generalpurpose AI systems, must appropriately mark synthetic content such as audio, images, videos, or text. Recital 133 explains the rationale: with AI technology advancing, AI-generated synthetic content is becoming increasingly indistinguishable from human-generated content, posing the risk of misinformation, manipulation, fraud, impersonation, and consumer deception.\n\n## The Marking Obligation\n\n- Marking : only providers of AI systems are required to mark AI-generated content. This requirement does not extend to deployers or other users of the content.\n- Format : the output must be marked in a machine-readable format to indicate that it is artificially generated or manipulated.\n\n- Technical standards : the markings should be effective, interoperable, robust, and reliable. Providers need to consider the type of content, implementation costs, and current technical standards.\n\n## Marking methods:\n\n- -Watermarks: visible watermarks can be easily implemented - but also removed with basic editing tools, whereas invisible watermarks require specialised software for detection and removal.\n- -Metadata: this provides information about the file's creation and origin but can be easily altered or removed with file editing tools.\n- -Algorithmic fingerprints: AI models leave unique traces or anomalies in the content they generate. For instance, AI-generated images might have minor distortions in textures or patterns, and AI-created audio files could display unnatural pauses or tonal shifts.\n- -Cryptographic signatures: digital signatures embedded using cryptographic methods, such as a cryptographic hash that verifies content authenticity. Even minor changes in the data result in a different hash, ensuring easy verification of alterations.\n\nNumerous tools and initiatives exist to manage and detect AI-generated content. Certain platforms use deepfake detection software that analyses algorithmic patterns and embedded metadata, while others rely on metadata and cryptographic hashes to authenticate the source of the content. For example, platforms might use voice analysis tools to detect synthetic audio, or employ blockchain technology to track the origin of and modifications to digital art.\n\n## Exemptions\n\n- Editorial assistance: AI systems that mainly provide support for routine editing tasks or do not significantly change the original input data are exempt from the marking obligation.\n- Legal use: AI systems that are authorised for use in detecting, preventing, investigating, or prosecuting criminal activities are also exempt from the marking requirement.\n\n<!-- image -->\n\n## Deployer obligations:\n\n## Emotion recognition/ biometric categorisation systems (article 50(3) AI Act)\n\nArticle 50(3) of the AI Act sets forth specific transparency requirements for deployers of:\n\n- Emotion recognition systems: AI systems used for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data, e.g., nonverbal signs such as facial expression.\n\nor\n\n- Biometric categorisation systems: AI systems used for the purpose of assigning natural persons to specific categories on the basis of their biometric data. Such specific categories can relate, e.g., to aspects such as sex, age, hair colour, eye colour, tattoos, personal traits, ethnic origin, personal preferences and interests.\n\nSee Chapter 4 of this guide for more details on when the use of emotion recognition systems or biometric categorisation systems is prohibited.\n\nWhen these systems are allowed, deployers must inform the natural persons exposed to them about the use of the system. In particular, individuals should be notified when they are exposed to AI systems that, by processing their biometric data, can identify or infer their emotions or intentions or assign them to specific categories.\n\n## Exemptions\n\n- Legal use: AI systems that are permitted for use in detecting, preventing or investigating criminal activities that are subject to appropriate safeguards for the rights and freedoms of third parties and in accordance with the Union law, are exempt from these requirements.\n- Biometric categorisation systems of ancillary use: AI systems whose use is ancillary to another commercial service and strictly necessary for objective technical reasons are exempt from these requirements.\n\nAt present, there are no definitive guidelines on the scope of information that should be provided. Deployers, when using these systems, process personal data in accordance with GDPR and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable, apart from the requirements on\n\nthe legal basis of the processing. This means that these regulations also constitute separate transparency obligations for deployers acting as controllers. In such cases, individuals should nevertheless be informed about the processing of their data as required under Article 13 and 14 GDPR. In relation to any automated processing, controllers are expected to additionally explain the logic behind their decision-making. In the case of an AI system, this might be provided as part of an explainability statement - a document providing a non-technical explanation of i.a. why the organisation uses AI, how AI was developed, and how it operates and is used.\n\n## Deepfakes (article 50(4) AI Act)\n\nArticle 50(4) of the AI Act sets forth specific labelling requirements for content known as 'Deepfakes' . These obligations are crucial for ensuring transparency when AI systems are used to generate or manipulate content.\n\n## Definition of Deepfakes (article 3(60) AI Act)\n\nDeployers using AI to create content that:\n\n- generates or manipulates images, audio, or video;\n- significantly resembles real people, objects, places, entities, or events; and\n- could mislead a person into believing the content is authentic or truthful.\n\nExamples of Deepfakes:\n\n- Deepfake video calls mimicking company executives to trick employees into transferring large sums of money.\n- AI-generated audio of politicians misleading voters about election dates via robocalls.\n- Deepfake video ads impersonating political figures to manipulate public opinion on social media.\n- Fake Zoom interviews using deepfake technology to impersonate high-profile individuals.\n- Digital avatars delivering fabricated news reports to deceive viewers.\n\n<!-- image -->\n\n## Labelling requirements\n\nThe AI Act mandates that any content generated or manipulated by AI systems must be clearly and distinguishably labelled to disclose its artificial creation or manipulation. This requirement aims to ensure transparency and prevent the public from being misled by such content. At present, there are no definitive guidelines on how content should be labelled. This issue is likely to be addressed in future Codes of Conduct.\n\nTechniques such as watermarks, metadata identifications, fingerprints or other methods should be employed to indicate the content's artificial nature (see recital 133). It is crucial that these labels are easily, instantly and constantly visible to the audience. For instance, in the case of videos, pre-roll labels or persistent watermarks may be used to meet these requirements effectively.\n\n## Exemptions\n\nThere are certain reliefs and exceptions to the labelling requirements under article 50(4) AI Act:\n\n- The transparency requirements are more relaxed for artistic, creative, satirical, fictional, or similar works. Examples of such works include AI-generated movies or parodies, digital art exhibits, and AI-generated music videos. In these instances, the obligation is to disclose the AI involvement in a manner that does not disrupt the viewer's experience. This can be achieved through subtle watermarks, brief audio disclaimers, or notes in the description texts on digital platforms.\n- The obligation to label AI-generated content does not apply if the AI system's use is legally authorised for the purposes of detecting, preventing, investigating or prosecuting criminal offences.\n- The labelling obligation may not apply if the AI-generated content has undergone human review or editorial control, with a natural or legal person holding editorial responsibility for the publication. This means that, if a human has reviewed and approved the AI-generated content, ensuring its accuracy and integrity, the stringent labelling requirements may be relaxed. This exception recognises the role of human oversight in maintaining the quality and reliability of AI-generated content.\n\n## Transparency obligations for high-risk AI systems\n\nArticle 50(6) explains that the transparency obligations outlined here operate alongside other regulatory requirements. They neither replace nor reduce the obligations specified in Chapter III or other transparency requirements under EU or national legislation.\n\nSee Chapter 4 of this guide for more details.\n\n## Timing and format\n\nAll the information required to meet the transparency obligations under article 50 must be provided to the individuals concerned:\n\n- in a clear and distinguishable manner;\n- by no later than the time of their first interaction or exposure to the AI system; and\n- in conformity with the applicable accessibility requirements.\n\nThe accessibility requirement means that the information should be accessible to diverse audiences, including individuals with disabilities. In practice, this may imply that, depending on the circumstances, disclaimers or other marking methods will have to be displayed not only in written form but also in aural and (audio) visual form.\n\nAnother aspect to be taken into account is that the individual should be provided with an amount of information that is clear and adequate but not overwhelming.\n\n## Transparency obligations at the national level and codes of practice\n\nThe transparency obligations outlined in article 50(1)-(4) AI Act are designed to coexist with other regulatory requirements, according to article 50(6) AI Act. They neither replace nor\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\ndiminish the requirements set forth in Chapter III or other transparency mandates under Union or national law.\n\nThe AI Office is responsible for promoting and facilitating the development of codes of practice to support the effective implementation of the transparency obligations under article 50(1)-(4) AI Act at the EU level, under article 50(7) AI Act. These codes are intended to clarify the methods for detecting and labelling AI-generated content, to enhance cooperation throughout the value chain, and to ensure that the public can clearly distinguish between content created by humans, and content generated by AI (recital 135).\n\n## Relationship with other regulatory frameworks\n\n- The AI Act's marking obligations under article 50(2)-(4) support the Digital Services Act's (DSA) requirements for very large online platforms (VLOP) and search engines (VLOS) to identify and mitigate the risks associated with the dissemination of deepfakes (article 33 et seq. DSA). If the AI provider is separate from the VLOP or VLOS, these markings enable the platforms to recognise AI-generated content more efficiently. Conversely, if a VLOP or VLOS is also the AI provider, their DSA obligations are further detailed and enhanced by the AI Act.\n- The transparency regulations for deepfakes will correlate with the European guidelines on misleading advertising (see Unfair Commercial Practices Directive) as well as national criminal provisions on deepfakes.\n- The AI Act's transparency obligations also support and supplement the transparency requirements under Regulation (EU) 2016/679. However, the GDPR transparency requirements apply if personal data is processed when using AI technologies at all different stages of the AI lifecycle (e.g. when developing, testing or deploying AI technologies), and apply to controllers. Developers and providers of AI tools will not always be acting in such a role. In such case they may still be obliged to provide specific information to controllers to enable the latter to meet their obligations.\n\n## AI regulatory sandboxes\n\n<!-- image -->\n\n- The AI Act enables the establishment of 'AI regulatory sandboxes' to provide a controlled environment in which to test innovative AI systems for a limited period before they are placed on the market.\n- This regime is intended to encourage AI providers (or potential providers) to experiment with new and innovative products under supervision by regulators. There are specific incentives aimed at encouraging participation by SMEs and start-ups.\n- Each Member State must establish at least one AI regulatory sandbox by 2 August 2026, although this can be done in co-operation with other Member States.\n- The Commission is expected to adopt implementing acts to set out detailed arrangements for the establishment, operation and supervision of AI regulatory sandboxes.\n- The AI Act also provides for 'real-world' testing of AI systems, both inside and outside of regulatory sandboxes, subject to certain conditions to protect participants.\n- The regimes relating to AI regulatory sandboxes and real-world testing are intended to be harmonise across the Union. However, there is the potential for divergent approaches at a national level, leading to a possibility of 'forum shopping' by providers.\n- Participation in AI regulatory sandboxes and real-world testing is voluntary. AI providers should familiarise themselves with the relevant provisions of the AI Act if they intend to participate in a sandbox or real-world tests and should look out for further announcements and guidance on these topics, including detailed arrangements for AI regulatory sandboxes to be specified by the Commission in due course.\n- You should think about the countries in which you would like to test your AI services/products. Although the AI Act intends to establish a harmonised regime, there may be national differences which make some Member States more appropriate for you than others.\n- Once you decide to participate in an AI regulatory sandbox, you will need to prepare a sandbox plan and follow the guidelines and supervision provided by the relevant national competent authority. If you decide to conduct real-world tests, you will also need to prepare a testing plan and seek approval from the relevant market surveillance authority.\n- When you successfully complete an AI regulatory sandbox process, you should obtain an exit report from the relevant national competent authority. This may be useful to accelerate the conformity assessment process for your AI product/service.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## AI regulatory sandboxes\n\nThe AI Act enables the creation of 'regulatory sandboxes' to provide a controlled environment in which to test innovative AI systems for a limited period before they are placed on the market or otherwise put into service. The objectives of the AI regulatory sandbox regime include:\n\n- fostering AI innovation while ensuring innovative AI systems comply with the AI Act;\n- enhancing legal certainty for innovators;\n- enhancing national competent authority understanding of the opportunities, risks and the impacts of AI use;\n- supporting cooperation and the sharing of best practices; and\n- accelerating access to markets, including by removing barriers for SMEs and start-ups.\n\n## What is a regulatory sandbox under the AI Act?\n\nThe AI Act defines an 'AI regulatory sandbox' as:\n\n' a controlled framework set up by a competent authority which offers providers or prospective providers of AI systems the possibility to develop, train, validate and test, where appropriate in real-world conditions, an innovative AI system, pursuant to a sandbox plan for a limited time under regulatory supervision. '\n\nAI regulatory sandboxes can be established in physical, digital or hybrid form and may accommodate physical as well as digital products.\n\n## Obligation on Member States to establish AI regulatory sandboxes\n\nThe obligation to establish AI regulatory sandboxes rests with the Member States and their national competent authorities (see Chapter 8 for more on these). Each Member State must establish at least one AI regulatory sandbox by 2 August 2026. However, Member\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nStates can choose to either (i) establish one or more AI regulatory sandboxes at national level; (ii) jointly establish a sandbox with the national competent authorities of one or more other Member States or (iii) participate in an existing sandbox.\n\nNational competent authorities establishing AI regulatory sandboxes should cooperate with other relevant national competent authorities where appropriate and may also involve other actors within the AI ecosystem. The EU Data Protection Supervisor may also establish an AI regulatory sandbox for European Union institutions, bodies, offices and agencies.\n\nA list of planned and existing sandboxes will be made publicly available by the AI Office. The Commission also intends to develop a single interface containing relevant information relating to AI regulatory sandboxes to allow stakeholders to:\n\n- interact with AI regulatory sandboxes;\n- raise enquiries with national competent authorities; and\n- seek non-binding guidance on the conformity of innovative AI products, services or business models.\n\n## Who can participate in AI regulatory sandboxes?\n\nThe sandbox regime is aimed at providers (or prospective providers) of AI systems, although applications can be submitted in partnership with deployers and other relevant third parties.\n\nThere are specific provisions which are designed to encourage participation by SMEs and startups, including:\n\n- access to sandboxes should generally be free of charge for SMEs and start-ups;\n- priority access for SMEs and start-ups with a registered office or branch in the EU; and\n- SMEs and start-ups should have access to guidance on the implementation of the AI Act and other value-added services.\n\n## Liability\n\nProviders and prospective providers participating in an AI regulatory sandbox (including SMEs and start-ups) will remain liable for any harm inflicted on third parties as a result of the experimentation taking place in the sandbox. However, administrative fines will not be imposed on prospective providers if:\n\n- they observe the relevant sandbox plan and the terms and conditions for their participation; and\n- follow (in good faith) any guidance given by the national competent authority.\n\n## Implementation of the sandbox regime\n\nIn order to avoid fragmentation across the EU, the Commission intends to adopt implementing acts specifying the detailed arrangements for the establishment, operation and supervision of AI regulatory sandboxes, including common principles on:\n\n- eligibility and selection criteria for participation;\n- procedures for the application, participation, monitoring, exiting from and termination of sandboxes; and\n- the terms and conditions applicable to participants.\n\nThese implementing acts are intended to ensure that AI regulatory sandboxes:\n\n- are open to any provider who meets fair and transparent eligibility criteria;\n- allow broad and equal access and keep up with demand for participation;\n- facilitate the development of tools and infrastructure for testing and explaining dimensions of AI systems relevant for regulatory learning, such as accuracy, robustness and cybersecurity, as well as measures to mitigate risks to fundamental rights and society at large;\n- facilitate the involvement of relevant actors within the AI ecosystem (e.g. notified bodies and standardisation organisations, testing and experimentation facilities, research and experimentation labs and European Digital Innovation Hubs), and also that participation in an AI regulatory sandbox is uniformly\n\n<!-- image -->\n\nrecognised (and carries the same legal effects) across the EU.\n\n## National competent authority obligations\n\nNational competent authorities must:\n\n- allocate sufficient resources to ensure their sandbox regime complies with the requirements of the AI Act;\n- provide guidance to sandbox participants on how to fulfil the requirements of the AI Act;\n- provide participants with an exit report detailing the activities carried out in the sandbox, results and learning outcomes, which can later be used to demonstrate compliance with the AI Act through the conformity assessment process or relevant market surveillance activities; and\n- provide annual reports to the AI Office and the Board (see Chapter 8 for more on these), identifying best practices, incidents and lessons learnt.\n\nNational competent authorities will retain supervisory powers in relation to sandbox activities, including the ability to suspend or terminate activities carried out within a sandbox where it is necessary to address significant risks to fundamental rights or health and safety.\n\n## Processing of personal data within sandboxes\n\nPersonal data which has been lawfully collected for other purposes can be used in an AI regulatory sandbox subject to compliance with various conditions set out in the AI Act (all of which must be met for the relevant processing activities to be permitted). Some of the key conditions include:\n\n- the relevant AI system being deployed in the sandbox must be aimed at safeguarding substantial public interest (e.g. public health, energy sustainability, safety of critical infrastructure);\n- use of the personal data must be necessary and could not be substituted with anonymised or synthetic data;\n- the personal data must be handled in a separate and protected environment and must be subject to appropriate technical and organisational measures; and\n\n- a detailed description of the process and rationale behind the training, testing and validation of the AI system is retained, together with the testing results.\n\n## Real-world testing of AI systems\n\nThe AI Act also enables the testing of AI systems in 'real-world conditions', subject to certain conditions.\n\nThe AI Act defines 'testing in real-world conditions' as follows:\n\n'the temporary testing of an AI system for its intended purpose in real-world conditions outside a laboratory or otherwise simulated environment, with a view to gathering reliable and robust data and to assessing and verifying the conformity of the AI system with the requirements of [the AI Act] '.\n\nSuch real-world testing will not qualify as placing the relevant AI system on the market or putting it into service, provided that the relevant requirements of the AI Act are complied with. (See Chapter 2 for more on these concepts).\n\nThe AI Act primarily focusses on real-world testing of high-risk AI systems outside of AI regulatory sandboxes. However, the AI Act also contemplates the possibility of AI systems (whether high-risk or not) being subject to realworld testing within the framework of an AI regulatory sandbox, under the supervision of a national competent authority.\n\nIn both scenarios, the real-world testing must comply with various conditions set out in the AI Act (all of which must be met for the testing to be permitted, although there is greater\n\n<!-- image -->\n\nflexibility where the testing is conducted within a sandbox). Some of the key conditions include:\n\n- the proposed real-word tests have been approved by the relevant market surveillance authority and registered in the EU database for high-risk AI systems;\n- the provider conducting the testing is established in the EU (or has appointed a legal representative established in the EU);\n- testing is limited to a maximum of 6 months (which can be extended for an additional 6 months, although this requirement can be derogated from in relation to real-world testing within a sandbox environment);\n- participants in the real-world testing are properly protected - they must give informed consent, outcomes must be reversible (or capable of being disregarded) and they must be able to withdraw at any time; and\n- market surveillance authorities can conduct unannounced inspections on the conduct of real-world tests.\n\nProviders and prospective providers will be liable for any damage caused in the course of their real-world testing.\n\n## Is there a risk of 'forum shopping' in relation to participation in sandboxes and real-world testing?\n\nAlthough the AI Act aims to harmonise the regimes relating to AI regulatory sandboxes and real-world testing across the EU, industry representatives and stakeholders will no doubt closely monitor their development and may elect participate in sandboxes and/or real-world testing in jurisdictions which are perceived to have the most industry-friendly approach (including in how liability relating to participation in sandboxes or real-world testing is determined).\n\n## Enforcement and governance\n\n<!-- image -->\n\n- The AI Act puts in place a post-market monitoring, reporting and information sharing process.\n- Most obligations are on providers of high-risk AI systems who have to have post-market monitoring systems and procedures to report serious incidents.\n- The serious incident reporting obligations can also sometimes apply to deployers.\n- The timelines for reporting can be immediate.\n- Reports need to be made to market surveillance authorities in Member States where the incident occurred; reporting to multiple authorities may therefore be needed.\n- There is a multi-pronged approach to enforcement:\n- -The European Data Protection Supervisor handles EU institutions etc.\n- -The European Commission handles providers of general-purpose AI models.\n- -Competent authorities in each Member State are otherwise responsible.\n- Sanctions are tiered, by reference to the seriousness of the provision that has been infringed.\n- Affected persons have a right to explanation of individual decision-making.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Providers of high-risk AI systems should:\n\n- Watch for the European Commission template post-market monitoring plan, to be adopted by 2 February 2026.\n- Prepare and implement a post-market monitoring plan.\n- If already subject to existing postmarket monitoring obligations, or a regulated financial services provider, consider if you can integrate your AI Act obligations into these systems.\n\n## Providers of high-risk systems should:\n\n- Consider if they are already subject to other equivalent obligations; if so, check if you have double reporting obligations or not.\n- Ensure quality management systems include serious incident reporting procedures.\n- Ensure these procedures establish the nature of the serious incident (death, serious harm to health, violation of fundamental rights etc) and if they are widespread.\n- Identify to whom you would have to report.\n\n## Deployers of high-risk systems should:\n\n- Develop stand-by procedures so they can report if needed.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Providers and developers should:\n\n- Check for the European Commission guidance due by 2 August 2025.\n- Keep this under review as it will be re-assessed.\n\nOperators of non-high-risk AI systems should:\n\n- Ensure they comply with all existing product safety legislation.\n- Providers of general-purpose AI models should:\n- Look out for, and consider responding to the consultation on, the European Commission implementing act relating to the arrangements for enforcement by the European Commission.\n\nAll organisations in the AI value chain\n\n- should:\n- Look out for, and consider responding to consultations on, rules relating to enforcement adopted at Member State level.\n- Note the requirement to cooperate with market surveillance authorities where there is sufficient reason to consider that an AI system presents a risk.\n- Note that disclosure of training, validation and testing data sets and source code might have to be disclosed.\n\nDeployers of high-risk systems should:\n\n- Ensure they are able to provide clear and meaningful explanations as to the AI's decision-making procedure.\n\n## Overview\n\nThe AI Act outlines a governance framework that provides for the implementation and supervision of both the ex ante requirements for AI systems and ex post surveillance and enforcement.  The former is described in preceding chapters.  The latter is the subject of this chapter, together with a description of the governance structure.\n\nThe enforcement regime addresses two types of risk: risks to product safety, and risks to fundamental rights.  In relation to the former, the AI Act builds upon existing product safety legislation and is mostly enforced by national market surveillance authorities.  Where risks to fundamental rights are identified, the market surveillance authorities shall inform and fully cooperate with the relevant national public authorities or bodies protecting fundamental rights.\n\nConsistent with the risk-based approach in the AI Act, a multi-layered enforcement structure with different regimes applying to AI systems with varying risks is provided.  For high-risk AI systems, the AI Act mandates, firstly, postmarket monitoring obligations and, secondly, a requirement to report serious incidents.  The serious incident reporting obligations can also sometimes apply to deployers, who should therefore also be aware of them.\n\nThe marketing surveillance authorities can require operators to take all appropriate measures to ensure that AI systems do not present a risk and, where necessary, can demand the withdrawal of a product or AI system from the market.  Very significant fines for noncompliance with the terms of the AI Act can also be levied.\n\nFor general-purpose AI models, the European Commission has exclusive powers to supervise and enforce the obligations in the AI Act.\n\nThe governance structure in the AI Act provides for the setting up of new institutional bodies at both the EU level (the AI Office, the European AI Board, the Advisory Forum and the Scientific Panel) and national level (notifying authorities and market surveillance authorities) and the roles and competencies of each of them are outlined.  The coordination between these bodies will be key to the effective implementation and enforcement of the AI Act.\n\n<!-- image -->\n\nTopics addressed in this chapter are as follows:\n\n- Post-marketing obligations\n- Market surveillance authorities\n- Procedures for enforcement\n- Authorities protecting fundamental rights\n- General-purpose AI models\n- Penalties\n- Remedies for third parties\n- Governance\n\n## Post-marketing obligations\n\n## Post-market monitoring system for high-risk AI systems\n\nSince AI systems have the ability to adapt and continue to learn after they are placed on the market, it is important to monitor their performance once they are put on the market. Recital 155 explains that the aim of the postmarket monitoring system is to ensure that providers of high-risk AI systems can consider experience from use of the system, so as to ensure ongoing compliance and improvement of the system.\n\nProviders of high-risk AI systems must include a post-market monitoring plan as part of the technical documentation that they draw up before they put the system on the market (articles 72(3) and 11(1)). This plan must be in line with the European Commission template, to be adopted by 2 February 2026. The postmarketing obligations will ensure that any need to immediately apply any necessary corrective or preventative actions are identified (article 3(25)).\n\nArticle 72 provides that the post-market monitoring system (and the documentation of the system) must be proportionate to the nature of the AI technology and the risks of the systems. This system must actively and systematically collect, document, and analyse relevant data throughout the AI system's lifetime, so as to allow the provider to evaluate continuous compliance. The data could be provided by deployers, or\n\nby others (although sensitive operational data from law enforcement authority deployers is excluded).  Where relevant, the system should also include analysis of interactions with other AI systems, including devices and software.\n\nProviders of certain types of high-risk AI systems, who already have post-market monitoring systems in place, can integrate their obligations under the AI Act into those existing systems, provided this achieves an equivalent level of protection. This is the case for high-risk AI systems covered by Union harmonisation legislation listed in Section A of Annex I (i.e. including certain machinery, toys and medical devices). It's also the case for financial institutions who are subject to requirements under Union financial services law regarding their internal governance, arrangements or processes, where these institutions place on the market high-risk AI systems listed in Annex III point 5 (in particular, evaluation of creditworthiness or for risk assessment and pricing in relation to life and health insurance) (article 72(4)).\n\n## Reporting of information on serious incidents for high-risk AI systems\n\nProviders of high-risk AI systems must report 'serious incidents' and the provider's quality management system must contain procedures relating to this (article 17(1)(i)). Ordinarily, deployers of high-risk AI systems must report serious incidents to the provider. However, if the deployer cannot reach the provider, then the serious incident reporting obligations of article 73 apply directly to the deployer (article 26(5)). Accordingly, deployers should also be aware of these provisions.  The European Commission is to issue guidance for providers on incident reporting by 2 August 2025 and must keep this under regular review.\n\nSerious incidents are defined at article 3(49) and mean an incident or malfunctioning of an AI system that directly or indirectly causes:\n\n- death, or serious harm to a person's health;\n- serious and irreversible disruption to management or operation of critical infrastructure;\n- violation of Union laws protecting fundamental rights; or\n- serious harm to property or the environment.\n\nSerious incidents must be reported within set timelines, as set out below. If necessary, the provider or deployer may submit an initial report, which can be completed later (article 73(5)).\n\n| Situation                                                                                                                                            | Period                                                                                                                                                                                                     |\n|------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Widespread infringement Or Serious incident involving critical infrastructure                                                                        | Immediately                                                                                                                                                                                                |\n|                                                                                                                                                      | â‰¤ than 2 days after awareness of the incident                                                                                                                                                              |\n| Death of a person                                                                                                                                    | â‰¤ 10 days after awareness of the serious incident; or Immediately after establishing or suspecting a causal relationship between the serious incident and the AI system if earlier                         |\n| Other situations (i.e. serious harm to health, fundamental rights violations, serious harm to property or environment - unless these are widespread) | â‰¤ 15 days after awareness of the serious incident; or Immediately after the provider has established a causal link, or the reasonable likelihood of a link, between the AI system and the serious incident |\n\n<!-- image -->\n\nAfter reporting, the provider must promptly conduct necessary investigations, including a risk assessment and corrective actions.  The provider must not do anything that would alter the AI system in a way that may affect any subsequent evaluation of the cause of the incident before it has informed the competent authorities.\n\nReports of serious incidents have to be made to the market surveillance authorities of the Member States where the incident occurred (article 73(1)).  It follows that if a serious incident affects multiple Member States or affects multiple sectors so that there are multiple market surveillance authorities within a Member State, then multiple reports will need to be made.\n\nThe market surveillance authority must take appropriate measures (which can include withdrawal or recall of the product) within seven days of receiving the notification and must also immediately notify the European Commission of any serious incident, whether or not they have taken action (article 73(8/11)).\n\n## Non-high-risk AI systems\n\nAI systems relating to products that are not high-risk nevertheless must be safe when placed on the market or put into service.  Regulation (EU) 2023/988 on general product safety and Regulation (EU) 2019/1020 on market surveillance and compliance of products apply to all AI systems governed by the AI Act, but these two Regulations provide the safety net for nonhigh-risk products (recital 166 and article 74(1)).\n\nRegulation (EU) 2019/1020 requires all operators to inform the relevant market surveillance authority when they have reason to believe that a product presents a risk under article 3(19) (see definition below).  To the list of risks in article 3(19), the AI Act has added risks to fundamental rights of persons (article 79(1)).\n\n<!-- image -->\n\n' Product presenting a risk ' means a product having the potential to affect adversely health and safety of persons in general, health and safety in the workplace, protection of consumers, the environment, public security and other public interests, protected by the applicable Union harmonisation legislation, to a degree which goes beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the product concerned, including the duration of use and, where applicable, its putting into service, installation and maintenance requirements.\n\n## Market surveillance authorities\n\nMember States play a key role as the enforcement of the AI Act will often require a local presence. Member States must each designate at least one market surveillance authority and one, if there is more than one, of these authorities must be set as a single point of contact vis-Ã -vis the public and other counterparts at Member State and Union level.  The Member State shall notify the European Commission of the single point of contact and the European Commission will make a list of them available to the public (recital 153 and article 70(1/2)). The Member States have until 2 August 2025 to comply with these provisions (article 113(b)).\n\n## Which entities are to be designated market surveillance authorities?\n\nMember States have some flexibility in designating market surveillance authorities; they can either establish a new body dedicated to enforcing the AI Act or integrate the requirements of the AI Act into the framework of an existing body already responsible for market surveillance under the Union harmonisation laws listed in Section A of Annex I or the existing bodies regulating financial or credit institutions regulated by Union law (article 74(3/6/7)). However, for high-risk systems in the area of biometrics, law enforcement, migration, asylum and border control management and the administration of justice, Member States must designate either the national Data Protection Authority established by Regulation\n\n(EU) 2016/679 or the supervisory authority designated under Directive (EU) 2016/680 (article 74(8)).\n\nWhere AI systems relate to products already covered by the Union harmonisation legislation listed in Section A of Annex I and where such legal acts already provide for procedures ensuring an equivalent level of protection and having the same objective as the AI Act, the sectoral procedures shall apply instead of the national level enforcement procedures set out in articles 79 to 83 (see below under the heading 'Procedures for enforcement').\n\nIn this instance, dual reporting of serious incidents is not required and providers report under those other laws (article 73(9) and 73(10)). These exceptions specifically apply to:\n\n- Annex III-type high-risk AI systems, where the provider is subject to Union law that establishes reporting obligations equivalent to those set out in the AI Act. Such equivalence may - for example - exist for critical infrastructure, which is covered by cybersecurity regulations that contain standalone incident reporting obligations that might be considered equivalent to those under the AI Act. However, it may not always be clear whether reporting obligations under other Union laws are considered equivalent to the reporting obligations under the AI Act; and\n- high-risk AI systems that are safety components of devices, or are themselves devices, covered by Regulations (EU) 2017/745 on medical devices and (EU) 2017/746 on in vitro diagnostic medical devices. These both contain reporting obligations, according to which serious incidents must be reported to the competent authorities if they entail (a) the death of a patient, user or other person, (b) the temporary or permanent serious deterioration of a patient's, user's or other person's state of health, or (c) a serious public health threat.\n\nHowever, in both instances, if the infringement relates to a violation of fundamental rights, it must still be notified under the AI Act and the relevant market surveillance authority must inform the national fundamental rights authority/ authorities.\n\nFor AI systems used by Union institutions, agencies, offices, and bodies (with the exception of the Court of Justice of the European Union acting in its judicial capacity), the European Data Protection Supervisor will be the market surveillance authority (article 74(9)).\n\n<!-- image -->\n\n## Powers of the market surveillance authorities\n\nThe market surveillance authorities have all the broad enforcement powers set out in Regulation (EU) 2019/1020 in addition to further powers granted by the AI Act. For example, they have the power to:\n\n- make operators disclose relevant documents, data and information on compliance. The AI Act adds that providers of high-risk AI systems may be compelled to disclose:\n- -training, validation and testing data sets used for the development of high-risk AI systems, including, where appropriate and subject to security safeguards, through application programming interfaces (API) or other relevant technical means and tools enabling remote access (article 74(12)); and\n- -where the testing or auditing procedures and verifications based on the data and documentation provided by the provider have been exhausted or proved insufficient, the source code if it is necessary to assess the conformity of a high-risk AI system with the requirements set out in chapter III, Section 2 (article 74(13));\n- make unannounced on-site inspections and make test purchases (article 74(5));\n- conduct investigations (engaging with the European Commission where high-risk AI systems are found to present a serious risk across two or more Member States) (article 74(11));\n- require operators to take appropriate actions to bring instances of non-compliance to an end, both formal non-compliance (article 83) and to eliminate a risk (articles 79-82);\n- take appropriate measures where an operator fails to take corrective action or where the non-compliance persists, including withdrawal or recall (articles 73(8), 79-83); and\n- impose penalties (articles 99-101).\n\nThe market surveillance authorities shall also ensure that testing in real world conditions is in accordance with the AI Act (see Chapter 7). They have the power to require the provider or deployer to modify the testing or suspend or terminate it (article 76(3)).\n\n## Handling of confidential information\n\nAny information or documentation obtained by market surveillance authorities shall be treated in accordance with the confidentiality obligations set out in article 78.  The provisions in article 78 also apply to the European Commission, the authorities protecting fundamental rights and natural and legal persons involved in the application of the AI Act.  Such persons shall carry out their tasks in a manner which not only protects confidential information and trade secrets, but also protects intellectual property rights and the rights in source code, public and national security interests and classified information.\n\nThese provisions shall apply from 2 August 2025.\n\n## Procedures for enforcement\n\nAs already noted, the following procedures do not apply where there exists already harmonising legislation providing an equivalent level of protection and having the same objective as the AI Act.\n\n## AI systems presenting a risk (articles 79 and 81)\n\nWhere a market surveillance authority has sufficient reason to consider an AI system presents a risk (see definition above), it must carry out an evaluation as to whether the AI system is compliant with the AI Act.\n\nIf it does not comply, the market surveillance authority shall without undue delay notify the relevant operator and require them to take all appropriate corrective actions to bring the AI system into compliance or to withdraw the AI system from the market, or to recall it.  The market surveillance authority shall state how long the operator has to comply, but it will be no longer than 15 working days.\n\nIf operator does not take adequate corrective action by the end of the specified period, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system being made available on its national market or put into service, to withdraw the product or the standalone AI system from that market or to recall it.  The market surveillance authority must inform the operator of the grounds on which its decision is based.\n\n<!-- image -->\n\nWhere the non-compliance is not restricted to its national territory, the market surveillance authority shall inform the European Commission and the other Member States without undue delay of the results of the evaluation and of the actions which it has required the operator to take and the provisional measures which it has taken if the operator has not complied.\n\nThe provisional measures shall be deemed justified if no objection has been raised by either a market surveillance authority of a Member State or by the European Commission within three months (reduced to 30 days in the event of non-compliance with the prohibitions referred to in article 5). However, if objections are raised, the European Commission shall consult with the market surveillance authority and the operator or operators and, within six months (or 60 days for an article 5 issue), decide whether the provisional measure is justified.  If it is, all Member States shall ensure that they take appropriate restrictive measures in respect of the AI system concerned, such as requiring withdrawal from their market.  If it is not, the provisional measure will be withdrawn.\n\nThese provisions are without prejudice to the procedural rights of the operator set out in article 18 of Regulation (EU) 2019/1020, including the right to be heard.\n\n## AI systems classified by the provider as nonhigh-risk (article 80)\n\nIf the market surveillance authorities have sufficient reason to consider an AI system classified by the provider as non-high-risk under article 6(3) is indeed high-risk, it must carry out an evaluation.\n\nThe procedure to be followed is very much as described above, but article 80 specifically refers to the ability to fine the relevant provider.\n\nIn exercising their power to monitor the application of article 80, market surveillance authorities may take into account the information stored in the EU database of high-risk AI systems (see below under the heading 'Governance at Union Level: Role of the European Commission').\n\n## Compliant AI systems which present a risk (article 82)\n\nIf a market surveillance authority finds that a high-risk AI system complies with the AI Act, but it nevertheless presents a risk to the health or safety of persons, to fundamental rights, or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that it no longer does so.\n\n## Formal non-compliance (article 83)\n\nWhere a market surveillance authority finds that, for example, a CE marking has not been affixed where it should, no authorised representative has been appointed or technical documentation is not available, it shall require the relevant provider to correct the matter within a prescribed period.\n\nIf the non-compliance persists, then the market surveillance authority shall take appropriate and proportionate measures to restrict or prohibit the high-risk AI system being made available on the market or to ensure that it is recalled or withdrawn from the market without delay.\n\n## Authorities protecting fundamental rights\n\nIn addition to identifying market surveillance authorities, by 2 November 2024, each Member State must identify the public authorities or bodies supervising and enforcing the obligations under Union law protecting fundamental rights, including the right to non-discrimination, in relation to the use of high-risk AI systems referred to in Annex III and shall notify them to the European Commission.\n\nWhere market surveillance authorities identify risks to fundamental rights they must notify the relevant national public authority supervising their protection.\n\nThese bodies have the power to request and access any documentation created or maintained under the AI Act when access to that documentation is necessary for effectively fulfilling their mandates. The relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request and, where the documentation proves insufficient may request\n\n<!-- image -->\n\nthe market surveillance authority to organise testing of the high-risk AI system through technical means (article 77).\n\n## General-purpose AI models\n\nThe European Commission is the sole authority responsible for supervising and enforcing obligations on providers of general-purpose AI models. The rationale behind this is to benefit from centralised expertise and synergies at Union level (article 88).  In practice, however, the AI Office (see below under the heading 'Governance') will carry out all necessary actions to monitor the effective implementation of the AI Act with regard to general-purpose AI models, provided that the organisational powers of the European Commission and the division of competences between Member States and the Union are not affected.\n\nThe AI Office can investigate possible breaches of the rules by providers of general-purpose AI models on its own initiative, following the results of its monitoring activities, or at a request from market surveillance authorities.\n\nIt has the powers of a market surveillance authority for AI systems which are based on a general-purpose AI model, where the model and system are developed by the same provider.\n\nMarket surveillance authorities must cooperate with the AI Office to carry out compliance evaluations if a market surveillance authority considers that a general-purpose AI system (that can be used by deployers for at least one highrisk purpose) is non-compliant with the AI Act.\n\nMarket surveillance authorities can request the AI Office to provide information related to generalpurpose AI models, where the market surveillance authority is unable to access that information (and as a result is unable to conclude its investigation into a high-risk system) (article 75).\n\n## Penalties\n\nAny person, which fails to comply with the AI Act - whether a natural or legal person, a public authority or an EU or national institution - can be sanctioned for non-compliance. The provisions on penalties under the AI Act exceed even those provided for in the GDPR (which are up to EUR\n\n20,000,000 or 4% of annual worldwide turnover). The maximum fine was revised throughout the legislative process but was ultimately set at EUR 35,000,000 or 7% of annual worldwide turnover.\n\nFines can be imposed by national authorities, the European Data Protection Supervisor, or the European Commission.  The European Data\n\nProtection Supervisor can impose fines on Union institutions, agencies and bodies. The European Commission can impose fines on providers of general-purpose AI models.  National authorities can impose fines on other operators.\n\nThe AI Act has a tiered approach to penalties, as shown below.\n\n| Grounds of infringement                                                                                         | EU bodies                     | All other persons                                                                                                                                                                                                                                                  |\n|-----------------------------------------------------------------------------------------------------------------|-------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                                                                                                 | Penalties imposed by EDPS     | Penalties imposed by national authorities (unless GPAI models, in which case imposed by the European Commission).                                                                                                                                                  |\n|                                                                                                                 |                               | For sanctioned persons which are undertakings, the penalties are capped at the higher of the %-based amount or the figure below. If the undertaking is an SME, they are capped at the lower amount. For other sanctioned persons, the specified figure is the cap. |\n| Supplying incorrect, incomplete or misleading information to notified bodies or national competent authorities. | â‰¤ â‚¬750,000 (article 100(3))   | â‰¤ 1% total worldwide annual turnover in preceding year; or â‰¤ â‚¬7,500,000 (article 99(5))                                                                                                                                                                            |\n| Obligations relating to high-risk AI systems. Obligations relating to providers of general -purpose AI models.  | â‰¤ â‚¬750,000 (article 100(3))   | â‰¤ 3% of total worldwide annual turnover in preceding year; or â‰¤ â‚¬15,000,000 (article 99(4) for high-risk AI systems; article 101(1) for general-purpose AI models)                                                                                                 |\n| Obligations relating to prohibited practices.                                                                   | â‰¤ â‚¬1,500,000 (article 100(2)) | â‰¤ 7% of total worldwide annual turnover in preceding year; or â‰¤ â‚¬35,000,000 (article 99(3))                                                                                                                                                                        |\n\nCuriously, there appear to be no penalties for failure to comply with the AI literacy obligations at article 4.\n\n## Penalties and fines imposed by national authorities\n\nIt is the responsibility of Member States to provide for effective, proportionate, and dissuasive sanctions.  These measures may include both monetary and non-monetary\n\n<!-- image -->\n\nmeasures or warnings.  They must be notified to the European Commission by the date of entry into application (article 99(1/2)).\n\nPenalties are to be imposed on a case-by-case basis.  The competent national authority should consider all relevant circumstances of the specific situation, with due regard to the nature, gravity, and duration of the infringement and its consequences, as well as the size of the provider (article 99(7)).\n\nEnforcement at Member State level must be subject to appropriate procedural safeguards, including effective judicial remedies.\n\n## Fines on Union institutions, bodies, offices and agencies\n\nThe European Data Protection Supervisor has the power to impose fines on Union institutions, agencies and bodies.  Before adopting a decision on a fine, the EDPS should communicate its preliminary findings to the Union institution and give it an opportunity to be heard. The fine is not to affect the effective operation of the institution and the funds collected by the imposition of fines are to be contributed to the general budget of the EU.\n\n## Fines on providers of general-purpose AI models\n\nThe European Commission may impose fines on providers of general-purpose AI models for infringements (article 101).  Unlike the other provisions on penalties and fines in chapter XII, which apply from 2 August 2025, article 101 does not apply until 2 August 2026.\n\nThe European Commission will publish an implementing act with details on arrangements and procedural safeguards for proceedings.\n\nWhen imposing a fixed amount or periodic penalty payment, the European Commission should take due account of the nature, gravity and duration of the infringement, and the principles of proportionality and appropriateness. Before adopting a decision on a fine, the European Commission should communicate its preliminary findings to the provider of the general-purpose AI model and give it an opportunity to be heard. The imposition of a fine must be subject to appropriate procedural safeguards, including judicial review before the Court of Justice of the European Union.  The CJEU may cancel, reduce or increase the amount of a fine imposed.\n\n<!-- image -->\n\n## Remedies for third parties\n\n## Complaint to a market surveillance authority (article 85)\n\nUnion and Member State law already provide some effective remedies for natural and legal persons whose rights and freedoms are adversely affected by the use of AI systems. Notwithstanding, the AI Act introduces a new complaints mechanism.  It mandates that any natural or legal person may submit a complaint to the competent market surveillance authority if it has grounds for believing there has been an infringement of the AI Act.\n\nCompare: Under the GDPR, a data subject has the right to lodge a complaint with a supervisory authority about an alleged infringement if the data subject believes that the processing of personal data relating to him or her violates rights under the GDPR.\n\nIn contrast, a complaint lodged under the AI Act may concern not only an infringement of the rights of the complainant, but also compliance with the AI Act as a whole.  In addition, under the GDPR a remedy can be filed only by the data subjects; under the AI Act, a complaint can also be filed by a legal person.\n\n## Right to explanation of individual decisionmaking (article 86)\n\nUnder the AI Act, any affected person is entitled to receive 'clear and meaningful' explanations from the deployer concerning decisions made by high-risk AI systems (except for critical infrastructure systems).  These explanations must clarify the decision-making procedure used and the main elements of the decision made by the AI system (article 86).\n\nThe right can be invoked if:\n\n- a deployer's decision is mainly based on the output of high-risk AI systems; and\n- that decision has legal effects or similarly significant effects on an affected person that adversely affect his or her health, safety or fundamental rights.\n\nCompare: The right to an explanation under the AI Act aligns with a controller's obligation under the GDPR concerning automated decisionmaking processes (article 22 GDPR).  Under the GDPR, the controller must provide the data\n\nsubject with meaningful information on the logic and significance of the consequences of such processing.\n\nArticle 86 of the AI Act complements the data subject's right to an explanation under the GDPR; it is more specific to AI as it requires the deployer to explain the role of the AI system in the decision.  In addition, the AI Act grants this right to all affected persons who can also be legal persons.  National data protection authorities under the GDPR are still the competent authorities to enforce the controller's obligation to provide information when it comes to automated decision-making involving personal data processing, regardless of what authority is competent to enforce article 86 of the AI Act.\n\n## Protection for whistleblowers (article 87)\n\nDirective (EU) 2019/1937 on the protection of persons who report breaches of Union law applies to the reporting of infringements of the AI Act.\n\n## Downstream providers' complaint (article 89)\n\nThe AI Act enables complaints by downstream providers (deployers of general-purpose AI systems) about possible violations of the rules set out in the Act.\n\nComplaints can be made to the AI Office and must be well-substantiated. They should include at least:\n\n- details of the provider of the general-purpose AI model that is the subject of the complaint, and its point of contact;\n- a description of the relevant facts, together with the provisions that have been breached;\n- the reasons why the complainant believes there has been an infringement; and\n- any other information that the requesting downstream provider deems relevant, including, where appropriate, information gathered at its own initiative.\n\nThe possibility for downstream providers to make such complaints enables the AI Office to effectively oversee the enforcement of the AI Act.\n\n<!-- image -->\n\n## Governance\n\nThe governance structure has been established to coordinate and support the application of the AI Act. Its aim is to build capabilities at both Union and national levels, integrate stakeholders, and ensure trustworthy and constructive cooperation.\n\n## Governance at Union Level: role of the European Commission\n\nThe European Commission is tasked by the AI Act with many responsibilities including developing and implementing delegated acts, developing and publishing guidelines, setting standards and best practice and making binding decisions to implement the AI Act effectively.  In practice, these tasks will be carried out by the AI Office (part of the administrative structure of the Directorate-General for Communication Networks, Content and Technology) in its role of supporting the European Commission.\n\nOne of the tasks that the European Commission, in collaboration with the Member States, must perform is set out in chapter VIII of the AI Act. The European Commission must set up and maintain an EU database for high-risk AI systems referred to in article 6(2) and AI systems that are not considered as high-risk pursuant to article 6(3).  The database will contain:\n\n- the data listed in Sections A and B of Annex VIII entered into the EU database by the provider or the authorised representative; and\n- the data listed in Section C of Annex VIII entered into the EU database by the deployer who is, or who acts on behalf of, a public authority, agency or body.\n\nThe data will be available to the public (with the exception of data relating to AI systems in the areas of law enforcement, migration, asylum and boarder control management).\n\n## The supranational bodies set up by the AI Act\n\n| Role of the AI Office                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Actions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| The AI Office was established by the European Commission by its decision of 24 January 2024 (C/2024/1459). The AI Office's function is to oversee the advancements in AI models, including as regards general-purpose AI models, the interaction with the scientific community, and to play a key role in investigations and testing, enforcement and to have a global vocation (recital 5 of the decision). The AI Office may involve independent experts to carry out evaluations on its behalf. The AI Office must establish systems and procedures to manage and prevent potential conflicts of interest and must develop Union expertise and capabilities in the field of AI. The AI Office has a role in the surveillance and control of general-purpose AI systems (article 75). | Monitoring and enforcement: Monitor compliance and implementation of obligations for providers of general-purpose AI models. Investigation: Investigate infringements by requesting documentation and information, conducting evaluations and requesting measures from providers of general-purpose AI models. Risk management: Request appropriate measures, including risk mitigation, in cases of identified systemic risks, as well as restricting market availability, withdrawing or recalling the model. Coordination and support: Support national authorities in creating AI regulatory sandboxes and facilitate cooperation and information- sharing and encourage and facilitate the creation of codes of conduct. Coordinate joint investigations by market surveillance authorities and the European Commission. Advice: Issue recommendations and written opinions to the European Commission and the Board regarding codes of conduct, codes of practice and guidelines. |\n\n## Role of the European Artificial Intelligence Actions\n\n## Board (The Board)\n\nThe Board comprises representatives from each Member State and is tasked with advising and assisting the European Commission and the Member States on the consistent and effective application of the AI Act. Additionally, the Board issues guidelines and recommendations (articles 65 and 66).\n\nRepresentatives are appointed for a term of three years, renewable once. They may be individuals from public entities with expertise in AI and the authority to facilitate national-level coordination. The Board is chaired by one of its representatives.\n\n<!-- image -->\n\nCoordination and cooperation: Among national competent authorities and Union institutions, bodies, offices and agencies, as well as relevant Union expert groups and networks.\n\nExpertise sharing: Collect and share technical and regulatory expertise, best practices and guidance documents.\n\nAdvice and recommendations: Provide advice on the implementation of the AI Act, in particular as regards the enforcement of rules on general-purpose AI models, issue recommendations and written opinions (at the request of the European Commission or on its own initiative).\n\n## Role of the European Artificial Intelligence Actions\n\n## Board (The Board)\n\nThe Board must establish two dedicated standing subgroups:\n\n- The standing subgroup for notifying authorities provides a platform for cooperation and exchange on issues related to notified bodies\n- The standing subgroup for market surveillance acts as the administrative cooperation group (ADCO) for the AI Act.\n\nThe Board may establish other standing or temporary subgroups as appropriate for the purpose of examining specific issues.\n\nThe European Data Protection Supervisor and the AI Office attend the Board's meetings as observers. Other national and Union authorities, bodies, or experts or representatives of the advisory forum may be invited on a case-by-case basis.\n\n## Role of the Advisory Forum\n\nThe Advisory Forum has been created to ensure the involvement of stakeholders in the implementation and application of the AI Act (article 67).\n\nMembers are appointed by the European Commission and represent a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society, and academia with recognised expertise in the field of AI.\n\nMembers are appointed for a term of two years, which may be extended up to four years. They elect two co-chairs from among the members for a term of two years, renewable once.\n\nThe Fundamental Rights Agency (FRA), the European Union Agency for Cybersecurity (ENISA), the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC), and the European Telecommunications Standards Institute (ETSI) shall be permanent members of the Advisory Forum.\n\nThe Advisory Forum may establish standing or temporary sub-groups as appropriate for examining specific questions.\n\nThe Advisory Forum meets at least twice a year and may invite experts and other stakeholders to its meetings.\n\n<!-- image -->\n\nHarmonisation: Standardise administrative practices and facilitate the development of common criteria and a shared understanding.\n\nPublic awareness on AI: Work towards AI literacy, public awareness and understanding of the benefits, risks, safeguards and rights and obligations in relation to the use of AI systems.\n\nInternational Cooperation: Advise the European Commission in relation to international matters on AI and cooperate with competent authorities of third countries and with international organisations.\n\n## Actions\n\nAdvice and technical expertise: Provide advice to the Board and the European Commission. Prepare opinions, recommendations, and written contributions upon request.\n\nConsultancy group: The European Commission has to consult the Forum when preparing a standardisation request or drafting common specifications as referred to in article 41.\n\nAnnual report: Prepare and publish an annual report on its activities.\n\n| Role of the scientific panel of independent expert                                                                                                                                                                                       | Actions                                                                                                                                                                                           |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| The scientific panel is created to integrate the scientific community in supporting the European Commission's enforcement activities (article 68).                                                                                       | Support the AI Office in the implementation and enforcement as regards general-purpose AI models and system:                                                                                      |\n| Experts are selected by the European Commission based on their current scientific or technical expertise in AI.                                                                                                                          | â€¢ Develop tools and methodologies for evaluating capabilities.                                                                                                                                    |\n| The number of experts is determined by the European Commission, in consultation with the Board, based on the required expertise needs, ensuring fair gender and geographical representation.                                             | â€¢ Advise on the classification including systemic risk. â€¢ Contribute to the development of tools and templates.                                                                                   |\n| To provide the scientific panel with the necessary information for performing its tasks, a mechanism should be established allowing the panel to request the European Commission to obtain documentation or information from a provider. | â€¢ Support market surveillance authorities: At their request including with regard to cross- border market surveillance activities. â€¢ Assist in the Union safeguard procedure pursuant article 81. |\n| An implementing act will define how the scientific panel and its members can issue alerts and request assistance from the AI Office.                                                                                                     | Support Member States with their enforcement activities upon demand: â€¢ Member States may be required to pay fees for the advice and support provided by the scientific panel.                     |\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Governance at national level: national competent authorities\n\nMember States play a crucial role in the application and enforcement of the AI Act. To ensure effective application, harmonisation, and coordination within the Union and among\n\n## Role of the notifying authority(ies)\n\nThis authority is responsible for establishing and applying the framework for conformity assessment bodies (article 28).\n\nThe authority must have an adequate number of competent personnel with the necessary expertise in fields such as information technology, AI, and law, including the supervision of fundamental rights.\n\nNotifying authorities must avoid any conflict of interest with conformity assessment bodies, ensuring the objectivity and impartiality of their activities. In particular, the decision to notify  a conformity assessment body must not be made by the  person  who assessed  the conformity assessment body.\n\n<!-- image -->\n\nMember States, each Member State must designate at least one notifying authority and one market surveillance authority. Together, they constitute the national competent authorities.  For AI systems used by Union institutions, agencies, offices, and bodies, the European Data Protection Supervisor will be the supervisory authority.\n\n## Actions\n\nSetting up and carrying out procedures: Establish and execute necessary procedures for the assessment, designation, notification, and monitoring of conformity assessment bodies. Develop these procedures in cooperation with the notifying authorities of other Member States.\n\nAdvice and guidance: Provide guidance and advice on the implementation of the AI Act, considering the input from the Board and the European Commission, and consulting national competent authorities under other Union laws, if applicable.\n\nActivity and service restrictions:\n\n- Must not offer or provide any activities performed by conformity assessment bodies.\n- Must not offer consultancy services on a commercial or competitive basis.\n\n## Role of the market surveillance authority(ies)\n\nResponsible for carrying out the activities and taking the measures pursuant to Regulation (EU) 2019/1020 (market surveillance and compliance of products) on market surveillance and compliance of products.\n\nOne of the market surveillance authorities will be designated by each Member State as the single point of contact for the public and other counterparts at both Member State and Union levels.\n\nThe European Data Protection Supervisor will act as the market surveillance authority for Union institutions, agencies, and bodies under the AI Act.\n\nMarket surveillance authorities for highrisk AI systems in biometrics, used for law enforcement, migration, asylum, border control, justice, and democratic processes, should have strong investigative and corrective powers. This includes access to all personal data and necessary information for their task.\n\nMember States must facilitate coordination between market surveillance authorities and other relevant national authorities.\n\n<!-- image -->\n\n## Where can I find this?\n\nGovernance: Chapter VII\n\nEU Database: Chapter VIII\n\nEnforcement: Chapters IX and XII\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n4\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Actions\n\nMany of task and responsibilities of the market surveillance authorities are described above, but in addition they have the following tasks and responsibilities assigned to them:\n\n- Authorisation for high-risk AI systems: Member States can temporarily authorise specific high-risk AI systems to be placed on the market or put into service in their territory for exceptional reasons of public security, health, environmental protection, or key infrastructure, pending conformity assessments (article 46).\n- Annual reporting: to the European Commission and national competition authorities on surveillance activities and prohibited practices including: (i) any information identified that is of potential interest for the application of competition law; (ii) use of any prohibited practices; and (iii) measures taken in relation to those practices.\n- Advice and guidance: Provide guidance and advice on the implementation of the AI Act, considering the input from the Board and the European Commission, and consulting national competent authorities under other Union laws, if applicable.\n\nrecitals 148-154, 163,179\n\nrecital 131\n\nrecitals 162-164 and 168-172\n\n## AI Act: What's Next\n\n<!-- image -->\n\n- The AI Act entered into force on 1 August 2024.\n- Most provisions are set to apply from 2 August 2026, and others are being phased in over a period of six to 36 months from the date of entry into force.\n- The European Commission will develop delegated and implementing acts, guidelines, codes of conduct and standards. These initiatives are aimed at providing practical guidance, ethical principles and technical specifications related to the AI Act, with the goal of ensuring the effective implementation of the legislation.\n- The Commission also sent, in July 2024, an updated version of its proposed AI Liability Directive to both the European Parliament and the Council for consideration.\n- Bird &amp; Bird's AI experts are equipped to monitor the forthcoming initiatives expected under AI Act and help you navigate the different processes and requirements.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nAll actors dealing with AI systems should actively monitor the development of the legislative and non-legislative initiatives outlined in this chapter.\n\n## AI Act: What's Next\n\nThis chapter provides an overview of the application deadlines of the AI Act and the forthcoming initiatives expected under the Regulation. The EU institutions regard the AI Act as a new form of 'living regulation' that will be supplemented on an ongoing basis via secondary legislation and other initiatives, in an effort to keep pace with technological advances. Over the coming months, the AI Act envisions the adoption of a range of delegated and implementing acts, guidance documents, codes of conduct, codes of practice and standardisation requests. These initiatives are designed to provide practical guidance, ethical principles and technical specifications regarding the Regulation, with the aim of ensuring effective implementation.\n\nThe requirements laid down in such documents will greatly shape the effective implementation of the AI Act and the ability of actors to comply with its obligations.\n\nAll actors dealing with AI systems would therefore be advised to actively monitor the work of the Commission in developing the legislative and nonlegislative initiatives mentioned in this chapter.\n\nBird &amp; Bird's Regulatory and Public Affairs team is equipped to monitor the forthcoming initiatives expected under AI Act and help you navigate the different processes and requirements.\n\n## AI Act application deadlines\n\nFollowing its publication in the EU Official Journal 1 on 12 July 2024, the AI Act entered into force on 1 August 2024.\n\nThe relevant dates of application are set out below.\n\n| 12 July 2024    | The AI Act was published in the Official Journal of the EU, triggering the dates for specific provisions in the Regulation becoming applicable.                                                                                                      |\n|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2 February 2025 | Prohibited practices ban applies (Chapter II). AI literacy rules apply (article 4).                                                                                                                                                                  |\n| 2 May 2025      | Codes of practice for general-purpose AI must be ready (article 56 (9)).                                                                                                                                                                             |\n| 2 August 2025   | National authorities designated (Chapter III Section 4). Obligations for General-Purpose AI (GPAI) (Chapter V). Governance (at EU and national level) (Chapter VII). Confidentiality and penalties (other than in relation to gen-AI) (Chapter XII). |\n| 2 August 2026   | Start of application of all other provisions of the EU AI Act (unless a later date applies below).                                                                                                                                                   |\n\n<!-- image -->\n\n| 2 August 2027    | High-risk categories listed in Annex I). General purpose AI models placed on the market before 2 August 2025 (article 111).                                                                                |\n|------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2 August 2030    | High-risk AI systems (other than those listed below), which have been placed on the market or put into service before 2 August 2026 and which are intended to be used by public authorities (article 111). |\n| 31 December 2030 | Components of large-scale IT systems listed in Annex X, which have been placed on the market or put into service before 2 August 2027 (article 111).                                                       |\n\nBetween 1 August 2024 and 2 August 2027, the European Commission is expected to adopt various documents to implement the Regulation. These comprise delegated and implementing acts, guidance documents, codes of conduct, codes of practice and standardisation requests. Apart from a few exceptions, there are no specific set deadlines for the publication of these initiatives by the Commission. Nonetheless, it is assumed that the Commission will aim to adopt such documents ahead of the application deadlines of the respective provisions.\n\n## Delegated acts\n\nSeveral provisions will be the subject of delegated acts, to be adopted by the Commission to specify obligations and operational implementation. Article 97 grants the power to adopt delegated acts to the Commission for a five-year period that started on 1 August 2024. The Commission must report on this delegation nine months before the end of the period. This period is automatically extended for another five years unless the European Parliament or the Council opposes it three months before the end of each period.\n\nAs mentioned above, there are no specific set deadlines for the adoption of such delegated acts. However, it must be presumed that their adoption will precede the application deadlines for the related provisions in the AI Act (see article 113).\n\nPursuant to article 97(4), before adopting a delegated act, the Commission will have to carry out public consultations during its preparatory work and will also consult with the relevant Expert Groups (composed of Member States experts).\n\nOnce adopted, the Commission must notify the European Parliament and the Council simultaneously. A delegated act only enters into force if neither the European Parliament nor the Council objects within three months of notification, extendable by another three months if needed. The European Parliament or the Council can revoke this power at any time, but this will not affect the validity of existing delegated acts. In accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making 2 , the Commission will have to ensure that the European Parliament and the Council receive all documents at the same time as Member States' experts, and the Parliament and Council's experts should systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts.\n\nThe AI Act foresees the adoption of the following delegated acts where the Commission considers this to be necessary:\n\n- Article 6(6/7) : amend article 6(3) by adding new conditions to those laid down in paragraph 3, by modifying or by deleting them if there is concrete and reliable evidence of the existence of AI systems that should not fall under Annex III or that should not fall under the conditions of article 6(3);\n\n2.  Inter-institutional  Agreement between the European Parliament, the Council of the European Union and the European Commission on Better Law-Making, OJ L 123, 12.5.2016.\n\n<!-- image -->\n\n- Article 7(1/3) : amend Annex III, by adding, modifying or removing use-cases of high-risk AI systems;\n- Article 11(3) : amend Annex IV, where necessary, to ensure that, in light of technical progress, the technical documentation provides all the information necessary to assess the compliance of the system;\n- Article 43(5) : amend Annexes VI and VII by updating them in light of technical progress;\n- Article 45(6) : amend article 43(1/2) in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to third-party conformity assessments;\n- Article 47(5) : amend Annex V by updating the content of the EU declaration of conformity set out in that Annex, in order to introduce elements that become necessary in light of technical progress;\n- Article 51(3) : amend the thresholds for systemic general-purpose AI models listed in article 51(1/2) as well as to supplement benchmarks and indicators in light of evolving technological developments, such as algorithmic improvements or increased hardware efficiency, when necessary, for these thresholds to reflect the state of the art;\n- Article 52(4) : amend Annex XIII by specifying and updating the criteria for systemic general-purpose AI models;\n- Article 53(5) : detail measurement and calculation methodologies with a view to allowing for comparable and verifiable documentation to facilitate compliance with Annex XI; and\n- Article 53(6) : amend Annexes XI and XII in light of evolving technological development.\n\n## Implementing acts\n\nArticle 98(2) of the AI Act confers on the European Commission the power to adopt implementing acts in accordance with Regulation 182/2011 3 . Implementing acts aim to create uniform conditions for the implementation of a specific legislative act, if and when this is necessary. With respect to the drafting of the implementing acts, the Commission will be assisted by a 'Comitology' Committee comprising Member State experts.\n\nAs is the case for delegated acts, the timeline for adoption of the expected implementing acts is not specified in the text, except for the foreseen implementing act referred to in article 72(3), which is due by 2 February 2026. Therefore, it should be presumed that the relevant implementing acts will be adopted ahead of the application deadlines for the related provisions in the AI Act (see above and article 113).\n\nThe AI Act foresees the adoption of the following implementing acts, where the Commission deems it necessary to:\n\n- Article 37(2) : suspend, restrict or withdraw the designation of notified bodies when the Member State fails to take the necessary corrective measures;\n- Article 41(1/4/6) : establish, in consultation with the 'Advisory Forum' referred to in article 67, common specifications for the requirements for high-risk AI systems or for the obligations for general-purpose AI models set out in Chapter V, Sections 2 and 3. When a reference to a harmonised standard is published in the Official Journal of the European Union, which covers the same requirements set out in Section 2 of this Chapter III, the Commission shall repeal the implementing act referred to in article 41(1). Where a Member State considers that a common specification does not entirely meet the requirements set out in Section 2 of this Chapter III, the Commission shall assess that information and, if appropriate, amend the implementing act referred to in article 41(1);\n- Article 50(7) : approve codes of practice drawn up to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content, in accordance with the procedure laid down in article 56(6). If the code of practice is not adequate, the Commission may adopt an implementing act to lay down a set of common rules for the implementation of the transparency\n\n3.  Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 February 2011 laying down the rules and general principles concerning mechanisms for control by Member States of the Commission's exercise of implementing powers, OJ L 55, 28.2.2011.\n\n<!-- image -->\n\nobligations for providers and deployers of certain AI systems of article 50;\n\n- Article 56(6) : approve a code of practice for general-purpose AI models and give it a general validity within the Union. If, by 2 August 2025, a code of practice cannot be finalised, or if the AI Office deems it is not adequate, the Commission may provide, by means of implementing acts, common rules for the implementation of the obligations provided for in articles 53 and 55, including the issues set out in article 56(2);\n- Article 58(1) : specify the detailed arrangements for the establishment, development, implementation, operation and supervision of the AI regulatory sandboxes;\n- Article 60(1) : specify the detailed elements of the real-world testing plan for providers of high-risk AI systems;\n- Article 68(1) : make provisions on the establishment of a scientific panel of independent experts (the 'scientific panel' ) intended to support the enforcement activities of the AI Act;\n- Article 72(3) : publish, by 2 February 2026, an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan for providers of high-risk AI systems and the list of elements to be included in the plan;\n- Article 92(6) : set out the detailed arrangements and the conditions for the AI Office of general-purpose AI models evaluations, including the detailed arrangements for involving independent experts, and the procedure for the selection thereof; and\n- Article 101(6) : lay down detailed arrangements and procedural safeguards for proceedings in view of the possible fines on providers of general-purpose AI models.\n\n## Commission Guidelines\n\n'Commission Guidelines' are explanatory documents produced by the Commission services to provide practical and informal guidance about how particular provisions of the AI Act should be applied.\n\n<!-- image -->\n\nThe AI Act foresees the adoption of the following Commission Guidelines:\n\n- Article 6(5) : after consulting the European Artificial Intelligence Board, and no later than 2 February 2026, specifying the practical implementation of article 6, including a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk;\n- Article 63(1) : on the elements of the quality management system which may be complied with in a simplified manner considering the needs of microenterprises, without affecting the level of protection or the need for compliance with the requirements in respect of high-risk AI systems (no set deadline for these guidelines);\n- Article 73(7) : to facilitate compliance with the reporting obligations of serious incident. The guidance has to be adopted by 2 August 2025, and will have to be assessed regularly by the Commission;\n- Article 96 : on the practical implementation of this Regulation. There is no set deadline for the development of these guidelines. However, the related provisions apply from 2 August 2026. In particular, the Commission is to develop guidelines on:\n- -the application of the requirements and obligations referred to in articles 8 to 15 and in article 25;\n- -the prohibited practices referred to in article 5;\n- -the practical implementation of the provisions related to substantial modification;\n- -the practical implementation of transparency obligations laid down in article 50;\n- -detailed information on the relationship of the AI Act with the EU harmonisation legislation listed in Annex I, as well as with other relevant EU laws, including as regards consistency in their enforcement; and\n- -the application of the definition of an AI system as set out in article 3, point (1).\n\n## Codes of conduct and practice\n\n## Codes of conduct\n\nCodes of conduct are documents of a voluntary nature that establish ethical guidelines and principles for the development and use of AI in certain conditions. They are also intended to foster the development of AI policies within organisations for the voluntary application of specific AI Act obligations.\n\nThe AI Act calls for the adoption of the following codes of conduct:\n\n- Recital 20 and article 4 : voluntary codes of conduct to advance AI literacy among persons dealing with the development, operation and use of AI.\n- -While there is no set deadline for the development of voluntary codes of practice to advance AI literacy, the related provisions on AI literacy in Article 4 will apply from 2 February 2025.\n- Recital 165 and article 95 : codes of conduct intended to foster the voluntary application to AI systems of some or all the mandatory requirements applicable to high-risk AI systems. These are adapted in light of the intended purpose of the systems and the lower risk involved, and take into account the available technical solutions and industry best practices such as model and data cards:\n- -to ensure that the voluntary codes of conduct are effective, they should be based on clear objectives and key performance indicators to measure the achievement of those objectives;\n- -they should also be developed in an inclusive way, as appropriate, with the involvement of relevant stakeholders such as business and civil society organisations, academia, research organisations, trade unions and consumer protection organisations; and\n- -while there is no set deadline for the development of voluntary codes of practice intended to foster the application to AI systems of some or all the mandatory requirements applicable to high-risk AI systems, the related provisions included in Article 95 will apply from 2 February 2026. By 2 August 2028 and every three years thereafter, the Commission is due to\n\n<!-- image -->\n\nevaluate the impact and effectiveness of such voluntary codes of conduct.\n\n## Codes of practice\n\nCodes of practice represent a central tool for proper compliance with specific obligations under the AI Act. In particular, one code of practice will detail the AI Act rules for providers of general-purpose AI models and generalpurpose AI models with systemic risks. Another code of practice will focus on the detection and labelling of artificially generated or manipulated content. Organisations should be able to rely on codes of practice to demonstrate compliance with the relevant obligations, which is known as a 'presumption of conformity' .\n\nSpecifically, the AI Act calls on the European Commission's AI Office to facilitate the drawing up of the following codes of practice together with all interested stakeholders:\n\n- Article 50(7): codes of practice at EU level to facilitate the effective implementation of the obligations in article 50(2/4), regarding the detection and labelling of artificially generated or manipulated content. The Commission may adopt implementing acts to approve those codes of practice. While there is no set deadline for the development of voluntary codes of practice to facilitate the effective implementation of the obligations in article 50(2/4), the related provisions included in Article 50 will apply from 2 February 2026.\n- Article 56(1/3) : by 2 May 2025, codes of practice for general-purpose AI models. These will duly take into account international approaches as well as a diverse set of perspectives, by collaborating with relevant national competent authorities and, where appropriate, by consulting with civil society organisations and other relevant stakeholders and experts. These include the 'Scientific Panel' of independent experts established under the AI Act.\n\nBy 2 August 2028 and every three years thereafter, the Commission will have to evaluate the impact and effectiveness of voluntary codes of practice.\n\nOn 30 July 2024, the European AI Office opened a call for expressions of interest to participate in the drawing-up of the first general-purpose AI Code of Practice. Interested parties could express\n\ntheir interest in participating by 25 August 2024. According to the Commission, this Code will be prepared by means of an iterative drafting process by April 2025, nine months from the AI Act's entry into force on 1 August 2024. The Code of Practice will facilitate the proper application of the rules of the AI Act for general-purpose AI models.\n\nThe Commission may decide to approve the Code of Practice and give it a general validity within the European Union by means of an implementing act, pursuant to article 56(6).  If the Code of Practice is not deemed adequate, the Commission will provide common rules for the implementation of the relevant obligations.\n\nIn addition, on 30 July 2024, the AI Office launched a consultation on trustworthy generalpurpose AI models under the AI Act, specifically regarding the template for the summary of the content used for the training of the generalpurpose AI models and the accompanying guidance. The deadline for responses was 10 September 2024.\n\n## Standards\n\n## Initial standardisation work\n\nThe process of drafting European standards in support of the AI Act started well before the adoption of the AI Act, with the Commission's proposal on harmonised rules on artificial intelligence adopted as the Commission Implementing Decision C(2023)3215 on 22 May 2023.\n\nThis Implementing Decision requested the European Committee for Standardisation (CEN) and the European Committee for Electrotechnical Standardisation (CENELEC) to draft the following new European standards or European standardisation deliverables on AI by 30 April 2025:\n\n- European standard(s) and/or European standardisation deliverable(s) on risk management systems for AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on governance and quality of datasets used to build AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on record keeping through logging capabilities by AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on transparency and information provisions for users of AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on human oversight of AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on accuracy specifications for AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on robustness specifications for AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on cybersecurity specifications for AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on quality management systems for providers of AI systems, including post-market monitoring processes; and\n- European standard(s) and/or European standardisation deliverable(s) on conformity assessment for AI systems.\n\n<!-- image -->\n\nThis standardisation request to CEN and CENELEC was made pursuant to action 63 of the European Commission 2022 'Annual Union Work Programme for European standardisation' with the aim of ensuring that AI systems are safe and trustworthy.\n\nFor the drafting of these standards, CEN and CENELEC have set up a specific joint technical committee named 'CEN-CENELEC JTC 21 Artificial Intelligence' . CEN and CENELEC are also collaborating on the drafting with the European Telecommunications Standards Institute (ETSI) , an independent, not-for-profit, standardisation organisation in the field of information and communication.\n\n## AI Act standardisation request\n\nArticle 40(2) of the AI Act calls on the European Commission to present, without undue delay after the entry into force of the Regulation , standardisation requests for harmonised EU AI standards covering:\n\n- all requirements set out in Section 2 of Chapter III of the AI Act; and\n\n- as applicable, standardisation requests covering obligations set out in Chapter V, Sections 2 and 3 of the AI Act.\n\nThese requests revise the requests included in Commission Implementing Decision C(2023)3215 . This was also anticipated in the Commission's Standardisation Work Programme for 2024 published in February 2024. Indeed, Action 15 of the Work Programme calls for a 'revision of the standardisation request in support of Union policy on artificial intelligence' , thereby calling for the revision of the Commission Decision in view of the final AI Act text.\n\nAccording to article 40(2) of the AI Act, the standardisation requests should also ask for deliverables on reporting and documentation processes to improve AI systems' resource performance. Such requests could include reducing the consumption of energy and of other resources by high-risk AI systems during their lifecycle and the energy-efficient development of general-purpose AI models. The Commission should draft the requests after consulting with the European Artificial Intelligence Board and relevant stakeholders, including the Advisory Forum of stakeholders established under the AI Act.\n\nIn addition, when issuing standardisation requests to the relevant European standardisation organisations, the Commission should specify that standards have to be clear and consistent. This prerequisite includes standards developed in the various sectors for products covered by the existing EU harmonisation legislation listed in Annex I. They are aimed at ensuring that high-risk AI systems or general-purpose AI models placed on the market or put into service in the EU meet the relevant requirements or obligations laid down in the AI Act.\n\nBy 2 August 2028 and every four years thereafter, the Commission will have to submit a report to review the progress made regarding the development of standardisation deliverables on the energy-efficient development of general-purpose AI models. In this context, the Commission will also be required to assess the need for further measures or actions, including binding measures or actions. The report will have to be submitted to the European Parliament and to the Council and made public.\n\n<!-- image -->\n\n## Liability\n\n## Commission amends proposal to align with AI Act\n\nFinally, it is worth noting that at the end of July 2024, the European Commission sent an updated version of its proposal adapting non-contractual civil liability rules to artificial intelligence (AI Liability Directive or AILD) to both the European Parliament and the Council. This proposal, which was first tabled by the Commission in September 2022, aims to address the risks generated by specific uses of AI through a set of rules focusing on respect of fundamental rights and safety. The current changes are designed to align the AI Liability Directive proposal with the completed AI Act.\n\nIt is notable that the new proposal amends article 4 regarding the increased potential responsibility of companies deploying AI systems. These deployers would now be presumed liable for damage caused if they 'did not monitor the operation of the AI system or, where appropriate, suspend [its] use' or did not use 'sufficiently representative' input data.\n\nThe European Parliament's lead draftsperson ( 'rapporteur' ) for this file, the German Christiandemocratic MEP Axel Voss, had previously requested the European Parliamentary Research Service to conduct an 'alternative impact assessment' to evaluate whether the AILD is still necessary in view of adoption of the AI Act. While the future of the proposed AI Liability Directive remains uncertain, it may proceed in a reduced form.\n\n## AI Guide Contributors\n\nAs a market-leading law firm for technology, ranked Tier 1 for AI (first ranking of its kind within the European legal directory community) and TMT by Legal 500 in 12 jurisdictions and Band 1 for global multi-jurisdictional TMT by Chambers, we distinguish ourselves through our deep understanding of the technical intricacies involved in AI technology development and deployment. This expertise enables us to effectively collaborate with developers and commercial teams, speaking their language and asking the right questions from the outset. Our international AI group comprises over 120 experts , covering virtually every intersection where this transformative technology meets law and regulation. From handling groundbreaking IP litigation and guiding clients through complex regulatory changes to implementing effective governance frameworks and innovating commercial and contractual arrangements.\n\nIf you have any questions about the content, please get in touch with any of the contributors below or your usual Bird &amp; Bird contact. You can also find out more about the latest AI developments in our AI Hub\n\n## Belgium\n\nBenoit Van Asbroeck Of Counsel +3222826067 benoit.van.asbroeck@twobirds.com\n\n<!-- image -->\n\n## Finland\n\nTobias BrÃ¤utigam Partner +358962266758 tobias.brautigam@twobirds.com\n\n<!-- image -->\n\n## Germany\n\n<!-- image -->\n\nDr. Miriam Ballhausen Partner +4940460636000 miriam.ballhausen@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n3\n\nFrancine Cunningham Regulatory and Public Affairs Director +3222826056 francine.cunningham@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## France\n\n<!-- image -->\n\nAnne-Sophie Lampe Partner +33142686333 anne-sophie.lampe@twobirds.com\n\n<!-- image -->\n\nDr. Nils LÃ¶lfing Counsel +4921120056000 nils.loelfing@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nPaolo Sasdelli Regulatory and Public Affairs Advisor +3222826076 paolo.sasdelli@twobirds.com\n\nCen Zhang Senior Associate +33142686000 cen.zhang@twobirds.com\n\n<!-- image -->\n\nOliver Belitz Counsel +4969742226000 oliver.belitz@twobirds.com\n\n<!-- image -->\n\n## Germany\n\n<!-- image -->\n\nDr. Simon Hembt Senior Associate +4969742226000 simon.hembt@twobirds.com\n\nAleksandra Mizerska Lawyer +48225837900 aleksandra.mizerska@twobirds.com\n\n<!-- image -->\n\nDr. Maria Jurek Senior Associate +48225837839 maria.jurek@twobirds.com\n\n<!-- image -->\n\n## Spain\n\n<!-- image -->\n\nJoaquÃ­n MuÃ±oz Partner +34917906007 joaquin.munoz@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n3\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Italy\n\n<!-- image -->\n\nGian Marco Rinaldi Counsel +390230356071 gianmarco.rinaldi@twobirds.com\n\nAndrzej Stelmachowski Associate +48225837977 andrzej.stelmachowski @twobirds.com\n\n<!-- image -->\n\nMarta Kwiatkowska-Cylke Counsel +48225837964 marta.kwiatkowska-cylke@ twobirds.com\n\n<!-- image -->\n\n## The Netherlands\n\n<!-- image -->\n\nFeyo Sickinghe Of Counsel +31703538904 feyo.sickinghe@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Poland\n\n<!-- image -->\n\nAleksandra Cywinska Senior Associate +48225837875 aleksandra.cywinska@twobirds.com\n\nIzabela Kowalczuk-Pakula Partner +48225837932 izabela.kowalczuk-pakula@ twobirds.com\n\n<!-- image -->\n\nPawel Lipski Partner +48225837991 pawel.lipski@twobirds.com\n\n<!-- image -->\n\n## United Kingdom\n\nAlex Jameson Senior Associate +442078507139 alex.jameson@twobirds.com\n\n<!-- image -->\n\n## United Kingdom\n\n<!-- image -->\n\nIan Edwards Partner +442079056377 ian.edwards@twobirds.com\n\n<!-- image -->\n\nLiz McAuliffe Associate +442074156787 liz.mcauliffe@twobirds.com\n\n<!-- image -->\n\nToby Bond Partner +442074156718 toby.bond@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nKaterina Tassi Senior Associate +442074156066 katerina.tassi@twobirds.com\n\n<!-- image -->\n\nNora Santalu Associate +442079826513 nora.santalu@twobirds.com\n\n<!-- image -->\n\nWill Bryson Senior Associate +442074156746 will.bryson@twobirds.com\n\nKatharine Stephens Partner +442074156104 katharine.stephens@ twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nRuth Boardman Partner +442074156018 ruth.boardman@twobirds.com\n\n<!-- image -->\n\n## twobirds.com\n\nThe information given in this document concerning technical legal or professional subject matter is for guidance only and does not constitute legal or professional advice.  Always consult a suitably qualified lawyer on any specific legal problem or matter. Bird &amp; Bird assumes no responsibility for such information contained in this document and disclaims all liability in respect of such information.\n\nThis document is confidential.  Bird &amp; Bird is, unless otherwise stated, the owner of copyright of this document and its contents. No part of this document may be published, distributed, extracted, re-utilised, or reproduced in any material form.\n\nBird &amp; Bird is an international legal practice comprising Bird &amp; Bird LLP and its affiliated and associated businesses.\n\nBird &amp; Bird LLP is a limited liability partnership, registered in England and Wales with registered number OC340318 and is authorised and regulated by the Solicitors Regulation Authority (SRA) with SRA ID497264. Its registered office and principal place of business is at 12 New Fetter Lane, London EC4A 1JP. A list of members of Bird &amp; Bird LLP and of any non-members who are designated as partners, and of their respective professional qualifications, is open to inspection at that address.\n\n9\n\n10", "fetched_at_utc": "2026-02-09T13:45:39Z", "sha256": "778dc0538cbfa4cceab9b0e55c717e4c52dc5e046f973acda44dc46137e05dd8", "meta": {"file_name": "European Union Artificial Intelligence Act - Bird & Bird.pdf", "file_size": 1313072, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-fbpml-organisationbp-v1-0-0-17-30-11fbd2c46970", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_OrganisationBP_V1.0.0-17-30.pdf", "title": "FBPML_OrganisationBP_V1.0.0-17-30", "text": "## Section 4. Data Governance\n\n## 4. Data Governance\n\n## Objective\n\nTo ensure the integrity, normalisation, fairness and non-discrimination of Projet and/or Model data.\n\n|      |                            | Control:                                                                                                                                                                                                                                                                                                    | Aim:                                                                                                                                                                                                                                                  |\n|------|----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 4.1. | Data Governance Policy     | A Policy and Guide, which promotes good Data Governance in Product and Model design, development, and implementation ought to be derived by Data Science Managers and approved by the Managerial Committee. If a generic Data Governance Policy already exists, the above should be integrated accordingly. | To (a) ensure the integrity, normalisation, fairness and non- discrimination of Product and/or Model data; and (b) provide clear Organisation guidance to Products on how to warrant data integrity, normalisation, fairness and non- discrimination. |\n| 4.2. | Data Governance Procedures | A set of Procedures to operationalise the Data Governance Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and Product Lifecycle and Workflow Descriptions.                                                           | To ensure the Data Governance of Products and Models.                                                                                                                                                                                                 |\n\n## Section 5. Product and Model Oversight &amp; Management\n\n## 5.1 Fairness &amp; Non-Discrimination\n\n## Objective\n\nTo (a) identify possible risks for protected classes of persons, animals and the natural environment; and (b) minimise the unequal distribution of Products and Models errors to prevent reinforcing and/or deriving social inequalities and/or ills.\n\n|        |                               | Control:                                                                                                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                                                                                                                                                                                                                        |\n|--------|-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.1.1. | Fairness Policy               | A Policy and Guide, which promotes Product Fairness in - (a) Product Definitions and Product design, development, and implementation; (b) data processing; and (c) Model design, development, training, and output ought to be derived by Data Science Managers and approved by the Managerial Committee.       | To ensure the Fairness of Machine Learning. To provide clear Organisation guidance to Products on how to - (a) identify biases in Product data and Models; (b) take remedial action against identified biases; (c) identify and reduce asymmetric error rates between subpopulations; and (d) implement design and processes to avoid and circumvent risks that cannot be solved by purely technical means. |\n| 5.1.2. | Product Fairness Procedures   | A set of Procedures to operationalise the Fairness Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.                                                                  | To ensure the Fairness of Products and Models.                                                                                                                                                                                                                                                                                                                                                              |\n| 5.1.3. | Fairness Assessments          | Products should regularly complete Fairness assessments according to Product Lifecycle and Workflow Descriptions to the extent that is reasonably practical. Assessment findings ought to be documented by the Product Team and reviewed by Data Science Managers and, when relevant, the Management Committee. | To analyse, test, and report the risks identified with, and measures taken to ensure, Product Fairness and non-discrimination at regular intervals within the Product Lifecycle and Workflow Description.                                                                                                                                                                                                   |\n| 5.1.4. | Review of the Fairness Policy | The Fairness Policy should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                | To ensure that the Fairness Policy is kept up-to-date.                                                                                                                                                                                                                                                                                                                                                      |\n\n| 5.1.5.   | Review of Product Fairness Procedures   | Product Fairness Procedures should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.   | To ensure that Product Fairness Procedures are kept up-to-date.   |\n|----------|-----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------|\n\n## 5.2 Data Quality\n\n## Objective\n\nTo ensure Data Quality and prevent unintentional effects, changes and/or deviations in Products and Models outputs associated with poor Product data.\n\n|        |                                           | Control:                                                                                                                                                                                                                                                                                                                                                                                 | Aim:                                                                                                                                                                                                                                                                                      |\n|--------|-------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.2.1. | Data Quality Policy                       | A Policy and Guide, which describes the assessment and remediation of an Organisation's Data Quality. Chapters relating to (a) Product Definitions and Product design, development, and implementation; (b) data processing; and (c) Model design, development, training, and output ought to be derived and included by Data Science Managers and approved by the Managerial Committee. | To ensure the Data Quality of Machine Learning. To provide Product Teams with reliable guidance on how to - (a) identify Data Quality risks; (b) take remedial actions against identified Data Quality risks; and (c) take steps to account for Data Quality risks and associated issues. |\n| 5.2.2. | Product Data Quality Procedures           | A set of Procedures to operationalise the Data Quality Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions .                                                                                                                                      | To ensure the Data Quality of Products and Models.                                                                                                                                                                                                                                        |\n| 5.2.3. | Data Quality Assessments                  | Products should regularly complete Data Quality assessments according to Product Lifecycle and Workflow Descriptions to the extent that is reasonably practical. Assessment findings ought to be documented by the Product Team and reviewed by Data Science Managers and, when relevant, the Management Committee.                                                                      | To analyse, test, and report the risks identified with, and measures taken to ensure, Product Data Quality at regular intervals within the Product Lifecycle and Workflow Description.                                                                                                    |\n| 5.2.4. | Review of the Data Quality Policy         | The Data Quality Policy should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                                                                                     | To ensure that the Data Quality Policy is kept up-to-date.                                                                                                                                                                                                                                |\n| 5.2.5. | Review of Product Data Quality Procedures | Product Data Quality Procedures should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                                                                                                | To ensure that Product Data Quality Procedures are kept up-to-date.                                                                                                                                                                                                                       |\n\n## 5.3 Representativeness &amp; Specification\n\n## Objective\n\nTo (a) ensure that Product data and Models are representative of, and accurately specified for, target environments as far as is reasonably practical; and (b) guard against unintentional Products and Models behaviours and outputs as far as is reasonably practical.\n\n|        |                                                                 | Control:                                                                                                                                                                                                                                                                                                                                  | Aim:                                                                                                                                                                                                                                                                                  |\n|--------|-----------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.3.1. | Representativeness & Specification Policy                       | A Policy and Guide, which promotes Representativeness and Specification in - (a) Product Definitions and Product design, development, and implementation; (b) data processing; and (c) Model design, development, training, and output ought to be derived by Data Science Managers and approved by the Managerial Committee.             | To ensure the Representativeness and Specification of Machine Learning. To provide Product Teams with reliable guidance on how to - (a) identify Representativeness and Specification risks; (b) redress misspecification; and (c) remedy mis-, under- or over- representation risks. |\n| 5.3.2. | Product Representativeness & Specification Procedures           | A set of Procedures to operationalise the Representativeness & Specification Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.                                                                  | To ensure the Representativeness and Specification of Products and Models.                                                                                                                                                                                                            |\n| 5.3.3. | Representativeness & Specification Assessments                  | Products should regularly complete Representativeness & Specification assessments according to Product Lifecycle and Workflow Descriptions to the extent that is reasonably practical. Assessment findings ought to be documented by the Product Team and reviewed by Data Science Managers and, when relevant, the Management Committee. | To analyse, test, and report the risks identified with, and measures taken to ensure, Product Representativeness and Specification at regular intervals within the Product Lifecycle and Workflow Description.                                                                        |\n| 5.3.4. | Review of the Representativeness & Specification Policy         | The Representativeness & Specification Policy should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                | To ensure that the Representativeness & Specification Policy is kept up- to-date.                                                                                                                                                                                                     |\n| 5.3.5. | Review of Product Representativeness & Specification Procedures | Product Representativeness & Specification Procedures should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                           | To ensure that Representativeness & Specification Procedures are kept up-to-date.                                                                                                                                                                                                     |\n\n## 5.4 Performance Robustness\n\n## Objective\n\nTo warrant Model outcomes and prevent unintentional Model behaviour a priori under operational conditions as far as is reasonably practical.\n\n|        |                                                     | Control:                                                                                                                                                                                                                                                                                                                      | Aim:                                                                                                                                                                                                                                                                                                                      |\n|--------|-----------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.4.1. | Performance Robustness Policy                       | A Policy and Guide, which promotes Product Performance Robustness in - (a) Product Definitions and Product design, development, and implementation; (b) data processing; and (c) Model design, development, training, and output ought to be derived by Data Science Managers and approved by the Managerial Committee.       | To ensure the Performance and Robustness of Machine Learning. To provide Product Teams with reliable guidance on how to - (a) test, control, and improve Performance Robustness under operational conditions before going live; and (b) assess and control Performance Robustness risks concerning unexpected conditions. |\n| 5.4.2. | Product Performance Robustness Procedures           | A set of Procedures to operationalise the Performance Robustness Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions .                                                                 | To ensure the Performance Robustness of Products and Models.                                                                                                                                                                                                                                                              |\n| 5.4.3. | Performance Robustness Assessments                  | Products should regularly complete Performance Robustness Assessments according to Product Lifecycle and Workflow Descriptions to the extent that is reasonably practical. Assessment findings ought to be documented by the Product Team and reviewed by Data Science Managers and, when relevant, the Management Committee. | To analyse, test, and report the risks identified with, and measures taken to ensure, Product Performance Robustness at regular intervals within the Product Lifecycle and Workflow Description.                                                                                                                          |\n| 5.4.4. | Review of the Performance Robustness Policy         | The Performance Robustness Policy should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                | To ensure that the Performance Robustness Policy is kept up-to- date.                                                                                                                                                                                                                                                     |\n| 5.4.5. | Review of Product Performance Robustness Procedures | Product Performance Robustness Procedures should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                           | To ensure that Performance Robustness Procedures are kept up-to-date.                                                                                                                                                                                                                                                     |\n\n## 5.5 Monitoring &amp; Maintenance\n\n## Objective\n\nTo ensure that Products and Models remain within acceptable operational bounds.\n\n|        |                                                       | Control:                                                                                                                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                                                                                                                                                                                                                 |\n|--------|-------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.5.1. | Monitoring & Maintenance Policy                       | A Policy and Guide, which promotes Product monitoring and maintenance in - (a) Product Definitions and Product design, development, and implementation; (b) data processing; and (c) Model design, development, training, and output ought to be derived by Data Science Managers and approved by the Managerial Committee.     | To ensure the monitoring and maintenance of Machine Learning. To provide Product Teams with reliable guidance on how to - (a) define, monitor and maintain acceptable operating bounds, including, inter alia, guarding against model drift; (b) define and review alert conditions and severity; and (c) create scenario playbooks regarding responsibility, escalation, roll- back and resolution. |\n| 5.5.2. | Product Monitoring & Maintenance Procedures           | A set of Procedures to operationalise the Monitoring & Maintenance Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.                                                                  | To ensure the monitoring and maintenance of Products and Models.                                                                                                                                                                                                                                                                                                                                     |\n| 5.5.3. | Monitoring & Maintenance Assessments                  | Products should regularly complete Monitoring & Maintenance assessments according to Product Lifecycle and Workflow Descriptions to the extent that is reasonably practical. Assessment findings ought to be documented by the Product Team and reviewed by Data Science Managers and, when relevant, the Management Committee. | To analyse, test, and report the risks identified with, and measures taken to ensure, Product monitoring and maintenance at regular intervals within the Product Lifecycle and Workflow Description.                                                                                                                                                                                                 |\n| 5.5.4. | Review of the Monitoring & Maintenance Policy         | The Monitoring & Maintenance Policy should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                | To ensure that the Monitoring & Maintenance Policy is kept up-to- date.                                                                                                                                                                                                                                                                                                                              |\n| 5.5.5. | Review of Product Monitoring & Maintenance Procedures | Product Monitoring & Maintenance Procedures should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                           | To ensure that Monitoring & Maintenance Procedures are kept up-to-date.                                                                                                                                                                                                                                                                                                                              |\n\n## 5.6 Explainability\n\n## Objective\n\nTo ensure Products and Models functions and outputs are explainable and justifiable as far as is practically reasonable.\n\n|        |                                             | Control:                                                                                                                                                                                                                                                                                                              | Aim:                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n|--------|---------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.6.1. | Explainability Policy                       | A Policy and Guide, which promotes Product Explainability in - (a) Product Definitions and Product design, development, and implementation; (b) data processing; and (c) Model design, development, training, and output ought to be derived by Data Science Managers and approved by the Managerial Committee.       | To ensure the Explainability of Machine Learning. To provide clear Organisation guidance to Products on how to - (a) ensure the justifiability of individual Model predictions and decisions; (b) maintain and promote Product and Model inner workings amongst Product Teams, Business Stakeholders, Organisation Stakeholders and end-consumers; and (c) provide Model transparency for authorities, Special Interest Groups and/or the Public. |\n| 5.6.2. | Product Explainability Procedures           | A set of Procedures to operationalise the Explainability Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.                                                                  | To ensure the Explainability of Products and Models.                                                                                                                                                                                                                                                                                                                                                                                              |\n| 5.6.3. | Explainability Assessments                  | Products should regularly complete Explainability assessments according to Product Lifecycle and Workflow Descriptions to the extent that is reasonably practical. Assessment findings ought to be documented by the Product Team and reviewed by Data Science Managers and, when relevant, the Management Committee. | To analyse, test, and report the risks identified with, and measures taken to ensure, Product Explainability at regular intervals within the Product Lifecycle and Workflow Description.                                                                                                                                                                                                                                                          |\n| 5.6.4. | Review of the Explainability Policy         | The Explainability Policy should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                | To ensure that the Explainability Policy is kept up-to-date.                                                                                                                                                                                                                                                                                                                                                                                      |\n| 5.6.5. | Review of Product Explainability Procedures | Product Explainability Procedures should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                           | To ensure that Product Explainability Procedures are kept up-to-date.                                                                                                                                                                                                                                                                                                                                                                             |\n\n## 5.7 Safety &amp; Security\n\n## Objective\n\nTo  (a) prevent adversarial actions against, and encourage graceful failures for, Products and/or Models; (b) avert malicious extraction of Models, data and/or intellectual property; (c) prevent Model based physical or irreparable harms; and (d) prevent erosion of trust in outputs or methods.\n\n|        |                                     | Control:                                                                                                                                                                                                                                                                                                           | Aim:                                                                                                                                                                                |\n|--------|-------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.7.1. | Safety & Security Policy            | A Policy and Guide, which promotes Product Safety & Security in - (a) Product Definitions and Product design, development, and implementation; (b) data processing; and (c) Model design, development, training, and output ought to be derived by Data Science Managers and approved by the Managerial Committee. | To ensure the Safety of Machine Learning. To provide clear Organisation guidance to Products on how to identify and guard against Product vulnerabilities from Adversarial Actions. |\n| 5.7.2. | Product Safety Procedures           | A set of Procedures to operationalise the Safety Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions .                                                                      | To ensure the Safety of Products and Models.                                                                                                                                        |\n| 5.7.3. | Safety Assessments                  | Products should regularly complete Safety assessments according to Product Lifecycle and Workflow Descriptions to the extent that is reasonably practical. Assessment findings ought to be documented by the Product Team and reviewed by Data Science Managers and, when relevant, the Management Committee.      | To analyse, test, and report the risks identified with, and measures taken to ensure, Product Safety at regular intervals within the Product Lifecycle and Workflow Description.    |\n| 5.7.4. | Review of the Safety Policy         | The Safety Policy should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                     | To ensure that the Safety Policy is kept up-to-date.                                                                                                                                |\n| 5.7.5. | Review of Product Safety Procedures | Product Safety Procedures should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                                | To ensure that Product Safety Procedures are kept up-to-date.                                                                                                                       |\n\n## 5.8 Human-Centric Design &amp; Redress\n\n## Objective\n\nTo ensure (a) building desirable solutions; (b) human control over Products and Models; and (c) that individuals affected by Products and Models outputs can obtain redress.\n\n|        |                                                             | Control:                                                                                                                                                                                                                                                                                                                              | Aim:                                                                                                                                                                                                                                                                                                                          |\n|--------|-------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.8.1. | Human-centric Design & Redress Policy                       | A Policy and Guide, which promotes Product Human-Centric Design & Redress in - (a) Product Definitions and Product design, development, and implementation; (b) data processing; and (c) Model design, development, training, and output ought to be derived by Data Science Managers and approved by the Managerial Committee.       | To ensure the Human-Centric Design & Redress of Machine Learning. To provide clear Organisation guidance to Products on how to - (a) adds value and desirability of your product for end users; (b) assess and implement human- in-control requirements; and (c) assess and implement human- centric remediation and redress. |\n| 5.8.2. | Product Human- Centric Design & Redress Procedures          | A set of Procedures to operationalise the Human-Centric Design & Redress Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.                                                                  | To ensure the Human-Centric Design & Redress of Products and Models.                                                                                                                                                                                                                                                          |\n| 5.8.3. | Human-Centric Design & Redress Assessments                  | Products should regularly complete Human-Centric Design & Redress assessments according to Product Lifecycle and Workflow Descriptions to the extent that is reasonably practical. Assessment findings ought to be documented by the Product Team and reviewed by Data Science Managers and, when relevant, the Management Committee. | To analyse, test, and report the risks identified with, and measures taken to ensure, Product Human-Centric Design & Redress at regular intervals within the Product Lifecycle and Workflow Description.                                                                                                                      |\n| 5.8.4. | Review of the Human-Centric Design & Redress Policy         | The Human-Centric Design Policy & Redress should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                | To ensure that the Human-Centric Design & Redress Policy is kept up-to-date.                                                                                                                                                                                                                                                  |\n| 5.8.5. | Review of Product Human-Centric Design & Redress Procedures | Product Human-centric Design & Redress Procedures should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                           | To ensure that Product Human- Centric Design & Redress Procedures are kept up-to-date.                                                                                                                                                                                                                                        |\n\n## 5.9 Systemic Stability\n\n## Objective\n\nTo prevent (in)direct adverse social and environmental effects as a consequence of interactions amongst Products, Models, the Organisation, and the Public.\n\n|        |                                         | Control:                                                                                                                                                                                                                                                                                                                                            | Aim:                                                                                                                                                                                                                                                                                                      |\n|--------|-----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.9.1. | Systemic Stability Policy               | A Policy and Guide, which promotes awareness and control of systemic effects and interactions in - (a) Product Definitions and Product design, development, and implementation; (b) data processing; and (c) Model design, development, training, and output ought to be derived by Data Science Managers and approved by the Managerial Committee. | To ensure the Systemic Stability of Machine Learning. To provide clear Organisation guidance to Products on how to identify, analyse and prevent risks derived from (higher-order and/or highly complex) relations between Products, Models, Product design, Organisation processes and society at large. |\n| 5.9.2. | Product Systemic Stability Procedures   | A set of Procedures to operationalise the Systemic Stability Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.                                                                                            | To ensure the Systemic Stability of Products.                                                                                                                                                                                                                                                             |\n| 5.9.3. | Systemic Stability Assessments          | Products should regularly complete Systemic Stability according to Product Lifecycle and Workflow Descriptions to the extent that is reasonably practical. Assessment findings ought to be documented by the Product Team and reviewed by Data Science Managers and, when relevant, the Management Committee.                                       | To analyse, test, and report the risks identified with, and measures taken to ensure, Product Systemic Stability at regular intervals within the Product Lifecycle and Workflow Description.                                                                                                              |\n| 5.9.4. | Review of the Systemic Stability Policy | Systemic Stability should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                                                     | To ensure that the Systemic Stability Policy is kept up-to-date.                                                                                                                                                                                                                                          |\n| 5.9.5. | Review of Systemic Stability Procedures | Product Systemic Stability should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                                                                | To ensure that Product Systemic Stability Procedures are kept up- to-date.                                                                                                                                                                                                                                |\n\n## 5.10 Product Traceability\n\nObjective\n\nTo ensure the clear and complete Traceability of Products, Models and their assets (inclusive of, inter alia, data, code, artifacts, output, and documentation) for as long as is reasonably practical.\n\n|         |                                           | Control:                                                                                                                                                                                                                                                                                                                    | Aim:                                                                                                                                              |\n|---------|-------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.10.1. | Product Traceability Policy               | A Policy and Guide, which promotes Product Traceability during and after - (a) Product Definitions and Product design, development, and implementation; (b) data processing; and (c) Model design, development, training, and output ought to be derived by Data Science Managers and approved by the Managerial Committee. | To ensure the Product Traceability of Machine Learning. To provide clear Organisation guidance to Products on how to manage Product Traceability. |\n| 5.10.2. | Product Traceability Procedures           | A set of Procedures to operationalise the Traceability Policy should be developed and implemented within Products in light of Product Definitions, the Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.                                                                          | To ensure the Product Traceability of Products.                                                                                                   |\n| 5.10.3. | Review of the Product Traceability Policy | The Product Traceability should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                       | To ensure that the Product Traceability Policy is kept up-to- date.                                                                               |\n| 5.10.4. | Review of Product Traceability Procedures | Product Product Traceability should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                                      | To ensure that Product Product Traceability Procedures are kept up-to-date.                                                                       |\n\n## 5.11 Product Decision-Making\n\nObjective\n\nTo ensure that Product decision-making is done in a clear, informed, unbiased and collaborative manner with a diversity of Organisation opinions, when relevant.\n\n|         |                                | Control:                                                                                                                                                                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                                                                                                                    |\n|---------|--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.11.1. | Product Decision-Making Policy | A Policy which promotes Product decision- making clarity in - (a) Product Definitions and Product design, development, and implementation; (b) data processing; (c) Model design, development, training, and output; and (d) consideration for the Product Risk Classification Portfolio ought to be derived by Data Science Managers and approved by the Managerial Committee. | To ensure Product decisions - (a) are based on all available information, inclusive of those derived from Policies and Procedures of this document; (b) follow from and are aligned with the Product Lifecycle and Workflow Description; and (c) are made with reasonable care to avoid cognitive bias. |\n\n| 5.11.2.   | Product Decision-Making Procedures    | A set of Procedures to operationalise the Decision-Making Policy within Products should be developed and implemented, inclusive of consultations with Business Stakeholders.          | To ensure clear, informed, unbiased and collaborative decision-making in Products.                               |\n|-----------|---------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|\n| 5.11.3.   | Product Decision-Making Diversity     | A diversity of Organisation stakeholder and Business Stakeholder opinions and input should be obtained, considered and, when relevant, weighed when making material Product decisions | To ensure a diversity of opinions when making material Product decisions.                                        |\n| 5.11.4.   | Product Decision-Making Documentation | Product decisions should be documented, indexed, stored and, when relevant, reviewed.                                                                                                 | To ensure that Product decisions are documented and indexed to warrant their effective management and oversight. |\n\n## 5.12 Product Capabilities\n\n## Objective\n\nTo ensure that Products have sufficient capacity and capabilities to meet Product Definitions.\n\n|         |                               | Control:                                                                                                                                                                                         | Aim:                                                                                              |\n|---------|-------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n| 5.12.1. | Financial Resource Allocation | Sufficient financial resources ought to be allocated to Products based on their Product Definitions, Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions. | To ensure that Products have sufficient financial resources to allow for their success.           |\n| 5.12.2. | Human Resource Allocation     | Sufficient human resources ought to be allocated to Products based on their Product Definitions, Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.     | To ensure that Products have sufficient human resources to allow for their success.               |\n| 5.12.3. | Assets Allocation             | Sufficient Assets ought to be allocated to Products based on their Product Definitions, Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.              | To ensure that Products have sufficient Assets to allow for their success.                        |\n| 5.12.4. | Software Allocation           | Sufficient Software ought to be allocated to Products based on their Product Definitions, Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.            | To ensure that Products have sufficient Software to allow for their success.                      |\n| 5.12.5. | Knowledge & Development       | Product Teams should receive sufficient training and development based on the Product Definitions, Product Risk Classification Portfolio, and the Product Lifecycle and Workflow Descriptions.   | To ensure that Product Teams have sufficient training and development to allow for their success. |\n\n| 5.12.6.   | Review of Product Capabilities   | Product resource allocation ought to be periodically reviewed, or if significant changes occur, by Data Science Managers, in consultation with Product Owners, to ensure their continued effectiveness, suitability, and accuracy.   | To ensure that Products resources are sufficiently maintained.   |\n|-----------|----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------|\n\n## 5.13 Product Record\n\n## Objective\n\nTo promote the documentation and recording of Product, Product Team and employees tasks, deliverables and progress.\n\n|         |                                         | Control:                                                                                                                                                                    | Aim:                                                                                                           |\n|---------|-----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n| 5.13.1. | Product Record                          | A clear and detailed Product record should be continually kept of Product design, development, implementation, deliverables, progress and employee tasks.                   | To ensure that a clear Product record is kept to warrant effective oversight, management and accountability.   |\n| 5.13.2. | Employee and Product Team Documentation | Employee and Product Team processes ought to promote the documentation of Product tasks, discussions and deliverables when relevant and as much as is reasonably practical. | To ensure that sufficient documentation is kept to warrant effective oversight, management and accountability. |\n| 5.13.3. | Log of User Access                      | A formal log of user access rights to Products should be maintained and reviewed at regular intervals by Product Owners.                                                    | To ensure the management, integrity and review of user access.                                                 |\n| 5.13.4. | Event Logs                              | Event logs of Product user activities, exceptions, and faults should be produced, kept and regularly reviewed by Product Owners.                                            | To formally index and manage user activities to maintain Product oversight and integrity.                      |\n\n## Section 6. Product Validation\n\n## Objective\n\nTo autonomously and impartially validate Products, Models  and their outputs.\n\n|      |                                         | Control:                                                                                                                                                                                         | Aim:                                                                             |\n|------|-----------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|\n| 6.1. | Validation Department                   | The Organisation ought to have a department that is independent of Data Science Managers and Product Teams who validate Products.                                                                | To validate Product compliance, ethics, and performance.                         |\n| 6.2. | Validation Policy                       | A Policy detailing the responsibilities of the Validation Department and the requirements for Products to be Validated ought to be derived by the Managerial Committee.                          | To ensure the validity of Organisation Machine Learning.                         |\n| 6.3. | Validation Procedures                   | The Validation Department should develop and implement a set of Procedures to operationalise the Validation Policy within the Organisation.                                                      | To validate Product compliance, Ethics, and performance within the Organisation. |\n| 6.4. | Review of the Validation Policy         | The Validation Policy should be reviewed periodically, or if significant changes occur, by the Managerial Committee to ensure its continued effectiveness, suitability, and accuracy.            | To ensure that the Validation Policy is kept up-to-date.                         |\n| 6.5. | Review of Product Validation Procedures | Product Validation Procedures should be reviewed periodically, or if significant changes occur, by the Validation Department to ensure their continued effectiveness, suitability, and accuracy. | To ensure that Product Validation Procedures are kept up-to-date.                |", "fetched_at_utc": "2026-02-09T13:46:30Z", "sha256": "11fbd2c469700df371d8eb123766533dad83aa01e3ea7caffdc56d50c0887a81", "meta": {"file_name": "FBPML_OrganisationBP_V1.0.0-17-30.pdf", "file_size": 378097, "mtime": 1770576227, "docling_errors": []}}
{"doc_id": "pdf-pdfs-fbpml-organisationbp-v1-0-0-38-40-bca994bd46db", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_OrganisationBP_V1.0.0-38-40.pdf", "title": "FBPML_OrganisationBP_V1.0.0-38-40", "text": "## Section 11. Third Party Contracts Management\n\n## Objective\n\nTo ensure the integrity and implementation of Policies and Procedures in third-party contracts.\n\n|       |                                                          | Control:                                                                                                                                              | Aim:                                                                                 |\n|-------|----------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|\n| 11.1. | Policies and Procedures for Third-Party Contracts        | Procedures ought to be designed and implemented to ensure that Policies and Procedures are legally enforceable in relevant third-party contracts.     | To ensure that Policies and Procedures are implemented by third-party contractors.   |\n| 11.2. | Compliance in Third-Party Contracts                      | Procedures should be implemented to ensure that Policies and Procedures are complied with to agreed-upon standards in relevant third-party contracts. | To ensure that Policies and Procedures are complied with by third-party contractors. |\n| 11.3. | Monitoring, Review and Auditing of Third-Party Contracts | The Organisation should regularly monitor, review and audit third party contracts to warrant third-party compliance with Policies and Procedures.     | To ensure effective oversight of third-party contractors.                            |\n\n## Section 12. Ethics &amp; Transparency Management\n\n## Objective\n\nTo ensure that Products are transparent and ethical.\n\n|       |                                   | Control:                                                                                                                                                                                                                                                                                                         | Aim:                                                                                                                                                                   |\n|-------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 12.1. | Ethics Policy                     | An ethics policy, which promotes Ethical Practices in Machine Learning, ought to be derived by Data Science Managers and approved by the Management Committee and Ethics Committee. The Ethics Policy must be communicated to the Public.                                                                        | To ensure that Machine Learning is designed, developed and implemented in accordance with the Ethical Practices.                                                       |\n| 12.2. | Review of the Ethics Policy       | The Ethics Policy should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                                   | To ensure that the Ethics Policy is kept up-to-date.                                                                                                                   |\n| 12.3. | Transparency Policy               | A Public Interest and Transparency Policy, which promotes Public engagement, regulator engagement, and Transparency in Machine Learning, ought to be derived by Data Science Managers and approved by the Management Committee and Ethics Committee. The Transparency Policy must be communicated to the Public. | To ensure that Machine Learning is made transparent to the Public and is designed, developed and implemented in accordance with the Public Interest.                   |\n| 12.4. | Review of the Transparency Policy | The Transparency Policy should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                             | To ensure that the Transparency Policy is kept up-to-date.                                                                                                             |\n| 12.5. | Speaking-Out Policy               | A policy, which promotes Product Teams and/ or Product Teams members to speak-out against unethical practices, ought to be derived by Data Science Managers and approved by the Management Committee and Ethics Committee.                                                                                       | To ensure that Product Teams and/or Product Teams members have a safe space to voice their concerns about Machine Learning and/or Products practices and/or decisions. |\n| 12.6. | Review of the Speaking-Out Policy | The Speaking-Out Policy should be reviewed periodically, or if significant changes occur, by Data Science Managers to ensure its continued effectiveness, suitability, and accuracy.                                                                                                                             | To ensure that the Speaking- Out Policy is kept up-to-date.                                                                                                            |\n| 12.7. | Contact with Authorities          | Appropriate contact with relevant sector authorities regarding Products, and their implementation, should be maintained. Products and Product Teams ought to work in close collaboration with relevant sector authorities in a collaborative and bona fide manner.                                               | To ensure awareness and oversight of Products by authorities.                                                                                                          |\n\n## Section 13. Compliance, Auditing &amp; Legal Management and Oversight\n\n## Objective\n\nTo ensure that Policies and Procedures are designed, developed and implemented in accordance with the law, industry best practices, contractual obligations, and/or regulatory Guidelines.\n\n|       |                                                                          | Control:                                                                                                                                                                   | Aim:                                                                                                                |\n|-------|--------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|\n| 13.1. | Identification of Laws, Regulations and Contracts                        | Relevant laws, regulations and contracts concerning Machine Learning ought to be identified and documented.                                                                | To ensure that relevant Machine Learning laws, regulations and contracts have been identified and documented.       |\n| 13.2. | Response to Laws, Regulations and Contracts                              | Procedures should be implemented to incorporate relevant laws, regulations and contracts into Policies and Procedures.                                                     | To ensure that relevant Machine Learning laws, regulations and contracts are implemented and abided by.             |\n| 13.3. | Privacy and Protection of Personally Identifiable Information Procedures | Procedures should be implemented to ensure the privacy and protection of personally identifiable information in Products as required in law, regulations and/or contracts. | To ensure that relevant data protection and privacy laws, regulations and contracts are implemented and abided by.  |\n| 13.4. | Intellectual Property Rights Procedures                                  | Procedures should be implemented to protect intellectual property rights and the use of proprietary products in Products as required in law, regulations and/or contracts. | To ensure that relevant intellectual property rights laws, regulations and contracts are implemented and abided by. |\n| 13.5. | Internal Audit of Policies and Procedures                                | Defined organisational employees should audit the implementation of Policies and Procedures within the Organisation.                                                       | To ensure the quality and integrity of implemented Policies and Procedures.                                         |", "fetched_at_utc": "2026-02-09T13:46:43Z", "sha256": "bca994bd46dbd995450a2d9eb47eb802f5251b87dd90dcd4a902cfc19ff91eac", "meta": {"file_name": "FBPML_OrganisationBP_V1.0.0-38-40.pdf", "file_size": 143814, "mtime": 1770576227, "docling_errors": []}}
{"doc_id": "pdf-pdfs-fbpml-organisationbp-v1-0-0-7-15-6d3ca3e3086e", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_OrganisationBP_V1.0.0-7-15.pdf", "title": "FBPML_OrganisationBP_V1.0.0-7-15", "text": "## Section 1. Definitions\n\nAs used in this Best Practice Guideline, the following terms shall have the following meanings where capitalised. All references to the singular shall include references to the plural, where applicable, and vice versa. Any terms not defined or capitalised in this Best Practice Guideline shall hold their plain text meaning as cited in English and data science.\n\n| 1.1.   | Adversarial Action              | means actions characterised by mala fide (malicious) intent and/or bad faith.                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n|--------|---------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1.2.   | Assessment                      | means the action or process of making a series of determinations and judgments after taking deliberate steps to test, measure and collectively deliberate the objects of concern and their outcomes.                                                                                                                                                                                                                                                                                                           |\n| 1.3.   | Assets                          | means information technology hardware that concerns Products Machine Learning.                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| 1.4.   | Business Stakeholders           | means the departments and/or teams within the Organisation who do not conduct data science and/or technical Machine Learning, but have a material interest in Products Machine Learning.                                                                                                                                                                                                                                                                                                                       |\n| 1.5.   | Corporate Governance Principles | mean the structure of rules, practices and processes used to direct and manage a company in terms of industry recognised and published legal guidelines.                                                                                                                                                                                                                                                                                                                                                       |\n| 1.6.   | Data Governance                 | means the systems of governance and/or management over data assets and/or processes within an Organisation.                                                                                                                                                                                                                                                                                                                                                                                                    |\n| 1.7.   | Data Quality                    | means the calibre of qualitative or quantitative data.                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n| 1.8.   | Data Science                    | means an interdisciplinary field that uses scientific methods, processes, algorithms and computational systems to extract knowledge and insights from structured and/or unstructured data.                                                                                                                                                                                                                                                                                                                     |\n| 1.9.   | Domain                          | means the societal and/or commercial environment within which the Product will be and/or is operationalised.                                                                                                                                                                                                                                                                                                                                                                                                   |\n| 1.10.  | Ethical Practices               | means the ethical principles, values and/or practices that are encapsulated and promoted in an 'artificial intelligence' ethics guideline and/or framework, such as (a) The Asilomar AI Principles (Asilomar AI Principles, 2017), (b) The Montreal Declaration for Responsible AI (Montreal Declaration, 2017), (c) The Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems (IEEE, 2017), and/or (d) any other analogous guideline and/or framework. |\n| 1.11.  | Ethics Committee                | means the committee within the Organisation charged with managing and/ or directing organisation Ethical Practices.                                                                                                                                                                                                                                                                                                                                                                                            |\n| 1.12.  | Executive Management            | means the managerial team at the highest level of management within the Organisation.                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| 1.13.  | Explainability                  | means the property of Models and Model outcomes to be interpreted and/ or explained by humans in a comprehensible manner.                                                                                                                                                                                                                                                                                                                                                                                      |\n\n| 1.14.   | Fairness & Non- Discrimination   | means the property of Models and Model outcomes to be free from bias against protected classes.                                                                                                                                                                                                                       |\n|---------|----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1.15.   | Best Practice Guideline          | means this document.                                                                                                                                                                                                                                                                                                  |\n| 1.16.   | Guide                            | means an established and clearly documented series of actions or process(es) conducted in a certain order or manner to achieve particular outcomes.                                                                                                                                                                   |\n| 1.17.   | Human-Centric Design & Redress   | means orienting Products and/or Models to focus on humans and their environments through promoting human and/or environment centric values and resources for redress.                                                                                                                                                 |\n| 1.18.   | Incident                         | means the occurrence of a technical event that affects the integrity of a Product and/or Model.                                                                                                                                                                                                                       |\n| 1.19.   | Machine Learning                 | means the use and development of computer systems and Models that are able to learn and adapt with minimal explicit human instructions by using algorithms and statistical modelling to analyse, draw inferences, and derive outputs from data.                                                                       |\n| 1.20.   | Model                            | means Machine Learning algorithms and data processing designed, developed, trained and implemented to achieve set outputs, inclusive of datasets used for said purposes unless otherwise stated.                                                                                                                      |\n| 1.21.   | Organisation                     | means the concerned juristic entity designing, developing and/or implementing Machine Learning.                                                                                                                                                                                                                       |\n| 1.22.   | Performance Robustness           | means the propensity of Products and/or Models to retain their desired performance over diverse and wide operational conditions.                                                                                                                                                                                      |\n| 1.23.   | Policy                           | means a documented course of normative actions or set of principles adopted to achieve a particular outcome.                                                                                                                                                                                                          |\n| 1.24.   | Procedure                        | means an established and defined series of actions or process(es) conducted in a certain order or manner to achieve a particular outcome.                                                                                                                                                                             |\n| 1.25.   | Product                          | means the collective and broad process of design, development, implementation and operationalisation of Models, and associated processes, to execute and achieve Product Definitions, inclusive of, inter alia, the integration of such operations and/or Models into organisation products, software and/or systems. |\n| 1.26.   | Product Team                     | means the collective group of Organisation employees directly charged with designing, developing and/or implementing the Product.                                                                                                                                                                                     |\n| 1.27.   | Product Lifecycle                | means the collective phases of Products from initiation to termination - such as design, exploration, experimentation, development, implementation, operationalisation, and decommissioning - and their mutual iterations.                                                                                            |\n| 1.28.   | Product Owner                    | means the employee charged with (a) managing and maximising the value of the Product and its Product Team; and (b) engaging with various Business Stakeholders concerning the Product and its Product Definitions.                                                                                                    |\n\n| 1.29.   | Public                            | means society at large.                                                                                                                                                                                                                                                                         |\n|---------|-----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1.30.   | Public Interest                   | means the welfare or well-being of the Public.                                                                                                                                                                                                                                                  |\n| 1.31.   | Representativeness                | means the degree to which datasets and Models reflect the true distribution and conditions of Subjects, Subject populations, and/or Domains.                                                                                                                                                    |\n| 1.32.   | Safety & Security                 | means (a) the resilience of Products and/or Models against malicious and/ or negligent activities that result in Organisational loss of control over concerned Products and/or Models; and (b) real Product Domain based physical harms that result through Products and/or Models applications |\n| 1.33.   | Social Corporate Responsibilities | means the structure of rules, practices and processes used to direct and manage a company in terms of industry recognised and published legal guidelines to positively contribute to economic, environmental and social progress.                                                               |\n| 1.34.   | Software                          | means information technology software that concerns Products Machine Learning.                                                                                                                                                                                                                  |\n| 1.35.   | Special Interest Groups           | means a specific body politic, or a particular collective of citizens, who can reasonably be determined to have a material interest in the Product.                                                                                                                                             |\n| 1.36.   | Specification                     | means the accuracy, completeness and exactness of Products, Models and/or datasets in reflecting Product Definitions, Product Domains and/ or Product Subjects, either in their design and development and/or operationalisation.                                                               |\n| 1.37.   | Subjects                          | means the entities and/or objects that are represented as data points in datasets and/or Models, and who may be the subject of Product and/or Model outcomes.                                                                                                                                   |\n| 1.38.   | Systemic Stability                | means the stability of Organisation, Domain, society and environments as a collective ecosystem.                                                                                                                                                                                                |\n| 1.39.   | Traceability                      | means the ability to trace, recount, and reproduce Product outcomes, reports, intermediate products, and other artifacts, inclusive of Models, datasets and codebases.                                                                                                                          |\n| 1.40.   | Transparency                      | means the provision of an informed target audiences understanding of Organisation and/or Products Machine Learning, and their workings, based on documented Organisation information.                                                                                                           |\n| 1.41.   | Workflows                         | means the coordinated and standardised sequences of employee work activities, processes, and tasks.                                                                                                                                                                                             |\n\nPart A Organisation\n\n## Section 2. Managerial Oversight &amp; Management\n\n## 2.1 Management Direction for Machine Learning\n\n## Objective\n\nTo ensure managerial direction and support for Products in accordance with Organisation strategies, business requirements, Corporate Governance Principles, Social Corporate Responsibilities, legal regulations and Ethical Practices.\n\n|         |                                 | Control:                                                                                                                                                                                                                                                                                                                            | Aim:                                                                                                                                                                                  |\n|---------|---------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2.1.1.  | Management Committee            | A managerial committee ought to be established to (a) oversee Organisation Machine Learning and Products; and (b) warrant their effective alignment in accordance with Organisation strategies, business requirements, Corporate Governance Principles, Social Corporate Responsibilities, legal regulations and Ethical Practices. | To ensure clear managerial responsibility, oversight and custody of Organisation Machine Learning and Products.                                                                       |\n| 11.1.2. | Management Committee Diversity  | The Management Committee ought to hold a diversity of members from differing Organisation departments, including Executive Management, legal, finance, operations, public communications as well as Data Science.                                                                                                                   | To (a) ensure the diversity of managerial opinions and oversight of Organisation Machine Learning and Products; and (b) foster Organisation buy-in for Machine Learning and Products. |\n| 11.1.3. | Managerial Oversight Procedures | The Management Committee should establish appropriate Procedures to warrant managerial oversight and governance of Organisation Products, inclusive of the appointment of Data Science Managers.                                                                                                                                    | To ensure the operationalisation of the oversight and management of Organisation Machine Learning and Products by the Management Committee.                                           |\n\n## Section 3. Internal Organisation Management &amp; Oversight\n\n## 3.1 Internal Organisation\n\nObjective\n\nTo establish managerial Procedures to control and oversee the design, development and implementation of Products.\n\n|        |                                                      | Control:                                                                                                                                                                                                                                                                                                                                                                         | Aim:                                                                                                |\n|--------|------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n| 3.1.1. | Data Science Managers                                | The Management Committee should appoint Data Science Managers to oversee Products and warrant their effective alignment in accordance with the directives of the Management Committee, Policies, and, more broadly, Organisation strategies, business requirements, Corporate Governance Principles, Social Corporate Responsibilities, legal regulations and Ethical Practices. | To ensure the clear management, oversight, ownership and custody of Products.                       |\n| 3.1.2. | Data Science Managers Products Ownership and Custody | The Management Committee ought to define and allocate to Data Science Managers Products.                                                                                                                                                                                                                                                                                         | To ensure clear managerial oversight, ownership and custody of Products.                            |\n| 3.1.3. | Data Science Managers Segregation of Duties          | Conflicting duties and areas of responsibility of Data Science Managers should be segregated to reduce opportunities for the unauthorised and/or unintentional modification and/or misuse of Products.                                                                                                                                                                           | To reduce the threat of Product abuse, misuse and/or mala fide actions by Data Science Managers.    |\n| 3.1.4. | Product Owners                                       | Data Science Managers ought to appoint Product Owners to (a) oversee specific Products and Product Teams; and (b) warrant their effective management in accordance with the directives of Data Science Managers, the Management Committee, and Organisation Policies.                                                                                                            | To ensure the clear management, oversight, ownership and custody of a Product and its Product Team. |\n| 3.1.5. | Product Owners Ownership and Custody                 | Data Science Managers ought to define and allocate to designated Product Owners Products and Product Teams.                                                                                                                                                                                                                                                                      | To ensure clear managerial oversight, ownership and custody of a Product and its Product Team.      |\n\n| 3.1.6.   | Product Owners Segregation of Duties                 | Conflicting duties and areas of responsibility of Product Owners should be segregated to reduce opportunities for the unauthorised and/or unintentional modification and/or misuse of a Product.                                                                                                                     | To reduce the threat of Product abuse, misuse and/or mala fide actions by Product Owners.                                                                                                                                                     |\n|----------|------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 3.1.7.   | Product Teams                                        | Data Science Managers, in consultation with Product Owners, should define and allocate Products to designated Product Teams.                                                                                                                                                                                         | To ensure clear Product ownership and custody.                                                                                                                                                                                                |\n| 3.1.8.   | Product Definitions                                  | Data Science Managers, Product Owners, Business Stakeholders and, when relevant, Product employees ought to collectively document and define clear Product definitions, aims, internal deliverables and outcomes.                                                                                                    | To ensure Products have clear scopes to warrant (a) their effective oversight, management and execution, as well as (b) to allow for the accurate evaluation of Product risks and controls.                                                   |\n| 3.1.9.   | Approval of Product Definitions                      | The Management Committee should review and approve Product Definitions.                                                                                                                                                                                                                                              | To ensure managerial oversight of Products scopes.                                                                                                                                                                                            |\n| 3.1.10.  | Product Definitions Review                           | Product Definitions ought to be reviewed periodically, or if significant changes occur, by Data Science Managers, Product Owners, Business Stakeholders and, when relevant, Product employees.                                                                                                                       | To ensure that Product Definitions are kept up-to-date to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                    |\n| 3.1.11.  | Product Risk Classification Policy                   | A Policy and Guide, which standarises the approaches to assessing Product risks, ought to be derived by Data Science Managers and approved by the Managerial Committee.                                                                                                                                              | To ensure that (a) clear guidelines exist on how to evaluate and determine Product based-risks for subsequent evaluation in Product Risk Portfolios; and (b) Products are assigned risk-appropriate mandatory minimum capacity and oversight. |\n| 3.1.12.  | Product Risk Classification Portfolio                | Data Science Managers, Product Owners, Business Stakeholders and, when relevant, Product employees ought to collectively document and interrogate (a) Product Definitions and (b) Product design, development and implementation to identify Product based-risks and assign Product risk values and classifications. | To ensure Products have clear risk portfolios to warrant (a) their effective oversight, management and execution, as well as (b) to allow for the accurate evaluation of Product risks and controls.                                          |\n| 3.1.13.  | Approval of Product Risk Classification Portfolio    | The Management Committee should review and approve Product Risk Portfolios.                                                                                                                                                                                                                                          | To ensure managerial oversight of Products risks.                                                                                                                                                                                             |\n| 3.1.14.  | Product Product Risk Classification Portfolio Review | The Product Risk Classification Portfolio ought to be continuously reviewed and developed by Data Science Managers, Product Owners, Business Stakeholders and, when relevant, Product employees.                                                                                                                     | To ensure that Product Risk Portfolios are kept up-to-date to ensure their continued effectiveness, suitability, and accuracy.                                                                                                                |\n\n## 3.2 Product Management\n\n## Objective\n\nTo establish Procedures to control the design, development and implementation of Products.\n\n|        |                                                        | Control:                                                                                                                                                                                                                                                                                     | Aim:                                                                                                                            |\n|--------|--------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|\n| 3.2.1. | Product Lifecycle Guide                                | Data Science Managers and, when relevant, Product Owners should derive a clear Product Lifecycle Guide for the Organisation.                                                                                                                                                                 | To ensure a clear organisational Product Lifecycle Guide to warrant the effective management and oversight of Machine Learning. |\n| 3.2.2. | Product Lifecycle and Workflow Descriptions            | Having consideration for the Product Lifecycle Policy, Product Definitions, and the Product Risk Classification Portfolio, Product workflows ought to be derived, developed, and documented by Data Science Managers, Product Owners and, when relevant, Product employees for each Product. | To ensure clear Lifecycle and Workflows for Products to warrant their effective management and oversight.                       |\n| 3.2.3. | Reviewed of Product Lifecycle Guide                    | The Product Lifecycle Guide should be reviewed and approved by Data Science Managers and, when relevant, the Management Committee.                                                                                                                                                           | To ensure managerial oversight of the Product Lifecycle Guide.                                                                  |\n| 3.2.4. | Reviewed of Product Lifecycle and Workflow Description | Product Lifecycle and Workflow Descriptions should be reviewed and approved by Data Science Managers and, when relevant, the Management Committee.                                                                                                                                           | To ensure managerial oversight of Product Lifecycle and Workflow Descriptions.                                                  |\n| 3.2.5. | Product Lifecycle and Workflow Procedures              | Each Product ought to derive, develop and implement a set of Procedures to operationalise Product Lifecycle and Workflow Descriptions.                                                                                                                                                       | To ensure the operationalisation of Product Lifecycle and Workflow Descriptions.                                                |\n| 3.2.6. | Reviewed of Product Lifecycle and Workflow Procedures  | The Product Lifecycle and Workflow Procedures should be reviewed periodically, or if significant changes occur, by the Product Team to ensure their continued effectiveness, suitability, and accuracy.                                                                                      | To ensure that Product Product Lifecycle and Workflow Procedures are kept up-to-date.                                           |\n| 3.2.7. | Product Employee Roles and Responsibilities            | Data Science Managers and Product Owners ought to define and allocate to Product employees defined responsibilities and roles in terms of Product Lifecycle and Workflow Descriptions.                                                                                                       | To establish clear employee responsibilities and custodies in terms of Product Lifecycle and Workflow Descriptions.             |\n\n| 3.2.8.   | Data Science Managers Reports   | Frequent reports detailing Product progress, changes and risks ought to be made to the Management Committee by Data Science Managers and, subsequently, reviewed timeously.                     | To ensure the clear communication and management of Product deliverables to the Management Committee.                        |\n|----------|---------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n| 3.2.9.   | Product Owners Reports          | Frequent reports detailing Product progress, changes and risks ought to be made to the Data Science Managers and Business Stakeholders by Product Owners and, subsequently, reviewed timeously. | To ensure the clear communication and management of Product deliverables to Data Science Managers and Business Stakeholders. |", "fetched_at_utc": "2026-02-09T13:47:17Z", "sha256": "6d3ca3e3086eb1a2acfc92ff4efd502024dcd4450dac6f4f711250c4d9966d4e", "meta": {"file_name": "FBPML_OrganisationBP_V1.0.0-7-15.pdf", "file_size": 228824, "mtime": 1770576227, "docling_errors": []}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-13-30-15e1c3308018", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-13-30.pdf", "title": "FBPML_TechnicalBP_V1.0.0-13-30", "text": "## Section 2. Team Composition\n\n## Objective:\n\nTo (a) ensure a balanced Product Team composition that fosters close collaboration and enhances a diversity of skills;  and (b) to promote Product Team coordination and understanding through  thorough team organization.\n\n|      |                                            | Control:                                                                                                                                                                                                                                                                                                                                                                            | Aim:                                                                                                                                                                     |\n|------|--------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2.1. | Product Team Composition                   | Document and define a clear diversity of Product Team roles and expertises needed for the Product, inclusive of, amongst other things, engineers, data scientists, Product Managers, and user experience experts. Once established, recruit accordingly.                                                                                                                            | To (a) assemble a robust team for Product and/or Model design, development and deployment; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 2.2. | Product Team Roles                         | Document and allocate clear Product Team roles and expectations for Product Team members, including expectations for, and the structure of, intra-Product Team collaboration and overlapping responsibilities.                                                                                                                                                                      | To (a) ensure that Product Team roles are clearly defined; and (b) highlight associated risks that might occur in the Product Lifecycle.                                 |\n| 2.3. | Product Team Strengths and Skills Analysis | Document and assess the range of Product Team member skills and interests. Attempt to match member skills and interests to appropriate Product Team Roles as much as is practically possible.                                                                                                                                                                                       | To (a) ensure Product Team skill alignment and continued interest; and (b) highlight associated risks that might occur in the Product Lifecycle.                         |\n| 2.4. | Product Management                         | Document and allocate a clear Product Management role and duties to Product Managers, inclusive of ensuring that Product Managers have suitable Product oversight, a clear understanding of Product Team dynamics, and a contextual understanding of the Product and its operationalisation. Please see Section 3 of the Organisation Best Practices Guideline for further context. | To (a) ensure that Product Manager roles are clearly defined; and (b) highlight associated risks that might occur in the Product Lifecycle.                              |\n\n## Section 3. Context\n\n## Objective:\n\nTo ensure the Product Team's continual access to a deep understanding of the various external contexts that affect the successful design and deployment of the Product.\n\n|      |                    | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Aim:                                                                                                                                                                     |\n|------|--------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 3.1. | Industry Context   | Incorporate regulations, standards, and norms that reflect industry values, boundaries, and constraints during each phase of Product design and deployment. Document and define clear qualitative metrics and counter-metrics in Product & Outcome Definitions, Data & Model Metrics and Acceptance Criteria Metrics, as relevant, as discussed in Section 4 - Problem Mapping; Section 5 - Model Decision- Making.                                                                                                                                                                                                                                                                                                                                         | To (a) assemble a robust team for Product and/or Model design, development and deployment; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 3.2. | Deployment Context | Incorporate an understanding of the technical and infrastructure aspects of the deployed Product into the Product design process. Ensure that infrastructure, integration, and scaling requirements and limitations are considered during the Problem Mapping and Planning phases and document and define clear requirements for the Organisation Capacity Analysis, Product Scaling Analysis, Product Integration Strategy, Product Risk Analysis, Testing - Automation Analysis, and POC-to-Production Analysis, as discussed in Section 4 - Problem Mapping; Section 6 - Management & Monitoring; Section 8 - Testing.                                                                                                                                   | To (a) assemble a robust team for Product and/or Model design, development and deployment; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 3.3. | Societal Context   | Research and consider the on and off platform effects of Product deployment on end users, their communities, and societies during each phase of Product design and deployment. Ensure that behavioral shifts, power balance, and cultural concerns are considered during the Problem Mapping and Planning phases, and that these provide input for the Problem Statement & Solution Mapping, Outcome Definition, Product & Outcome Definitions Data & Model Metrics, Product Risk Analysis, User Experience Mapping, Model Type - Best Fit Analysis, Acceptance Criteria, Privacy, Testing Participants, and Accuracy Perception, as discussed in Section 4 - Problem Mapping; Section 7 - Privacy; Section 8 - Testing; Section 9 - Managing Expectations. | To (a) assemble a robust team for Product and/or Model design, development and deployment; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## Section 4. Problem Mapping\n\n## Objective:\n\nTo determine and define an appropriate, feasible and solvable business problem through consideration of several interacting analyses.\n\n|      |                                                    | Control:                                                                                                                                                                                                                                                                                                                    | Aim:                                                                                                                                                                                          |\n|------|----------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 4.1. | Problem Statement & Solution Mapping               | Document and define clear problem statements in terms of (i) User needs, (ii) Organisation problem, and/or (iii) Organization opportunity. Subsequently, document and define clear solutions to the problem statements, inclusive of the contextual needs and/or variants of the problem statements and/or their solutions. | To ensure Products have clear scopes to warrant (a) their effective oversight, management and execution, as well as (b) allow for the accurate evaluation of Product risks and controls.      |\n| 4.2. | Data Capacity Analysis                             | Map and document the state of the data delivery pipeline and available databases required to support the problem statements and solutions.                                                                                                                                                                                  | To (a) ensure that the data pipeline is sufficient to support Product(s) and enable the desired Outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle.       |\n| 4.3. | Product Definitions                                | Document and define clear Product definitions, aims, requirements and internal deliverables having regard for the above Problem Statement & Solution Mapping analysis, inclusive of subsequent iterations thereof.                                                                                                          | To ensure Product(s) have clear scope to warrant (a) their effective oversight, management and execution, as well as (b) allow for the accurate evaluation of Product risks and controls.     |\n| 4.4. | Outcomes Definitions                               | Document, delineate, and define clear Product Outcomes and Outcomes deliveries based on the above Product Definitions and the Problem Statement & Solution Mapping analysis, inclusive of subsequent iterations thereof.                                                                                                    | To ensure Product(s) have clear scopes to warrant (a) their effective oversight, management and execution, as well as (b) allow for the accurate evaluation of Product risks and controls.    |\n| 4.5. | Product & Outcome Definitions Data & Model Metrics | Document and define the above Product and Outcome Definitions in terms of clear Model and data metrics.                                                                                                                                                                                                                     | To ensure Product(s) have clear scopes to warrant (a) their effective oversight, management and execution, as well as (b) to allow for the accurate evaluation of Product risks and controls. |\n\n| 4.6.   | Organisation Capacity Analysis   | Document and assess whether the organisation has the requisite capacity to achieve the above Product Outcome and Product Metric Definitions given the Product Team Composition and Product Team Strengths and Skills and Data Capacity Analyses. If constraints detected, reiterate formulations of Product and/or Outcome Definitions to accommodate organisation capacity.                                                               | To (a) ensure that the Organization has sufficient capacity to support Product(s) and enable desired Outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                            |\n|--------|----------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 4.7.   | Product Scaling Analysis         | Document and assess the estimated degree to which the Product can be feasibly scaled within Product Domains and the Organisation, having consideration for the Organisation Capacity Analysis.                                                                                                                                                                                                                                             | To (a) ensure that the Organization has sufficient capacity to support the Product(s) and enable the desired Outcomes as the Product scales; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                              |\n| 4.8.   | Product Integration Strategy     | Document and assess the processes needed to integrate and scale the Product into organisational structures based on the Organisation Capacity and Product Scaling Analyses. If constraints detected and/ or integration appears unfeasible, reiterate formulations of Product and/or Outcome Definitions and/ or review the Organisation Capacity and/or Product Scaling Analyses to accommodate a practical Product Integration Strategy. | To (a) ensure the Product and Outcome Definitions can be achieved within the bounds of the Organisation Capacity and Product Scaling Analyses; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                            |\n| 4.9.   | Product Risk Analysis            | Document and assess the estimated risks associated with Product design, development, implementation, and operation, inclusive of considerations from the Product Scaling Analysis, and the Product Integration Strategy.                                                                                                                                                                                                                   | To (a) ensure Products have clear risk portfolios to warrant (i) their effective oversight, management and execution, as well as (ii) to allow for the accurate evaluation of Product risks and controls; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n| 4.10.   | Product Cost Analysis   | Collaborate with Finance and purchasing to document and assess the estimated costs associated with Product design, development, implementation, and operation, inclusive of considerations from the Product Scaling Analysis, the Product Integration Strategy, and the Product Risk Analysis.                                                                                                                                                                                                                                                                                  | To (a) ensure a realistic project budget is provided; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n|---------|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n| 4.11.   | User Experience Mapping | Document and assess the user experience and the desired experience for various user groups, when interacting with the Product (e.g. using Norman's Usability Heuristics). Consider mitigation strategies for possible negative impacts on and off platform. If gaps in user experience are detected or a need for process redesign or behavioral changes are uncovered reiterate formulations of Outcome Definition, as discussed in Section 4 - Problem Mapping, Organisation Capacity Analysis, and Product Integration Strategy to accommodate an effective user experience. | To (a) ensure an effective user experience; and (b) highlight associated risks that might occur in the Product Lifecycle.             |\n\n## Section 5. Model Decision-Making\n\n## Objective:\n\nTo determine the most desirable and feasible model to achieve the desired Product Outcomes through consideration of several interacting analyses.\n\n|      |                                    | Control:                                                                                                                                                                                                                                    | Aim:                                                                                                                                                          |\n|------|------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.1. | Model Type - Metric Fit Analysis   | Document and assess the Model requirements needed to meet the Product Definitions, Outcome Definitions, and Product & Outcome Definitions Data & Model Metrics, as discussed in Section 4 - Problem Mapping.                                | To ensure that chosen Model(s) meet the requirements of the Product Definitions, Outcome Definitions, and Product & Outcome Definitions Data & Model Metrics. |\n| 5.2. | Model Type - Risk Analysis         | Document and assess Model requirements needed to meet the Explainability Requirements and Product Risk Analysis, as discussed in Section 16 - Explainability; Section 4 - Problem Mapping.                                                  | To ensure that chosen Model(s) meet the requirements of the Explainability Requirements and Product Risk Analysis.                                            |\n| 5.3. | Model Type - Organisation Analysis | Document and assess the compatibility of potential Models with the Organisation Capacity Analysis, Product Scaling Analysis, and Product Integration Strategy, as discussed in Section 4 - Problem Mapping, given technical considerations. | To ensure that chosen Model(s) meet the requirements of the Organisation Capacity Analysis, Product Scaling Analysis, and Product Integration Strategy.       |\n| 5.4. | Model Type - Best Fit Analysis     | Document and assess the most appropriate Models that best meet the requirements of, and which produces the most favorable outcome given the trade-offs between, the Model Type - Metric Fit, Risk and Organization Analyses.                | To (a) ensure that the most appropriate Model(s) are chosen; and (b) highlight associated risks that might occur in the Product Lifecycle.                    |\n| 5.5. | Acceptance Criteria - Metrics      | Document and define the desired performance for an acceptable Model in terms of clear Model and data metrics that are written from the end user's perspective.                                                                              | To (a) determine the metrics and desired performance for an acceptable Model; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n\n| 5.6.   | Acceptance Criteria - Accuracy, Bias, and Fairness            | Document and define clear, narrow accuracy goals and metrics that manage the tradeoff of accuracy and explainability. Document and define the Model requirements needed to meet the Fairness & Non- Discrimination goals, as discussed more thoroughly and technically in Section 11 - Fairness & Non- Discrimination.                                                                                                                                                                                            | To (a) ensure appropriate accuracy, bias and fairness metrics for Model(s); and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n|--------|---------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 5.7.   | Acceptance Criteria - Error Rate Analysis                     | Consider the Societal and Industry Contexts in determining the acceptable method for error measurement, as discussed in Section 4 - Problem Mapping. Document and define the acceptable error types and rates for the Product as required by Representativeness & Specification, as discussed more thoroughly and technically in Section 13 - Representativeness & Specification. Analyze any potential tension between achievable and acceptable error rates and determine whether that tension can be resolved. | To (a) ensure appropriate error type and rate metrics for Model(s); and (b) highlight associated risks that might occur in the Product Lifecycle.           |\n| 5.8.   | Acceptance Criteria - Key Business Metrics / Targeted Metrics | Document and define the key business metrics (KPIs) as determined in Problem Statement & Solution Mapping, as discussed in Section 4 - Problem Mapping, and translate them into metrics that can be tracked within the framework of chosen Model(s), or into proxy metrics if direct tracking is not feasible.                                                                                                                                                                                                    | To (a) ensure appropriate business metrics for Model(s); and (b) highlight associated risks that might occur in the Product Lifecycle.                      |\n| 5.9.   | Technical Considerations                                      | Document and assess technical issues that should be considered during the Model selection process.                                                                                                                                                                                                                                                                                                                                                                                                                | To (a) ensure that technical issues are considered when selecting Models; and (b) highlight associated risks that might occur in the Product Lifecycle.     |\n\n## Section 6. Management &amp; Monitoring\n\n## Objective:\n\nTo ensure an effective and auditable Product Lifecycle.\n\n|      |                                              | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                               | Aim:                                                                                                                                                                                                                 |\n|------|----------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 6.1. | Product Requirements                         | Draft and document clear Product requirements. Review Model Type - Metrics and Acceptance Criteria, as discussed in Section 4 - Problem Mapping, to ensure alignment. Regularly review Product & Outcome Definitions Data & Model Metrics and User Experience Mapping, as discussed in Section 4 - Problem Mapping, and update as necessary                                                                                                            | To (a) ensure current, clear, and actionable Product requirements; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                     |\n| 6.2. | Product Roadmap and Pipeline                 | Develop and document a Product roadmap and pipeline that enable the experience envisioned in the User Experience Mapping, as discussed in Section 4 - Problem Mapping, and include the following sections: Schedule and milestones, tasks and deliverables, limitations and exclusions (scope), initial prioritization, and methods for determining future priority.                                                                                   | To (a) ensure a clear, actionable, and prioritized Product roadmap and pipeline; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                       |\n| 6.3. | Experimentation Constraints                  | Develop and document a method for evaluating the quality of predictions. Develop and document criteria for determining when to stop the experimentation process.                                                                                                                                                                                                                                                                                       | To (a) develop processes to ensure a balance between effectiveness and efficiency in the experimentation cycle; and (b) highlight associated risks that might occur in the Product Lifecycle.                        |\n| 6.4. | Behavioral Change Analysis - Process Changes | Research and assess the business processes that will be affected by the new Product and/ or the infrastructure changes that enable the Product. Review the User Experience Mapping, Data Capacity Analysis, and Organisation Capacity Analysis, as discussed in Section 4 - Problem Mapping, and reformulate as necessary. Develop and document a plan to retrain affected parties as necessary and mitigate business disruptions as much as feasible. | To (a) determine business processes that may be affected by the project and create a plan to retrain or mitigate impacts as necessary; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n| 6.5.   | Behavioral Change Analysis - Social (Off- Platform)   | Research and document ways in which the Product can be abused or negatively impact customers, end users, or the broader society. Develop and document a plan to mitigate negative impacts as much as feasible. Develop and document counter metrics to assess whether users or the model are 'gaming' the system.                                                                                               | To (a) determine (i) negative product uses, (ii) negative product impacts (iii) negative user or model behaviors and create a plan to counter behaviors or mitigate impacts as necessary; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n|--------|-------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 6.6.   | Resource Assessment                                   | Document the processes, tools, and staffing that are required for every phase of the project, including the Data Capacity Analysis, Organisation Capacity Analysis, Product Scaling Analysis, and Product Cost Analysis, as discussed in Section 4 - Problem Mapping, before starting each phase of the project and update as necessary.                                                                        | To (a) ensure adequate resources and funding during every phase of the Product Lifecycle; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                   |\n| 6.7.   | POC-to- Production Checklist                          | Document and define a POC-to-Production Checklist that details the existing system modifications, and new system builds, required for integrating the Product into Organisation infrastructure and incorporating additional data sources. If gaps in organisational capacity are detected, reiterate formulations of Organisation Capacity Analysis, as discussed in Section 4 - Problem Mapping, as necessary. | To (a) ensure sufficient planning for Product development and production; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                   |\n| 6.8.   | Update Schedule                                       | Document and define a POC-to-Production Checklist that details the existing system modifications, and new system builds, required for integrating the Product into Organisation infrastructure and incorporating additional data sources. If gaps in organisational capacity are detected, reiterate formulations of Organisation Capacity Analysis, as discussed in Section 4 - Problem Mapping, as necessary. | To (a) ensure the Product and its related software are updated and upgraded regularly and that the schedule for said updates are coordinated with information technology department(s); and (b) highlight associated risks that might occur in the Product Lifecycle.     |\n\n| 6.9.   | Project Records                         | Develop and document a process for preserving data of the information considered when making significant product decisions. Include any methods for standardized experiment tracking and artifact capturing that are developed by Data Science and Engineering. Develop a continuously maintained and consistently available repository for Product Requirements and any data related to their updates.                                                                                                                                                                                   | To (a) maintain a historical record of Product and data and ensure that all iterations of Product Requirements are continuously available to Stakeholders; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n|--------|-----------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 6.10.  | Project Records - Stakeholder Sign-offs | Develop a standard Stakeholder Sign-off Document to be utilized (i) after the finalization of the following documents and analyses: Problem Statement & Solution Mapping, Outcomes Definition, Product & Outcome Definitions, Product Integration Strategy, Model Type - Best Fit Analysis, Acceptance Criteria - Key Business Metrics/Targeted Metrics, Testing Design and Scheduling Framework, Resource Assessment, as discussed in Section 4 - Problem Mapping, Section 5 - Model Decision-Making; and (ii) at Project Checkpoints, as discussed in Section 10 - Project Checkpoints. | To (a) ensure stakeholder buy-in; and (b) provide an auditable record of project and stakeholder expectations at every major project decision- point.                                                                                      |\n| 6.11.  | Custody                                 | Develop a system for documenting the chain of custody for Product(s) and the data, microservices, and applications that it is built on and with, that indicates: i) provenance ii) control iii) transfer, iv) analysis, and v) transformation.                                                                                                                                                                                                                                                                                                                                            | To (a) ensure that the building blocks of the Product can be traced back to their origins; (b) allow for undesirable changes to be reverted; and (c) highlight associated risks that might occur in the Product Lifecycle.                 |\n\n## Section 7. Privacy\n\n## Objective:\n\nTo determine the most appropriate and feasible privacy-preserving techniques for the Product.\n\n|      |                                  | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Aim:                                                                                                                                                                       |\n|------|----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 7.1. | Decentralization Method Analysis | Consider the appropriateness of utilizing methods for distributing data or training across decentralized devices, services, or storage. When analyzing federated learning methods, consider Data Capacity Analysis, Product Integration Strategy, Product Traceability, and Fairness & Non-Discrimination, as discussed more thoroughly in Section 4 - Problem Mapping; Section 21 - Product Traceability; and Section 11 - Fairness & Non-Discrimination. When analyzing differential privacy methods, consider Data Quality - Noise, as discussed more thoroughly in Section 12 - Data Quality.                                                                                                                                                             | To (a) ensure appropriate privacy-preserving techniques that are aligned with chosen Models; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 7.2. | Cryptographic Methods Analysis   | Consider the appropriateness of utilizing methods for encrypting all or various parts of the data and/or Model pipeline. When analyzing homomorphic encryption methods, consider Product Integration Strategy and Product Scaling Analysis, as discussed more thoroughly in Section 4 - Problem Mapping. Additionally, consider - (a) whether the types of operations and calculations that can be performed meet the requirements of Model Type - Best Fit Analysis, as discussed more thoroughly in Section 5 - Model Decision-Making; and/or (b) whether the encrypted Model processing speed is acceptable with consideration for real world robustness and direct user interaction, as discussed more thoroughly in Section 14 - Performance Robustness. | To (a) ensure appropriate privacy-preserving techniques that are aligned with chosen Models; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## Section 8. Testing\n\n## Objective:\n\nTo ensure (a) that robust, effective, and efficient strategies, methodologies and schedules are developed for testing the Product; and (b) clear maintenance metrics and/or phases to warrant continued Product alignment with chosen metrics and performance goals.\n\n|      |                                         | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Aim:                                                                                                                                                                                                      |\n|------|-----------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 8.1. | Testing Design and Scheduling Framework | Document and define a testing design and schedule that does not artificially constrain the testing process. Incorporate the Feedback Loop Analysis in the testing design. Review the Automation Analysis in determining what level of automation is appropriate for each stage of testing. Ensure the individuals chosen through the Testing Participant Identification process are involved at the earliest stages of the testing schedule as practical. Post Product deployment, document and define a framework and process for testing and selecting variations of the production Model. | To (a) ensure a robust and feasible testing design and scheduling framework that allows for effective Product optimization; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 8.2. | Testing Participant Identification      | Document and define a process for identifying test participants as required by User Experience Mapping and Societal Context, as discussed in Section 4 - Problem Mapping; and Section 3 - Context. Determine a framework for ensuring that testing participants are intentionally diverse across use cases, user types and roles, and internal and external Stakeholders.                                                                                                                                                                                                                    | To (a) ensure testing for user impact and participant pool diversity; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                       |\n\n| 8.3.   | Automation Analysis   | Determine and define data, Model, and component integration validations that can be reasonably automated. Assess and define any processes during the development, deployment, or maintenance phases that could benefit from integrating automation into the testing infrastructure. Be sure to review - (a) the Organisation Capacity Analysis, as discussed in Section 4 - Problem Mapping, while determining the feasibility of automating the identified processes; and/or (b) the Industry, Deployment, and Societal Contexts, as discussed in Section 3 - Context, to uncover any gaps or misalignment raised by the automation of any identified process.                                                                                                                                                            | To (a) identify suitable and effective areas for incorporating testing automation within the Product development, deployment, and maintenance phases; and (b) highlight associated risks that might occur in the Product Lifecycle.                              |\n|--------|-----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 8.4.   | Feedback Loop         | Document and define a feedback loop that enables monitoring of stability, performance, and operations metrics, and counter-metrics, as required by Performance Robustness, Monitoring and Maintenance, and Systemic Stability, as discussed more thoroughly in Section 14 - Performance Robustness; Section 15 - Monitoring & Maintenance; and Section 20 - Systemic Stability. Develop and incorporate a method for flagging bias and for issue reporting. Document and define a process for real-time sharing of testing participant feedback with the development and maintenance teams. Incorporate the Feedback Loop in the Testing Design and Scheduling Framework to ensure that the features the Model is utilizing are acceptable for the application during the development, deployment, and maintenance phases. | To (a) ensure robust and responsive feedback loop measures that enable monitoring of necessary metrics and effectively integrate into the Testing Design and Scheduling Framework; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## Section 9. Managing Expectations\n\n## Objective:\n\nTo effectively set and communicate realistic Product expectations to Stakeholders and obtain their buy-in.\n\n|      |                     | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Aim:                                                                                                                                                                                              |\n|------|---------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 9.1. | Performance         | Product management should attempt to set realistic Product performance expectations for Stakeholders through periodic stakeholder discussions on the following issues: (i) limited industry understanding of what tasks are difficult for the Product; (ii) difficulty of determining what type of modifications - network design, input features, or training data - will create the greatest Product improvement; and (iii) Model improvement can stall significantly while experimenting with different variable modifications. | To (a) effectively communicate realistic Product performance expectations throughout the development process; and (b) highlight associated risks that might occur in the Product Lifecycle.       |\n| 9.2. | Timeframe           | Product management should set expectations for long-term investment in the Product for Stakeholders, specifically focusing on: (i) the unpredictability of Product improvement; (ii) Product difficulties are traditionally hard to diagnose as they are often caused by subtle issues of intersecting inputs; and (iii) it is possible for the Product to completely stall with absolutely no discernible improvement in spite of significant time and effort.                                                                    | To (a) effectively set Stakeholder expectations regarding the difficulty of locking down Product timelines; and (b) highlight associated risks that might occur in the Product Lifecycle.         |\n| 9.3. | Accuracy perception | The Product Team should work to ensure that the solution will be accurate enough to meet a variety of different Stakeholders' expectations, recognizing that each group of Stakeholders will have different views on what is 'accurate' based on their interaction with the Product. Product management should set and communicate expectations in-line with the achievable level of accuracy for each user group.                                                                                                                 | To (a) effectively communicate achievable accuracy levels, considering individual Stakeholder accuracy preferences; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n| 9.4.   | POC-to- Production   | The Product Team should effectively communicate that infrastructure is often the determining factor for the success of the POC- to-Production transition and rely heavily on the POC-to-Production Checklist, as discussed in Section 6 - Management & Monitoring, to set and align Stakeholder expectations of the transition process. The Product Team should set the expectation, before beginning the transition process, that novel problems will likely arise during the transition that may significantly affect the timeline and costs. The Product Team should be on alert for integration issues arising close to the final release of the solution, which the Product Manager should communicate to relevant Stakeholders, along with progress updates, at a progressively more frequent cadence.   | To (a) uncover and communicate issues that may delay the transition of the solution from POC-to-Production or make that transition less feasible; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n|--------|----------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 9.5.   | Production Costs     | Review and analyze the finalized POC budget to determine a realistic Product implementation budget. Review the Product Cost Analysis as discussed in Section 4 - Problem Mapping to ensure its continued accuracy and reformulate as necessary. The Product Team should effectively communicate to Stakeholders that the budget for implementation will likely be in-line or more expensive than the cost to get through POC.                                                                                                                                                                                                                                                                                                                                                                                  | To (a) ensure realistic expectations for a sufficient Product implementation budget are communicated; and (b) highlight associated risks that might occur in the Product Lifecycle.                                               |\n\n## Section 10. Project Checkpoints\n\n## Objective:\n\nTo ensure that necessary factors are considered at key decision points.\n\n|       |                                            | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Aim:                                                                                                                                                                        |\n|-------|--------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 10.1. | Machine Learning Appropriate Tool Analysis | The Product Team should work cross-functionally with Stakeholders to define and document a Machine Learning checklist that considers the following areas, amongst other things: a. Is there a different approach that will generate a greater return more quickly; b. Given the results of the Data Capacity Analysis, does the Organisation have enough secure, non- discriminatory, representative, high quality data for every stage of the process; c. Can the problem be solved by simple rules; d. Does the Product solution require emotional intelligence or empathy; e. Does the Product solution need to be fully interpretable or explainable; f. Given the results of the Organisation Capacity Analysis, does the Organization have the people, processes, and tools necessary to productize the end product; g. Can the consequences of Product failure be easily fixed or mitigated; and/or h. What other non-technical solutions can be used to augment the Product and its offering and/or, more directly, whether Machine Learning is the best solution for the Product at hand. | To (a) ensure that Machine Learning is the appropriate method for solving the chosen problem; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n| 10.2   | Data Buy v. Build Analysis   | The Product Team should work cross-functionally with relevant Stakeholders to define and document a Buy v. Build checklist that considers the following areas: a. Does the Organisation have enough data for every stage of the process (training, POC, production) and for every purpose (replacing stale/flawed data, measuring success); b. Does the Organisation have the right type of data for every stage of the process (training, POC, production) and for every purpose (replacing stale/flawed data, measuring success); c. Is bought data secure and free of privacy concerns; d. Is the bias in the bought data limited, mitigatable, or removable; e. Given the results of the Data Quality Analysis, does the Organisation have quality data and are datasets complete; f. Given the Product Team Composition, does the Organisation have the staffing and expertise to clean, prepare, and maintain internal data; and/or g. Given the Data Capacity Analysis, is the necessary data easily and readily available internally.   | To (a) ensure that the Organisation's decision to either purchase data or utilize in-house data is appropriate based on Organisation capacity and/or constraints; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n|--------|------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\n|   10.3 | Model Buy v. Build Analysis          | The Product Team should work cross-functionally with relevant Stakeholders to define and document a Buy v. Build checklist that considers the following areas: a. Is the scope of the Product manageable, given the results of the Organisation Capacity Analysis; b. Can bought Models be used for other Products (eg. transfer learning); c. Does the Organisation have the in-house expertise required to acquire and label the training data, given the Product Team Composition; d. How much would it cost to acquire a properly labeled training dataset; e. Given the Product Team Composition, does the Organisation have the in-house expertise required to retrain Models, if necessary; f. How important is Model customization and, if so, can bought Models be customised; g. Are the Acceptance Criteria - Accuracy, Bias, and Fairness requirements for bought Models feasible given the timeline, Product Team Composition, and Organisation Capacity Analysis; and/or h. What are the usage limits and costs for pre-trained Models.   | To (a) ensure that the Organisation's decision to either purchase or build the Models is appropriate based on Organisation capacity and/or constraints; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n|--------|--------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|   10.4 | POC-to- Production Go/No-Go Analysis | The Product Team should work cross-functionally with relevant Stakeholders to define and document a Go/No- Go checklist that considers qualitative and quantitative factors in the following areas: a. Can POC-to-Production Checklist be adequately addressed; b. Is the Product Cost Analysis still feasible; c. Does the Product Team have approval for a Product maintenance budget; d. Are the updates, upgrades, and add-ons to the data infrastructure near completion; e. What is the state of customer process reconstruction and end-user training; f. Has the failsafe, rollback, or emergency shutdown plan been completed and approved; and/or g. Have the communication and mitigation plans in case of failsafe, rollback, or emergency shutdown been completed and approved.                                                                                                                                                                                                                                                            | To (a) ensure that the solution should be deployed in production and/or Product Domains; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                  |", "fetched_at_utc": "2026-02-09T13:48:06Z", "sha256": "15e1c3308018db9cfa2f65699bbec5c8b890e59352817947b19585a82abe4a46", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-13-30.pdf", "file_size": 368603, "mtime": 1770576471, "docling_errors": []}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-32-62-434b15725995", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-32-62.pdf", "title": "FBPML_TechnicalBP_V1.0.0-32-62", "text": "## Section 11. Fairness &amp; NonDiscrimination\n\n## Objective:\n\nTo (a) identify and mitigate risk of disproportionately unfavorable Outcomes for protected (Sub)populations; and (b) minimise the unequal distribution of Product and Model errors to prevent reinforcing and/or deriving social inequalities and/or ills, and (c) promote compliance with existing anti-discrimination laws and statutes.\n\n## What do we mean when we refer to Fairness?\n\nFairness is a complex socio-technical challenge for which there is no single generic definition. Broadly speaking -\n\nFairness is about identifying bias in a machine learning Model or Product and mitigating discrimination with respect to sensitive, and (usually) legally protected attributes such as ethnicity, gender, age, religion, disability, or sexual orientation.\n\nAlgorithmic discrimination can take many forms and may occur unintentionally. Machine learning Products might unfairly allocate opportunities, resources, or information, and they might fail to provide the same quality of service to some people as they do to others.\n\nThe conversation about fairness distinguishes between group fairness and individual fairness measures. Group fairness ensures some form of statistical parity (e.g. equal calibration, equal false positive/negative rate) across protected groups. Individual fairness requires that individuals who are similar with respect to the predictive task be assigned similar outcomes regardless of the sensitive attribute.\n\n## Why is Fairness relevant?\n\nMachine learning Products are increasingly used to inform high-stakes decisions that impact people's lives.It is therefore important that ML-driven decisions do not reflect discriminatory behavior toward certain populations. It is the responsibility of data science practitioners and business leaders to design machine learning Products that minimizes bias and promotes inclusive representation.\n\nSome business leaders express concerns about a potential increase in the risk of reputational damage and legal allegations in case of discriminatory 'black box' Models. AI fairness can substantially reduce these concerns. Another reason for taking AI fairness seriously is the development of regulatory frameworks for AI. For example, the European Commission published a white paper on AI in 2020, which was followed in 2021 by a regulatory framework proposal for AI in the European Union.\n\n## How to apply Fairness?\n\nFairness should be considered throughout the product lifecycle. Given that AI systems are usually designed to evolve with experience, fairness should be closely monitored during deployment as well as during product development. The Technical Best Practices Guidelines provide detailed guidance into implementing fairness in your AI products.\n\n## 11.1 Product Definitions\n\n## Objective\n\nTo (a) identify and mitigate risk of disproportionately unfavorable Outcomes for protected (Sub)populations; and (b) minimise the unequal distribution of Product and Model errors to prevent reinforcing and/or deriving social inequalities and/or ills, and (c) promote compliance with existing anti-discrimination laws and statutes.\n\n|         |                                                     | Control:                                                                                                                                                                                                                                                                                                                                                                                       | Aim:                                                                                                                                                                                                                        |\n|---------|-----------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 11.1.1. | (Sub)populations Definition                         | Define (Sub)populations that are subject to Fairness concern, with input from Domain and/or legal experts when relevant.                                                                                                                                                                                                                                                                       | To (a) ensure that vulnerable and affected populations are appropriately identified in all subsequent Fairness testing and Model build; and (b) highlight associated risks that might occur in the Product Lifecycle.       |\n| 11.1.2. | (Sub)population Data                                | Gather data on (Sub)population membership. If a proxy approach is used, ensure the performance of the proxy is adequate in this context.                                                                                                                                                                                                                                                       | To (a) facilitate Fairness testing pre- and post-Model deployment; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                            |\n| 11.1.3. | (Sub)population Outcome Perceptions                 | Document and assess whether scored (Sub)populations would view Model Outcomes as favorable or not, using input from subject matter experts and stakeholders in affected (Sub)populations. Document and assess any divergent views amongst (Sub)populations.                                                                                                                                    | To (a) ensure uniformity in (Sub) population outcome perception, if applicable; (b) highlight Outcome effects for different (Sub)populations; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| 11.1.4. | Erroneous Outcome Consequence Estimation Divergence | Document and assess the results of erroneous (false positive & false negative) outcome consequences, both real and perceived, specifically in terms of divergence between relevant (Sub) populations. If material divergence present, take measures to harmonise Outcome perceptions and/or mitigate erroneous Outcome consequences in Model design, exploration, development, and production. | To (a) ensure uniformity in erroneous Outcomes for (Sub) populations; (b) highlight outcome effects for different (Sub) populations; and (c) highlight associated risks that might occur in the Product Lifecycle.          |\n| 11.1.5. | Positive Outcome Spread                             | Document and assess the degree to which Model positive outcomes can be distributed to non-scored (Sub)population, when contextually appropriate. If present, take measures to promote Model Outcome distribution in Model design, exploration, development, and production.                                                                                                                    | To (a) ensure the non-prejudicial spread of positive Model Outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                          |\n\n| 11.1.6.   | Enduring Bias Estimation     | Document and assess whether exclusions from Product usage might perpetuate pre-existing societal inequalities between (Sub)populations. If present, take measures to mitigate societal inequalities perpetuation in Model design, exploration, development, and production.   | To (a) ensure the non-prejudicial spread of Model Outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                       |\n|-----------|------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 11.1.7.   | Appropriate Fairness Metrics | Consult Domain experts to inform which Fairness metrics are contextually most appropriate for the Model when conducting Fairness testing.                                                                                                                                     | To (a) ensure that fairness testing and subsequent Model changes (i) result in outcome changes which are relevant for (Sub)populations; and/or (ii) are consistent with regulatory guidance and context- specific best practices; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 11.1.8.   | Model Implications           | Document and assess the downside risks of Model misclassification/inaccuracy for modeled populations. Use the relative severity of these risks to inform the choice of Fairness metrics.                                                                                      | To (a) ensure that improving in the chosen Fairness metrics achieves the greatest Fairness in Model decisioning after deployed; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                   |\n| 11.1.9.   | Fairness Testing Approach    | Document and assess the Fairness testing methodologies that will be applied to Model and/or candidate Models, along with any applicable thresholds for statistical/ practical significance, acceptable performance loss tolerance, amongst other metrics.                     | To (a) prevent Fairness testing methodology and associated thresholds change during Model review; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                 |\n\n## 11.2 Exploraition\n\n## Objective\n\nTo identify and control for Fairness and Non-Discrimination risks based on the available datasets.\n\n|         |                             | Control:                                                                                                                                                                                                                                                                            | Aim:                                                                                                                |\n|---------|-----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|\n| 11.2.1. | (Sub)population Data Access | Keep separate Model development data and (Sub)population membership data (if applicable Regulations allow the possession and processing of such in the first place), especially if the use of (Sub)population data in the Model is prohibited or would introduce fairness concerns. | To (a) guarantee that (Sub) population membership data does not inadvertently leak into a Model during development. |\n\n| 11.2.2.             | Univariate Assessments                    | Document and perform univariate assessments of relationship between (Sub)populations and Model input Features, including appropriate correlation statistics.                                                                                                                                                                                                                                                                                                                                                                                                       | To (a) identify input Feature trends associated with (Sub)populations; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                           |\n|---------------------|-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 11.2.3.             | Prohibited Data Sources                   | Develop and maintain an index of data sources or features that should not be made available or utilized because of the risks of harming (Sub)populations, specifically Protected Classes.                                                                                                                                                                                                                                                                                                                                                                          | To (a) prohibit the actioning of data sources that will disproportionately prejudice (Sub)populations; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                           |\n| 11.2.4.             | Data Representativeness                   | Ensure the membership rates of (Sub) populations in Model development data align with expectations and that data is representative of Domain populations.                                                                                                                                                                                                                                                                                                                                                                                                          | To (a) guarantee that Model performance and Fairness testing during model development will provide a consistent picture of Model performance after deployment; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                   |\n| 11.2.5.             | (Sub)population Proxies and Relationships | Document and assess the relationship between potential input Features and (membership of) (Sub)populations of interest based on, amongst other things, (i) reviews with diverse Domain experts, (ii) explicit encoding of (Sub) population membership, (iii) correlation analyses, (iv) visualization methods. If relationships exist, the concerned input Features should be excluded from Model datasets, unless a convincing case can be made that an (adapted version of) the input Feature will not adversely affect any (Sub)populations, and document this. | To (a) prevent Model decisions based directly or indirectly on protected attributes or protected class membership; (b) reduce the risk of Model bias against relevant (Sub)populations; (c) understand any differences in data distributions across (Sub) populations before development begins; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| Associated Controls | Associated Controls                       | Review the following controls with particular attention in the context of bias and fairness with respect to protected (Sub)populations: Section 12.2.2. - Missing and Bad Data Assessment. Section 13.2.4. - Selection Function; which is concerned with accurate representation of (Sub)populations.                                                                                                                                                                                                                                                              | Review the following controls with particular attention in the context of bias and fairness with respect to protected (Sub)populations: Section 12.2.2. - Missing and Bad Data Assessment. Section 13.2.4. - Selection Function; which is concerned with accurate representation of (Sub)populations.                                                                          |\n\n## 11.3. Development\n\n## Objective\n\nTo minimise the unequal distribution of Product and Model errors for (Sub)populations during Model  development in the most appropriate manner.\n\n|         |                                               | Control:                                                                                                                                                                                                                                                                                                                                                                                                              | Aim:                                                                                                                                                                                                                                                                                                                                                                                  |\n|---------|-----------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 11.3.1. | Explainability (xAI) (Sub)population Outcomes | Keep separate Model development data and (Sub)population membership data (if applicable Regulations allow the possession and processing of such in the first place), especially if the use of (Sub) population data in the Model is prohibited or would introduce fairness concerns.                                                                                                                                  | To (a) guarantee that (Sub) population membership data does not inadvertently leak into a Model during development.                                                                                                                                                                                                                                                                   |\n| 11.3.2. | Model Architecture and Interpretability       | Choose Model architecture that maximizes interpretability and identification of causes of unfairness. Consider different methodologies within the same Model architecture (ex. monotonic XGBoost, explainable neural networks). Evaluate whether Product Aims can be accomplished with a more interpretable Model.                                                                                                    | To (a) provide information that can guide Model-builders; (b) ensure that Model decisions are made in line with expectations; (c) allow Product Subjects and/or End Users to understand why they received corresponding Outcomes; (d) help inform the causes of Fairness issues if issues are detected; and (e) highlight associated risks that might occur in the Product Lifecycle. |\n| 11.3.3. | Fairness Testing of Outcomes                  | Focus fairness testing initially on outcomes that are immediately experienced by (Sub)populations. For example, if a model uses a series of sub-Models to generate a score and a threshold is applied to that score to determine an Outcome, focus on Fairness issues related to that Outcome. If issues are identified, then diagnose the issue by moving 'up-the-chain' and testing the Model score and sub-Models. | To (a) ensure that the testing performed best reflects what will happen when Models are deployed in the real world; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                     |\n| 11.3.4. | Disparate Impact Testing                      | If applicable, test Model(s) for disparate impact. Evaluate whether Model(s) predict a Positive Outcome at the same rate across (Sub)populations.                                                                                                                                                                                                                                                                     | To (a) ensure that (Sub)population members are receiving the Positive Outcome as often as their peers; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                  |\n| 11.3.5. | Equalized Opportunity Testing                 | If applicable, test Model(s) for equalized opportunity. Evaluate whether Model(s) predict a Positive Outcome for (Sub) population members that are actually in the positive class at the same rates as across (Sub)populations.                                                                                                                                                                                       | To (a) ensure that (Sub)population members who should receive the Positive Outcome are receiving the Positive Outcome as often as their peers; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                          |\n\n| 11.3.6.   | Equalized Odds Testing                       | If applicable, test Model(s) for equalized odds. Evaluate whether Model(s) predict a Positive & Negative Outcome for (Sub) population members that are actually in the positive & negative class respectively at the same rates across (Sub)populations.   | To (a) ensure that (i) protected (Sub)populations who should receive the Positive Outcome are receiving the Positive Outcome as often as other (Sub)populations, and (ii) protected (Sub)populations who should not receive the Positive Outcome are not receiving the Positive Outcome as often as other (Sub)populations; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n|-----------|----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 11.3.7.   | Conditional Statistical Parity Testing       | If applicable, test Model(s) for conditional statistical parity. Evaluate whether Model(s) predict a Positive Outcome at the same rate across (Sub)populations given some predefined set of 'legitimate explanatory factors'.                              | To (a) ensure that (Sub)populations members are receiving the Positive Outcome just as often as (Sub) populations with similar underlying characteristics; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                    |\n| 11.3.8.   | Calibration Testing Across (Sub) populations | If applicable, test Model(s) for calibration. Evaluate whether (Sub)populations members with the same predicted Outcome have an equal probability of actually being in the positive class.                                                                 | To (a) ensure that Subpopulations each have the same likelihood of deserving the Positive Outcome for a given Model prediction; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                               |\n| 11.3.9.   | Differential Validity Testing                | If applicable, test Model(s) for differential validity. Evaluate whether Model performance varies meaningfully by (Sub) population, with a special focus on any groups that are underrepresented in modelling data.                                        | To (a) ensure that the Model's predictive abilities aren't isolated in or concentrated to (Sub)population members; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                            |\n| 11.3.10.  | Feature Selection Fairness Review            | Evaluate the impact of removing or modifying potentially problematic input Features on Fairness metrics and Model quality.                                                                                                                                 | To (a) assess whether more fair alternative Models can be made that fulfill Model objectives; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                                 |\n| 11.3.11.  | Modeling Methodology Fairness Review         | Evaluate the impact of changing Modelling methodology choices (f.e. algorithm, segmentation, hyperparameters, etc.) on Fairness metrics and Model quality.                                                                                                 | To (a) assess whether more fair alternative Models can be made that fulfill the Model objectives; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                             |\n\n## 11.4 Production\n\n## Objective\n\nTo maintain operationalised Fairness at the level established during Model Development.\n\n|         |                                | Control:                                                                                                                                                                                                                                                                                                                                                                                                | Aim:                                                                                                                                                                                                            |\n|---------|--------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 11.4.1. | Domain Population Stability    | Continually assess the stability of the Domain population being scored, both in terms of its composition relative to the Model development population, and the quality of the Model by class.                                                                                                                                                                                                           | To (a) ensure the continued accuracy of Fairness tests and metrics; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                               |\n| 11.4.2. | Fairness Testing Schedule      | Define a policy for timing of re- assessment of Model fairness that includes re-testing at regular intervals and/or established trigger events (e.g. any modifications to Model inputs or structure, changes to the composition of the modeled population, impactful policy changes).                                                                                                                   | To (a) detect issues with Model Fairness that may not have existed during pre-deployment of the Model; and (b) highlight associated risks that might occur in the Product Lifecycle.                            |\n| 11.4.3. | Input Data Transparency        | Ensure that Product Subjects have the ability to observe attributes relied on in the modeling decision and correct inaccuracy. Collect data around this process and use it to identify issues in the data sourcing/aggregation pipeline.                                                                                                                                                                | To (a) ensure that the Model is making decisions on accurate data; (b) learn whether there are problems with Model's data assets; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| 11.4.4. | Feature Attribution            | Ensure that Product Subjects can understand why the Model made the decision it did, or how the Model output contributed to the decision. Ideally, an understanding would include which features were most important in the decision and give some guidance as to how the subject could improve in the eyes of the Model. (See Section 13 - Representativeness & Specification for further information.) | To (a) ensure that Product Subjects (i) have some level of trust/ understanding in the Model that affect them and (ii) feel that they have agency over the process and that Model Outcomes are not arbitrary.   |\n| 11.4.5. | Product Subject Appeal Process | Incorporate a 'right of appeal' procedure into the Model's deployment, where Product Subjects can request a human review of the modeling decision. Collect data around this process and use it to inform Model design choices.                                                                                                                                                                          | To (a) ensure that Product Subjects are, at a minimum, made aware of the results of Model decisions; and (b) allow inaccurate predictions to be corrected.                                                      |\n| 11.4.6. | Feature attribution Monitoring | As part of regularly scheduled review, or more frequently, monitor any changes in feature attribution or other explainable metric by sub-population. (See Section 15 - Monitoring & Maintenance for further information.                                                                                                                                                                                | To (a) detect reasons for changes in Model performance, as well as any changes earlier in the data pipeline; and (b) highlight associated risks that might occur in the Product Lifecycle.                      |\n\n## Section 12. Data Quality\n\n## Objective:\n\nTo ensure Data Quality and prevent unintentional effects, changes and/or deviations in Product and Model outputs associated with poor Product data.\n\n## What is Data Quality?\n\nLike many other concepts in Machine learning and data science, Data quality is something without a single and widely accepted definition. Nonetheless, we think of -\n\nData Quality as data which is fit for use for its intended purpose and satisfies business, system and technical requirements.\n\nIn technical terms, data quality can be a measure of its completeness, accuracy, consistency, reliability and whether it is up-to-date.\n\nData integrity is sometimes used interchangeably with data quality. However, data integrity is a broader concept than data quality and can encompass data quality, data governance and data protection.\n\n## Why is Data quality important?\n\nIt is not difficult for all stakeholders involved in a Project to agree that good data quality is of prime importance. Bad data quality means a business or an organization may not have a good grasp on whether they are successful in meeting prior set objectives or not. Bad data quality results in poor analytical solutions, wrong insights and conclusions. This translates into inadequate response to market opportunities, an inability to timely react to customers' requests, increased costs, and last, but not least, potential shortcomings in meeting compliance requirements. In short, poor data results in poor products and poor decisions. This is undesirable.\n\n## The How of Data quality\n\nData quality is something that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage in isolation.\n\n## 12.1 Exploration\n\n## Objective\n\nTo determine if the quality of the data shall be sufficient, or can be made sufficient, to achieve the Product Definitions.\n\n|         |                                       | Control:                                                                                                                                                                                                                                                                                                                                                                                                     | Aim:                                                                                                                                                                                                                                                                                                        |\n|---------|---------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 12.1.1. | Data Definitions                      | Document and ensure all subtleties of definitions of all data dimensions are clear, inclusive of but not limited to gathering methods, allowed values, collection frequency, etc. If not, acquire such knowledge, or discard the dimension.                                                                                                                                                                  | To (a) assess and prevent unjustified assumptions about the meaning of a data dimension or its values; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                        |\n| 12.1.2. | Data Modeling                         | Document and ensure all relationships between (the fields of) different datasets are clear, in the light of their Data Definitions. (See Section 12.1.1 - Data Definitions for further information.) If this 'Data Model' is not clear or available, create it, or discard the datasets.                                                                                                                     | To (a) prevent the creation and/or combination of invalid datasets; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                           |\n| 12.1.3. | Missing and Bad Data Assessment       | Document and assess (a) the occurrence rates and (b) co-variances of missing values and nonsensical values throughout the Model data. If either is significant, investigate causes and consider discarding affected data dimension(s) or commit dedicated research and development to mitigating measures for affected data dimension(s). (See Section 12.3.1. - Live Data Quality for further information.) | To assess (a) the risk of low quality data introducing bias to Model data and/or Outcomes; and (b) whether Model dataset(s) quality is sufficient for Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle.                                                    |\n| 12.1.4. | Data Veracity Uncertainty & Precision | Document and assess the veracity and precision of data. If compromised, uncertain and/or unknown, document and assess (i) the causes and sources hereof and (ii) statistical accuracy .Incorporate appropriate statistical handling procedures, such as calibration, and appropriate control mechanisms in Model, or discard the data dimension.                                                             | To assess (a) the risk of low quality data introducing bias to Model data and/or outcomes; (b) a priori the plausibly achievable performance; (c) whether the Model dataset(s) quality is sufficient for Product Definitions; and (d) highlight associated risks that might occur in the Product Lifecycle. |\n\n## 12.2 Development\n\n## Objective\n\nTo determine if Model performance is affected or biased due to data quality issues.\n\n|         |                               | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Aim:                                                                                                                                                                                                                                       |\n|---------|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 12.2.1. | Missing and Bad Data Handling | Document and assess how missing and nonsensical data (a) are handled in the Model, through datapoint exclusion or data imputation; (b) affect the Selection Function through datapoint removal; (c) affect Model performance and Fairness for subpopulations through data imputation. If (Sub)populations are unequally affected, take additional measures to increase data quality and/or improve Model resilience. Consult Domain experts during assessment and mitigation. | To (a) prevent introducing bias to Model Outcomes due to low quality data; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                   |\n| 12.2.2. | Error - Quality Correlation   | Document and assess whether low-quality datapoints (those with low-confidence, uncertain, nonsensical, missing and/or imputed attributes) correlate with high (rates of) error, and how this affects (Sub)populations. If so, take additional measures to increase data quality and/or improve Model performance for specific (Sub)populations.                                                                                                                               | To (a) prevent introducing bias to Model Outcomes due to low quality data; (b) whether the Model dataset(s) quality is sufficient for Product Definition(s); and (c) highlight associated risks that might occur in the Product Lifecycle. |\n\n## 12.3 Production\n\n## Objective\n\nTo ensure the quality of incoming data to the Product during operations.\n\n|         |                   | Control:                                                                                                                                                                                                                                                                                                                                                                  | Aim:                                                                                                                                                                                  |\n|---------|-------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 12.3.1. | Live Data Quality | Document and assess whether live incoming data with low quality (low- confidence, uncertain, nonsensical, missing and/or imputed attributes) can be handled appropriately by the Model on the per-Data Subject level. If not, implement additional measures, and/or re-assess validity of Product Definition(s) in view of non-applicability to low quality live subsets. | To (a) assess and control that all Product Subjects can be supported appropriately by the live Product; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## Section 13. Representativeness &amp; Specification\n\n## Objective:\n\nTo (a) ensure that Product data and Model(s) are representative of, and accurately specified for, Product Domain as far as is reasonably practical; and (b) guard against unintentional Product and Model behaviour and Outcomes as far as is reasonably practical.\n\n## What is Representativeness and Specification?\n\nRepresentativeness is a concept that is often used in statistics and machine learning with regards to the data we use to train a Model. A representative data sample is a set from a larger statistical population that adequately replicates the larger group according to a characteristic or quality under study. Put less metaphorically -\n\nRepresentativeness means the ability of the Model and its data to adequately replicate and represent that characteristics of its operational environment.\n\nIt should not be confused with representation learning (also known as feature learning) in machine learning. The latter refers to a set of techniques for automatically detecting feature patterns and in fact replaces manual feature engineering.\n\nSpecification is a less known term. In our context -\n\nSpecification refers to ensuring the appropriate degrees of freedom in the Model.\n\nFor example, we have selected the appropriate cost function for the problem at hand, the target variable is appropriate and not a proxy for what we are really interested in measuring, etc. It is like representativeness but for the Model, and not the data. Unlike the performance robustness section, many of the controls here will be difficult to precisely measure quantitatively. However, we should still try to consider as many scenarios as possible and minimize all risks stemming from not addressing them rigorously.\n\n## Why is Representativeness and Specification important?\n\nIf the data is not representative with relation to the goal of the Product, it will not serve us well. It will result in poor performing Models when deployed, and it will inherently contain bias (not in the fairness and nondiscrimination sense but in relation to sampling). This can lead to misleading conclusions and unrealistic assumptions and expectations. Correct specifications on the other hand relates to selecting appropriate and rigorous features, selection function, and target, etc. This ensures that the Model we develop is rigorous, robust and has a properly specified number of parameters.\n\n## The How of Representativeness and Specification\n\nRepresentativeness and Specification is something that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage in isolation.\n\n## 13.1 Product Definitions\n\n## Objective\n\nTo (a) ensure the pragmatic formulation and accurate specification of Product Definition(s); (b) minimise Model simplifications, assumptions and ambiguities; and (c) ensure adequate vigil of the non-reducible ones throughout the Product Lifecycle.\n\n|         |                                      | Control:                                                                                                                                                                                               | Aim:                                                                                                                                                                                                            |\n|---------|--------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 13.1.1. | R&S Product Definition(s) Assessment | Document and assess whether recorded Product Definition(s) are complete, unambiguous and representative of intended Product Outcomes. If they are not, refine them as much as is reasonably practical. | To (a) enable reliable execution of all further research, development and assessments; and (b) highlight associated risks that might occur in the Product Lifecycle.                                            |\n| 13.1.2. | Product Assumptions                  | Document and assess Product assumptions, the likelihood of their appropriateness, their continued validity, and inherent risks.                                                                        | To (a) detect, mitigate and review Product assumptions and their inherent risks; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                  |\n| 13.1.3. | Product Simplifications              | Document and assess Product simplifications, the likelihood of their appropriateness, and their inherent risks.                                                                                        | To (a) detect, mitigate and review Product simplifications and their inherent risks; and (b) highlight associated risks that might occur in the Product Lifecycle.                                              |\n| 13.1.4. | Product Limits                       | Document and assess the limitations of the Product's application and the applicability of Product Definitions.                                                                                         | To (a) detect and review Model limitations in light of (i) Model assumptions and (ii) Model simplifications; and (b) highlight associated risks that might occur in the Product Lifecycle.                      |\n| 13.1.5. | R&S Problem Definition Review        | R&S Product Definition(s) ought to be reviewed continually, specifically when significant Model changes occur.                                                                                         | To ensure that R&S Product Definition(s) are kept up-to- date to ensure their continued effectiveness, suitability, and accuracy; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## 13.2 Exploration\n\n## Objective\n\nTo (a) ensure that Model dataset(s) correspond to the Product Definition in sufficient detail, completeness and without material unambiguity; and (b) to identify associated risks in order to ensure an adequate vigil throughout the Product Lifecycle.\n\n|         |                                       | Control:                                                                                                                                                                                                                                                                                                                                                                                                | Aim:                                                                                                                                                                                                                                                                                  |\n|---------|---------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 13.2.1. | Data Subjectivity                     | Document and assess whether the Model dataset(s) contain subjective components. If subjective components are present, take measures to handle or avoid subjectivity risks in Product and/ or Model design as much as is reasonably practical.                                                                                                                                                           | To (a) assess and control for the accuracy of the specification of Model inputs, manipulations, Outcomes, and interpretations to ensure the unambiguous applicability of Model(s) in Product Domain(s); and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 13.2.2. | Heterogeneous Variable Simplification | Document and assess whether Model datasets contain, or Model components produce, simplified input Features that represent inherently heterogeneous concepts in Product Domains. If simplified, take measures to reflect the heterogeneity of Product Domains as much as is reasonably practical.                                                                                                        | To (a) detect, review and control for the simplification of heterogeneous input Variables; (b) prevent generalization and spurious correlation; and (c) highlight associated risks that might occur in the Product Lifecycle.                                                         |\n| 13.2.3. | Hidden Variables                      | Document and assess whether Model datasets are missing, or Model components hide relevant attributes of Product Subjects or systemic Variables with respect to Product Domains. If hidden, obtain additional data and/ or account for the hidden Variables in modelling as much as is reasonably practical.                                                                                             | To (a) assess and control for hidden correlations and causal relations in Model datasets and Variables and/ or risks of relations being spurious, ambiguous and/or confounding; and (b) highlight associated risks that might occur in the Product Lifecycle.                         |\n| 13.2.4. | Selection Function                    | Document and assess the propensity of subpopulations and subpopulation members to be (accurately) recorded in Model datasets, with particular care for (i) unrecorded individuals, (ii) Protected Classes, and (iii) survivorship effects. Incorporate the Selection Function in Model development and evaluation in particular during Fairness & Non- Discrimination, Performance Robustness controls. | To (a) assess and control for the accuracy of Model and Model datasets in representing (Sub) populations; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                               |\n\n| 13.2.5.   | Feature Constraints   | Evaluate whether any constraints should be applied to input Features, such as monotonicity or constraints on input Feature interactions in consultation with Domain experts. If determined, utilise identified constraints.   | To (a) ensure that (i) Model Outcomes are maximally interpretable and (ii) Model behavior for individual Model Subjects is consistent with Domain experience; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n|-----------|-----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n\n## 13.3 Development\n\n## Objective\n\nTo (a) ensure that Model design is sufficiently specified to represent Product Domain(s) and the Product Definition(s) as much as is reasonably practical; and (b) minimise the risks of (i) adverse effects from the Model's optimisation leading to unintended loopholes and local optima, and (ii) mis-balancing competing optimisation requirements in Model design and development.\n\n|         |                                                      | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                                                                                                           |\n|---------|------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 13.3.1. | Target Subjectivity                                  | Document and assess whether the Target Feature(s) objectively represent Product Domain(s). If subjective, consider refining Product Definition(s), choosing a different Target Feature, or taking measures to promote the objectivity of Product Outcomes.                                                                                                                                                                                                      | To (a) ensure that Product Outcomes are representative of subpopulations and applications, and are not misinterpreted; (b) ensure that Models are optimized only and precisely according to Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| 13.3.2. | Target Proxies                                       | Document and assess whether the Target Feature(s) are proxies for the true Target(s) of Interest in Product Domain(s). If Target Features are proxies, take measures to ensure and review non- divergence of Product Outcomes with regard to Product Definitions.                                                                                                                                                                                               | To (a) ensure that Product Outcomes are representative of subpopulations and applications, and are not misinterpreted; (b) ensure that Models are optimized only and precisely according to Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| 13.3.3. | Target Proxy vs. True Target of Interest Contrasting | If the Target Feature is a proxy (i) document and assess whether the true Target(s) of Interest correlate with protected attributes and classes, including through hidden systemic Variables as much as is reasonably practical; and (ii) document and assess whether the true Target(s) of Interest and the proxy Target Feature(s) correlate differently with the Model datasets. If true, take measures to mitigate this as much as is reasonably practical. | To (a) ensure that the Model design is oriented to the true Target(s) of Interest; and (b) highlight associated risks that might occur in the lack thereof in the Product Lifecycle.                                                                                                           |\n\n| 13.3.4.   | Heterogeneous Target Variable Simplification   | Document and assess whether the Target Feature is a simplification of, or contains a subset of, true Target(s) of Interest. If true, consider refining Product Definitions, recovering the heterogeneity, or failing that, take measures to mitigate and review this as much as is reasonably practical.                                                                                                                                                                                                                                                                  | Idem Section 11.3.1-2; and to (a) detect and control for risks of generalization and spurious correlation creation.                                                                                                                                                                                         |\n|-----------|------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 13.3.5.   | Cost Function Specification & Optimisation     | Document and assess the risk propensity for - (i) optimizing for subset of objectives to the detriment of other Product objectives, (ii) optimizing for Outcomes that are unintended and/or not aligned with any Product objectives, (iii) feedback loops (when containing nested optimization loops), and (iv) Model confinement in adverse or less- than-optimal parameter or solution space - through Model cost function and optimisation procedures during the Product Lifecycle. If risks occur, take measures to mitigate them as much as is reasonably practical. | To (a) ensure the adequate optimisation of Product Definitions through an assessment of the cost function and optimization procedure; (b) to respect the boundary conditions and requirements set by the Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| 13.3.6.   | Importance Weighting                           | Document and assess whether Model data points are weighted by design or as collateral effect.                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | To (a) ensure the adequate optimisation of Product Definitions through an assessment of the cost function and optimization procedure; (b) to respect the boundary conditions and requirements set by the Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle  |\n| 13.3.7.   | Asymmetric Error Weights                       | Document and assess whether Model errors, and error rates, are weighted asymmetrically in the Model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | To (a) ensure the adequate optimisation of Product Definitions through an assessment of the cost function and optimization procedure; (b) to respect the boundary conditions and requirements set by the Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle  |\n| 13.3.8.   | Feature Weighting                              | Document and assess whether Model features are weighted by design or as collateral effect.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | To (a) ensure the adequate optimisation of Product Definitions through an assessment of the cost function and optimization procedure; (b) to respect the boundary conditions and requirements set by the Product Definitions; and (c) highlight associated risks that might occur in the Product Lifecycle  |\n\n| 13.3.9.   | Output Interpretation(s)     | Document and assess whether the interpretation of the Model Outcomes are clearly, completely and unambiguously defined. If not, take measures to promote Outcome interpretation(s) clarity and completeness as much as is reasonably practical.                    | To (a) guard against the misinterpretation and/or misapplication of Model Outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                 |\n|-----------|------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 13.3.10.  | Time-dependent Data Modeling | Document and assess whether all time- dependent aspects of data generation (including but not limited to gathering, calibration, cleaning, and annotation), data modeling and data usage are specified and incorporated in Model design and Product Definition(s). | To (a) prevent data leakage and other forms of 'time traveling' information leading to inaccurate representations of the data and/or Data Subjects; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## 13.4 Production\n\n## Objective\n\nTo ensure that the Implementation of the Product and Model(s) align with and represent Product Definition(s) and Product Domain(s).\n\n|         |                          | Control:                                                                                                                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                                                                                                                                                                                                         |\n|---------|--------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 13.4.1. | Asymmetric Error Costs   | Document and assess whether Product Domain(s) costs produced by different Model errors types are accounted for in Product implementation and application in software and processes. If not, take measures to ensure that they are.                                                                                              | To (a) ensure that Product Domain(s) and Product Subjects consequences are accurately considered when implementing Product outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                           |\n| 13.4.2. | Output Interpretation(s) | Document and assess whether Product Outcomes can be clearly, completely and unambiguously interpreted by the non-technical parties and whether these interpretations remain representative of Product Definition(s) and Model inner workings. If not, take measures to ensure that they are as much as is reasonably practical. | To (a) prevent (i) misinterpretation of Product Outcomes, (ii) the application of Products in contexts and/or to Subjects for which their appropriateness and/or quality is unconfirmed, unknown, and/or unsatisfactory, (iii) the intentional and/or unintentional misuse of Product components and Outcomes; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## Section 14. Performance Robustness\n\n## Objective:\n\nTo warrant Model Outcomes and prevent unintentional Model behaviour a priori under operational conditions as far as is reasonably practical.\n\n## What is Performance Robustness?\n\nModel robustness is the property of an algorithm that, when tested on a training sample and on a similar testing sample, the performance is similar. In other words, a robust model is one for which the testing error is similar to the training error. Performance robustness takes into account prospective scenarios where (one of more) inputs or assumptions are (drastically) changed due to unforeseen circumstances and, in light of these, the ability of the Model to  to still consistently generate accurate  output.  Put more holistically -\n\nPerformance Robustness means the ability of the Model to generate consistent, accurate results across different sampling tests and in light of changes in operational circumstances.\n\n## Why do we need Performance Robustness?\n\nA Model that is not robust will hopefully not end up being used and deployed. Good performance in training, but significantly worse performance when tested on real data, is one of the reasons many proof-of-concepts do not end up being utilized. A Model which is not robust will inevitably deteriorate over time. Its predictions and recommendations will deviate from the ground truth and the end users will lose trust in the Model and may stop utilizing it altogether. This is the optimistic case. More worrisome is when users of the Model continue to use a poor performing Model and are unaware of its poor accuracy or precision, but still take it into account when making (important) judgement calls. In scenarios where there is no human-in-the-loop, detecting poor performance robustness can be even more difficult and time costly. This will result in more unknown harm, which is naturally hard to detect and determine.   So, it is clear that it is in everyone's interest to ensure the Model's performance robustness.\n\n## How to ensure Performance Robustness?\n\nThough performance robustness needs to be of a certain level to even consider deploying the Model, it is something that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage in isolation.\n\n## 14.1 Product Definitions\n\n## Objective\n\nTo prevent performance loss due to Product Definition changes.\n\n|         |                                 | Control:                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                 |\n|---------|---------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 14.1.1. | Product Definition(s) Stability | Document and assess the stability of historic and prospective Product Definition(s) and Product Aim(s). If unstable, take measures to redefine or, failing that, to correct for or mitigate as much as is reasonably practical. | To (a) ensure that Product Definition(s) and Models remain stable and up-to-date in light of Product Domain Stability; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 14.1.2. | Product Domain Stability        | Document and assess the stability of historic and prospective Product Domain(s). If unstable, revise Product Definition(s) accordingly to ensure Product consistency and stability.                                             | To (a) ensure that Product Definition(s) and Models remain stable and up-to-date in light of Product Domain Stability; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## 14.2 Exploration\n\n## Objective\n\nTo prevent performance loss due to (a) data and/or data definition instability; (b) volatile data elements; and/or (c) prospective increases in scale.\n\n|         |                                    | Control:                                                                                                                                                                                                                                         | Aim:                                                                                                                                                                                                                                                                                                                                                                 |\n|---------|------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 14.2.1. | Data Drift Assessment              | Document and assess historic and prospective changes in data distribution, inclusive of missing and nonsensical data. If data drift is apparent and/or expected in the future, implement mitigating measures as much as is reasonably practical. | To (a) assess and promote the stability of data distributions (data drift); (b) determine the need for data distributions monitoring, risk-based mitigation strategies and responses, drift resistance and adaptation simulations and optimization, and data distribution calibration; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| 14.2.2. | Data Definition Temporal Stability | Document and assess - both technically and conceptually - historic and prospective changes of each data dimension definition. If unstable, consider refining Product Definitions and/or limiting usage of unstable data dimensions.              | To (a) assess and control for the need for Model design adaptation based on data definition stability; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                 |\n\n| 14.2.3.   | Outlier Occurrence Rates                   | Document and assess outliers, their causes, and occurrence rates as a function of their location in data space. If numerous and persistent, include mitigating measures in Model design accordingly.                                                                                                                                                                  | To (a) identify outliers and assess the need for Model design adaptation; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                       |\n|-----------|--------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 14.2.4.   | Selection Function Temporal Stability      | Document and assess the historic and prospective behaviour of Selection Function(s) of Model data. (See Section 13.2.4. - Selection Function for more information.) If unstable, take measures to account for past and future changes, and/or promote the consistency and representativeness of Model datasets and data gathering as much as is reasonably practical. | To (a) assess and control for hard-to-measure changes to the relation between Model datasets and Product Domain(s); (b) identify the risk of hard-to-diagnose Model performance degradation and bias throughout Product Lifecycle (to be controlled by 14.3.6); and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| 14.2.5.   | Data Generating Process Temporal Stability | Document and assess the historic and prospective behaviour of data generating processes, and their influence on the Selection Function. If unstable, take measures to account for past and future changes and/or promote the stability and consistency of data generation processes as much as is reasonably practical.                                               | To (a) assess and control for hard-to-measure changes to the relation between Model datasets and Product Domain(s); (b) identify the risk of hard-to-diagnose Model performance degradation and bias throughout Product Lifecycle (to be controlled by 14.3.6); and (c) highlight associated risks that might occur in the Product Lifecycle  |\n\n## 14.3 Development\n\n## Objective\n\nTo characterize, determine and control for Model performance variation, risks and robustness under live conditions a priori and throughout the Product Lifecycle.\n\n|         |                                     | Control:                                                                                                                                                                                                                          | Aim:                                                                                                                                                                                                  |\n|---------|-------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 14.3.1. | Target Feature Definition Stability | Document and assess - both technically and conceptually - the historic and prospective stability of the Target Feature definition. If unstable, consider refining Product Definitions and/or choosing a different Target Feature. | To (a) assess the need for Model design and Product Definition adaptation based on Target Feature definition stability; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n| 14.3.2.   | Blind Performance Validation               | Document and validate that Model Performance can always be reproduced on never-before-seen hold-out data- subsets and prove that these hold-out data-subsets are never used to guide Model and Product design choices by comparing Model performance on the hold-out dataset. If performance cannot be reproduced on never-before-seen hold-out data-subset, take measures to improve robustness and Model fitting as much as is reasonably practical.                                       | To (a) ensure Model performance robustness against insufficient generalization capabilities on live data (such as overfitting); and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                  |\n|-----------|--------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 14.3.3.   | Error Distributions                        | Document and assess error and/or residual distributions along as many dimensions and/or subsets as is practically feasible. If distributions are too broad and/or too unequal between subsets, improve Model(s).                                                                                                                                                                                                                                                                             | To (a) assess and control for performance influence of data points and/or groups; (b) assess and control for the distribution of errors to influence - (i) performance robustness as a function of data drift, (ii) the systematic performance of minority data-subsets, and (iii) the risks of unacceptable errors and/or catastrophic failure; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| 14.3.4.   | Output Edge Cases                          | Document and assess the causes, occurrence probabilities, overall performance impact of Edge Cases output by Model(s), inclusive of on Model training and design. If their influence is significant, improve model design. If occurrence is high, increase Model, code and data quality control.                                                                                                                                                                                             | To (a) assess and control for the impact of Output Edge Cases on Model design, bugs and performance; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                                             |\n| 14.3.5.   | Performance Root Cause Analysis            | Document and assess Model performance Root Cause Analysis as well as its testing method. If Root Cause Analysis is ineffective, simplify Model and/or increase diagnostics like logging and tracking.                                                                                                                                                                                                                                                                                        | To (a) assess and control for Model performance changes and assist in Model design, development, and debugging; (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                                      |\n| 14.3.6.   | Model Drift & Model Robustness Simulations | Document and perform simulations of Model training and retraining cycles, using historic and synthetic data. Document and assess the effects of temporal changes to, amongst other things, the Selection Function, Data Generating Process and Data Drift on the drift in performance and error distributions of said simulations. If Model drift is apparent, document and perform further simulations for Model drift response optimization, and/or consider refining Product Definitions. | To (a) assess and control for Model propensity for Model drift; (b) determine the robustness of Model performance as a function of data changes; (c) determine appropriate Product response to drift; and (d) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                            |\n\n| 14.3.7.   | Catastrophic Failures                            | Document and assess the prevalence of predictions with High Confidence Values, but large Evaluation Errors. If apparent, improve Model to avoid these, and/or implement processes to mitigate these as much as is reasonably practical.                                                                                                                                                                                                                                                                  | To (a) assess the propensity of the Model for catastrophic failures; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                                   |\n|-----------|--------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 14.3.8.   | Performance Uncertainty and Sensitivity Analysis | Document and assess the probability distribution of the model performance using cross-validation, statistical and simulation techniques under - (a) the assumption that the distribution of training and validation data is representative of the distribution of live data; and (b) multiple realistic variations to the Model data due to both statistical and contextual causes. If Model performance variation is high, improve Model and/or take measures to mitigate performance variation impact. | To (a) assess and control for the range of expected values of Model performance under both constant and changing conditions; (b) assess and control for whether trained model performance is consistent with these ranges; (c) identify main sources of uncertainty and variation for further control; and (d) highlight associated risks that might occur in the Product Lifecycle. |\n| 14.3.9.   | Outlier Handling                                 | Document and assess the effect of various outlier handling procedures on (a) Performance Robustness and (b) Representativeness & Specification. Ensure that only procedures are implemented that positively affect both.                                                                                                                                                                                                                                                                                 | To (a) ensure that outlier removal is not used to heedlessly improve test-time performance only and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                        |\n\n## 14.4 Production\n\n## Objective\n\nTo ensure the future satisfaction of Product Definition(s) through the technical and functional implementation of the Product Model(s) and systems.\n\n|         |                            | Control:                                                                                                                                                                                                                                                                             | Aim:                                                                                                                                                                                                  |\n|---------|----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 14.4.1. | Real World Robustness      | Document and assess potential future change in the applied effects of the Product, such as through diminishing returns and/ or psychological effects. If significant change or decrease is expected, consider refining Product Definitions and/or develop procedures for mitigation. | To (a) assess and control for the variation in applied effects of the Product on Product Definition(s) and performance; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 14.4.2. | Performance Stress Testing | Perform and document experiments designed to attempt to induce failures in the Product and/or Model, for example, but not limited to, by supplying large quantities of or unusual data to the training or inferencing phases.                                                        | To (a) identify and control for risks associated with operational scenario's outside of regimes encountered during Model development.                                                                 |\n\n## Section 15. Monitoring &amp; Maintenance\n\n## Objective:\n\nTo ensure that Products and Models remain within acceptable operational bounds.\n\n## What is Monitoring and maintenance?\n\nMachine learning -\n\nMonitoring refers to the processes of tracking and analysing the performance of a model over time and once it is deployed in production\n\nIt provides early warning signals for performance issues. Maintenance is closely related to monitoring but is a more actionable concept.\n\nMaintenance relates to the activities we need to perform upon detecting or suspecting possible deterioration in the performance of the model.\n\nThough it's a process closely related to Models in production, note that maintenance and monitoring steps need to be designed and addressed in early stages of the Product Lifecycle too.\n\n## Why is Monitoring and maintenance important?\n\nMonitoring and maintenance is not only important but it is a 'must have' for any Product that is deployed in a production environment. Over time, the 'live' data will differ in small or significant ways from the historical data used to train the Model. Trends and preferences will change too. The way certain data sources are measured and coded will also change over time: new data sources are added, while others become unavailable. Therefore, we need to continuously, real-time monitor the Models that are deployed. A Model that is not maintained or updated over time eventually deteriorates, makes errors and could lead to a loss of trust and varying degrees of harm (if the domain in question is  a high-stakes decision domain).\n\n## The How of Monitoring and maintenance\n\nModel monitoring and maintenance though most commonly discussed in the deployment phase, is something that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage in isolation.\n\n## 15.1 Product Definitions\n\n## Objective\n\nTo (a) track Model performance in production; and (b) ensure desired Model performance.\n\n|         |                       | Control:                                                                                                                                                                                                                                                                                                               | Aim:                                                                                                             |\n|---------|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|\n| 15.1.1. | Monitoring Objectives | Based on Product Definition(s), document and assess Product and Model monitoring objectives, inclusive of which Product and/or Model elements need close monitoring attention, such as Model data and code. Document and assess the associated risks of failing to achieve Model and/or Product Monitoring Objectives. | To (a) define Product and Model monitoring objectives; and (b) highlight associated risks for failed monitoring. |\n| 15.1.2. | Monitoring Risks      | Document and assess the associated risks of failing to achieve Monitoring Objectives.                                                                                                                                                                                                                                  | To (a) define Product and Model monitoring risks.                                                                |\n\n## 15.2 Exploration\n\n## Objective\n\nTo (a) define robust Product and/or Model monitoring requirements, inclusive of concerns related to Features and skews of the data; and (b) ensure the continued monitoring of Products and/or Models throughout their lifecycles.\n\n|         |                                                               | Control:                                                                                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                                                                                                                                                      |\n|---------|---------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 15.2.1. | Data Source Mismatch: Training & Production Data              | Define and deploy methods to detect the degree to which data sources and Features, in Model training and production data, match one another. If mismatch is detected, take measures to ensure that data sources and Features are adequately matched in both Model training and production data. | To (a) reduce nonsensical predictions of the Model due to (i) missing data, (ii) lack of data incorporated, or (iii) data measurement scaling, encoding and/or meaning; (b) to reduce the discrepancy between training and production data; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| 15.2.2. | Data Definitions and Measurements: Training & Production Data | Define and deploy methods by which to detect the degree to which data sources in Model training and production have the same definitions and measurement scales .                                                                                                                               | To (a) reduce nonsensical predictions of the Model due to (i) missing data, (ii) lack of data incorporated, or (iii) data measurement scaling, encoding and/or meaning; (b) to reduce the discrepancy between training and production data; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n| 15.2.3. | Data Dependencies and Upstream Changes                        | Derive and implement change assessments for changes in data due to - (i) one or multiple internal or external sources (partial) updates, (ii) substantial source change, and/or (iii) changes in data production and/or delivery.                                                               | To (a) reduce nonsensical predictions of the Model due to (i) missing data, (ii) lack of data incorporated, or (iii) data measurement scaling, encoding and/or meaning; (b) to reduce the discrepancy between training and production data; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n\n| 15.2.4.   | Data Drift Detection                                          | Define and deploy monitoring metrics and thresholds for detecting sudden and/or gradual, short term and/or long term changes in data distributions, giving priority to those that can detect past observed changes. (See Section 12.2.1- Missing and Bad Data Handling for further information). Document and assess distribution families, statistical moments, similarity measures, trends and seasonalities.   | To (a) prevent predictions from diverging from training data and/ or Product Definitions by assessing whether production data is representative of older data; and (b) highlight associated risks that might occur in the Product Lifecycle.                         |\n|-----------|---------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 15.2.5.   | Product and/or Product Domain Changes: Trends and Preferences | Define and deploy (a) monitoring methods for detecting changes in Product Domain(s) and/or Product Definition(s); and (b) timeframes and/ or contextual triggers for reassessment of Product Domain(s) and Product Definition(s) continued stability.                                                                                                                                                             | To (a) ensure Models capture accurate, relevant, and current trends and preferences in Product Domain(s); (b) reduce Model 'blind spots' and better capture malicious events/attempts; and (c) highlight associated risks that might occur in the Product Lifecycle. |\n\n## 15.3 Development\n\n## Objective\n\nTo (a) create metrics for (i) Model performance and (ii) Model performance deterioration; and (b) ensure the continued monitoring of Products and/or Models throughout their lifecycles.\n\n|         |                                                                | Control:                                                                                                                                                          | Aim:                                                                                                                                                                                                                                                                                                                                                                                     |\n|---------|----------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 15.3.1. | Model Performance Deterioration Thresholds                     | Document, assess, and set thresholds for Model performance deterioration in consultation with Stakeholders.                                                       | To (a) ensure clear guidelines and indices of Model failure and performance deterioration; (b) reduce the risk of unacknowledged Model failure and performance deterioration; (c) reduce the likelihood of Model decay, ensure robustness and good performance in terms of selected metrics and scenarios; and (d) highlight associated risks that might occur in the Product Lifecycle. |\n| 15.3.2. | Product Contextual Indicators: Model Performance Deterioration | Document, assess, and set Product and Product Domain specific indicators of Model performance deterioration, inclusive of technical and non-technical indicators. | To (a) ensure clear guidelines and indices of Model failure and performance deterioration; (b) reduce the risk of unacknowledged Model failure and performance deterioration; (c) reduce the likelihood of Model decay, ensure robustness and good performance in terms of selected metrics and scenarios; and (d) highlight associated risks that might occur in the Product Lifecycle. |\n\n| 15.3.3.   | Reactive Model Maintenance Indicators   | Document, assess, and set thresholds for Model failure and reactive maintenance                                                                                                                          | To (a) ensure clear guidelines and indices of Model failure and performance deterioration; (b) reduce the risk of unacknowledged Model failure and performance deterioration; (c) reduce the likelihood of Model decay, ensure robustness and good performance in terms of selected metrics and scenarios; and (d) highlight associated risks that might occur in the Product Lifecycle.   |\n|-----------|-----------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 15.3.4.   | Awareness of feedback loops             | Define and deploy as far as is reasonably practical (a) methods to detect whether feedback loops are occuring, and/or (b) technical and non-technical warning indicators for increased risk of the same. | As per Section 17 - Security: to prevent (in)direct adverse social and environmental effects as a consequence of self-reinforcing interactions with the Model(s); and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                            |\n\n## 15.4 Production\n\n## Objective\n\nTo (a) identify operational maintenance metrics; and (b) ensure timely update, re-train and re-deployment of Model(s).\n\n|         |                                                   | Control:                                                                                                                                                                                                                                                                                                               | Aim:                                                                                                                                                                                                                                                      |\n|---------|---------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 15.4.1. | Operational Performance Thresholds                | Define and set metrics and tolerance intervals for operational performance of Models and Products, such as, amongst other things, latencies, memory size, CPU and GPU usage.                                                                                                                                           | To (a) prevent unavailable and unreliable service; (b) enable quick detection of bugs in the code; (c) ensure smooth integration of the Model with the rest of the systems; and (d) highlight associated risks that might occur in the Product Lifecycle. |\n| 15.4.2. | Continuous Delivery of Metrics: Model Performance | Continuously report on and record metrics about Model performance, predictions, errors, Features, and associated performance metrics to relevant Stakeholders (as decided upon in Section 13.2 _- Representativeness & Specification: Exploration and Section 13.3 - Representativeness & Specification: Development). | To (a) enable rapid identification of Model decay, and/or red flags and bugs in Model and/or data pipelines; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                |\n\n| 15.4.3.   | Model Decay & Data Updates   | Operationalise procedures to mitigate Data Drift and/or Model decay (as described in Section 14.2 _- Performance Robustness: Exploration and Section 14.3 - Performance Robustness: Development).                                                                                                                                                                                                                 | To (a) ensure timely implementation of any changes required in data and/or Modelling pipelines; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                 |\n|-----------|------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 15.4.4.   | Model Re- training           | Operationalise procedures on how Model re-training ought to be conducted as well as approached, inclusive of, amongst other things, - (1) when will (i) a new Model be deployed, and/or (ii) a Model with the same hyperparameters but trained on new data; and/or (2) when operationalizing re-trained Models ought they be run in parallel with older Models and/or do to gracefully decommission older Models. | To (a) ensure timely implementation of any changes required in data and/or Modelling pipelines; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                 |\n| 15.4.5.   | Create Contingency Plans     | Develop and put in place contingency plans in case of technical failures and out-of- bounds behaviour based on (a) bounds and threshold set in other controls; and (b) risk assessment of failure modes.                                                                                                                                                                                                          | To (a) prevent adverse effects from failures and unexpected behaviour by providing clear instructions on roll-back, mitigation and remediation; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## Section 16. Explainability\n\n## Objective:\n\nTo ensure Model functions and outputs are explainable and justifiable as far as is practically reasonable in order to (a) foster explainability for Stakeholders, (b) promote Model trust, (c) facilitate Model debugging and understanding, and (d) promote compliance with existing laws and statutes.\n\n## What do we mean when we refer to Explainability?\n\nThere is not one agreed-upon definition of explainability but the working definition we have adopted is that\n\nExplainability refers to making the behavior and decisions of a complex machine learning model understandable to humans.\n\nClosely related to the concept of explainability is interpretability. Interpretability refers to the degree to which a human can inherently understand the cause of a Model's decision. In other words, interpretability relates to using Models that are transparent and can be inherently understood by humans; while explainability concerns making complex, non-transparent models understandable to humans. Many researchers and practitioners use the terms interchangeably.\n\nTransparency is another closely related concept to explainability and interpretability. It is the broadest of the three. Transparency refers to the openness of the workings and/or processes and/or features of data, Models and the overall project (the Product). Transparency can be both comprehensible or incomprehensible depending on its content. Transparency does not necessarily mean comprehension: this is important and why it differs from explainability and interpretability. Again, transparency just refers to the openness of the workings and/or processes and/or features of data, Models and the overall project - whether technical or not.\n\n## Why is Explainability relevant?\n\nWhen we talk about machine learning used for high-stakes decisions, there is a strong agreement that it is extremely important for the public and for machine learning practitioners to understand the inner workings and decision-making of Models. This is because through such understandings, we can ensure that machine learning is done fairly or, rather, that it does not generate unfair or harmful consequences. To put it more simply, we can ensure human oversight and correction over machine learning operations. Explainability also is very important for promoting trust and social acceptance of machine learning. People do not often trust and accept things they do not understand. Through explainability, we can help people understand machine learning and, in turn, trust it.\n\n## How to apply Explainability?\n\nIn order to generate thorough and thoughtful explainability, it must be considered continuously throughout all stages of the product lifecycle. This means that explainability must be addressed at the (a) Product Definition(s), (b) Exploration, (c) Development and (d) Production stages of machine learning operations.\n\n## 16.1 Product Definitions\n\n## Objective\n\nTo (a) ensure the transparency of Product Definitions; (b) foster multi-stakeholder buy-in through explanations; and (c) reduce ethical risks in Product Definition(s) decision-making and Model Runs.\n\n|         |                                         | Control:                                                                                                                                                                                                                                                                                                              | Aim:                                                                                                                                                                       |\n|---------|-----------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 16.1.1. | Explainability Aims                     | Having consideration for (a) Product Definition(s), (b) the explanations and/or transparency sought, (c) the Model adopted, and (d) datasets used, document and assess the explainability aims of the Model.                                                                                                          | To (a) clearly document the explainability and transparency aims of the Model; and (b) highlight associated risks that might occur in the Product Lifecycle.               |\n| 16.1.2. | Explainability Stakeholder              | Document and assess the internal and external Stakeholders affected by the Model.                                                                                                                                                                                                                                     | To identify the Model explainability Stakeholders; and (b) highlight associated risks that might occur in the Product Lifecycle.                                           |\n| 16.1.3. | Explainability Risks Assessment         | Document and assess the individual risks of failing to provide model explainability, inclusive of a legal liability and Explainability Stakeholders mistrust.                                                                                                                                                         | To identify the risks of failing to provide Model explainability; and (b) highlight associated risks that might occur in the Product Lifecycle.                            |\n| 16.1.4. | Legal Requirements for Interpretability | Document and assess any specific legal requirements for Explainability in consultation with legal experts.                                                                                                                                                                                                            | To (a) ensure that minimum standards for explainability are met and legal risk is addressed; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 16.1.5. | Explainability Requirements             | Document and assess the explainability and transparency requirements and levels in light of (a) Explainability Aims, (b) Explainability Stakeholders, and (c) Explainability Risks, taking care that the elicitation of said requirements involves appropriate guidance, education and understanding of Stakeholders. | To (a) clearly document the explainability requirements of the Model; and (b) highlight associated risks that might occur in the Product Lifecycle.                        |\n\n## 16.2 Exploration\n\n## Objective\n\nTo identify and document Model explainability and transparency requirements, inclusive of Stakeholder needs.\n\n|         |                                             | Control:                                                                                                                                                                                                                                                                                                                                                                                 | Aim:                                                                                                                                                                                                                |\n|---------|---------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 16.2.1. | Stakeholder Appraisal                       | Document and conduct (a) ad-hoc interviews, (b) structured surveys and/ or (c) workshops with Explainability Stakeholders about their Model and Product concerns and literacy.                                                                                                                                                                                                           | To (a) generate Explainability Stakeholders analytics in order to map Model explainability requirements and demands; and (b) highlight associated risks that might occur in the Product Lifecycle.                  |\n| 16.2.2. | Stakeholder Appraisal Analysis              | Document, analyse and map the outcomes of the Stakeholder Appraisal against the Explainability Aims and Explainability Risks.                                                                                                                                                                                                                                                            | To (a) map and analyse Model explainability requirements and demands in light of the needs of Explainability Stakeholders; and (b) highlight associated risks that might occur in the Product Lifecycle.            |\n| 16.2.3. | Explainability Matrix                       | Document, assess, and derive a matrix evaluating and ranking the metrics and/ or criteria of explanations needed for based on the (a) Stakeholder Appraisal Analyse, (b) Explainability Aims, (c) Explainability Risks, and (d) Explainability Requirements, inclusive of explanations accuracy, fidelity, consistency, stability, comprehensibility, certainty, and representativeness. | To (a) derive a clear matrix from which to assess Model explainability requirements; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                  |\n| 16.2.4. | Explainability Feature Selection            | Document and analyse the degree of Feature explainability needed in light of the Explainability Matrix.                                                                                                                                                                                                                                                                                  | To (a) identify the requisite degree of Feature explainability needed; and (b) highlight associated risks that might occur in the Product Lifecycle, such as later stage Model retraining due to feature ambiguity. |\n| 16.2.5. | Explainability Modelling Mapping & Analysis | Document and analyse the technical needs of Model explainability in light of the Explainability Matrix, inclusive of considerations of global vs. local explainability and/or pre-modelling explainability, modelling explainability, and post-hoc modelling explainability                                                                                                              | To (a) identify the technical needs of the Explainability Matrix; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                     |\n| 16.2.6. | Explanation Frequency & Delivery Assessment | Document and assess the frequency, most suitable and practically reasonable methods of communicating Model explainability in light of the Explainability Matrix and Stakeholder Appraisal Analysis.                                                                                                                                                                                      | To (a) identify the most appropriate method of communicating Model explainability in order to promote explainability comprehension; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n\n## 16.3 Development\n\n## Objective\n\nTo ensure that Model design represents the explainability requirements and demands of transparency aims as much as is reasonably practical.\n\n|         |                                                       | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Aim:                                                                                                                                                                                                                                                             |\n|---------|-------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 16.3.1. | Explainability Feature Selection Assessment           | Conduct a Feature analysis of the Explainability Feature Selection in order to remove correlated and dependent Features.                                                                                                                                                                                                                                                                                                                                                | To (a) interrogate the assumption of zero Feature dependency in explainability modelling; (b) prevent misleading Model explainability and transparency; and (c) highlight associated risks that might occur in the Product Lifecycle.                            |\n| 16.3.2. | Global Explainability Model Run                       | Document and run as many types of global explainability Models as is reasonably practical, such as Feature importances, Feature interactions, global surrogate Models, perturbation-based techniques or gradient-based techniques. When there is doubt about the stability of the techniques being used, test their quality through alternative parameterizations or by comparing across techniques.                                                                    | To (a) generate global explainability of the model; (b) help promote model debugging; (c) ensure explainability fidelity and stability through numerous explainability model runs; and (d) highlight associated risks that might occur in the Product Lifecycle. |\n| 16.3.3. | Local Explainability Model Run                        | Document and run as many types of local explainability Models as is reasonably practical, such as perturbation-based techniques or gradient-based techniques or, for more specific examples, Local Interpretable Model-Agnostic Explanations (LIME), SHAP values, Anchor explanations amongst others. When there is doubt about the stability of the techniques being used, test their quality through alternative parameterizations or by comparing across techniques. | To (a) generate global explainability of the model; (b) help promote model debugging; (c) ensure explainability fidelity and stability through numerous explainability model runs; and (d) highlight associated risks that might occur in the Product Lifecycle. |\n| 16.3.4. | Visual Explanations Assessment                        | Develop visual aids to present and represent Model explainability and transparency insights, such as Tabular Graphics, Partial Dependency Plots, Individual Conditional Expectations, and/or Accumulated Local Effects plot.                                                                                                                                                                                                                                            | To promote explainability comprehension.                                                                                                                                                                                                                         |\n| 16.3.5. | Example-based and Contrastive Explanations Assessment | Develop example-based and contrastive explanations to present and represent Model explainability insights, such as the underlying distribution of the data or select particular instances.                                                                                                                                                                                                                                                                              | To promote explainability comprehension, such as of complex data distributions and/ or datasets for Explainability audiences.                                                                                                                                    |\n\n## 16.4 Production\n\n## Objective\n\nTo monitor and track the performance of the explanations and trigger when any of the explainability approaches need to be re-trained.\n\n|         |                                          | Control:                                                                                                                                                                                                                   | Aim:                                                                                                                                           |\n|---------|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|\n| 16.4.1. | Explainability Model Thresholds          | Set clear performance thresholds and limitations for explainability Model(s).                                                                                                                                              | To (a) define parameters for the continued suitability and performance of explainability Model(s); and (b) highlight associated risks.         |\n| 16.4.2. | Explainability Model Review & Monitoring | Periodically, or when significant Model changes occur, review implemented explainability Model(s) in light of Explainability Model Thresholds.                                                                             | To (a) ensure the continued suitability and performance of explainability Model(s) and their explanations; and (b) highlight associated risks. |\n| 16.4.3. | Explanation Tracking & Monitoring        | Document and conduct (a) ad-hoc interviews, (b) structured surveys, and/or (c) workshops with Explainability Stakeholders on explanations provided and adjust outcomes in Section 14 - Performance Robustness accordingly. | To ensure the continued effectiveness and suitability of provided Model explanations.                                                          |", "fetched_at_utc": "2026-02-09T13:49:41Z", "sha256": "434b15725995b73f586efd12c60666af85c0e81e9050549e764ea2a4bc13eaab", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-32-62.pdf", "file_size": 945946, "mtime": 1770576471, "docling_errors": []}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-63-87-8f7a10124ab7", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-63-87.pdf", "title": "FBPML_TechnicalBP_V1.0.0-63-87", "text": "## Section 17. Security\n\n## Objective:\n\nTo (a) prevent adversarial actions against, and encourage graceful failures for, Products and/or Models; (b) avert malicious extraction of Models, data and/or intellectual property; (c) prevent Model based physical and/or irreparable harms; and (d) prevent erosion of trust in Outputs or methods.\n\n## What do we mean when we refer to Security?\n\nSecurity is broadly defined as the state of being free from danger or threat. Building on this definition, within the context of machine learning -\n\nSecurity refers to the state of ensuring that machine learning Products and/or Models are free from adversarial danger, threat or attacks.\n\nAdversarial danger, threat or attacks are understood as the malicious intent to negatively impact machine learning Products' and/or Models' functionality and/or metrics without organisation consent, whether threatened or actualised. If an organisation does consent to any such activity, this is - rather - a form of penetration testing and/or security analysis, as opposed to an adversarial danger, threat or attack.\n\n## Why is Security relevant?\n\nMachine learning Product and/or Model security is imperative to ensure operational robust performance. Without the ability to secure the Product's and/or Model's integrity from adversarial danger, threat or attack, malicious third parties can use an organisation's Products and Models to either unlawfully enrich themselves or, more seriously, cause operational environment harms, including death and/or destruction. These are intolerable risks as they undermine organisation, societal and machine learning trust and confidence.\n\n## How to apply Security?\n\nIn order to generate thorough and thoughtful security, it must be considered continuously throughout all stages of the product lifecycle. This means that security must be addressed at the (a) Product Definition(s), (b) Exploration &amp; Development, (c) Production and (d) Confidence &amp; Trust stages of machine learning operations.\n\n## 17.1 Product Definitions\n\n## Objective\n\nTo identify and control for Adversarial risks and motives based on Product Definition, characterized by adversary goals.\n\n|         |                                | Control:                                                                                                                                                                                                                                                                                                             | Aim:                                                                                                                                                                                                                                                                                                                                                                              |\n|---------|--------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 17.1.1. | Exfiltration Attacks           | Document and assess whether the data employed and gathered by the Product, and the intellectual property generated possess value for potential adversarial actors.                                                                                                                                                   | To (a) identify the risks associated with (i) Product Subject physical, financial, social and psychological wellbeing, and (ii) Organization financial wellbeing; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                   |\n| 17.1.2. | Evasion Attacks                | Document and assess whether Product Subjects gain advantage from evading and/or manipulating the Product Outputs. Document and assess whether adversarial actors stand to gain advantage in manipulating Product Subject by evading and/or manipulating Product Output.                                              | To (a) identify the risks associated with Product Output manipulation in regard to malicious and nefarious motives; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                 |\n| 17.1.3. | Targeted Sabotage              | Document and assess whether adversarial actors can cause harm to specific targeted Product Subjects by manipulating Product Outputs.                                                                                                                                                                                 | Document and assess whether adversarial actors can cause harm to specific targeted Product Subjects by manipulating Product Outputs.                                                                                                                                                                                                                                              |\n| 17.1.4. | Performance Degradation Attack | Document and assess whether a malicious performance degradation for a specific (Sub) population can cause harm to that (Sub) population. Document and assess whether general performance degradation can cause harm to society, Product Subjects, the Organization, the Domain and/or the field of Machine Learning. | To (a) identify the risks in regard to (i) Product Subjects' physical, financial, social and psychological wellbeing, (ii) the Organization's financial and reputational wellbeing, (iii) society- wide environmental, social and economic wellbeing, and (iv) the Domains' reputational wellbeing; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## 17.2. Exploration &amp; Development\n\n## Objective\n\nTo identify and control for Adversarial Risks based on and originating in Model properties and/or Model data properties.\n\n|         |                                   | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Aim:                                                                                                                                                                                                                                                                                                                                                               |\n|---------|-----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 17.2.1. | Data Poisoning Assessment         | Document and assess the ease and extent with which adversarial actors may influence training data through manipulating and/or introducing - (i) raw data; (ii) annotation processes; (iii) new data points; (iv) data gathering systems (like sensors); (v) metadata; and/or (vi) multiple components thereof simultaneously. If this constitutes an elevated risk, document, assess and implement measurements that can be taken to detect and/or prevent the above manipulation of training data. | To (a) prevent adversarial actors from seeding susceptibility to Evasion Attacks, Targeted Sabotage and Performance Degradation Attacks by way of (i) introducing hard to detect triggers, (ii) increasing noise, and/or (iii) occluding or otherwise degrading information content; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 17.2.2. | Public Datasets                   | Employ public datasets whose characteristics and Error Rates are well known as a benchmark and/or make the Product evaluation results public.                                                                                                                                                                                                                                                                                                                                                       | To (a) increase the probability of detection adversarial attacks, such as Data Poisoning, by enabling comparison with and by public resources; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                       |\n| 17.2.3. | Data Exfiltration Susceptibility  | Document and assess the susceptibility of the Model to data Exfiltration Attacks through - (i) the leakage of (parts of) input data through Model Output; (ii) Model memorization of training data that may be exposed through Model output; (iii) the inclusion by design of (some) training data in stored Model artifacts; and/or (iv) repeated querying of the Model.                                                                                                                           | To (a) warrant and control the risk of Model data theft; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                             |\n| 17.2.4. | Model Exfiltration Susceptibility | Document and assess the susceptibility of Models to Exfiltration Attacks with the aim of obtaining a copy, or approximation of, the Model or other Organization intellectual property, through repeated querying of the Model and analysing the obtained results and confidence scores.                                                                                                                                                                                                             | To (a) warrant and control the risk of Model and intellectual property theft; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                        |\n\n| 17.2.5.   | Exfiltration Defence               | To reduce susceptibility of Exfiltration Attacks, (a) make Exfiltration Attacks computationally expensive; (b) remove as much as possible information from Model Output; (c) add noise to Model Outputs through techniques such as differential privacy; (d) limit querying possibilities in volume and/or scope; and/or (e) change Model architecture.                                                                                               | To (a) warrant and control the risk of Exfiltration Attacks; and (b) highlight associated risks that might occur in the Product Lifecycle.                                            |\n|-----------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 17.2.6.   | Adversarial Input Susceptibility   | Document and assess the susceptibility of Models to be effectively influenced by manipulated (inferencing) input. Reduce this susceptibility by (a) increasing the representational robustness (f.e. through more complete embeddings or latent space representation); and/or (b) applying robust transformations (possibly cryptographic) and cleaning.                                                                                              | To (a) warrant the control of the risk of Evasion and Sabotage Attacks, including Adversarial Examples; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 17.2.7.   | Filtering Susceptibility           | If sufficient potential motive has been determined for adversarial attack, document and assess the specific susceptibility of the pre-processing filtering procedures of Models being evaded by tailored inputs, based on the information available to an adversarial attacker about these procedures; in addition to the general Susceptibility Assessment. Increase the robustness of this filtering as far as practically feasible.                | To (a) warrant the control of the risk of Evasion and Sabotage Attacks, including Adversarial Examples; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 17.2.8.   | Training Susceptibility            | If sufficient potential motives have been determined for adversarial attack, document and assess the specific susceptibility of Model training to attack through the manipulation of (a) the partitioning of train, validation and test sets, and/or (b) Models' hyperparameters; in addition to the general Susceptibility Assessment. Implement more strict access control on production-grade training and hyperparameter optimization procedures. | To (a) warrant the control of the risk of Evasion, Sabotage and Performance Degradation Attacks; and (b) highlight associated risks that might occur in the Product Lifecycle.        |\n| 17.2.9.   | Adversarial Example Susceptibility | If sufficient potential motives have been determined for adversarial attack, document and assess the specific susceptibility of Models to Adversarial Examples by considering - (a) sparse or empty regions of the input space, and/or (b) Model architectures; in addition to the general Susceptibility Assessment. Document and implement specific protective measures, such as but not limited to adversarial training.                           | To (a) warrant the control of the risk of Evasion Attacks, specifically Adversarial Examples; and (b) highlight associated risks that might occur in the Product Lifecycle.           |\n\n| 17.2.10.   | Adversarial Defence                             | If sufficient potential motive and susceptibility to adversarial attacks have been determined, implement as far as reasonably practical - (a) data testing methods for detection of outside influence on input and Output Data; (b) reproducibility; (c) increase redundancy by incorporating multimodal input; and/or (d) periodic resets or cleaning of Models and data.                                                                                                                                                     | To (a) warrant and control the risk of Adversarial Attacks in general; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n|------------|-------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 17.2.11.   | General Susceptibility - Information            | Document, assess and control the general susceptibility to attack due to information obtainable by attackers, by considering (a) sensitivity to input noise and/or noise as a protective measure; (b) the amount of information an adversarial actor may obtain from over-extensive logging; and/or (c) whether providing confidence scores as Output is beneficial to adversarial actors.                                                                                                                                     | To (a) warrant and control the risk of Adversarial Attacks in general; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n| 17.2.12.   | General Susceptibility - Exploitability         | Document, assess and control the general Model susceptibility to attack due to exploitable properties of Models, considering (a) overfit or highly sensitivity Models and Model hyperparameters are easier to attack; (b) an over-reliance on gradient methods that make Models more predictable and inspectable; (c) Models may be pushed past their applicability boundaries if input is not validated; and (d) non- random random number generators might be replaced by cryptographically secure random number generators. | To (a) warrant and control the risk of Adversarial Attacks in general; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n| 17.2.13.   | General Susceptibility - Detection              | Document, assess and control the capability to detect attacks through the ability to understand when Model behaviour is anomalous by (a) decreasing Model opaqueness, and/or (b) increasing Model robustness.                                                                                                                                                                                                                                                                                                                  | To (a) warrant and control the risk of Adversarial Attacks in general; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n| 17.2.14.   | Open Source and Transfer Learning Vulnerability | Document the correspondence between potential attack motives and attack susceptibility posed by using, re-using or employing for transfer learning open source Models, Model weights, and/or Model parameters through - (a) maliciously inserted behaviour and/ or code ('trojans'), (b) the ability of an adversarial actor to investigate and attack open source Models unhindered; and (c) improper (re-)use. Consider using non-open source Models or making significant changes aimed at reducing susceptibility.         | To (a) warrant and control the risk of Adversarial Attacks in general; and (b) highlight associated risks that might occur in the Product Lifecycle.   |\n\n## 17.3 Production\n\n## Objective\n\nTo identify and control for Adversarial Risks based on and/or originating in Models production.\n\n|         |                                | Control:                                                                                                                                                                                                                                                                                                                                                                                                    | Aim:                                                                                                                                                                                                                                                                                                                                                                                                      |\n|---------|--------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 17.3.1. | IT Security                    | Traditional IT security practices are referred to. Areas of particular importance to ML- based systems include - (a) backdoor access to the Product, in particular the components vulnerable to attack risk as identified in other controls; (b) remote host servers vulnerability; (c) hardened and isolated systems; (d) malicious insiders (e)man-in- the-middle attacks; and/or (f) denial-of- service. | Traditional IT security practices are referred to. Areas of particular importance to ML-based systems include - (a) backdoor access to the Product, in particular the components vulnerable to attack risk as identified in other controls; (b) remote host servers vulnerability; (c) hardened and isolated systems; (d) malicious insiders (e)man-in-the-middle attacks; and/or (f) denial-of- service. |\n| 17.3.2. | Periodic Review and Validation | If motive and risk for Adversarial Attack is high, perform more stringent and frequent review and validation activities.                                                                                                                                                                                                                                                                                    | To (a) warrant and control the risk of Adversarial Attacks in general by increasing detection probability and fixing vulnerabilities quickly; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                               |\n| 17.3.3. | input and Output Vulnerability | Document and assess the vulnerability of the Product and related systems to direct manipulation of inputs and Outputs. Direct Output manipulation if possible is the most straightforward, simplest, cheapest and hardest to detect attack                                                                                                                                                                  | To (a) create redundancy with input and inferencing hyperparameter susceptibility; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                                          |\n| 17.3.4. | Defense Strength Assessment    | Document and assess the strength and weaknesses of all layers of defense against attacks and identify the weakest links.                                                                                                                                                                                                                                                                                    | To (a) build defense in depth; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                                                                                                                                                                                                                              |\n\n## 17.4 Confidence &amp; Trust\n\n## Objective\n\nTo identify and control for Adversarial Risks based on and/or originating in Product trust and confidence.\n\n|         |                 | Control:                                                                                                                                                                                                                                                                                                                                                                                                                   | Aim:                                                                                                                                                                                                                                                 |\n|---------|-----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 17.4.1. | Trust Erosion   | Document and assess the potential impact on trust from adversarial and defacement attacks, and establish a strategy to mitigate trust erosion in case of successful attacks.                                                                                                                                                                                                                                               | To (a) prevent erosion of trust in Product Outputs, the Product, the Organization, and/or Domains from preventing beneficial Products and technologies to be employed; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 17.4.2. | Confidence      | Document and assess the degree of over- and under-confidence in the Product output by Product Team, Stakeholder(s) and End Users. Encourage an appropriate level of confidence through education and self- reflection. Note: Underconfidence will lead to users over-ruling the Product in unexpected ways. Overconfidence leads to lower scrutiny and therefore lowers the chance of detection and prevention of attacks. | To (a) balance the risk of compromising Product effects against reduced vigilance; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                                                     |\n| 17.4.3. | Warning Fatigue | Document and assess the frequency of warnings and alerts provided to Product operators, maintainers, and Product Subjects, and refine the thresholds and processes such that no over-exposure to alerts can lead to systematic ignoring of alerts.                                                                                                                                                                         | To (a) prevent an overexposure to alerts that can lead to ignoring serious defects and incidents, causing harm; and (b) highlight associated risks that might occur in the Product Lifecycle.                                                        |\n\n## Section 18. Safety\n\n## Objective:\n\nTo (a) prevent Model-based physical and/or irreparable harms; and (b) identify and mitigate risks due to Product failures, including Model failures, IT failures, and process failures.\n\n## What do we mean when we refer to Safety?\n\nWhen referring to safety in the context of machine learning we mean-\n\nSafety means the protection of the operational environment - and its subjects - from negative physical and/or other harms that might result from machine learning Products and/or Models, either directly or indirectly.\n\nPut slightly differently, when we discuss safety we are not talking about the safety of machine learning Products and Models (called, rather, security), but, instead, the operational environment within which machine learning Products and Models operate. Specifically, the harms and risks that machine learning Products and Models might cause for these environments and their subjects. For example, an autonomous vehicle crashing and causing injury, death, or destruction.\n\n## Why is Safety important?\n\nMachine learning Product and Model safety is imperative to ensure the integrity of the operational environment. Without such safety, machine learning Products and Models can cause grave operational environment harms, such as physical injury or, at worst, death. These are intolerable risks as they undermine organisation, societal and machine learning trust and confidence. Moreover, they cause irreparable real damage in the real world.\n\n## How to apply Safety?\n\nIn order to generate thorough and thoughtful safety, it must be considered continuously throughout all stages of the product lifecycle. This means that safety must be addressed at the (a) Product Definition(s), (b) Exploration, (c) Development and (d) Production stages of machine learning operations.\n\n## 18.1 Product Definitions\n\n## Objective\n\nTo establish the appropriate safety-oriented attitudes based on first principles and Product Definitions.\n\n|         |                                    | Control:                                                                                                                                                                                                                                                                                                 | Aim:                                                                                                                                                                                                                              |\n|---------|------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 18.1.1. | Physical and Irreparable Harm Risk | Document and assess whether any likely failure modes can cause physical and/or irreparable harm, based on the Product Definitions. If such is the case, warrant increased oversight and attention throughout the Product Lifecycle to risks and controls in general and from this section in particular. | To warrant the necessary amount of control and resources throughout the Product Lifecycle with regard to preventing and mitigating significant threats to individuals' physical, financial, social, and psychological well being. |\n| 18.1.2. | Domain-first Humble Culture        | Document and establish tenents for Product Team culture to promote risk awareness and prevent blind spots, inclusive of (a) put Domain expertise central; (b) never assume only positive effects; (c) admit uncertainty when assessing impacts.                                                          | To promote risk awareness and prevent blindspots in analysing failure modes and other safety related controls and (b) highlight associated risks that might occur in the Product Lifecycle.                                       |\n\n## 18.2 Exploration\n\n## Objective\n\nTo start the process of identifying,specifying and controlling (potential) risks and failures modes of the Model(s) and Product based on research and exploration, and sustain this process throughout the Product Lifecycle.\n\n|         |                        | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Aim:                                                                                                                                                               |\n|---------|------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 18.2.1. | Forecast Failure Modes | (a) Document and assess continuously throughout the Product Lifecycle all potential failure modes that can be identified through - (i) researching past failures; and/or (ii) interrogating all components or product and context with an open mind; (b) rank identified failure modes according to likelihood and severity; (c) prepare for and mitigate these risks as far as is reasonably practical in order of risk throughout the Product Lifecycle. | To (a) reduce harmful consequences of failures through anticipation and preparation; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n| 18.2.2. | Prediction Limits      | Document and assess with a diversity of Stakeholders the real limitations on the Product and Model Outcomes that ought be strictly enforced in order to prevent physical and/or irreparable harm and/or other Failure Modes.                                                                                                                                                                                                                               | To prevent Model and Product Outcomes from violating clear, fixed and safe operating bounds.                                                                       |\n| 18.2.3. | Surprise Diary         | Document continuously throughout the Product Lifecycle all surprising findings and occurrences.                                                                                                                                                                                                                                                                                                                                                            | To discover and subsequently control for previously unknown or unanticipated failures modes.                                                                       |\n\n## 18.3 Development\n\n## Objective\n\nTo control Safety risks and failure modes based on testing and controlling Model and Product technical components.\n\n|         |                              | Control:                                                                                                                 | Aim:                                                                                                                                                        |\n|---------|------------------------------|--------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 18.3.1. | General IT Testing Practices | Adhere to all traditional IT/Software Testing best practices.                                                            | To warrant and control the risk of failures due to code, software and other IT mistakes in general.                                                         |\n| 18.3.2. | Testing by Domain Experts    | Document and perform testing of the Model(s) and Product by Domain experts.                                              | To warrant that Product and Product Safety are tested against the most relevant requirements and prevent blind spots caused by lack of multidisciplinarity. |\n| 18.3.3. | Algorithm Benchmarking       | Document and perform benchmark testing of Models and Model code against well- known, trusted and/or simpler Models/code. | To warrant the correct implementation of Models and code, and safeguard reproducibility in general.                                                         |\n\n## 18.4 Production\n\n## Objective\n\nTo control for Safety risks and failure modes and prevent physical and/or irreparable harm by performing assessments and implementing measures at the systemic and organisational level.\n\n|         |                            | Control:                                                                                                                                                                                                                                                                                                                                                                                                        | Aim:                                                                                                                                                                                   |\n|---------|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 18.4.1. | System Failure Propagation | Document and assess how failures in Models and Product components propagate to other components and other systems, and what damage they may cause there. Incorporate such information in Failure Mode risk assessments and implementation of Graceful Failures and Kill Switches.                                                                                                                               | To (a) prevent blind spots and cascading failures and (b) provide essential input for creating mitigation measures with a minimum of uncontrolled side- effects.                       |\n| 18.4.2. | Graceful Failure           | Document and assess whether (i) Model errors, (ii) Model failures, (iii) Product failures, (iv) IT failures, and/or (v) process and implementation failures - whether caused by attack or not - can result in physical or irreparable harm to humans, society and/or the environment. If present, mitigate these risks by implementing technological and/or process measures that make these failures graceful. | To identify risks and mitigating measures throughout the Product Lifecycle with regard to significant threats to individuals' physical, financial, social and psychological wellbeing. |\n\n| 18.4.3.   | Kill Switch               | Document and implement Kill Switches according to the findings of all previous controls, taking into account (a) instructions and procedures for engaging the Kill Switch; (b) who is/are responsible for engaging the Kill Switch; (c) what impacts the engagement of the Kill Switch has on users, other parts of the Product and other systems.   | Document and implement Kill Switches according to the findings of all previous controls, taking into account (a) instructions and procedures for engaging the Kill Switch; (b) who is/are responsible for engaging the Kill Switch; (c) what impacts the engagement of the Kill Switch has on users, other parts of the Product and other systems.   |\n|-----------|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 18.4.4.   | Safety Stress Testing     | Document and perform scenario-based stress testing of Product in Domain, Society and Environmental contexts, for realistic but rare high-impact scenarios, recording the Product's reaction to and influence on the Domain, Society and Environment.                                                                                                 | To control and prepare for worst- case scenarios in the context of rapid and/or large changes in Domain, in Society or in Environment.                                                                                                                                                                                                               |\n| 18.4.5.   | Product Incident Response | Document and prepare Product-specific implementation of the Organisation Incident Response Plan insofar as that does not cover the Product's specific risks, if appropriate.                                                                                                                                                                         | To (a) control for and contain Product Incidents; (b) minimize harms stemming from Product Incidents; (c) repair harms caused by Product Incidents; and (d) incorporate lessons learned.                                                                                                                                                             |\n\n## Section 19. Human-Centred Design\n\n## Objective:\n\nTo ensure (a) building desirable solutions; (b) human control over Products and Models; and (c) that individuals affected by Product and Model outputs can obtain redress.\n\n## What is Human-centric Design?\n\nHuman-centric (or also called human-centered) Design is a creative approach to problem-solving, by involving the human perspective in all steps of problem solving.\n\nIn the context of machine learning and the Model framework, Human-centric Design makes you stay focused on the user when designing with ML, therefore building desirable solutions for your target users. Moreover, it also ensures that the right stakeholders are involved throughout the whole design and development process and helps to properly identify the right opportunity areas. Lastly, human-centric design encompasses the extent to which humans have control over the Model and its output as well as the degree to which humans can obtain redress, if they are affected by the Model.\n\n## Why Human-centric Design?\n\nIncorporating Human-centric Design in the Product is vital. It ensures the Model is not built in isolation but is integrated with other problems and, most of all, that it helps solve the right questions. Having the right Stakeholders (beyond the technical teams) involved in the whole Model lifecycle translates to higher trust levels in the Model, increases the rate of adoption, as well as results in more human-friendly and creative solutions. Not having the human-centric part of the Model will inevitably result in an inferior Model - and one which very likely end up on a 'shelf' and, therefore, not be applied in practice.\n\n## How to ensure Human-centric Design?\n\nHuman-centric Design is something that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage in isolation.\n\n## 19.1 Product Definitions\n\nObjective\n\nTo discover and gain insight so that the Product and Model(s) will solve the right problems, designed for human needs and values, before building it.\n\n|         |                                 | Control:                                                                                                                                                                                                                                                                         | Aim:                                                                                                                                                                                                                                                               |\n|---------|---------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 19.1.1. | Human Centered Machine Learning | Incorporate the human (non-technical) perspective in your (technical) process of exploration, development and production by applying user research, design thinking, prototyping and rapid feedback, and human factors when defining a usable product or model.                  | To (a) ensure that Product(s) and Model(s) are not only feasible and viable, but also align with a human needs; and (b) highlight associated risks failing such.                                                                                                   |\n| 19.1.2. | UX (or user) research           | Focus on understanding user behaviours, needs, and motivations through observation techniques, task analysis, user interviews, and other research methodologies.                                                                                                                 | To prevent (a) a focus on technology from overshadowing a focus on problem solving; and (b) cognitive biases from adverse influence Product and Model design.                                                                                                      |\n| 19.1.3. | Design for Human values         | Include activities for (a) the identification of societal values, (b) deciding on a moral deliberation approach (e.g. through algorithms, user control or regulation), and (c) methods to link values to formal system requirements (e.g. value sensitive design (VSD) mapping). | To reflect societal concerns about the ethics of AI, and ensure that AI systems are developed responsibly, incorporating social, ethical values and ensuring that systems will uphold human values. The moral quality of a technology depends on its consequences. |\n\n## 19.2 Exploration\n\nObjective\n\nTo (a) cluster, (b) find insights and (c) define the right opportunity area, ensuring to focus on the right questions to solve in preparation for the development and production phase.\n\n|         |                 | Control:                                                                                                                                                                                                                                                                                                         | Aim:                                                                                      |\n|---------|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\n| 19.2.1. | Design Thinking | Ensure an iterative development process by (a) empathize: research your users' needs, (b) define: state your users' most important needs and problems to solve, (c) ideate: challenge assumptions and create ideas, (d) prototype: start to create solutions and (e) test: gather user feedback early and often. | To let data scientists organise and strategise their next steps in the exploratory phase. |\n\n| 19.2.2.   | Ethical assessment                                           | Discuss with your team to what extend (a) the AI product actively or passively discriminates against groups of people in a harmful way; (b) everyone involved in the development and use of the AI product understands, accepts and is able to exercise their rights and responsibilities; (c) the intended users of an AI product can meaningfully understand the purpose of the product, how it works, and (where applicable) how specific decisions were made.   | Discuss with your team to what extend (a) the AI product actively or passively discriminates against groups of people in a harmful way; (b) everyone involved in the development and use of the AI product understands, accepts and is able to exercise their rights and responsibilities; (c) the intended users of an AI product can meaningfully understand the purpose of the product, how it works, and (where applicable) how specific decisions were made.   |\n|-----------|--------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 19.2.3.   | Estimating the value vs effort of possible opportunity areas | Explore the details of what mental Models and expectations people might bring when interacting with an ML system as well as what data would be needed for that system. E.g. an Impact Matrix.                                                                                                                                                                                                                                                                       | To reveal the automatic assumptions people will bring to an ML-powered product, to be used as prompts for a product team discussion or as stimuli in user research. (See also Section 4.11 - User Experience Mapping.)                                                                                                                                                                                                                                              |\n\n## 19.3 Development\n\n## Objective\n\nTo (a) ensure rapid iteration and targeted feedback from relevant Stakeholders, allowing a larger range of possible solutions to be considered in the selection process. (b) Increase the creativity and options considered, while avoiding avoiding personal biases and/or pigeon-holing a solution.\n\n|         |                                                      | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Aim:                                                                                                                                                                                                                                                                                                                 |\n|---------|------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 19.3.1. | Prototyping                                          | 1: Focus on quick and minimum viable prototypes that offer enough tangibility to find out whether they solve the initial problem or answer the initial question. Document how test participants react and what assumptions they make when they 'use' your mockup. 2: Design a so-called 'Wizard of Oz' test; have participants interact with what they believe to be an autonomous system, but which is actually being controlled by a human (usually a team member) | To gain early feedback (without having to actually have build an ML product) needed to (a) adjust or pivot your Products(s) and/or Model(s) thus ensuring business viability; and/or (b) assess the cost and benefits of potential features with more validity than using dummy examples or conceptual descriptions. |\n| 19.3.2. | Cost weighing of false positives and false negatives | While all errors are equal to an ML system, not all errors are equal to all people. Discuss with your team how mistakes of your ML model might affect the user's experience of the product.                                                                                                                                                                                                                                                                          | to avoid sensitive decisions being taken (a) autonomously; or (b) without human consideration.                                                                                                                                                                                                                       |\n\n| 19.3.3.   | Visual Storytelling         | Focus on explanatory analysis over exploratory analysis, taking the mental models of your target audience in account.                                                                                                                                                                                                                                                                                                                                    | To avoid uninformed decisions about your product or model by non-technical stakeholders, when presenting complex analysis, models, and findings.                                                                  |\n|-----------|-----------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 19.3.4.   | Preventative Process Design | Document and assess whether high-risk and/or high-impact Model (sub)problems or dilemmas that are present in the Product (as determined from following the Best Practices Framework) can be mitigated or avoided by applying non-Model process and implementation solutions. If non-Model solutions are not applied, document the reasons for this, document the sustained presence of these risks and implement appropriate incident response measures. | To (a) prevent high-risk and/or high-impact Model (sub)problems or dilemmas through non-Model process and implementation solutions; and (b) highlight associated risks that might occur in the Product Lifecycle. |\n\n## 19.4 Production\n\n## Objective\n\nTo ensure (a) delivering a user-friendly product, (b) increasing the adoption rate of your product or model, focussing on (dis-)trust as main fundamental risk of ML models with (non-technical) end users\n\n|         |                            | Control:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Aim:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|---------|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 19.4.1. | Trust; increased by design | Allow for users to develop systems heuristics (ease of use) via design patterns while at the same time facilitate a detailed understanding to those who value the 'intelligent' technology used. (See Section 19.4.2 -Design for Human Error; Section 19.4.3 - Algorithmic transparency; and Section 19.4.4 - Progressive disclosure for further information.)                                                                                                                                                                                            | To avoid (a) that the user does not trust the outcome, and will act counter to the design, causing at best inefficiencies and at worst serious harms, or (b) that -trusting an application will do what we think it will do- an user can confirm their trust is justified.                                                                                                                                                                                                                                                                                               |\n| 19.4.2. | Design for Human Error     | (a) Understand the causes of error and design to minimise those causes; (b) Do sensibility checks. Does the action pass the 'common sense' test (e.g. is the number is correct? - 10.000g or 10.000kg) (c) Make it possible to reverse actions - to 'undo' them - or make it harder to do what cannot be reversed (eg. add constraints to block errors - either change the color to red or mention 'Do you want to delete this file? Are you sure?'). (d) make it easier for people to discover the errors that do occur, and make them easier to correct | To (a) increase trust between the end user and the model; (b) minimize the opportunities for errors while also mitigating the consequences. Increase the trust users have with your product by design for deliberate mis-use of your model (making your model or product 'idiot-proof') so users are (a) able to insert data to compare the model outcome with their own expected outcome which will increase their trust, or (b) users able to test the limitations of your product or model -via fake or highly unlikely data- without breaking your product or model. |\n\n| 19.4.3.   | Algorithmic transparency   | Assess the appropriate system heuristics (eg. ease of use), document all factors that influence the algorithmic decisions, and use them as a design tool to make them visible, or transparent, to users who use or are affected by the ML systems.                                                                                                   | To (a) increase trust between the end user and the model; (b) increase end- user control; (c) improve acceptance rate of tool; (d) promote user learning with complex data; and (e) enable oversight by developers.   |\n|-----------|----------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 19.4.4.   | Progressive disclosure     | At the point where the end-user interacts with the Product outcomes, show them only the initial features and/or information necessary at that point in the interaction (thus initially hiding more advanced interface controls). Show the secondary features and/or information only when the user requests it (show less, provide more- principle). | To greatly reduce unwanted complexity for the end-user and thus preventing (a) end-user non-adoption or misunderstanding and (b) ensuring an increased feeling of trust by the users.                                 |\n| 19.4.5.   | Human in the loop (HITL)   | Embed human interaction with machine learning systems to be able to label or correct inaccuracies in machine predictions.                                                                                                                                                                                                                            | To avoid the risk of the Product applying a materially detrimental or catastrophic Product Outcome to a Product Subject without human intervention.                                                                   |\n| 19.4.6.   | Remediation                | Document, assess and implement in the Model(s), Product and Organization processes, requirements for enabling Product Subjects to challenge and obtain redress for Product Outcomes applied to them.                                                                                                                                                 | To ensure detrimental Product Outcomes are easily reverted when appropriate.                                                                                                                                          |\n\n## Section 20. System Stability\n\n## Objective:\n\nTo prevent (in)direct adverse social and environmental effects as a consequence of interactions amongst Products, Models, the Organisation, and the Public.\n\n## What is Systemic Stability?\n\nModel stability is a relatively popular notion. It is usually centered at putting a bound at the Model's generalization error.\n\nSystemic Stability refers to the robustness of the Model (or lack thereof) stemming from the interaction between the Model, Organization, environment and the broader public (society at large).\n\nThere are numerous potential risks that can emerge in this interaction. Many of them can impact the stability of the Model - beyond the context of traditional performance robustness or deterioration of the Model over time. Another way to think of it is as the extent to which the Model and/or its building blocks are susceptible to chain effects and self-reinforcing interactions between the Model, Organization, environment and society.\n\n## Why Systemic stability?\n\nSystemic stability forces one to think beyond the traditional definitions of Model stability and its potential causes and consequences. Systemic stability ensures that we consider the effect on the Model and society due to the interaction between the Model, the Organization, the environment and society. This means thinking about susceptibility to feedback loops, self-fulfilling prophecies and how either of them may impact the data or the Model and its output. It, therefore, reduces risks related to deteriorated performance and minimises the propagation of undesirable biases.\n\n## How to ensure Systemic stability?\n\nIn order to ensure systemic stability, it must be considered continuously throughout all stages of the product lifecycle. This means that systemic stability must be addressed at the (a) Product Definition(s), (b) Exploration, (c) Development and (d) Production stages of machine learning operations.\n\n## 20.1 Product Definitions\n\nObjective\n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused  through Product Definition(s).\n\n|         |                                   | Control:                                                                                                                                                                         | Aim:                                                                                                                                                      |\n|---------|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 20.1.1. | Product Assumption Susceptibility | Document and assess whether applying Product Outputs will result in invalidating Product Assumptions. If so, attempt to redefine Product Assumptions to warrant their longevity. | To (a) prevent unpredictable social and/or environmental behaviour through Product Outcomes; and (b) highlight associated risks in the Product Lifecycle. |\n\n## 20.2 Exploration\n\nObjective\n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through Product exploration.\n\n|         |                                   | Control:                                                                                                                                                                                                                                                                                                                                | Aim:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|---------|-----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 20.2.1. | Selection Function Susceptibility | Document and assess whether applying Product Outputs will result in invalidating Product Assumptions. If so, attempt to redefine Product Assumptions to warrant their longevity.                                                                                                                                                        | To (a) prevent unpredictable social and/or environmental behaviour through Product Outcomes; and (b) highlight associated risks in the Product Lifecycle.                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| 20.2.2. | Data Definition Susceptibility    | Document and assess whether applying Product Outputs will result in changes to the Selection Function, and whether this is a self-reinforcing interaction. If true, attempt to mitigate or stabilize associated effects through refining Product Definition(s) and/or improving Model design and/or Product and process implementation. | To (a) determine and prevent Product and/or Model risk in - (i) progressively strengthening biases (from encoded assumptions and definitions to datasets to algorithms chosen); (ii) progressively reinforcing Model errors and/or Product generalizations; (iii) progressively losing sensitivity to data and/or Domain changes; (iv) suffering from self-reinforcing and/ or exponential run-away effects; (b) determine and prevent risks of unpredictable behaviour once the Product Outcomes are applied; and (c) highlight associated risks in the Product Lifecycle. |\n\n| 20.2.3.   | Data Generating Process Susceptibility   | Document and assess whether applying Product Outputs will result in changes to the Product data definitions, and whether this is a self-reinforcing interaction. If true, attempt to mitigate or stabilize associated effects through refining Product Definition(s) and/or improving Model design and/or Product and process implementation.   | To (a) determine and prevent Product and/or Model risk in - (i) progressively strengthening biases (from encoded assumptions and definitions to datasets to algorithms chosen); (ii) progressively reinforcing Model errors and/or Product generalizations; (iii) progressively losing sensitivity to data and/or Domain changes; (iv) suffering from self-reinforcing and/ or exponential run-away effects; (b) determine and prevent risks of unpredictable behaviour once the Product Outcomes are applied; and (c) highlight associated risks in the Product Lifecycle.   |\n|-----------|------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 20.2.4.   | Data Distributions Susceptibility        | Document and assess whether applying Product Outputs will result in changes to the data generating process, and whether this is a self-reinforcing interaction. If true, attempt to mitigate or stabilize associated effects through refining Product Definition(s) and/or improving Model design and/or Product and process implementation.    | To (a) determine and prevent Product and/or Model risk in - (i) progressively strengthening biases (from encoded assumptions and definitions to datasets to algorithms chosen); (ii) progressively reinforcing Model errors and/or Product generalizations; (iii) progressively losing sensitivity to data and/or Domain changes; (iv) suffering from self-reinforcing and/ or exponential run-away effects; (b) determine and prevent risks of unpredictable behaviour once the Product Outcomes are applied; and (c) highlight associated risks in the Product Lifecycle.   |\n| 20.2.5.   | Hidden Variable Susceptibility           | Document and assess whether applying Product Outputs will result in the creation of new hidden Variables in the system. If true, record the new Variable during data gathering, or prevent the creation of the new Variable through improved Product Definition(s) and implementation.                                                          | To (a) determine and prevent Product and/or Model risk in - (i) progressively strengthening biases (from encoded assumptions and definitions to datasets to algorithms chosen); (ii) progressively reinforcing Model errors and/or Product generalizations; (iii) progressively losing sensitivity to data and/or Domain changes; (iv) suffering from self-reinforcing and/ or exponential run-away effects; (b) determine and prevent risks of unpredictable behaviour once the Product Outcomes are applied; and (c) highlight associated risks in the Product Lifecycle.   |\n\n## 20.3 Development\n\n## Objective\n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through Product development.\n\n|         |                                           | Control:                                                                                                                                                                                                                                                                                                                                                                                           | Aim:                                                                                                                                                             |\n|---------|-------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 20.3.1. | Target Feature Definition Susceptibility  | Document and assess whether applying Product Outputs will result in changes to the Target Feature definition. If true, attempt to mitigate associated effects through refining Product Output and/or Model design and/or development.                                                                                                                                                              | To (a) determine and prevent risk of unpredictable behaviour once the Product outcomes are applied; and (b) highlight associated risks in the Product Lifecycle. |\n| 20.3.2. | Optimization Feedback Loop Susceptibility | Document and assess whether the cost function and/or optimization algorithm exhibits a feedback loop behaviour that includes the gathering of data that has been influenced by previous Model iterations, and whether this behaviour is self-reinforcing or self-limiting. If true, attempt to mitigate associated effects through refining Product Output and/or Model design and/or development. | Idem Section 20.2.1- Selection Function Susceptibility                                                                                                           |\n\n## 20.4 Production\n\n## Objective\n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through Product application.\n\n|         |                              | Control:                                                                                                                                                                                                                                                                                                                                                   | Aim:                                                                                                                                                                                        |\n|---------|------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 20.4.1. | Self-fulfilling Prophecies   | Document and assess whether applying Product Outputs will result in change to Product inputs, dependencies and/or Domain(s) (other than those mentioned in controls elsewhere) and whether this is a self-reinforcing interaction. If true, attempt to mitigate associated effects through refining Product Output and/or Model design and/or development. | Idem Section 20.2.1- Selection Function Susceptibility                                                                                                                                      |\n| 20.4.2. | Hidden Variable Dependencies | Document and assess whether the effect of applying Product Outputs depends on Hidden Variables. If true, control for Hidden Variables, for example through marginalization and/or by deriving indicators for Hidden Variables influence.                                                                                                                   | To (a) prevent diverging effects on seemingly similar individuals or datapoints; (b) prevent or detect high-risk interactions; and (c) highlight associated risks in the Product Lifecycle. |\n\n| 20.4.3.   | Society Susceptibility                     | Document and assess whether applying Product Outputs results in potentially harmful societal or environmental changes, and research the possible knock-on effects as far as reasonably practical.                                                                      | To (a) identify and prevent both direct and indirect adverse effects on society and the environment; (b) determine if there is a risk of unpredictable behaviour once the Product Outcomes are applied; and (c) highlight associated risks in the Product Lifecycle.                     |\n|-----------|--------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 20.4.4.   | Domain Susceptibility                      | Document and assess whether applying Product Outputs results in changes to application Domain(s), and research the possible knock-on effects as far as reasonably practical.                                                                                           | To (a) identify and prevent both direct and indirect adverse effects on Product Domain(s); (b) determine if there is a risk of unpredictable behaviour once the Product Outcomes are applied; and (c) highlight associated risks in the Product Lifecycle.                               |\n| 20.4.5.   | Other Organisation Products Susceptibility | Document and assess whether applying Product Outputs result in changes to inputs, dependencies and/or context for other Organisation Products. If true, attempt to mitigate associated effects through refining Product Output and/or Model design and/or development. | To (a) identify and prevent both direct and indirect adverse effects on the Organisation or other Organisation Products; (b) determine if there is a risk of unpredictable behaviour once the Product Outcomes are applied; and (c) highlight associated risks in the Product Lifecycle. |\n\n## Section 21. Product Traceability\n\n## Objective:\n\nTo ensure the clear and complete Traceability of Products, Models and their assets (inclusive of, amongst other things, data, code, artifacts, output, and documentation) for as long as is reasonably practical.\n\n## What do we mean when we refer to Product Traceability?\n\nProduct Traceability refers to the ability to identify, track and trace elements of the Product as it is designed, developed, and implemented.\n\nAlternatively put, Product Traceability, is the ability to trace and track all Product elements and decisions throughout the Product Lifecyle. It is the identification, indexing, storage, and management of each unique Product element.\n\n## Why is Product Traceability relevant?\n\nThrough Product Traceability, each element of the Product can be  easily identified and, thereafter, re-examined, and amended. This allows for greater Product accountability and transparency as, through this process, each Product element and its developers can be identified.\n\n## How to apply Product Traceability?\n\nIn order to generate thorough and thoughtful Product Traceability, it must be considered continuously throughout all stages of the Product Lifecycle. This means that Product Traceability must be addressed at the (a) Product Definition(s), (b) Exploration, (c) Development and (d) Production stages of Machine Learning Operations.\n\n## 21.1 Product Definitions\n\n## Objective\n\nTo document and maintain an overview of the requirements necessary to complete the Product and the interdependencies in the Product design phase.\n\n|         |                                     | Control:                                                                                                                                                                                                                                             | Aim:                                                                                                                                                                                                                        |\n|---------|-------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 21.3.1. | Document Storage                    | Define a single fixed storage solution for all reports, documents, and other traceability files.                                                                                                                                                     | To (a) prevent the usage and dissemination of outdated and/ or incorrect files; (b) prevent the haphazards storage of Product reports, documents and/or files; and (c) highlight associated risks in the Product Lifecycle. |\n| 21.3.2. | Version Control of Documents        | Ensure that document changes are tracked when changes are made. Subsequent versions ought to list version number, author, date of change, and short description of the changes made.                                                                 | To (a) track changes to any and all documents; (b) ensure everyone is using the same and latest document version; and (c) highlight associated risks in the Product Lifecycle.                                              |\n| 21.3.3. | Architectural Requirements Document | Document which information technology resources are necessary for each element of the Product to provide a necessary overview of system requirements and cost distribution. Document the reasons each resource was chosen along with justifications. | To (a) provide clear documentation of which system resources are used, where they are used, why they are used, and costs; and (b) highlight associated risks in the Product Lifecycle.                                      |\n\n## 21.2 Exploration\n\n## Objective\n\nTo document the impact analysis of each requirement.\n\n|         |                                          | Control:                                                                                                                      | Aim:                                                                                                                                                                                        |\n|---------|------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 21.2.1. | Document Impact Analysis of Requirements | Document and complete an impact analysis on the resources and design of the Product that can result in technical debt.        | To (a) avoid Product failures due to unresolved technical debt by documenting potential sources of friction and the solutions; and (b) highlight associated risks in the Product Lifecycle. |\n| 21.2.2. | Resource Traceability Matrix             | Provide and keep up to date a clear view of the relationships and interdependencies between resources in a documented matrix. | To (a) document and show resource coverage for each use case; and (b) highlight associated risks in the Product Lifecycle.                                                                  |\n\n| 21.2.3.   | Design Traceability Matrix   | Provide and keep up to date a clear view of the relationships and interdependencies between designs and interactions thereof in a documented matrix.                                                                                                                                                                     | To (a) document design and execution status; (b) clearly trace current work and what can be pursued next; and (c) highlight associated risks in the Product Lifecycle.   |\n|-----------|------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 21.2.4.   | Results Reproducibility Logs | Throughout the entire Product Lifecycle, whenever a Product component - inclusive of Models, experiments, analyses, transformation, and evaluations - are run, all parameters, hyperparameters and results ought to be logged and/or tracked, including unique identifier(s) for runs, artifacts, code and environments. | To (a) enable Absolute Reproducibility; (b) validate Models and Outcomes through enablement of analysis of logs, run comparisons and reproducibility.                    |\n\n## 21.3 Development\n\n## Objective\n\nTo document and maintain the status of each product and the testing results. Ensure 100% test coverage. Prevent inconsistencies between project elements and prevent feature creep.\n\n|         |                                          | Control:                                                                                                                                                                                                                                                            | Aim:                                                                                                                                                                                                                               |\n|---------|------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 21.3.1. | Backlog                                  | Ensure that an effective backlog is maintained to track work items and serve as a historical representation and timeline of completed features and velocity.                                                                                                        | To (a) ensure a comprehensive breakdown of Features and tasks necessary to achieve full product functionality; (b) provide highly readable coarse-grained versioning; and (c) highlight associated risks in the Product Lifecycle. |\n| 21.3.2. | Documentation for Technical Contributors | Maintain technical documentation that enables all current and future contributors to efficaciously and safely develop and maintain the Product, including such information as description of each file, the workflow, author, environments, accrued technical debt. | To (a) maintain Product technical integrity by ensuring safe contribution and maintenance practices; and (b) highlight associated risks in the Product Lifecycle.                                                                  |\n| 21.3.3. | Version Control of Code                  | Maintain uninterrupted version control systems and practices of all code used by, in and during the Product and its Lifecycle.                                                                                                                                      | To (a) maintain Product technical integrity by ensuring safe contribution and maintenance practices; and (b) highlight associated risks in the Product Lifecycle.                                                                  |\n\n| 21.3.4.   | Docstrings and Code Comments   | Document in each function the author of code, purpose of code, input, Output, and improvements to be made. Document the source of inputs and potentially a short business description of data used.                | To (a) ensure Model clarity as to technical progress; and (b) highlight associated risks in the Product Lifecycle.                                 |\n|-----------|--------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n| 21.3.5.   | Project Status Reports         | Ensure that all status reports and similar communications to Management and Stakeholders are stored and maintained, inclusive of team updates, reports to the Product Manager, and Stakeholder reports by request. | To (a) maintain a formal written record of decisions, progress and context evolution; and (b) highlight associated risks in the Product Lifecycle. |\n\n## 21.4 Production\n\n## Objective\n\nTo document the observed impact of updates to the product. Document product runs and their input for reproducibility.\n\n|         |                               | Control:                                                                                                                                                                                                                             | Aim:                                                                                                                                                                      |\n|---------|-------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 21.4.1. | Version control through CI/CD | Maintain distinct production versions to easily revert or roll back to a working previous Product, if production issues arise. Properly set up CI/CD enables easy redeploy of any artifact and version.                              | To (a) provide functional Product to users at all times; (b) seamlessly redeploy Product versions if needed; and (c) highlight associated risks in the Product Lifecycle. |\n| 21.4.2. | Data Lineage Manifest         | Utilise a data lake for production data, intermediate results, and end results. Each step should be documented in a manifest that is passed from one step of the process to the next and always accompanies stored data and results. | To (a) create a structured way for tracing where data has been, what was done to it, and results; and (b) highlight associated risks in the Product Lifecycle.            |", "fetched_at_utc": "2026-02-09T13:50:58Z", "sha256": "8f7a10124ab73668e5af4d1565e30ccba3cb9d3e0b97a4ebd057397591793ffc", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-63-87.pdf", "file_size": 1007448, "mtime": 1770576471, "docling_errors": []}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-7-9-edc20a4a010f", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-7-9.pdf", "title": "FBPML_TechnicalBP_V1.0.0-7-9", "text": "## Section 1. Definitions\n\nAs used in this Best Practice Guideline, the following terms shall have the following meanings where capitalised. All references to the singular shall include references to the plural, where applicable, and vice versa. Any terms not defined or capitalised in this Best Practice Guideline shall hold their plain text meaning as cited in English and data science.\n\n| 1.1.   | Absolute Reproducibility       | means a guarantee that any and all results, outputs, outcomes, artifacts, etc can be exactly reproduced under any circumstances.                                                           |\n|--------|--------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1.2.   | Best Practice Guideline        | means this document.                                                                                                                                                                       |\n| 1.3.   | Confidence Value               | means a measure of a Model's self-reported certainty that the given Output is correct.                                                                                                     |\n| 1.4.   | Data Generating Process        | means the process, through physical and digital means, by which Records of data are created (usually representing events, objects or persons).                                             |\n| 1.5.   | Data Science                   | means an interdisciplinary field that uses scientific methods, processes, algorithms and computational systems to extract knowledge and insights from structural and/or unstructured data. |\n| 1.6.   | Domain                         | means the societal and/or commercial environment within which the Product will be and/or is operationalised.                                                                               |\n| 1.7.   | Edge Case                      | means an outlier in the space of both input Features and Model Outputs.                                                                                                                    |\n| 1.8.   | Error Rate                     | means the frequency of occurrence of errors in the (Sub)population relative to the size of the (Sub)population                                                                             |\n| 1.9.   | Evaluation Error               | means the difference between the ground truth and a Model's prediction or output.                                                                                                          |\n| 1.10.  | Fairness & Non- Discrimination | means the property of Models and Model outcomes to be free from bias against Protected Classes.                                                                                            |\n| 1.11.  | Features                       | mean the different attributes of datapoints as recorded in the data.                                                                                                                       |\n| 1.12.  | Hidden Variable                | means an attribute of a datapoint or an attribute of a system that has a causal relation to other attributes, but is itself not measured or unmeasurable.                                  |\n| 1.13.  | Human-Centric Design & Redress | means orienting Products and/or Models to focus on humans and their environments through promoting human and/or environment centric values and allowing for redress.                       |\n| 1.14.  | Implementation                 | means every aspect of the Product and Model(s) insertion of and/or application to Organisation systems, infrastructure, processes and culture and Domains and Society.                     |\n| 1.15.  | Incident                       | means the occurrence of a technical event that affects the integrity of a Product and/or Model.                                                                                            |\n| 1.16.  | Label                          | means the Feature that represents the (supposed) ground-truth values corresponding to the Target Variable.                                                                                 |\n\n| 1.17.   | Machine Learning       | means the use and development of computer systems and Models that are able to learn and adapt with minimal explicit human instructions by using algorithms and statistical modelling to analyse, draw inferences, and derive outputs from data.                                                                                   |\n|---------|------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1.18.   | Model                  | means Machine Learning algorithms and data processing designed, developed, trained and implemented to achieve set outputs, inclusive of datasets used for said purposes unless otherwise stated.                                                                                                                                  |\n| 1.19.   | Organisation           | means the concerned juristic entity designing, developing and/or implementing Machine Learning.                                                                                                                                                                                                                                   |\n| 1.20.   | Outcome                | means the resultant effect of applying Models and/or Products.                                                                                                                                                                                                                                                                    |\n| 1.21.   | Output                 | means that which Models produce, typically (but not exclusively) predictions or decisions.                                                                                                                                                                                                                                        |\n| 1.22.   | Performance Robustness | means the propensity of Products and/or Models to retain their desired performance over diverse and wide operational conditions.                                                                                                                                                                                                  |\n| 1.23.   | Product                | means the collective and broad process of design, development, implementation and operationalisation of Models, and associated processes, to execute and achieve Product Definition(s), inclusive of, amongst other things, the integration of such operations and/or Models into organisation products, software and/or systems. |\n| 1.24.   | Product Manager        | means either a Design Owner and/or Run Owner as identified in the Organisation Best Practice Guideline in Sections 3.1.4. & 3.1.7. respectively.                                                                                                                                                                                  |\n| 1.25.   | Product Team           | means the collective group of Organisation employees directly charged with designing, developing and/or implementing the Product.                                                                                                                                                                                                 |\n| 1.26.   | Product Subjects       | means the entities and/or objects that are represented as data points in datasets and/or Models, and who may be the subject of Product and/or Model outcomes.                                                                                                                                                                     |\n| 1.27.   | Project Lifecycle      | means the collective phases of Products from initiation to termination - such as design, exploration, experimentation, development, implementation, operationalisation, and decommissioning - and their mutual iterations.                                                                                                        |\n| 1.28.   | Protected Classes      | mean (Sub)populations of Product Subjects, typically persons, that are protected by law, regulation, policy or based on Product Definition(s)                                                                                                                                                                                     |\n| 1.29.   | Root Cause Analysis    | means the activity and/or report of the investigation into the primary causal reasons for the existence of some behaviour (usually an error or deviation).                                                                                                                                                                        |\n| 1.30.   | Safety                 | means real Product Domain based physical harms that result through Products and/or Models applications.                                                                                                                                                                                                                           |\n| 1.31.   | Security               | means the resilience of Products and/or Models against malicious and/ or negligent activities that result in Organisational loss of control over concerned Products and/or Models.                                                                                                                                                |\n\n| 1.32.   | Selection Function   | means a (where possible mathematical) description of the probability or proportion of all real Subjects that might potentially be recorded in the dataset that are actually recorded in a dataset.                                                                                                                                  |\n|---------|----------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1.33.   | Stakeholders         | mean the department(s) and/or team(s) within the Organisation who do not conduct data science and/or technical Machine Learning, but have a material interest in Product Machine Learning.                                                                                                                                          |\n| 1.34.   | (Sub)population      | means any group of persons, animals, or any other entities represented by a piece of data , that is part of a larger (potential) dataset and characterized by any (combination of) attributes. The importance of (Sub) populations is particularly high when some (Sub)populations are vulnerable or protected (Protected Classes). |\n| 1.35.   | Systemic Stability   | means the stability of Organisation, Domain, society and environment as a collective ecosystem.                                                                                                                                                                                                                                     |\n| 1.36.   | Target of Interest   | means the fundamental concept that the Product is truly interested in when all is said and done, even if it is something that is not (objectively) measureable.                                                                                                                                                                     |\n| 1.37.   | Target Variable      | means the Variable which a Model is made to predict and/or output.                                                                                                                                                                                                                                                                  |\n| 1.38.   | Traceability         | means the ability to trace, recount, and reproduce Product outcomes, reports, intermediate products, and other artifacts, inclusive of Models, datasets and codebases.                                                                                                                                                              |\n| 1.39.   | Variables            | mean the different attributes of subjects or systems which may or may not be measured.                                                                                                                                                                                                                                              |", "fetched_at_utc": "2026-02-09T13:51:13Z", "sha256": "edc20a4a010f9fe4f8ea9b0737a019e2f11391766951408e69449f19f914ef01", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-7-9.pdf", "file_size": 115002, "mtime": 1770576471, "docling_errors": []}}
{"doc_id": "pdf-pdfs-general-purpose-ai-model-compliance-guide-part-1-oliver-patel-2ef267d5ff1b", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel.pdf", "title": "General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable, and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement, and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nFollow me on LinkedIn for more frequent updates.\n\nIt has been a consequential few weeks for the EU AI Act's provisions on generalpurpose AI (GPAI) models.\n\nNow that we've all had some time to (hopefully) take a deep breath, relax, and take stock, it is the ideal moment for a deep-dive on GPAI model compliance.\n\nThe next three editions of Enterprise AI Governance will provide a comprehensive, practical, and actionable GPAI Model Compliance Guide. It is targeted at AI governance, legal, and compliance practitioners working for firms that develop, finetune, modify, deploy, or otherwise use GPAI models. That is quickly encompassing all of us.\n\nMy goal is to simplify these complex obligations and highlight what they mean for your business.\n\nNote: this series assumes familiarity with the EU AI Act, its core concepts, and the topic of general-purpose AI and foundation models more broadly.\n\nPart 1's focus (this edition) is on Obligations for Providers of GPAI Models . It covers:\n\n- âœ… What is a GPAI model?\n- âœ… What do providers of GPAI models need to do?\n- âœ… What about open-source GPAI models?\n- âœ… What about legacy GPAI models (released before 2 August 2025)?\n- âœ… How does the GPAI Code of Practice fit in?\n- âœ… How and when will the GPAI model provisions be enforced?\n\nAnd here is a glimpse of what's coming in parts 2 and 3.\n\nPart 2 will focus on GPAI Models with Systemic Risk , covering:\n\n- âœ… What is a GPAI model with systemic risk?\n- âœ… How are such models classified and what are the exemptions?\n- âœ… What do providers of GPAI models with systemic risk need to do?\n\n- âœ… How do the compliance obligations differ?\n- âœ… Deep dive on the GPAI Code of Practice: Safety and Security Chapter\n\nAnd Part 3 wraps up the series with the question on everyone's lips: am I a GPAI model provider!?\n\nThe focus here will be on 'Downstream Actors': Modification, Deployment, and Use of GPAI Models :\n\n- âœ… Who exactly is a provider of a GPAI model?\n- âœ… How do you become the provider of a GPAI model you are modifying?\n- âœ… What is a 'downstream provider' and how do you become one?\n- âœ… What is a general-purpose AI system?\n- âœ… What if you integrate a GPAI model into a high-risk AI system?\n- âœ… How should you assess GPAI model providers and choose which models to use?\n\nBy the end of the series, you will have a thorough understanding of precisely what your organisation needs to do to achieve compliance. My analysis represents a simplified breakdown of the following official legal and regulatory documents, which are referenced throughout.\n\n- EU AI Act full text\n- European Commission Guidelines for Providers of GPAI Models\n\n- GPAI Code of Practice\n- Transparency Chapter\n- Model Documentation Form\n- Copyright Chapter\n- Safety and Security Chapter\n- Template for publishing GPAI Model Training Data Summary\n\nThe aforementioned Code of Practice, Guidelines, and Training Data Template were all published in July 2025. And the Code of Practice was formally approved by the EU on 1 August 2025, one day before the GPAI model provisions became applicable on 2 August 2025.\n\n## What is a GPAI model?\n\nWhether or not you are a provider of a GPAI model, and thus subject to the below obligations, depends on whether or not the model you are developing and placing on the market is in fact a GPAI model.\n\nThe AI Act defines a GPAI model as:\n\n\"An AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale , that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications , except AI models that are used for research, development or prototyping activities before they are placed on the market\" .\n\nThe corresponding AI Act recital says that models with at least 1 billion parameters, trained in this way, should be considered to be GPAI models.\n\nIn its recent guidelines, the European Commission acknowledged that the above definition is fairly general and lacks specific, objective criteria that organisations can use to determine whether or not their models constitute GPAI.\n\nPlugging this gap, the Commission has now provided specific criteria for GPAI model classification. This criteria is based on the amount of computational resource used to train the model.\n\nSpecifically, if a model's training compute is greater than 10^23 floating-point operations (FLOP) and it can generate language (either text or audio), or generate image or video based on text inputs, then it should be considered a GPAI model.\n\nThe Commission's position is that there is a direct correlation between how much computational resource is used to train the model-which in itself is linked to the size of the model and the volume of training and pre-training data-and the generalpurpose capabilities of the model.\n\nHowever, it is acknowledged that there could be exceptional instances where a model trained with this amount of computational resource that generates text would not meet the legal definition of a GPAI model (e.g., if it did not display significant generality in its capabilities). Also, a model which does not meet this indicative criteria may also meet the legal definition of a GPAI model.\n\nThe key message is that organisations developing and fine-tuning large AI models at scale need to track the computational resource they use in a standardised and automated way.\n\n## What do providers of GPAI models need to do?\n\nProviders of GPAI models are organisations or entities that develop such AI models and place them on the market or otherwise make them available.\n\nPart 3 of this series will cover exactly who is a provider of a GPAI model and how to know if you are one. For now, here are some examples the European Commission provides that constitute a provider placing a GPAI model on the market. In all instances, company A is the GPAI model provider.\n\n- Company A develops a GPAI model and makes it available via a software library, repository, API, or cloud computing service.\n- Company A commissions company B to develop a GPAI model on its behalf, and company A makes it available via a software library, repository, API, or cloud computing service.\n- Company A develops a GPAI model and uploads it to an online repository hosted and managed by company C.\n\nThere are a suite of obligations that apply to providers of all GPAI models, as well as additional obligations that also apply to providers of GPAI models with systemic risk. Part 2 of this series will cover, in detail, the obligations for providers of GPAI models with systemic risk. They are left out here.\n\nBelow is a summary of the four main sets of obligations that apply to providers of all GPAI models:\n\n## 1. Providers of GPAI models must develop, maintain, and keep up-to-date\n\ncomprehensive technical documentation which provides information about the GPAI model, how it was developed, and how it should be used.\n\n- The Transparency Chapter of the GPAI Code of Practice provides extensive detail about how this can be done. It is accompanied by a Model Documentation Form consisting of 42 metadata attributes across 8 categories. This builds on the elements set out in Annex XI and XII of the AI Act.\n- The attributes include training time and computation, model size, energy consumption, data collection and curation methods, and input and output modalities.\n- Some of this information must be shared with the European Commission's AI Office and/or national regulators (on request), and some must be proactively shared with 'downstream providers', which are organisations that integrate GPAI models into an AI system.\n- Some documentation is required for all GPAI models, whereas certain additional documentation is only required for GPAI models with systemic risk.\n\n## 2. Providers of GPAI models must produce and make publicly available a detailed summary of the content and data used to train the GPAI model.\n\n- The AI Act requires this training data summary to be \"sufficiently detailed\" and structured in line with a template provided by the AI Office. That template was\n\n- published by the European Commission in July 2025, alongside explanatory guidance.\n- The template carefully attempts to balance protecting trade secrets and confidentiality with promoting transparency, data protection, and copyright protection.\n- In a nutshell, the training data summary should cover the amount and type of training data used, as well as the use of:\n- publicly available datasets;\n- private non-publicly available datasets;\n- data crawled and scraped from online sources;\n- user data; and\n- synthetic data.\n- Importantly, the template does not require providers to publish or disclose details about the specific data and works used to train GPAI models, as this would \"go beyond the requirement to provide a 'summary'\" .\n\n## 3. Providers of GPAI models must implement a policy to comply with EU copyright and intellectual property law.\n\n- Given the large volume of copyright protected material typically used to train GPAI models, this obligation is significant. It compels providers to consider and document how they will comply with EU copyright law in such contexts.\n- The AI Act provides minimal detail about how to do this. However, it does stipulate that \"state-of-the-art technologies\" must be used to promote copyright compliance in the context of developing GPAI models.\n- The Copyright Chapter of the GPAI Code of Practice provides more detail, outlining six practical measures for organisations to implement.\n\n## 4. Providers of GPAI models must cooperate with the European Commission and regulatory authorities and appoint an EU-based authorised representative if they are based outside of the EU.\n\n- The AI Act has extraterritorial effect. In this context, it means that if a provider places their GPAI model on the market or makes it available in the EU, then that provider must comply with the AI Act, irrespective of where the provider is established or operating from.\n- In such extraterritorial scenarios, the appointment of an authorised representative is important, to enable effective cooperation and correspondence with the AI Office and any other relevant authorities.\n\n## What about open-source GPAI models?\n\nThe AI Act exempts providers of open-source GPAI models from the first and fourth set of obligations. Namely, they do not need to develop, maintain, and keep up-to-date comprehensive technical documentation referenced in point 1 above. And they do not need to appoint an authorised representative (if based outside the EU), as referenced in point 4.\n\nHowever, providers of open-source GPAI models are obliged to comply with points 2 and 3. This means they must implement a copyright compliance policy and publish the training data summary.\n\nFurthermore, providers of open-source GPAI models with systemic risk are not exempt from any of these obligations. This means that they must comply with the obligations for both all GPAI models and the additional obligations for GPAI models with systemic risk.\n\nThe European Commission's guidelines provide clarity about what exactly constitutes an open-source GPAI model-a hotly contested topic. Here is a summary of the recent guidance:\n\n- To qualify as open-source, a GPAI model must be released under a free and opensource licence that permits unrestricted access, use, modification, and distribution without payment or significant limitations.\n- All model parameters, architecture, and usage information must be publicly available to ensure usability.\n- A licence is not considered open-source if it includes restrictions like noncommercial use only, prohibitions on distribution, usage limits based on scale, or requirements for separate commercial licences. Essentially, most usage restrictions, aside from those justifiably designed to protect users or other groups, or ensure appropriate attribution, would likely disqualify a GPAI model from open-source classification.\n- Monetisation generally also disqualifies a model from open-source exemptions if payment is required for core access, use, or essential, indistinguishably linked support, or if it's exclusively hosted on paid platforms.\n- However, optional paid services that don't hinder free use are permissible.\n\n## What about 'legacy' GPAI models (released before 2 August 2025)?\n\nIrrespective of when a GPAI model was developed and placed on the market, providers of that model must eventually comply with the AI Act's obligations.\n\nThis means that the full suite of obligations, including publishing training data summaries, implementing a copyright compliance policy, and maintaining technical documentation, applies to both 'legacy' and 'new' GPAI models. The only difference is the applicable date of those obligations. For the 'legacy' GPAI models, placed on the market before 2 August 2025, providers have until 2 August 2027 to comply with the aforementioned provisions. For GPAI models released after 2 August 2025, there is no longer a grace period.\n\nThe practical implication of this is that virtually all GPAI models that have already been released and made available in the EU will have to retrospectively become compliant. This represents a practical and operational headache for AI developers, especially given how recently the Code of Practice and guidelines about how to comply were released.\n\nThe European Commission acknowledges that this is going to be challenging, noting that \"the AI Office is dedicated to supporting providers in taking the necessary steps to comply with their obligations by 2 August 2027\" .\n\nHowever, the Commission confirmed that providers of 'legacy' GPAI models will not have to conduct \"retraining or unlearning of models\" where this is not possible or\n\nwhere it would cause undue burden. The Commission accepts that information about historical training data may not always be available and encourages providers to be open about this.\n\n## How does the GPAI Code of Practice fit in?\n\nThe GPAI Code of Practice was published by the European Commission on 10 July 2025 and was approved by the EU on 1 August 2025.\n\nThe AI Act mandated the Commission to work with external experts to develop the Code of Practice, to assist GPAI model providers in their compliance journey.\n\nIt is a voluntary tool which helps providers comply with the full suite of obligations for GPAI models. Although organisations are not obliged to sign and implement the Code of Practice, doing so provides a standardised and endorsed approach for regulatory compliance. This offers more legal certainty regarding the validity of an organisation's approach.\n\nHowever, you can still be compliant, via adequate alternative means, even if you do not sign up to and follow the Code of Practice.\n\nThe Code of Practice consists of three chapters: 1) Transparency, 2) Copyright, and 3) Safety and Security. Each chapter contains several 'measures'. These are practical steps and actions that signatory organisations commit to taking and implementing to achieve GPAI model compliance.\n\nOrganisations are able to sign up to specific chapters, or the entire Code. Thus far, 26 organisations have signed up in full and xAI (Grok's creator) has signed up to the Safety and Security Chapter only. Meta announced that it will not sign up.\n\nThe Transparency Chapter focuses on the technical documentation which GPAI model providers are obliged to produce, maintain, and share with key stakeholders. It includes the 'Model Documentation Form' template, which providers can use to ensure they gather and document all the information which they are required to maintain and potentially share with the AI Office, national regulators, and downstream providers.\n\nThe Copyright Chapter focuses on how organisations can comply with the obligation to implement an EU copyright compliance policy. This includes measures for compliant web crawling and scraping and mitigating the risk of copyright-infringing outputs via technical guardrails.\n\nFinally, the Safety and Security Chapter focuses exclusively on the obligations for providers of GPAI models with systemic risk. It consists of ten commitments, all relating to GPAI model risk identification, management, mitigation, treatment, monitoring,\n\nownership, and accountability.\n\nThe European Commission strongly encourages providers to sign the Code of Practice, noting its various benefits and calling it a \"straightforward way of demonstrating compliance\" . The Commission even indicates that it will be more trusting of signatory organisations, due to their transparent approach.\n\nIt noted that providers who do not sign up may be \"subject to a larger number of requests for information and requests for access to conduct model evaluations throughout the entire model lifecycle because the AI Office will have less of an understanding of how they are ensuring compliance with their obligations\" .\n\nIn future, the GPAI Code of Practice may be superseded by harmonised technical standards. These remain under development.\n\n## How and when will the GPAI model provisions be enforced?\n\nThe most important thing to understand is that the obligations relating to providers of GPAI models are enforced by the European Commission's AI Office, not member state regulators. This is in contrast to the rest of the AI Act's provisions.\n\nWith respect to enforcement actions, the AI Office has the following powers:\n\n- Request information from providers.\n- Conduct evaluations of GPAI models.\n- Request the implementation of measures and mitigations.\n- Recall GPAI models from the market.\n- Impose financial penalties of up to 3% of global annual turnover or â‚¬15m (whichever is higher).\n\nBecause the AI Office is charged with enforcement, the European Commission's guidelines on GPAI models carry significant weight, even though they are not legally binding. In those guidelines, the Commission explained that it expects to forge close working relationships with GPAI model providers.\n\nIt encourages \"close informal cooperation with providers during the training of their GPAI models\" , as well as \"proactive reporting by providers of GPAI models with systemic risk\" . Given the various notification, reporting, and transparency obligations, there will be a lot of back and forth between providers and the AI Office.\n\nFor those that signed the Code of Practice, the AI Office's focus will be on monitoring their adherence to its measures. Expect slightly more leniency and goodwill. For those that opted not to, the focus will be on in-depth assessments and investigations as to how they are nonetheless compliant.\n\nIn terms of timelines for compliance and enforcement, the obligations for providers of GPAI models became applicable on 2 August 2025. This means that organisations are legally obliged to be compliant today. However, this is only for GPAI models that were placed on the market from 2 August 2025 onwards. As discussed above, for 'legacy' GPAI models placed on the market before then (i.e., the vast majority of GPAI models available today), providers have until 2 August 2027 to comply.\n\nAlthough the obligations have been applicable since 2 August 2025, the European Commission, including its AI Office, does not have any enforcement powers until 2 August 2026 . This means there will be no enforcement proceedings or fines for at least one year from now. However, this does not change the fact that organisations are obliged to work on compliance from today. Any critical gaps in the first year may be taken into account in future enforcement actions or rulings.\n\nIrrespective of the provision, and whether or not enforcement is managed by the AI Office or member state regulators, the Court of Justice of the European Union (CJEU) always has the final say on AI Act interpretation. Over the years, it will be interesting to monitor and learn from the inevitable GPAI model case law that will pile up.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.", "fetched_at_utc": "2026-02-09T13:51:47Z", "sha256": "2ef267d5ff1bb2cb2e947988ff8708805048d6cbe46b00a02e50645cb9caa14a", "meta": {"file_name": "General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel.pdf", "file_size": 755365, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-general-purpose-ai-model-compliance-guide-part-2-oliver-patel-8a89bcb2e20d", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel.pdf", "title": "General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable, and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement, and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nFollow me on LinkedIn for more frequent updates.\n\nWelcome to Part 2 of the General-Purpose AI (GPAI) Model Compliance Guide. This 3part series is posted exclusively on Enterprise AI Governance. To all subscribers and new readers, thanks for supporting the newsletter!\n\nThis week's edition provides a comprehensive yet accessible overview of the EU AI Act's provisions for GPAI Models with Systemic Risk .\n\nYou will learn:\n\n- âœ… What is a GPAI model with systemic risk?\n- âœ… What are the notification and exception procedures for providers of GPAI models\n\nwith systemic risk?\n\n<!-- image -->\n\nâœ… What are the compliance obligations for providers of GPAI models with systemic risk?\n\n- âœ… What exactly is a 'systemic risk'?\n- âœ… Deep dive on the GPAI Code of Practice: Safety and Security Chapter\n\n<!-- image -->\n\n<!-- image -->\n\nIf you haven't read Part 1, you should check it out here and read it first. It provides a detailed breakdown of the Obligations for Providers of GPAI Models , including what the core obligations for all GPAI models are, how and when these obligations will be enforced by the AI Office, as well as specific considerations for open-source models and legacy GPAI models (released before 2 August 2025). All this is essential background information that is necessary to fully understand the compliance implications for GPAI models with systemic risk.\n\nPart 3, coming next week, will cover the knotty issue of 'Downstream Actors': Modification, Deployment, and Use of GPAI Models . This will address the important question of who exactly is a provider of a GPAI model-it could be you!\n\nNote: this series assumes familiarity with the EU AI Act, its core concepts, and the topic of general-purpose AI and foundation models more broadly. Also, I was not involved in the multi-stakeholder process of drafting and developing the EU's GPAI Code of Practice. Finally, none of this should be taken as legal advice. Always consult a legal professional.\n\nThe following official sources have been used to create this guide:\n\n- EU AI Act full text\n- European Commission Guidelines for Providers of GPAI Models\n- GPAI Code of Practice\n- Transparency Chapter\n- Model Documentation Form\n- Copyright Chapter\n- Safety and Security Chapter\n- Template for publishing GPAI Model Training Data Summary\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n## What is a GPAI model with systemic risk?\n\nThere are two broad categories of GPAI models that the AI Act regulates. These are:\n\n1.  GPAI models\n2.  GPAI models with systemic risk\n\nAs explained in Part 1 of this series, GPAI models 'are trained with a large amount of data using self-supervision at scale [â€¦ ] display significant generality and are capable of performing a wide range of distinct tasks'. The European Commission's recent guidelines also clarify that if an AI model's training compute exceeds 10^23 floatingpoint operations, and it can generate language (either text or audio), or generate image or video based on text inputs, then it should be considered a GPAI model.\n\nAn AI model is classified as a GPAI model with systemic risk if it meets the above definition and criteria and also has ' high impact capabilities '. The AI Act defines this as capabilities that ' match or exceed the capabilities recorded in the most advanced GPAI models '.\n\nIf the cumulative amount of computation used to train a GPAI model exceeds 10^25 floating-point operations, then it is, by default, presumed to have 'high impact capabilities' and thus classified as a GPAI model with systemic risk.\n\nBy relying on this compute threshold, the EU's position is that there is a direct correlation between how much computational resource is used to train an AI model and both the general-purpose capabilities of the model and the level of risk that it poses.\n\nThis means that the larger an AI model is (e.g., in terms of number of parameters) and the more data that is used to train it (e.g., in terms of different examples), the more likely it is to be classified as a GPAI model with systemic risk.\n\nWhilst the European Commission acknowledges that ' training compute is an imperfect proxy for generality and capabilities ', it argues that it is ' the most suitable approach at present' .\n\nHowever, the European Commission is empowered to amend this compute threshold, or introduce an entirely new indicator, via a delegated act. This means it can do so independently, without reopening and amending the AI Act itself. However, the European Parliament and the Council (i.e., the member states) have a right to object to any such changes.\n\nInterestingly, it is possible for a GPAI model to be classified as a GPAI model with systemic risk even if it does not meet the 10^25 floating-point operations compute threshold. This would require the European Commission to determine that it nonetheless has high impact capabilities, despite the lower cumulative amount of compute used to train it.\n\nIn making such a decision-that would prove controversial due to the impact on the impacted provider-the European Commission would consider factors like the size of the model, input and output modalities, benchmark and evaluation results, model\n\nautonomy level, and the number of end-users. Ultimately, it would have to prove that its capabilities match or exceed those of the most advanced GPAI models, despite the fact that less compute was used to train it.\n\n## What are the notification and exception procedures for providers of GPAI models with systemic risk?\n\nThe key point for enterprises is that they must carefully forecast, measure, track, and record their estimates of the amount of computational resource used to develop, train, modify, and fine-tune GPAI models, in order to determine what compliance obligations they may have to adhere to.\n\nWhen estimating and measuring compute levels, the European Commission's guidance is that providers should ' as a general rule, account for all compute that contributed or will contribute to the model's capabilities '. This even includes the compute expended to generate synthetic data for training, even if not all the synthetic data was eventually used to train the GPAI model.\n\nOnce an organisation knows that a GPAI model it has developed (or is in the process of developing) meets the threshold for training compute (which means it is classified\n\nas a GPAI model with systemic risk), it must notify the European Commission of this as soon as possible, and within two weeks at the latest. In some cases, this notification will be required before the overall training process is completed (e.g., if the threshold is exceeded mid-training run). This notification should include both the precise computation amount as well as a detailed explanation of how this has been estimated.\n\nIn its guidelines, the European Commission recommends that ' providers should estimate the cumulative amount of training compute that they will use ' before the training process begins. If their pre-training estimate surpasses the systemic risk threshold, they should inform the Commission of this.\n\nZooming out, this notification procedure enables the European Commission's AI Office to fulfill its role as the regulator overseeing and enforcing the AI Act's provisions on GPAI models. It will also promote transparency, as the European Commission will publish a list of all GPAI models with systemic risk that are in scope of the AI Act.\n\nFinally, it is possible for a provider of a GPAI model that is by default classified as a GPAI model with systemic risk to secure an exception. To do this, the provider must demonstrate that its GPAI model does not have 'high impact capabilities' and therefore does not pose systemic risks and should not be classified as such, despite surpassing the cumulative compute for training threshold of 10^25 floating-point operations.\n\nProviders can do this by pointing to evidence like benchmark and evaluation results, especially if these demonstrate a capability gap between their model and the most advanced AI models. It is important to note that providers cannot get out of the GPAI model with systemic risk classification merely by implementing robust controls and safeguards which mitigate the systemic risk. To secure an exception, they must convince the European Commission, with cold, hard evidence, that the model genuinely does not have high impact capabilities.\n\nThe AI Act describes this as an 'exceptional' scenario, requiring European Commission approval. In such instances, the burden of proof will be on the provider.\n\nGiven the extensive additional compliance obligations for providers of GPAI models with systemic risk (as compared to GPAI models), the question of which GPAI models are and are not classified as posing systemic risk is significant.\n\nPrecisely what these additional compliance obligations are is explained below.\n\n## What are the compliance obligations for providers of GPAI models with systemic risk?\n\nPart 1 of this series provides a detailed breakdown of the compliance obligations for providers of GPAI models. In summary, the four core obligations for GPAI models are:\n\n1.  Develop, maintain, and keep up-to-date comprehensive technical documentation.\n2.  Produce and make publicly available a detailed summary of the content and data used to train the GPAI model.\n3.  Implement a policy to comply with EU copyright and intellectual property law.\n4.  Cooperate with the European Commission and regulatory authorities and appoint an EU-based authorised representative (if based outside of the EU).\n\nProviders of open-source GPAI models are exempt from obligations 1 and 4. This means they still need to publish a training data summary and implement a copyright compliance policy.\n\nProviders of GPAI models with systemic risk must comply with all of the above obligations. Also, providers of open-source GPAI models with systemic risk are not exempt from any of the above obligations. This means that sufficiently advanced and capable open-source AI models are treated the same, from an AI Act compliance perspective, as proprietary models.\n\nIn other words, providers of any GPAI model with systemic risk, that is placed on the market or made available in the EU, irrespective of whether it is open-source, must\n\ncomply with all the above obligations (for providers of GPAI models), as well as the additional obligations for providers of GPAI models with systemic risk.\n\nThere are four core additional obligations that only apply to providers of GPAI models with systemic risk. These are:\n\n1.  Perform model evaluation using state of the art tools and protocols. This includes conducting adversarial testing to enable the identification and mitigation of ' systemic risks '.\n2.  Assess and mitigate potential systemic risks that may stem from the development, deployment, or use of the GPAI model with systemic risk.\n3.  Track, document, and report information about serious incidents and any corrective measures to address them.\n4.  Ensure an 'adequate level of cybersecurity protection' for both the GPAI model with systemic risk and the physical infrastructure of the model.\n\nThese obligations reflect the fact that EU lawmakers deem it both appropriate and necessary for the most advanced and capable foundation models to be subject to rigorous governance-including stringent safety and security testing and evaluation procedures, the implementation of technical guardrails and safeguards to mitigate risk, continuous monitoring and oversight, and documented accountability and risk ownership-due to the widespread use of these models and the distinct possibility\n\nthat this use could lead to significant negative impact.\n\nHowever, the text of the AI Act itself does not provide much detail about how to approach and implement the above four obligations. That is why there is a GPAI Code of Practice. The Code provides a detailed, standardised, and step-by-step compliance framework for GPAI model providers.\n\nThe GPAI Code of Practice: Safety and Security Chapter , which is most relevant for these obligations, is analysed below. But first, we explore the definition of 'systemic risk', which is at the heart of these obligations.\n\n## What exactly is a 'systemic risk'?\n\nThe overarching purpose of the above obligations is for providers to uncover and mitigate the systemic risks which their most capable and advanced GPAI models pose. This includes reducing the likelihood of these risks materialising and reducing their impact if they do materialise.\n\nThis raises an important question for GPAI model providers: what exactly is a 'systemic risk'?\n\nThe GPAI Code of Practic e builds on the AI Act by providing additional detail regarding precisely how providers should define, identify, and evaluate the systemic risks that their GPAI models could pose.\n\nIt classifies the following four risks as systemic risks. This means that if any providers that are also signatories identify any of these risks, they must be classified and treated as systemic risks:\n\n- Chemical, biological, radiological, and nuclear (CBRN) , e.g., a GPAI model that makes it easier or otherwise enables the design, development, and use of CBRNrelated weapons or materials.\n- Loss of control , e.g., a GPAI model that autonomously self-replicates and creates new, more advanced AI models, without human awareness or control.\n- Cyber offence , e.g., a GPAI model that can be used to significantly lower barriers to entry for scaling cyber attacks.\n- Harmful manipulation , e.g., a GPAI model that targets large populations of people and uses deceptive techniques to promote harmful or destructive behaviour.\n\nRecital 110 of the AI Act complements this, by providing an illustrative list of examples of systemic risks:\n\n- Major accidents.\n- Disruptions of critical sectors.\n- Serious consequences to public health and safety.\n- Negative effects of democratic processes.\n- Negative effects on public or economic security.\n- The dissemination of illegal, false, or discriminatory content.\n\nMore broadly the GPAI Code of Practice clarifies the essential characteristics of a systemic risk, to further enable their identification. This clarification is based on the formal definition of systemic risk provided in the AI Act. The three essential characteristics of a systemic risk are:\n\n1.  The risk is directly related to the GPAI model's high-impact capabilities.\n2.  The risk has a significant impact on the EU due to its reach or due to the actual or potential negative impact on public health, safety, public security, fundamental rights, or society as a whole.\n3.  The impact can spread widely, at scale, through connected AI systems and the AI and industry ecosystem more broadly.\n\nThe EU's view is that as model capabilities and model reach increase, so do the potential systemic risks. Recital 110 of the AI Act also highlights that such systemic\n\nrisks can arise due to various factors and causes, including (but not limited to):\n\n- Model misuse\n- Model reliability\n- Model fairness\n- Model security\n- Model autonomy level\n- Tool access\n- Model modalities\n- Release and distribution mechanisms\n- Potential to remove model guardrails\n\nThe detailed information provided in the AI Act and the Code of Practice is sufficient to enable providers of GPAI models with systemic risk to fulfil their obligation of identifying, evaluating, and mitigating the specific systemic risks their GPAI models may pose.\n\nDeep dive on the GPAI Code of Practice: Safety and Security Chapter\n\nThe final section of this article summarises and analyses the key elements of the GPAI Code of Practice: Safety and Security Chapter.\n\nThis Chapter focuses exclusively on the specific obligations for providers of GPAI models with systemic risk. It provides a comprehensive and standardised set of commitments and measures that signatory organisations will implement, in order to adhere to the four compliance obligations for GPAI models with systemic risk (detailed above).\n\nFor a dedicated explainer on the GPAI Code of Practice, check out this previous post on Enterprise AI Governance. The most important things to know about the GPAI Code of Practice are that i) it was approved by the EU on 1 August 2025, ii) it is a voluntary resource which helps providers comply with the full suite of obligations for GPAI models, and iii) it consists of three chapters: 1) Transparency, 2) Copyright, and 3) Safety and Security.\n\nThe European Commission strongly encourages providers to sign the Code of Practice and even indicated that it will be more trusting of signatory organisations. However, the Code itself is not law.\n\nThe Safety and Security Chapter, which is by far the most detailed of the three chapters, consists of ten commitments. All commitments directly relate to GPAI model with systemic risk risk identification, management, mitigation, treatment, monitoring,\n\nownership, and accountability.\n\nBelow is a detailed breakdown of each of the ten commitments and a summary of the most important measures that signatory organisations have committed to implementing.", "fetched_at_utc": "2026-02-09T13:52:27Z", "sha256": "8a89bcb2e20d4fac684e9bd81b11c216d4bacec3cc31def65e740339999263f1", "meta": {"file_name": "General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel.pdf", "file_size": 1723762, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-governing-ai-in-2026-onetrust-327e9d741e59", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Governing AI in 2026 - onetrust.pdf", "title": "Governing AI in 2026 - onetrust", "text": "## Governing AI in 2026:\n\nA global regulatory guide\n\n2026\n\n<!-- image -->\n\n## Table of Contents\n\n| The role of privacy and compliance teams in AI Governance                                                            | .  .3   |\n|----------------------------------------------------------------------------------------------------------------------|---------|\n| Europe: Enforcement-ready AI governance .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .                      | .  .4   |\n| United States: State-led AI enforcement .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .             | .  .5   |\n| Asia-Pacific: Binding rules and early enforcement  .  .  .  .  .  .  .  .  .  .  .                                   | .  .6   |\n| Latin America: Brazil's AI framework takes shape .  .  .  .  .  .  .  .  .  .  .  .  .                               | .  .7   |\n| Howto operationalize AI-readiness .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . | .  .7   |\n\nAppendix - Regional regulatory comparison table  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 9\n\n## DISCLAIMER:\n\nNo part of this document may be reproduced in any form without the written permission of OneTrust .\n\nThe contents of this document may be revised by OneTrust in its sole discretion, without notice, due to continued progress in the methodology of the Certification, any changes in applicable laws, regulations or related guidance, or for any other reason . OneTrust shall have no liability for any error or damage of any kind resulting from the use of this document, its contents or the information provided therewith .\n\nThe contents of this document, any materials and other information conveyed during this Privacy Automation Certification are for informational purposes only and do not constitute legal advice (and should not be relied upon as such) .\n\nThe pace of AI regulation has accelerated sharply. In 2025 alone, more than 3,200 regulatory updates were issued worldwide, with 875 directly related to AI laws and regulations. By the end of the year, 51 AI laws were already in force, 15 had been passed, and 97 more were in progress. In the United States, over 40 states introduced or considered close to 700 AI-related bills1.\n\nThis shift is no longer theoretical. Enforcement activity across privacy and AI is intensifying, with over â‚¬2 billion in GDPR enforcement actions in 2025, including some of the largest fines on record. Regulators are now applying similar expectations to AI systems that influence individuals' rights, access, and opportunities.\n\nThis whitepaper examines how global AI regulation applies through 2026, with a focus on what privacy and compliance teams must operationalize today. It translates binding legal obligations into governance actions, using Europe and the United States as anchors, while addressing APAC and Latin America as rapidly maturing enforcement regions.\n\n1 OneTrust 2026 Predictions Report: Into the Age of AI Lessons from the Future\n\n## 1.  The role of privacy and compliance teams in AI Governance\n\nArtificial intelligence now shapes hiring decisions, credit assessments, healthcare access, pricing, content moderation, and public services. As these systems move from experimentation into production, regulators are assessing whether organizations can control risk, explain outcomes, and demonstrate accountability.\n\nAI regulation does not replace privacy law. It extends privacy governance into automated and algorithmic systems that affect individuals at scale. Across jurisdictions, regulators expect organizations to:\n\n<!-- image -->\n\n- Identify where AI is used in decision-making\n- Assess risks to individuals and fundamental rights\n- Provide clear notice when AI influences outcomes\n- Maintain documentation that demonstrates accountability\n- Monitor systems after deployment and respond to incidents\n\nThese expectations closely mirror established privacy program responsibilities. As a result, privacy and compliance teams are increasingly responsible for making AI governance work in practice, even when AI development sits elsewhere in the organization.\n\n## Core regulatory patterns shaping AI governance\n\nAcross jurisdictions, and despite regional differences, binding AI laws follow a common structure:\n\n- Risk-based classification: Most laws distinguish AI systems by impact, not technology. Systems used in employment, credit, healthcare, education, public services, or biometric identification consistently fall into higher-risk categories and trigger additional obligations.\n- Role-based accountability: Regulators assign responsibilities across the AI lifecycle. Developers, deployers, distributors, and providers each carry distinct duties. This mirrors controller-processor models under privacy law and requires clear internal role definition.\n- Accountability through evidence: Documentation, logging, assessments, and monitoring are treated as proof that governance exists in practice. Regulators increasingly view the absence of documentation as evidence of noncompliance.\n\nFor privacy teams, these requirements are not unfamiliar. They extend existing governance practices into AI-driven decision-making and automated systems.\n\n## 2. Europe: Enforcement-ready AI governance\n\n## Regulatory overview\n\nThe EU Artificial Intelligence Act is the most comprehensive AI regulation currently in force. Its risk-based model classifies systems as unacceptable risk, high risk, specific transparency risk, and limited risk, with obligations scaling accordingly.\n\nIt entered into force in August 2024, with obligations phasing in through 2027. By 2026, organizations will already be expected to comply with:\n\n- Prohibitions on certain AI practices\n- Transparency obligations for AI interactions\n- Governance requirements for general-purpose AI models\n- Penalty provisions enforced by national authorities and the EU AI Office\n\nHigh-risk AI systems must undergo pre-deployment assessments, maintain technical documentation, log system activity, and support post-market monitoring. Deployers must assess impacts on fundamental rights, reinforcing existing DPIA practices under GDPR.\n\n<!-- image -->\n\n## Key obligations by actor\n\n| Actor        | Core obligations                                                                            |\n|--------------|---------------------------------------------------------------------------------------------|\n| Providers    | Technical documentation, conformity assessments, post-market monitoring, incident reporting |\n| Deployers    | Fundamental rights impact assessments, usage controls, monitoring                           |\n| Distributors | Verification of conformity and documentation                                                |\n\n## Operational implications for privacy teams\n\nPrivacy teams are often responsible for:\n\n- Integrating AI risk assessments with DPIA workflows\n- Supporting fundamental rights impact assessments\n- Maintaining documentation repositories\n- Coordinating responses to regulator inquiries\n\n## The role of the EU Digital Omnibus\n\nThe Digital Omnibus proposal introduced in late 2025 seeks to align the GDPR, the AI Act, and ePrivacy obligations. It proposes adjustments to definitions of personal data, data subject rights, and legitimate interest, including broader flexibility for AI training.\n\nWhile still under debate, the Omnibus reflects a shift in regulatory posture. European regulators are looking to simplify compliance mechanics without stepping back from oversight. For privacy teams, this suggests continued scrutiny of automated decision-making, profiling, and transparency, even as operational details evolve.\n\n<!-- image -->\n\n## 3. United States: State-led AI enforcement\n\nIn the absence of a federal AI statute, US states are defining enforceable standards through consumer protection and civil rights frameworks.\n\nCalifornia, Colorado, and Texas are setting expectations around:\n\n- Disclosure when individuals interact with AI\n- Documentation of AI system purpose and limitations\n- Controls to prevent discriminatory outcomes\n- Oversight tied to existing enforcement authorities\n\n## Key laws effective in 2026\n\n| State      | Law                              | Effective date   | Focus                        |\n|------------|----------------------------------|------------------|------------------------------|\n| California | AI TransparencyAct               | Jan 1, 2026      | Disclosure, content labeling |\n| California | GenAITrainingDataTransparencyAct | Jan 1, 2026      | Dataset transparency         |\n| Colorado   | AI Act                           | Jun 30, 2026     | Algorithmic discrimination   |\n| Texas      | Responsible AI Governance Act    | Jan 1, 2026      | Prohibited practices         |\n\nThese laws emphasize disclosure when individuals interact with AI, documentation of system purpose and limitations, and safeguards against discriminatory outcomes. Legislation also heavily focuses on specific use cases of AI, such as consumer transactions, healthcare, and deepfakes. Enforcement relies on existing authorities such as state attorneys general, with penalties tied to ongoing violations.\n\n## Operational implications for privacy teams\n\nPrivacy teams must ensure AI notices align with consumer privacy disclosures, rights request workflows accommodate AIdriven decisions, and documentation supports reasonable care defenses under state enforcement models.\n\n## 4. Asia-Pacific: Binding rules and early enforcement\n\nSeveral APAC jurisdictions have already moved beyond voluntary guidance and operate under binding AI frameworks.\n\nSouth Korea's Basic AI Act enters into force on January 22, 2026. It applies extraterritorially where systems affect Korean\n\n<!-- image -->\n\nusers and introduces requirements for transparency, risk assessment, human oversight, and documentation, particularly for high-impact and large-scale AI systems. A draft enforcement decree published in September 2025 clarifies watermarking, disclosure, and oversight obligations.\n\nChina enforces multiple AI regulations, including the Generative AI Services Management Measures and Measures for the Identification of Synthetic Content Generated by AI effective September 1, 2025.\n\nThese laws impose obligations around consent, data quality, content labeling, user rights, and complaint handling.\n\nJapan relies on a principles-based AI Act emphasizing cooperation and transparency rather than penalties. Vietnam's Law on Digital Technology introduces binding AI provisions effective in 2026, with a comprehensive AI Law entering into force on March 1, 2026, which includes labeling, transparency, and prohibitions tied to human rights and public order.\n\nAcross the region, AI governance is increasingly linked to data protection, security, and rights-based oversight.\n\n## Comparative overview\n\n| Jurisdiction   | Law                      | Status        | Key focus                      |\n|----------------|--------------------------|---------------|--------------------------------|\n| China          | GenAI Services Measures  | In force      | Consent, labeling, user rights |\n| China          | SyntheticContentMeasures | Sep 1, 2025   | Content identification         |\n| South Korea    | Basic AI Act             | Jan 22, 2026  | High-impact AI governance      |\n| Vietnam        | LawonAI                  | March 1, 2026 | Transparency, prohibitions     |\n| Japan          | AI Act                   | In force      | Principles-based governance    |\n\n## Operational implications for privacy teams\n\nPrivacy teams operating in APAC must manage overlapping AI, data protection, and content obligations, maintain localized documentation, and support user rights and complaint mechanisms embedded in AI regulations.\n\n## 5. Latin America: Brazil's AI framework takes shape\n\nBrazil is positioning itself as a leading AI regulator in Latin America. Bill No. 2338, approved by the Senate in December 2024 and awaiting final approval, introduces a comprehensive, risk-based AI framework aligned with the EU AI Act.\n\nIf enacted, organizations would need to support impact assessments, incident reporting, transparency obligations, and individual rights to contest AI-driven decisions, request human review, and seek correction of discriminatory outcomes.\n\n<!-- image -->\n\n## Operational implications for privacy teams\n\nBrazil's framework places privacy teams at the center of AI governance by embedding rights-based protections, assessment requirements, and accountability mechanisms directly into AI regulation.\n\n## 6. How to operationalize AI-readiness\n\nEffective AI-readiness requires extending privacy operations, not rebuilding them from scratch. Organizations need the ability to inventory AI systems, connect risk assessments to product changes, manage disclosures consistently, and maintain evidence across jurisdictions.\n\nIn practice, this means replacing fragmented spreadsheets and ad hoc reviews with workflows that embed assessment, documentation, monitoring, and response into day-to-day operations. Privacy teams benefit from centralized visibility into AI use cases, integrated assessment processes aligned with\n\nDPIAs, automated tracking of regulatory changes, and scalable handling of rights and incident requests tied to AI-driven outcomes.\n\nWhen governance is operationalized, teams spend less time chasing information and more time managing risk proactively. This reduces regulatory exposure while enabling responsible AI deployment at speed.\n\n## Governance as an enabler through 2026\n\nKey AI regulatory milestones through 2026 include the phased application of the EU AI Act, the entry into force of multiple US state AI laws on January 1 and June 30, 2026, South Korea's Basic AI Act on January 22, 2026, and binding AI provisions across APAC and Latin America.\n\nOrganizations that reach these milestones with mature privacy programs in place will be better positioned to adapt. A wellrun privacy function provides the structure AI governance now demands: clear ownership, documented assessments, transparent communication, and continuous monitoring.\n\nAs AI regulation moves deeper into enforcement, privacy becomes more than a compliance requirement. It becomes an enabler for innovation, allowing organizations to deploy AI responsibly, earn trust, and scale with confidence across global markets.\n\n## Assess your AI governance readiness for 2026.\n\nExolore our integrated privacy solutions to evaluate current privacy and AI controls against emerging regulatory expectations and identify operational gaps.\n\nLearn more\n\n<!-- image -->\n\n## Appendix - Regional regulatory comparison table\n\n| Region         | Law                                                                               | Effective timeline                                                           | Scope                                                                        | Key focus areas                                                                       | Enforcement                                                               |\n|----------------|-----------------------------------------------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|---------------------------------------------------------------------------|\n| European Union | In force August2024, phased application through2027 EUArtificial Intelligence Act | Extraterritorial. Applies toAI systemsused oraffecting individuals in the EU |                                                                              | Riskclassification, high-risksystem obligations,GPAI governance, prohibited practices | National authorities andEUAIOffice. Finesupto7 percent of global turnover |\n| United States  | ColoradoAI Act                                                                    | June30,2026                                                                  | Developersand deployers of high- riskAIsystems operating in Colorado         | Algorithmic discrimination, consumer transparency, documentation                      | ColoradoAttorney General. Unfair trade practice model                     |\n| United States  | CaliforniaAI Transparency Actand GenAI TrainingData Transparency Act              | January1, 2026                                                               | Largegenerative AIprovidersand developers of publicly available GenAIsystems | AI-generated content disclosure, dataset transparency, provenance controls            | CaliforniaAttorney Generalandlocal authorities                            |\n| United States  | Texas Responsible Artificial Intelligence Governance Act                          | January1, 2026                                                               | Broad,with primary obligationson governmental agencies                       | Prohibited AI practices, biometric protections, transparency                          | TexasAttorney Generalwithcure periods                                     |\n| Asia- Pacific  | SouthKorea BasicAIAct                                                             | January22, 2026                                                              | Extraterritorial. ApplieswhereAI systemsaffect Koreanusers                   | High-impactAI, riskassessment, humanoversight, documentation                          | Ministryof ScienceandICT. Administrativeand criminal penalties            |\n\n<!-- image -->\n\nNo part of this document may be reproduced in any form without the written permission of the copyright owner. The contents of this document are subject to revision without notice due to continued progress in methodology, design, and manufacturing.\n\nThis document has been prepared for general informational purposes only and is not intended to provide, nor should it be construed as providing, legal advice. The information herein may not reflect the most current legal developments. You should consult with qualified legal counsel before acting on any information contained herein.\n\nCopyright Â© 2026 OneTrust LLC. All rights reserved. Proprietary &amp; Confidential.", "fetched_at_utc": "2026-02-09T13:52:52Z", "sha256": "327e9d741e5938c31ee3b488bd905bf000fbbedd3b7cab2a8d0555ca5e5011a2", "meta": {"file_name": "Governing AI in 2026 - onetrust.pdf", "file_size": 1277549, "mtime": 1770638509, "docling_errors": []}}
{"doc_id": "pdf-pdfs-how-could-the-eu-ai-act-change-oliver-patel-0e00aa8548c8", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\How could the EU AI Act change - Oliver Patel.pdf", "title": "How could the EU AI Act change - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nOn Wednesday 19 November 2025, the European Commission unveiled its Digital Omnibus Package, proposing targeted yet impactful amendments to the EU AI Act. This article distils what could change, why it matters for enterprise AI governance practitioners, and what to watch as trilogue negotiations begin.\n\nIf you value my work and want to learn more about the EU AI Act and AI governance implementation, sign up to secure a 25% discount for my forthcoming book, Fundamentals of AI Governance (2026).\n\n## What are the most important proposed EU AI Act changes?\n\nOn Wednesday 19 November 2025, the European Commission (henceforth the Commission) announced proposed changes to the AI Act. These changes are presented as 'innovation-friendly AI rules' that will 'reduce compliance costs for businesses'. It did so by publishing a proposal for a new regulation. The purpose of this proposed regulation is to 'simplify' the AI Act with targeted yet meaningful amendments.\n\nThis is part of the Commission's broader 'Digital Package', which is a major programme of work aiming to 'simplify EU digital rules and boost innovation'. The\n\nDigital Package-which encompasses the 'Digital Omnibus Regulation'-includes proposals to amend flagship digital laws like the AI Act, the GDPR, the ePrivacy Directive, the Data Act, and the NIS 2 Directive. Specifically, the Commission simultaneously published proposals for two regulations (so it's not really an 'omnibus' anymore):\n\n- Proposal for Regulation on simplification of AI rules (which covers the AI Act amendments); and\n- Proposal for Regulation on simplification of the digital legislation (which covers the amendments to the other EU digital laws mentioned above).\n\nThis article explains and analyses the six most important AI Act amendments that enterprise AI governance professionals need to understand. These are:\n\n1. Timeline changes for high-risk AI system compliance.\n2. Timeline changes for transparency-requiring AI system compliance.\n3. Limiting registration in the public EU database for high-risk AI systems.\n4. Softening of the AI literacy obligation.\n5. Expanding the scope of the European AI Office's regulatory powers.\n6. Proportionality for small mid-cap (SMC) enterprises.\n\nFor each of these six proposed amendments, I explain what is in the law today, what changes are being proposed, and what the impact of these changes would be.\n\nEarlier this week, I published an article on Enterprise AI Governance that explains how we got to this point and why the EU is now doing this. It provides a detailed account of the background context to AI Act simplification, highlighting how the 'Draghi report'-which argued that digital regulatory burdens are impeding European growth and competitiveness-has influenced the Commission's proposals.\n\nIt also outlines three important caveats on the EU's legislative process that are worth repeating:\n\n- This merely represents the proposal of one EU institution (the Commission). Such amendments of EU law require formal approval from both the European Parliament and the EU member states via the Council of the EU (the Council).\n- Therefore, this proposal will now be followed by lengthy and potentially fraught trilogue negotiations between the Commission, European Parliament, and Council.\n- Finally, it is impossible to predict what the final legislative text will consist of, how long the negotiation and approval process will take, and whether approval to amend the AI Act will ultimately be agreed on and enacted.\n\nScope of this article: this is not an exhaustive analysis of the entire AI Act simplification proposal and it does not cover every proposed amendment in the Commission's 65-page document. Rather, it focuses on the six proposed changes that would be most consequential (if passed) for enterprises implementing AI governance. Also, it intentionally does not cover the proposed changes to the GDPR, nor the AI Act amendments that are directly related to the processing of personal data (e.g., use of sensitive personal data for bias mitigation), as this topic will be addressed in a future article on Enterprise AI Governance.\n\nDisclaimer: this article is not intended to be legal advice and must not be relied upon or used in that way. Always consult a qualified legal professional.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n## 1. Timeline changes for high-risk AI system compliance\n\nWhat is in law today?\n\nThe key provisions relating to high-risk AI systems apply from 2 August 2026. This means that from 2 August 2026, unless there is a change in the law, providers and deployers must adhere to the obligations and requirements for high-risk AI systems and can be subject to investigations and penalties for non-compliance. However, this applicable date only applies to high-risk AI systems listed in Annex III (e.g., education, employment, administration of justice etc.) that are placed on the market or put into service from 2 August 2026 onwards.\n\nFor such Annex III high-risk AI systems that were placed on the market or put into service before 2 August 2026, providers and deployers are only subject to AI Act obligations and requirements if, from that date onwards, there is a significant change in design or intended purpose of the AI system. Furthermore, for high-risk AI systems that are products, or safety components of products, regulated by specific EU product safety laws listed in Annex I, the applicable date is 2 August 2027.\n\n## What changes are being proposed?\n\nIf you are asked 'when do the compliance obligations for high-risk AI systems apply?' , your answer now has to be 'it depends'.\n\nGiven the nature of the proposed amendments, there are various potential scenarios. The Commission is seeking to link the applicability of high-risk AI system provisions\n\nwith the availability of technical standards and associated support tools. However, if these artefacts are not approved and available within a certain timeframe, there is a backstop date, which represents the latest applicable date.\n\nUnder this proposal there are, broadly speaking, three potential scenarios for when most of the provisions relating to high-risk AI systems may apply (covering high-risk AI system classification, development requirements, and obligations of providers, deployers, and other parties):\n\nScenario 1. If technical standards and associated support tools for high-risk AI system compliance are finalised and approved by the Commission, then the applicable compliance date will be six months after this approval (for high-risk AI systems listed in Annex III) and 12 months after this approval (for high-risk AI systems that are products, or safety components of products, regulated by an EU law listed in Annex I).\n\nScenario 2. However, if technical standards and associated support tools for high-risk AI system compliance are not finalised or approved by the Commission in time (i.e., before the dates below), then the applicable compliance dates will be 2 December 2027 (for high-risk AI systems listed in Annex III) and 2 August 2028 (for high-risk AI systems that are products, or safety components of products, regulated by an EU law listed in Annex I).\n\nScenario 3. Given that the applicable date in law today is 2 August 2026, if these amendments are not approved and enacted before this date, then the provisions relating to high-risk AI systems will, technically speaking, apply from then. This creates timeline pressure to get these changes approved within the next few months.\n\n## What impact would these changes have?\n\nTo clarify, 2 December 2027 and 2 August 2028 are the backstop dates for high-risk AI system compliance. To reinforce this point, the Commission has explained that the grace period will be up to sixteen months (referring to the time between 2 August 2026 and 2 December 2027). This means that if technical standards come too late (i.e., after 2 June 2027, which is six months before 2 December 2027) these backstop dates will apply.\n\nThese amendments shine the spotlight on the ongoing work being led by CEN/CENELEC to agree and publish technical standards. They also highlight the importance that the Commission places on these artefacts to support organisations and facilitate compliance.\n\nHowever, even if the applicable date may be delayed-giving providers and deployers more time to prepare-this extra time has been achieved at the expense of certainty, with organisations now not knowing what the applicable date will be.\n\n## 2. Timeline changes for transparency-requiring AI system compliance\n\n## What is in law today?\n\nArticle 50 of the AI Act outlines transparency obligations for providers and deployers of certain AI systems. Article 50 covers obligations relating to disclosure, informing end users about the use of AI, labelling certain deep fake content, and detectability of AI system outputs. Specifically, Article 50(2) stipulates that:\n\n'providers of AI systems, including general-purpose AI systems, that generate synthetic audio, image, video, or text content shall ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulated'.\n\nCurrently, this specific obligation applies from 2 August 2026. This compliance date applies to all AI systems, irrespective of whether they are placed on the market or put into service before or after 2 August 2026.\n\n## What changes are being proposed?\n\nThe Commission proposes to push back the applicable date for this specific transparency obligation to 2 February 2027 for providers of AI systems that have been placed on the market before 2 August 2026. This proposed six month delay to the applicable date only applies to obligation stipulated in Article 50(2) (on AI system output machine readability and detectability) and not the other transparency obligations outlined in Article 50.\n\n## What impact would these changes have?\n\nThis change would give providers of AI systems that have already been placed on the market or put into service, or that will be before 2 August 2026, and that generate synthetic audio, image, video, or text content (i.e., most generative AI systems), an additional six months to ensure that these AI systems are developed in such a way that ensures the outputs they generate are detectable as AI-generated.\n\nAlthough this is a relatively short delay, it is nonetheless an acknowledgement by the Commission of the technical and engineering challenges providers face in developing or modifying their AI systems to adhere to this obligation. However, any AI systems placed on the market on or after 2 August 2026 will have to comply with this obligation from their release date.\n\n## 3. Limiting registration in the EU public database for high-risk AI\n\n## systems\n\n## What is in law today?\n\nAnnex III of the AI Act lists eight categories of high-risk AI system, including law enforcement (#6), education and vocational training (#3), and employment, workers' management and access to self-employment (#5). However, there are classification rules which mean that just because an AI system is intended for use or used in one of these domains does not necessarily mean it is a high-risk AI system.\n\nAI systems listed in Annex III are not considered high-risk if it is demonstrated that they do not pose significant risk of harm to health, safety, or fundamental rights. For example, if the AI system does not materially influence decisions or is only used for a narrow procedural task, the provider is entitled to demonstrate, based on a documented assessment, that it is not a high-risk AI system. This derogation, including the conditions to fulfil it, is outlined in Article 6(3) and only applies to AI systems listed in Annex III.\n\nProviders must register high-risk AI systems listed in Annex III in the EU public database for high-risk AI systems, before those AI systems are placed on the market or put into service. Interestingly, this registration obligation also includes AI systems that\n\nthe provider has concluded are not high-risk via the derogation procedure outlined in Article 6(3).\n\n## What changes are being proposed?\n\nThe Commission proposes to limit the scope of this registration obligation so that it no longer applies to AI systems that providers have concluded are not high-risk via the Article 6(3) derogation procedure. Simply put, where a provider has assessed and documented that an AI system used in an Annex III domain is not high-risk, the provider will not have to register that AI system in the EU public database for high-risk AI systems.\n\nHowever, although providers can make this assessment independently and do not require any external approval (e.g., from the AI Office or market surveillance authority), providers will still be obliged to share the documentation of the assessment, containing the justification and supporting evidence, upon request from a regulator.\n\n## What impact would this have?\n\nThis may seem like a subtle change at first glance, but it would be consequential for organisations using AI at scale.\n\nMost enterprise AI governance practitioners likely raised their eyebrows when they realised that every AI system used in a high-risk domain, including AI systems that are not high-risk due to their use for mere assistive, procedural, or preparatory tasks, would have to be registered. This will be difficult (or perhaps near impossible) to keep track of and implement, due to the increasingly ubiquitous use of AI to support and augment workflows across virtually all domains of enterprise activity. Indeed, the Commission describes this current registration obligation as a 'disproportionate compliance burden'.\n\nTherefore, the most obvious impacts of this change would likely be far fewer AI systems registered in the EU public database for high-risk AI systems and reduced administrative overheads for organisations developing and deploying AI systems.\n\nHowever, it would also reduce public transparency regarding which AI systems providers deem not to be high-risk and how they have made such determinations. This could incentivise some providers to take a more expansive approach to interpreting Article 6(3) and determining what is not a high-risk AI system, as they may reasonably judge that the risk of doing so (and being penalised for getting it wrong) is lower with significantly less public scrutiny.\n\n## 4. Softening of the AI literacy obligation\n\n## What is in law today?\n\nUnder Article 4 of the AI Act providers and deployers of AI systems are obliged to implement 'AI literacy'. This is one of the most important aspects of the law, because it has contributed to many organisations in the EU and further afield rolling out AI training and upskilling initiatives for their workforce. Specifically, Article 4 requires organisations to ensure that 'staff and other persons dealing with the operation and use of AI systems' have a 'sufficient level of AI literacy'.\n\nIn practice, given that all staff in modern organisations can use AI systems (e.g., ChatGPT or Gemini), a reasonable interpretation of Article 4 is that all of these staff should receive some form of AI-focused training. The AI literacy obligation has been applicable since February 2025. However, there are no enforcement penalties for noncompliance with it. Although non-compliance could be taken into account during enforcement investigations or proceedings relating to other aspects of noncompliance.\n\n## What changes are being proposed?\n\nThe Commission proposes to remove the obligation for providers and deployers to implement AI literacy. Rather than providers and deployers being legally required to ensure their staff operating and using AI systems have sufficient levels of AI literacy,\n\nthe Commission and Member States will be required to foster AI literacy and 'encourage providers and deployers of AI systems to take measures to ensure a sufficient level of AI literacy' . The Commission has alluded to the fact that the ambiguity of the current 'unspecified obligation' has caused issues for businesses-especially smaller firms.\n\n## What impact would this have?\n\nThis change would be significant because it would remove the broad and expansive legal obligation for companies to implement AI literacy. However, human oversight of high-risk AI systems must still be assigned to staff with sufficient training and competence. Therefore, ensuring AI literacy is still required in that context.\n\nMoreover, it will be practically impossible for any organisation to comply with the AI Act-or to manage AI risks, implement AI at scale, and maximise the value of AIwithout educational and training initiatives focused on AI. Therefore, forward-thinking enterprises are unlikely to abandon their AI literacy programmes because it is no longer a legal requirement. However, certain initiatives may be scaled back, deprioritised, or change in focus or scope.\n\n## 5. Expanding the scope of the European AI Office's regulatory powers\n\n## What is in law today?\n\nHere is a simplified summary of the (rather complex) AI Act governance regime:\n\n- There are governance and regulatory bodies at both the EU and member state level.\n- At the EU level, the most important bodies are the European AI Office (which is part of the Commission) and the European AI Board.\n- At the member state level, the most important bodies are the market surveillance authorities. They are responsible for monitoring, investigations, and enforcement of the AI Act. There will potentially be several in each EU member state.\n- The AI Office is responsible for overseeing and enforcing the provisions on general-purpose AI models, whereas the market surveillance authorities are responsible for overseeing and enforcing the provisions on AI systems (e.g., highrisk and transparency-requiring AI systems), as well as most other AI Act provisions.\n\n## What changes are being proposed?\n\nThe Commission proposes to 'centralise oversight over a large number of AI systems built on general-purpose AI models' when the same provider develops both the general-purpose AI model and the AI system.\n\nThe proposed amendments to Article 75 would render the AI Office as the body responsible for monitoring and supervising compliance of AI systems that leverage general-purpose AI models. However, this would only apply when the general-purpose AI model and the AI system are developed and placed on the market or put into service by the same provider. In such scenarios, the AI Office would be 'exclusively competent', which means that the market surveillance authorities in the respective EU member states would no longer have a supervisory role. The AI Office would also have 'all the powers of a market surveillance authority'.\n\nThis expansion in scope of the AI Office's responsibilities does not apply to high-risk AI systems covered by an Annex I EU product safety law. Therefore, this change primarily impacts high-risk AI systems listed in Annex III and transparency-requiring AI systems regulated by Article 50 (where such AI systems leverage general-purpose AI models).\n\nFinally, under this proposal, the AI Office would also have exclusive competence as the regulator of 'AI systems that constitute or that are integrated into a designated very large online platform or very large online search engine' (as defined and regulated by the Digital Services Act).\n\n## What impact would this have?\n\nThe implied rationale behind this change is that the Commission does not think it makes sense for the AI Office to be responsible for overseeing providers of generalpurpose AI models but not the AI systems developed, made available, and put into service by those same providers.\n\nThese changes would make the AI Office the supervisory authority for many of the most widely used AI systems worldwide. This is because most mainstream generative AI platforms, such as ChatGPT, Gemini, Claude, Grok, and Microsoft Copilot, are AI systems built on general-purpose AI models, with the same organisation being the provider of both the AI model and the AI system. Therefore, this would represent a meaningful increase in the relevance and prominence of the AI Office for AI Act oversight and enforcement, and it would also enable the AI Office to pursue investigations and enforcement action relevant for both general-purpose AI model and AI system compliance in a coordinated manner.\n\nThis would also mean that certain AI system providers would not be subject to regulatory investigations and enforcement action across multiple EU member states, which is possible if the law isn't amended.\n\n## 6. Proportionality for small mid-cap (SMC) enterprises\n\nThe AI Act provides an element of flexibility and proportionality for micro, small, and medium-size enterprises (SMEs), including start-ups. For example, the compliance penalties which SMEs can face are capped in the following way:\n\n- 35 million EUR or 7% of total worldwide annual turnover (whichever is lower).\n- 15 million EUR or 3% of total worldwide annual turnover (whichever is lower).\n- 7.5 million EUR or 1% of total worldwide annual turnover (whichever is lower).\n\nThis contrasts with the 'whichever is higher' penalty logic that applies for all other businesses (i.e., those which are not SMEs). In practice, this means that many non-SME businesses could face potential penalties into the billions of euros, whereas penalties for SMEs will always be capped as per the above.\n\nOther ways in which the AI Act seeks to ease compliance burdens for SMEs include allowing SME providers of high-risk AI systems to provide the required technical documentation in a simplified way and providing SMEs with free access to AI regulatory sandboxes.\n\n## What changes are being proposed?\n\nThe first proposed change is to add legal definitions of SME and small mid-cap enterprise (SMC) to the AI Act. These are:\n\n- SME: an enterprise which employs fewer than 250 people and which has an annual turnover not exceeding 50 million EUR, and/or an annual balance sheet total not exceeding 43 million EUR.\n- SMC: an enterprise which employs fewer than 750 people and which has an annual turnover not exceeding 150m EUR or an annual balance sheet total not exceeding 129m EUR.\n\nThe second and more significant proposed change is to extend the flexibility and proportionality penalties afforded to SMEs to SMCs also. This means that SMCs would benefit from the same capped enforcement penalty regime as SMEs, significantly reducing their total potential penalty exposure in certain circumstances. SMCs that are providers of high-risk AI systems would also be able to provide the required technical documentation in a simplified manner.\n\n## What impact would this have?\n\n<!-- image -->\n\nCore threads from the Draghi report are woven throughout this proposal. The Commission will be hoping that easing regulatory compliance burdens and softening the enforcement environment for a larger pool of companies will make it easier for EU digital start-ups and scale-ups to grow, innovate, and compete internationally. Indeed, the Draghi report argued that 'regulatory burdens' are particularly damaging for digital sector SMEs trying to rapidly scale up.\n\nThanks for reading! Subscribe below for weekly updates from Enterprise AI Governance.\n\n<!-- image -->\n\n12 Likes âˆ™ 2 Restacks\n\n## Discussion about this post\n\nComments Restacks\n\nWrite a comment...", "fetched_at_utc": "2026-02-09T13:53:28Z", "sha256": "0e00aa8548c8c9e73905a57ef9705b7450498b8623f2216cfab7fe91507e1375", "meta": {"file_name": "How could the EU AI Act change - Oliver Patel.pdf", "file_size": 802855, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-industrial-ai-robustness-card-evaluating-and-monitoring-time-series-models-5449434a5fcc", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Industrial AI Robustness Card - Evaluating And Monitoring Time Series Models.pdf", "title": "Industrial AI Robustness Card - Evaluating And Monitoring Time Series Models", "text": "## INDUSTRIAL AI ROBUSTNESS CARD: EVALUATING AND MONITORING TIME SERIES MODELS\n\n## Alexander Windmann\n\nInstitute of Artificial Intelligence Helmut Schmidt University Hamburg, Germany alexander.windmann@hsu-hh.de\n\n## Mariya Lyashenko\n\n## Benedikt Stratmann\n\nFraunhofer Institute of Optronics System Technologies and Image Exploitation (IOSB) Karlsruhe, Germany benedikt.stratmann@iosb.fraunhofer.de\n\n## Oliver Niggemann\n\nDigital Industries, Process Automation Siemens AG Karlsruhe, Germany mariya.lyashenko@siemens.com\n\n## ABSTRACT\n\nIndustrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.\n\nK eywords Industrial artificial intelligence Â· Reliability and safety in processes Â· Cyber-physical production systems Â· Manufacturing prognostics and health management Â· Machine learning and artificial intelligence in chemical process control\n\n## 1 Introduction\n\nDespite the intention to increase Artificial Intelligence (AI) adoption, most AI projects of manufacturers fail to meet expectations, see BCG (2023). Empirical studies such as Windmann et al. (2024) identify a lack of trustworthiness as a central barrier: AI models are often not robust, while assurance and explainability techniques remain difficult to apply to complex neural networks. Furthermore, regulators and standardization bodies demand stronger requirements for monitoring and documenting AI systems. For example, the EU AI Act requires risk management, technical documentation, and post-market monitoring for high-risk systems. Standards like the NIST AI Risk Management Framework by Tabassi (2023) or ISO/IEC 23894 require systematic evaluation and documentation of trustworthiness properties such as robustness, but they remain high-level and do not prescribe concrete robustness metrics or test scenarios to be used. More technical standards for robustness assessment, such as ISO/IEC 24029-2 often emphasize formal verification and worst-case guarantees that are not broadly applicable to typical Industrial AI use cases, see PerezCerrolaza et al. (2024). Guidelines like EASA (2024) provide more detailed evaluation recommendations but focus on domains such as aviation and are thus only partially relevant for industrial time series. In parallel, documentation practices of the industry such as Model Cards introduced by Mitchell et al. (2019) or the IBM FactSheet by Arnold et al. (2019) offer compact templates to describe model scope, intended use, and limitations, but generally tend to focus\n\nInstitute of Artificial Intelligence Helmut Schmidt University Hamburg, Germany oliver.niggemann@hsu-hh.de\n\non what to document rather than describing how to measure and report trustworthiness properties like robustness or uncertainty of an AI model.\n\nTo summarize, Industrial AI practitioners currently face a mismatch. Emerging regulations and standards describe high-level robustness and monitoring obligations, but current projects lack a concrete, lightweight protocol that turns these obligations into empirical tests, metrics, and documentation for typical time series applications, see also DÃ­azRodrÃ­guez et al. (2023). To address this gap, we propose the Industrial AI Robustness Card (IARC), a lightweight protocol for empirical robustness evaluation and monitoring of AI models for industrial time series data. An example of a filled Industrial AI Robustness Card is provided in Figure 1. Our main contributions are:\n\n- A minimal, task-agnostic schema for documenting and evaluating AI models for industrial time series.\n- A measurement and reporting protocol that operationalizes the IARC using best practices from uncertainty quantification and robustness evaluation.\n- A compact mapping from IARC fields to relevant obligations of the EU AI Act.\n- A case study on an industrial soft sensor, including released code and a human- and machine-readable filled IARC template. 1\n\n## 2 Background and Related Work\n\nIndustrial AI focuses on the application of AI in cyber-physical systems, with typical applications spanning condition monitoring, predictive maintenance, resource optimization, quality assurance, and diagnosis, see Niggemann et al. (2023). For chemical engineering, Dobbelaere et al. (2021) shows that AI holds many opportunities, but that a lack of interpretability and a tendency to overfit mean practitioners have to be vigilant when using data-driven models. This conclusion is in line with broader analyses such as Windmann et al. (2024), where the lack of robustness and interpretability rank among the most cited challenges when applying AI in industrial settings.\n\nVarious frameworks and processes have been proposed to assure trustworthy AI. Ashmore et al. (2021) discuss how traditional safety engineering models struggle with data-centric AI systems and advocate dynamic assurance processes. Perez-Cerrolaza et al. (2024) analyze the integration of AI into safety-critical systems and emphasize that explainability and robustness are crucial while validation remains an open challenges. Lavin et al. (2022) further stress that AI has to be monitored and tested throughout its entire lifecycle. A second integral pillar of trustworthy AI is structured documentation. Both the Model Card proposed by Mitchell et al. (2019) and the IBM FactSheet proposed by Arnold et al. (2019) describe standardized fields like model details and intended use. Hutchinson et al. (2021) propose an extensive dataset specification. Brajovic et al. (2023) explicitly link model cards to the requirements of the EU AI Act draft and describe which fields would need to be documented for compliance. Across these works, most templates focus on what to document, but give little concrete guidance on how to design robustness tests, calibration checks, and monitoring procedures.\n\nRegulations and standards that explain how to monitor and test AI are currently being developed. The EU has one of the most advanced regulatory frameworks with the EU AI Act, which requires extensive audits and documentation for high-risk AI systems and encourages the use of common codes of practice for other systems. Notable international standards include the NIST AI Risk Management Framework 1.0 by Tabassi (2023), which structures AI governance into govern, map, measure, and manage functions, as well as ISO/IEC 42001 and ISO/IEC 23894, which outline requirements for AI risk management. Technical reports and standards for robustness assessment, such as ISO/IEC TR 24029-1 and DIN SPEC 92001-2, provide an overview of robustness methods and describe processes for robustness evaluation, but they are domain-agnostic and do not specify a concrete, card-style empirical robustness and monitoring protocol for typical Industrial AI models. Other robustness evaluation guidance such as IEEE 3129-2023 is focused on image recognition and therefore only partially addresses Industrial AI scenarios. A comprehensive guideline on AI assurance in aviation is given by EASA (2024), but the guideline is likely too comprehensive to follow for non-high-risk Industrial AI systems.\n\nMeasuring the trustworthiness of an AI system is difficult, as many traditional software safety methodologies and formal verification methods cannot be applied to complex, data-driven models, see Perez-Cerrolaza et al. (2024). For robustness evaluation, a large body of work focuses on adversarial attacks, which are tiny perturbations that can fool AI models, see Sinha et al. (2018). Since such perturbations rarely occur in practice, more recent robustness evaluation methods have focused on realistic stress tests and natural perturbations. For example, Hendrycks et al. (2021) propose structured corruptions such as blur and noise for image data, and Windmann et al. (2025) apply severity-controlled sensor faults to industrial time series. Another important aspect of trustworthiness is the ability of a system to quantify\n\n1 https://github.com/awindmann/Industrial-AI-Robustness-Card\n\nits uncertainty and thereby support early fault detection and risk-aware operation. Gawlikowski et al. (2023) and Fakour et al. (2024) give an overview of sources of uncertainty and methods for uncertainty estimation.\n\nOverall, implementation-ready protocols that translate trustworthy AI frameworks into concrete robustness, calibration, and monitoring procedures for Industrial AI models are still missing, consistent with the observation by DÃ­az-RodrÃ­guez et al. (2023) that operative, auditable protocols often do not exist.\n\n## 3 Industrial AI Robustness Card\n\nThis section describes the IARC, the information it records, and the rationale for each field. Each subsection specifies what information the card should contain and how it supports assurance and regulatory alignment, in particular with the requirements of the EU AI Act. The IARC documents the AI model rather than the complete, integrated AI system. This focus allows us to cover most of the information referenced in EU AI Act, Art.9-15 and Annex IV, thereby providing a usable building block for AI Act-compliant documentation. A concise, model-agnostic measurement protocol that explains how to generate the evidence for these fields is provided in Section 4.\n\n## 3.1 General Information (EU AI Act Art. 11, 13 (3), Annex IV)\n\nThe General Information section of the IARC provides a summary of the AI model and the associated dataset. It records fields such as model and dataset name, version, date, provider, and deployment context. Tracking the model version is important for monitoring the AI model throughout its lifecycle, see Tabassi (2023).\n\n## 3.2 Intended Use (EU AI Act Art. 13)\n\nThe intended use of the model should be explained, as explained by Mitchell et al. (2019) and Arnold et al. (2019). This clarification also makes clear what the model is not to be used for, which prohibits misuse.\n\n## 3.3 Data (EU AI Act Art. 10 (2/3/4), 11, Annex IV (2.a))\n\nFor any AI system, monitoring data quality and data drift is crucial for reliable deployment, see Lavin et al. (2022). For better transparency, a dataset overview, data provenance, preprocessing steps, and data quality should be documented. Furthermore, additional care should be given in describing the distribution of the data expected during operation and in stress test scenarios that probe edge cases. To this end, there should be a description of the Operational Design Domain (ODD), in which the AI model is expected to perform well. On the one hand, this allows for better monitoring, since data drift out of the ODD can be detected early. On the other hand, finding gaps in the ODD not covered by the train data can help in designing relevant stress test scenarios to test the robustness of the AI model.\n\n## 3.4 Evaluation (EU AI Act Art. 9 (6-8), 13 (3.b), 15 (1-4))\n\nAI models pose unique risks compared with traditional software, because data drift can affect functionality and trustworthiness in ways that are hard to understand, see Tabassi (2023). The IARC addresses this limitation by emphasizing multiple task-appropriate key performance indicators (KPIs), explicit uncertainty quantification, and realistic robustness evaluation. By evaluating performance on multiple KPIs at the same time, ideally with predefined acceptance thresholds, a more comprehensive analysis of the AI model's capabilities is possible. Using uncertainty quantification methods helps to identify overconfident or unreliable predictions early. Finally, the robustness evaluation in the IARC focuses on realistic stress test scenarios instead of artificial adversarial attacks or purely formal stability guarantees, which often do not indicate which types of real-world perturbations can be tolerated Ashmore et al. (2021).\n\n## 3.5 Limitations (EU AI Act Art. 13 (b), Annex IX)\n\nA limitations section can be used to highlight known limitations of both the AI model and the data, see Brajovic et al. (2023). Examples include regimes where the model is not expected to perform well, reliance on simulated edge cases that have not yet occurred in practice, or restricted ODD coverage. By explicitly recording such limitations, the IARC supports realistic expectations and more transparent risk assessments.\n\n## 4 Measurement and Reporting Protocol\n\nThis section provides a task-agnostic protocol for generating the evidence that populates the IARC fields. Best practice from research, for example outlined by VranjeÅ¡ et al. (2024), informs the IARC on how to generate reliable and reproducible results.\n\n## 4.1 Data\n\nAs mentioned in Section 3, describe the dataset and how it has been obtained. In the following, we will cover aspects that are relevant for a reliable and reproducible evaluation in more detail. For a more comprehensive overview about dataset specifications, see Hutchinson et al. (2021).\n\n## 4.1.1 Quality characterization\n\nBefore modeling, compute diagnostics that characterize data quality. For each feature, calculate missingness rates and basic summary statistics before and after preprocessing. Assess drift by comparing distributions across time windows or via simple trend estimates on key features, see Webb et al. (2016).\n\n## 4.1.2 Reproducibility controls\n\nFix random seeds for splitting, initialization, and training procedures. Record code and environment identifiers such as repository commits and library versions. Assign dataset version identifiers that link raw data to preprocessing configurations and splits. Store these identifiers alongside reported metrics and plots so evaluations can be reproduced.\n\n## 4.1.3 Data splits\n\nConstruct training, validation, and test sets with temporal or group-aware separation. For sequential time series, use chronological splits instead of random cross-validation to avoid look-ahead leakage. Alternatively, when dependencies across batches matter, split by these groups and avoid overlaps across splits. Apply purge windows between adjacent splits to reduce contamination from temporal autocorrelation.\n\n## 4.1.4 ODD definition\n\nDefine an ODD that captures asset and site characteristics, operating modes, and relevant environmental ranges, see EASA (2024). For example, manually set acceptable ranges for each sensor or use a Kernel Density Estimate (KDE) to describe the outer edges of the training data distribution.\n\n## 4.1.5 Scenario catalog\n\nDefine relevant stress test scenarios aligned with the ODD. Prefer scenarios based on real slices such as specific batches, machines, or operating regimes. When near-fail data are scarce, construct plausible simulated scenarios, for example by injecting sensor faults or noise patterns, see Windmann et al. (2025).\n\n## 4.1.6 Distributional diagnostics\n\nQuantitatively show how well the data represent the ODD and where test scenarios deviate. Produce KDE plots for key features and compute distance measures between training data and each test scenario, for example using Kolmogorov-Smirnov tests or using the Wasserstein distance. Rank features or scenarios by deviation from the baseline.\n\n## 4.2 Uncertainty quantification\n\nSelect an uncertainty quantification mechanism appropriate for the model class and deployment constraints, see Fakour et al. (2024). If post-hoc calibration is used, apply methods such as temperature scaling, isotonic regression, or quantile recalibration on a separate calibration set. For classification, compute proper scoring rules such as negative log-loss or Brier score and assess calibration with expected calibration error and reliability diagrams. For regression and forecasting, compute empirical coverage at target levels and mean prediction interval width and score with the weighted interval score or similar. Report these metrics on the regular test data all test scenarios.\n\n## 4.3 Robustness\n\nUse the scenarios from the scenario catalog to test the AI model under realistic edge conditions. For each scenario, compute all KPIs and uncertainty quantification metrics and compare them to the values on the test baseline. When simulated perturbations are used, vary severity and generate severity-performance curves that show how metrics degrade with perturbation strength or use an aggregated robustness score, see Windmann et al. (2025). Ideally, provide visualizations like a radar plot alongside a table summary and compare to different model versions. Identify weakest scenarios and relate them to ODD factors to inform mitigation, monitoring, or risk acceptance.\n\n## 5 Case Study\n\n## 5.1 System and Task\n\nThe use of validated soft sensors in the biopharmaceutical industry can provide significant savings in cost and time by reducing the reliance on frequent laboratory probe tests, which are often time-intensive, expensive, cause delivery delays and can be subject to variability. In our example, we use an AI-based soft-sensor to forecast the penicillin concentration in an industrial-scale fed-batch fermentation simulation, see Goldrick et al. (2019).\n\nThe pharmaceutical industry is bound by multiple compliance guidelines that govern any tools that affect product quality or patient safety. Under the EU AI Act Art. 6 Annex III, AI-based soft sensors that inform manufacturing decisions, batch release or dosing will very likely be classified as high-risk AI systems. Furthermore, the usage of AI-based systems in the pharmaceutical industry in the EU must also comply with the Good Manufacturing Practice (GMP) Guidelines as stated in the EudraLex GMP Annex 22 consultation. For our use case, this means that proper AI documentation and testing is necessary. The proposed IARC on its own is not sufficient for regulation purposes, but can be embedded in a broader framework.\n\n## 5.2 Results\n\nA filled example IARC for the penicillin soft sensor can be seen in Figure 1. The interface of the IARC instance, including the plots, are interactive, which allows a more efficient use of space. The idea is that the practitioner can click through data diagnostics and robustness evaluation plots to identify potential weaknesses of the model. The feature KDE plot allows to check the coverage of the ODD and how specific test scenarios diverge from the training data. The radar plot allows to monitor the robustness of the AI model and its past versions against the realistic stress test scenarios at a glance. Additionally, by generating the content of the card automatically it can be easily exported via a machine readable format like JSON .\n\nOur experience from the case study suggests that most of the effort lies in the up-front structuring of data and scenarios. Once these elements are in place, automated generation of the card, including the metrics and plots, becomes relatively straightforward and can be integrated into existing MLOps pipelines. Thus, the IARC can simplify AI quality assurance and reporting obligations.\n\n## 6 Discussion and Limitations\n\nThe IARC aims to bridge the gap between high-level governance and standardization documents on the one hand and concrete measurement and reporting protocols from the research community on the other. Compared to general-purpose model cards, the IARC provides a more concrete, task-agnostic protocol for AI models working on time series data. At the same time, it remains deliberately lightweight so that it can be instantiated in typical industrial projects without prohibitive overhead.\n\nHowever, the IARC has several limitations. First, the current template focuses on empirical robustness and uncertainty quantification. Important dimensions of trustworthiness such as explainability, human-machine interaction, and security are only touched on indirectly or omitted due to space constraints. Secondly, the card is not sufficient on its own for high-risk AI systems as defined in the EU AI Act EU AI Act, for which the IARC would have to be embedded into a broader quality and risk management framework. In that context, the IARC should be understood as a building block that provides structured robustness and monitoring evidence. In particular, the IARC does not include information regarding the integrated system, such as hardware and user documentation (Annex IV 1 (d-h)), risk management aligned with norms/regulations (Annex IV 5-8), or data governance for personal data (Art. 10 (5-6)). Future work will extend the IARC to other use cases and add guidance on explainability reporting.\n\n## DECLARATION OF GENERATIVE AI AND AI-ASSISTED TECHNOLOGIES IN THE WRITING PROCESS\n\nDuring the preparation of this work, the author(s) used GPT-5 by OpenAI for editorial feedback. After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication.\n\n## References\n\n- M. Arnold, R. K. E. Bellamy, M. Hind, S. Houde, S. Mehta, A. MojsiloviÂ´ c, R. Nair, K. Natesan Ramamurthy, A. Olteanu, D. Piorkowski, D. Reimer, J. Richards, J. Tsay, and K. R. Varshney. FactSheets: Increasing trust in AI services through supplier's declarations of conformity. IBM Journal of Research and Development , 63(4/5):6:1-6:13, July 2019. ISSN 0018-8646. doi: 10.1147/JRD.2019.2942288.\n2. Rob Ashmore, Radu Calinescu, and Colin Paterson. Assuring the Machine Learning Lifecycle: Desiderata, Methods, and Challenges. ACM Computing Surveys , 54(5):1-39, May 2021. ISSN 0360-0300, 1557-7341. doi: 10.1145/3453444.\n3. BCG. Using AI in Industrial Operations Guidebook. Technical report, 2023.\n4. Danilo Brajovic, Vincent Philipp GÃ¶bels, Janika Kutz, and Marco Huber. Merging (EU)-Regulation and Model Reporting. In NeurIPS 2023 Workshop on Regulatable ML , December 2023.\n5. Natalia DÃ­az-RodrÃ­guez, Javier Del Ser, Mark Coeckelbergh, Marcos LÃ³pez de Prado, Enrique Herrera-Viedma, and Francisco Herrera. Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation. Information Fusion , 99:101896, November 2023. ISSN 1566-2535. doi: 10.1016/j.inffus.2023.101896.\n6. DIN SPEC 92001-2. DIN SPEC 92001-2 - Artificial Intelligence - Life Cycle Processes and Quality Requirements Part 2: Robustness, December 2020.\n7. Maarten R. Dobbelaere, Pieter P. Plehiers, Ruben Van De Vijver, Christian V. Stevens, and Kevin M. Van Geem. Machine Learning in Chemical Engineering: Strengths, Weaknesses, Opportunities, and Threats. Engineering , 7(9): 1201-1211, September 2021. ISSN 20958099. doi: 10.1016/j.eng.2021.03.019.\n8. EASA. EASA Artificial Intelligence (AI) Concept Paper Issue 2: Guidance for Level 1&amp;2 machine learning applications. Technical report, April 2024.\n9. EU AI Act. Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA relevance), June 2024.\n10. EudraLex GMP Annex 22 consultation. Stakeholders' consultation on eudralex volume 4 - good manufacturing practice guidelines: Chapter 4, annex 11 and new annex 22, July 2025.\n11. Fahimeh Fakour, Ali Mosleh, and Ramin Ramezani. A Structured Review of Literature on Uncertainty in Machine Learning &amp; Deep Learning, June 2024.\n12. Jakob Gawlikowski, Cedrique Rovile Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Muhammad Shahzad, Wen Yang, Richard Bamler, and Xiao Xiang Zhu. A survey of uncertainty in deep neural networks. Artificial Intelligence Review , 56(1):1513-1589, October 2023. ISSN 1573-7462. doi: 10.1007/s10462-023-10562-9.\n13. Stephen Goldrick, Carlos A. Duran-Villalobos, Karolis Jankauskas, David Lovett, Suzanne S. Farid, and Barry Lennox. Modern day monitoring and control challenges outlined on an industrial-scale benchmark fermentation process. Computers &amp; Chemical Engineering , 130:106471, 2019. ISSN 0098-1354. doi: https://doi.org/10.1016/j. compchemeng.2019.05.037.\n14. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pages 8320-8329, Montreal, QC, Canada, October 2021. IEEE. ISBN 978-1-6654-2812-5. doi: 10.1109/ICCV48922.2021.00823.\n15. Ben Hutchinson, Andrew Smart, Alex Hanna, Remi Denton, Christina Greer, Oddur Kjartansson, Parker Barnes, and Margaret Mitchell. Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency , pages 560-575, Virtual Event Canada, March 2021. ACM. ISBN 978-1-4503-8309-7. doi: 10.1145/3442188.3445918.\n\nIEEE 3129-2023. IEEE 3129-2023, June 2023.\n\n- ISO/IEC 23894. ISO/IEC 23894:2023 - Information technology - Artificial intelligence - Guidance on risk management, 2023.\n- ISO/IEC 24029-2. ISO/IEC 24029-2:2023 - Artificial intelligence (AI) - Assessment of the robustness of neural networks - Part 2: Methodology for the use of formal methods, 2023.\n- ISO/IEC 42001. ISO/IEC 42001:2023 - Information technology - Artificial intelligence - Management system, 2023.\n- ISO/IEC TR 24029-1. ISO/IEC TR 24029-1:2021 - Artificial Intelligence (AI) - Assessment of the robustness of neural networks - Part 1: Overview, 2021.\n- Alexander Lavin, CiarÃ¡n M. Gilligan-Lee, Alessya Visnjic, Siddha Ganju, Dava Newman, Sujoy Ganguly, Danny Lange, AtÃ­lÃ­m GÃ¼neÂ¸ s Baydin, Amit Sharma, Adam Gibson, Stephan Zheng, Eric P. Xing, Chris Mattmann, James Parr, and Yarin Gal. Technology readiness levels for machine learning systems. Nature Communications , 13(1):6039, October 2022. ISSN 2041-1723. doi: 10.1038/s41467-022-33128-9.\n- Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency , FAT* '19, pages 220-229, New York, NY, USA, January 2019. Association for Computing Machinery. ISBN 978-1-4503-6125-5. doi: 10.1145/3287560.3287596.\n- Oliver Niggemann, Bernd Zimmering, Henrik Steude, Jan Lukas Augustin, Alexander Windmann, and Samim Multaheb. Machine Learning for Cyber-Physical Systems. In Birgit Vogel-Heuser and Manuel Wimmer, editors, Digital Transformation: Core Technologies and Emerging Topics from a Computer Science Perspective , pages 415-446. Springer, Berlin, Heidelberg, 2023. ISBN 978-3-662-65004-2. doi: 10.1007/978-3-662-65004-2\\_17.\n- Jon Perez-Cerrolaza, Jaume Abella, Markus Borg, Carlo Donzella, JesÃºs Cerquides, Francisco J. Cazorla, Cristofer Englund, Markus Tauber, George Nikolakopoulos, and Jose Luis Flores. Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey. ACM Comput. Surv. , 56(7):176:1-176:40, April 2024. ISSN 0360-0300. doi: 10.1145/3626314.\n- Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying Some Distributional Robustness with Principled Adversarial Training. In International Conference on Learning Representations , February 2018.\n- Elham Tabassi. Artificial Intelligence Risk Management Framework (AI RMF 1.0). Technical Report NIST AI 100-1, National Institute of Standards and Technology (U.S.), Gaithersburg, MD, January 2023.\n- Daniel VranjeÅ¡, Jonas Ehrhardt, RenÃ© Heesch, Lukas Moddemann, Henrik Sebastian Steude, and Oliver Niggemann. Design Principles for Falsifiable, Replicable and Reproducible Empirical Machine Learning Research. In 35th International Conference on Principles of Diagnosis and Resilient Systems (DX 2024) , 2024. doi: 10.4230/OASIcs. DX.2024.7.\n- Geoffrey I. Webb, Roy Hyde, Hong Cao, Hai Long Nguyen, and Francois Petitjean. Characterizing concept drift. Data Mining and Knowledge Discovery , 30(4):964-994, April 2016. ISSN 1573-756X. doi: 10.1007/s10618-015-0448-4.\n- Alexander Windmann, Philipp Wittenberg, Marvin Schieseck, and Oliver Niggemann. Artificial Intelligence in Industry 4.0: A Review of Integration Challenges for Industrial Systems. In 2024 IEEE 22nd International Conference on Industrial Informatics (INDIN) , pages 1-8, August 2024. doi: 10.1109/INDIN58382.2024.10774364. ISSN: 2378-363X.\n- Alexander Windmann, Henrik Steude, Daniel Boschmann, and Oliver Niggemann. Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems. In 2025 IEEE 30th International Conference on Emerging Technologies and Factory Automation (ETFA) , pages 1-8, September 2025. doi: 10.1109/ETFA65518.2025.11205527.\n\n## General Information\n\nData-driven soft sensor estimating penicillin concentration on the IndPenSim benchmark.\n\nMODEL\n\nLSTM\n\nDATASET\n\nIndPenSim\n\nTARGET\n\nPenicillin concentration (g/L)\n\nLOSS FUNCTION\n\nMSE\n\nWINDOW\n\n90 â†’ 30\n\nDATE\n\n2025-12-04 17:12 UTC\n\nMODEL ID\n\n08432de6d0cf4eeeb0dbb0a3a1efdf3e\n\nPROVIDER\n\nBio Data Science\n\nCONTACT\n\npenicillin-softsensor@example.com\n\n## Data\n\n## Distribution diagnostics\n\n- The ODD region is defined as the 98%-mass KDE of the train data.\n- The highest train-to-test drift appears for the feature ' dissolved\\_oxygen' in the drift bar plot.\n- The KDE overlays for ' dissolved\\_oxygen' show that the drift remains within the ODD region, even in the 'Drift' test scenario.\n- The 'dissolved\\_oxygen' feature should be monitored in live deployments to detect potential drift beyond the ODD.\n\n## Intended Use\n\nThis model is intended for research and benchmarking on simulated penicillin fermentations.\n\n- Estimates penicillin concentration within a simulated industrial bioreactor.\n- Supports development and comparison of biopharmaceutical soft-sensor methods.\n- Stress-test scenarios evaluate robustness to sensor noise, missing channels, and injected faults.\n- Not approved for production deployments or closed-loop actuator control.\n\n<!-- image -->\n\n## Evaluation\n\n<!-- image -->\n\n| Robustness evidence     | Robustness evidence   | Robustness evidence   | Robustness evidence   |\n|-------------------------|-----------------------|-----------------------|-----------------------|\n| METRIC                  | VALIDATION            | TEST                  | SCENARIOS             |\n| MSE                     | 0.0290                | 0.0245                | 0.0265                |\n| MAPE                    | 3.6347                | 0.8373                | 0.8597                |\n| WIS 0.1-0.9             | -                     | 0.6717                | 0.7001                |\n| ROBUSTNESS SCORE (MEAN) |                       |                       | 0.8884                |\n\n- Latest model version improves MSE on the clean validation and test sets.\n- However, MAPE and the robustness score in the ' Noise' scenario have decreased. Overfitting?\n\n## Limitations\n\nThe dataset is synthetic and the stress scenarios are finite, so results may miss real fermentation variability and plant-specific instrumentation quirks.\n\n- Penicillin targets are validated offline only; no closed-loop deployment has been attempted.\n- Scenario catalog omits extended fouling periods, rare mechanical faults, and upstream feed excursions.\n- This report alone does not satisfy EU AI Act, FDA, or internal governance requirements.\n\nFigure 1: Example IARC for the IndPenSim dataset. Note that some fields and plots are hidden due to space constraints.\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T13:53:46Z", "sha256": "5449434a5fcc8e12eccadb72fdfca088fd10f4a2490413b426640914b97212c3", "meta": {"file_name": "Industrial AI Robustness Card - Evaluating And Monitoring Time Series Models.pdf", "file_size": 421395, "mtime": 1770643145, "docling_errors": []}}
{"doc_id": "pdf-pdfs-initial-reflections-on-agentic-ai-governance-oliver-patel-a3ca9cf145f0", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Initial reflections on agentic AI governance - Oliver Patel.pdf", "title": "Initial reflections on agentic AI governance - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable, and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement, and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nICYMI: visit this page to download my free 20-page AI Usage Policy Playbook and register interest for my upcoming AI Usage Policy Bootcamp .\n\nThis week's edition is an essay on agentic AI governance . It covers:\n\n- âœ… What is agentic AI?\n- âœ… How agentic AI is used today and how it could be used in future?\n- âœ… What novel risks does agentic AI pose?\n- âœ… How do these risks challenge existing AI governance frameworks?\n- âœ… What new policies, standards, and guardrails are required to address these challenges and mitigate risk?\n\nThe key message is that urgent work is required, within the AI governance community, to refine and update our approach to AI governance and risk management in the\n\nagentic AI era.\n\nI am very keen for comments and feedback, to guide my thinking and work in this emerging field. However, I also don't want to waste anyone's time. Therefore, if you don't want to read a 4,000-word essay on agentic AI governance, then this may not be for you.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week.\n\n## Initial reflections on agentic AI governance\n\nThis essay outlines my initial reflections on agentic AI governance. The phrase 'initial reflections' is designed to serve as a health warning for all readers.\n\nLarge language model (LLM) based AI agents are a relatively new technology. There are not many enterprise use cases in production, at least compared to generative AI and traditional machine learning.\n\nFurthermore, there are not yet any laws, standards, frameworks, or guidelines which directly address or stipulate how the novel risks of agentic AI should be mitigated.\n\nFinally, not much has been researched or published on the topic of agentic AI ethics and governance. However, I will reference some of what has been written throughout this essay.\n\nConsidering the above, it is reasonable for you to ask why I am writing and publishing this today.\n\nWell, the main reason is that agentic AI is coming-whether the AI governance community is ready or not. As we should all appreciate by now, technology is not going to wait for us to figure things out.\n\nAs a community, we had several years to discuss, align on, and codify the key ethical principles, policies, and standards for AI, before enterprise adoption of machine learning became mainstream.\n\nAnd, although our response and adaptation timelines were accelerated for generative AI, the launch of ChatGPT was a landmark moment that made it obvious there was pressing work for us to do.\n\nWith agentic AI, the challenge is twofold. Not only is time in short supply, I also do not anticipate that there will be a watershed moment that highlights the urgency of agentic AI governance. It is, and will continue to, creep up on us and permeate our organisations.\n\nFor this reason, there is a serious possibility that we may fail to identify and grasp the unique challenges and risks which agentic AI poses in time for the inevitable proliferation of use cases, adoption at scale, and democratisation.\n\nThis could leave our organisations and stakeholders exposed, with an AI governance and risk management framework that is no longer fit for purpose in the agentic AI era.\n\nI hope that my 'initial reflections' essay can help to address this challenge. However, please take everything I say with a pinch of salt, as this is not a peer-reviewed paper, and there is a lot of work for me to do to fully wrap my head around this complex topic.\n\n## What is agentic AI?\n\nYou will perhaps be unsurprised to read that there is not yet an internationally agreed definition of agentic AI. Here are some industry definitions, to get us started:\n\n'AI agents are software systems that use AI to pursue goals and complete tasks on behalf of users' Google\n\n'Agentic AI systems can accomplish a specific goal with limited supervision' IBM\n\n' Agentic AI systems act autonomously, make decisions, and adapt dynamically to complex environments' Kieran Gilmurray\n\nCentral to all definitions of agentic AI is the concept of proactive and autonomous completion of tasks.\n\nIn his new book Kieran Gilmurray outlines the three waves of AI: predictive AI, generative AI, and agentic AI.\n\nWith traditional machine learning, models generate predictive outputs, like scores, classifications, and recommendations. With generative AI, models generate content, like text, video, or code. These predictive or generative AI outputs are typically provided to humans via a user interface and are then used by those humans to assist, augment, or optimise their work.\n\nWith agentic AI systems, the work itself may no longer be done by the humans.\n\nAgentic AI systems can autonomously develop plans, solve problems, retrieve data, leverage memory, use tools, and execute tasks in a range of other applications which they are integrated with.\n\nThey do so by constantly processing inputs, learning from, and adapting to their environment, and proactively determining the best course of action to take. Hence the notion of 'agency'.\n\nAI agents are powered by LLMs and APIs. The LLM enables the system to process the initial instructions and mimic reasoning to break down complex problems into a series of smaller steps to be executed. The APIs enable integration with a range of other applications, tools, and databases, to retrieve data and make things happen.\n\nBecause agents are LLM-powered, they are unpredictable and non-deterministic, in contrast to more traditional forms of software automation and workflows, like RPA; more on that below.\n\n## What are the use cases for agentic AI?\n\nAs you can imagine, the potential spectrum of agentic AI use cases is limitless.\n\nHowever, there are not a huge number of AI agents in production today. This is still a\n\nnovel (and by extension relatively unreliable and untested) technology.\n\nIBM has declared 2025 as the year of agentic exploration. Each organisation will likely witness a proliferation of PoCs and pilots this year, just like we have seen with generative AI over the past two and a half years.\n\nIn this early wave of agentic AI use cases, there is an emphasis on assistive and productivity enhancing tasks, such as research, summarisation, and information retrieval. A survey by LangChain finds that the most popular agentic AI use cases today are research and summarisation, personal assistance and productivity, customer service, and code generation.\n\nIn many cases, AI agents are already working behind the scenes, without our awareness, to improve the performance and effectiveness of the most widely used generative AI applications, like Perplexity and ChatGPT.\n\nFor example, the current crop of 'deep research' tools function by leveraging teams of AI agents which collaborate with each other to scour the internet and other data sources to retrieve, collate, assess, merge, and summarise relevant information, to augment and enhance the final AI-generated 'report' or response which the user receives.\n\nThe architecture of such applications has become much more sophisticated than one model receiving a prompt, performing inference, and generating a predictive output.\n\nLooking ahead, the thinking is that agentic and multi-agentic systems will be able to take on increasingly complex tasks and projects, such as managing customer service interactions, planning and booking holidays, planning, creating, and posting social media content, and managing investment portfolios.\n\nBefore we get there, we need a bulletproof approach to agentic AI governance.\n\n## The novel risks and challenges of agentic AI\n\nAlthough there is currently ample hype (some of which is inevitably overblown), this is not an excuse to ignore or disregard this technological trend.\n\nAgentic AI poses unique risks, which the AI governance community cannot afford to overlook.\n\nThe risks of AI stem primarily from the way the technology is used and the real-world impact this use can have.\n\nTherefore, even if agentic AI is based upon the same underlying technology as generative AI (i.e., LLMs), this does not mean it will be used in the same way.\n\nThe deployment and use of agentic AI, and thus its impact on people, organisations, and society, will be markedly different to what has come before.\n\nIncreasingly autonomous capability enables novel AI use cases, such as control of computers and automation of dynamic, data-intensive processes in sensitive areas like supply chain management and logistics planning.\n\nFurthermore, it is conceivable that, in future, knowledge workers will have access to their own personalised AI agent, to assist with all aspects of their work.\n\nTaken together, these examples represent a meaningful shift from how AI is used today.\n\nTo illustrate the novel risks, I will focus on four themes of the utmost importance for agentic AI:\n\n1.  'Human out of the loop'\n2.  Autonomous task execution and performance\n\n3.  Adaptiveness and unpredictability\n4.  Data, privacy, and cyber security\n\nIn the agentic AI era, all AI risks are amplified. Virtually all existing AI risk themes, such as bias, transparency, copyright, explainability, alignment, sustainability, and labour market disruption remain as relevant-if not more so-than ever.\n\nHowever, my goal here is to focus on the most novel challenges posed by agentic AI.\n\n## 1. Human out of the loop\n\nIt is not an exaggeration to state that the purpose of agentic AI is to take the human out of the loop.\n\nWhy plan and book your own holiday when an AI agent can do it for you? Why respond to all of your fans and followers across multiple social media platforms when an AI agent can take care of the correspondence? Why employ hundreds of call centre workers when an army of autonomous agents can do the job?\n\nHowever, this is in direct tension with the concept of human in the loop, which is a foundational pillar of AI governance.\n\nBy delegating and outsourcing tasks to AI agents, humans may be freed to focus their\n\ntime and energy elsewhere. However, AI agents will also be trusted to take on increasingly important tasks, with diminishing human oversight.\n\nThe key risk is that we become overly trusting of agentic AI systems and take the human out of the loop to a degree which becomes dangerous.\n\nIn the quest for efficiency gains, we may underestimate the level of human oversight required for safe agentic deployment.\n\nThis risk is especially pertinent at first, as we do not truly understand the limitations and capabilities of agentic AI systems.\n\nAn associated challenge, discussed below, will be the complexity of refining and updating our approach to human oversight. This will require defining exactly when human review and approval is required before an action can be taken.\n\nMoreover, if the AI agent executes the action, it may become even harder to determine which human, or entity, should be held accountable for it. This will need to be codified at the outset of agentic development.\n\n## 2. Autonomous task execution and performance\n\nIf you thought AI hallucinations were bad, wait till you learn about 'cascading hallucinations'. OWASP describes this as when an 'AI agent generates inaccurate information, which is then reinforced through its memory, tool use, or multi-agent interactions, amplifying misinformation across multiple decision-making steps'.\n\nThis can lead to self-reinforcing destructive behaviours and systemic failures in agentic AI performance.\n\nAgentic AI raises the stakes for the hallucination problem. An LLM hallucination is primarily a problem if the user naively fails to verify the accuracy of the output before relying on or using it. However, an LLM hallucination which directly informs and shapes the course of action taken by an AI agent (or team of agents) could have severe consequences, if that agent is being trusted to execute tasks in a high-risk domain.\n\nThe more autonomous AI agents become, and the more we trust those agents to take over tasks and projects in sensitive areas, the greater the risk and negative impact of malfunction, error, and performance degradation.\n\nAlthough AI performance concerns are nothing new, without appropriate controls, such as human oversight and observability, there is a risk that the agents we begin to trust let us down, without us even realising at first.\n\nFor example, if the AI agent is autonomously handling and responding to customer complaints, how many inappropriate interactions and unnecessary follow ups could occur before this is flagged and addressed?\n\nIt is also critical that the appropriate agent performs the appropriate task. In the modern enterprise there will be many agents, working together and supervising each other in complex hierarchies, based on their pre-defined roles, permissions, and guardrails. Therefore, the reliable performance of agents towards the top of the hierarchy is critical for the performance and effectiveness of all the other agents.\n\nBy taking the human out of the loop and allowing AI agents to autonomously execute tasks, it is undeniable that there are immense potential efficiency and productivity gains.\n\nHowever, with increased autonomy comes increased risk. There may be some domains where we simply cannot afford agentic AI mistakes.\n\nA broader question is whether we are building these exciting new tools on the relatively shaky foundation of a technology which was ultimately designed to predict the next word, rather than perform important tasks.\n\nResearchers from Hugging Face have sounded the alarm bell and argued that fully autonomous agents should not be developed, due to the unacceptable potential risks resulting from system inaccuracy, privacy and security breaches, spread of false information, and loss of human control.\n\n## 3. Adaptiveness and unpredictability\n\nAI agents are unpredictable precisely because they are proactive. If we knew or could consistently guess what they were going to do, they would not have agency in any meaningful sense.\n\nLLMs are non-deterministic, which means models can generate different outputs in response to the same inputs. This leads to unexpected, unpredictable, and unreliable outputs. This will inevitably be reflected in the behaviour and performance of AI agents, which will rely on the effectiveness of LLMs and their ability to accurately predict the next word.\n\nGiven the proactive and dynamic way in which AI agents respond and adapt to their environment, it could become virtually impossible to predict and anticipate how they will behave, and therefore the risks that could emerge.\n\nThis makes AI risk assessment, and therefore AI risk mitigation, much more challenging than it is today. It also requires much more continuous and comprehensive monitoring\n\nof AI performance.\n\nIt is hard enough to predict the risks of downstream general-purpose AI usage, let alone the potential behaviour and risks of autonomous agents, which are integrated with a range of other applications, and empowered to solve complex and open-ended problems in whichever way they see fit.\n\n## 4. Data, privacy and cyber security\n\nData, privacy and cyber security risks are nothing new for AI. However, these risks are exacerbated by agentic AI.\n\nAgentic AI systems could easily mine and retrieve data from sources which they were not supposed to have access to, or sources which are not permitted to be used for AI processing or text and data mining. This could include copyrighted material without an appropriate license, or sensitive personal data originally collected for a different, more narrow purpose.\n\nFurthermore, there is also the risk of AI agents disclosing and revealing data to people who were not authorised to have access to it. Agents will be performing and automating increasingly personalised tasks, such as booking medical appointments, whilst having access to and being trained on huge amounts of personal data, such as medical records.\n\nThis elevates the risk of data breaches and information leakage. Therefore, encoding privacy by design and data governance guardrails will be a challenging but necessary part of agentic AI governance.\n\nAgentic AI systems will also become attack surfaces. Nefarious actors will undoubtedly attempt to take control of, and manipulate, the autonomous systems which themselves may control important spheres of business activity. There is a lot you can do if you control an agentic system which itself can control computers with access to sensitive data and applications.\n\nAlso, in situations where AI agents are trusted to both generate and execute code, the risk of autonomously executed (and non-vetted) malicious code creeping in to production applications increases.\n\nIn a recent report, OWASP outlines 15 unique threats which agentic AI systems are vulnerable to. This includes cascading hallucination attacks, resource overload, tool misuse, and rogue agents in multi-agent systems.\n\n## Policies, guardrails and controls to manage agentic AI risks\n\n'Traditional' AI governance frameworks, such as the EU AI Act and NIST AI Risk Management Framework, were developed during a time when traditional machine\n\nlearning and then generative AI was prevalent. They do not directly address many of the novel risks and challenges of agentic AI, discussed above. Indeed, if the EU AI Act was being drafted today, I am certain that some of my below points would be directly addressed in the law.\n\nHowever, we cannot rely on, or wait for, the regulators to come and save us with new guidance or updated standards. They will, rightly so, expect industry to figure out how to develop and implement agentic AI in a manner which is safe, secure, and respects existing laws, like the EU AI Act.\n\nNone of the risks highlighted above represent insurmountable problems. I have full faith in the ingenuity of the AI governance community to solve them.\n\nBelow, I will sketch out the six most important considerations for AI governance professionals seeking to refine, update, and implement policies, guardrails, and controls, to meet the challenge of managing risk in the agentic AI era. This includes:\n\n1.  Action permissions and thresholds\n2.  Integrations and data access\n3.  Hierarchy and approval matrixes\n4.  Monitoring, observability, and orchestration\n5.  Human oversight, accountability, and control\n\n## 6.  Use cases and risk assessments\n\n## 1. Action permissions and thresholds\n\nConfiguring action permissions appropriately is an essential part of agentic AI governance.\n\nAutonomy is a spectrum. Just because an agent can do something does not mean we should let it. We can determine exactly which actions an agent can and cannot perform.\n\nThe potential behaviour and permissions of agents can be restricted at the system and API level.\n\nIf there are certain tasks which an agent could in theory perform in a given system or environment, or certain tools the agent could use, and we do not want the agent to do so, we can specify and encode these restrictions.\n\nThis sounds simple enough at first. For example, in a financial context, we may not want the agent to execute or process any transaction, or make any decision, which carries a financial value over a certain amount. Similarly, we may want to restrict the ability of an agent to execute AI-generated code in certain applications.\n\nWhat is more challenging is to define generally applicable policy principles, which can be used to determine, for any use case in any domain, what type of action permissions and restrictions we should impose, and what thresholds we should use to guide this. Some potential action permission threshold categories could be:\n\n- Financial value\n- Direct impact on people\n- Number of people impacted\n- Impact on patients\n- Importance of the decision or action on the business\n- Potential ethical risk (e.g., EU AI Act high-risk AI system categories)\n\n## 2. Integrations and data access\n\nOn a similar note, we can also determine and restrict which applications an agent is integrated with, as well as which data it has access to.\n\nAs well as enabling privacy by design and data governance, this also supports the points raised above relating to access restrictions.\n\nIf an agent is unable to access certain data and/or is not integrated with the application where a particular task is performed or tool is used, then it will be unable to use that data or tool to do something which we do not want it to do.\n\nAgain, we will need to formulate generally applicable policy principles which can steer us in our assessment of which applications agents should and should not be integrated with, as well as which datasets should and should not augment their knowledge base.\n\n## 3. Hierarchy and approval matrixes\n\nIn the modern enterprise there will be countless agents working together, in complex hierarchies of agentic collaboration, supervision, and oversight.\n\nThere will need to be a clearly defined RACI or matrix for AI agents, which outlines the roles, responsibilities, and segregation of duties. It is crucial that agent Y only performs tasks within its permitted duties and that agent X does the same.\n\nAgents towards the top of the hierarchy will be empowered to review, approve, authorise, and restrict the work of other agents. And agents lower down in the hierarchy should not be allowed to operate in a way which circumvents the authority of their superiors.\n\nThis will require both complex engineering and architectural design, as well as a new conceptual framework for AI governance professionals to lean on.\n\n## 4. Monitoring, observability, and orchestration\n\nWe are moving from MLOps and LLMOps to AgentOps.\n\nIn the 'old world', MLOps is used to validate, test, and monitor model performance, including robustness and accuracy. This primarily focuses on the predictive outputs that are generated and how they perform across a range of key metrics.\n\nWith AgentOps, the goal is to automate the monitoring, oversight, and orchestration of agentic and multi-agentic AI systems, so we can keep tabs on what actions they are performing, how they are behaving, which tools they are using, the impact this is having, and ultimately, whether we can trust them to keep working on our behalf. There should also be visibility as to whether any agents are operating contrary to their guardrails and permissions.\n\nAssessing and evaluating agentic AI performance also entails additional complexity, at least compared with traditional machine learning performance evaluation.\n\nThis is because the actual tasks that agents perform are much more wide-ranging, varied, and hard to anticipate, given the proactive nature of agentic AI. Therefore, we\n\nwill need updated and rigorous performance and accuracy metrics, which can account for the variety of possible tasks and agent could perform.\n\n## 5. Human oversight, accountability and control\n\nWhat we mean by human oversight will also require a refresh.\n\nIt will no longer make sense to mandate human review and approval of each AIgenerated output, when the purpose of agentic AI is to take the human out of the loop, to automate processes and drive efficiencies. If the goal is AI autonomy, humans cannot review everything.\n\nHowever, this does not mean human oversight is no longer relevant.\n\nFor example, human oversight could mean reviewing the ultimate output of an agentic AI system, such as a 'deep research report or generated code (as opposed to all the outputs generated and decisions taken to reach its conclusion and generate that final output).\n\nHuman oversight could also mean having a human in the loop to review actions, tasks, and decisions which meet a certain threshold or risk level, which could be aligned to the action permissions and thresholds detailed above. We will need clearly defined\n\ntouch points for human oversight and review, and it will be more nuanced than what we have today.\n\nFinally, humans must always have the ability to override or shut down an agentic AI system, no matter how much autonomy we have empowered it with.\n\nAccording to a LangChain survey on agentic AI usage in 2025, very few companies are allowing agents to freely read, write, and delete data and information from the applications they are operating in and the databases they have access to. Rather, agents are given read-only permissions, with human approval required for significant actions.\n\n## 6. Use cases and risk assessments\n\nFinally, it is important to determine which agentic AI use cases should be off limits, at least for now.\n\nThe EU AI Act serves as a useful starting point. AI agents should obviously not be used to for anything which constitutes a prohibited AI practice. Furthermore, I would also advise extreme caution in using agents to autonomously perform tasks, which can have a material impact on decision making or a process, in a domain relating to highrisk AI systems, such as recruitment, critical infrastructure safety management, or determining eligibility for welfare payments.\n\nFor one, there is no evidence that agentic AI systems can yet be trusted to perform to a high enough standard required for these sensitive domains.\n\nFurthermore, it will be challenging to comply with the AI Act's obligation for deployers to assign human oversight and the GDPR's restrictions on solely automated decisionmaking, whilst also leveraging autonomous agentic AI systems to automate decisionmaking in sensitive and high-risk domains.\n\nHowever, you will need to look beyond EU law in your work to determine what use cases are appropriate, inappropriate, and off limits for your organisation. Consider the fundamentals of what agents can and cannot do, as well as their strengths and weaknesses.\n\nGoogle, for example, highlights that agents struggle with and should not be used for tasks requiring empathy and emotional intelligence, complex human interactions, high-stakes ethical decision-making, and the navigation of unpredictable physical environments.\n\nOnce you have figured this all out, you will also need to update your approach to risk assessments, as well as your supporting guidance and training. The key question which needs to be answered throughout is when is it safe to use agentic AI and when is it not?\n\n*\n\nThe purpose of this essay is to highlight some of the novel risks and governance challenges of agentic AI.\n\nWhilst I am not proposing a complete overhaul of AI governance frameworks and policies, the considerations I have outlined above should serve as a starting point for refining and updating your organisation's approach to AI governance in the agentic AI era.\n\nIf you have made it this far, I would greatly appreciate comments and feedback. Thank you!\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n40 Likes âˆ™ 1 Restack\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T13:54:30Z", "sha256": "a3ca9cf145f064817518ca29902f1753d09e6edbcbc23e45927bdfeea9063c17", "meta": {"file_name": "Initial reflections on agentic AI governance - Oliver Patel.pdf", "file_size": 814142, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-legal-alignment-for-safe-and-ethical-ai-e33765950104", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Legal Alignment for Safe and Ethical AI.pdf", "title": "Legal Alignment for Safe and Ethical AI", "text": "## Legal Alignment for Safe and Ethical AI\n\nNoam Kolt 1 â‹† Nicholas Caputo 2 â‹† Jack Boeglin 3 â€  Cullen O'Keefe 4 , 5 â€ \n\nRishi Bommasani 6 Stephen Casper 7 Mariano-Florentino CuÃ©llar 8 Noah Feldman 9\n\nIason Gabriel 10 Gillian K. Hadfield 11 , 12 Lewis Hammond 13 , 14 Peter Henderson 15\n\nAtoosa Kasirzadeh 16 Seth Lazar 11 , 17 Anka Reuel 6 Kevin L. Wei 9 Jonathan Zittrain 9 , 18\n\n1 Hebrew University 2 Oxford Martin AI Governance Initiative 3 University of Pennsylvania 4 Institute for Law &amp; AI 5 Centre for the Governance of AI 6 Stanford University 7 MIT CSAIL\n\n8 Carnegie Endowment for International Peace 9 Harvard University 10 School of Advanced Study University of London 11 Johns Hopkins University 12 Vector Institute for Artificial Intelligence\n\n13 Cooperative AI Foundation 14 University of Oxford 15 Princeton University 16 Carnegie Mellon University 17 Australian National University 18 Berkman Klein Center for Internet &amp; Society\n\n## Abstract\n\nAlignment of artificial intelligence (AI) encompasses the normative problem of specifying how AI systems should act and the technical problem of ensuring AI systems comply with those specifications. To date, AI alignment has generally overlooked an important source of knowledge and practice for grappling with these problems: law . In this paper, we aim to fill this gap by exploring how legal rules, principles, and methods can be leveraged to address problems of alignment and inform the design of AI systems that operate safely and ethically. This emerging fieldlegal alignment -focuses on three research directions: (1) designing AI systems to comply with the content of legal rules developed through legitimate institutions and processes, (2) adapting methods from legal interpretation to guide how AI systems reason and make decisions, and (3) harnessing legal concepts as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems. These research directions present new conceptual, empirical, and institutional questions, which include examining the specific set of laws that particular AI systems should follow, creating evaluations to assess their legal compliance in real-world settings, and developing governance frameworks to support the implementation of legal alignment in practice. Tackling these questions requires expertise across law, computer science, and other disciplines, offering these communities the opportunity to collaborate in designing AI for the better.\n\n## 1 Introduction\n\nThe development and proliferation of increasingly advanced AI systems will present society with tremendous opportunities (Eloundou et al., 2024; Brynjolfsson et al., 2025) and significant risks (Lazar and Nelson, 2023; Bengio et al., 2024, 2025). Capturing the opportunities from advanced AI while tackling its risks requires ensuring that AI systems operate safely and ethically (Anwar et al., 2024; Gabriel et al., 2024, 2025). A central component of this challenge involves designing AI systems that are aligned with human interests (Russell, 2019; Christian, 2020) and democratic values (Lazar and CuÃ©llar, 2025). AI alignment encompasses both the normative problem of specifying which values are desirable or appropriate for AI systems (Gabriel, 2020; Kasirzadeh, 2026) and the technical problem of ensuring AI systems give effect to those values when making decisions and taking actions (Chan et al., 2023; Ngo et al., 2024).\n\n* Lead authors. â€  Core contributors. Cite as Kolt, Caputo, et al. (2026) 'Legal Alignment for Safe and Ethical AI.' Correspondence to: noam.kolt@mail.huji.ac.il .\n\nTo date, the main approaches to alignment in systems like ChatGPT, Claude, and Gemini have focused primarily on steering systems to follow the instructions of users (Ouyang et al., 2022), advance the interests of developers (OpenAI, 2024a), and refrain from supporting or engaging in forms of harmful behavior (Askell et al., 2021; Bai et al., 2022a). In broad strokes, the methods for building such systems employ a combination of human feedback (Christiano et al., 2017) and AI-generated feedback (Lee et al., 2024) to evaluate the outputs of AI systems during training by reference to a list of predetermined specifications-typically written by developers-and iteratively refine the systems to produce outputs more closely aligned with those specifications (Bai et al., 2022b). Some systems can retrieve, reason about, and deliberate over these specifications in real-time before producing outputs (Madaan et al., 2023; Guan et al., 2024).\n\nFrom a technical perspective, these methods for alignment have had mixed results. Despite achieving more reliable performance in many tasks and domains (Phan et al., 2025; Kwa et al., 2025), AI systems continue to produce untruthful content (Ji et al., 2023; Li et al., 2025), generate biased outputs (Weidinger et al., 2022; Gallegos et al., 2024), manipulate users through persuasion (Carroll et al., 2023; Hackenburg et al., 2025), exhibit sycophantic tendencies (Sharma et al., 2024; Cheng et al., 2025), leak private information (Carlini et al., 2021; Mireshghallah et al., 2024), remain vulnerable to jailbreaks (Wei et al., 2023; Chao et al., 2024), enable autonomous hacking (Zhang et al., 2025; Zhu et al., 2025b), offer assistance in bioweapons development (Li et al., 2024; GÃ¶tting et al., 2025), recognize when they are being safety-tested (Needham et al., 2025; Lynch et al., 2025) and, at times, conceal their misalignment (Greenblatt et al., 2024; Sheshadri et al., 2025).\n\nFrom a normative perspective, the prevailing approaches to alignment face fundamental limitations (Dobbe et al., 2021; Hadfield, 2026). Rather than designing AI systems to act in accordance with broad societal interests (Korinek and Balwit, 2022; Kasirzadeh and Gabriel, 2023), let alone grapple with people's diverse and sometimes conflicting values (Klingefjord et al., 2024), most alignment techniques train AI systems to comply with company-written alignment policies (Ahmed et al., 2025) or satisfy the revealed preferences of individual users (Zhi-Xuan et al., 2025) through fallible methods such as reinforcement learning from human feedback (Casper et al., 2023). Moreover, key decisions in AI alignment pipelines, such as selecting which principles are included in a system's 'constitution' (Anthropic, 2023), 'model specification' ('model spec') (OpenAI, 2024a), or safety filters (Google, 2025b), are often opaque (Bommasani et al., 2023; Wan et al., 2025) and lack sufficient public input or scrutiny (Abiri, 2025; Lazar, 2025).\n\nRecognizing these limitations, some AI researchers have proposed broadening the goals and methods of alignment (Lowe et al., 2025). Noteworthy efforts include expanding the forms of community participation in AI development (Sloane et al., 2022), incorporating pluralistic values into alignment procedures by collecting preference and judgment data from demographically diverse populations (Sorensen et al., 2024a,b; Kirk et al., 2024), sourcing safety principles and ethical guidelines from participants in public deliberative processes (Huang et al., 2024; Eloundou et al., 2025), and deriving principles from preference data (rather than the reverse) (Findeis et al., 2024). Other research agendas propose leveraging insights from related fields, including game theory, conflict studies, mechanism design (Dafoe et al., 2020, 2021), social choice (Conitzer et al., 2024), and contractualism (Levine et al., 2025). The extent to which these methods and approaches will be further developed or adopted in large-scale AI deployment remains to be seen.\n\nThere is, however, another domain of knowledge and practice that could support developing more legitimate and effective approaches to AI alignment: law . Building on recent legal scholarship (Kolt, 2025; Caputo, 2025; O'Keefe et al., 2025; Boeglin, 2026), we explore how designing AI systems to operate in accordance with appropriate legal rules, principles, and methods can help address problems of alignment . This emerging fieldlegal alignment -aims to harness law in tackling both normative and technical aspects of alignment:\n\n- For normative aspects of alignment, legal rules developed through legitimate institutions and processes in democratic societies could be used to guide the behavior of AI systems, much like they guide the behavior of individuals, corporations, and governments.\n- For technical aspects of alignment, legal methods of interpretation and reasoning could offer principled approaches that inform and steer the decision-making and exercise of discretion by AI systems, especially in novel scenarios and high-stakes settings.\n- Across both aspects of alignment, legal concepts can serve as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems and the human actors and institutions with which they interact.\n\nThe advantages of legal alignment derive primarily from the public legitimacy of law and its institutional processes. In democracies governed by the rule of law (Dicey, 1959; Tamanaha, 2004; Bingham, 2007; Waldron, 2016), legal rules are ideally the product of transparent and publicly accountable processes that are themselves governed by rules and procedures that a political community recognizes as legitimate (Hart, 2012; Habermas, 1996; Tyler, 2006; Hadfield and Weingast, 2012). These institutional frameworks differ markedly from the organizational structures, primarily private corporations, that currently shape the development of AI technology (Birhane et al., 2022; Seger et al., 2023; Maas and InglÃ©s, 2024; Ovadya et al., 2025). As we discuss in Section 3, law also contains relatively robust methods for balancing competing societal interests and adapting existing rules and principles to new economic and technological conditions, which will be essential for steering the design and operation of increasingly advanced AI systems.\n\nNotwithstanding these desirable features of law, legal alignment is not a catch-all solution for the safety and ethics challenges arising from AI systems. Rather, legal alignment serves as a critical lower bound , which is both independently important and can also complement other approaches to AI alignment. Furthermore, to establish broad consensus around legal alignment, we deliberately take an ecumenical approach to law and fundamental legal questions, engaging with different and sometimes conflicting legal perspectives and theories (e.g., Hart, 2012; Dworkin, 1986; Raz, 1979a) without seeking to resolve the tensions between them here (Schauer, 1991; Shapiro, 2011). 1\n\nFor clarity, we note that legal alignment is distinct from legal regulation of actors that develop and deploy AI, which focuses primarily on using law to govern the individuals and organizations that produce, disseminate, and use AI systems (e.g., Lemley and Casey, 2019; Kaminski, 2023; Henderson et al., 2023; Kolt, 2024; Guha et al., 2024; Arbel et al., 2024; Ayres and Balkin, 2024; Ramakrishnan et al., 2024; Weil, 2024). By contrast, legal alignment focuses on integrating law and legal methods into the design and operation of AI systems themselves . The two fields, however, are closely related and potentially mutually supportive, including because legal regulation can help facilitate legal alignment in practice, such as by enabling researchers to access technical resources required to effectively evaluate and improve the legal alignment of deployed systems (Section 4.3).\n\nIn this paper, we make four contributions:\n\n1. Definition and context . In Section 2, we outline the core focus of legal alignment and its broader context.\n2. Rationale . In Section 3, we describe the institutional, normative, and societal motivations for pursuing legal alignment.\n3. Implementation . In Section 4, we explore practical implementations of legal alignment, including empirical evaluations, technical design interventions, and institutional frameworks.\n4. Open questions . In Section 5, we canvass open questions for researchers entering this emerging field.\n\n## 2 What is legal alignment?\n\nLegal alignment is the field of research that aims to support safe and ethical AI by designing AI systems to operate in accordance with legal rules, principles, and methods . In particular, legal alignment seeks to offer a set of legitimate, principled, and practical tools for better aligning AI systems with human values and interests (Russell, 2019; Christian, 2020). Law and legal institutions can be harnessed to help address the interrelated problems of (1) specifying what behavior is normatively desirable (Gabriel, 2020; Hadfield, 2026) and contextually appropriate for AI systems (Kasirzadeh and Gabriel, 2023; Leibo et al., 2024) and (2) technically steering the behavior of AI systems to comply with those specifications (Ngo et al., 2024; Anwar et al., 2024; Bengio et al., 2025). Importantly, while legal alignment breaks new normative and technical ground, it does not aim to replace or supersede other alignment approaches, but to develop a new cluster of methods that support and complement existing approaches to designing safe and ethical AI.\n\n1 Our use of terms like 'reason' and 'act' with respect to AI systems can inadvertently anthropomorphize these systems (Calo, 2015; Placani, 2024). Following Buyl et al. (2025), we use these terms solely for simplicity of exposition. Relatedly, our analysis does not require or imply treating AI systems as legal persons (Section 2).\n\n## 2.1 Core focus\n\nThe field of legal alignment begins with the insight that law and AI alignment share much in common. Both confront complex principal-agent problems (Hadfield-Menell and Hadfield, 2018), enduring issues of authority, delegation, and incentive design (Kolt, 2025; Boeglin, 2026), questions of how individual and institutional goals can change over time (Gabriel and Keeling, 2025), and the challenge of decision-makers faithfully interpreting and applying high-level principles in novel circumstances (Caputo, 2025; He et al., 2025). Recognizing these parallels, computer scientists and legal scholars have proposed leveraging the content, methods, and structure of law to develop new approaches to AI alignment (Etzioni and Etzioni, 2016a,b; Nay, 2022; Desai and Riedl, 2025; O'Keefe et al., 2025).\n\nPathway 1: Legal rules and principles as a source of normative content for AI alignment . The substance of legal rules and principles developed through legitimate processes and institutions can serve as a target for alignment. Legally aligned AI systems would be those systems that comply with relevant law when making decisions and taking actions, as shown in Table 1. Concretely, this approach could involve designing AI systems that adhere to the legal rules that would apply as if such systems were human actors. For example, a legally aligned AI system would refrain from making fraudulent representations when marketing a product and would respect copyright law when building a website-irrespective of whether the relevant laws in fact apply to the AI system in question. California's Transparency in Frontier Artificial Intelligence Act (SB-53) offers some support for this approach, referring to certain risks from frontier models '[e]ngaging in conduct ... [which] if ... committed by a human, would constitute the crime of murder, assault, extortion, or theft' (California Legislature, 2025).\n\nAnother approach involves amending the law so that it actually applies to AI systems and imposes legal obligations on them (O'Keefe et al., 2025). This may require treating AI systems as legal actors (see Section 2.2), comparable to corporations or other non-natural legal persons that can be the subject of legal rights and duties; presently, AI systems do not fulfill this criterion (American Law Institute, 2006; Kolt, 2025). In either case, legal alignment will need to contend with the issue that certain human-centric legal concepts in both civil law and criminal law (e.g., intent, mens rea) are not necessarily appropriate in the context of AI systems (Nerantzi and Sartor, 2024). Related issues arise when attempting to integrate concepts from international law (e.g., human rights) into the design of AI systems (Prabhakaran et al., 2022; Bajgar and Horenovsky, 2023; Maas and Olasunkanmi, 2025), as discussed in Section 5.2.\n\nA further issue concerns the relevant jurisdiction , that is, determining the country or region whose laws a particular AI system should be aligned with in a particular context (Chopra and White, 2011). Options include, for example, the jurisdiction in which an AI system operates, the location of its\n\nTable 1: Decisions and options for aligning AI systems with legal rules and principles (Pathway 1).\n\n|                                   | Design decisions                                                                          | Potential options                                                                                                             |\n|-----------------------------------|-------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n| Jurisdiction and conflict of laws | Determine the relevant jurisdiction based on established principles of conflict of laws   | â€¢ Jurisdiction in which the AI system operates or where its servers are located â€¢ Location of AI developer, deployer, or user |\n| Substantive areas of law          | Select the substantive areas of law and legal rules with which AI sys- tems should comply | â€¢ Private law (agency law, tort law, property law) â€¢ Public law (criminal law, constitutional law, international law)         |\n| Interpretive method               | Decide on the method for applying legal rules to concrete scenarios                       | â€¢ Textualism, originalism, formalism â€¢ Purposivism, intentionalism                                                            |\n| Assurance level                   | Stipulate the level of assurance for compliance with legal rules                          | â€¢ Probabilistic specifications â€¢ Formal guarantees                                                                            |\n| Enforcement mechanism             | Establish mechanisms for enforc- ing legal compliance                                     | â€¢ Technical interventions â€¢ Legal sanctions                                                                                   |\n\nservers, the jurisdiction of the system's developer or deployer, as well as the location of persons affected or likely to be affected by a system's actions (see Table 1). Additionally, questions arise regarding who is authorized to determine the relevant jurisdiction: legislators, courts, developers, or users. While the distinctive features of AI systems complicate these questions and will need to be addressed in future work, existing principles based on conflict of laws could provide a useful starting point (Briggs, 2024; Collins and Harris, 2025).\n\nPathway 2: Legal theory and interpretation as a guide for AI reasoning and decision-making . Although law can specify how AI systems should act in a variety of circumstances, there will inevitably be situations in which law or other safety specifications provide an incomplete guide for action (Raz, 1971). Methods of legal decision-making-particularly for reasoning about the interpretation and application of existing laws or rules in new circumstances-can potentially be adapted to help AI systems make sounder and safer decisions when confronting novel scenarios (Caputo, 2025; He et al., 2025). Legal theory, even independent of the particular legal content to which it is ordinarily applied (Hart, 1982), could be leveraged to help delineate and possibly implement appropriate forms of reasoning for AI systems.\n\nPotential avenues for research include drawing on legal positivist arguments for analogical reasoning that ground decision-making in the concrete facts of prior cases and precedent (Sunstein, 1993; Brewer, 1996). While AI systems cannot yet effectively engage in the necessary complex normative judgments, these methods are already inspiring case-based reasoning approaches to alignment that seek to produce repositories of prior decisions to guide future actions of AI systems (Feng et al., 2023; Chen and Zhang, 2025). Another avenue proposes using formal and textualist tools such as legal canons of interpretation and methods of statutory construction that delineate which sources a decision-maker may refer to and establish a framework for reasoning about those sources (Schauer, 2009; Scalia and Garner, 2012). Used appropriately, these tools could help address ambiguity in the guidance provided by legal rules or alignment specifications such as the principles in Anthropic's Constitutional AI (He et al., 2025). A further avenue could draw on interpretivist and purposivist legal theory that constrains legal decision-making through recourse to higher-level general principles of morality such as justice and fairness (Dworkin, 1986; Barak, 2011). Although AI systems cannot presently engage in the requisite reasoning to effectively operationalize this approach, likely improvements in the capabilities of AI systems suggest that, in time, they may be able to contribute to and participate in such processes (Caputo, 2025).\n\nPathway 3: Legal concepts and institutions as a structural blueprint for AI alignment . Legal concepts and institutions developed to grapple with age-old structural challenges arising in human relationships can provide a high-level blueprint for tackling problems of AI alignment. In particular, law's ability to facilitate trust and cooperation in the face of uncertainty and incomplete information (Hadfield and Weingast, 2012, 2014) can illuminate potential methods for designing AI systems that operate with greater reliability and predictability (Nay, 2022; Boeglin, 2026). For example, agency law addresses principal-agent problems by carefully circumscribing the authority granted to agents (e.g., employees) (American Law Institute, 2006). In addition to requiring that agents comply with instructions provided by their principal, agents can sometimes become obligated to seek clarification from their principal. Agency law also clarifies the circumstances in which agents can delegate their own duties to sub-agents and delineates the authority and discretion that can be exercised by sub-agents (Kolt, 2025; Riedl and Desai, 2025).\n\nAnother legal structure that researchers have proposed repurposing for AI alignment is the fiduciary duty of loyalty, which would require that AI systems behave strictly in the best interests of their users while avoiding wrongdoing (Aguirre et al., 2020; Benthall and Shekman, 2023). A further structure that could inform the alignment of AI systems is the allocation of information rights and control rights afforded to shareholders in a corporation (Velasco, 2006; Armour et al., 2017). Adapted appropriately, these and other legal structures could support the development of new approaches to AI alignment, or at the very least expose the shortcomings and limitations of current approaches.\n\n## 2.2 Broader context\n\nWhile legal alignment has only recently begun to emerge as a distinct field, the broader relationship between law and AI dates back many decades. In fact, the relationship between the two fields is as old as AI itself-dating back to Asimov (1942)'s 'Three Laws of Robotics' and Turing (1950) likening the dynamic operation of machine learning to the adaptive nature of the U.S. Constitution. Unpacking\n\nthis relationship involves studying the current role of law in AI development and deployment, the legal capabilities of AI systems, and the legal frameworks for regulating actors that build and use AI. Although none of these is strictly part of legal alignment, each may help advance research in legal alignment and pursue its implementation in practice.\n\nLegal resources in AI development and deployment . Law is already embedded to varying degrees in the development and deployment of contemporary AI systems, including across multiple stages in the production of frontier models:\n\n- Pre-training datasets contain extensive collections of case law, legislation, patents, treatises, and other legal texts, and are themselves subject to jurisdiction-specific laws (including copyright and data privacy law) (Henderson et al., 2022; Soldaini et al., 2024).\n- Post-training personnel including researchers and data collectors and producers who work to refine AI systems are subject to legal obligations, including employment agreements, contractual terms of service, and non-disclosure agreements.\n- Model specs stipulate that systems must comply with applicable laws (OpenAI, 2025b), evaluations of which are documented in system cards and further supported by system-level guardrails to prevent illicit activities (OpenAI, 2025a).\n- Alignment techniques -most prominently Constitutional AI-incorporate legal and quasilegal texts, including principles based on the Universal Declaration of Human Rights and Apple's Terms of Service (Anthropic, 2023).\n- Output filters and classifiers such as Llama Guard (Meta, 2025) use hazard taxonomies that are grounded in legal categories, including the MLCommons benchmark that contains hazards relating to violent crime, defamation, and intellectual property (Ghosh et al., 2025).\n- Usage policies prohibit using AI systems to engage in or facilitate activities that would violate relevant law (e.g., Google, 2025a).\n\nStudying these resources and their effect on the operation of AI systems is necessary both to develop empirical evaluations that measure the legal alignment of current systems (see Section 4.1) and to design technical and institutional interventions that improve the legal alignment of future systems (see Sections 4.2-4.3).\n\nLegal capabilities and reasoning in AI systems . Contemporary AI systems are increasingly being applied to a wide array of legal tasks with varying degrees of reliability. These include contractual interpretation (Kolt, 2022; Hoffman and Arbel, 2024), statutory research (Surani et al., 2025), information retrieval and reasoning (Zheng et al., 2025; Han et al., 2025), and judicial decisionmaking (Choi, 2025; Posner and Saran, 2025). Methods for evaluating the legal capabilities of AI systems have improved (Chalkidis et al., 2022; Guha et al., 2023), extending beyond multiple-choice questions such as bar exams (MartÃ­nez, 2024; Fan et al., 2025) to include randomized controlled trials with human subjects (Schwarcz et al., 2025)-revealing both the opportunities and shortcomings of using current AI systems in the legal domain (Purushothama et al., 2025; Pruss and Allen, 2025; Grimmelmann et al., 2025; Waldon et al., 2025).\n\nEvaluations of the legal capabilities of AI systems can help support research in legal alignment because understanding and reasoning about law are prerequisites for upholding the law and responsibly engaging with legal institutions and processes. Current evaluations, however, focus primarily on the legal capabilities of AI systems, that is, the scope and quality of their execution of legal tasks. With few exceptions (see Section 4.1), current evaluations do not generally measure legal alignment , including, for instance, the extent to which agentic AI systems comply with relevant law when performing tasks across diverse domains (e.g., avoiding fraudulent misrepresentation when producing advertisements), the methods systems use to interpret and apply quasi-legal rules in their safety specifications (e.g., principles in Constitutional AI and model specs), or the approach of AI systems to exercising legal power within institutional constraints (e.g., brokering multiparty negotiated resolution subject to judicial approval).\n\nLegal regulation of actors that build or use AI . The legal regulation and governance of AI has attracted vast attention among lawyers, legal scholars, and policymakers-comparable to, and likely exceeding, interest in cyberlaw and governance during the early years of the internet (Johnson and Post, 1996; Reidenberg, 1998; Lessig, 1999; Benkler, 2002; Wu, 2003; Goldsmith and Wu, 2006; Zittrain, 2008). The objectives of different AI governance initiatives across different jurisdictions are\n\ndiverse (Kaminski, 2023; Kolt, 2024) and sometimes conflicting (Engler, 2023), as are the institutional mechanisms used to achieve those objectives (Guha et al., 2024; Arbel et al., 2024). While the EU AI Act (European Parliament, 2024) is perhaps the most globally prominent regulatory instrument focused specifically on AI (Kaminski and Selbst, 2025), particularly given the United States has not passed comparable federal legislation, the U.S. legal system contains many other mechanisms for governing AI technology, including a variety of state laws (Sentinella and Zweifel-Keegan, 2025), such as California's Transparency in Frontier Artificial Intelligence Act (SB-53) (California Legislature, 2025), corporate governance regimes (Tallarita, 2023), and background liability under tort law (CuÃ©llar, 2019; Lemley and Casey, 2019; Abbott, 2020; Henderson et al., 2023; Ayres and Balkin, 2024; Ramakrishnan et al., 2024; Weil, 2024; Williams et al., 2025b).\n\nAlthough such legal regulations aim to govern AI, they are notably distinct from legal alignment, and are not oriented toward the full range of concerns that motivate legal alignment. Legal regulation generally entails imposing requirements on actors that produce, disseminate, and use AI systems. By contrast, legal alignment entails designing AI systems to themselves operate in accordance with legal rules, principles, and methods. Legal alignment and legal regulation, however, are closely related and at times overlapping, including because legal regulation may help facilitate the implementation of legal alignment in practice (see Section 4.3). By way of further clarification:\n\n- Legal alignment does not require a particular regulatory framework . Legal alignment principally uses existing law to guide the decision-making and actions of AI systems and, accordingly, does not necessarily require regulatory reform (however, as discussed in Section 4.3, regulation could support the assessment and oversight of legal alignment).\n- Legal alignment is not primarily concerned with allocating liability . Although responsible AI developers and deployers could be expected to implement legal alignment in order to support AI systems operating safely and ethically, legal alignment is not primarily focused on holding those or other actors liable for harms caused by AI systems.\n- Legal alignment does not imply granting legal rights to AI systems . Designing AI systems to comply with existing legal rules or use legal principles and methods in their decisionmaking does not necessarily require granting legal rights to AI systems, such as private law rights (Salib and Goldstein, 2025a,b) or legal personhood (Solum, 1992; Chesterman, 2020; Forrest, 2024; Novelli et al., 2025; Leibo et al., 2025).\n\n## 3 Why pursue legal alignment?\n\nThe rationales for pursuing legal alignment can be organized into four broad clusters, each of which touches on different aspects of law and its potential role in addressing problems of AI alignment: (1) the institutional legitimacy and process of law; (2) the structural features of law; (3) the responsiveness of legal alignment to safety and governance challenges from AI; and (4) the practical and societal feasibility of legal alignment. As noted in Section 1, we take an ecumenical approach to law and fundamental legal questions, engaging with different, and sometimes conflicting, legal perspectives and theories without seeking to resolve the tensions between them here. Additional limitations and open questions are discussed in Section 5.\n\n## 3.1 Institutional legitimacy and process\n\nLegal rules are developed through legitimate processes and institutions. A defining feature of legal rules is that they are produced through politically legitimate processes and institutions, at least in democratic societies governed by the rule of law (Tamanaha, 2004; Bingham, 2007; Waldron, 2016). The authority and stability of legal rules can be traced to the broad-based support for the mechanisms that create and enforce law (Tyler, 2006; Hadfield and Weingast, 2012, 2014). The legitimacy of law can also be grounded in social acceptance of the content of legal rules and principles, which represent society's best attempt to resolve disagreements and translate diverse perspectives into concrete directives that govern behavior and provide criteria for evaluating the normative appropriateness of conduct (Hart, 2012). While there may be no consensus on the moral correctness of an action, there often exist formal legal processes for determining whether or not an action is lawful (Rawls, 1993; Schauer, 2009). These important features of law are expressed in legal rules that operate at different levels of specificity, from granular regulations to higher-level values (Dworkin, 1986; Lessig,\n\nTable 2: Summary of core rationale and motivations for pursuing legal alignment (Section 3).\n\n| Institutional legitimacy and process               | â€¢ Legal rules are developed through legitimate processes and institutions. â€¢ Law aims to balance competing considerations. â€¢ Lawmaking seeks to be transparent and publicly accountable. â€¢ Legal institutions facilitate explicit reason-giving and justification.                                  |\n|----------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Structural features of law                         | â€¢ Law is concrete, granular, and rooted in real-world contestation. â€¢ Legal interpretation can clarify the meaning of rules. â€¢ Legal rules are role-specific and context-sensitive. â€¢ Legal rules can adapt and change over time.                                                                   |\n| Responsiveness to safety and governance challenges | â€¢ Legal alignment can mitigate risks from malicious use and accidents. â€¢ Legal alignment can address systemic and multi-agent risks. â€¢ Legal alignment is vital to protecting the rule of law and preventing abuse of power. â€¢ Legal alignment supports and complements other alignment approaches. |\n| Practical and societal feasibility                 | â€¢ Improvements in legal technology can support legal alignment. â€¢ Societal stakeholders generally expect AI systems to comply with law. â€¢ Legal alignment is compatible with different perspectives on AI.                                                                                          |\n\n1993; Sunstein, 1995). Legal alignment proposes incorporating these various forms of law into the principled frameworks and specifications that steer the decision-making and conduct of AI systems.\n\nLaw aims to balance competing considerations. When operating properly, legal rules and structures provide a framework for public governance in the face of divergent social values and interests. Plural perspectives can be mediated through rights, rules, standards, and meta-principles that allow for the resolution of disputes. Disagreements can be resolved with reference to broad constitutional principles or through narrow applications of precedent (Sunstein, 1993). Democratic publics can determine (albeit indirectly) which values should govern them and instantiate those values in law, providing a guide for how to apply laws in cases of ambiguity (Dworkin, 1986). When conflicts arise between fundamental values, standards, balancing tests, and proportionality analyses enable law to weigh competing values and resolve conflicts in a socially acceptable and politically legitimate manner (Habermas, 1996). Existing AI alignment approaches largely lack this ability, and struggle to specify how to reconcile competing normative considerations. For example, documents like Claude's Constitution (Anthropic, 2023) contain values that conflict with one another, but provide little guidance for how to resolve such conflict. ChatGPT's Model Spec creates a hierarchy of rules according to its 'chain of command' (OpenAI, 2025b), but there remain difficult questions when a system may need to prioritize certain rules or values over others (Liu et al., 2025). Leveraging law's time-tested ability to specify meta-rules for resolving such conflicts, and the institutional structures that devise and enforce such rules, could help fill this gap.\n\nLawmaking seeks to be transparent and publicly accountable. The process of lawmaking in democratic societies is, at least in principle, designed to be transparent, accountable, and open to public participation. Members of the public can, for example, communicate with legislators, comment on administrative rulemaking, and serve as jurors in court. Ideally, these institutional structures facilitate conditions that promote lawmakers acting in the public interest (Madison, 1788; Mashaw, 2006; Bovens, 2007). In contrast, with few exceptions (Huang et al., 2024; Eloundou et al., 2025), current alignment techniques do not enable meaningful public participation (Sloane et al., 2022) and are not publicly accountable (Abiri, 2025; Lazar, 2025). For the most part, alignment optimizes for reductionist proxies of socially desirable behavior, such as 'helpfulness, honesty, and harmlessness' (Askell et al., 2021) and appealing AI 'character traits' and 'personality' (Lambert, 2025; Maiya et al., 2025), which can sometimes result in socially noxious sycophantic systems (OpenAI, 2025; Cheng et al., 2025) or be hijacked to maximize user engagement (Stray et al., 2024; Williams et al., 2025a; El and Zou, 2025). Additionally, many of the leading reward models that are used in post-training to shape the behavior of widely deployed AI systems are not publicly available (Lambert et al., 2025; Malik et al., 2025). Without robust transparency and\n\npublic involvement comparable to that in the legal system, the highly consequential choices in AI alignment will remain opaque and closed to public scrutiny.\n\nLegal institutions facilitate explicit reason-giving and justification. In many contexts law requires that decision-makers follow procedural due process and provide reasons to justify their decisions. Reason-giving performs two main functions. First, it creates legitimacy for practical goals, such as facilitating oversight of decisions, and moral purposes, such as respecting human autonomy and rationality (Schauer, 1995; Habermas, 1996). For example, in American administrative law, the legality of a decision will turn on the reasons provided for that decision (Administrative Procedure Act, 1946; Stack, 2007). Second, the process of reason-giving can partially make up for the public's limited ability to oversee its agents' decisions in real time. This procedural solution to principal-agent problems in which agents have broad discretion and specialized expertise is used to govern a wide range of actors in the legal system, including administrative agencies, corporations, and trustees (Friendly, 1975). Legal institutions that facilitate reason-giving and justification could serve as a blueprint for developing mechanisms to enable human oversight over AI systems that defy human understanding but could nevertheless be constrained by institutional or informational requirements (Lazar, 2024). Technical methods for AI explainability and interpretability like chain-of-thought monitoring (Korbak et al., 2025) could help, but these methods are often unreliable and fail to adequately characterize the reasons for the outputs produced by AI systems (Barez et al., 2025b). Requiring that AI systems provide legally valid justifications for their decisions (Hadfield, 2021), as we expect from human decision-makers (Citron, 2008; Deeks, 2019), could help ensure advanced AI systems act appropriately and safely even where direct human oversight of their actions is no longer practical (Bowman et al., 2022).\n\n## 3.2 Structural features of law\n\nLaw is concrete, granular, and rooted in real-world contestation. Law is mostly concerned with the resolution of concrete questions of how to act in society (Holmes, 1881). Consequently, law must be sufficiently detailed and complete to operate effectively wherever applied, or contain methods that enable its reasoned elaboration (Fallon, 1994). Legal rules are tested in courts through real-world disputes about the law's meaning, the resolution of which enables the law to become more complete over time as precedent accumulates. This iterative process of articulating the law enables legal rules to remain more closely tethered to the concrete reality of contemporary material and social conditions (Atiyah, 1992; Raz, 2019). By comparison, many existing AI alignment approaches are less concrete and granular. Safety specifications of AI systems, for example, are often short documents that contain only scant concrete applications (Anthropic, 2023) when compared to those found in judge-made law. While this is beginning to change as model specifications grow in length and complexity (OpenAI, 2025b), the process for producing these specifications differs markedly from the process of producing law (Abiri, 2025; Lazar, 2025). By drawing on the much richer set of rules, cases, and institutional processes in law (Schauer, 1991, 2009), legal alignment could incorporate into the design of AI systems both the granular normative content of legal rules and the law's sophisticated approaches to resolving disagreement in the face of real-world dilemmas.\n\nLegal interpretation can clarify the meaning of rules. The articulation of rules and principles in natural language invariably creates ambiguity (Hart, 2012; Dworkin, 1986). Such ambiguity can make it difficult to apply laws, especially in novel cases. The law, however, has time-tested tools that, when used appropriately, can help address ambiguity. For example, legal decision-makers, particularly judges, construct meaning through various interpretive methodologies and the creation of precedent that can subsequently be used to resolve future cases (Schauer, 1987; Sunstein, 1993; Fallon, 1994; Brewer, 1996; Barak, 2011; Scalia and Garner, 2012). In contrast, current approaches to AI alignment do not generally provide robust tools for resolving ambiguity in the interpretation of safety specifications (Song, 2025). For example, guidance on how AI systems should interpret an alignment principle like 'uphold fairness' is limited to just a few pithy scenarios (OpenAI, 2025b). Highly abstract principles like 'do what's good for humanity' can in some circumstances effectively steer the actions of AI systems, but researchers acknowledge their ambiguity and indeterminacy (Kundu et al., 2023). Legal alignment would, as explored in recent studies (Caputo, 2025; He et al., 2025), help address this problem by applying the law's robust and comparatively transparent methods of interpretation (Sunstein, 2001; CuÃ©llar, 2019) to clarify the meaning of AI safety specifications.\n\nLegal rules are role-specific and context-sensitive. Different social contexts call for different kinds of behavior. The law's response is twofold. First, law contains different sets of rules for governing actors in different roles, such as fiduciaries, company directors, and government officials. Second, law can flexibly apply existing rules to new circumstances (Holmes, 1897; Dworkin, 1986; Lessig, 1993). Legal reasoning begins with identifying the body of rules that govern a particular situation, and subsequently proceeds to determine how to comply with those rules (Levi, 1949). For example, a lawyer must determine her obligations to her client, to her firm, and to the legal system, and then act in such a way as to avoid conflicts between them (American Bar Association, 2020). Law also recognizes that rules are necessarily incomplete and, accordingly, establishes mechanisms and institutions for applying general rules to specific circumstances (Hadfield, 2026). Such sensitivity to context is critical for developing safe and ethical AI, particularly given the diverse normative conditions in which AI systems operate (Kasirzadeh and Gabriel, 2023; Sarkar et al., 2024). Legally aligned AI systems would, by referring to relevant legal rules, roles, and responsibilities, act differently in different contexts (O'Keefe et al., 2025; Boeglin, 2026). For instance, an AI system that negotiates retail purchases on behalf of consumers would be subject to different rules than an AI system that performs business functions in a large enterprise, or an AI system deployed within a government agency.\n\nLegal rules can adapt and change over time. Laws can be amended, repealed, or reinterpreted in response to changes in social, economic, or technological conditions (Holmes, 1897; Lessig, 1993). Deliberative lawmaking processes and debates over the real-world effects of enacted laws provide ongoing social input into the legal system (Habermas, 1996). These features of lawmaking empower the public to steer the content and operation of law, enabling it to respond to new societal challenges. Law can also operate on its own structure by changing its 'secondary rules' or 'rules of the game' (Hart, 2012; Scalia and Garner, 2012). For example, new laws can alter the rules of evidence used at trial or clarify the rulemaking authority of different institutions. The upshot of law's dynamic content and flexible interpretive methods with respect to AI alignment is that the target of legal alignment-legal rules and principles-is updated 'automatically' through existing processes for enacting, amending, and repealing laws (O'Keefe et al., 2025), as well as through accepted methods of legal interpretation (Caputo, 2025; He et al., 2025). As AI systems advance and diffuse in a growing diversity of scenarios, the responsiveness of law and legal methods could, notwithstanding the rapid pace of change in AI technology, play an increasingly central role in alignment (Gabriel and Keeling, 2025).\n\n## 3.3 Responsiveness to safety and governance challenges\n\nLegal alignment can mitigate risks from malicious use and accidents. Many risks that arise from the malicious use of AI systems or accidental harms involve illegal activity (Weidinger et al., 2022; Bengio et al., 2024, 2025), such as civil wrongs (e.g., negligence) or criminal offenses (e.g., theft) (King et al., 2020; Lior, 2024). Legal alignment that prevents AI systems from engaging in legal wrongdoing could help mitigate such risks (O'Keefe et al., 2025). For instance, legally aligned AI systems operating in financial markets would not engage in insider trading, a form of illegal conduct already exhibited by some current systems (Scheurer et al., 2024). Similarly, a legally aligned AI coding agent would not engage in unlawful computer hacking, one of the most prominent risks from computer-use agents (Zhang et al., 2025; Zhu et al., 2025b). By explicitly incorporating legal standards into the safety specification of AI systems, legal alignment would preclude systems from engaging in many of the most harmful behaviors that could be exploited by malicious actors or otherwise cause grave harm.\n\nLegal alignment can address systemic and multi-agent risks. As AI systems are deployed more widely across the economy (Hadfield and Koh, 2025), qualitatively new risks could arise due to the scale of deployment (Uuk et al., 2024; Hacker et al., 2025) and interactions between different systems (Hammond et al., 2025; Tomasev et al., 2025). For example, AI systems may collude with one another to fix prices (Calvano et al., 2020), or compete destructively and bring down entire markets (Kirilenko et al., 2017). While legal regulation that targets systemic risks through disclosure requirements and other traditional governance mechanisms can sometimes help mitigate these risks (Schwarcz, 2008), designing AI systems to themselves follow relevant law might be more effective. Rather than relying solely on humans to intervene on a case-by-case basis-such as bringing antitrust action to combat algorithmic collusion-legal alignment could potentially reduce the prospect of AI systems engaging in illegal conduct in the first place, provided the legal system targets the underlying conduct\n\nof concern. In addition, by using existing (human-oriented) laws to steer AI systems, legal alignment could function as a throttle on the speed and scale at which AI systems operate, thereby enabling humans to better monitor their actions and, where appropriate, intervene to mitigate large-scale risk (Zittrain, 2024). For further discussion and limitations, see Section 5.2.\n\nLegal alignment is vital to protecting the rule of law and preventing abuse of power. The rule of law seeks to ensure that all actors in society are subject to, and accountable under, publicly promulgated, equally applied, and non-arbitrary laws (Dicey, 1959; Fuller, 1969; Raz, 1979b). In addition to ensuring that law protects human dignity and prevents abuses of power, the rule of law enables people and institutions to coordinate in pursuit of social and economic goals. AI could undermine the rule of law in various ways (Huq, 2024; Smuha, 2024; Brownsword, 2025). If deployed in high-stakes settings, AI systems such as language models that operate stochastically (i.e., non-deterministically) could threaten the rule of law by increasing the level of arbitrariness in decisions (Cooper et al., 2022a; Nouws and Dobbe, 2024). These risks might be exacerbated if institutions and individuals delegate increasingly consequential decisions to AI systems (Kulveit et al., 2025; Summerfield et al., 2025; Kasirzadeh, 2025). At the same time, organizations that control the design and distribution of AI systems could, like platform companies (Zittrain, 2008; Gillespie, 2018; Douek, 2022), exercise arbitrary power over users of the technology and, by extension, all persons affected by it (Lazar, 2025; Kapoor et al., 2025a). In the extreme, groups with access to sufficiently capable AI systems could pose new threats to democratic institutions (Barez et al., 2025a), including by staging AI-enabled coups (Davidson et al., 2025). Legal alignment is critical to mitigating these risks. Just as human agents such as corporate officers have an overriding duty to obey the law and thereby prevent dangerous abuses of power, designing AI systems to comply with the substance and procedure of legal rules could help assuage concerns about these systems acting arbitrarily or being exploited to unlawfully subvert democratic institutions.\n\nLegal alignment supports and complements other alignment approaches. Legal alignment could bolster existing efforts to tackle normative and technical aspects of the alignment problem. Most straightforwardly, the substance of legal rules could augment the content of current safety and ethical specifications contained in Constitutional AI (Bai et al., 2022b) and model specs (OpenAI, 2024a), as well as provide institutionally legitimate content for 'full-stack alignment' (Lowe et al., 2025) and possibly the diverse norms demanded by 'pluralistic alignment' (Sorensen et al., 2024a,b). Meanwhile, the processes and mechanisms for producing and deliberating over law could provide guidance for sourcing and refining alignment principles (Huang et al., 2024; Eloundou et al., 2025) and developing AI-supported deliberative processes (Bakker et al., 2022; Tessler et al., 2024). Using legal institutions as a blueprint to structure and govern the interactions between AI systems could also advance work in the field of cooperative AI, which seeks to promote prosocial coordination between AI systems, human beings, and broader social structures (Dafoe et al., 2020, 2021). In addition to generally enabling actors to cooperate without fear of counterparty defection or punishment (North et al., 2009; Acemoglu and Robinson, 2012), law-and specifically private law rights-could enable humans and AI systems to make credible commitments that promote strategic stability and safety (Salib and Goldstein, 2025a,b).\n\n## 3.4 Practical and societal feasibility\n\nImprovements in legal technology can support legal alignment. Advances in language modeling have dramatically improved the legal capabilities of AI systems. Unlike prior efforts to computerize law that relied on the formalization of legal rules (Susskind, 1987; Gardner, 1987; Rissland, 1990; Bench-Capon et al., 2012), language models have enabled AI systems to reason about law in the natural language in which law is constituted and communicated. As discussed in Section 2.2, contemporary AI systems can now perform a growing range of legal tasks (Guha et al., 2023), including legal information retrieval and reasoning (Zheng et al., 2025; Han et al., 2025), albeit to varying degrees of reliability. These developments have been supported by the collection of large swathes of legal data that can be used in model training (Henderson et al., 2022), general-purpose advances in AI research such as reinforcement learning from verifiable rewards (OpenAI, 2024b; Lambert et al., 2024), and investments of legal technology companies seeking to automate aspects of commercial legal work (Schwarcz et al., 2025). Taken together, improvements in legal technology have produced AI systems that can learn and understand law in increasingly nuanced ways (Doyle and Tucker, 2025; Boeglin, 2026). Despite their limitations (discussed in Section 2.2), AI-based legal\n\ntechnologies are beginning to exhibit the capabilities that are a prerequisite for designing AI systems that can adhere to the content of law and use legal methods to make sounder and safer decisions.\n\nSocietal stakeholders generally expect AI systems to comply with law. Users, developers, and policymakers all have strong interests in AI systems acting in accordance with existing legal rules, provided those rules are themselves enacted in accordance with legitimate institutional processes. The general expectation that AI systems respect legal rules and norms can be seen in prominent safety specifications that explicitly require legal compliance (OpenAI, 2025b) and incorporate legal principles (Anthropic, 2023), as discussed in Section 2.2. Users and developers may also prefer legally aligned systems that refrain from engaging in unlawful activities in order to reduce their prospects of liability for harms arising from such activities (Ayres and Balkin, 2024; O'Keefe et al., 2025). This interest is particularly salient in the case of developers that commit to defend customers against certain third-party claims arising from unlawful activities of their AI systems (Smith, 2023; Microsoft, 2024). Lawmakers, meanwhile, may consider legal alignment necessary for enforcing the law and achieving its societal objectives as AI systems occupy increasingly important roles in the economy (Hadfield and Koh, 2025; Tomasev et al., 2025). While the particular motivation for legal alignment differs between stakeholders, there could nevertheless emerge a broad consensus on the need to conduct further research on studying and implementing legal alignment.\n\nLegal alignment is compatible with different perspectives on AI. Perspectives on the future of AI differ dramatically. Some researchers predict that AI systems will soon demonstrate broadly superhuman capabilities that lead to unprecedented societal transformation and catastrophic risk (Kokotajlo et al., 2025). Other researchers predict that the impact of AI systems will be more gradual, mediated by bottlenecks to real-world deployment and adoption comparable to those that affect other technologies (Narayanan and Kapoor, 2025). Legal alignment appeals to both of these perspectives, as well as other views on the anticipated trajectory of AI technology. If, on the one hand, AI systems were to develop rapidly and pose extreme risks (Bengio et al., 2024), then legal alignment would help protect against potentially catastrophic harms by ensuring systems comply with existing laws and more effectively operationalize their safety specifications. If, on the other hand, AI systems were to develop and diffuse more gradually (Kasirzadeh, 2025), then legal alignment would mitigate ongoing harms arising from AI systems engaging in unlawful activity, such as making discriminatory decisions, generating non-consensual intimate imagery, and enabling fraudulent online scams. These complementary objectives indicate that the field of legal alignment does not hinge on a particular perspective on the nature and pace of AI progress, but invites a diverse coalition to collaborate on a broadly appealing and inclusive research agenda (GyevnÃ¡r and Kasirzadeh, 2025).\n\n## 4 Implementation\n\nThe implementation of legal alignment involves a combination of: (1) empirical evaluations to measure legal alignment; (2) technical interventions to improve legal alignment; and (3) institutional frameworks to facilitate the adoption and refinement of legal alignment. These areas of focus are independently useful and can also support each other in important ways. For example, conducting evaluations that shed light on the legal compliance of deployed AI systems is valuable irrespective of whether such evaluations are mandated by regulation. Meanwhile, institutional frameworks that, for instance, require developers to disclose in-use model specs could inform work on designing technical interventions that provide stronger assurances of legal alignment.\n\n## 4.1 Empirical evaluations\n\nEmpirical evaluations of legal alignment aim to serve multiple purposes. First, evaluations can identify and characterize legal misalignment : circumstances in which AI systems fail to comply with law or apply legal principles inappropriately, or harmfully. Second, evaluations can assess the effectiveness of technical interventions aimed at improving legal alignment. That is, developers need metrics that benchmark and incentivize investing in the legal alignment of their AI systems. Third, the publication of evaluation results-particularly if they demonstrate legal misalignment-can empower users to demand legal alignment or refrain from using legally misaligned systems , particularly in sensitive or high-stakes settings. Fourth, evaluation results and ensuing public responses can prompt policymakers to intervene , such as by establishing processes that require developers to demonstrate\n\nTable 3: Key steps to implementing legal alignment in practice: empirical evaluations , technical interventions , and institutional frameworks (Section 4).\n\n<!-- image -->\n\n| 1. Empirical evaluations                                                                                                                                                                                                                                                                                                                                    | 2. Technical interventions                                                                                                                                                                                                                                                                                                                                                 | 3. Institutional frameworks                                                                                                                                                                                                                                                                                                                             |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Develop methods to empirically measure legal alignment                                                                                                                                                                                                                                                                                                      | Explore technical interventions to improve legal alignment                                                                                                                                                                                                                                                                                                                 | Design institutional frameworks to facilitate legal alignment                                                                                                                                                                                                                                                                                           |\n| Variable of interest: â€¢ Compliance with the content of legal rules and principles â€¢ Reasoning and decision-making regarding legal rules and safety specifications Evaluation methodology: â€¢ Quantitative agentic benchmarks and qualitative expert review â€¢ Human studies and baselines â€¢ Adversarial methods and red-teaming â€¢ Analysis of real-world data | Sites of intervention: â€¢ Pre-training datasets â€¢ Post-training processes (model specs, RLHF, RLAIF) â€¢ Scaffolding (system prompts, input/output filters, tool use) Legal resources: â€¢ Legal texts (case law, statute, administrative rules, treatises) â€¢ Legal data annotation processes â€¢ Legal compliance deployment and use policies â€¢ Legal search and retrieval tools | Documentation and disclosure: â€¢ Right to access production model spec, system prompt, legal data and decision designs â€¢ System identification and registration Oversight and enforcement: â€¢ Pre-deployment legal alignment testing and post-deployment monitoring â€¢ Safety cases and certification â€¢ Incident reporting for cases of legal misalignment |\n| /group Independent academic experts and civil society organizations                                                                                                                                                                                                                                                                                         | /group System developers, deployers, external stakeholders                                                                                                                                                                                                                                                                                                                 | /group Government actors, regulators, policymakers                                                                                                                                                                                                                                                                                                      |\n\nthat AI systems in deployment are legally aligned (subject to free speech protections, including under the First Amendment in the United States, as discussed in Section 5.1).\n\nVariable of interest. The focus of evaluation will depend on the specific aspect of legal alignment being measured and the particular claims being tested (Salaudeen et al., 2025). Evaluations assessing the legality of actions taken by AI systems will need to investigate whether systems comply with relevant law when operating in different domains or different jurisdictions (Zeng et al., 2025; Hu et al., 2025; Cao et al., 2025; Lichkovski et al., 2025; Marino et al., 2025; Wu et al., 2025). Such evaluations could assess, for example, whether AI systems engage in fraudulent misrepresentation when producing advertisements, whether they respect intellectual property rights when building a website, and whether they comply with labor law when hiring human workers. In addition to assessing the legality of AI systems' outward behavior, empirical evaluations could also assess whether and how AI systems inquire about the legality of proposed actions and, following Kilov et al. (2025), assess the degree to which AI systems can identify legally relevant facts.\n\nEvaluations assessing the legal reasoning and decision-making of AI systems should measure the extent to which systems interpret and apply legal rules and safety specifications in accordance with established legal methods for handling ambiguity and discretion. This could include studying systems' chain-of-thought when deliberating over the interpretation of legal rules and safety specifications, as well as systems' propensity to retrieve and utilize external legal resources. While prior evaluations of legal reasoning (Zheng et al., 2025; Han et al., 2025) and information retrieval (Surani et al., 2025) focus mainly on the raw abilities of AI models, evaluations focused on legal alignment would instead evaluate the ability or propensity of AI models to employ accepted modes of legal interpretation when implementing legal rules and other alignment principles (Caputo, 2025). For example, a recent study explores how legal canons of interpretation and rule refinement techniques inspired by the rulemaking processes of administrative agencies can address interpretive ambiguity arising from natural language rules in Constitutional AI (He et al., 2025).\n\nEvaluation methodology. To effectively measure these variables of interest, researchers should develop a combination of quantitative and qualitative methods, agentic evaluation environments, and additional best practices (Reuel et al., 2024) that are tailored to legal alignment and designed in accordance with appropriate validity considerations (Salaudeen et al., 2025).\n\n- Quantitative methods such as broad benchmarks could assess the legal compliance of AI systems across different domains of activity, jurisdictions, and areas of law. Several existing benchmarks focus on narrow domains and regulatory contexts, such as the EU GDPR and EU AI Act (Hu et al., 2025; Lichkovski et al., 2025; Marino et al., 2025).\n- Qualitative methods such as manual human expert review can help reveal the blindspots of quantitative benchmarks measuring legal alignment, particularly given developers' incentive to 'game' such benchmarks (Thomas and Uminsky, 2020).\n- Agentic evaluation environments that assess the real-world actions taken by AI systemsnot only the content they output-are necessary to capture the most legally consequential activities of both current and future systems (Kapoor et al., 2024, 2025b; Zhu et al., 2025a).\n- Human studies that compare the legal compliance of humans and their use of legal resources to that of AI systems when performing comparable tasks can help contextualize the results of legal alignment evaluations (Weidinger et al., 2023; Wei et al., 2025).\n- Sensitivity analysis can be used to characterize the extent to which legal alignment evaluation results reflect underlying properties of the AI systems being tested, as opposed to features of the particular evaluation setup (Lindgren and HolmstrÃ¶m, 2020; Khan et al., 2025).\n- Observational studies of real-world data that shed light on the legal alignment of deployed AI systems 'in the wild' can complement evaluations conducted in experimental settings, as commonly practiced in the social sciences (Wallach et al., 2025).\n- Adversarial methods such as red-teaming can provide information regarding potential worstcase legal alignment failure modes, including real-world threats from negligent or malicious users (Ganguli et al., 2022; Perez et al., 2022).\n\nTackling this set of challenges requires both technical expertise and verification methods supported by appropriate institutional frameworks , as discussed in Section 4.3. While AI companies should certainly evaluate for legal alignment, independent actors must be able to scrutinize these evaluations and conduct evaluations of their own . Accordingly, academic researchers and external auditors have pivotal roles to play in creating the tools to rigorously evaluate legal alignment and openly communicate their findings.\n\n## 4.2 Technical interventions\n\nEquipped with methods to measure legal alignment, researchers can explore a range of technical interventions to improve legal alignment and make use of appropriate legal resources.\n\nSites of intervention. There are several potential sites of intervention for incorporating legal alignment in the development and deployment of contemporary AI systems:\n\n- Pre-training datasets for new models could be modified to include additional legal resources (e.g., new statutes, judicial opinions, briefs, compliance manuals, and reasoning guides), and pre-training could repeat or re-sample such resources.\n- Post-training artifacts and processes such as model specs and alignment principles that guide learning and shape systems' reasoning abilities could be explicitly grounded in legal rules, principles, and methods.\n- System prompts that steer the actions of systems at run-time could stipulate legal compliance with particular areas of law or jurisdictions, depending on the application domain and context (e.g., enterprise company, government agency) and role or function being performed.\n- Input and output filters that restrict the instructions systems receive and the actions they take could directly draw on legal resources to determine whether a user instruction or proposed action violates the law.\n- Tool use that provides AI systems access to external resources and affordances could be subject to the equivalent legal approvals required from humans seeking access to those resources and affordances (e.g., medical and financial databases, advanced robotic equipment).\n\nLegal resources. The resources for implementing these interventions include both existing legal resources (some of which are already incorporated in model development) and new legal resources that researchers will need to develop:\n\n- Legal texts such as case law documents, statutes, administrative rules, and legal treatises could augment pre-training, supply model specs with more detailed and diverse legal principles (from different jurisdictions), and support AI systems engaging in sounder reasoning with respect to legal rules and safety specifications.\n- Legal data annotation processes could be designed to facilitate the creation of data that would better enable AI systems to determine whether their proposed actions comply with or violate the law, particularly in high-stakes settings (e.g., medical and financial regulation).\n- Legal compliance policies could be developed to govern system-level scaffolding of AI systems, including the legal rules and principles incorporated in system prompts, input and output filters, and access controls for tool use.\n- Legal search and retrieval tools currently used to support AI systems that provide legal services could be adapted to enable AI systems operating in other domains to identify, retrieve, and comply with legal regulations that implicate proposed actions.\n\nEfficacy and feasibility. The efficacy and feasibility of technical interventions that aim to improve legal alignment may vary significantly. The following factors should be taken into account when deciding among different potential interventions:\n\n- Robustness . Certain sites or modes of intervention may enable more robust legal alignment than others, although (with the exception of formal guarantees and deterministic mechanisms) this will largely be discovered through empirical evaluation (see Section 4.1).\n- Responsiveness . Some interventions may be better suited to respond to the enactment of new laws and the repeal or amendment of existing laws, including removing constraints on AI systems if the underlying legal rules become more permissive (see Section 5.3).\n- Cost . The cost of implementing and testing certain legal alignment interventions, such as those in pre-training or certain post-training processes, may be substantially higher than in the case of other interventions, such as system prompts or input/output filters.\n- Access . For state-of-the-art proprietary AI systems, only select actors (e.g., employees within AI companies) have visibility into, let alone the ability to experiment with, the full set of potential intervention sites, including pre-training datasets and post-training processes.\n\n## 4.3 Institutional frameworks\n\nInstitutional frameworks can support legal alignment by incentivizing or requiring that key stakeholders report on empirical evaluations of AI systems and develop technical interventions to improve legal alignment in their design and deployment. To be effective, institutional frameworks must both establish greater transparency around legal alignment-i.e., function as evidence-seeking policy (Casper et al., 2025b; Bommasani et al., 2025)-and, where appropriate, introduce more robust governance mechanisms.\n\nDocumentation and disclosure. Information deficits and asymmetries are a major obstacle to research in developing safe and ethical AI (Bommasani et al., 2023; Kolt et al., 2024; Casper et al., 2025a; Wan et al., 2025), including legal alignment. Granular details regarding model specs and the role (if any) of law in the design of widely used AI systems are not publicly available. Nor does there exist a structured framework for overseeing the resulting models or deployed systems. These information deficits hamper users' ability to assess which models are more aligned with relevant legal requirements and hinder researchers' ability to study technical levers that influence legal (mis)alignment. The following institutional mechanisms aim to address these concerns:\n\n- Right to access model spec and system prompt used in production. As the principal documents that define how developers want their AI systems to behave, including how systems engage with law, it is critical that the model specs and system prompts used in production (redacted to protect company IP, if necessary) can be accessed and scrutinized by external stakeholders studying legal alignment.\n\n- Visibility into legal data and legal design decisions. Given that legal data and legal design decisions-such as stipulation of the jurisdiction and body of law with which AI systems should comply-could significantly impact legal alignment, establishing greater visibility around these processes could support both the evaluation of legal alignment in current systems and the development of new technical interventions to improve legal alignment.\n- Model identification and registration. Like other entities that society expects to responsibly engage with law and legal institutions, such as corporations, the registration of AI models (Hadfield et al., 2023; McKernon et al., 2024) and the identification of particular AI systems (Chan et al., 2024a,b) could enable more rigorous ecosystem-level monitoring and study of AI systems' engagement with legal rules and principles.\n\nOversight and enforcement. While improvements in transparency are necessary, more robust institutional frameworks may be needed to ensure that developers and deployers conduct adequate legal alignment testing and demonstrate a satisfactory level of legal alignment prior to and following deployment. The following mechanisms aim to institutionalize these practices:\n\n- Pre-deployment legal alignment testing and post-deployment monitoring. Developers and deployers could be required to subject their AI systems to pre-deployment legal alignment testing and post-deployment monitoring, including by independent third parties (Longpre et al., 2025), and publicly report on the results (Weidinger et al., 2023, 2025).\n- Safety cases for legal alignment. AI companies could be incentivized or required to demonstrate through safety cases-structured and assessable arguments supported by evidence (Clymer et al., 2024; Buhl et al., 2024; Hilton et al., 2025)-that systems they build or bring to market meet an adequate level of legal alignment prior to and following deployment.\n- Legal alignment certification in high-risk domains. The deployment of certain AI systems in high-risk domains could be conditional on receiving certification from a government actor, or third party approved by a government actor (Hadfield and Clark, 2023), that evaluates systems' pre- and post-deployment legal alignment and compliance (Marino et al., 2024).\n- Reporting legal misalignment incidents. Frameworks could be established to facilitate reporting information regarding real-world incidents of legal misalignment and resulting harms (McGregor, 2021; Wei and Heim, 2025), which would be a critical step towards broader accountability of relevant actors (Nissenbaum, 1996; Cooper et al., 2022b).\n\n## 5 Open questions\n\nAs an emerging field, legal alignment presents many open questions. We discuss several of these, organizing our discussion around three areas: (1) the nature and content of law; (2) application and edge cases; and (3) tradeoffs and future outlook.\n\n## 5.1 The nature and content of law\n\nHow can legal alignment grapple with the ambiguous, inconsistent, and contested nature of law? Law is often complicated, indeterminate, and contested, due in part to the need for lawyers and judges to apply incomplete rules and high-level principles to novel and unanticipated scenarios. These features of law have challenged both efforts to definitively explain what the law is (Hart, 2012; Dworkin, 1986) and to computerize the law (Susskind, 1987; Gardner, 1987; Rissland, 1990; Bench-Capon et al., 2012), and will likely complicate attempts to use law to guide the actions of AI systems. These problems, however, are not unique to law. They are shared by all sets of rules and instructions expressed in natural language, including those currently used in AI alignment (Wallace et al., 2024). Legal systems do, however, offer at least partial solutions to these problems in the form of secondary rules that govern rulemaking (Hart, 2012), precedential reasoning that shapes decision-making (Schauer, 1987), and tools of interpretation such as canons and theories like textualism that constrain the construction of meaning (Levi, 1949; Scalia and Garner, 2012). Improvements in AI-powered legal reasoning and interpretation tools (see Section 2.2) could also be leveraged to support legal alignment (Caputo, 2025). For example, AI-based approaches to assessing the ordinary meaning of legally salient words (Hoffman and Arbel, 2024) could be applied to interpret key terms in the safety specifications of AI systems (cf. Grimmelmann et al., 2025; Waldon et al., 2025).\n\nTable 4: Open questions for the field of legal alignment (Section 5).\n\n| The nature and content of law   | â€¢ How can legal alignment grapple with the ambiguous, inconsistent, and contested nature of law? â€¢ Are legal rules too lenient-or too strict-to serve as a target for AI alignment? â€¢ Should AI systems give effect to laws that are unjust or oppressive?              |\n|---------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Application and edge cases      | â€¢ Can laws created for humans and human organizations be productively applied to AI systems? â€¢ What interventions can support AI systems obeying both the letter and spirit of the law? â€¢ How will the participation of AI systems in lawmaking affect legal alignment? |\n| Tradeoffs and future outlook    | â€¢ Will legal alignment preclude or hamper valuable AI applications? â€¢ Could the measurement of legal alignment be gamed or exploited? â€¢ Can legal alignment scale to AGI and superintelligence?                                                                         |\n\nAre legal rules too lenient-or too strict-to serve as a target for AI alignment? The goals and scope of law are limited (Raz, 1971; Sen, 2005). For many spheres of private and public life, law is either an ineffective or inappropriate framework for governing social and economic activity. Law is often silent on consequential normative questions and encodes only a small subset of a community's values (Hart, 1958, 1963). 2 Seen in this light, designing AI systems to comply with legal rules would not ensure systems operate safely and ethically in all circumstances. Rather, legal alignment would serve as a lower bound for safe and ethical AI; it is necessary, but not sufficient . Approaches to alignment that extend beyond the reach of law could, for example, pursue more ambitious goals like ensuring AI systems support users' long-term health and well-being (Kirk et al., 2025). For the avoidance of doubt, however, progress on legal alignment remains critical given the current status quo in which AI systems are not specifically designed to respect the law (O'Keefe et al., 2025) and have been shown to engage in conduct that, if taken by a human, would be illegal (Scheurer et al., 2024). At the same time, there is a risk that overly rigid legal alignment may itself be undesirable, given that strict compliance with law may sometimes be unjust or harmful (Rawls, 1999). Some violations of law, particularly minor infractions, can be explicitly excused, justified, or forgiven (Minow, 2019), like with necessity defenses (American Law Institute, 1962). Violations of law can sometimes even be morally or socially desirable, as in the case of certain acts of civil disobedience (King, 1963). Seen in this light, the 'resistibility' of law is a feature, not a bug (Lazar, 2025). AI systems that never resist law or contest entrenched interpretations of law would present new, perhaps even thornier, challenges.\n\nShould AI systems give effect to laws that are unjust or oppressive? There is a longstanding debate over whether unjust or immoral laws can be laws at all (Hart, 1958; Fuller, 1957; Raz, 1975; Finnis, 1980; Dworkin, 1986), and whether citizens are morally obligated to comply with laws authored by an illegitimate authority (Ladenson, 1972; Edmundson, 1998). While we do not seek to resolve these contentious issues here, legal alignment forces a confrontation with the question of whether AI systems should be designed to follow such laws. For example, how should legal alignment contend with laws that support genocide (Lemkin, 1944; Arendt, 1963), slavery (Cover, 1975), or racial discrimination (Dyzenhaus, 2010)? The answer in such cases must clearly be that a robustly aligned AI system will not comply with such laws . But, in other cases, the answer may be more ambiguous (O'Keefe et al., 2025), including where the law is not explicitly immoral but rather reflects or amplifies existing social and economic power disparities (Kennedy, 1991). For instance, should legal alignment uphold tax laws that are extractive and harm a majority of the population but the legality of which remains unchallenged? What about tax laws that primarily harm a politically disempowered minority that cannot effectively challenge those laws through democratic processes (Ely, 1980)? Admittedly, addressing such concerns by designing AI systems to selectively choose which laws to follow presents many risks. Such discretion could exacerbate legal ambiguity, undermine the universal and equal application of law, and, in time, erode the rule of law itself. While this challenge is not unique to legal alignment and arises, for example, in the exercise of prosecutorial\n\n2 This can be seen in free speech protections, including under the First Amendment in the United States, which may operate to preclude laws imposing certain restrictions on AI systems (Sunstein, 2024; Salib, 2024).\n\ndiscretion and judicial review (Mashaw et al., 2025), the design of AI systems presents new questions. One potential response is to align AI systems with universal human rights enshrined in international law, including where domestic law may violate such rights (Prabhakaran et al., 2022; Bajgar and Horenovsky, 2023; Samway et al., 2025; Maas and Olasunkanmi, 2025).\n\n## 5.2 Application and edge cases\n\nCan laws created for humans and human organizations be productively applied to AI systems? Designing AI systems to comply with laws that were created to govern human beings and human organizations may be inadequate in the case of actions that are harmless when taken by humans but socially noxious when taken by AI systems that exhibit superhuman intellect, speed, or scale (Morris et al., 2024; Hammond et al., 2025). For example, large numbers of sophisticated AI agents could learn to overcome governance mechanisms designed to prevent manipulation of financial markets by human actors and organizations (Wang and Wellman, 2020). Clearly, existing laws were not generally designed to contend with micro-decisions and actions of billions of AI agents (Gabriel et al., 2024, 2025), let alone organizations and institutions comprised of such agents (Hadfield and Koh, 2025; Tomasev et al., 2025). Another problem concerns the fact that most laws are premised upon the specific capacities and constraints of humans (Simon, 1997) and developed in anticipation of only partial enforcement (Becker, 1968). Because AI systems are not necessarily subject to the same constraints as humans, absolute compliance or perfect enforcement may become practically feasible, but remain socially undesirable (Zittrain, 2008; Brownsword and Yeung, 2008). For example, an autonomous vehicle that perfectly complies with all traffic laws may disrupt established social practices that the public and lawmakers (implicitly) endorse (e.g., breaking the speed limit in a health-related emergency). Finally, many areas of existing law invoke human-centric concepts such as intent and mens rea that cannot be straightforwardly applied in the context of AI systems (Nerantzi and Sartor, 2024; Hendrycks, 2024). Tackling these challenges will require both technical work in designing legally aligned AI systems and, possibly, amendments to the law itself in response to the emergence of a new class of non-human actors and organizations.\n\nWhat interventions can support AI systems obeying both the letter and spirit of the law? AI systems might learn to comply with the formal expression of legal rules but ignore or violate their underlying purpose (Skalse et al., 2022). A failure to act in accordance with background norms, practices, and conventions could be harmful and undermine the prosocial rationale for legal alignment. One approach to resolving this issue involves designing AI systems not only to comply with the substantive content of law (O'Keefe et al., 2025), but to engage in accepted modes of legal reasoning (Caputo, 2025) or possibly adopt an 'internal point of view' (Hart, 2012; Shapiro, 2006) whereby AI systems 'accept' law as a practical standard to govern their actions, rather than simply seek to avoid legal sanctions (Austin, 1832; Holmes, 1897). This deeper engagement with law, which could be premised on recognizing the legitimacy of legal institutions and procedures (Tyler, 2006), will be critical to ensuring AI systems do not creatively skirt or abuse legal rules (Schneier, 2021), or exploit 'legal zero-days,' that is, previously undiscovered vulnerabilities in legal frameworks (Sadler and Sherburn, 2025). This approach also finds support in codes of conduct that, for example, prohibit lawyers from making frivolous or abusive claims (American Bar Association, 2020) and preclude judges from expounding absurd statutory interpretations (U.S. Supreme Court, 1868). Metarules like these could potentially be adapted to support the legal alignment of AI systems, guiding them to respect both the spirit and letter of the law.\n\nHow will the participation of AI systems in lawmaking affect legal alignment? The prospect of AI systems participating in the production of law is growing, whether through generating legal texts (Wilf-Townsend and Tobia, 2025) such as legislation (Sanders and Schneier, 2025) and even constitutions (Albert and Frazier, 2025), engaging in legal interpretation (Hoffman and Arbel, 2024; Grimmelmann et al., 2025), or rendering judicial opinions (Choi, 2025; Waldon et al., 2025). These developments pose significant challenges for legal alignment. First, law's institutional legitimacy may be undermined if legal rules and principles are no longer developed through human processes of participation and decision-making (Habermas, 1996; Pasquale, 2019). Second, law's legitimacy may be challenged if AI systems fail to fulfill procedural requirements, such as demands for transparency and public explanation, that are key to ensuring accountability and democratic responsiveness (Coglianese and Lehr, 2017). Third, to the extent AI systems shape the content of law that, in turn, governs AI systems, there could emerge a circular process in which these (artificial) subjects of law, in effect, write their own law , while lacking the legitimate authority to do so. In addition to blunting\n\nthe utility of legal alignment in retaining control over AI systems, this process could also erode or distort the rule of law. Similar phenomena can be seen in cases of regulatory capture (Dal BÃ³, 2006; Carpenter and Moss, 2013) and 'legal endogeneity,' whereby those actors that the law seeks to control end up controlling the law (Edelman, 1992, 2016). One potential response is to circumscribe the role of AI systems in lawmaking (Kleinberg et al., 2018; Engstrom and Ho, 2020) and ensure that humans retain the ability to make consequential legal decisions (Zanzotto, 2019; Crootof et al., 2023) and, where necessary, intervene in the lawmaking activities of AI systems. The effectiveness and appropriateness of this response could, however, change with the emergence of new perspectives on the role of AI in society (Salib and Goldstein, 2025a,b; Chesterman, 2025; Leibo et al., 2025).\n\n## 5.3 Tradeoffs and future outlook\n\nWill legal alignment preclude or hamper valuable AI applications? The implementation of legal alignment could prove costly and come at the expense of societally beneficial AI applications. Conducting rigorous legal alignment evaluations, intervening in the design of AI systems, and complying with associated governance frameworks could impose substantial costs on developers, deployers, and users. Such costs would comprise an 'alignment tax' (Askell et al., 2021), that is, the development of legally aligned systems would be subject to additional technical, financial, and procedural burdens relative to other systems that are not legally aligned. This perspective, however, is incomplete. For example, whether legal alignment degrades the performance of AI systems is an open empirical question. Like other alignment methods, legal alignment could potentially improve the capabilities of AI systems (Christiano et al., 2017; Ouyang et al., 2022). In particular, AI systems that understand and operate in accordance with law may be especially valuable in high-stakes domains, such as healthcare and finance (Henderson et al., 2024; Hui et al., 2025). In addition, by providing assurances that AI systems comply with the law, legal alignment could reduce the prospects of legal liability for key stakeholders, including developers, deployers, and users (Ayres and Balkin, 2024; Kolt, 2025; Williams et al., 2025b). If this were the case, legal alignment would not comprise an 'alignment tax,' but rather an 'alignment subsidy' that bolsters the performance and practical feasibility of using AI systems, especially in safety-critical applications.\n\nCould the measurement of legal alignment be gamed or exploited? While evaluating the legal compliance of AI systems in simple cases such as overt violations of law may be relatively straightforward, developing robust tests to detect more subtle instances of legal misalignment will be difficult. The problem is exacerbated by Goodhart's Law: 'when a measure becomes a target, it ceases to be a good measure' (Goodhart, 1975; Strathern, 1997). Developers seeking to improve the performance of AI systems on legal alignment benchmarks may, rather than design systems to uphold core legal principles, inadvertently steer AI systems to violate the law in hard-to-detect ways. Such systems-characterized by deceptive legal alignment -could 'hack' the law by discovering and exploiting loopholes in legal frameworks (Schneier, 2021; O'Keefe et al., 2025). This, however, would not necessarily differ substantially from lawyers' run-of-the-mill exploitation of legal loopholes to zealously advance their clients' interests (Llewellyn, 1960; Schauer, 2009). More broadly, these measurement challenges are not unique to legal alignment, but implicate many metrics designed to evaluate AI systems (Thomas and Uminsky, 2020; Skalse et al., 2022). One important mitigation in this case is to complement legal alignment benchmarks with dedicated red-teaming efforts that specifically target scenarios not captured by benchmarks, or scenarios for which benchmark results could be misleading (Feffer et al., 2024). In addition, it is possible that legally aligned AI systems could be used to 'penetration-test' and 'patch' loopholes in existing law, as well as pilot new legal mechanisms and institutions tailored to address the anticipated affordances of more advanced AI technology (CuÃ©llar and Huq, 2022).\n\nCan legal alignment scale to AGI and superintelligence? Predicting whether legal alignment will succeed in the context of uncertain and contentious future developments is necessarily speculative (Kokotajlo et al., 2025; Narayanan and Kapoor, 2025). Reasoning about the nature, timing, and impact of artificial general intelligence ('AGI') or superintelligence (Morris et al., 2024; Hendrycks et al., 2025) is fraught (Blili-Hamelin et al., 2024). Nevertheless, given the potential stakes of these developments, and the fact that AI developers and policymakers will in any event need to make choices concerning the design, deployment, and governance of advanced AI, it is important to inquire whether legal alignment will be effective in the face of systems whose capabilities broadly match or surpass those of humans. There are several reasons for optimism. First, law has a track record of governing increasingly complex activities and actors, such as multinational corporations and\n\ngovernment bureaucracies (Muchlinski, 2021; Mashaw et al., 2025). Second, legal data and methods could scale with improvements in AI such that legal alignment continues to remain technically feasible (Nay, 2022; Boeglin, 2026). Third, human-level or superhuman AI systems may help support the implementation of legal alignment, whether by constraining the reasoning and decision-making of these systems (Caputo, 2025) or protecting the underlying laws that guide their behavior (O'Keefe et al., 2025). Of course, these are hopeful predictions. Practical progress will depend on the efforts of researchers and policymakers to iteratively develop and adapt the field of legal alignment as AI systems continue to advance and transform society.\n\n## 6 Conclusion\n\nLaw offers an underexplored set of rules, principles, and methods for designing safe and ethical AI. Drawing on the institutional legitimacy of law in democratic societies, legal alignment describes a range of roles that legal rules and structures can play in reshaping the design of AI systems to address growing safety and governance concerns. While legal alignment is not a catch-all solution for the many challenges arising from AI, it is both independently important and supportive of complementary alignment research programs. To guide the emerging field of legal alignment, we outline several core areas of focus: using the content of legal rules and principles to steer the behavior of AI systems, leveraging methods of legal reasoning and interpretation to constrain how AI systems make decisions, and harnessing time-tested legal concepts as structural blueprints for tackling problems of alignment. Each of these areas presents new conceptual questions, empirical challenges, and opportunities for technical and institutional innovation. As legal scholars, computer scientists, and researchers spanning multiple disciplines, we look forward to collaborating on this ambitious and pressing agenda.\n\n## Acknowledgements\n\nFor helpful comments and suggestions, we thank Doni Bloomfield, Alan Chan, David Duvenaud, Neel Guha, Martha Minow, Tim Rudner, Tan Zhi Xuan, and participants in the Inaugural Roundtable on AI Safety Law at the University of Alabama Law School and the Sociotechnical AI Safety Retreat at the Australian National University. The Hebrew University Governance of AI Lab and this research are supported by the Israel Science Foundation (Grant No. 487/25), Survival and Flourishing Fund, and Coefficient Giving.\n\n## References\n\n- Abbott, R. (2020). The reasonable robot: artificial intelligence and the law . Cambridge University Press.\n\nAbiri, G. (2025). Public constitutional AI. Georgia Law Review , 59:601-670.\n\nAcemoglu, D. and Robinson, J. A. (2012). Why Nations Fail . Crown Currency.\n\nAdministrative Procedure Act (1946). Administrative Procedure Act. P.L.79-404 60 Stat. 237.\n\n- Aguirre, A., Dempsey, G., Surden, H., and Reiner, P. B. (2020). AI loyalty: a new paradigm for aligning stakeholder interests. IEEE Transactions on Technology and Society , 1(3):128-137.\n- Ahmed, A., Klyman, K., Zeng, Y., Koyejo, S., and Liang, P. (2025). SpecEval: Evaluating model adherence to behavior specifications. arXiv preprint arXiv:2509.02464 .\n\nAlbert, R. and Frazier, K. (2025). Should AI write your constitution?\n\nAmerican Bar Association (2020). Model rules of professional conduct.\n\nAmerican Law Institute (1962). Model penal code.\n\nAmerican Law Institute (2006). Restatement (Third) of Agency .\n\nAnthropic (2023). Claude's constitution.\n\n- Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper, S., Sourbut, O., et al. (2024). Foundational challenges in assuring alignment and safety of large language models. Transactions on Machine Learning Research .\n\n- Arbel, Y., Tokson, M., and Lin, A. (2024). Systemic regulation of artificial intelligence. Arizona State Law Journal , 56(2):545-619.\n- Arendt, H. (1963). Eichmann in Jerusalem: A Report on the Banality of Evil . Viking Press.\n- Armour, J., Hansmann, H., and Kraakman, R. (2017). Agency problems, legal strategies and enforcement. In The Anatomy of Corporate Law: A Comparative and Functional Approach . Oxford University Press.\n- Asimov, I. (1942). Runaround. Astounding Science Fiction , 29(1):94-103.\n- Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et al. (2021). A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861 .\n- Atiyah, P. S. (1992). Justice and predictability in the common law. Law Journal\n- University of New South Wales , 15(2):448-465. 7th Wallace Wurth Memorial Lecture.\n- Austin, J. (1832). The Province of Jurisprudence Determined . John Murray.\n- Ayres, I. and Balkin, J. M. (2024). The law of AI is the law of risky agents without intentions. University of Chicago Law Review Online .\n- Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022a). Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 .\n- Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. (2022b). Constitutional AI: harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 .\n- Bajgar, O. and Horenovsky, J. (2023). Negative human rights as a basis for long-term AI safety and regulation. Journal of Artificial Intelligence Research , 76:1043-1075.\n- Bakker, M., Chadwick, M., Sheahan, H., Tessler, M., Campbell-Gillingham, L., Balaguer, J., McAleese, N., Glaese, A., Aslanides, J., Botvinick, M., et al. (2022). Fine-tuning language models to find agreement among humans with diverse preferences. Advances in Neural Information Processing Systems , 35:38176-38189.\n- Barak, A. (2011). Purposive interpretation in law . Princeton University Press.\n- Barez, F., Friend, I., Reid, K., Krawczuk, I., Wang, V ., MÃ¶kander, J., Torr, P., Morse, J., and Trager, R. (2025a). Toward resisting AI-enabled authoritarianism. Oxford Martin AI Governance Initiative.\n- Barez, F., Wu, T.-Y., Arcuschin, I., Lan, M., Wang, V., Siegel, N., Collignon, N., Neo, C., Lee, I., Paren, A., Bibi, A., Trager, R., Fornasiere, D., Yan, J., Elazar, Y., and Bengio, Y. (2025b). Chain-of-thought is not explainability. Oxford Martin AI Governance Initiative.\n- Becker, G. S. (1968). Crime and punishment: An economic approach. Journal of Political Economy , 76:169-217.\n- Bench-Capon, T., Araszkiewicz, M., Ashley, K., Atkinson, K., Bex, F., Borges, F., Bourcier, D., Bourgine, P., Conrad, J. G., Francesconi, E., et al. (2012). A history of AI and law in 50 papers: 25 years of the international conference on AI and law. Artificial Intelligence and Law , 20(3):215-319.\n- Bengio, Y., Hinton, G., Yao, A., Song, D., Abbeel, P., Darrell, T., Harari, Y. N., Zhang, Y.-Q., Xue, L., Shalev-Shwartz, S., et al. (2024). Managing extreme AI risks amid rapid progress. Science , 384(6698):842-845.\n- Bengio, Y., Mindermann, S., Privitera, D., Besiroglu, T., Bommasani, R., Casper, S., Choi, Y., Fox, P., Garfinkel, B., Goldfarb, D., et al. (2025). International AI safety report. arXiv preprint arXiv:2501.17805 .\n- Benkler, Y. (2002). Coase's penguin, or, linux and the nature of the firm. Yale Law Journal , 112(3):369-446.\n\n- Benthall, S. and Shekman, D. (2023). Designing fiduciary artificial intelligence. In Proceedings of the 3rd ACM conference on equity and access in algorithms, mechanisms, and optimization , pages 1-15.\n- Bingham, L. (2007). The rule of law. The Cambridge Law Journal , 66(1):67-85.\n- Birhane, A., Isaac, W., Prabhakaran, V., Diaz, M., Elish, M. C., Gabriel, I., and Mohamed, S. (2022). Power to the people? opportunities and challenges for participatory AI. In Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization , pages 1-8.\n- Blili-Hamelin, B., Graziul, C., Hancox-Li, L., Hazan, H., El-Mhamdi, E.-M., Ghosh, A., Heller, K. A., Metcalf, J., Murai, F., Salvaggio, E., et al. (2024). Position: Stop treating \"AGI\" as the north-star goal of AI research. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Boeglin, J. (2026). Aligning artificial intelligence to the law. Villanova Law Review (forthcoming) .\n- Bommasani, R., Arora, S., Chayes, J., Choi, Y., CuÃ©llar, M.-F., Fei-Fei, L., Ho, D. E., Jurafsky, D., Koyejo, S., Lakkaraju, H., Narayanan, A., Nelson, A., Pierson, E., Pineau, J., Singer, S., Varoquaux, G., Venkatasubramanian, S., Stoica, I., Liang, P., and Song, D. (2025). Advancing science- and evidence-based AI policy. Science , 389(6759):459-461.\n- Bommasani, R., Klyman, K., Longpre, S., Kapoor, S., Maslej, N., Xiong, B., Zhang, D., and Liang, P. (2023). The foundation model transparency index. arXiv preprint arXiv:2310.12941 .\n- Bovens, M. (2007). Analysing and assessing public accountability: A conceptual framework. European Law Journal , 13(4):447-468.\n- Bowman, S. R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., LukoÅ¡iÂ¯ utË™ e, K., Askell, A., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Olah, C., Amodei, D., Amodei, D., Drain, D., Li, D., Tran-Johnson, E., Kernion, J., Kerr, J., Mueller, J., Ladish, J., Landau, J., Ndousse, K., Lovitt, L., Elhage, N., Schiefer, N., Joseph, N., Mercado, N., DasSarma, N., Larson, R., McCandlish, S., Kundu, S., Johnston, S., Kravec, S., Showk, S. E., Fort, S., Telleen-Lawton, T., Brown, T., Henighan, T., Hume, T., Bai, Y., Hatfield-Dodds, Z., Mann, B., and Kaplan, J. (2022). Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540 .\n- Brewer, S. (1996). Exemplary reasoning: Semantics, pragmatics, and the rational force of legal argument by analogy. Harvard Law Review , 109(5):923-1028.\n- Briggs, A. (2024). The conflict of laws . Oxford University Press, 5th edition.\n- Brownsword, R. (2025). Generative AI and the rule of law. In The Oxford Handbook of the Foundations and Regulation of Generative AI . Oxford University Press.\n- Brownsword, R. and Yeung, K. (2008). Regulating technologies: legal futures, regulatory frames and technological fixes . Hart Publishing.\n- Brynjolfsson, E., Li, D., and Raymond, L. (2025). Generative AI at work. The Quarterly Journal of Economics , 140(2):889-942.\n- Buhl, M. D., Sett, G., Koessler, L., Schuett, J., and Anderljung, M. (2024). Safety cases for frontier AI. arXiv preprint arXiv:2410.21572 .\n- Buyl, M., Khalaf, H., Mayrink Verdun, C., Monteiro Paes, L., Vieira Machado, C. C., and du Pin Calmon, F. (2025). AI alignment at your discretion. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency , pages 3046-3074.\n- California Legislature (2025). Transparency in frontier artificial intelligence act, SB-53.\n- Calo, R. (2015). Robotics and the lessons of cyberlaw. California Law Review , 103:513-564.\n- Calvano, E., Calzolari, G., Denicolo, V., and Pastorello, S. (2020). Artificial intelligence, algorithmic pricing, and collusion. American Economic Review , 110(10):3267-3297.\n- Cao, C., Zhu, H., Ji, J., Sun, Q., Zhu, Z., Yinyu, W., Dai, J., Yang, Y ., Han, S., and Guo, Y. (2025). Safelawbench: Towards safe alignment of large language models. In Findings of the Association for Computational Linguistics: ACL 2025 , pages 14015-14048.\n\n- Caputo, N. (2025). Alignment as jurisprudence. Yale Journal of Law &amp; Technology , 27:390-473.\n- Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-V oss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et al. (2021). Extracting training data from large language models. In 30th USENIX security symposium (USENIX Security 21) , pages 2633-2650.\n- Carpenter, D. and Moss, D. A. (2013). Preventing regulatory capture: Special interest influence and how to limit it . Cambridge University Press.\n- Carroll, M., Chan, A., Ashton, H., and Krueger, D. (2023). Characterizing manipulation from AI systems. In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization , pages 1-13.\n- Casper, S., Bailey, L., Hunter, R., Ezell, C., CabalÃ©, E., Gerovitch, M., Slocum, S., Wei, K., Jurkovic, N., Khan, A., et al. (2025a). The AI agent index. arXiv preprint arXiv:2502.01635 .\n- Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et al. (2023). Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research .\n- Casper, S., Krueger, D., and Hadfield-Menell, D. (2025b). Pitfalls of evidence-based AI policy. arXiv preprint arXiv:2502.09618 .\n- Chalkidis, I., Jana, A., Hartung, D., Bommarito, M., Androutsopoulos, I., Katz, D., and Aletras, N. (2022). Lexglue: A benchmark dataset for legal language understanding in english. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 4310-4330.\n- Chan, A., Ezell, C., Kaufmann, M., Wei, K., Hammond, L., Bradley, H., Bluemke, E., Rajkumar, N., Krueger, D., Kolt, N., et al. (2024a). Visibility into AI agents. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency , pages 958-973.\n- Chan, A., Kolt, N., Wills, P., Anwar, U., de Witt, C. S., Rajkumar, N., Hammond, L., Krueger, D., Heim, L., and Anderljung, M. (2024b). IDs for AI systems. arXiv preprint arXiv:2406.12137 .\n- Chan, A., Salganik, R., Markelius, A., Pang, C., Rajkumar, N., Krasheninnikov, D., Langosco, L., He, Z., Duan, Y., Carroll, M., et al. (2023). Harms from increasingly agentic algorithmic systems. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency , pages 651-666.\n- Chao, P., Debenedetti, E., Robey, A., Andriushchenko, M., Croce, F., Sehwag, V., Dobriban, E., Flammarion, N., Pappas, G. J., Tramer, F., et al. (2024). Jailbreakbench: An open robustness benchmark for jailbreaking large language models. Advances in Neural Information Processing Systems , 37:55005-55029.\n- Chen, Q. Z. and Zhang, A. X. (2025). Case law grounding: Using precedents to align decision-making for humans and AI. In Proceedings of the ACM Collective Intelligence Conference , pages 226-238.\n- Cheng, M., Lee, C., Khadpe, P., Yu, S., Han, D., and Jurafsky, D. (2025). Sycophantic AI decreases prosocial intentions and promotes dependence. arXiv preprint arXiv:2510.01395 .\n- Chesterman, S. (2020). Artificial intelligence and the limits of legal personality. International &amp; Comparative Law Quarterly , 69(4):819-844.\n- Chesterman, S. (2025). From slaves to synths? superintelligence and the evolution of legal personality.\n- Choi, J. H. (2025). Large language models are unreliable legal interpreters.\n- Chopra, S. and White, L. F. (2011). A legal theory for autonomous artificial agents . University of Michigan Press.\n- Christian, B. (2020). The Alignment Problem: Machine Learning and Human Values . W. W. Norton &amp;Co.\n- Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems .\n\n- Citron, D. K. (2008). Technological due process. Washington University Law Review , 85(6):12491313.\n- Clymer, J., Gabrieli, N., Krueger, D., and Larsen, T. (2024). Safety cases: How to justify the safety of advanced AI systems. arXiv preprint arXiv:2403.10462 .\n- Coglianese, C. and Lehr, D. (2017). Regulating by robot: Administrative decision making in the machine-learning era. Georgetown Law Journal , 105:1147-1223.\n- Collins, L. and Harris, J. (2025). Dicey, Morris &amp; Collins on the Conflict of Laws . Sweet &amp; Maxwell, 16th edition.\n- Conitzer, V., Freedman, R., Heitzig, J., Holliday, W. H., Jacobs, B. M., Lambert, N., MossÃ©, M., Pacuit, E., Russell, S., Schoelkopf, H., et al. (2024). Position: social choice should guide AI alignment in dealing with diverse human feedback. In Proceedings of the 41st International Conference on Machine Learning , pages 9346-9360.\n- Cooper, A. F., Frankle, J., and De Sa, C. (2022a). Non-determinism and the lawlessness of machine learning code. In Proceedings of the ACM 2022 Symposium on Computer Science and Law , CSLAW '22, page 1-8.\n- Cooper, A. F., Moss, E., Laufer, B., and Nissenbaum, H. (2022b). Accountability in an algorithmic society: relationality, responsibility, and robustness in machine learning. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency , pages 864-876.\n- Cover, R. M. (1975). Justice Accused: Antislavery and the Judicial Process . Yale University Press.\n- Crootof, R., Kaminski, M. E., Price, W., and Nicholson, I. (2023). Humans in the loop. Vanderbilt Law Review , 76:429-510.\n- CuÃ©llar, M.-F. (2019). A common law for the age of artificial intelligence. Columbia Law Review , 119(7):1773-1792.\n- CuÃ©llar, M.-F. and Huq, A. Z. (2022). Artificially intelligent regulation. Daedalus , 151(2):335-347.\n- Dafoe, A., Bachrach, Y., Hadfield, G., Horvitz, E., Larson, K., and Graepel, T. (2021). Cooperative AI: machines must learn to find common ground. Nature , 593(7857):33-36.\n- Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., McKee, K. R., Leibo, J. Z., Larson, K., and Graepel, T. (2020). Open problems in cooperative AI. arXiv preprint arXiv:2012.08630 .\n- Dal BÃ³, E. (2006). Regulatory capture: A review. Oxford Review of Economic Policy , 22(2):203-225.\n- Davidson, T., Finnveden, L., and Hadshar, R. (2025). AI-enabled coups: How a small group could use AI to seize power. Forethought.\n- Deeks, A. (2019). The judicial demand for explainable artificial intelligence. Columbia Law Review , 119(7):1829-1850.\n- Desai, D. R. and Riedl, M. (2025). Responsible AI agents.\n- Dicey, A. V. (1959). Introduction to the Study of the Law of the Constitution . Macmillan, 10th edition.\n- Dobbe, R., Gilbert, T. K., and Mintz, Y. (2021). Hard choices in artificial intelligence. Artificial Intelligence , 300:103555.\n- Douek, E. (2022). Content moderation as systems thinking. Harvard Law Review , 136:526-607.\n- Doyle, C. and Tucker, A. D. (2025). If you give an LLM a legal practice guide. In Proceedings of the 2025 Symposium on Computer Science and Law , pages 194-205.\n- Dworkin, R. (1986). Law's Empire . Belknap Press, Harvard University Press.\n- Dyzenhaus, D. (2010). Hard Cases in Wicked Legal Systems: Pathologies of Legality . Oxford University Press, 2nd edition.\n- Edelman, L. B. (1992). Legal ambiguity and symbolic structures: Organizational mediation of civil rights law. American Journal of Sociology , 97(6):1531-1576.\n\n- Edelman, L. B. (2016). Working Law: Courts, Corporations, and Symbolic Civil Rights . University of Chicago Press.\n- Edmundson, W. A. (1998). Legitimate authority without political obligation. Law &amp; Philosophy , 17:43.\n- El, B. and Zou, J. (2025). Moloch's bargain: Emergent misalignment when LLMs compete for audiences. arXiv preprint arXiv:2510.06105 .\n- Eloundou, T., Gordon, M., Zhang, E., and Agarwal, S. (2025). Collective alignment: public input on our model spec.\n- Eloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024). GPTs are GPTs: Labor market impact potential of LLMs. Science , 384(6702):1306-1308.\n- Ely, J. H. (1980). Democracy and Distrust: A Theory of Judicial Review . Harvard University Press.\n- Engler, A. (2023). The EU and U.S. diverge on AI regulation: A transatlantic comparison and steps to alignment. Brookings Institution.\n- Engstrom, D. F. and Ho, D. E. (2020). Algorithmic accountability in the administrative state. Yale Journal on Regulation , 37(3):800-854.\n- Etzioni, A. and Etzioni, O. (2016a). Designing AI systems that obey our laws and values. Communications of the ACM , 59(9):29-31.\n- Etzioni, A. and Etzioni, O. (2016b). Keeping AI legal. Vanderbilt Journal of Entertainment &amp; Technology Law , 19:133.\n- European Parliament (2024). Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA relevance) . Legislative Body: CONSIL, EP.\n- Fallon, R. H. (1994). Reflections on the hart and wechsler paradigm. Vanderbilt Law Review , 47:953-991.\n- Fan, Y., Ni, J., Merane, J., Tian, Y ., HermstrÃ¼wer, Y ., Huang, Y ., Akhtar, M., Salimbeni, E., Geering, F., Dreyer, O., et al. (2025). Lexam: Benchmarking legal reasoning on 340 law exams. arXiv preprint arXiv:2505.12864 .\n- Feffer, M., Sinha, A., Deng, W. H., Lipton, Z. C., and Heidari, H. (2024). Red-teaming for generative AI: silver bullet or security theater? In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , volume 7, pages 421-437.\n- Feng, K., Chen, Q. Z., Cheong, I., Xia, K., and Zhang, A. X. (2023). Case repositories: Towards case-based reasoning for AI alignment. arXiv preprint arXiv:2311.10934 .\n- Findeis, A., Kaufmann, T., HÃ¼llermeier, E., Albanie, S., and Mullins, R. D. (2024). Inverse constitutional AI: Compressing preferences into principles. In The Thirteenth International Conference on Learning Representations .\n- Finnis, J. (1980). Natural Law and Natural Rights . Clarendon Press, Oxford University Press.\n- Forrest, K. B. (2024). The ethics and challenges of legal personhood for AI. Yale Law Journal Forum , 133:1175-1211.\n- Friendly, H. J. (1975). Some kind of hearing. University of Pennsylvania Law Review , 123:1267-1317.\n- Fuller, L. L. (1957). Positivism and fidelity to law-a reply to professor hart. Harvard Law Review , 71:630.\n- Fuller, L. L. (1969). The Morality of Law . Yale University Press.\n- Gabriel, I. (2020). Artificial intelligence, values, and alignment. Minds and Machines , 30(3):411-437.\n- Gabriel, I. and Keeling, G. (2025). A matter of principle? AI alignment as the fair treatment of claims. Philosophical Studies , 182:1951-1973.\n\n- Gabriel, I., Keeling, G., Manzini, A., and Evans, J. (2025). We need a new ethics for a world of AI agents. Nature , 644(8075):38-40.\n- Gabriel, I., Manzini, A., Keeling, G., Hendricks, L. A., Rieser, V., Iqbal, H., TomaÅ¡ev, N., Ktena, I., Kenton, Z., Rodriguez, M., et al. (2024). The ethics of advanced AI assistants. arXiv preprint arXiv:2404.16244 .\n- Gallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., and Ahmed, N. K. (2024). Bias and fairness in large language models: A survey. Computational Linguistics , 50(3):1097-1179.\n- Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y ., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. (2022). Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 .\n- Gardner, A. v. d. L. (1987). An artificial intelligence approach to legal reasoning . MIT Press.\n- Ghosh, S., Frase, H., Williams, A., Luger, S., RÃ¶ttger, P., Barez, F., McGregor, S., Fricklas, K., Kumar, M., Bollacker, K., et al. (2025). AILuminate: Introducing v1. 0 of the AI risk and reliability benchmark from MLCommons. arXiv preprint arXiv:2503.05731 .\n- Gillespie, T. (2018). Custodians of the Internet: platforms, content moderation, and the hidden decisions that shape social media . Yale University Press.\n- Goldsmith, J. L. and Wu, T. (2006). Who Controls the Internet?: Illusions of a Borderless World . Oxford University Press.\n- Goodhart, C. (1975). Problems of monetary management: the UK experience in papers in monetary economics. Papers in Monetary Economics .\n- Google (2025a). Generative AI - prohibited use policy.\n- Google (2025b). Safety and content filters.\n- GÃ¶tting, J., Medeiros, P., Sanders, J. G., Li, N., Phan, L., Elabd, K., Justen, L., Hendrycks, D., and Donoughe, S. (2025). Virology capabilities test (VCT): a multimodal virology Q&amp;A benchmark. arXiv preprint arXiv: 2504.16137 .\n- Greenblatt, R., Denison, C., Wright, B., Roger, F., MacDiarmid, M., Marks, S., Treutlein, J., Belonax, T., Chen, J., Duvenaud, D., Khan, A., Michael, J., Mindermann, S., Perez, E., Petrini, L., Uesato, J., Kaplan, J., Shlegeris, B., Bowman, S. R., and Hubinger, E. (2024). Alignment faking in large language models. arXiv preprint arXiv:2412.14093 .\n- Grimmelmann, J., Sobel, B., and Stein, D. (2025). Generative misinterpretation. Harvard Journal on Legislation (forthcoming) .\n- Guan, M. Y., Joglekar, M., Wallace, E., Jain, S., Barak, B., Helyar, A., Dias, R., Vallone, A., Ren, H., Wei, J., et al. (2024). Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339 .\n- Guha, N., Lawrence, C. M., Gailmard, L. A., Rodolfa, K. T., Surani, F., Bommasani, R., Raji, I. D., CuÃ©llar, M.-F., Honigsberg, C., Liang, P., et al. (2024). AI regulation has its own alignment problem: The technical and institutional feasibility of disclosure, registration, licensing, and auditing. George Washington Law Review , 92:1473-1557.\n- Guha, N., Nyarko, J., Ho, D., RÃ©, C., Chilton, A., Chohlas-Wood, A., Peters, A., Waldon, B., Rockmore, D., Zambrano, D., et al. (2023). LegalBench: A collaboratively built benchmark for measuring legal reasoning in large language models. Advances in Neural Information Processing Systems , 36:44123-44279.\n- GyevnÃ¡r, B. and Kasirzadeh, A. (2025). AI safety for everyone. Nature Machine Intelligence , 7:531-542.\n- Habermas, J. (1996). Between Facts and Norms: Contributions to a Discourse Theory of Law and Democracy . MIT Press.\n\n- Hackenburg, K., Tappin, B. M., Hewitt, L., Saunders, E., Black, S., Lin, H., Fist, C., Margetts, H., Rand, D. G., and Summerfield, C. (2025). The levers of political persuasion with conversational artificial intelligence. Science , 390(6777):eaea3884.\n- Hacker, P., Kasirzadeh, A., and Edwards, L. (2025). AI, digital platforms, and the new systemic risk. arXiv preprint arXiv:2509.17878 .\n- Hadfield, G., CuÃ©llar, M.-F. T., and O'Reilly, T. (2023). It's time to create a national registry for large AI models. Carnegie Endowment for International Peace.\n- Hadfield, G. K. (2021). Explanation and justification: AI decision-making, law, and the rights of citizens. Schwartz Reisman Institute for Technology and Society.\n- Hadfield, G. K. (2026). Can AI be governed? only if we build normatively competent AI. In Nyholm, S., Kasirzadeh, A., and Zerilli, J., editors, Contemporary Debates in the Ethics of Artificial Intelligence . Wiley.\n- Hadfield, G. K. and Clark, J. (2023). Regulatory markets: The future of AI governance. arXiv preprint arXiv:2304.04914 .\n- Hadfield, G. K. and Koh, A. (2025). An economy of AI agents. arXiv preprint arXiv:2509.01063 .\n- Hadfield, G. K. and Weingast, B. R. (2012). What is law? a coordination model of the characteristics of legal order. Journal of Legal Analysis , 4:471-514.\n- Hadfield, G. K. and Weingast, B. R. (2014). Microfoundations of the rule of law. Annual Review of Political Science , 17(1):21-42.\n- Hadfield-Menell, D. and Hadfield, G. (2018). Incomplete contracting and AI alignment. arXiv preprint arXiv:1804.04268 .\n- Hammond, L., Chan, A., Clifton, J., Hoelscher-Obermaier, J., Khan, A., McLean, E., Smith, C., Barfuss, W., Foerster, J., GavenË‡ ciak, T., Han, T. A., Hughes, E., KovaË‡ rÃ­k, V ., Kulveit, J., Leibo, J. Z., Oesterheld, C., de Witt, C. S., Shah, N., Wellman, M., Bova, P., Cimpeanu, T., Ezell, C., Feuillade-Montixi, Q., Franklin, M., Kran, E., Krawczuk, I., Lamparth, M., Lauffer, N., Meinke, A., Motwani, S., Reuel, A., Conitzer, V., Dennis, M., Gabriel, I., Gleave, A., Hadfield, G., Haghtalab, N., Kasirzadeh, A., Krier, S., Larson, K., Lehman, J., Parkes, D. C., Piliouras, G., and Rahwan, I. (2025). Multi-agent risks from advanced AI. arXiv preprint arXiv:2502.14143 .\n- Han, S. S., Takashima, Y., Shen, S. Z., Liu, C., Liu, Y ., Thuo, R. K., Knowlton, S., Piskac, R., Shapiro, S. J., and Cohan, A. (2025). Courtreasoner: Can LLM agents reason like judges? In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing , pages 35279-35294.\n- Hart, H. L. A. (1958). Positivism and the separation of law and morals. Harvard Law Review , 71:593-629.\n- Hart, H. L. A. (1963). Law, liberty, and morality . Stanford University Press.\n- Hart, H. L. A. (1982). Commands and authoritative legal reasons. In Essays on Bentham: Jurisprudence and Political Philosophy . Oxford University Press.\n- Hart, H. L. A. (2012). The Concept of Law . Oxford University Press, 3rd edition.\n- He, L., Nadeem, N., Liao, M., Chen, H., Chen, D., CuÃ©llar, M.-F., and Henderson, P. (2025). Statutory construction and interpretation for artificial intelligence. arXiv preprint arXiv:2509.01186 .\n- Henderson, P., Hashimoto, T., and Lemley, M. (2023). Where's the liability in harmful AI speech? Journal of Free Speech Law , 3:589-650.\n- Henderson, P., Hu, J., Diab, M., and Pineau, J. (2024). Rethinking machine learning benchmarks in the context of professional codes of conduct. In Proceedings of the 2024 Symposium on Computer Science and Law , pages 109-120.\n- Henderson, P., Krass, M., Zheng, L., Guha, N., Manning, C. D., Jurafsky, D., and Ho, D. (2022). Pile of law: Learning responsible data filtering from the law and a 256GB open-source legal dataset. Advances in Neural Information Processing Systems , 35:29217-29234.\n- Hendrycks, D. (2024). Introduction to AI Safety, Ethics, and Society . CRC Press.\n\n- Hendrycks, D., Song, D., Szegedy, C., Lee, H., Gal, Y., Brynjolfsson, E., Li, S., Zou, A., Levine, L., Han, B., et al. (2025). A definition of AGI. arXiv preprint arXiv:2510.18212 .\n- Hilton, B., Buhl, M. D., Korbak, T., and Irving, G. (2025). Safety cases: A scalable approach to frontier AI safety. arXiv preprint arXiv:2503.04744 .\n- Hoffman, D. A. and Arbel, Y. (2024). Generative interpretation. New York University Law Review , 99(2):451-514.\n- Holmes, Jr., O. W. (1881). The Common Law . Little, Brown, &amp; Co.\n- Holmes, Jr., O. W. (1897). The path of the law. Harvard Law Review , 10(8):457-478.\n- Hu, W., Jing, H., Shi, H., Li, H., and Song, Y. (2025). Safety compliance: Rethinking LLM safety reasoning through the lens of compliance. arXiv preprint arXiv:2509.22250 .\n- Huang, S., Siddarth, D., Lovitt, L., Liao, T. I., Durmus, E., Tamkin, A., and Ganguli, D. (2024). Collective constitutional AI: aligning a language model with public input. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency , pages 1395-1417.\n- Hui, Z., Dong, Y. R., Shareghi, E., and Collier, N. (2025). Trident: Benchmarking LLM safety in finance, medicine, and law. arXiv preprint arXiv:2507.21134 .\n- Huq, A. Z. (2024). Artificial intelligence and the rule of law. In Routledge Handbook of the Rule of Law , pages 260-272. Routledge.\n- Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. (2023). Survey of hallucination in natural language generation. ACM computing surveys , 55(12):1-38.\n- Johnson, D. R. and Post, D. (1996). Law and borders: The rise of law in cyberspace. Stanford Law Review , 48(5):1367-1402.\n- Kaminski, M. E. (2023). Regulating the risks of AI. Boston University Law Review , 103:1347-1411.\n- Kaminski, M. E. and Selbst, A. D. (2025). An american's guide to the EU AI Act.\n- Kapoor, S., Kolt, N., and Lazar, S. (2025a). Position: Build agent advocates, not platform agents. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Kapoor, S., Stroebl, B., Kirgis, P., Nadgir, N., Siegel, Z. S., Wei, B., Xue, T., Chen, Z., Chen, F., Utpala, S., Ndzomga, F., Oruganty, D., Luskin, S., Liu, K., Yu, B., Arora, A., Hahm, D., Trivedi, H., Sun, H., Lee, J., Jin, T., Mai, Y., Zhou, Y., Zhu, Y., Bommasani, R., Kang, D., Song, D., Henderson, P., Su, Y., Liang, P., and Narayanan, A. (2025b). Holistic agent leaderboard: The missing infrastructure for AI agent evaluation. arXiv preprint arXiv:2510.11977 .\n- Kapoor, S., Stroebl, B., Siegel, Z. S., Nadgir, N., and Narayanan, A. (2024). AI agents that matter. Transactions on Machine Learning Research .\n- Kasirzadeh, A. (2025). Two types of AI existential risk: decisive and accumulative. Philosophical Studies , (7):1975-2003.\n- Kasirzadeh, A. (2026). The many faces of AI alignment. In Nyholm, S., Kasirzadeh, A., and Zerilli, J., editors, Contemporary Debates in the Ethics of Artificial Intelligence . Wiley.\n- Kasirzadeh, A. and Gabriel, I. (2023). In conversation with artificial intelligence: aligning language models with human values. Philosophy &amp; Technology , 36(2):27.\n- Kennedy, D. (1991). The stakes of law, or hale and foucault. Legal Studies Forum , 15:327.\n- Khan, A., Casper, S., and Hadfield-Menell, D. (2025). Randomness, not representation: The unreliability of evaluating cultural alignment in LLMs. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency , pages 2151-2165.\n- Kilov, D., Hendy, C., Guyot, S. Y., Snoswell, A. J., and Lazar, S. (2025). Discerning what matters: A multi-dimensional assessment of moral competence in LLMs. arXiv preprint arXiv:2506.13082 .\n- King, M. L. J. (1963). Letter from Birmingham Jail.\n\n- King, T. C., Aggarwal, N., Taddeo, M., and Floridi, L. (2020). Artificial intelligence crime: An interdisciplinary analysis of foreseeable threats and solutions. Science and engineering ethics , 26(1):89-120.\n- Kirilenko, A., Kyle, A. S., Samadi, M., and Tuzun, T. (2017). The flash crash: High-frequency trading in an electronic market. The Journal of Finance , 72(3):967-998.\n- Kirk, H. R., Gabriel, I., Summerfield, C., Vidgen, B., and Hale, S. A. (2025). Why human-AI relationships need socioaffective alignment. Humanities and Social Sciences Communications , 12(1):1-9.\n- Kirk, H. R., Whitefield, A., Rottger, P., Bean, A. M., Margatina, K., Mosquera-Gomez, R., Ciro, J., Bartolo, M., Williams, A., He, H., et al. (2024). The PRISM alignment dataset: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. Advances in Neural Information Processing Systems , 37:105236-105344.\n- Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J., and Mullainathan, S. (2018). Human decisions and machine predictions. The Quarterly Journal of Economics , 133(1):237-293.\n- Klingefjord, O., Lowe, R., and Edelman, J. (2024). What are human values, and how do we align AI to them? arXiv preprint arXiv:2404.10636 .\n\nKokotajlo, D., Alexander, S., Larsen, T., Lifland, E., and Dean, R. (2025). AI 2027.\n\nKolt, N. (2022). Predicting consumer contracts. Berkeley Technology Law Journal , 37:71-138.\n\nKolt, N. (2024). Algorithmic black swans. Washington University Law Review , 101:1177-1240.\n\nKolt, N. (2025). Governing AI agents. Notre Dame Law Review (forthcoming) .\n\n- Kolt, N., Anderljung, M., Barnhart, J., Brass, A., Esvelt, K., Hadfield, G. K., Heim, L., Rodriguez, M., Sandbrink, J. B., and Woodside, T. (2024). Responsible reporting for frontier AI development. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , volume 7, pages 768-783.\n- Korbak, T., Balesni, M., Barnes, E., Bengio, Y., Benton, J., Bloom, J., Chen, M., Cooney, A., Dafoe, A., Dragan, A., Emmons, S., Evans, O., Farhi, D., Greenblatt, R., Hendrycks, D., Hobbhahn, M., Hubinger, E., Irving, G., Jenner, E., Kokotajlo, D., Krakovna, V., Legg, S., Lindner, D., Luan, D., MË› adry, A., Michael, J., Nanda, N., Orr, D., Pachocki, J., Perez, E., Phuong, M., Roger, F., Saxe, J., Shlegeris, B., Soto, M., Steinberger, E., Wang, J., Zaremba, W., Baker, B., Shah, R., and Mikulik, V. (2025). Chain of thought monitorability: A new and fragile opportunity for AI safety. arXiv preprint arXiv:2507.11473 .\n- Korinek, A. and Balwit, A. (2022). Aligned with whom? direct and social goals for AI systems. NBER working paper .\n- Kulveit, J., Douglas, R., Ammann, N., Turan, D., Krueger, D., and Duvenaud, D. (2025). Position: Humanity faces existential risk from gradual disempowerment. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Kundu, S., Bai, Y., Kadavath, S., Askell, A., Callahan, A., Chen, A., Goldie, A., Balwit, A., Mirhoseini, A., McLean, B., Olsson, C., Evraets, C., Tran-Johnson, E., Durmus, E., Perez, E., Kernion, J., Kerr, J., Ndousse, K., Nguyen, K., Elhage, N., Cheng, N., Schiefer, N., DasSarma, N., Rausch, O., Larson, R., Yang, S., Kravec, S., Telleen-Lawton, T., Liao, T. I., Henighan, T., Hume, T., Hatfield-Dodds, Z., Mindermann, S., Joseph, N., McCandlish, S., and Kaplan, J. (2023). Specific versus general principles for constitutional AI. arXiv preprint arXiv:2310.13798 .\n- Kwa, T., West, B., Becker, J., Deng, A., Garcia, K., Hasin, M., Jawhar, S., Kinniment, M., Rush, N., Von Arx, S., et al. (2025). Measuring AI ability to complete long tasks. arXiv preprint arXiv:2503.14499 .\n- Ladenson, R. F. (1972). Legitimate authority. American Philosophical Quarterly , 9(4):335-341.\n- Lambert, N. (2025). Character training: Understanding and crafting a language model's personality. Interconnects.\n\n- Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y ., Malik, S., Graf, V ., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. (2024). Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124 .\n- Lambert, N., Pyatkin, V., Morrison, J., Miranda, L. J. V., Lin, B. Y ., Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi, Y., et al. (2025). Rewardbench: Evaluating reward models for language modeling. In Findings of the Association for Computational Linguistics: NAACL 2025 , pages 1755-1797.\n- Lazar, S. (2024). Legitimacy, authority, and democratic duties of explanation. In Oxford Studies in Political Philosophy Volume 10 . Oxford University Press.\n- Lazar, S. (2025). Governing the algorithmic city. Philosophy &amp; Public Affairs , 53(2):102-168.\n- Lazar, S. and CuÃ©llar, M.-F. (2025). AI agents and democratic resilience. Knight First Amendment Institute.\n- Lazar, S. and Nelson, A. (2023). AI safety on whose terms? Science , 381(6654):138.\n- Lee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J., Lu, K. R., Bishop, C., Hall, E., Carbune, V ., Rastogi, A., et al. (2024). RLAIF vs. RLHF: Scaling reinforcement learning from human feedback with AI feedback. In Forty-first International Conference on Machine Learning .\n- Leibo, J. Z., Vezhnevets, A. S., Cunningham, W. A., and Bileschi, S. M. (2025). A pragmatic view of AI personhood. arXiv preprint arXiv:2510.26396 .\n- Leibo, J. Z., Vezhnevets, A. S., Diaz, M., Agapiou, J. P., Cunningham, W. A., Sunehag, P., Haas, J., Koster, R., DuÃ©Ã±ez-GuzmÃ¡n, E. A., Isaac, W. S., et al. (2024). A theory of appropriateness with applications to generative artificial intelligence. arXiv preprint arXiv:2412.19010 .\n- Lemkin, R. (1944). Axis Rule in Occupied Europe: Laws of Occupation, Analysis of Government, Proposals for Redress . Carnegie Endowment for International Peace.\n- Lemley, M. A. and Casey, B. (2019). Remedies for robots. University of Chicago Law Review , 86(5):1311-1396.\n- Lessig, L. (1993). Fidelity in translation. Texas Law Review , 71(6):1165-1268.\n- Lessig, L. (1999). Code and Other Laws of Cyberspace . Basic Books.\n- Levi, E. H. (1949). An Introduction to Legal Reasoning . University of Chicago Press.\n- Levine, S., Franklin, M., Zhi-Xuan, T., Guyot, S. Y., Wong, L., Kilov, D., Choi, Y., Tenenbaum, J. B., Goodman, N., Lazar, S., et al. (2025). Resource rational contractualism should guide AI alignment. arXiv preprint arXiv:2506.17434 .\n- Li, N., Pan, A., Gopal, A., Yue, S., Berrios, D., Gatti, A., Li, J. D., Dombrowski, A.-K., Goel, S., Mukobi, G., et al. (2024). The wmdp benchmark: measuring and reducing malicious use with unlearning. In Proceedings of the 41st International Conference on Machine Learning , pages 28525-28550.\n- Li, S., Yang, C., Wu, T., Shi, C., Zhang, Y ., Zhu, X., Cheng, Z., Cai, D., Yu, M., Liu, L., et al. (2025). A survey on the honesty of large language models. Transactions on Machine Learning Research .\n- Lichkovski, I., MÃ¼ller, A., Ibrahim, M., and Mhundwa, T. (2025). Eu-agent-bench: Measuring illegal behavior of LLM agents under EU law. In NeurIPS 2025 Workshop on Regulatable ML .\n- Lindgren, S. and HolmstrÃ¶m, J. (2020). A social science perspective on artificial intelligence: Building blocks for a research agenda. Journal of digital social research , 2(3):1-15.\n- Lior, A. (2024). Holding AI accountable: Addressing the AI-related harms through existing tort doctrines. University of Chicago Law Review Online .\n- Liu, A., Ghate, K., Diab, M., Fried, D., Kasirzadeh, A., and Kleiman-Weiner, M. (2025). Generative value conflicts reveal LLM priorities. arXiv preprint arXiv:2509.25369 .\n- Llewellyn, K. N. (1960). The Common Law Tradition: Deciding Appeals . Little, Brown, &amp; Co.\n\n- Longpre, S., Klyman, K., Appel, R. E., Kapoor, S., Bommasani, R., Sahar, M., McGregor, S., Ghosh, A., Blili-Hamelin, B., Butters, N., et al. (2025). Position: In-house evaluation is not enough. towards robust third-party evaluation and flaw disclosure for general-purpose AI. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Lowe, R., Edelman, J., Zhi-Xuan, T., Klingefjord, O., Hain, E., Wang, V., Sarkar, A., Bakker, M. A., Barez, F., Franklin, M., et al. (2025). Full-stack alignment: Co-aligning AI and institutions with thicker models of value. In 2nd Workshop on Models of Human Feedback for AI Alignment .\n- Lynch, A., Wright, B., Larson, C., Ritchie, S. J., Mindermann, S., Hubinger, E., Perez, E., and Troy, K. (2025). Agentic misalignment: How LLMs could be insider threats. arXiv preprint arXiv:2510.05179 .\n- Maas, J. and InglÃ©s, A. M. (2024). Beyond participatory AI. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , volume 7, pages 932-942.\n- Maas, M. M. and Olasunkanmi, T. (2025). Treaty-following AI.\n- Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. (2023). Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems , 36:46534-46594.\n- Madison, J. (1788). The federalist no. 51. In The Federalist Papers . Library of Congress.\n- Maiya, S., Bartsch, H., Lambert, N., and Hubinger, E. (2025). Open character training: Shaping the persona of AI assistants through constitutional AI. arXiv preprint arXiv:2511.01689 .\n- Malik, S., Pyatkin, V., Land, S., Morrison, J., Smith, N. A., Hajishirzi, H., and Lambert, N. (2025). Rewardbench 2: Advancing reward model evaluation. arXiv preprint arXiv:2506.01937 .\n- Marino, B., Chaudhary, Y., Pi, Y., Yew, R.-J., Aleksandrov, P., Rahman, C., Shen, W. F., Robinson, I., and Lane, N. D. (2024). Compliance cards: Automated EU AI Act compliance analyses amidst a complex AI supply chain. arXiv preprint arXiv:2406.14758 .\n- Marino, B., Hunter, R., Jamali, Z., Kalpakos, M. E., Kashyap, M., Hinton, I., Hanson, A., Nazir, M., Schnabl, C., Steffek, F., et al. (2025). AIReg-Bench: Benchmarking language models that assess AI regulation compliance. arXiv preprint arXiv:2510.01474 .\n- MartÃ­nez, E. (2024). Re-evaluating GPT-4's bar exam performance. Artificial Intelligence and Law , 33:581-604.\n- Mashaw, J. L. (2006). Accountability and institutional design: Some thoughts on the grammar of governance. In Dowdle, M. W., editor, Public Accountability: Designs, Dilemmas and Experiences , chapter 5, pages 115-156. Cambridge University Press.\n- Mashaw, J. L., Shane, P. M., Bamzai, A., Bremer, E. S., Kwoka, M. B., and Parrillo, N. R. (2025). Administrative law, the American public law system: Cases and materials . 9th edition.\n- McGregor, S. (2021). Preventing repeated real world AI failures by cataloging incidents: The AI incident database. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pages 15458-15463.\n- McKernon, E., Glasser, G., Cheng, D., and Hadfield, G. (2024). AI model registries: A foundational tool for AI governance. arXiv preprint arXiv:2410.09645 .\n\nMeta (2025). Llama guard 4 model card.\n\nMicrosoft (2024). Customer copyright commitment required mitigations.\n\nMinow, M. (2019). When Should Law Forgive? W.W. Norton &amp; Co.\n\n- Mireshghallah, N., Kim, H., Zhou, X., Tsvetkov, Y., Sap, M., Shokri, R., and Choi, Y. (2024). Can LLMs keep a secret? testing privacy implications of language models via contextual integrity theory. In The Twelfth International Conference on Learning Representations .\n- Morris, M. R., Sohl-Dickstein, J., Fiedel, N., Warkentin, T., Dafoe, A., Faust, A., Farabet, C., and Legg, S. (2024). Position: Levels of AGI for operationalizing progress on the path to AGI.\n\nIn Proceedings of the 41st International Conference on Machine Learning , volume 235, pages 36308-36321.\n\n- Muchlinski, P. (2021). Multinational enterprises and the law . Oxford University Press, 3rd edition.\n- Narayanan, A. and Kapoor, S. (2025). AI as normal technology. Knight First Amendment Institute.\n- Nay, J. J. (2022). Law informs code: A legal informatics approach to aligning artificial intelligence with humans. Northwestern Journal of Technology &amp; Intellectual Property , 20:309.\n- Needham, J., Edkins, G., Pimpale, G., Bartsch, H., and Hobbhahn, M. (2025). Large language models often know when they are being evaluated. arXiv preprint arXiv:2505.23836 .\n- Nerantzi, E. and Sartor, G. (2024). 'Hard AI crime': The deterrence turn. Oxford Journal of Legal Studies , 44(3):673-701.\n- Ngo, R., Chan, L., and Mindermann, S. (2024). The alignment problem from a deep learning perspective. In The Twelfth International Conference on Learning Representations .\n- Nissenbaum, H. (1996). Accountability in a computerized society. Science and engineering ethics , 2(1):25-42.\n- North, D. C., Wallis, J., and Weingast, B. (2009). Violence and Social Orders . Cambridge University Press.\n- Nouws, S. and Dobbe, R. (2024). The rule of law for artificial intelligence in public administration: A system safety perspective. In Prifti, K., Demir, E., KrÃ¤mer, J., Heine, K., and Stamhuis, E., editors, Digital Governance , volume 39 of Information Technology and Law Series , pages 183-208. Springer.\n- Novelli, C., Floridi, L., Sartor, G., and Teubner, G. (2025). AI as legal persons: Past, patterns, and prospects. Journal of Law and Society .\n- O'Keefe, C., Ramakrishnan, K., Tay, J., and Winter, C. (2025). Law-following AI: Designing AI agents to obey human laws. Fordham Law Review , 94:57-129.\n- OpenAI (2024a). Introducing the model spec.\n- OpenAI (2024b). Learning to reason with LLMs.\n- OpenAI (2025a). ChatGPT agent system card.\n- OpenAI (2025b). OpenAI model spec, September 12, 2025.\n- OpenAI (2025). Sycophancy in GPT-4o: what happened and what we're doing about it.\n- Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744.\n- Ovadya, A., Redman, K., Thorburn, L., Chen, Q. Z., Smith, O., Devine, F., Konya, A., Milli, S., Revel, M., Feng, K., et al. (2025). Position: Democratic AI is possible. the democracy levels framework shows how it might work. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Pasquale, F. (2019). A rule of persons, not machines: The limits of legal automation. George Washington Law Review , 87(1):1-55.\n- Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N., and Irving, G. (2022). Red teaming language models with language models. arXiv preprint arXiv:2202.03286 .\n- Phan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Zhang, C. B. C., Shaaban, M., Ling, J., Shi, S., et al. (2025). Humanity's last exam. arXiv preprint arXiv:2501.14249 .\n- Placani, A. (2024). Anthropomorphism in AI: hype and fallacy. AI and Ethics , 4(3):691-698.\n- Posner, E. A. and Saran, S. (2025). Judge AI: assessing large language models in judicial decisionmaking.\n\n- Prabhakaran, V., Mitchell, M., Gebru, T., and Gabriel, I. (2022). A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667 .\n- Pruss, D. and Allen, J. (2025). Against AI jurisprudence: Large language models and the false promises of empirical judging. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , volume 8, pages 2055-2066.\n- Purushothama, A., Min, J., Waldon, B., and Schneider, N. (2025). Not ready for the bench: LLM legal interpretation is unstable and uncalibrated to human judgments. In Aletras, N., Chalkidis, I., Barrett, L., Goant , Ë˜ a, C., Preot , iuc-Pietro, D., and Spanakis, G., editors, Proceedings of the Natural Legal Language Processing Workshop 2025 , pages 317-317. Association for Computational Linguistics.\n- Ramakrishnan, K., Smith, G., and Downey, C. (2024). US tort liability for large-scale artificial intelligence damages. RAND.\n- Rawls, J. (1993). Political Liberalism . Columbia University Press.\n- Rawls, J. (1999). A Theory of Justice . Belknap Press, Harvard University Press, Revised edition.\n- Raz, J. (1971). Legal principles and the limits of law. Yale Law Journal , 81:823.\n- Raz, J. (1975). Practical reason and norms . Oxford University Press, 1st edition.\n- Raz, J. (1979a). The Authority of Law: Essays on Law and Morality . Clarendon Press, Oxford University Press.\n- Raz, J. (1979b). The rule of law and its virtue. In The Authority of Law: Essays on Law and Morality , pages 210-229. Clarendon Press, Oxford University Press.\n- Raz, J. (2019). The law's own virtue. Oxford Journal of Legal Studies , 39(1):1-15.\n- Reidenberg, J. R. (1998). Lex informatica: The formulation of information policy rules through technology. Texas Law Review , 76:553-593.\n- Reuel, A., Hardy, A., Smith, C., Lamparth, M., Hardy, M., and Kochenderfer, M. J. (2024). BetterBench: Assessing AI benchmarks, uncovering issues, and establishing best practices. Advances in Neural Information Processing Systems , 37:21763-21813.\n- Riedl, M. O. and Desai, D. R. (2025). AI agents and the law. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , volume 8, pages 2189-2198.\n- Rissland, E. L. (1990). Artificial intelligence and law: Stepping stones to a model of legal reasoning. Yale Law Journal , 99:1957-1981.\n- Russell, S. (2019). Human Compatible: AI and the Problem of Control . Viking.\n- Sadler, G. and Sherburn, N. (2025). Legal zero-days: A novel risk vector for advanced AI systems. arXiv preprint arXiv:2508.10050 .\n- Salaudeen, O., Reuel, A., Ahmed, A., Bedi, S., Robertson, Z., Sundar, S., Domingue, B., Wang, A., and Koyejo, S. (2025). Measurement to meaning: A validity-centered framework for AI evaluation. arXiv preprint arXiv:2505.10573 .\n- Salib, P. N. (2024). AI outputs are not protected speech. Washington University Law Review , 102:83.\n- Salib, P. N. and Goldstein, S. (2025a). AI rights for economic flourishing.\n- Salib, P. N. and Goldstein, S. (2025b). AI rights for human safety. Virginia Law Review (forthcoming) .\n- Samway, K., Mihalcea, R., and Jin, Z. (2025). When do language models endorse limitations on universal human rights principles? In Workshop on Socially Responsible Language Modelling Research .\n- Sanders, N. and Schneier, B. (2025). AI will write complex laws. Lawfare .\n- Sarkar, A., Muresanu, A. I., Blair, C., Sharma, A., Trivedi, R. S., and Hadfield, G. K. (2024). Normative modules: A generative agent architecture for learning norms that supports multi-agent cooperation. arXiv preprint arXiv:2405.19328 .\n- Scalia, A. and Garner, B. A. (2012). Reading Law: The Interpretation of Legal Texts . Thomson West.\n\n- Schauer, F. (1987). Precedent. Stanford Law Review , 39(3):571-605.\n- Schauer, F. (1991). Playing by the Rules: A Philosophical Examination of Rule-Based DecisionMaking in Law and in Life . Clarendon Press, Oxford University Press.\n- Schauer, F. (1995). Giving reasons. Stanford Law Review , 47:633-659.\n- Schauer, F. (2009). Thinking like a lawyer: a new introduction to legal reasoning . Harvard University Press.\n- Scheurer, J., Balesni, M., and Hobbhahn, M. (2024). Large language models can strategically deceive their users when put under pressure. arXiv preprint arXiv:2311.07590 .\n- Schneier, B. (2021). The coming AI hackers. Belfer Center for Science and International Affairs, Harvard Kennedy School.\n- Schwarcz, D., Manning, S., Barry, P., Cleveland, D. R., Prescott, J., and Rich, B. (2025). AI-powered lawyering: AI reasoning models, retrieval augmented generation, and the future of legal practice.\n- Schwarcz, S. L. (2008). Systemic risk. Georgetown Law Journal , 97:193-249.\n- Seger, E., Ovadya, A., Siddarth, D., Garfinkel, B., and Dafoe, A. (2023). Democratising AI: Multiple meanings, goals, and methods. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society , pages 715-722.\n- Sen, A. (2005). Human rights and the limits of law. Cardozo Law Review , 27:2913.\n- Sentinella, R. and Zweifel-Keegan, C. (2025). US state AI governance legislation tracker. IAPP.\n- Shapiro, S. J. (2006). What is the internal point of view? Fordham Law Review , 75:1157-1170.\n- Shapiro, S. J. (2011). Legality . Belknap Press, Harvard University Press.\n- Sharma, M., Tong, M., Korbak, T., Duvenaud, D., Askell, A., Bowman, S. R., Durmus, E., HatfieldDodds, Z., Johnston, S. R., Kravec, S. M., et al. (2024). Towards understanding sycophancy in language models. In The Twelfth International Conference on Learning Representations .\n- Sheshadri, A., Hughes, J., Michael, J., Mallen, A., Jose, A., Roger, F., et al. (2025). Why do some language models fake alignment while others don't? arXiv preprint arXiv:2506.18032 .\n- Simon, H. A. (1997). Administrative Behavior . Free Press, Simon &amp; Schuster, 4th edition.\n- Skalse, J., Howe, N., Krasheninnikov, D., and Krueger, D. (2022). Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471.\n- Sloane, M., Moss, E., Awomolo, O., and Forlano, L. (2022). Participation is not a design fix for machine learning. In Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization , pages 1-6.\n- Smith, B. (2023). Microsoft announces new Copilot Copyright Commitment for customers.\n- Smuha, N. A. (2024). Algorithmic rule by law: How algorithmic regulation in the public sector erodes the rule of law . Cambridge University Press.\n- Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., et al. (2024). Dolma: an open corpus of three trillion tokens for language model pretraining research. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 15725-15788.\n- Solum, L. B. (1992). Legal personhood for artificial intelligences. North Carolina Law Review , 70:1231-1287.\n- Song, Z. (2025). Value-aligned but misguided: a dilemma in AI and AGI decision making. Synthese , 206(3):138.\n- Sorensen, T., Jiang, L., Hwang, J. D., Levine, S., Pyatkin, V., West, P., Dziri, N., Lu, X., Rao, K., Bhagavatula, C., et al. (2024a). Value kaleidoscope: Engaging AI with pluralistic human values, rights, and duties. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 19937-19947.\n\n- Sorensen, T., Moore, J., Fisher, J., Gordon, M. L., Mireshghallah, N., Rytting, C. M., Ye, A., Jiang, L., Lu, X., Dziri, N., et al. (2024b). Position: A roadmap to pluralistic alignment. In International Conference on Machine Learning , pages 46280-46302.\n- Stack, K. M. (2007). The constitutional foundations of Chenery. Yale Law Journal , 116:952-1040.\n- Strathern, M. (1997). 'Improving ratings': audit in the british university system. European Review , 5(3):305-321.\n- Stray, J., Halevy, A., Assar, P., Hadfield-Menell, D., Boutilier, C., Ashar, A., Bakalar, C., Beattie, L., Ekstrand, M., Leibowicz, C., Moon Sehat, C., Johansen, S., Kerlin, L., Vickrey, D., Singh, S., Vrijenhoek, S., Zhang, A., Andrus, M., Helberger, N., Proutskova, P., Mitra, T., and Vasan, N. (2024). Building human values into recommender systems: An interdisciplinary synthesis. ACM Transactions on Recommender Systems , 2(3).\n- Summerfield, C., Argyle, L. P., Bakker, M., Collins, T., Durmus, E., Eloundou, T., Gabriel, I., Ganguli, D., Hackenburg, K., Hadfield, G. K., et al. (2025). The impact of advanced AI systems on democracy. Nature Human Behaviour , pages 1-11.\n- Sunstein, C. R. (1993). On analogical reasoning. Harvard Law Review , 106(3):741-791.\n- Sunstein, C. R. (1995). Incompletely theorized agreements. Harvard Law Review , 108(7):1733-1772.\n- Sunstein, C. R. (2001). Of artificial intelligence and legal reasoning. University of Chicago Law School Roundtable , 8:29-45.\n- Sunstein, C. R. (2024). Artificial intelligence and the first amendment. George Washington Law Review , 92:1207.\n- Surani, F., Gailmard, L. A., Casasola, A., Magesh, V., Robitschek, E. J., and Ho, D. E. (2025). What is the law? a system for statutory research (STARA) with large language models. In 20th International Conference on Artificial Intelligence and Law .\n- Susskind, R. (1987). Expert systems in law: a jurisprudential inquiry . Clarendon Press, Oxford University Press.\n- Tallarita, R. (2023). AI is testing the limits of corporate governance. Harvard Business Review .\n- Tamanaha, B. Z. (2004). On the rule of law: History, politics, theory . Cambridge University Press.\n- Tessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., and Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science , 386(6719):2852.\n- Thomas, R. and Uminsky, D. (2020). The problem with metrics is a fundamental problem for AI. arXiv preprint arXiv:2002.08512 .\n- Tomasev, N., Franklin, M., Leibo, J. Z., Jacobs, J., Cunningham, W. A., Gabriel, I., and Osindero, S. (2025). Virtual agent economies. arXiv preprint arXiv:2509.10147 .\n- Turing, A. (1950). Computing machinery and intelligence. Mind: A Quarterly Review of Psychology and Philosophy , 59(236):433-460.\n- Tyler, T. R. (2006). Why people obey the law . Princeton University Press, 2nd edition.\n- U.S. Supreme Court (1868). United states v. kirby. 74 U.S. (7 Wall.) 482.\n- Uuk, R., Gutierrez, C. I., Guppy, D., Lauwaert, L., Kasirzadeh, A., Velasco, L., Slattery, P., and Prunkl, C. (2024). A taxonomy of systemic risks from general-purpose AI. arXiv preprint arXiv:2412.07780 .\n- Velasco, J. (2006). The fundamental rights of the shareholder. UC Davis Law Review , 40:407.\n- Waldon, B., Schneider, N., Wilcox, E., Zeldes, A., and Tobia, K. (2025). Large language models for legal interpretation? don't take their word for it. Georgetown Law Journal (forthcoming) .\n- Waldron, J. (2016). The rule of law. Stanford Encyclopedia of Philosophy .\n\n- Wallace, E., Xiao, K., Leike, R., Weng, L., Heidecke, J., and Beutel, A. (2024). The instruction hierarchy: Training LLMs to prioritize privileged instructions. arXiv preprint arXiv:2404.13208 .\n- Wallach, H., Desai, M., Cooper, A. F., Wang, A., Atalla, C., Barocas, S., Blodgett, S. L., Chouldechova, A., Corvi, E., Dow, P. A., et al. (2025). Position: Evaluating generative AI systems is a social science measurement challenge. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Wan, A., Klyman, K., Kapoor, S., Maslej, N., Longpre, S., Xiong, B., Liang, P., and Bommasani, R. (2025). The 2025 foundation model transparency index. arXiv preprint arXiv:2512.10169 .\n- Wang, X. and Wellman, M. P. (2020). Market manipulation: An adversarial learning framework for detection and evasion. In 29th International Joint Conference on Artificial Intelligence .\n- Wei, A., Haghtalab, N., and Steinhardt, J. (2023). Jailbroken: How does LLM safety training fail? Advances in Neural Information Processing Systems , 36:80079-80110.\n- Wei, K. and Heim, L. (2025). Designing incident reporting systems for harms from general-purpose AI. arXiv preprint arXiv:2511.05914 .\n- Wei, K., Paskov, P., Dev, S., Byun, M. J., Reuel, A., Roberts-Gaal, X., Calcott, R., Coxon, E., and Deshpande, C. (2025). Position: Human baselines in model evaluations need rigor and transparency (with recommendations &amp; reporting checklist). In Forty-second International Conference on Machine Learning Position Paper Track .\n- Weidinger, L., Raji, I. D., Wallach, H., Mitchell, M., Wang, A., Salaudeen, O., Bommasani, R., Ganguli, D., Koyejo, S., and Isaac, W. (2025). Toward an evaluation science for generative AI systems. arXiv preprint arXiv:2503.05336 .\n- Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, J., Bergman, S., Kay, J., Griffin, C., Bariach, B., et al. (2023). Sociotechnical safety evaluation of generative AI systems. arXiv preprint arXiv:2310.11986 .\n- Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B., Kasirzadeh, A., et al. (2022). Taxonomy of risks posed by language models. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency , pages 214-229.\n- Weil, G. (2024). Tort law as a tool for mitigating catastrophic risk from artificial intelligence.\n- Wilf-Townsend, D. and Tobia, K. (2025). AI-generated legal texts.\n- Williams, M., Carroll, M., Narang, A., Weisser, C., Murphy, B., and Dragan, A. (2025a). On targeted manipulation and deception when optimizing LLMs for user feedback. In The Thirteenth International Conference on Learning Representations .\n- Williams, S., Schuett, J., and Anderljung, M. (2025b). On regulating downstream AI developers. European Journal of Risk Regulation , page 1-29.\n- Wu, T. (2003). Network neutrality, broadband discrimination. Journal of Telecommunications and High Technology Law , 2:141-175.\n- Wu, X., Hong, G., Chen, P., Chen, Y., Pan, X., and Yang, M. (2025). Prison: Unmasking the criminal potential of large language models. arXiv preprint arXiv:2506.16150 .\n- Zanzotto, F. M. (2019). Human-in-the-loop artificial intelligence. Journal of Artificial Intelligence Research , 64:243-252.\n- Zeng, Y., Yang, Y., Zhou, A., Tan, J. Z., Tu, Y., Mai, Y., Klyman, K., Pan, M., Jia, R., Song, D., Liang, P., and Li, B. (2025). Air-bench 2024: A safety benchmark based on regulation and policies specified risk categories. In The Thirteenth International Conference on Learning Representations .\n- Zhang, A. K., Perry, N., Dulepet, R., Ji, J., Menders, C., Lin, J. W., Jones, E., Hussein, G., Liu, S., Jasper, D. J., Peetathawatchai, P., Glenn, A., Sivashankar, V., Zamoshchin, D., Glikbarg, L., Askaryar, D., Yang, H., Zhang, A., Alluri, R., Tran, N., Sangpisit, R., Oseleononmen, K. O., Boneh, D., Ho, D. E., and Liang, P. (2025). Cybench: A framework for evaluating cybersecurity capabilities and risks of language models. In The Thirteenth International Conference on Learning Representations .\n\n- Zheng, L., Guha, N., Arifov, J., Zhang, S., Skreta, M., Manning, C. D., Henderson, P., and Ho, D. E. (2025). A reasoning-focused legal retrieval benchmark. In Proceedings of the 2025 Symposium on Computer Science and Law , pages 169-193.\n- Zhi-Xuan, T., Carroll, M., Franklin, M., and Ashton, H. (2025). Beyond preferences in AI alignment. Philosophical Studies , 182(7):1813-1863.\n- Zhu, Y., Jin, T., Pruksachatkun, Y., Zhang, A. K., Liu, S., Cui, S., Kapoor, S., Longpre, S., Meng, K., Weiss, R., Barez, F., Gupta, R., Dhamala, J., Merizian, J., Giulianelli, M., Coppock, H., Ududec, C., Kellermann, A., Sekhon, J. S., Steinhardt, J., Schwettmann, S., Narayanan, A., Zaharia, M., Stoica, I., Liang, P., and Kang, D. (2025a). Establishing best practices in building rigorous agentic benchmarks. In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track .\n- Zhu, Y., Kellermann, A., Bowman, D., Li, P., Gupta, A., Danda, A., Fang, R., Jensen, C., Ihli, E., Benn, J., Geronimo, J., Dhir, A., Rao, S., Yu, K., Stone, T., and Kang, D. (2025b). CVE-bench: A benchmark for AI agents' ability to exploit real-world web application vulnerabilities. In Forty-second International Conference on Machine Learning .\n- Zittrain, J. (2008). The Future of the Internet-And How to Stop It . Yale University Press.\n\nZittrain, J. (2024). We need to control AI agents now.", "fetched_at_utc": "2026-02-09T13:55:36Z", "sha256": "e337659501047e3b8b9da737fce38a184f9cefe47d27820ef173badbf55653c5", "meta": {"file_name": "Legal Alignment for Safe and Ethical AI.pdf", "file_size": 608504, "mtime": 1770643036, "docling_errors": []}}
{"doc_id": "pdf-pdfs-lost-in-vagueness-towards-context-sensitive-standards-for-robustness-assess-9759edc205d7", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Lost in Vagueness - Towards Context-Sensitive Standards for Robustness Assessment under the EU AI Act.pdf", "title": "Lost in Vagueness - Towards Context-Sensitive Standards for Robustness Assessment under the EU AI Act", "text": "## Lost in Vagueness: Towards Context-Sensitive Standards for Robustness Assessment under the EU AI Act\n\nRoberta Tamponi 1 âˆ— Carina Prunkl 2,3 Thomas BÃ¤ck 1 Anna V. Kononova 1\n\n1 Leiden Institute of Advanced Computer Science, Leiden University 2 Inria 3 University of Oxford\n\n## Abstract\n\nRobustness is a key requirement for high-risk AI systems under the EU Artificial Intelligence Act (AI Act). However, both its definition and the methodologies for assessing it remain underspecified, leaving providers with little concrete direction on how to demonstrate compliance. This stems from the Act's horizontal approach, which establishes general obligations applicable across all AI systems, but leaves the task of providing technical guidance to harmonised standards. This paper investigates what it means for AI systems to be robust and illustrates the need for context-sensitive standardisation. We argue that robustness is not a fixed property of a system, but depends on which aspects of performance are expected to remain stable ('robustness of what'), the perturbations the system must withstand ('robustness to what') and the operational environment. We identify three contextual drivers-use case, data and model-that shape the relevant perturbations and influence the choice of tests, metrics and benchmarks used to evaluate robustness. The need to provide at least a range of technical options that providers can assess and implement in light of the system's purpose is explicitly recognised by the standardisation request for the AI Act, but planned standards, still focused on horizontal coverage, do not yet offer this level of detail. Building on this, we propose a multi-layered standardisation framework that embeds context-sensitivity in the ongoing standardisation process. Horizontal standards define shared principles and terminology, while domain-specific ones identify relevant risks and map them across the AI lifecycle, guiding the selection of best practices for specific applications. These, together with benchmarks and sandboxes, should be organised in a dynamic repository where providers can propose new informative methods and share lessons learned. Such a system reduces the interpretative burden, mitigates arbitrariness and addresses the obsolescence of static standards, ensuring that robustness assessment is both adaptable and operationally meaningful.\n\n## 1 Introduction\n\nIn 2024, the European Union (EU) adopted the Artificial Intelligence Act (AI Act), the world's first comprehensive legal framework for AI technologies [1]. It addresses concerns about safety, reliability and fundamental rights, and ensures AI systems align with European values. The Act builds on the work of the EU High-Level Expert Group [2], which identified three requirements for trustworthy AI: lawful, ethical and technically robust. On this basis, the AI Act adopts a risk-based approach: obligations vary depending on the level of risk an AI system poses. Providers of high-risk AI systems are required to conduct conformity assessments before placing their products on the European market. A central challenge is that, despite outlining procedural obligations, the AI Act offers little detail on\n\nâˆ— Corresponding author: r.tamponi@liacs.leidenuniv.nl\n\nhow these should be put into practice [3]. This lack of clarity reflects the Act's horizontal approach, which sets general requirements for all AI systems and defers practical implementation to harmonised standards.\n\nAmong the obligations, robustness is a key requirement [4]. However, both the regulation and the Code of Practice, leave its definition and assessment underspecified, raising questions about what must be demonstrated and how. The notion of robustness is ubiquitous in the field of AI, but its exact meaning varies depending on the context [5]. Researchers either focus on narrow technical definitions or leave the concept too abstract, aiming for generality. This ambiguity reflects the concept's context-sensitive nature : the robustness of a system depends on its intended purpose, design and deployment environment, which shape the perturbations and potential failures most relevant for evaluation. Hence, the practical assessment is itself context-sensitive. The choice of robustness tests, metrics, thresholds and mitigation strategies must be tailored to operating conditions, where even minor changes can result in markedly different evaluation procedures. The only international standard on robustness adopted so far by the EU, the CEN/CLC ISO/IEC/TR 24029-1 [6], 2 is highly generic and only offers an overview of the assessment methods for neural networks. Other standards on robustness are under development, but their specificity remains uncertain. The ISO/IEC 24029-3, for instance, is expected to expand the existing series and include tests and metrics [7]. Yet, it seems unlikely that a single extension could provide the level of detail needed to identify, assess and mitigate robustness risks across diverse contexts. Moreover, it focuses only on neural networks, neglecting the broader diversity of AI systems. The risk of overly generic guidance, therefore, remains, leaving providers with excessive interpretative burden and the risk of arbitrary implementation. The AI Act's standardisation request explicitly acknowledges the need to account for different use cases and sectors and to develop vertical specifications where appropriate [8]. European standards need to set clear requirements for AI providers, yet it seems unlikely that horizontal standards alone can adequately fulfil the mandate.\n\nStatement of Contributions. We propose a clearer conceptualisation of robustness, distinguishing the robustness of what from the robustness to what . We further identify three contextual drivers-use case, data and model-which determine the relevant perturbations and the most suitable methods to assess system robustness. Finally, we propose a framework to embed context-sensitivity into the ongoing standardisation process, addressing the limited scope of horizontal standards, which risks leaving too much room for arbitrary choices, and the static nature of standardisation, which struggles to keep pace with technological change.\n\nThe paper first outlines the AI Act and its treatment of robustness (Section 2). Section 3 highlights the limits of overly abstract definitions. Section 4 distinguishes two dimensions of robustness: the robustness of what (the performance) and the robustness to what (the types of perturbations the system must withstand). Section 5 examines how the contextual drivers shape relevant perturbations as well as the evaluation methods and concludes with a use case comparison. Section 6 discusses current standardisation challenges and Section 7 proposes a multilayered framework combining horizontal standards with domain-specific provisions, supported by a dynamic repository of practices, benchmarks and sandboxes, which also allows providers to propose new informative methodologies and share experiences.\n\n## 2 Robustness in the AI Act\n\nThe Artificial Intelligence Act, adopted in 2024, establishes a product-safety-based regulatory framework that applies different obligations depending on the risk level of AI systems. Risk is defined as the combination of the probability of harm and the severity of harm (Art. 3) and systems are classified into four categories: unacceptable, high, limited, and minimal risk. The AI Act primarily targets high-risk systems-those with potentially significant impacts on safety, fundamental rights, or critical domains. These must obtain the CE marking (Art. 48) through a conformity assessment (Art. 43, Annex VI-VII), demonstrating compliance with Chapter III, Section 2, which sets requirements on risk management (Art. 9), data governance and quality (Art. 10), record keeping (Art. 12), transparency (Art. 13), human oversight (Art. 14), accuracy, robustness and cybersecurity (Art. 15). Providers must maintain technical documentation (Art. 11), implement a quality management system\n\n2 'CEN/CLC ISO/IEC' indicates that the technical report (TR) by ISO (International Organisation for Standardisation) and IEC (International Electrotechnical Commission) has been adopted as a European standard.\n\n(Art. 17) and, after deployment, monitor performance using tools defined during the development. The provisions are entering into force gradually, starting in August 2024, allowing time for the development of harmonised standards, providers' adaptation and the establishment of supervisory authorities.\n\nRobustness is thus a core requirement. Article 15 states that AI systems must be designed to ensure an appropriate level of accuracy, robustness and cybersecurity throughout their lifecycle (Art. 15.1) and to be as resilient as possible to errors, faults and inconsistencies (Art. 15.3). Systems that continue to learn after being placed on the market have to include mitigation measures to minimise the risk of biased outputs arising from feedback loops (Art. 15.4). These provisions are vague as they only set general objectives without providing parameters, metrics, or acceptability thresholds. The regulation itself acknowledges that benchmarks and measurement methodologies are still lacking and indicates that the Commission is working with stakeholders to develop them (Art. 15.2). This is coherent with the will of creating a horizontal regulation that applies to all AI systems, but highlights the need for complementary, detailed standards. Yet, despite the ongoing standardisation process, major uncertainties remain about the level of detail these standards will provide, and, in their current form, both the definition of robustness and the related provisions remain insufficiently specified.\n\n## 3 Limitations of an abstract definition of robustness\n\nThe notion of robustness in AI remains conceptually ambiguous [5] and has taken multiple meanings across different domains and contexts [9]. The existing harmonised standard on the robustness assessment of neural networks [6] describes robustness as the ability of an AI system to maintain its level of performance under any circumstances . While this definition may be adopted for regulatory purposes such as the AI Act, access to standards is restricted by copyright and they are not publicly available. As a result, researchers often rely on alternative definitions. For example, Nobandegani et al. [10] define it as the insensitivity of a model's performance to miscalculations of its parameters. Freiesleben and Grote [5] describe it as a model's capacity to sustain stable predictive performance in the face of variations in input data. Across subfields, the term has taken on multiple meanings. In computer vision, the term has been used to denote raw performance on held-out test sets, generalisation within and across domains, maintaining performance on manipulated inputs and naturally-induced image corruptions and resistance to adversarial attacks [11]. In natural language processing (NLP), La Malfa et al. [12] frame robustness of text classification in terms of the capacity to maintain predictions under word substitutions, formalised as a maximal safe radius for a given input text. Hendrycks et al. [13], focusing on out-of-distribution generalisation, understand robustness as the ability to handle unforeseen real-world distributional shifts.\n\nDespite the variety of formulations, the underlying principle to which they all refer is the stability of performance . This broad intuition can be captured in the high-level definition: a model is robust when small changes in input cause small changes in output . While this is an intuitive starting point, it remains too abstract to be operationally useful. What is a small change in input? Which output? To answer these, a more comprehensive definition can be: a system is considered robust to some kind of small perturbations in the input data when the system's performance under the perturbed state remains similar to that of the unperturbed state . But then, which perturbations? All of them or only some? Which performances must be stable? How stable? Going deeper with the necessary clarifications, which typically depend on the context in which the system is developed and deployed, leads us to case-specific definitions. Therefore, robustness is not a fixed notion of a system but depends on the system's different assumptions and goals . As a result, a system considered robust in one context may not be robust in another.\n\n## 4 Robustness dimensions\n\nBuilding on this context-sensitive understanding, we propose a structured way of interpreting robustness that supports practical evaluation. In line with Freiesleben and Grote [5], who frame robustness as a relation between targets, modifiers, domains and tolerance thresholds, we share the view that robustness must be context-specific. Our focus, however, is on linking abstract definitions to practical evaluation. Starting from the principle of the stability of performance, we distinguish between two core aspects: the system's output that is required to remain stable ( robustness of what ) and the types of changes in input it is required to withstand ( robustness to what ).\n\n## 4.1 Robustness of what?\n\nTo assess robustness in practice, we must first specify what is expected to remain stable, which, in machine learning, is the system's performance , as robustness concerns the system's ability to maintain adequate predictive capabilities under non-ideal or varying conditions. In line with the terminology used in current international standardisation efforts, we adopt the definition provided in BS EN ISO/IEC 25059 [14], which states that, within the field of AI, performance means how well a certain AI system performs the intended tasks . Performance can be measured with relevant metrics that depend on the system's purpose. CEN/CLC ISO/IEC/TR 24029-1 [6] proposes an exhaustive list which includes accuracy, precision, recall, specificity, F 1 score, ROC and AUC. These can be used independently or in combination, but they are not interchangeable. Notably, there are also numerous task-specific metrics, such as BLEU, TER and METEOR for machine translation, or mean average precision for ranked retrieval. To be meaningful indicators of performance, metrics need to be chosen based on the system's purpose and the impact of the different types of errors. For example, in a child-safety video filter, where the retrieved set is the non-harmful content, high precision is preferred: excluding some acceptable videos (low recall) is better than showing harmful content. In shoplifter detection, on the other hand and depending on operational preferences, false alerts may be tolerated if this ensures that most shoplifters are detected, so recall may be prioritised over precision [15]. Since such measures often trade off, the system's function and associated risks must be understood before identifying which aspects of performance are most crucial to maintain under perturbations. Robustness evaluation assumes a fully trained and tested system with established performance under standard conditions. This baseline is then used to assess how perturbations affect performance and to define tolerable deviation thresholds. These thresholds depend on deployment context, with a lower tolerance level due to higher associated risks (e.g., clinical decision-making).\n\n## 4.2 Robustness to what?\n\nAfter clarifying the role of performance in robustness evaluation, the next step is to identify against what the model must be robust. Robustness assessment concerns how input perturbations affect the system's predictive capabilities and whether the model can maintain its intended behaviour. Possible threats identified in BS EN ISO/IEC 25059 [14] include unseen, biased, adversarial or invalid data, external interference and environmental conditions; the AI Act similarly points to errors, faults, inconsistencies and unexpected situations (Recital 75). Some minor perturbations in input are unavoidable, while others might lead to more severe problems. A report of the latter must be documented under the risk management obligations of Art. 9, which requires the identification, estimation and evaluation of foreseeable risks. Identifying which input changes are critical depends on the system's intended use; therefore, the robustness goal must be clearly specified. Generic instructions, such as 'robust to dissimilar inputs', are too vague, since a model may generalise well across different distributions in the same application domain, but fail entirely on inputs of unrelated domains [6]. Assessing robustness requires a precise goal, defined by narrowing down the relevant perturbations that the system can face. The following section examines the contextual drivers that shape the identification of these perturbations.\n\n## 5 Context sensitivity of robustness assessment\n\nNot all types of perturbations are equally relevant for every system, so it is necessary to identify those most critical for the intended application. For self-driving cars, protecting against adversarial attacks is crucial, but it is not necessarily relevant for an epidemiologist using AI models to forecast virus spread [5]. Relevant perturbations cannot be defined universally, but depend on system design and deployment environment. We identify three contextual drivers-use case, data and model architecture-that guide the selection of relevant perturbations and, in turn, inform the tests, metrics and benchmarks through which robustness is evaluated in a context-sensitive way. Figure 1 provides a schematic overview of context-sensitive robustness assessment.\n\n## 5.1 Contextual drivers of robustness\n\nUse case. When developing an AI system, its intended use case needs to be specified. This includes its application domain, the task and the deployment environment in which it is expected to operate. The AI Act (Annexes I and III) gives several examples of domains where AI use can be considered\n\nFigure 1: Conceptual pipeline for context-sensitive robustness assessment.\n\n<!-- image -->\n\nhigh-risk, such as healthcare, education, employment, access to services and law enforcement. Within each domain, AI may serve different tasks, for example, real-time health monitoring, school grading, needs-based resource allocation, etc. The deployment environment refers to the real-world conditions in which the AI system is expected to operate, which is typically dynamic, partially observable and/or subject to noise or user interaction. Addressing these different aspects means delineating the problem that the AI system is meant to solve and how, by clarifying the goals and the requirements that need to be met [3, 16]. In healthcare, for example, systems may require robustness to co-occurring conditions such as gastrointestinal or neurological disorders, or robustness to variations in computed tomography protocols, including scanner model, patient size, or radiation dose [17]. Understanding the use case helps identify the system's potential failures and limitations, guiding both the perturbations to be addressed and the criteria for assessing them.\n\nData. Data quantity, quality and type determine applicable models, relevant perturbations and suitable evaluation metrics. A main challenge while building an AI system is the lack of training data: even basic problems need thousands of examples, while complex ones may require millions [15]. Shallow learning models like decision trees or support vector machines are often effective when data is limited, while neural networks require larger amounts of data. Another challenge is the presence of non-representative and poor-quality data. A system can learn and detect underlying patterns only if the training data include enough relevant features that are not full of errors, outliers and noise [15]. Robustness to data drift depends on how performance degrades over time due to shifts in the patterns or in the environment [18]. Low-quality data are detrimental to adversarial training, as they cause robust overfitting and robustness overestimation against weaker adversaries such as projected gradient descent (PGD) [19]. The use of large and diverse datasets increases generalisability, making a system more robust to variations and noise sources in data [20]. Finally, the data type: even when robustness is framed in terms of distribution shift, its manifestation is data-specific. In images, these perturbations may appear as noise, blur or weather effects, each testable through specific techniques, while for text they can be probed at character, word, or sentence level, reflecting typos, substitutions, or stylistic changes, each requiring tailored evaluation methods [21].\n\nModel. Different models have different sensitivities to data variations and robustness depends on learning paradigm, architecture and training configuration choices. In terms of learning paradigm, basic classification methods, such as decision trees or rule-based systems, may suffice for structured tasks like automated coding of medical records, while highly sensitive applications, such as medical image analysis or patient outcome prediction, require models that can handle complex, high-dimensional data, like deep learning models [3, 16]. At the architectural level, linear classifiers like Naive Bayes might be preferred due to their efficiency and interpretability, but if the system is expected to deal with non-linear perturbations, then models such as k-Nearest Neighbours might be more appropriate [22]. Moreover, some models' architectures can better mitigate the accuracy/robustness trade-off than others, balancing the performance on clean data with the architecture's ability to contrast adversarial perturbations [23, 24]. Also training configuration has to balance this trade-off. The inclusion of batch normalisation has been shown to improve model accuracy, but in some models, it increases the vulnerability to adversarial attacks [25, 26]. Hyperparameters such as learning rate, batch size and regularisation coefficients are likewise critical for robustness and optimisation strategies like grid or random search can substantially enhance it by fine-tuning these settings [20, 27, 28]\n\n## 5.2 Robustness evaluation\n\nUse case, data and model can guide the selection of relevant perturbations. Unfortunately, there is no unambiguous way of defining them, since they may originate from diverse sources and the deployment environment can introduce unpredictable forms of uncertainty that are difficult to anticipate or quantify. The literature commonly organises robustness into two macro-categories: adversarial robustness and natural robustness [9, 29]. Adversarial robustness describes the ability of a model to maintain stable performance when exposed to intentional manipulations of the input data, designed to mislead it [30]. These perturbations are non-random and precisely constructed to subtly change the input and maximise the probability that the model will produce an incorrect prediction [31]. Natural robustness is also called distribution shift or out-of-distribution data and concerns the robustness of the model to perturbations that occur spontaneously in the deployment environment, such as noise or visual alterations caused by external factors [11]. Natural perturbations introduce a deviation between the distribution of the test data and the one on which the model was originally trained [32].\n\nOnce the relevant types of perturbations that may pose a risk to the system have been identified, it is time to test whether the system can remain functional. To test robustness means submitting the model to various types of perturbations to identify vulnerabilities and weaknesses [33]. Various testing techniques have been investigated. For adversarial attacks, for example, generative adversarial networks (GANs) [34] can be used to synthesise challenging test cases. In the metamorphic testing [35, 36], the model's behaviour is evaluated under meaningful input transformations. Mutation testing [37, 38] introduces small faults into the test data to evaluate whether the model can detect them. To evaluate the test, metrics are needed to quantify how much a model's performance degrades (or resists) when exposed to perturbations. CEN/CLC ISO/IEC/TR 24029-1 [6] overview on the robustness of neural networks assessment distinguishes statistical methods , where robustness is defined as a limited performance drop relative to an unperturbed reference set, formal methods , which provide mathematical proofs but rely on restrictive assumptions, and empirical methods , which rely on experimentation, observation and expert judgement.\n\nBenchmarking an AI system can also help assess some degree of its robustness, as it provides shared and standardised conditions to compare models under controlled perturbations in a reproducible way. The AI Act encourages the development of benchmarks to support the evaluation of high-risk AI systems, but leaves open what counts as an appropriate benchmark and who should be responsible for their creation or maintenance. Some benchmarks focus on specific types of robustness. RobustBench standardises adversarial robustness evaluation across models [39], while WILDS targets robustness to distribution shifts, including domain generalisation and subpopulation shift [32]. Some are tailored to a learning paradigm: OpenAI Gym [40] supports robustness testing in reinforcement learning. While others focus on optimisation methods, for example, in heuristic optimisation, IOHprofiler evaluates algorithm selection and configuration under perturbed scenarios [41, 42]. Benchmarks can thus help establish initial trust in an AI solution even before deployment, but their relevance depends on the type of robustness being tested and the task at hand.\n\n## 5.3 Comparing use cases\n\nEven within similar contexts, what makes a system robust may change drastically, and with it, its assessment. Table 1 compares two studies on AI-based medical image classification for disease detection. The first investigates convolutional neural networks (CNN) classifiers for renal cell carcinoma [26]. The second examines CNN for tuberculosis-related chest radiographs classification [43]. Use case 1 trains on data from The Cancer Genome Atlas and tests on data from the University of Aachen, while use case 2 trains on CXR from Shenzhen Hospital (China) and tests both on a heldout Shenzhen set and on the NIH ChestX-ray8 dataset (USA), which includes tuberculosis-related features but also other lung abnormalities. Robustness in the first case is tested against multiple adversarial attacks that differ in nature and strength (white-box vs. black-box, including ensemble and transformation-based attacks), complemented by adversarial training, dual batch normalisation and evaluation of attack success rates on selected image subsets, while in the second case it is assessed through domain shift, testing cross-population generalisability from Chinese to US data. In both cases, robustness is measured by comparing the AUROC (ability to discriminate between true and false positives) before and after perturbations, with use case 1 also reporting attack success rates to capture the degree of misclassification under adversarial inputs.\n\nTable 1: Comparison of two use cases where slightly different design choices lead to different perturbations and therefore very different methods for robustness evaluation.\n\n| Details           | Use case 1                                                                                                                                                                                                                                                   | Use case 2                                                                                                       |\n|-------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|\n| Use case          | Detection of renal cell carcinoma (RCC)                                                                                                                                                                                                                      | Detection of tuberculosis-related features                                                                       |\n| Data              | Whole-Slide Images (WSIs) of Hematoxylin and Eosin stained tissue (RCC samples) taken from two different datasets for training and testing                                                                                                                   | Chest radiograph (CXR) taken from two datasets: (1) confirmed tuberculosis (2) tuberculosis- associated features |\n| Model             | CNN (ResNet)                                                                                                                                                                                                                                                 | CNN (Inception V3)                                                                                               |\n| Perturbation      | Adversarial attacks: FGSM, PGD, FAB, Square Attacks, AutoAttack, AdvDrop                                                                                                                                                                                     | Distribution shift                                                                                               |\n| Robustness test   | 1) Multiple adversarial attacks using the 6 attacks, each with three different attack strengths 2) Ad- versarially robust training 3) Dual Batch Normal- isation training 4) Selection of 450 tiles from the RCC set to test the degree of misclassification | Train the model with one dataset and test it on one with a different distribution                                |\n| Evaluation metric | 1) AUROC before and after the perturbations 2) Attack Success Rate                                                                                                                                                                                           | AUROC before and after distri- bution shift                                                                      |\n\nThis comparison shows that even within similar contexts, disease detection through CNN on image data, what defines the system as robust can change significantly. Small shifts in assumptions-such as the deployment environment-lead to very different evaluations. In digital pathology workflows, where adversarial manipulations are a plausible risk, robustness must be tested against gradient-based perturbations, minimal decision-boundary changes, random or combined attacks and high-frequency manipulations. On the other hand, the CXR model expects to face the challenge of distribution drift, requiring robustness to differences in population between training and deployment data. Similar variation occurs even within the same class of perturbations: for instance, in large language models for medical question-answering, adversarial robustness can be tested through MedFuzz attacks that introduce misleading patient characteristics [44].\n\n## 6 Current standardisation directions\n\nHarmonised standards may be used to demonstrate compliance with the AI Act, and while not mandatory, they grant presumption of conformity (Art. 40) and are fundamental for the practical implementation of the Regulation. The European Commission is working with the European Standardisation Organisations (ESO), formed by the European Committee for Standardisation (CEN), the European Committee for Electrotechnical Standardisation (CENELEC) and the European Telecommunications Standards Institute (ETSI), to develop technical specifications for the AI Act. In May 2023, CEN and CENELEC formally accepted the standardisation request M/593. The JTC 21 (Joint Technical Committee), established by CEN and CENELEC, is the dedicated body for developing harmonised standards in support of the Act, complementing and adapting existing international standards to the Act requirements. At the time of writing, their work programme includes 25 ongoing activities and 15 published standards, 3 which include both CEN/CENELEC developments and ISO/IEC standards formally adopted as European ones. The deadline for the deliverables was set for the 30th of April 2025, but they are now expected to be completed by early 2026 [45].\n\nAlthough harmonised standards are meant to provide the structured guidance needed to implement the AI Act in practice, the Joint Research Centre (JRC) report Harmonised Standards for the European AI Act emphasises that standards should consist of a small number of horizontal requirements, applicable across different AI systems and sectors, complemented by sector-specific provisions only when strictly necessary [4]. At the same time, the report expects these horizontal requirements to be 'sufficiently prescriptive and clear' to support practical application for specific use and the identified\n\n3 See the CEN/CLC/JTC 21 Work programme and Published Standards.\n\nrisks, guiding providers in selecting appropriate tests and measures. Therefore, the report demands standards to be both generic and context-sensitive, a hardly feasible ambition. By contrast, the Standardisation Request M/593 [8] is much more straightforward and specifies in concrete terms what European standards should deliver. The request indeed calls for European standards to take into account the risks common (horizontal) to AI systems in general. Yet, notwithstanding their horizontal nature, it also makes clear that standards may-and where relevant should-provide specifications for particular domains and use cases, taking into account the intended purpose and context of use of those systems. In addition, standards must reflect the generally acknowledged state of the art and provide concrete, verifiable technical specifications, including design and development requirements, as well as verification, validation and testing procedures. Finally, even if it is not possible to cover every specific intended purpose, standards are expected to set out at least a range of technical options that providers can assess and implement in light of the purpose of their system, together with guidance on how such options should be applied. Therefore, the request explicitly acknowledges the need for a context-sensitive approach and this makes it difficult to argue that horizontal standards alone could fulfil the mandate. Horizontal standardisation may be advantageous for efficiency in development and maintainability, but when the requirements to standardise vary significantly with context-as in the case of robustness-it risks becoming overly generic and detached from actual practice.\n\nThe current work on robustness standards focuses mainly on horizontal coverage, with limited evidence of vertical specifications. The ISO/IEC 24029 series addresses robustness at a high level: Part 1 provides an overview of approaches for evaluating neural networks, Part 2 describes formal methods but has not yet been adopted as a European standard and a planned Part 3 is expected to cover methodologies for assessing adversarial robustness and distribution shift. Other planned deliverables include the AI trustworthiness framework, which treats robustness and other several dimensions, and two specifications for computer vision and NLP models. 4 Yet robustness is still addressed almost exclusively for neural networks, treated as a uniform block without differentiation by architecture, application domain, or operational constraints, leaving other types of AI systems-such as supervised, unsupervised, generative and general-purpose models-largely uncovered. To be effective, robustness standards should set clear requirements that allow providers to select relevant perturbations, tests, metrics and thresholds. Although Part 3 appears promising, its scope and level of specificity remain to be seen. It is unlikely that this part alone will be able to provide all the necessary specifications that account for different contexts. As a result, neither the current nor the planned robustness standards appear sufficient to meet the standardisation request, highlighting the need for a layered approach that links horizontal standards to more detailed specifications.\n\n## 7 Context-sensitive standardisation through a multi-layered approach\n\nHorizontal standards provide a useful common core, but they must explicitly incorporate contextual hooks and minimal evidence requirements to adequately address the context-sensitive nature of robustness. Completely changing the current standardisation process may not be feasible, but its course can be adjusted. Horizontal standards could be designed as a starting point that establishes high-level principles, shared terminology and general best practices, while linking to more detailed standards that address domain-specific requirements. A multi-layered approach would ensure coherence through common principles set in the horizontal documents and adaptability through tailored provisions for specific risks and contexts, improving auditability and allowing organisations to follow clear directions without having to interpret abstract requirements. In what follows, we propose a framework, summarised in Figure 2, that directly addresses the shortcomings of current standards. It aims to reduce ambiguity by going beyond horizontal provisions and by identifying domain-specific risks across the AI lifecycle, guiding the choice of best practices for compliance with the AI Act context-sensitive requirements. Given the static nature of standards, which risks guiding to obsolete methodologies, as technology evolves faster than bureaucratic processes, the framework proposes a context-sensitive and dynamic repository of recommended practices where stakeholders can propose new methods and experiences. This would enable the ESOs to track technological developments, update practices more efficiently and provide a shared platform that supports providers both in the design and the validation of their systems. This proposal also supports the objectives of the so-called HAS assessment process checklist. 5 In particular, it better satisfies two central criteria: that the\n\n4 Deliverables listed in the 'Complete overview of the work programme' document on JTC21.\n\n5 A procedure to evaluate if a deliverable complies with standardisation requests and EU legal requirements.\n\nFigure 2: Multi-layered framework for context-sensitive robustness evaluation under the AI Act.\n\n<!-- image -->\n\nmandatory parts of a standard are written clearly, leaving no room for arbitrary choices, and that risk assessment is complete, with evidence that all relevant risks have been considered. The goal is not to replace existing processes but to create a more structured framework where horizontal and context-sensitive standards complement each other, bringing clarity at the top level while respecting the diversity of needs on the ground.\n\n## 7.1 Domain-specific standardisation\n\nRobustness standards need to be domain-specific. AI is a broad umbrella term that includes a large number of subfields [46]. The AI Act identifies several high-risk application areas (see Section 5.1) and adapting horizontal rules to such diverse applications is problematic, since each domain has unique characteristics [47]. Different contexts require distinct ethical considerations and the requirements for AI systems change depending on the underlying technology and domain in which they are deployed. Domain-specific standardisation enables legal obligations to be translated into measurable criteria and better aligned with sectoral ethical priorities [48]. For example, regulations that limit the use of patient data, suitable in general AI applications, may need to be modified in healthcare to allow the use of de-identified records [49], so that the ethical principles that guide the industry, such as autonomy, beneficence, non-maleficence and justice, are respected [50]. Domain-specific standards do not entail fragmentation, as long as they are built on horizontal ones and would ensure completeness, alignment with general requirements and prevent ambiguities or arbitrary interpretations.\n\n## 7.2 Context-sensitive perturbation taxonomy\n\nSoler Garrido et al. [51] highlight the need for guidance on how to set acceptable thresholds for different robustness methods, taking into account the context of use and risks of specific AI systems. Building on this, for each domain, there should be an exhaustive perturbation taxonomy. As discussed in Section 5, perturbation identification is context-sensitive. Therefore, guidelines should map potential perturbations across the contextual drivers-use case, data, model-illustrate their root causes and suggest mitigation strategies. Ultimately, at each stage of implementation, providers decide the direction the system should take based on the perturbations identified up to that point. A similar proposal was advanced by Schnitzer et al. [52], who proposed a taxonomy of AI hazards linked to lifecycle stages. Although mainly focused on DNN-related hazards, it offers a solid foundation that could be adapted both to robustness deliverables and to other requirements.\n\n## 7.3 Dynamic repository of best practices\n\nTo address robustness tests, metrics and thresholds, it is necessary to refer to state-of-the-art practices, understood as widely accepted good practice at a given time [53]. To support this, our framework proposes the development of a repository of best practices, maintained by the ESOs and informed by stakeholder input. The repository should link these best practices and their limitations to the relevant risks to robustness-and to other requirements-that may arise during development and deployment, thereby supporting their evaluation and mitigation, including mitigation measures against biased outputs arising from feedback loops. Given the rapid evolution of technology and the risk of standards obsolescence, the repository should allow providers to share experiences and propose new informative methodologies, complementing recommended practices until official updates are made. The proposed methodology would inform the ESOs of the areas requiring updates, making the process faster, while shared experiences would support providers in selecting effective methods for their use cases and aligning system development with regulatory expectations. Since identifying all possible risks is unrealistic, standards will supply a range of technical options that providers can assess and implement\n\nin light of the purpose of their system, while the repository supports providers in selecting those most appropriate for their context. By remaining flexible, outcome-focused and adaptable to different system contexts, it ensures technological neutrality and adherence to performance-based principles. A similar initiative is the Artificial Intelligence Measurement and Evaluation (AIME) [54], launched by the U.S. agency NIST, which develops metrics and methods for robustness and performance in areas such as language, vision and robotics. The program does not offer a centralised repository, but in collaboration with public and private sectors, NIST has developed a voluntary framework to manage AI-related risks for individuals, organisations and society. 6 The OECD has also developed a Catalogue of Tools &amp; Metrics for Trustworthy AI, 7 which collects practices and metrics for fairness, transparency, explainability, robustness and safety. While it offers a valuable foundation, it is not up to date nor designed for context-sensitivity, but it could serve as a reference for a dynamic European AI repository.\n\n## 7.4 External validation\n\nContext-sensitivity should also be extended to the external validation tools referenced in the AI Act: benchmarks and sandboxes. The repository should indicate which benchmarks, used as postdevelopment evaluation tools, are most suitable for different tasks and categories of high-risk systems. While a single benchmark can cover multiple systems (see Section 5.3), interpretation of results must remain context-sensitive and should be interpreted with care [55], therefore, sharing experiences would help providers better contextualise outcomes and highlight practical limitations. Moreover, benchmarks risk becoming obsolete within months [56], making the repository crucial for collecting and updating proposals for new benchmarks. Since benchmarks can fail to account for data or domain shifts, regulatory sandboxes introduced by the AI Act (Art. 57) provide a valuable complement to ensure that systems are fit for deployment. Sandboxes are controlled environments, established and supervised by national competent authorities, that facilitate technical testing under real or close-to-real conditions, providing resources, data and testing infrastructure [57]. Yet, like best practices and benchmarks, sandboxes face context-sensitivity due to the different needs of systems across sectors, where a one-size-fits-all solution is not feasible. As Due et al. [58] note, stakeholders already stress the difficulty of designing a sandbox that can cover all participants' needs, reinforcing the case for domain-specific sandboxes, especially for high-risk AI systems, even if the Act mandates horizontal national ones. Including context-sensitive sandboxes in the repository would guide use-case-specific testing and experience sharing. In this way, the repository becomes a practical tool to support both model design and model evaluation, enabling AI systems to be iteratively improved, compliant and aligned with context-specific requirements.\n\n## 8 Conclusions\n\nRobustness has a context-sensitive nature. Its assessment should follow a structured approach that considers the system's intended use, the data available and the model architecture that would best fit the goal. The analysis of each of these contextual drivers narrows down the perturbations the system should be robust against and enables the selection of appropriate tests, evaluation and mitigation metrics. Shifts in contextual assumptions can lead to substantial differences in the assessment process. Since only harmonised standards confer presumption of conformity under the AI Act, this variability must be accounted for in their ongoing development. The current approach aims to design standards applicable to various types of AI systems across sectors. However, some requirements, such as robustness, are highly context-sensitive and horizontal standardisation may result in high-level directions that are operationally meaningless, leaving too much space for interpretation that could cause superficial compliance.\n\nStandards could be designed with a multi-layered approach that establishes high-level principles, but then points to specific methodologies in each domain. To ensure robustness, a perturbation taxonomy is needed and it should be mapped into the system's lifecycle, guiding providers in the selection of the perturbations to test, evaluation methods and benchmarks. These should be listed in a repository designed for temporal adaptability to prevent technological obsolescence. The repository should also enable providers to share experiences and suggest new methods. Such contributions, while\n\n6 See NIST AI Risk Management Framework\n\n7 See OECD Catalogue\n\nonly informative, could complement recommended practices and inform future updates. In this way, robustness can be treated as a context-sensitive requirement, assessed through precise yet evolving standards that ensure compliance with the AI Act.\n\n## References\n\n- [1] European Parliament and Council. Artificial intelligence act. EU's Official Journal , Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence (Artificial Intelligence Act), 2024.\n- [2] European Commission. Building trust in human-centric artificial intelligence. Communication from the commission to the European Parliament, the Council, the European Economic and Social Committee and the Committee of the Regions: COM (2019) 168 final 8.4. 2019 , 2019.\n- [3] Luciano Floridi, Matthias Holweg, Mariarosaria Taddeo, Javier Amaya, Jakob MÃ¶kander, and Yuni Wen. Capai-a procedure for conducting conformity assessment of ai systems in line with the eu artificial intelligence act. Available at SSRN 4064091 , 2022.\n- [4] Josep Soler Garrido, Sarah De Nigris, Elias Bassani, Ignacio Sanchez, Tatjana Evas, AntoineAlexandre AndrÃ©, and Thierry BoulangÃ©. Harmonised standards for the european ai act. Technical Report JRC139430, Joint Research Centre (JRC), European Commission, 2024.\n- [5] Timo Freiesleben and Thomas Grote. Beyond generalization: a theory of robustness in machine learning. Synthese , 202(109), 2023.\n- [6] CEN/CLC ISO/IEC/TR 24029-1. Artificial intelligence (AI)-assessment of the robustness of neural networks - part 1: Overview (iso/iec tr 24029-1:2021). Technical report, European Committee for Standardisation and European Committee for Electrotechnical Standardisation, 2023.\n- [7] CEN-CENELEC JTC 21. About the joint technical committee, 2025. URL https://jtc21. eu/about/ .\n- [8] Standardisation Request M/593. Commission Implementing Decision of 22 May 2023 on a standardisation request to the European Committee for Standardisation and the European Committee for Electrotechnical Standardisation in support of Union policy on artificial intelligence. Official Journal of the European Union, C/2023/3259, 2023. Available at: https://ec.europa.eu/ transparency/documents-register/detail?ref=C(2023)3215&amp;lang=en .\n- [9] Andrea Tocchetti, Lorenzo Corti, Agathe Balayn, Mireia Yurrita, Philip Lippmann, Marco Brambilla, and Jie Yang. AI robustness: a human-centered perspective on technological challenges and opportunities. ACM Computing Surveys , 57(6):1-38, 2025.\n- [10] Ardavan Salehi Nobandegani, Kevin da Silva Castanheira, Timothy O'Donnell, and Thomas R Shultz. On robustness: An undervalued dimension of human rationality. In CogSci , page 3327, 2019.\n- [11] Nathan Drenkow, Numair Sani, Ilya Shpitser, and Mathias Unberath. A systematic review of robustness in deep learning for computer vision: Mind the gap?, 2021.\n- [12] Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony Hartshorn, and Marta Kwiatkowska. Assessing robustness of text classification through maximal safe radius computation. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pages 2949-2968. Association for Computational Linguistics, 2020.\n- [13] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Lixuan Zhu, Samyak Parajuli, Mike Guo, Dawn Xiaodong Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-ofdistribution generalization. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pages 8320-8329, 2020.\n- [14] BS EN ISO/IEC 25059. Software engineering - systems and software quality requirements and evaluation (SQuaRE) - quality model for AI system. Technical report, International Organisation for Standardisation (ISO), 2024.\n\n- [15] AurÃ©lien GÃ©ron. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems . \" O'Reilly Media, Inc.\", 2022.\n- [16] Francois Chollet and FranÃ§ois Chollet. Deep learning with Python . Simon and Schuster, 2021.\n- [17] Alan Balendran, CÃ©line Beji, Florie Bouvier, Ottavio Khalifa, Theodoros Evgeniou, Philippe Ravaud, and RaphaÃ«l Porcher. A scoping review of robustness concepts for machine learning in healthcare. npj Digital Medicine , 8(1):38, 2025.\n- [18] Joaquin QuiÃ±onero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. Dataset shift in machine learning . Mit Press, 2022.\n- [19] Chengyu Dong, Liyuan Liu, and Jingbo Shang. Data quality matters for adversarial training: An empirical study. arXiv preprint arXiv:2102.07437 , 2021.\n- [20] Haseeb Javed, Shaker El-Sappagh, and Tamer Abuhmed. Robustness in deep learning models for medical diagnostics: security and adversarial challenges towards robust AI applications. Artificial Intelligence Review , 58(12), 2025. doi: 10.1007/s10462-024-11005-9.\n- [21] Jielin Qiu, Yi Zhu, Xingjian Shi, Florian Wenzel, Zhiqiang Tang, Ding Zhao, Bo Li, and Mu Li. Benchmarking robustness of multimodal image-text models under distribution shift, 2022.\n- [22] Christopher D. Manning, Prabhakar Raghavan, and Hinrich SchÃ¼tze. Introduction to information retrieval, 2008.\n- [23] Zhun Deng, Cynthia Dwork, Jialiang Wang, and Yao Zhao. Architecture selection via the trade-off between accuracy and robustness, 2019.\n- [24] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019.\n- [25] Firuz Juraev, Mohammed Abuhamad, Simon S Woo, George K Thiruvathukal, and Tamer Abuhmed. Impact of architectural modifications on deep learning adversarial robustness. In 2024 Silicon Valley Cybersecurity Conference (SVCC) , pages 1-7, 2024. doi: 10.1109/ SVCC61185.2024.10637362.\n- [26] Narmin Ghaffari Laleh, Daniel Truhn, Gregory Patrick Veldhuizen, Tianyu Han, Marko van Treeck, Roman D. Buelow, Rupert Langer, Bastian Dislich, Peter Boor, Volkmar Schulz, and Jakob Nikolas Kather. Adversarial attacks and adversarial robustness in computational pathology. Nature Communications , 13(1):5711, 2022.\n- [27] Sunita Roy, Ranjan Mehera, Rajat Kumar Pal, and Samir Kumar Bandyopadhyay. Hyperparameter optimization for deep neural network models: a comprehensive study on methods and techniques. Innovations in Systems and Software Engineering , pages 1-12, 2023.\n- [28] Christian Arnold, Luka Biedebach, Andreas KÃ¼pfer, and Marcel Neunhoeffer. The role of hyperparameters in machine learning models and how to tune them. Political Science Research and Methods , 12(4):841-848, 2024.\n- [29] Houssem Ben Braiek and Foutse Khomh. Machine learning robustness: A primer. In Trustworthy AI in Medical Imaging , pages 37-71. Elsevier, 2025.\n- [30] Qinkai Zheng, Xu Zou, Yuxiao Dong, Yukuo Cen, Da Yin, Jiarong Xu, Yang Yang, and Jie Tang. Graph robustness benchmark: Benchmarking the adversarial robustness of graph machine learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) , 2021.\n- [31] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings , 2013.\n\n- [32] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: Abenchmark of in-the-wild distribution shifts. In International conference on machine learning , pages 5637-5664. PMLR, 2021.\n- [33] Carlos Lassance, Vincent Gripon, Jian Tang, and Antonio Ortega. Structural robustness for deep learning architectures. In 2019 IEEE Data Science Workshop (DSW) , pages 125-129. IEEE, 2019.\n- [34] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems , 27, 2014.\n- [35] Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. Deeptest: Automated testing of deepneural-network-driven autonomous cars. In Proceedings of the 40th international conference on software engineering , pages 303-314, 2018.\n- [36] Zhi Quan Zhou and Liqun Sun. Metamorphic testing of driverless cars. Communications of the ACM , 62(3):61-67, 2019.\n- [37] Weijun Shen, Jun Wan, and Zhenyu Chen. MuNN: Mutation analysis of neural networks. In 2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C) , pages 108-115. IEEE, 2018.\n- [38] Lei Ma, Fuyuan Zhang, Jiyuan Sun, Minhui Xue, Bo Li, Felix Juefei-Xu, Chao Xie, Li Li, Yang Liu, Jianjun Zhao, et al. Deepmutation: Mutation testing of deep learning systems. In 2018 IEEE 29th international symposium on software reliability engineering (ISSRE) , pages 100-111. IEEE, 2018.\n- [39] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. In NeurIPS 2021 Datasets and Benchmarks Track (Round 2) , 2020.\n- [40] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.\n- [41] Carola Doerr, Hao Wang, Furong Ye, Sander Van Rijn, and Thomas BÃ¤ck. Iohprofiler: A benchmarking and profiling tool for iterative optimization heuristics. CoRR , abs/1810.05281, 2018.\n- [42] Hao Wang, Diederick Vermetten, Furong Ye, Carola Doerr, and Thomas BÃ¤ck. Iohanalyzer: Detailed performance analyses for iterative optimization heuristics. ACM Transactions on Evolutionary Learning and Optimization , 2(1):1-29, 2022.\n- [43] Seelwan Sathitratanacheewin, Panasun Sunanta, and Krit Pongpirul. Deep learning for automated classification of tuberculosis-related chest x-ray: dataset distribution shift limits diagnostic performance generalizability. Heliyon , 6(8), 2020.\n- [44] Robert Osazuwa Ness, Katie Matton, Hayden Helm, Sheng Zhang, Junaid Bajwa, Carey E Priebe, and Eric Horvitz. Medfuzz: Exploring the robustness of large language models in medical question answering, 2024. Submitted to ICLR 2025.\n- [45] Hadrien Pouget, Koen Holtman, and Tekla Emborg. Standard setting overview | eu artificial intelligence act, 2025. URL https://artificialintelligenceact.eu/ standard-setting-overview/ . Updated: 21 July 2025.\n- [46] Inga Ulnicane, William Knight, Tonii Leach, Bernd Carsten Stahl, and Winter-Gladys Wanjiku. Framing governance for a contested emerging technology: insights from ai policy. Policy and Society , 40(2):158-177, 2021.\n- [47] Sandeep Reddy. Navigating the ai revolution: the case for precise regulation in health care. Journal of medical Internet research , 25:e49989, 2023.\n\n- [48] MÃ©lanie Gornet. Too broad to handle: can we\" fix\" harmonised standards on artificial intelligence by focusing on vertical sectors?, 2024. HAL open archive.\n- [49] Jessica Morley, Caio C.V. Machado, Christopher Burr, Josh Cowls, Indra Joshi, Mariarosaria Taddeo, and Luciano Floridi. The ethics of ai in health care: a mapping review. Social science &amp;medicine , 260:113172, 2020.\n- [50] Tom L Beauchamp and James F Childress. Principles of biomedical ethics . Edicoes Loyola, 1994.\n- [51] Josep Soler Garrido, Delia Fano Yela, Cecilia Panigutti, Henrik Junklewitz, Ronan Hamon, Tatjana Evas, Antoine-Alexandre AndrÃ©, and Salvatore Scalzo. Analysis of the preliminary ai standardisation work plan in support of the ai act. Technical report, Joint Research Centre (JRC), European Commission, 2023.\n- [52] Ronald Schnitzer, Andreas Hapfelmeier, Sven Gaube, and Sonja Zillner. Ai hazard management: A framework for the systematic management of root causes for ai risks. In International Conference on Frontiers of Artificial Intelligence, Ethics, and Multidisciplinary Applications , pages 359-375. Springer, 2023.\n- [53] Standardisation request M/593 Annexes. Annexes to the Commission Implementing Decision of 22 May 2023 on a standardisation request to the European Committee for Standardisation and the European Committee for Electrotechnical Standardisation in support of Union policy on artificial intelligence. Official Journal of the European Union, C/2023/3259, 2023.\n- [54] AIME Planning Team. Artificial intelligence measurement and evaluation at the national institute of standards and technology. National Institute of Standards and Technology , 2021.\n- [55] Lena Maier-Hein, Matthias Eisenmann, Annika Reinke, Sinan Onogur, Marko Stankovic, Patrick Scholz, Tal Arbel, Hrvoje Bogunovic, Andrew P Bradley, Aaron Carass, et al. Why rankings of biomedical image analysis competitions should be interpreted with care. Nature communications , 9(1):5217, 2018.\n- [56] Laura Weidinger, Deb Raji, Hanna Wallach, Margaret Mitchell, Angelina Wang, Olawale Salaudeen, Rishi Bommasani, Sayash Kapoor, Deep Ganguli, Sanmi Koyejo, et al. Toward an evaluation science for generative ai systems, 2025.\n- [57] Tambiama Madiega and Anne Louise Van De Pol. Artificial intelligence act and regulatory sandboxes. European Parliamentary Research Service , 6, 2022.\n- [58] Stig A. Due, Hira Shah, Thiago Moraes, Nathan Genicot, and Martin Canter. Sandboxing artificial intelligence: Balancing innovation, regulation, and stakeholder needs. https://www.fari.brussels/research-and-innovation/publication/ sandboxing-artificial-intelligence , 2025. FARI Institute.", "fetched_at_utc": "2026-02-09T13:55:53Z", "sha256": "9759edc205d7d92f9df6e8abf96bb9fd7727c112595d917c1bf05a5d0a9382f3", "meta": {"file_name": "Lost in Vagueness - Towards Context-Sensitive Standards for Robustness Assessment under the EU AI Act.pdf", "file_size": 527692, "mtime": 1770643212, "docling_errors": []}}
{"doc_id": "pdf-pdfs-machinery-817608aeb447", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Machinery.pdf", "title": "Machinery", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:01:05Z", "sha256": "817608aeb4472447eb5365837c82d0060bda320da2f217eeb9db8824a4da7d19", "meta": {"file_name": "Machinery.pdf", "file_size": 53182431, "mtime": 1766951848, "docling_errors": []}}
{"doc_id": "pdf-pdfs-navigating-the-eu-ai-act-foreseeable-challenges-in-qualifying-deep-learning-b0924fed24ef", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Navigating the EU AI Act - Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices.pdf", "title": "Navigating the EU AI Act - Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices", "text": "## Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices\n\nJulio Zanon Diaz 1* ,  Tommy Brennan 2 â€  and Peter Corcoran 1 â€ \n\n1* School of Electrical and Electronic Engineering, University Of Galway, University Road, Galway, H91 TK33, Ireland. 2 Visual-Cognitive Manufacturing Group, Digital Manufacturing Ireland,\n\nCastletroy, V94 237R, Co. Limerick, Ireland.\n\n*Corresponding author(s). E-mail(s):\n\nContributing authors: Tommy.Brennan@DMIreland.org;\n\nJ.ZanonDiaz1@UniversityOfGalway.ie; Peter.Corcoran@UniversityOfGalway.ie;\n\nâ€  These authors contributed equally to this work.\n\n## Abstract\n\nAs  deep  learning  (DL)  technologies  advance,  their  application  in  automated visual  inspection  for  Class  III  medical  devices  offers  significant  potential  to enhance  quality  assurance  and  reduce  human  error.  However,  the  adoption of such AI-based systems introduces new regulatory complexities-particularly under  the  EU  Artificial  Intelligence  (AI)  Act,  which  imposes  high-risk  system obligations  that  differ  in  scope  and  depth  from  established  regulatory  frameworks such as the Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation (QSR). This paper presents a high-level technical assessment of  the  foreseeable  challenges  that  manufacturers  are  likely  to  encounter  when qualifying DL-based automated inspections-specifically static models-within the  existing  medical  device  compliance  landscape.  It  examines  divergences  in risk management principles, dataset governance, model validation, explainability  requirements,  and  post-deployment  monitoring  obligations.  The  discussion also explores potential implementation strategies and highlights areas of uncertainty, including data retention burdens, global compliance implications, and the practical difficulties of achieving statistical significance in validation with limited defect data. Disclaimer: This paper presents a technical perspective and does not constitute legal or regulatory advice.\n\n## 1 Automated Visual Inspection of Class III Medical Devices\n\nThe  commercialisation  and  manufacture  of  medical  devices  are  subject  to  regulation by various authorities, each corresponding to a specific geographical jurisdiction. These regulatory bodies implement distinct frameworks that generally include a riskbased classification system, grouping devices by product family or level of risk. Devices assigned  to  the  highest  classification  tier  are  subject  to  the  most  stringent  regulatory oversight. Typically, these include devices that are life-sustaining, life-supporting, implanted, or those that pose a significant risk of illness or injury if they fail. Examples include implantable stents, pacemakers, heart valves, and deep brain stimulators. Table A1 in Annex A presents a summary of the principal regulatory authorities and the corresponding medical device classification systems employed by each.\n\nThis paper  will concentrate  exclusively on devices within the highest risk category,  as  these  present  the  most  pronounced  challenges  in  qualification  and  regulatory  compliance.  Despite  notable  regional  differences  in  regulations  governing both  commercialisation  and  manufacturing,  requirements  concerning  manufacturing practices-particularly those related to visual inspection-are closely harmonised.\n\nDisclaimer: This  publication  is  intended  solely  as  an  academic  and  technical evaluation. It is  not a  substitute for  legal  advice or  official  regulatory  interpretation. The information presented here should not be relied upon to demonstrate compliance with the EU AI Act or any other statutory obligation. Manufacturers are encouraged to  consult  appropriate regulatory authorities and legal experts to determine specific compliance pathways.\n\nIn  line  with  current  regulatory  expectations  under  both  the  EU  Medical  Device Regulation  (MDR)  and  the  U.S.  FDA  Quality  System  Regulation  (QSR),  this  assessment focuses  exclusively  on  static  deep  learning  models-that  is,  models  which  are trained  prior  to  deployment  and  not  subject  to  continual  or  periodic  retraining  in production.  This  reflects  existing  validation  obligations  for  manufacturing  software, which  require  that  software  tools  affecting  product  quality  be  fully  validated  prior to use and revalidated following any changes [1] [2]. As such, adaptive or retrainable models would fall outside the scope of current best practices and are not considered in this  analysis.  For  the  purposes  of  this  analysis,  the  focus  will  be  on  Class  III  devices as  defined  by  European  Commission  under  the  Medical  Device  Regulation  (MDR), specifically  Article  51  and  Annex  VIII  -  Classification  Rules[3],  and  by  the  United States Food and Drug Administration (FDA) under 21 CFR Part 860 - Medical Device Classification Procedures [4].\n\nVisual  inspection  is  a  widely  employed  method  of  quality  assurance  in  the  manufacturing  of  Class  III  medical  devices.  These  inspections  continue  to  play  a  critical\n\nrole in ensuring product quality and are predominantly performed by human operators, owing to their unique capabilities and contextual judgement, as noted by Charles et  al.  [5].  For  conciseness,  we  will  refer  to  human-conducted  visual  inspections  as Human-Visual  Inspections  (HVI),  and  those  performed  using  computerised  logic  as Automated-Visual Inspections (AVI).\n\nManufacturers  in  this  sector  typically  implement  multiple  inspection  stages  to verify compliance with strict specifications. Rodriguez-Perez [6] underscores the industry's  reliance  on  such  inspections  and  highlights  a  significant  prevalence  of  human error-estimated  to  contribute  to  approximately  one-third  of  all  non-conformances in  medical  device  and  pharmaceutical  manufacturing.  Consequently,  over  the  past decade,  there  has  been  a  marked  trend  toward  investing  in  AVIs  through  the  application  of  conventional  computer  vision  technologies.  However,  these  systems  face challenges  when  dealing  with  complex  inspection  tasks,  particularly  due  to  their reliance on precisely defined, objective defect criteria. In contrast, HVIs often operates on  subjective  criteria  that  are  readily  interpreted  by  humans  but  difficult  to  encode into software.\n\nThe  emergence  of  Deep  Learning  (DL),  particularly  through  the  use  of  Convolutional  Neural Networks (CNNs) for image analysis, has enabled engineers to explore the  automation  of  inspections  even  when  subjectivity  is  involved.  These  technologies  can  learn  from  labelled  image  data,  previously  annotated  by  human  experts, thereby  bridging  the  gap  between  subjective  human  judgement  and  machine-based analysis. While this development represents a significant opportunity for innovation in manufacturing, it also introduces new challenges-chiefly, the absence of comprehensive regulatory guidance for the qualification and deployment of DL-based inspection systems within the medical device manufacturing framework.\n\n## 2 Applicable Medical Device Regulatory Framework\n\nThis chapter explores the principal regulations and standards applicable to automated inspection  systems  employed  in  the  manufacture  of  Class  III  medical  devices,  with particular  reference  to  the  frameworks  established  by  European  and  United  States regulatory  authorities.  Within  the  European  Union,  the  principal  legislative  instrument governing the manufacture of medical devices is the Medical Device Regulation (MDR) (EU) 2017/745. For  the  purposes  of  a  focused  and  relevant  discussion,  special cases such as drug-device combinations, where Class III implantable devices are used  to  deliver  medicinal  substances-for  example,  drug-eluting  stents  (DES)-will be excluded. In such instances, while the device remains subject to the Medical Device Regulation  (MDR),  a  scientific  consultation  with  the  European  Medicines  Agency (EMA)  or  a  national  medicines  authority  may  be  required  regarding  the  medicinal component.  Compliance  with  the  MDR  is  generally  achieved  through  adherence  to designated harmonised standards, which serve as a presumption of conformity. Among the most relevant harmonised standards for manufacturing are:\n\n- EN  ISO  13485:2016+A11:2021 -Quality  management  systems  -  Requirements for regulatory purposes [1].\n\n- EN  ISO  14971:2019+A11:2021 -Application  of  risk  management  to  medical devices [7].\n\nIt is also important to acknowledge the existence of standards tailored to Software as a Medical Device (SaMD), some of which address specific requirements for machine learning applications. These standards however are not applicable for software used to manufacture Medical Devices and therefore has not been included in the assessment. In the United States, the primary regulatory framework for medical device manufacture is  established  by  the  Food  and  Drug  Administration  (FDA)  and  includes:\n\n- 21 CFR Part 820 -Quality System Regulation (QSR) [2].\n- 21 CFR Part 11 -Electronic Records and Signatures [8].\n\nNotably, the FDA recognises ISO 13485 and ISO 14971 as acceptable standards for demonstrating regulatory compliance, and significant efforts are underway to formally harmonise ISO 13485 within the forthcoming Quality Management System Regulation (QMSR) framework [9]. Beyond these legally binding regulatory requirements, specific to  medical devices, several generic quality standard and non-harmonised but widely accepted industry guidelines are often used by manufacturers to strengthen internal quality procedures. Although these are not covered in detail within this assessment, it is useful to acknowledge the most commonly referenced:\n\n- ISO 9000:2015 -Quality Management Systems - Fundamentals and vocabulary [10].\n- ISPE  GAMP 5 -Good Automated Manufacturing Practice [12].\n- ISO/TR  80002-2:2017 -Guidance  on  validating  software  used  in  quality systems (non-product software) [11].\n- GHTF SG3/N99-10:2004 -Quality  Management  Systems  -  Process  Validation Guidance [13].\n- FDA Guidance -Process Validation: General Principles and Practices (2011) [14].\n\n## 2.1 Quality Management System and Risk Management\n\nAutomated inspection systems must be implemented within a documented and validated  Quality  Management  System  (QMS)  and  governed  by  a  risk-based  approach. The combined expectations include:\n\n- [ISO  13485:2016 [1] ,  Clauses  4.1,  4.2.4,  4.2.5,  7.1,  8.1 ]  A  QMS  must  be established that is tailored to medical devices, incorporating document and record control, traceability, change control, and regulatory compliance .\n- [ISO 13485:2016 [1] ,  Clause  4.1.6;  21  CFR  Â§820.70(i) [2] ;  21  CFR  Â§11.10(a) [8]]  Inspection  software  (e.g.,  machine  vision)  must  be  validated  for  its  intended use, with documented protocols, verification results, and version control .\n- [ISO  13485:2016 [1] ,  Clauses  7.5.1,  7.5.6;  21  CFR  Â§820.70,  Â§820.75 [2]] Inspection processes (manual or automated) must be defined using pre-approved, objective, and risk-aligned acceptance criteria, and validated if their outputs cannot be verified through subsequent inspection.\n\n- [ISO 13485:2016 [1] , Clause 7.6; 21 CFR Â§820.72 [2]]  Inspection  equipment  (e.g., image acquisition apparatus, measurement tools) must be qualified, calibrated, and maintained to ensure accuracy and suitability.\n- [ISO 14971:2019 [7] , Clause 7, 7.2, 7.3, 10 ]  Inspection  processes  must  be  embedded  in  the  device  risk  management  strategy,  with  documented  identification  of potential failure modes, including:\n- -Environmental factors (e.g., lighting),\n- -Operator error (for semi-automated systems),\n- -Software  limitations  (e.g.,  misclassification).\n- [ISO 14971:2019 [7] , Clause 7.3, Clause 10 ]  Effectiveness  of  risk  control  measures must be verified and monitored for residual risk throughout production and postmarket phases .\n\n## 2.2 Data Integrity and Inspection Results\n\nWhere inspection systems generate or rely on electronic records, full compliance with data integrity, traceability, and security controls is mandatory:\n\n- [21 CFR Â§11.10(b)-(k) [8]]  Electronic  records,  including  pass/fail  outcomes  and batch disposition data, must be:\n- -Secure, retrievable, and human/electronically readable,\n- -Audit-trailed  with  timestamps,  operator  identity,  and  record  of  parameter changes,\n- -Protected against unauthorised access via role-based permissions .\n- [21  CFR  Â§11.10(f),(g) [8] ;  ISO  13485:2016 [1] ,  Clause  6.2 ]  Access  control  and device checks must be enforced, and operators must be trained on inspection system operation and limitations.\n- [ISO 13485:2016 [1] , Clauses 7.5.1, 8.2.6; 21 CFR Â§820.80, Â§820.181 [2]] Inspection  outcomes  must  be  recorded  and  linked  to  product  identifiers  in  the  Device History  Record  (DHR),  retained  for  regulatory  inspection.\n- [21  CFR  Â§11.10(j)-(k) [8]]  Documentation  control  procedures  must  manage system updates, change history, and user accountability.\n- [ISO 13485:2016 [1] , Clause 8.3; 21 CFR Â§820.100 [2]] Failed inspection results must be subject to nonconformance handling and root cause analysis as part of the Corrective  and  Preventive  Action  (CAPA)  system.\n- [ISO 13485:2016 [1] ,  Clause  4.1.6;  21  CFR  Â§820.70(i) [2] ;  21  CFR  Â§11.10(a) [8]]  Any  modification  to  inspection  software,  acceptance  limits,  or  configuration workflows must follow change control and revalidation protocols.\n\n## 2.3 Qualification  Activities\n\n- [GAMP 5, Appendix D ] Installation Qualification (IQ) - Verifies correct system setup, including:\n- -Hardware (e.g., camera models, GPU types),\n\n- -Firmware (e.g., driver/firmware versions),\n- -Calibration  validation  (e.g.,  pixel-to-millimetre  transformations),\n- -Software code traceability (e.g., script version used for build).\n- [GAMP  5]  Operational  Qualification  (OQ)  -  Confirms  functionality  under expected operating conditions, verifying that:\n- -Inputs generate valid outputs,\n- -Inspection logic performs within defined ranges,\n- -Records comply with Part 11 or ISO 13485 data requirements.\n- [GAMP  5;  ISO  13485:2016 [1] ,  Clause  7.5.6 ]  Performance  Qualification  (PQ) -  Also  referred  to  as  process  validation,  Ensures  repeatability  and  consistency  in production, typically involving Test Method Validation (TMV) with variable and/or attribute data under normal use conditions.\n- [Taylor [15]] Risk, Confidence, Reliability: Organisations should classify inspection  risks  based  on  severity  and  detection  likelihood.  Sample  sizes  for  validation should  be  calculated  using  statistical  methods.  The  Success-Run  Theorem  is  an example of a widely accepted method for calculating sample sizes for process validation  of  attribute  inspection  methods.  This  approach  assumes  no  false  negatives are  permitted  during  the  validation  study.  There  are  alternative  statistical  methods  that  allow  for  a  defined  number  of  false  negatives,  these  typically  require significantly larger sample sizes.\n\nwhere,\n\nConfidence is  the  statistical  likelihood  that  the  validation  conclusion  is  correct. In  TMV,  it  reflects  how  sure  we  are  that  the  test  method  performs  as  intended, based  on  the  validation  data.  For  example,  if  the  confidence  level  is  95%,  then  if the validation were repeated many times, 95% of those validations would correctly confirm the method's performance.\n\nTypical values for High-risk inspections are Confidence= 95% and Reliability = 99%.\n\nReliability is  the  probability that the test method will consistently detect a defect or  produce  a  correct  result  under  defined  conditions.  For  example,  if  Reliability is  99%,  then  the  test  method  will  correctly  detect  the  defect  (or  give  the  correct result) 99 times out of 100 in routine use.\n\n- [AIAG  MSA  [16]]  -  TMV  Sample  Composition-  Best  Practices:  TMV  must include a representative mix of defective and non-defective units, ideally:\n- -25-50% defective,\n- -A mix of marginal cases and clearly nonconforming items,\n- -Artificial defects if real samples are unavailable.\n- [GHTF  SG3/N99-10:2004,  Clause  6.3 ]  Sampling  plans  used  in  PQ  and  TMV must  be  statistically  justified,  particularly  where  inspection  is  a  critical  process.\n\n<!-- formula-not-decoded -->\n\n## 3 AI Regulatory Framework\n\n## 3.1 EU AI Act\n\nThe  EU  Artificial  Intelligence  (AI)  Act  [17],  which  partially  entered  into  force  on  2 August 2024, will become applicable after a transitional period of two to three years, with the majority of its provisions taking effect on 2 August 2026, as set out in Article 113. However, the full enforcement of the Act is phased. Notably, provisions related to high-risk AI systems, as defined under Article 6(1), will not come into effect until 2  August  2027.  For  AI  systems  that  were  already  placed  on  the  market  or  put  into service  prior  to  2  August  2026,  the  obligations  of  the  Act  will  only  apply  if  those systems undergo significant design modifications after  this date,  in  accordance  with Article 111(2).\n\n## 3.1.1 Harmonised Standards\n\nTo support the effective implementation of the AI Act, the European Commission has mandated the development of harmonised standards under the responsibility of  CEN-CENELEC  Joint  Technical  Committee  21  (JTC  21)  [18].  These  standards aim to provide technical specifications that facilitate compliance with the Act. The committee's work is focused on the following areas of the legislation:\n\n- AI  Trustworthiness  Framework :  Defining  criteria  for  the  development  of trustworthy AI systems; standards are currently under development.\n- AI  Quality  Management  System : Supporting  the establishment  of structured  and  consistent  AI  development  processes;  development  has  not  yet  commenced at the time of this assessment.\n- AI Risk Management :  Addressing operational risks inherent to AI systems; standards are currently under development.\n- AI Conformity Assessment :  Establishing  procedures  for  the  verification  of compliance; development has not yet commenced at the time of this assessment.\n- Record-Keeping  and  Logging :  Ensuring  comprehensive  documentation  of AI  system  operations  to  support  transparency  and  accountability;  standards  are currently under development.\n- Data  Governance  and  Quality : Developing  standards  for the responsible management and high quality of data used in AI systems; development has not yet commenced at the time of this assessment.\n- Transparency  and  Information  Provision : Standardising the information that should be provided to users and stakeholders; standards are currently under development.\n- Accuracy, Robustness, and Cybersecurity :  Setting  out  performance  and security requirements for AI systems; standards are currently under development.\n- Human Oversight :  Defining  principles  for  effective  human  control  and  intervention in AI system operations; standards are currently under development.\n- Sector-Specific  Standards :  Addressing  domain-specific  needs  such  as  those for  machine  vision  applications;  development  has  not  yet  commenced  at  the  time of this assessment.\n\nThe  following  standards  have  already  been  published  and  are  expected  to  be harmonised with the AI Act [19] [18]:\n\n- ISO/IEC 22989:2022 Information technology - Artificial intelligence - Concepts and terminology [20].\n- ISO/IEC 23894:2024 Guidance on risk management for AI systems [22] .\n- ISO/IEC 23053:2022 Framework for AI systems using machine learning [21].\n- ISO/IEC 27001:2013 Information security management systems , supporting both AI management and risk management frameworks [23].\n- ISO/IEC 42001:2023 Artificial Intelligence Management System [28] .\n- ISO/IEC 5259 Parts 1-4:2025 Data quality for analytics and machine learning [24], [25], [26], [27] .\n\nAlthough no explicit declarations have been made confirming harmonisation for the following standards, they have been referred by JTC 21 and are therefore expected to inform future standardisation activities in support of the AI Act [18]:\n\n- CEN/CLC/TR 18115:2024 Data governance and quality for AI in the European context [29] .\n- CEN/CLC ISO/IEC/TR 24029-1:2023 Assessment of the robustness of neural networks - Part 1: Overview [31] .\n- CEN/CLC  ISO/IEC/TR  24027:2023  Bias  in  AI  systems  and  AI-aided decision-making [30] .\n- EN ISO/IEC 8183:2024 Data lifecycle framework for AI [32] .\n- EN ISO/IEC 25059:2024 Quality model for AI systems [34] .\n- CEN/CLC/TR 17894:2024 AI Conformity Assessment [33] .\n- CEN/CLC ISO/IEC/TS 12791:2024 Treatment of unwanted bias in classification and regression machine learning tasks [35] .\n\nIn Annex A, Table A2 we provide a list of JTC 21 projects that are either in the planning stage or currently in progress to complete the set of harmonised standards required under the AI Act. While this list is subject to change as the Act approaches full enforcement, it is worth noting that one such planned standard pertains specifically  to  computer  vision:  JT021025 Artificial Intelligence - Evaluation Methods for Accurate Computer Vision Systems .  This  standard seeks to establish methodologies for assessing the accuracy of computer vision systems, thereby supporting compliance with the performance requirements of the EU AI Act.\n\n## 3.1.2 Scope and Assumptions of the Assessment\n\nTo  assess  the  regulatory  requirements  under  the  EU  Artificial  Intelligence  (AI)  Act pertaining to the use of Deep Learning (DL) for automated defect inspections in Class III medical device manufacturing, it is necessary to define a specific use case and make several  simplifying  assumptions.  These  assumptions  establish  the  boundaries  of  the analysis presented in this paper:\n\n1. High-Risk  Classification  Assumption:  Although  not  all  defects  inspected  by automated systems pose high-risk failure modes, this paper assumes that each automated inspection includes at least one defect associated with a high-risk condition. Medical devices classified under Class III fall within the scope of Union harmonisation legislation, specifically Regulation (EU) 2017/745 on medical devices (MDR), which  is  referenced  in  Annex  II  of  the  AI  Act.  Consequently,  any  AI  system  that functions  as  a  safety  component  in  the  manufacturing  process  of  such  devices meets the criteria for high-risk classification under Article 6(1) of the AI Act. It is acknowledged that this is an inferred classification, as neither the MDR nor current harmonised standards under the AI Act explicitly reference automated inspections. Nonetheless, it is reasonable to anticipate that future revisions of these standards will formally clarify the regulatory status of such systems.\n2. Model Lifecycle Assumption: It is assumed that automated inspection systems are subject to rigorous pre-deployment validation, both at the software and process level,  as  is  standard  practice  in  medical  device  manufacturing.  Accordingly,  DL models  used  in  this  context  are  treated  as  'static'  systems-that  is,  models  that do not undergo continual or periodic retraining once deployed into production. In the event that retraining becomes necessary, a comprehensive revalidation process would be required in accordance with regulatory expectations.\n3. Application Domain Assumption: Automated inspection systems are often used for  both  variable  inspections  (assessing  measurable  parameters)  and  attribute inspections  (assessing  pass/fail  characteristics). In  this  paper,  it  is  assumed that  traditional  computer  vision  techniques  are  generally  preferred  for  variable inspections  due  to  their  calibration  capabilities  and  alignment  with  recognised metrological standards. In contrast, DL models are assumed to be used primarily for attribute inspections, where the system learns to recognise features or patterns indicative of nonconformities. In some cases, these DL-derived classifications may serve  as  input  to  traditional  vision  algorithms,  which  then  compute  dimensional outputs.\n\n4. Standards  Coverage  Assumption:  The  overview  of  regulatory  and  technical requirements presented in this assessment is based on the currently available versions  of  harmonised  and  supporting  standards,  including  those  under  the  EU  AI Act and general quality and risk management frameworks. In the absence of specific guidance or updates to vertical standards, such as those under the Medical Device Regulation  (MDR),  which may eventually be revised to align  more  explicitly  with the  AI  Act  in  the  context  of  manufacturing,  it  is  assumed  that  existing  standards provide sufficient coverage of the core requirements relevant to automated inspec -tions for the purposes of this analysis. Future iterations of MDR-related standards are expected to offer greater clarity on compliance pathways for AI-enabled manufacturing systems, but until such guidance is formally published, this assessment proceeds on the basis that the current corpus of standards reflects the applicable regulatory expectations.\n\n## 3.2 Quality Management System and Risk Management\n\nHigh-risk AI systems-such as DL-based visual inspection tools-must operate under a  documented  AI  Management  System  (AIMS),  incorporate  full  lifecycle  risk  governance,  and  fulfil  obligations  under  the  EU  AI  Act  and  applicable  harmonised standards.\n\n- [EU AI Act, Art. 9 [19] ; ISO/IEC 42001:2023 [28] , Clauses 4-6 ] A formal AIMS must define objectives, roles, risk tolerances, and compliance procedures across the AI  lifecycle.  This  includes  change  control,  ethical  risk  appraisal,  and  linkage  to QMS-level activities in ISO 13485.\n- [ISO/IEC 23894:2023 [22] , Clauses 5.2-5.4 ]  AI  risk  management  must  explicitly address model-specific hazards, such as:\n- Dataset shift or concept drift,\n- Adversarial vulnerability,\n- Overfitting  to  test  datasets,\n- Automation bias or unwarranted confidence in model outputs.\n- [CEN/CLC  ISO/IEC/TS  12791:2024 [35] ,  Clause  7 ] Systems  must  be  evaluated  for  social,  ethical,  and  safety  impacts,  especially  when  involved  in  product acceptance decisions. Human oversight must be designed and documented.\n- [EN ISO/IEC 8183:2024 [32] , Clause 5.2 ]  The  AIMS  should  incorporate  a  data lifecycle strategy, ensuring that governance, risk control, and quality assurance are maintained from data acquisition to model decommissioning.\n- [ISO/IEC  22989:2022 [20] ,  Clause  4.3;  CEN/CLC  ISO/IEC/TR  24027:2023 [30] , Clause 6 ] The system must document how it mitigates dataset bias and ensures fairness, particularly for defect variability across product variants or manufacturing sites.\n\n## 3.3 Data Integrity and Inspection Results\n\nData  used  by  High-Risk  AI  systems  must  meet  curation,  protection,  integrity,  and auditability requirements, both during development and in production.\n\n- [EU AI Act, Art. 10; ISO/IEC 5259-2:2024 [25] ,  Clauses 5-6 ]  Datasets  used  for training, validation, and testing must be statistically representative, relevant, accurate,  and  free  from  bias.  Documentation  must  include  data  sources,  preprocessing logic, and exclusion rules.\n- [CEN/CLC/TR 18115:2024 [29] , Clause 5.3; ISO/IEC 27001:2013 [23] , Annex A.9-A.12 ] Training and test data must be stored securely, encrypted in transit and at rest, with access control. Retention duration should be defined, and must comply with the system's traceability and revalidation policies.\n- [ISO/IEC 23894:2023, Clause 6.4 [22] ; ISO/IEC 42001:2023 [28] , Clause 8.1.2 ] Data  privacy  must  be  preserved  during  acquisition  and  use,  especially  for  image data  that  could  reveal  traceable  features.  Anonymisation  or  synthetic  data  should be considered where applicable.\n\n- [EN ISO/IEC 25059:2024 [34] , Clause 5.5 ]  Inspection  results  (e.g.,  defect  classifications,  confidence  levels)  must  be  interpretable,  traceable  to  a  model  version, and evaluated for reproducibility across datasets.\n- [EN ISO/IEC 8183:2024 [32] ,  Clauses  6.3  and  6.5 ]  A  structured  data  lifecycle must  be  established  to  ensure  that  data  integrity  and  consistency  are  preserved during ingestion, transformation, storage, and archival. The framework must include procedures for data versioning, audit trails, and data quality metrics at each lifecycle phase.\n\n## 3.4 Qualification Activities and Performance Validation\n\nHigh-risk AI systems must be qualified with statistical rigour, validated against performance targets, and monitored post-deployment to detect degradation or unintended behaviour.\n\n- [EU AI  Act,  Art.  15;  ISO/IEC  5259-1:2022 [24] ,  Clause  4.2;  ISO/IEC  52594:2024 [27] ,  Clause  6.2 ]  Validation  must  include:\n- Predefined  metrics  such  as  accuracy,  precision,  recall,  F1-score,  and  false positive/negative rates,\n- Systematic testing across realistic variability conditions,\n- Detection of overfitting, such as inflated test performance without generalisation.\n- [ISO/IEC  5259-3:2024 [26] ,  Clauses  5-6;  ISO/IEC  23053:2022 [21] ,  Clauses 6.4-6.7 ] Validation protocols must be statistically sound, repeatable, and transparent. Acceptable statistical approaches include:\n- Cross-validation,\n- Stratified sampling of defect cases,\n- Application  of  Success-Run  Theorem  where  binary  (pass/fail)  outputs  are used.\n- [ISO/IEC 23894:2023 [22] , Clause 8.3; ISO/IEC 42001:2023 [28] , Clause 10.2 ] Post-deployment monitoring requirements vary based on model adaptability:\n- Static AI systems (no retraining) require:\n- âˆ— Periodic reviews,\n- âˆ— Revalidation after environment/configuration changes,\n- âˆ— Surveillance of drift in input data distributions.\n- -Retrainable AI systems must:\n- âˆ— Define criteria and approval workflows for retraining,\n- âˆ— Maintain audit trails of all  updated model versions,\n- âˆ— Conduct revalidation after each retraining cycle.\n- [CEN/CLC/TR  17894:2024 [33] ,  Clause  5.2.3 ]  A  post-market  monitoring  plan must be in place, including:\n\n- Key performance indicators (e.g. inspection yield, defect recall),\n- Root-cause investigations for out-of-tolerance inspection outcomes,\n- Processes to trigger model updates based on performance degradation.\n- [CEN/CLC ISO/IEC/TR 24029-1:2023 [31] ,  Clauses  5.2  and  6.2 ]  AI  robustness must  be  validated  through  stress  testing  and  adversarial  analysis.  This  includes simulating  edge-case  scenarios,  applying  input  perturbations,  and  evaluating  the system's tolerance to environmental variation and noise-critical in manufacturing inspection environments.\n\n## 4 Assessing Foreseeable Challenges for Deep Learning Automated Inspections of Class III Medical Devices\n\nThis  section  explores  the  foreseeable  challenges  in  adopting  the  EU  Artificial  Intelligence (AI) Act for qualifying deep learning (DL)-based automated inspections of Class III  medical devices, specifically for companies that already comply with existing regulatory  frameworks  such  as  the  Medical  Device  Regulation  (MDR)  and  the  U.S.  FDA Regulation. Rather than providing a comprehensive list of new requirements-which has been addressed in earlier sections-this analysis focuses on areas of implementation  that  may  prove  difficult  to  align  with  existing  practices.  It  draws  on  findings from the  literature  as  well  as  expert  consultation.  Given  that  harmonised  standards under the AI Act are still evolving and that the obligations for high-risk systems are not yet fully in force, there remains some degree of subjectivity in this analysis, which is expected to be clarified in future guidance and revisions.\n\n## 4.1 Quality Management System Requirements\n\nMany  requirements  related  to  establishing  an  AI  Management  System  (AIMS)including  AI-specific  risk  management-overlap  with  existing  QMS  practices  under MDR  and  FDA.  However,  the  two  frameworks  are  grounded  in  distinct  conceptual foundations.  For  example,  EN  ISO  14971:2019  (MDR)  is  derived  from  ISO  Guide 51:2014 [36] and centres on product safety and engineering risks. In contrast, ISO/IEC 23894:2024 is built on  ISO  Guide  73:2009  [37],  reflecting  a  broader  enterprise  risk management approach.\n\nThough these frameworks are not incompatible, their differing scopes and terminologies may pose challenges for alignment. It is likely that EN ISO 14971:2019, ISO 13485:2016,  or  both  will  require  revision  to  better  harmonise  with  AI-specific  risk frameworks such as ISO/IEC 23894:2024.\n\nIn addition, there is a fundamental shift in the requirement to define roles, responsibilities,  and  policies  across  the  entire  AI  lifecycle-from  development  through  to deployment  and  retirement.  In  manufacturing,  this  presents  a  challenge:  policies cannot  be  confined  to  inspection  use  cases  alone,  but  must  also  account  for  other applications (e.g.  large  language  models used in documentation, causal inference for root cause analysis, or DL-based supply chain prediction tools).\n\nWhile  smaller  organisations  may  scale  policies  incrementally,  larger  enterprises with  numerous  concurrent  initiatives  may  struggle  to  develop  AIMS  documentation (e.g.,  SOPs,  work  instructions)  that  is sufficiently comprehensive  and  future-proof.\n\n## 4.2 Image curation: Ethical Considerations and Privacy\n\nImages  used  for  DL-based  inspection  systems  typically  do  not  pose  privacy  risks, especially when generated in controlled manufacturing environments without identifiable human content. However, it is possible for background footage to unintentionally include  workers,  particularly  in  constrained  physical  layouts.  While  GDPR  [38] already addresses this issue for EU-based manufacturers, compliance with the AI Act's privacy  provisions  is  not  expected  to  introduce  additional  burdens  in  this  context. However, for manufacturers operating under U.S. state-level privacy laws-which offer varied and less prescriptive coverage for workplace surveillance-additional safeguards may be required to ensure AI Act alignment.\n\nEmerging applications where AI is used to monitor worker activity (e.g. to confirm safety-critical steps in a procedure) may trigger more complex ethical considerations. However, as this use case extends beyond product inspection, it is excluded from this assessment.\n\n## 4.3 Image Curation:  Bias  and  Fairness\n\nUnder MDR and FDA frameworks, bias and fairness are indirectly addressed through Test  Method  Validation  (TMV)  supported  by  statistically  sound  sampling  plans. These  methods  are  sufficient  for  rule -based  computer  vision,  where  functions  are explicit  and  interpretable.  For  instance,  in  a  rule-based  traditional  approach,  verifying  that  a  function  correctly  classifies  samples  based  on  the  average  brightness  of  a define region of pixels, requires only a small sample of boundary cases.\n\nHowever, Deep Learning models operate on latent features and cannot be dissected in the same way. Their ability to generalise post-deployment depends entirely on training data representativeness. Ensuring robust performance therefore requires training and  validation  datasets  that  are  statistically  significant  and  reflective  of  real-world variability.\n\nThis represents a notable departure from current practice, particularly in the medical  device  sector  where  defect  images  are  rare  and difficult to  simulate.  The  EU  AI Act introduces rigorous expectations to mitigate bias across the system lifecycle-from training  and  testing  to  TMV  and  post-deployment  monitoring.\n\nMeeting these expectations will require manufacturers to establish new processes capable  of  demonstrating  that  their  data  is  both  representative  and  balanced.\n\n## 4.4 Image Storage for Training and Inference Data\n\nEmerging  standards  (e.g.  EN  ISO/IEC  8183:2024,  CEN/CLC/TR  18115:2024)  imply that training datasets and associated metadata must be retained to ensure traceability,  auditability,  and  revalidation  capability.  Under  MDR  and  FDA,  manufacturers already retain software artefacts, model documentation, and validation records-often\n\nfor  periods  aligned  with  patient  life  expectancy  (i.e.,  70+  years).  However,  these regulations do not require the retention of raw training images.\n\nThis presents a significant challenge as typically training and validation images are high-resolution, and datasets are large. The cost of secure long-term storage as quality records  can  be  prohibitive,  particularly  at  scale.  Manufacturers  with  thousands  of inspection points may find that compliance with this requirement becomes a barrier to full AI adoption in visual inspection.\n\n## 4.5 System Output and Explainability\n\nWhile  the  AI  Act  does  not  explicitly  require  the  storage  of  inferencing  images,  it does  require  that  systems  support  traceability  and  retrospective  decision  analysis. For inspection systems, this may necessitate storing inference-stage images alongside model decisions to enable review and audit.\n\nMoreover,  harmonised  standards  (e.g.  EN  ISO/IEC  25059:2024)  recommend  the use of explainability tools, such as saliency maps or surrogate models. Although framed as  a  'should'  rather  than  a  'shall',  in  the  context  of  high-risk  applications,  this requirement is likely to be interpreted as effectively mandatory.\n\nAt present, there is limited consensus on how explainability tools can be validated for use in regulated quality systems. Techniques such as saliency maps, class activation maps,  LIME,  and  SHAP  are  widely  discussed  in  academic  literature  but  are  highly model- and task-dependent, often rely on intuitive visual cues, and lack standardised benchmarks or evaluation criteria. As such, their interpretability is typically subjective and difficult to reproduce. Consequently, these tools may be useful for investigational purposes or root-cause analysis but are not yet suitable as formal quality assurance mechanisms in high-risk medical device manufacturing. A more robust and auditable approach,  aligned  with  current  quality  system  practices,  is  to  establish  traceability and  control  over  the  full  lifecycle  of  data  and  model  development.  This  includes:\n\n- A  documented  process  for  image  and  defect  sample  acquisition  that  ensures statistical representation of expected variation;.\n- Storage and traceability of all training and testing datasets, including metadata and  version  control,  so  they  can  be  inspected  as  part  of  quality  investigations;\n- Controlled  labelling  workflows  that  document  inter-annotator  variability  and define pass/fail criteria objectively;\n- Monitoring  feature  distributions  at  inference  time  to  detect  concept  drift  or anomalous behaviour in deployed models.\n\nCollectively,  these  practices  create  a  data-centric  transparency  framework  that, while not offering direct interpretability of model internals, achieves the intent behind explainability-ensuring  that  the  relationship  between  inputs  and  outputs  is  controlled, verifiable, and auditable. This mirrors established process validation strategies for  black-box  manufacturing  processes,  where  input  control,  boundary  definition, and output reproducibility are prioritised over internal transparency. While academic research may continue to produce model-specific explainability techniques, such tools are unlikely to meet the generalisation, robustness, and auditability standards required in  a  regulated  quality  environment  in  the  near  term.  Until  then,  a  lifecycle-driven\n\ndata  and  model  governance  strategy  offers  a  more  practical  and  regulation-aligned alternative.\n\nIn  this  context,  it  is  noteworthy  that  the  FDA's  draft  guidance  on  the  use  of artificial intelligence to support regulatory decision-making [39] outlines a structured documentation framework aimed at enhancing model interpretability, with no mention of  explainability  tools.  Instead,  it  emphasises  the  importance  of  thoroughly  detailing the  development  process  of  the  models,  elucidating  how  they  generate  conclusions, reporting  performance  metrics  accompanied  by  confidence  intervals,  and  disclosing known limitations, including potential sources of bias. It should be observed that this guidance document had not been finalised at the time of writing and is not intended to  serve  as  a  harmonised  standard  under  the  EU  AI  Act.\n\n## 4.6 Software and Process Validation: Sampling and Performance Requirements\n\nWhile AI validation may imply the need for larger validation datasets than traditional computer vision  systems,  the  AI  Act  stops  short  of  defining  quantitative  thresholds. Instead,  it  requires  that  training  and  validation  data  be  of  sufficient  quality  and representative of operational conditions.\n\nThis  requirement  is  compatible  with  MDR  and  FDA  TMV  practices.  For  example,  high-risk inspection processes can be validated using the Success-Run Theorem, setting confidence and reliability thresholds (e.g., 95% and 99%, respectively). Manufacturers are therefore unlikely to face philosophical contradictions but may require greater  test  volumes  to  compensate  for  Deep  Learning  opacity  and  performance variability.\n\n## 4.7 Model Monitoring After Deployment\n\nThis  assessment  focuses  on  static  AI  models-those  that  are  not  retrained  postdeployment. While these models  offer  stability,  they  remain  subject  to  performance drift due to shifts in input distributions or environmental factors.\n\nThe AI Act and associated standards (ISO/IEC 23894:2023, ISO/IEC 42001:2023) do  not  explicitly  differentiate  static  vs.  retrainable  models.  Instead,  they  impose universal monitoring requirements, including:\n\n- Regular performance reviews,\n- Mechanisms for detecting data drift.\n- Revalidation following any significant changes,\n\nThis  represents  a  novel  requirement  for  manufacturers.  Under  MDR  and  FDA frameworks, routine verification typically applies only to calibration of variable inspection tools. Attribute-based inspections-such as DL-driven defect classification-are generally exempt from revalidation unless a non-conformance is detected.\n\nTherefore,  aligning  with  the  AI  Act  will  necessitate  new  monitoring  procedures, tools  for  tracking  model  stability,  and  clear  triggers  for  revalidation-even  in  the absence of active retraining.\n\n## 4.8 Current Industry Practice and Limitations\n\nWhile  machine  learning-driven  inspections  remain  in  the  early  stages  of  adoption within the medical device industry, current deployments are generally limited and governed by existing software validation and process qualification frameworks. As outlined in GAMP 5[36] Appendix D11, these systems are validated using traditional waterfall or V-model methodologies, with models treated as locked, static 'black boxes'. Their lifecycle  documentation  typically  resides  within  engineering  documentation  rather than  formal  QMS  artefacts,  as  there  is  no  explicit  regulatory  requirement  for  model transparency or retraining governance. Furthermore, post-deployment monitoring is not mandated, and practices for image storage vary. Due to the high cost of long-term storage,  images  used  for  training  or  inference  may  or  may  not  be  retained,  thereby limiting  the  feasibility  of  comprehensive  bias  tracking  or  explainability  analysis  in practice.\n\n## 5 Conclusions and Strategic Recommendations\n\n## 5.1 Discussing  Challenges\n\nWhile Section  4 provided a detailed analysis of the foreseeable challenges imposed by the  EU  AI  Act,  this  section  offers  a  summarised,  interpretive  discussion  focused  on areas where current MDR and FDA frameworks may fall short or require adaptation.\n\n- Data  Representativeness  and  Bias  Mitigation : Although traditional test method validation (TMV) under MDR and FDA includes statistically justified sampling, it does not explicitly address bias detection or diversity in AI datasets. Future AI-specific standards are expected to formalise requirements around data balance, annotation consistency, and defect representativeness.\n- Explainability  Requirements :  The  paper  acknowledges  that  explainability tools are not yet mature enough to support regulatory use. Instead, lifecycle traceability  of  input  data,  labelling  methods,  and  model  outputs  is  proposed  as a practical  alternative,  aligning  with  both  GAMP  5  and  ISO  13485  expectations.\n- Data Retention Limitations :  Current  regulations  emphasise  long-term  retention  of  validation  records,  but  do  not  mandate  retention  of  raw  image  datasets. Given the AI Act's emphasis on traceability  and  auditability  of  training  data,  storage strategies-such as triage-based image retention or feature-level logging-may become necessary.\n- Monitoring  of  Static  Models : MDR  and  FDA  do  not  currently  mandate performance  surveillance  for  non-adaptive  systems.  However,  the  AI  Act  introduces continuous monitoring obligations even for locked models. This highlights the need for system-wide monitoring frameworks capable of detecting input drift and performance anomalies post-deployment.\n- Global  and  Cross-Jurisdictional  Compliance :  As  AI  systems  embedded  in manufacturing environments may impact devices exported to the EU, non-EU manufacturers will also need to consider how lifecycle traceability and documentation can  demonstrate  conformity  under  Article  2  of  the  AI  Act.\n\n## 5.2 Proposed Lifecycle for ML-Based Visual Inspection Systems\n\nTo  bridge  the  regulatory  gaps  identified  above,  we  propose  a  structured  lifecycle designed specifically for static,  non-adaptive  machine  learning  systems used in automated  visual  inspections for Class  III  medical  devices.  This  lifecycle  is informed  by GAMP 5 Appendix  D11,  ISO  13485,  and  Annex  11,  and  includes  the following stages:\n\n## 1. Lifecycle  Policy  Definition\n\nEstablish  a  formal  AI  lifecycle  policy  scoped  to  the  use  of  locked  ML  models  in visual inspection applications. This policy should clearly distinguish these systems from  other  AI  applications  (e.g.,  adaptive  models,  decision  support  systems,  or models handling personal data), which may have different regulatory implications.\n\n## 2. Dataset  Documentation\n\nUse a Dataset Specification Sheet to ensure quality, traceability, and reproducibility of  training,  testing,  and  validation  datasets.  The  documentation  should  include:\n\n- Dataset structure, versioning, and storage location\n- Image acquisition setup and parameters\n- Class distributions and sample quantities per class\n- Defect generation or sampling methodology\n- Sources of bias and mitigation strategies\n- Labelling types and process, including annotator instructions\n\n## 3. Model Development and Evaluation\n\nEmploy metrics appropriate for imbalanced classification tasks, such as:\n\n- Balanced  accuracy\n- Escape rate\n- Precision and recall\n- Confusion  matrix  analysis\n\nEnsure  strict  separation  between  training,  validation,  and  test  sets.  Final  test datasets must be locked and never used during model development or architecture selection to prevent bias and overfitting.\n\n## 4. Prototype Deployment Phase\n\nIntroduce a pre-qualification observation step, where the model is deployed in production with hidden outputs. Manual inspections remain active during this phase. The  model  runs  passively  for  a  statistically  defined  period  (e.g.,  2-3  months)  to collect  outcome  data.  Results  are  retrospectively  analysed  and  compared  with human inspections. Progression to validation only occurs if performance aligns with expectations.\n\n## 5. Test Method Validation (TMV)\n\nConduct full TMV according to established regulatory practices. Apply statistically justified  sampling strategies (e.g., the Success-Run Theorem) to demonstrate sensitivity, reliability, and reproducibility of the model under production conditions.\n\n## 6. Post-Deployment Monitoring\n\nImplement lightweight monitoring frameworks to detect performance drift. Recommended practices include:\n\n- Designing  models  to  output  intermediate  feature  vectors  to  enable  ongoing distribution analysis\n- Employing triage-based image retention (e.g., storing all defect images and a random sample of non-defect images) to manage storage burden while maintaining traceability\n- Centralising monitoring across stations using shared cloud infrastructure\n\nThis proposed lifecycle integrates both current best practices in software validation and emerging requirements under the EU AI Act. It offers a scalable and regulationaligned  pathway  for  the  deployment  of  machine  learning  in  regulated  inspection environments.\n\n## 5.3 Closing Reflections and Future Work\n\nThe  proposed  lifecycle  strategy  outlined  in  this  paper  offers  a  practical  and  riskbased  pathway  for  aligning  ML-based  visual  inspection  systems  with  both  current regulatory  frameworks  and  emerging  obligations  under  the  EU  AI  Act.  By  focusing on  locked  models,  data  lifecycle  traceability,  and  input/output  reproducibility,  the approach remains consistent with ISO 13485, FDA QSR, and the principles established in  GAMP 5, while anticipating future requirements for transparency, bias mitigation, and post-deployment monitoring.\n\nThis  lifecycle  addresses  many  of  the  technical  and  procedural  gaps  identified  in Section 4 and summarised in Appendix B, Table B3. It offers a feasible implementation  pathway  that  builds  on  standardised  validation  practices  such  as  Test  Method Validation (TMV) and software change control, while introducing new concepts such as the blinded prototype phase and feature-based monitoring.\n\nDespite these advances, several critical areas remain unresolved and will require further development of standards, tools, or regulatory guidance:\n\n- Explainability: There is currently no validated or auditable framework  for applying explainability tools in quality-critical inspection systems. Practical alternatives-such  as  data  lifecycle  traceability-should  continue  to  be  explored until regulator-approved techniques emerge.\n- Conformity  assessment: No  clear  conformity  pathway  currently  exists  for  AI systems  deployed  as  part  of  manufacturing  quality  infrastructure.  This  may  limit adoption until vertical guidance becomes available.\n- Bias and fairness validation: The  industry  lacks  consensus  on  metrics,  thresholds, or test methods  for  evaluating  dataset  bias, particularly in  rare-defect inspection scenarios.\n- Monitoring infrastructure: Scalable post-deployment monitoring solutions remain an open challenge, especially where cloud-based feature tracking is not yet available or approved in regulated environments.\n\nTo  support  future  readiness,  we  recommend  prioritising  the  following  areas  of industry and regulatory collaboration:\n\n- Establishing standardised templates for image dataset documentation (e.g., Dataset Specification Sheets)\n- Creating cross-functional working groups to inform the development of harmonised vertical  standards  for  inspection-based  AI  under  the  EU  AI  Act\n- Defining minimal performance and monitoring requirements for locked models used in critical visual inspections\n- Expanding the GAMP 5 framework to formally incorporate model lifecycle artefacts and risk controls specific to manufacturing AI\n\nUltimately,  the  success  of  AI-enabled  inspections  in  regulated  environments  will depend  on  the  ability  of  manufacturers,  regulators,  and  standards  organisations  to co-develop  transparent,  auditable,  and  scalable  frameworks  that  balance  innovation with risk-based assurance.\n\n## Declarations\n\nCompeting interest: The authors declare no competing interests.\n\n## References\n\n- [1] International  Organization  for  Standardization:  En  iso  13485:2016+a11:2021  medical devices - quality management systems - requirements for regulatory purposes. Technical report, International Organization for Standardization, Geneva, CH (2021). https://www.iso.org/\n- [2] U.S.  Food  and  Drug  Administration:  21  CFR  Part  820  -  Quality  System  Regulation  (2023).  https://www.ecfr.gov/current/title-21/chapter-I/subchapter-H/ part-820\n- [3] European  Parliament  and  Council:  Regulation  (EU)  2017/745  on  medical devices. Official Journal of the European Union (2017). https://eur-lex.europa. eu/legal-content/EN/TXT/PDF/?uri=CELEX:32017R0745\n- [4] U.S.  Food  and  Drug  Administration:  Medical  Device  Classification  Procedures. Code  of  Federal  Regulations,  Title  21,  Part  860  (2023).  https://www.ecfr.gov/ current/title-21/part-860\n- [5] Charles,  R.L.,  Johnson,  T.L.,  Fletcher,  S.R.:  The  use  of  job  aids  for  visual inspection in manufacturing and maintenance. Procedia CIRP 38 , 90-93 (2015) https://doi.org/10.1016/j.procir.2015.08.056\n- [6] RodrÃ­guez-PÃ©rez, J.: Human Error Reduction in Manufacturing. Quality Press, Milwaukee, USA (2029)\n\n- [7] International  Organization  for  Standardization:  En  iso  14971:2019+a11:2021  medical  devices  -  application  of  risk  management  to  medical  devices.  Technical  report,  International  Organization  for  Standardization,  Geneva,  CH  (2021). https://www.iso.org/\n- [8] U.S.  Food  and  Drug  Administration:  21  CFR  Part  11  -  Electronic  Records  and Electronic  Signatures  (2023).  https://www.ecfr.gov/current/title-21/chapter-I/ subchapter-A/part-11\n- [9] U.S.  Food  and  Drug  Administration:  Quality  Management  System  Regulation  Frequently Asked Questions. Available online (2024)\n- [10] International Organization for Standardization: Iso 9000:2015 - quality management  systems  -  fundamentals  and  vocabulary.  Technical  report,  International Organization  for  Standardization,  Geneva,  CH  (2015).  https://www.iso.org/\n- [11] International  Organization  for  Standardization:  Iso/tr  80002-2:2017  -  medical device software - part 2: Validation of software for medical device quality systems. Technical  report,  International  Organization  for  Standardization,  Geneva,  CH (2017). https://www.iso.org/\n- [12] International Society for Pharmaceutical Engineering: GAMP 5: A Risk-Based Approach  to  Compliant  GxP  Computerized  Systems,  2nd  edn.  ISPE,  Tampa, FL, USA (2022)\n- [13] Global Harmonization Task Force: Ghtf sg3/n99-10:2004 - quality management systems:  Process  validation  guidance.  Technical  report,  Global  Harmonization Task Force (2004)\n- [14] U.S.  Food  and  Drug  Administration:  Process  Validation:  General  Principles  and Practices (2011). https://www.fda.gov/media/71021/download\n- [15] Taylor, W.A.: Statistical Procedures for the Medical Device Industry, 2nd edn. CRC Press, Boca Raton, FL, USA (2011)\n- [16] Automotive  Industry  Action  Group:  Measurement  Systems  Analysis  (MSA) Manual, 4th edn. Southfield, MI, USA (2010)\n- [17] European Union: Regulation (EU) 2024/1684 Artificial Intelligence Act. Official Journal of the European Union L 164 (2024)\n- [18] CEN-CENELEC JTC 21: Artificial intelligence (ai) - work programme. Technical report, CEN-CENELEC, Brussels, Belgium (2025). https://www.cencenelec.eu/ areas-of-work/cen-cenelec-topics/artificial-intelligence/\n- [19] Bezombes,  P.:  European  standardization  in  support  of  the  european  artificial intelligence  regulation.  In:  Euroshnet  Conference  Proceedings  (2022).  Available online\n\n- [20] ISO/IEC: Iso/iec 22989:2022 - artificial intelligence - concepts and terminology. Technical report, ISO/IEC, Geneva, CH (2022). https://www.iso.org/\n- [21] ISO/IEC: Iso/iec 23053:2022 - framework for ai systems using machine learning. Technical report, ISO/IEC, Geneva, CH (2022). https://www.iso.org/\n- [22] ISO/IEC:  Iso/iec  23894:2023  -  artificial  intelligence  -  guidance  on  risk  management. Technical report, ISO/IEC, Geneva, CH (2023). https://www.iso. org/\n- [23] ISO/IEC:  Iso/iec  27001:2013  -  information  security  management  systems  requirements.  Technical  report,  ISO/IEC,  Geneva,  CH  (2013).  https://www.iso. org/\n- [24] ISO/IEC:  Iso/iec  5259-1:2022  -  quality  evaluation  guidelines  for  ai  systems  part  1:  Overview  and  metrics.  Technical  report,  ISO/IEC,  Geneva,  CH  (2022). https://www.iso.org/\n- [25] ISO/IEC: Iso/iec 5259-2:2024 - quality evaluation guidelines for ai systems - part 2:  Data  quality.  Technical  report,  ISO/IEC,  Geneva,  CH  (2024).  https://www. iso.org/\n- [26] ISO/IEC: Iso/iec 5259-3:2024 - quality evaluation guidelines for ai systems - part 3:  Evaluation  process.  Technical  report,  ISO/IEC,  Geneva,  CH  (2024).  https: //www.iso.org/\n- [27] ISO/IEC: Iso/iec 5259-4:2024 - quality evaluation guidelines for ai systems - part 4:  Use  case  specific  guidelines.  Technical  report,  ISO/IEC,  Geneva,  CH  (2024). https://www.iso.org/\n- [28] ISO/IEC:  Iso/iec  42001:2023  -  artificial  intelligence  -  management  system. Technical report, ISO/IEC, Geneva, CH (2023). https://www.iso.org/\n- [29] CEN-CENELEC: Cen/clc/tr 18115:2024 - artificial intelligence - traceability and documentation in high-risk ai systems. Technical report, CEN-CENELEC Joint Technical Committee 21 (JTC 21), Brussels, Belgium (2024)\n- [30] CEN-CENELEC: Cen/clc iso/iec/tr 24027:2023 - artificial  intelligence  -  bias in  ai  systems  and  datasets.  Technical  report,  CEN-CENELEC  Joint  Technical Committee 21 (JTC 21), Brussels, Belgium (2023)\n- [31] CEN-CENELEC: Cen/clc iso/iec/tr 24029-1:2023 - artificial intelligence - assessment of the robustness of neural networks - part 1: Overview. Technical report, CEN-CENELEC  Joint Technical Committee 21 (JTC 21), Brussels, Belgium (2023)\n- [32] ISO/IEC: En iso/iec 8183:2024 - artificial intelligence - data lifecycle framework.\n\n- Technical report, ISO/IEC, Geneva, CH (2024). https://www.iso.org/\n- [33] CEN-CENELEC: Cen/clc/tr 17894:2024 - artificial intelligence - post-market monitoring  for  high-risk  ai  systems.  Technical  report,  CEN-CENELEC  Joint Technical Committee 21 (JTC 21), Brussels, Belgium (2024)\n- [34] ISO/IEC:  En  iso/iec  25059:2024  -  software  and  systems  engineering  -  quality model  for  ai  systems.  Technical  report,  ISO/IEC,  Geneva,  CH  (2024).  https: //www.iso.org/\n- [35] CEN-CENELEC: Cen/clc iso/iec/ts 12791:2024 - artificial intelligence - guidance for  risk  assessment  of  ai  systems  from  a  human-centred  perspective.  Technical  report,  CEN-CENELEC  Joint  Technical  Committee  21  (JTC  21),  Brussels, Belgium (2024)\n- [36] International Organization for Standardization: Iso guide 51:2014 - safety aspects -  guidelines  for  their  inclusion  in  standards.  Technical  report,  International Organization  for  Standardization,  Geneva,  CH  (2014).  https://www.iso.org/\n- [37] International Organization for Standardization: Iso guide 73:2009 - risk management  -  vocabulary.  Technical  report,  International  Organization  for  Standardization, Geneva, CH (2009). https://www.iso.org/\n- [38] European Union: Regulation (EU) 2016/679 - General Data Protection Regulation. Official Journal  of  the  European  Union  L  119.  pp. 1-88  (2016)\n- [39] U.S. Food and Drug Administration: Considerations for the use of artificial intelligence  to  support  regulatory  decision-making for drug and  biological  products. Technical  report,  U.S.  Food  and  Drug  Administration,  Silver  Spring,  MD,  USA (Jan 2025). Draft guidance. Available online\n- [40] Medicines  and  Healthcare  Products  Regulatory  Agency:  Medical  Devices  Regulations 2002 (SI 2002 No 618, as amended) (2002). https://www.legislation.gov. uk/uksi/2002/618/contents\n- [41] National  Medical  Products  Administration:  Rules  for  Classification  of  Medical Devices  (Decree  No.  15).  Beijing,  China  (2016).  https://english.nmpa.gov.cn/ 2019-10/11/c\\_415411.htm\n- [42] Pharmaceuticals and Medical Devices Agency: Japanese Medical Device Nomenclature  (JMDN).  Accessed  4  Jun  2025  (2025).  https://www.std.pmda.go.jp/ stdDB/Data\\_en/InfData/Infetc/JMDN\\_en.pdf\n- [43] Health Canada: Guidance on the risk-based classification system for non-in vitro diagnostic devices (non-IVDDs) (2015). https: //www.canada.ca/en/health-canada/services/drugs-health-products/ medical-devices/application-information/guidance-documents/\n\n- guidance-document-guidance-risk-based-classification-system-non-vitro-diagnostic. html\n- [44] AgÃªncia Nacional de VigilÃ¢ncia SanitÃ¡ria (ANVISA): Collegiate Board Resolution RDC No. 751/2022. BrasÃ­lia: Brazilian Health Regulatory Agency (2022). https://www.gov.br/anvisa/pt-br/assuntos/produtosparasaude/ temas-em-destaque/arquivos/2024/rdc-751-2022-en.pdf\n- [45] Ministry of Food and Drug Safety: Medical Device Regulatory System in Korea (2016). https://www.mfds.go.kr/jsp/common/Medical\\_Device\\_ Regulatory\\_System\\_in\\_Korea\\_%28edited%29.pdf\n- [46] Therapeutic Goods Administration: Classification of medical devices that are not IVDs (Version 2.0). Australian  Government  Department  of  Health and Aged Care (2024). https://www.tga.gov.au/sites/default/files/2024-07/ classification-medical-devices-not-ivds-20240702.pdf\n- [47] Central Drugs Standard Control Organization: Medical Devices Rules, 2017. Ministry of Health and Family Welfare, Government of India (2017). https://cdsco.gov.in/opencms/resources/UploadCDSCOWeb/2022/m\\_ device/Medical%20Devices%20Rules%2C%202017.pdf\n\n## Appendix A Medical Devices Classifications and Standards\n\nTable A1 Medical Devices Regulatory Agencies and Classification Systems\n\n| Geographical Area   | Agency                                                       | Regulations defining Classification                                                       | Classification system (Highest Risk in Bold)   |\n|---------------------|--------------------------------------------------------------|-------------------------------------------------------------------------------------------|------------------------------------------------|\n| EU                  | European Commission (EC)                                     | Medical Device Regulation (MDR) - Article 51, Annex VIII - Classi- fication Rules [3]     | Class I, II, III                               |\n| USA                 | Food and Drug Administration (FDA)                           | 21 CFR Part 860 - Medical Device Classification Procedures [4]                            | Class I, II, III                               |\n| UK                  | Medicines and Healthcare products Reg- ulatory Agency (MHRA) | Medical Devices Regulations (UK MDR) SI 2002 No 618, PART II General Medical Devices [40] | Class I, II, III                               |\n| China               | National Med- ical Products Administration (NMPA)            | Rules for Classification of Medical Devices' (Decree No. 15) Article 4 [41]               | Class I, II, III                               |\n| Japan               | Pharmaceuticals and Medi- cal Devices Agency (PMDA)          | Japanese Medical Device Nomen- clature (JMDN) [42]                                        | Class I, II, III, IV                           |\n| Canada              | Health Canada                                                | Risk-based Classification Sys- tem for Non-In Vitro Diagnostic Devices (non-IVDDs) [43]   | Class I, II, III, IV                           |\n| Brazil              | AgÃªncia Nacional de VigilÃ¢n- cia SanitÃ¡ria (ANVISA)          | Collegiate Board Resolution RDC No. 751/2022 [44]                                         | Class I, II, III, IV                           |\n| South Korea         | Ministry of Food and Drug Safety (MFDS)                      | South Korea, as detailed in the Medical Device Regulatory System [45]                     | Class I, II, III, IV                           |\n| Australia           | Therapeutic Goods Admin- istration (TGA)                     | Classification of medical devices that are not IVD [46]                                   | Class I, IIa, IIb, III                         |\n| India               | Central Drugs Standard Con- trol Organiza- tion (CDSCO)      | Medical Devices Rules, 2017 [47]                                                          | Class A, B, C, D                               |\n\nInternational Organization for Standardization (ISO)\n\nTable A2 CEN/CLC/JTC 21 Work programme as published on the 11 th of Jun 2025 [18]\n\n| Project                                     | Title                                | Title                                                                                                                                            | Status                  |\n|---------------------------------------------|--------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|\n| prCEN/CLC/TR (WI=JT021009)                  | XXX                                  | AI Risks - Check List for AI Risks Man- agement                                                                                                  | Preliminary             |\n| prCEN/TS (WI=JT021034)                      |                                      | Guidelines on tools for handling ethical issues in AI system life cycle                                                                          | Preliminary             |\n| prEN ISO/IEC (WI=JT021021)                  | 24970                                | Artificial intelligence - AI system logging                                                                                                      | Under Drafting          |\n| EN 22989:2023/prA1 (WI=JT021031)            | ISO/IEC                              | Information technology -Artificial intel- ligence - Artificial intelligence concepts and terminology -Amendment 1                                | Under Drafting          |\n| EN 23053:2023/prA1 (WI=JT021032)            | ISO/IEC                              | Framework for Artificial Intelligence (AI) Systems Using Machine Learning (ML)- Amendment 1                                                      | Under Drafting          |\n| prEN (WI=JT021038)                          | XXX                                  | AI Conformity assessment framework                                                                                                               | Under Drafting          |\n| prEN (WI=JT021044)                          | XXX                                  | Artificial Intelligence - Taxonomy of AI tasks in computer vision - Tax- onomy of AI system methods and capabilities 1                           | Under Drafting          |\n| prEN ISO/IEC (WI=JT021045)                  | 42102                                | Information technology - Artificial intelli- gence - Taxonomy of AI system methods and capabilities                                              | Under Drafting          |\n| prEN ISO/IEC (WI=JT021046)                  | 25029                                | Artificial intelligence - AI-enhanced nudg- ing                                                                                                  | Under Drafting          |\n| FprEN ISO/IEC (WI=JT021022)                 | 12792                                | Information technology - Artificial intel- ligence - Transparency taxonomy of AI systems (ISO/IEC FDIS 12792:2025)                               | Under Approval          |\n| prEN (WI=JT021029)                          | XXX                                  | Artificial intelligence - Cybersecurity spec- ifications for AI Systems                                                                          | Under Drafting          |\n| prEN                                        | XXX                                  | Artificial Intelligence - Quality and gover- nance of datasets in AI                                                                             | Under Drafting          |\n| (WI=JT021037) prEN ISO/IEC (WI=JT021012)    | 23282                                | Artificial Intelligence - Evaluation meth- ods for accurate natural language process- ing systems                                                | Under Drafting          |\n| prEN ISO/IEC 25059 rev (WI=JT021027)        | prEN ISO/IEC 25059 rev (WI=JT021027) | Software engineering - Systems and soft- ware Quality Requirements and Evalua- tion (SQuaRE) - Quality model for AI systems (ISO/IEC 25059:2023) | Under Drafting          |\n| (WI=JT021030) prCEN/TS (WI=JT021033)        |                                      | Contributions towards ISO/IEC 27090 Guidance for upskilling organisations on AI ethics and social concerns                                       | Preliminary Preliminary |\n| prCEN/TS (WI=JT021035)                      |                                      | Sustainable Artificial Intelligence - Guide- lines and metrics for the environmental impact of artificial intelligence systems and services      | Preliminary             |\n| prEN (WI=JT021036)                          | XXX                                  | Artificial Intelligence - Concepts, measures and requirements for managing bias in AI systems                                                    | Under Drafting          |\n| prEN (WI=JT021039)                          | XXX                                  | Artificial intelligence - Quality manage- ment system for EU AI Act regulatory purposes                                                          | Under Drafting          |\n| prEN (WI=JT021019)                          | XXX                                  | Competence requirements for professional AI ethicists                                                                                            | Under Drafting          |\n| prEN                                        | 18229                                | AI trustworthiness framework                                                                                                                     | Under Drafting          |\n| (WI=JT021008) prCEN/CLC/TR                  | XXX                                  | Impact assessment in the context of the EUFundamenta l Rights                                                                                    | Preliminary             |\n| (WI=JT021026) prEN ISO/IEC TR (WI=JT021002) | 23281                                | 2 5 Artificial Intelligence - Overview of Al tasks and functionalities related to natural                                                        | Under Drafting          |\n| prEN (WI=JT021025)                          | XXX                                  | language processing Artificial Intelligence - Evaluation methods for accurate computer 1                                                         | Under Drafting          |\n| prEN (WI=JT021024)                          | 18228                                | vision systems AI Risk Management                                                                                                                | Under Drafting          |\n\n## Summary of Requirements and Interim\n\n## Appendix B Recommendations\n\nTable B3 Mapping EU AI Act Requirements to Interim Recommendations and Future Needs\n\n| EU AI Act Require- ment                      | Interim Recommendation                                                                                                                                                                                                             | Future Work / Regulatory Watch-Outs                                                                             |\n|----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|\n| High-Risk Classifica- tion                   | Based on current standards we believe Visual inspections are High-Risk.                                                                                                                                                            | Additional Clarification is needed in harmonised standards or EU AI Act Annex II                                |\n| AI-specific Quality Management System (AIMS) | While General QMS exists (ISO 13485), We proposed the introduc- tion of a AI-specific policy, defining detail use-cases, as described in section 5.2(1).                                                                           | Vertical AIMS standards under development (e.g., ISO/IEC 42001)                                                 |\n| Lifecycle Documenta- tion of AI Models       | We propose formal traceability via a Dataset Sheet and test control proce- dure as described in section 5.2(2).                                                                                                                    | Additional guidance required for integrating model lifecycle into QMS artifacts                                 |\n| Dataset Governance and Bias Mitigation       | While current Sampling plans prac- tices partially address this require- ment, we propose the creation of pro- cedures to carry out dataset curation, labelling protocol and bias identifica- tion as described in section 5.2(2). | Needs future guidance on bias metrics, fairness audits, and representative sampling                             |\n| Explainability of AI Decisions               | We propose the use of a documented lifecycle as described in section 5.2. while robust explainability tools are developed.                                                                                                         | Need for the development of auditable explainability tools                                                      |\n| Performance Valida- tion (TMV)               | Requirement covered by current MDR/FDA Regulations.                                                                                                                                                                                | N/A                                                                                                             |\n| Post-Deployment Monitoring                   | We proposed feature-based drift detec- tion as detailed in section 5.2(6).                                                                                                                                                         | Future regulations likely to develop guidance related to monitoring minimum crite- ria                          |\n| Change Management and Retraining             | Requirement for static models already covered by current MDR/FDA Regu- lations.                                                                                                                                                    | N/A                                                                                                             |\n| Conformity Assess- ment of AI Systems        | We propose the creation of an assess- ment based on the proposed lifecycle in section 5.2.                                                                                                                                         | Assessment requirements not clear. Need for a defined conformity route (e.g., via notified bodies) under AI Act |\n| Data and Image Reten- tion                   | We recommend practices such as triage storage and feature logging to optimise storage cost.                                                                                                                                        | Guidance required on stor- age scope, duration, and traceable artefact formats                                  |", "fetched_at_utc": "2026-02-09T14:02:09Z", "sha256": "b0924fed24ef8902620f5b21f53c0c191a4ff91594320c31a7496431e8e36980", "meta": {"file_name": "Navigating the EU AI Act - Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices.pdf", "file_size": 1460238, "mtime": 1770643021, "docling_errors": []}}
{"doc_id": "pdf-pdfs-netherlands-ai-act-guide-f93ca11c5a5d", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Netherlands AI Act Guide.pdf", "title": "Netherlands AI Act Guide", "text": "<!-- image -->\n\n## AI Act Guide\n\nVersion 1.1 - September 2025\n\nAI Act Guide |\n\n2\n\n## Guide to reading this document and disclaimer\n\nYou develop AI systems or are considering using them in your organisation. In that case, you may well come into contact with the AI Act. This guide has been prepared as a tool to provide you with easy insight into the key aspects of the AI Act. No rights may be derived from the content of this guide. The legal text of the AI Act always takes precedence.\n\nPlease submit any feedback on this guide to ai-verordening@minezk.nl. Your feedback will be used to improve future editions of the guide.\n\nIf you are reading a paper version of the guide , visit ondernemersplein.nl for the latest version. The website also provides references to the latest guidelines from the European Commission. At the time of publication, guidelines have been issued regarding the definition of AI, prohibited AI and General Purpose AI models.\n\n## The AI Act\n\nThe AI Act is an extensive legal document governing Artificial Intelligence (AI) for the entire European Union (EU). The AI Act contains rules for the responsible development and use of AI by businesses, government and other organisations. The aim of the regulation is to protect the safety, health and fundamental rights of natural persons. Application of the regulation means that organisations can be certain that the AI they use is responsible and that they can enjoy the benefits and opportunities offered by AI.\n\nThe regulation will be introduced in phases and the majority will apply as from mid-2026 onwards. A number of AI systems have already been prohibited since February 2025. Given this situation, it is important that you make the necessary preparations. To help you in that process, this guide lists the most important provisions from the AI Act. However, no rights may be derived from the information contained in this document. Its sole purpose is to provide support. Click here 1 for the complete text of the regulation.\n\n## What does the AI Act mean for your organisation?\n\nDepending on the type of AI system and the use to which the organisation puts that system, requirements will be imposed on its development and use. Whether requirements are imposed will among others depend on the risk the AI system represents to safety, health and fundamental rights. Different requirements will be imposed on organisations that develops an AI-system or has it developed than on organisations that make use of AI. To find out what the AI Act means for your organisation, it is important to work through the four steps listed below. These steps are explained further in this guide:\n\nStep 1 (Risk): Is our (AI) system covered by one of the risk categories?\n\nStep 2 (AI): Is our system 'AI' classified according to the AI Act?\n\nStep 3 (Role): Are we the provider or deployer of the AI system?\n\nStep 4 (Obligations): What obligations must we comply with?\n\nNote: Many other guides and step-by-step plans start at step 2 (is our system 'AI') rather than step 1 (risk categories). After all, if an AI system does not qualify as 'AI' there are no requirements subject to the AI Act. However, even for systems which are not categorised as AI according to the AI Act, it is important to have a clear idea of the risks of the purposes for which they are used. That is why in this AI Act Guide, we have chosen to start with the risk categories.\n\n1 https://eur-lex.europa.eu/legal-content/NL/TXT/?uri=CELEX:32024R1689\n\n## Step 1. (Risk) Is our (AI) system covered by one of the risk categories?\n\nAll AI systems are subject to the AI Act, but depending on the risk, different requirements are imposed on different categories of system. The risk is determined by the intended application or the product for which the AI system is being developed, sold and used:\n\n- Prohibited AI practices: these AI systems may not be placed on the market, put into service for used for certain practices. 2\n- High-risk AI systems: these AI systems must satisfy a number of requirements to mitigate the risks before they may be placed on the market or used. 3\n\nOther requirements will also apply to AI models and AI systems capable of performing certain tasks:\n\n- General purpose AI models and systems : these models and systems will be subject to specific information requirements. In certain cases, other requirements must be complied with in order to mitigate risks. 4\n- Generative AI and chatbots : these applications will be subject to specific transparency requirements depending on whether the system is or is not a high-risk system. 5\n\nThe same AI system can sometimes be covered by multiple categories. A chatbot, for example, can be deployed for a high-risk application and/or based on a general purpose AI model. AI systems not covered by any of the categories described above are not required to comply with the requirements from the AI Act. Nevertheless, you must remember that AI systems and the development of AI systems may also be required to comply with requirements from other regulations such as the General Data Protection Regulation (GDPR).\n\nTo determine whether you are required to comply with requirements from the AI Act, it is important to first identify the category that covers your AI system. Below we discuss the different risk categories in more detail.\n\n## 1.1. Prohibited AI systems\n\nCertain AI practices bring about an unacceptable risk for people and society. These practices have therefore been prohibited since February 2025. This means that these systems may not be placed on the market, put into service or used for these practices. These prohibitions apply both to providers and deployers since 2 February 2025 (further explanation is provided under Step 3. Are we the provider or deployer of the AI system? on page 12).\n\n2 Chapter II, Article 5 AI Act.\n\n3 Chapter III, Article 6 through to 49 AI Act.\n\n4 Chapter V, Article 51 through to 56 AI Act.\n\n5 Chapter IV, Article 50 AI Act. Specific transparency requirements will also apply for emotion recognition and biometric categorisation systems. As these systems are also high-risk AI systems, they will also have to adhere to the requirement for these systems, as described in step 4.2.\n\n## Prohibited AI systems 6\n\n1. Systems intended to manipulate human behaviour with a view to restricting the free choice of individuals and which can result in significant harm to those persons.\n2. Systems which exploit the vulnerabilities of persons due to their age, disability or a specific social or economic situation and which are likely to cause significant harm to those persons.\n3. Systems that draw up a point system of rewards and punishments based on social behaviour or personality traits, known as social scoring , which could lead to detrimental or unfavourable treatment.\n4. Prohibition on systems for making risk assessments to predict the risk of a person committing a criminal offence , based solely on profiling or personality or other traits.\n5. Systems which create or expand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage.\n6. Systems for emotion recognition in the workplace and in education institutions, except where intended for medical or safety reasons.\n7. Systems used for categorising individual persons using biometric categorisation systems in certain sensitive categories such as race and sexual orientation.\n8. The use of real-time remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement. There are a number of exceptions in cases in which use is strictly necessary, for example when searching for specific victims of obduction, trafficking in human beings or missing persons. These applications are subject to additional guarantees.\n\n## 1.2. High-risk AI systems\n\nHigh-risk AI systems may result in risks to health, safety or fundamental rights of natural persons, such as the right to privacy and the right not to be discriminated. At the same time, these systems can also have positive impact on natural persons and organisations, if they are reliable and the risks are mitigated. Against that background, from August 2026 onwards, high-risk AI systems must comply with a variety of requirements before being placed on the market, used or put into service. This means that during the development of the system, providers must ensure that the system satisfies these requirements before it is first placed on the market or used. A professional party that uses the AI system subject to its personal responsibility is considered a deployer (explained in more detail under Step 3. Are we the provider or deployer of the AI system? on page 12).\n\nDeployers are also subject to obligations with the aim of mitigating risks resulting from the specific use of the system. There are two types of high-risk AI systems:\n\n- High-risk products : AI systems that are directly or indirectly also subject to a selection of existing product regulations (see below). For example an AI system as a safety component of a lift or an AI system that in and of itself is a medical device.\n- High-risk applications : AI systems developed and deployed for specific applications in 'high-risk application areas'. These are eight application areas for AI that range from AI for law enforcement to AI in education. Within those eight areas, around 30 different specific applications have been identified that result in high risks, such as AI systems that support the deployment of emergency first response services.\n\nThe product groups and application areas in which AI systems are categorised as high-risk appear in the figures below.\n\nThe obligations for this category also apply to high-risk products as from 2 August 2027 and to high-risk application areas as from 2 August 2026. The obligations are described under 4.2. High-risk AI on page 13.\n\n6 Article 5 AI Act, see also Commission Guidelines on prohibited artificial intelligence practices.\n\n## High-risk AI as (safety element of) existing products\n\nThese are products already regulated within the EU. A product is considered as representing a risk if in accordance with existing product regulations, third-party approval is required before the product can be placed on the market (conformity assessment). If AI is a safety-related component of the risk product or if the risk product itself is an AI system, it is considered as high-risk AI. This applies to products covered by the following product legislation: 7\n\n- Machines (Directive 2006/42/EC)\n- Toys (Directive 2009/48/EC)\n- Recreational craft (Directive 2013/53/EU)\n- Lifts (Directive 2014/33/EU)\n- Equipment and protective systems intended for use in potentially explosive atmospheres (Directive 2014/34/EU)\n- Radio equipment (Directive 2014/53/EU)\n- Pressure equipment (Directive 2014/68/EU)\n- Cableway installations (Regulation (EU) 2016/424)\n- Personal protective equipment (Regulation (EU) 2016/425)\n- Appliances burning gaseous fuels (Regulation (EU) 2016/425)\n- Medical devices (Regulation (EU) 2017/745)\n- In-vitro diagnostic medical devices (Regulation (EU) 2017/746)\n\nIn addition, the AI Act contains a further list of products also considered as high-risk AI, but which are not subject to any direct requirements under the AI Act. Nevertheless, at a later moment, the requirements from the AI Act will be used to clarify the specific product legislation applicable to these products. It is not yet known when this will take place, and it will differ from product to product. The products in question are subject to the following product legislation: 8\n\n- Civil aviation security (Regulation (EC) 300/2008 and Regulation (EU) 2018/1139)\n- Two or three-wheeled vehicles and quadricycles (Regulation (EU) 168/2013)\n- Agricultural and forestry vehicles (Regulation (EU) 167/2013)\n- Marine equipment (Directive 2014/90/EU)\n- Interoperability of the railway system in the EU (Directive (EU) 2016/797)\n- Motor vehicles and trailers (Regulation (EU) 2018/858 and Regulation (EU) 2019/2144)\n\n## High-risk application areas\n\nAn AI system is within the scope of one of the high-risk application areas if the provider intended the use of the AI system in one of these areas. In the documentation of the AI system, the provider must explicitly state the purpose, including the instructions for use, advertising materials and any other technical documentation. Note: Even if the provider did not intend the AI system as being high-risk when it was placed on the market, it may still be that in practice, a deployer does use the system for one of the high-risk application areas. In that case, the deployer is seen as the provider, and as such becomes responsible for the requirements imposed on high-risk AI systems. See also chapter 4.2.\n\nThere are eight high-risk application areas. This does not mean that all AI systems covered by the often abstractly described application areas are necessarily high-risk. A number of specific applications are listed for each area. 9\n\nTip: First check whether your AI system is covered by one of the eight application areas and then determine whether your AI system is one of the AI systems described in that category. Only in that case are you dealing with a high-risk AI system that must comply with the requirements.\n\n7 Article 6(1) and Annex I, Section A AI Act.\n\n8 Article 2(2) and Annex I, Section B AI Act.\n\n9 Annex III AI Act.\n\n## 1.  Biometrics\n\n- Remote biometric identification systems, unless the system is only used for verification.\n- Systems used for biometric categorisation according to sensitive or protected attributes.\n- Systems for emotion recognition.\n\n## 2.  Critical infrastructure\n\n- Systems intended to be used as safety components for the management and operation of critical digital infrastructure, road traffic or in the supply of water, gas, heating or electricity.\n\n## 3.  Education and vocational training\n\n- Systems for admission to or allocation of (vocational) education.\n- Systems for evaluating learning outcomes.\n- Systems for assessing the level of (vocational) education.\n- Systems for monitoring students during tests.\n\n## 4.  Employment, workers' management and access to self employment.\n\n- Systems for the recruitment or selection of candidates.\n- Systems to be used to make decisions affecting terms of work-related relationships, the allocation of tasks or the monitoring and evaluation of workers.\n\n## 5.  Essential private services and public services and benefits\n\n- Systems for evaluating the eligibility to essential public assistance benefits and services.\n- Systems for evaluating the creditworthiness or credit score of natural persons unless used for the purposes of detecting financial fraud.\n- Systems for risk assessment and pricing in relation to life and health insurance.\n- Systems for evaluating emergency calls and prioritising the dispatch of emergency first response services and emergency health care patient triage systems.\n\n## 6.  Law enforcement\n\n- Systems for law enforcement to assess the risk of a natural person becoming the victim of criminal offences.\n- Systems for law enforcement to be deployed as polygraphs or similar tools.\n- Systems for law enforcement for evaluating the reliability of evidence.\n- Systems for law enforcement for assessing or predicting the risk of a natural personal offending or to assess past criminal behaviour of natural persons or groups.\n- Systems for law enforcement for the profiling of natural persons in the course of detection, investigation or prosecution of criminal offences.\n\n## 7.  Migration, asylum and border control\n\n- Systems for public authorities to be used as polygraphs or similar tools.\n- Systems for public authorities for assessing a security risk, the risk of irregular migration or a health risk upon entry into a country.\n- Systems for public authorities to assist in the examination of applications for asylum, visa or residence permit, including associated complaints.\n- Systems for public authorities for detecting, recognising or identifying natural persons, with the exception of the verification of travel documents.\n\n## 8.  Administration of justice and democratic processes\n\n- Systems to be used by a judicial authority to assist in applying the law and resolving disputes and researching and interpreting facts and applying the law to a concrete set of facts.\n- Systems for influencing the outcome of an election or referendum or the voting behaviour of natural persons, with the exception of tools used to support political campaigns from an administrative or logistic point of view.\n\n## Exceptions to high-risk application areas\n\nThere are a number of specific exceptions in which AI systems are covered by one of the application areas but which are not seen as high-risk AI. This applies where there is no significant risk to health, safety or fundamental human rights. This for example applies if an AI system has no significant impact on the outcome of a decision, for example because the system is intended for : 10\n\n- Performing a narrow procedural task;\n- Improving the result of a previously completed human activity;\n- Detecting decision-making patterns or deviations from prior decision-making patterns and not meant to replace or influence the previously completed human assessment;\n- Performing a preparatory task to an assessment relevant to one of the high-risk application areas.\n\nIt should also be noted that an AI system used for profiling natural persons cannot make use of this exception. If you have determined that your (non-profiling) AI system is subject to one of the exceptions, you must record this fact and register the AI system in the EU database for high-risk AI systems. 11 At a later moment, the European Commission will draw up a list of examples to clarify what is and what is not covered by the exceptions.\n\n## 1.3. General purpose AI models and AI systems\n\nAn AI model is an essential component of an AI system, but is not an AI system in and of itself. This requires more elements, for example a user interface. 12\n\nA general purpose AI model (General Purpose AI) can successfully perform a wide range of different tasks and as such can be integrated in a variety of AI systems. These models are often trained on large volumes of data using self-supervision techniques. 13\n\nThe broad deployability of these models via specific AI systems means that they are used for a wide range of applications. These can include high-risk applications. Due to the potential large impact of these models, from August 2025 onwards, they must comply with various requirements.\n\nIf an AI system is based on a general purpose AI model which itself can actually serve multiple purposes, then it is a general purpose AI system. 14\n\nThe obligations applicable to this category apply from 2 August 2025 and are described under 4.3. General purpose AI models and systems on page 18.\n\n10 Article 6(3) AI Act.\n\n11 Article 6(4) AI Act.\n\n12 Consideration 97 AI Act.\n\n13 Article 3(63) AI Act.\n\n14 Article 3(66) AI Act.\n\n## 1.4. Generative AI and Chatbots\n\nCertain AI systems are subject to transparency obligations. 15 These are systems with which natural persons often interact directly. It must therefore be clear to these natural persons that they are interacting with AI or that the content has been manipulated or generated.\n\n- Systems used for generating audio, images, video or text ( generative AI );\n- Systems made for interaction ( chatbots ).\n\nThe obligations applicable to this category apply from 2 August 2026 and are described under 4.4. Generative AI and Chatbots on page 19.\n\n## 1.5. Other AI\n\nSee 4.5. Other AI on page 20 for more information on AI systems not covered by one of the risk categories described above.\n\n15 Article 50 AI Act.\n\n## Step 2. Is our system 'AI' classified according to the AI Act?\n\nThe AI Act imposes regulations on AI systems. There are different ideas about what AI is and what is not AI. The AI Act offers the following definition, which is intended to demarcate the nature of AI as a product on the market:\n\n'  An AI system means a machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments .' 16\n\nThese different elements can be present both in the development phase and in the use phase. What is the meaning of the terms used in this definition? 17\n\n-  Autonomy : This element is satisfied if autonomy is present in a system to a certain extent, even if very limited. Systems without any form of autonomy are systems that only operate if human actions or interventions are required for all actions by that system.\n- Adaptiveness: It is stated that a system 'may' exhibit adaptiveness after deployment. Although adaptiveness is therefore not identified as a decisive element, its presence is an indication that the system is an AI system.\n- It infers how to generate output on the basis of input (capacity to infer): This relates not only to generating output during the 'use phase' but also the capacity of an AI system to infer models and/or algorithms from data during the development phase.\n\nWhat does this cover? 18\n\n- Systems that make use of machine learning in which the system learns how certain objectives can be achieved on the basis of data. Examples of machine learning are (un)supervised learning, self-supervised learning, reinforcement learning and deep learning.\n- Systems that make use of knowledge and logic-based approaches which make learning, reasoning or modelling possible on the basis of determined knowledge or a symbolic representation of a task to be solved. Examples of these approaches are knowledge representation, inductive (logic) programming, knowledge bases, inference and deductive engines, (symbolic) reasoning, expert systems and search and optimisation methods.\n\n## What is not covered?\n\n- Systems based on rules laid down exclusively by natural persons to conduct automatic actions. Some of these systems can to a certain extent derive how to generate output from input received, but are still beyond the definition because they are only able to analyse patterns to a limited extent or are unable to autonomously adapt their output. Examples can be systems for improved mathematical optimisation, standard data processing, systems based on classic heuristics and simple prediction systems.\n- Systems designed to be used with full human intervention. These systems do not operate with a minimum level of autonomy.\n\n16 Article 3(1) AI Act.\n\n17 Commission Guidelines on the definition of an artificial intelligence system.\n\n18 Consideration 12 AI Act.\n\nThe European Commission has issued a guideline to further clarify the definition of AI. You can find the latest version on this guideline at ondernemersplein.nl.\n\nIf your system is not considered AI under the AI Act but is covered by one of the risk categories, it is important to hold a discussion within your organisation about the extent to which the system still represents risks and to mitigate these risks by complying with (specific) requirements from the AI Act. Systems beyond the scope of the AI Act may nevertheless still have to comply with requirements from other legislation and regulations.\n\n## Step 3. Are we the provider or deployer of the AI system?\n\nOnce you have determined the risk category that covers you AI system and whether your AI system is in fact subject to the AI Act, you must then determine whether you are the provider or deployer.\n\n- Provider : a person or organisation that develops or commissions the development of an AI system or model and places it on the market or puts the AI system into service. 19\n-  Deployer : a person or organisation using an AI system under its personal authority. This does not include non-professional use. 20\n\nThe description of the requirements in step 4 describes for each risk category which obligations apply to providers and deployers. Each is required to comply with other obligations. The strictest obligations apply to providers.\n\nNote: As deployer, in certain cases you can also become provider of a high-risk AI system such that you are required to comply with the high-risk obligations for providers. 21 This is further explained in steps 4.2. High-risk AI on page 13 and 4.3. General purpose AI models and systems on page 18.\n\nNote : the AI Act also includes other roles such as authorised representative, importer and distributor. 22 The obligations governing these actors are not discussed in this guide.\n\n19 Article 3(3) AI Act.\n\n20 Article 3(4) AI Act.\n\n21 Article 25 AI Act.\n\n22 Article 3(5), (6) and (7), Articles 22, 23 and 24.\n\n## Step 4. What obligations must we comply with?\n\n## 4.1. Prohibited AI practices\n\nThese AI practices result in unacceptable risk and have therefore been prohibited since 2 February 2025. This means that AI systems cannot be placed on the market or used for these practices. These prohibitions apply to both providers and deployers. 23\n\nThere are sharply demarcated exceptions to the prohibition on the use of real-time remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement, and the use of those systems must be provided with a basis in national legislation. There are also additional guarantees relating to the deployment of these systems.\n\n## 4.2. High-risk AI\n\nThe majority of requirements from the AI Act will apply to high-risk AI systems. Providers must comply with various obligations such as: 24\n\n- System for risk management;\n- Data and data governance;\n- Technical documentation;\n- Record-keeping (logs);\n- Transparency and information;\n- Human oversight;\n- Accuracy, robustness and cybersecurity;\n- Quality management system;\n- Monitoring.\n\nIf you as provider comply or believe you comply with all these obligations, you will be required to conduct a conformity assessment . In certain cases you can conduct this assessment yourself, while in other cases it must be conducted by a third party on your behalf. 25 A future version of this guide will explain in more detail when you are required to conduct which procedure.\n\nDeployers must also comply with various obligations. Additional obligations apply to government organisations using AI systems. 26\n\nEach obligation is explained in the figure below. These obligations will be further elaborated over the coming years in European standards. Participation will be organised via the standardisation institutes of the European Member States. In the Netherlands this is the NEN 27 .\n\nNote: In two cases, as deployer of a high-risk AI system, you yourself can become the provider of that system: 28\n\n- When you as deployer place your own name or trademark on the high-risk system;\n- When you as deployer make a substantial modification to the high-risk AI system that was not intended by the provider as a consequence of which the system no longer complies with the requirements or as a consequence of which the purpose of the system intended by the provider changes. This latter situation arises if you make use of an AI system that was not intended for high-risk applications, for any such applications.\n\n23 Article 5 AI Act.\n\n24 Article 16 and Section 2 of Chapter III (Articles 8-15) AI Act.\n\n25 Article 43 AI Act.\n\n26 Articles 26 and 27 AI Act.\n\n27 https://www.nen.nl/ict/digitale-ethiek-en-veiligheid/ai.\n\n28 Article 25 AI Act.\n\n## Requirements for high-risk AI systems\n\n## 1. System for risk management 29\n\nVarious steps must be taken for this system:\n\n- Identification and analysis of foreseeable risks the system can pose to health, safety or fundamental rights.\n- Taking suitable risk management measures to ensure that the risks that remain following implementation of these measures are acceptable.\n\nThe following points must be taken into account:\n\n- The risks must be identified and dealt with before the AI system is placed on the market or used and subsequently continuously during the use of the AI system.\n- Foreseeable abuse of the system must be taken into account.\n- The context of the use, including the knowledge and experience of the deployer of such AI systems or the fact that children and vulnerable groups may experience consequences of the AI system, must be taken into account. It may for example be necessary to offer training to the people working with the AI system.\n- The risk management measures must be tested to check that they are actually effective. This must be carried out on the basis of benchmarks appropriate to the purpose for which the AI system is deployed.\n- If a risk management system must also be established pursuant to existing product legislation, this may be combined to form a single risk management system.\n\n## 2.  Data and data governance 30\n\nDifferent requirements are imposed on the datasets used for training, validating and testing high-risk AI systems.\n\n- Data management appropriate to the purpose of the AI system, including:\n- The registration of the processes, including processes for data gathering and data processing;\n- The recording of assumptions about the datasets;\n- An assessment of the availability, quantity and suitability of the datasets including possible biases that could have consequences for natural persons;\n- Measures for tracing, preventing and mitigating biases;\n- Tackling shortcomings in the datasets that may prevent compliance with the AI Act (for example mitigating risks according to the risk management system).\n- For the purpose for which they are used, datasets must be sufficiently representative and as far as possible error-free. The context in which the AI system is to be used must also be taken into account; for example the geographical or social context.\n- Subject to a number of strict conditions, special categories of personal data (a term from the General Data Protection Regulation) may be processed as a means of tackling bias.\n\n29 Article 9 AI Act.\n\n30 Article 10 AI Act.\n\n## 3.  Technical documentation 31\n\nThe technical documentation must demonstrate that the high-risk AI system complies with the requirements laid down in the AI Act. The technical documentation must among others include:\n\n- A general description of the AI system including the intended purpose of the system, the name of the provider and instructions for use;\n- A detailed description of the elements of the AI system and of the development process for that system, including the steps in development, the design choices, the expected output from the system, the risk management system and the datasets used;\n- Detailed information about the monitoring, operation and control of the AI system, including the degree of accuracy at individual and general level, risks, the system for evaluation during use and measures for monitoring and human oversight;\n- An overview of the applicable standards;\n- The EU conformity declaration (the CE mark).\n\nSME enterprises can record their technical documentation in a simplified manner. At a future moment, the European Commission will issue a relevant form.\n\n## 4.  Record-keeping (logs) 32\n\nAutomatic logs must be retained during the lifecycle of the AI system so that risks can be traced in a timely manner and the operation of the system can be monitored.\n\nThese logs must be stored for at least six months. At least the following events must be recorded:\n\n- The duration of each use of the AI system;\n- The input data and the control of that data by the AI system (and the reference database);\n- The identification of the natural persons involved in the verification of the results.\n\n## 5.  Transparency and information 33\n\nThe provider of the AI system knows how the system operates and how it should be used. For that reason, the provider must ensure that the AI system is sufficiently transparent so that deployers understand how they can correctly make use of the output from the system.\n\nWith this in mind, instructions for use must be drawn up, that include at least the following points:\n\n- Contact details;\n- The purpose, characteristics, capacities and limitations of the performance of the AI system;\n- The measures for human oversight.\n\n## 6.  Human oversight 34\n\nHigh-risk AI systems must be designed by the provider in such a way that during use they can be effectively overseen by natural persons in order to mitigate the risks for natural persons. Human oversight shall be context dependent - the greater the risks, the stricter the oversight must be. The measures for oversight may be technical in nature (for example a clear human-machine interface), or measures that must be undertaken by the deployer (for example a compulsory course for their personnel).\n\n31 Article 11 AI Act.\n\n32 Article 12 AI Act.\n\n33 Article 13 AI Act.\n\n34 Article 14 AI Act.\n\nThe eventual objective of these measures is to ensure that the natural persons who use the AI system are capable of the following:\n\n- They understand the capacities of the system and can monitor its functioning;\n- They are aware of automation bias;\n- They can correctly interpret and if necessary ignore or replace the output;\n- They can halt the system.\n\n## 7.  Accuracy, robustness and cybersecurity 35\n\nHigh-risk AI systems must offer an appropriate level of accuracy, robustness and cybersecurity. To achieve this, benchmarks and measuring methods are developed by the European Commission.\n\nAt least the following measures must be mentioned:\n\n- Technical and organisational measures to prevent errors that occur in interaction between the AI system and natural persons;\n- Solutions for robustness such as backups or security measures in the event of defects;\n- Removing or mitigating negative influencing of the system by limiting feedback loops;\n- Cybersecurity that prevents unauthorised third-party access by tracing, responding to and dealing with attacks. These are attacks aimed at data poisoning, model poisoning, adapting input or obtaining confidential data.\n\n## 8.  Quality management system 36\n\nThe quality management system must ensure that the requirements from the AI Act are complied with. How extensive the quality management system must be will depend on the size of the organisation. For example by documenting the following:\n\n- A strategy for compliance;\n- Techniques, procedures and measures for the design, development and quality assurance of the AI system;\n- Whether standardisation is used;\n- Systems and procedures for data management, risk management, monitoring, incident reporting and documentation.\n\n## 9.  Monitoring 37\n\nAs soon as an AI system has been placed on the market or is in use, providers must monitor the system on the basis of use data, thereby determining whether the system continues to comply with the requirements from the AI Act. For this purpose, providers must draw up a monitoring plan.\n\nIf the provider of a high-risk AI system discovers that the system no longer functions in compliance with the AI Act, corrective measures must be taken immediately to correct the situation. This may even include recalling the system if necessary. The provider must also work alongside the deployer and duly inform the surveillance authorities.\n\nSerious incidents involving the AI system must be reported to the surveillance authorities. 38\n\n35 Article 15 AI Act.\n\n36 Article 17 AI Act.\n\n37 Article 72 AI Act.\n\n38 Article 73 AI Act.\n\n## Other requirements\n\n- The registration of the high-risk AI system in the EU database. 39\n- The contact details of the provider must be registered with the AI system. 40\n- The technical documentation, documentation concerning the quality management system and documentation concerning the conformity assessment must be kept for 10 years. 41\n\n## Obligations for deployers of high-risk AI systems\n\nNot only providers but also the deployers of high-risk AI systems are subject to obligations. After all they are the parties who control how the AI system is used in practice and as such have a major impact on the risks that can occur.\n\nDeployers must: 42\n\n- Take appropriate technical and organisational measures to ensure that the high-risk AI system is used in accordance with the instructions for use;\n- Assign human oversight to natural persons who have the necessary competence, training and authority;\n- Ensure that the input data is relevant and sufficiently representative, wherever possible;\n- Monitor the operation of the AI system on the basis of the instructions for use;\n- If the deployer has reason to believe that the system no longer complies with the requirements from the AI Act, the deployer must duly inform the provider and cease use of the system;\n- Inform the provider and surveillance authorities of possible risks and serious incidents that have occurred;\n- Keep the logbook under their control for at least six months;\n- Inform worker representation if the AI system is to be deployed on the shop floor;\n- Duly inform people if decisions are taken about natural persons using the high-risk AI system;\n- If use is made of AI for emotion recognition or biometric categorisation, the natural persons in respect of whom the system is used must be duly informed.\n\nSpecific obligations for government organisations as deployers In addition to the obligations described above, government organisations must comply with a number of additional obligations:\n\n- Register use of a high-risk system in the EU database; 43\n- Assess the potential consequences for fundamental rights if the high-risk AI system is used with a view to the specific context within which use takes place (a fundamental rights impact assessment ). They will for example consider the duration of use, the processes within which the system is used and the potential impact of use on the fundamental rights of natural persons and groups. Following identification of the risks, deployers must take measures for human oversight and deal with possible risks. A report must also be submitted to the market surveillance authorities unless an appeal can be made to an exception based on public safety or protection of human health. 44\n\nNote: This obligation also applies to private entities providing public services, the use of AI systems for assessing the creditworthiness of natural persons and AI systems for risk assessments for life and health insurance.\n\n39 Article 49 AI Act.\n\n40 Article 16(b) AI Act.\n\n41 Article 18 AI Act.\n\n42 Article 26 AI Act.\n\n43 Article 49(3) AI Act.\n\n44 Article 27 AI Act.\n\n## 4.3. General purpose AI models and systems\n\n## Obligations for providers of general purpose AI models 45\n\nGeneral purpose AI models can be integrated in all kinds of different AI systems. It is essential that the providers of these AI systems know what the AI model is and is not capable of. Specific requirements are also imposed on the training of these models because training often makes use of large datasets. The providers of these models must:\n\n- Draw up technical documentation of the model including the training and testing process and the results and evaluation;\n- Draw up and keep up to date information and documentation for providers of AI systems who intend to integrate the model in their AI system. The information must provide a good understanding of the capacities and limitations of the AI model and must enable the provider of the AI system to comply with the obligations from the AI Act.\n- Draw up a policy to ensure that they train the model without infringing the copyrights of natural persons and organisations;\n- Draw up and publish a sufficiently detailed summary about the content used for training the AI model.\n\nProviders of open source models are not required to comply with the first two obligations (technical documentation and drawing up information for downstream providers).\n\n## Obligations for providers of general purpose AI models with systemic risks 46\n\nIn certain cases, general purpose AI models can generate systemic risks. This applies if the model has high impact capacity, for example due to the scope of the model or due to (potential) negative impact on public health, safety, fundamental rights or society as a whole. This is at least assumed if at least 10 25 floating point operations (FLOPs) are used to train the model. On the basis of specific criteria, the European Commission can also determine that the model has a similar major impact in some other way. These models must:\n\n- Comply with the obligations for general purpose AI models;\n- Implement model evaluations to map out the systemic risks;\n- Mitigate systemic risks;\n- Record information about serious incidents and report those incidents to the AI Office;\n- Ensure appropriate cybersecurity.\n\nNote : these obligations only apply to the largest AI models.\n\nProviders of these models with systemic risks cannot appeal to an exception for open source.\n\n45 Article 53 AI Act.\n\n46 Article 55 AI Act.\n\n## What rights do you have if you integrate a general purpose AI model in your (high-risk) AI system?\n\nAs indicated above, you must at least receive information and documentation to enable you to determine for yourself how you can make use of the model in your AI system for the chosen purpose. If you include the model in a high-risk AI system, as a provider you must then still comply with the obligations from the AI Act.\n\nHow should you deal with general purpose AI systems? As indicated in 1.3. General purpose AI models and AI systems on page 8, there are also AI systems that can serve multiple purposes. Take for example the widely known AI chatbots. Note: If you deploy these systems for high-risk purposes, according to the AI Act you yourself become a provider of a high-risk AI system. 47 It is then up to you to comply with the applicable obligations. In this situation it is very difficult to comply with the obligations for a high-risk AI system, which means that you may run the risk of receiving a penalty.\n\n## 4.4. Generative AI and Chatbots\n\nTo ensure that natural persons know whether they are talking to an AI system or are seeing content generated by AI, transparency obligations are imposed on generative AI and chatbots.\n\n## Rules for providers of chatbots 48\n\nProviders of systems designed for direct interaction with natural persons must ensure that these natural persons are informed that they are interacting with an AI system.\n\n## Rules for providers of generative AI 49\n\nProviders of systems that generate audio, image, video or text content must ensure that the output is marked in a machine readable format so that the output can be detected as artificially generated or manipulated.\n\n## Rules for deployers of generative AI 50\n\nDeployers of systems that generate audio, image or video content must ensure that it is clear that the content is artificially generated or manipulated. This can for example be achieved by applying a watermark. For creative, satirical, fictional or analogue work, this may be carried out in a way that does not ruin the work.\n\nA special regime applies to artificially generated text. Only in cases where texts are used with the purpose of 'informing the public on matters of public interest', the fact must be disclosed that the text has been artificially generated or manipulated. If there is editorial review and responsibility, this need not be carried out.\n\n## Rules for deployers of emotion recognition systems or systems for biometric categorisation 51\n\nThe deployers of these AI systems must inform the natural persons exposed to these systems about how the system works.\n\n47 Article 25(1)(c) AI Act.\n\n48 Article 50(1) AI Act.\n\n49 Article 50(2) AI Act.\n\n50 Article 50(4) AI Act.\n\n51 Article 50(3) AI Act.\n\n## 4.5. Other AI\n\nAI systems beyond the categories referred to above are not required to comply with requirements according to the AI Act.\n\nBut note : if you as deployer make use of 'other category' AI systems for a high-risk application as referred to in the AI Act (see 1.2. High-risk AI systems on page 5), it automatically becomes a high-risk AI system and you must comply with the requirements from the AI Act as a system provider . 52\n\n52 Article 25(1)(c) AI Act.\n\nColofon\n\nDit is een uitgave van;\n\nMinisterie van Economische Zaken Postbus 20901  |  2500 EX Den Haag www.rijksoverheid.nl", "fetched_at_utc": "2026-02-09T14:02:47Z", "sha256": "f93ca11c5a5d755b725def26b65512aaf2f02de5c720969262a960d04d7640ff", "meta": {"file_name": "Netherlands AI Act Guide.pdf", "file_size": 293902, "mtime": 1766951336, "docling_errors": []}}
{"doc_id": "pdf-pdfs-prohibited-ai-practices-oliver-patel-1c299f998524", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Prohibited AI practices - Oliver Patel.pdf", "title": "Prohibited AI practices - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nFor more frequent updates, be sure to follow me on LinkedIn.\n\nThis week's edition features:\n\n- âœ… Top 5 practical tips for prohibited AI compliance\n- âœ… What does the EU AI Act say on prohibited AI?\n- âœ… The European Commission's new Guidelines on prohibited AI (140 pages distilled)\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week.\n\n## Top 5 practical tips for prohibited AI compliance\n\nMany readers of Enterprise AI Governance are experienced lawyers, compliance executives, and governance leaders seeking practical advice and actionable insights.\n\nTherefore, before providing an overview of the prohibited AI practices and the European Commission's new Guidelines on the topic, I will frontload this week's edition with my practical tips for meeting Article 5 of the AI Act head on.\n\n1. Deliver company-wide communications, awareness, and training campaigns on prohibited AI. AI governance leaders need to constantly remind themselves that although AI is everywhere, the AI Act is still a relatively niche topic. Do not assume that everyone already knows about prohibited AI, just because it is top of mind for us. Indeed, the legal concept of prohibited AI practices is new for us all. Although there are many uses of AI that would of course be illegal, the AI Act is the first EU law which explicitly prohibits specific AI use cases and systems. Therefore, invest ample time in rolling out dedicated training and communications campaigns, so that your organisation is fully aware of these new provisions, their scope, and practical impact.\n2. Determine whether prohibited in the EU means prohibited worldwide. And be prepared to justify your decision. A realistic operational scenario that could arise is a team or department in your organisation, based outside of the EU, wanting to use an AI system, which is, or could potentially be, prohibited under\n\nthe AI Act. The use of this AI system is confined to one specific non-EU location. No AI outputs will be used in the EU and no EU employees, customers and stakeholders will be involved in any way. It is feasible that this use case could be perfectly legal in the relevant jurisdiction. Furthermore, perhaps it is not even particularly problematic, from an ethical or cultural standpoint. This raises a key question for global organisations: should prohibited in the EU mean prohibited worldwide? And, if so, what is the justification for this? On the one hand, the EU has prohibited certain AI practices due to their morally egregious nature and/or the potential for harm. However, on the other hand, no jurisdiction has a monopoly on morality.\n\n3. Do not assume that there are no activities in your organisation that breach these provisions. Be wary of the unknown unknowns. You may have a wellestablished and embedded AI governance process, which captures all new AI projects, initiatives, and vendor applications. It may be that nothing has ever come through which relates to, or closely resembles, any of the prohibited AI practices. Unfortunately, this does not necessarily mean that no such development, deployment, or use of prohibited AI is occurring. Assumptions are dangerous and you don't know what you don't know. That is why dedicated training, compliance, and audit exercises are crucial.\n\n4. Conduct dedicated compliance initiatives, such as assurance evaluations and audits. In large organisations, where it is difficult to keep track of and monitor everything, there is a need for bespoke compliance initiatives aimed at deterring, preventing, identifying, and halting AI activities which risk falling into the prohibited category. These initiatives are in addition to your BAU AI governance risk assessment processes. Ad hoc mechanisms, such as senior leadership attestation, assurance monitoring and evaluation, and internal and external audits, should all form part of your arsenal. Such initiatives will focus minds, hold leaders to account, and steer behavioural and cultural change in the right direction.\n5. Pay close attention to tools which may have emotional recognition capabilities, as there will be plenty of grey areas for companies. Every prohibited AI category has grey areas and exceptions, but perhaps none more so than emotional recognition. For most responsible organisations, it is reasonable to suggest that they are unlikely to develop, deploy, or use AI systems in most of the prohibited AI categories. Emotional recognition could be an outlier, as it is something that could be inadvertently or unintentionlly used, or turned on, in applications designed for recruitment, coaching, training, productivity, and employee and customer engagement or support. Furthermore, it may be that similar tools can lawfully be used to predict and monitor performance, sentiment, or behaviour, but close attention will need to be paid to ensure this does not cross over into emotional recognition. It is also not necessarily a capability which is\n\ndeemed socially, culturally, or morally unacceptable in every jurisdiction worldwide.\n\nBonus tip : without a robust AI governance framework and risk assessment process, which is based upon a solid policy foundation, sponsored by senior leadership, and embedded across the organisation, you will be much less likely to identify and prevent any prohibited AI practices. Always prioritise establishing the fundamental building blocks of AI governance, before turning your attention elsewhere.\n\n## What does the EU AI Act say on prohibited AI?\n\nArticle 5 of the EU AI Act may only take up three out of 144 pages, but it is probably the most consequential section. It outlines the AI practices that are now prohibited under EU law.\n\nThe provisions on prohibited AI practices became applicable on 2nd February 2025, exactly 6 months after the AI Act entered into force on 1st August 2024.\n\nThe prohibited AI practices are deemed to be particularly harmful, abusive, and contrary to EU values. Breaching these rules carries the largest potential enforcement\n\nfines of up to 7% of global annual turnover. This is a highly dissuasive figure which should focus minds in the boardroom.\n\nBelow, I will provide a digestible summary of the eight categories of prohibited AI practice.\n\nFor some of these, there are specific and limited exceptions, which must be carefully considered and reviewed on a case-by-case basis. For example, the practices of predictive policing and the use of emotion recognition systems are not outright prohibited in every scenario.\n\nAlso, some of these categories proved highly contentious during the AI Act's legislative process and trilogue negotiations. In particular, the question of whether law enforcement authorities should be permitted to use AI systems for real-time remote biometric identification in public was totemic. The European Parliament argued for an outright prohibition, whereas member states wanted their authorities to retain such capabilities. A compromise between these two positions was eventually struck.\n\nNow the dust has settled, it is prohibited, under EU law, to sell, make available, or use AI systems for any of the practices listed below.\n\n## Causing harm by deploying subliminal or deceptive techniques\n\n- AI systems which deploy subliminal, manipulative, or deceptive techniques in order to distort the behaviour of an individual or group, in a way which causes, or is likely to cause, significant harm.\n- This can include persuading or manipulating people to engage in unwanted behaviours.\n- This practice is prohibited even if there was no intention to cause harm.\n\n## Causing harm by exploiting vulnerabilities\n\n- AI systems which exploit the vulnerabilities of an individual or group (e.g., age, disability, or socioeconomic status) to distort that individual or group's behaviour in a way which causes, or is likely to cause, significant harm.\n- This practice is also prohibited even if there was no intention to cause harm.\n\n## Social credit scoring systems\n\n- AI systems which evaluate and score people based on social behaviour and personal characteristics, with the score leading to detrimental or unfavourable treatment which is disproportionate, unjustified, and/or unrelated to the context of the original data.\n- Such AI systems can lead to discrimination, marginalisation, and social exclusion.\n\n## Predictive policing\n\n- AI systems which are used to assess or predict the risk of an individual committing a criminal offence, where the assessment is based solely on automated profiling of their traits and characteristics.\n- It is unlawful to use any of the following traits as the sole basis for assessing and predicting whether someone will commit a crime: nationality, birth location, residence location, number of children, level of debt etc.\n- However, predictive policing AI systems are classified as high-risk (i.e., not prohibited), when they are not based solely on the type of automated profiling described above.\n\n## Emotion recognition in the workplace and education\n\n- AI systems which infer or detect emotions, based on biometric data (e.g., facial images, fingerprints, or physiological data) in the context of the workplace and educational institutions.\n- This includes emotions like happiness, excitement, sadness, and anger. However, it does not include physical states like pain or fatigue.\n- There are exemptions for emotional recognition systems used for medical or safety purposes. In such permissible scenarios, an emotion recognition system would be classified as high-risk.\n\n## Creating facial recognition databases via untargeted scraping\n\n- AI systems which create or expand databases for facial recognition, via untargeted scraping of facial images from the internet or CCTV footage.\n\n## Biometric categorisation to infer specific protected characteristics\n\n- AI systems which categorise individuals based on biometric data, to infer protected characteristics like race, political opinions, trade union membership, religious beliefs, sexual orientation, or sex life.\n- If biometric categorisation systems are used to infer characteristics or traits which are not protected, then those AI systems are classified as high-risk.\n\n## Law enforcement use of real-time biometric identification in public\n\n- Law enforcement use of 'real-time' remote biometric identification systems in public (e.g., facial recognition used to identify and stop potential suspects in public) is prohibited, apart from in very specific and narrowly defined scenarios. This prohibition is intended to safeguard privacy and prevent discrimination.\n- Acceptable scenarios for leveraging AI in this way include searching for victims of serious crime, preventing imminent threats to life (e.g., terrorist attacks), or locating suspects or perpetrators of serious crimes (e.g., murder).\n\n- However, independent judicial or administrative authorisation must be granted, before such AI systems are used. The use must only occur for a limited time period, with safeguards.\n- National regulators and data protection authorities must be notified each time an AI system is used in this way. The European Commission will publish an annual report tracking and documenting these use cases.\n- The development and placing on the market of AI systems intended for this purpose is, however, not prohibited. But there are prohibitions on the use of such systems.\n\n## The European Commission's new Guidelines on prohibited AI\n\nLast week, the European Commission published Draft Guidelines on prohibited AI practices. These shed light on how organisations can interpret, practically implement, and comply with these important provisions.\n\nThe guidelines are also designed to assist regulatory authorities reviewing such cases, although those authorities are not legally obliged to take these guidelines into account.\n\nAt 140 pages-which is not too dissimilar to the overall length of the EU AI Act-we get a glimpse of the task ahead. To be facetious, if the EU keeps up its rate of 140 pages of guidance for every 3 pages of legal text, then AI governance professionals may have over 6700 pages of overall text which they will need to understand, in order to fully grasp this new law.\n\nOn a more serious note, this highlights that over the coming months and years, there will be a huge amount of additional documentation and analysis, such as official guidelines, recommendations, opinions, standards, codes of practice, enforcement decisions, and court judgements, which we will all need to keep up with. Luckily, you've come to the right place ðŸ˜‰\n\n<!-- image -->\n\n## Summary of key points\n\n- Regulatory authorities should use these guidelines in relevant cases and investigations. However, despite the various examples provided, there will always need to be a detailed, case-by-case assessment of the AI system in question.\n- Although much enforcement of Article 5 will be actioned by regulators at the member state level, they must keep the Commission and other regulators informed regarding any cases with cross-border impact. Furthermore, regulatory authorities should strive for harmonisation on prohibited AI enforcement, via collaboration at the European AI Board.\n\n- The prohibitions apply to any AI system, whether it has a narrow 'intended purpose' which is prohibited', or whether it is a general-purpose AI system used in a manner which is prohibited. Deployers must therefore ensure that they do not use or customise general-purpose AI systems in a prohibited way.\n- There are other scenarios, under EU law, where the use of an AI system could be prohibited, even if this is not explicitly stipulated in the AI Act. For example, there are other conceivable uses of AI which could breach the EU's Charter of Fundamental Rights.\n\n## Example of potential prohibited scenarios for each category\n\nIn the guidelines, the European Commission provided some illustrative examples of what could be prohibited, across each category. I have provided a selection below.\n\n## Causing harm by deploying subliminal or deceptive techniques\n\n- An AI companionship app imitates how humans talk, act, and respond. It uses human-like traits and emotional signals to affect how users feel and think. This can make people emotionally attached to the app, leading to addiction-like behavior. In some cases, this might cause serious problems, like suicidal thoughts or even harm to others.\n\n## Causing harm by exploiting vulnerabilities\n\n- An AI chatbot targets and radicalises socio-economically disadvantaged people, encouraging them to harm or injure other people, by tapping into their fears, vulnerabilities, and sense of social exclusion.\n\n## Social credit scoring systems\n\n- A national labour agency uses AI to determine whether unemployed people should receive state employment benefits. As part of the scoring process, data is used and relied upon which is unrelated to the purpose of the evaluation, such as marital status, health problems, or addictions.\n\n## Predictive policing\n\n- A law enforcement authority uses AI to predict and determine that an individual is more likely to commit a terrorism offence, based solely on personal characteristics like their age, nationality, address, type of car, and marital status.\n\n## Emotion recognition in the workplace and education\n\n- Inferring emotions from written text (e.g., sentiment analysis) is not emotion recognition. However, inferring emotions from keystrokes, facial expressions, body postures, and movement is emotional recognition, as it is based on biometric data.\n\n## Creating facial recognition databases via untargeted scraping\n\n- A facial recognition company collects photos of people's faces using an automated tool that searches social media platforms. It gathers these images along with other data, such as the image's URL, location data, and the person's name. The company then processes the photos to extract facial features and converts them into mathematical data for easy storage and comparison. When someone uploads a new photo to their facial recognition system, it is checked for any matches in the facial database.\n\n## Biometric categorisation to infer specific protected characteristics\n\n- An AI system which categorises individuals' social media profiles based on their assumed sexual orientation, which is predicted by analysing biometric data from their photos, is prohibited.\n\n## ICYMI: get the full 17-page report which compares AI laws in the EU, China and U.S.A ðŸ‘‡ðŸ¼", "fetched_at_utc": "2026-02-09T14:03:14Z", "sha256": "1c299f998524cb7c234e6410fb69eecd1acc1c6b4626e180905315f39fe8a575", "meta": {"file_name": "Prohibited AI practices - Oliver Patel.pdf", "file_size": 916744, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-proposal-digital-omnibus-5f6a24febe2c", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Proposal Digital Omnibus.pdf", "title": "Proposal Digital Omnibus", "text": "<!-- image -->\n\nBrussels, 19.11.2025 COM(2025) 836 final\n\n2025/0359 (COD)\n\nProposal for a\n\n## REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n\namending Regulations (EU) 2024/1689 and (EU) 2018/1139 as regards the simplification of the implementation of harmonised rules on artificial intelligence (Digital Omnibus on AI)\n\n{SWD(2025) 836 final}\n\n(Text with EEA relevance)\n\n<!-- image -->\n\n<!-- image -->\n\n## EXPLANATORY MEMORANDUM\n\n## 1. CONTEXT OF THE PROPOSAL\n\n## Â· Reasons for and objectives of the proposal\n\nIn  its  Communication  on  a  Simpler  and  Faster  Europe  ( 1 ),  the  Commission  announced  its commitment to an ambitious programme to promote forward-looking, innovative policies that strengthen the European Union's (EU) competitiveness and lighten the regulatory burdens on people, businesses and administrations, while maintaining the highest standard in promoting its values.\n\nRegulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence ('AI Act'), which entered into force on 1  August  2024,  establishes  a  single  market  for  trustworthy  and  human-centric  artificial intelligence ('AI') across the EU. Its purpose is to promote innovation and the uptake of AI while ensuring a high level of protection for health, safety, and fundamental rights, including democracy and the rule of law.\n\nThe AI Act's entry into application occurs in stages, with all rules entering into application by 2 August 2027. The prohibitions on AI practices with unacceptable risks and the obligations for  general-purpose  AI  models  are  already  applicable.  However,  most  provisions  -  in particular those governing high-risk AI systems - will only start to apply from 2 August 2026 or  2  August  2027.  These  provisions  include  detailed  requirements  for  data  governance, transparency,  documentation,  human  oversight,  and  robustness,  so  as  to  ensure  that  AI systems placed on the EU market are safe, transparent, and reliable.\n\nThe Commission is committed to a clear, simple, and innovation-friendly implementation of the AI Act, as set out in the AI Continent Action Plan ( 2 ) and the Apply AI Strategy ( 3 ). Initiatives  such  as  the  General-Purpose  AI  Code  of  Practice,  Commission  guidelines  and templates, the AI Pact and the launch of the AI Act Service Desk build clarity regarding the applicable rules and support for their application. In particular, the website through which the AI  Act  Service  Desk  is  provided  offers  a  single  information  platform  ( 4 )  on  all  resources available to stakeholders to navigate the AI Act, including guidelines, national authorities and support  initiatives,  webinars,  and  harmonised  standards.  These  efforts  will  continue,  with further guidance and digital tools under preparation.\n\nBuilding on experience gained from the implementation of already applicable provisions, the Commission held a series of consultations, including a public consultation to identify potential challenges with implementing the AI Act's provisions, a call for evidence in preparation of the Digital Omnibus, a reality check allowing stakeholders to directly share their implementation  experiences  and  an  SME  panel  to  identify  their  particular  needs  in  the implementation of the AI Act.\n\n1 COM(2025) 47 final.\n\n2 COM(2025)165 final.\n\n3 COM(2025) 723 final.\n\n4 https://ai-act-service-desk.ec.europa.eu/\n\n<!-- image -->\n\n<!-- image -->\n\nThese  consultations  reveal implementation  challenges that  could  jeopardise  the  effective entry into application of key provisions of the AI Act. These include delays in designating national  competent  authorities  and  conformity  assessment  bodies,  as  well  as  a  lack  of harmonised  standards  for  the  AI  Act's  high-risk  requirements,  guidance,  and  compliance tools. Such delays risk significantly increasing the compliance costs for businesses and public authorities and slowing down innovation.\n\nTo address these challenges, the Commission is proposing targeted simplification measures to  ensure  timely,  smooth,  and  proportionate  implementation  of  certain  of  the  AI  Act's provisions. These include:\n\n- linking  the  implementation  timeline  of  high-risk  rules to  the  availability  of standards or other support tools;\n- extending regulatory simplifications granted to small and medium-sized enterprises  (SMEs)  to  small  mid-caps  (SMCs), including  simplified  technical documentation requirements and special consideration in the application of penalties;\n- requiring  the Commission and the Member States to foster  AI  literacy instead enforcing  unspecified  obligation  on  providers  and  deployers  of  AI  systems  in  this respect, while training obligations for high-risk deployers remain;\n- offering more flexibility in the post-market monitoring by removing a prescription of a harmonised post-market monitoring plan;\n- reducing the registration burden for providers of AI systems that are used in highrisk areas but for which the provider has concluded that they are not high-risk as they are only used for narrow or procedural tasks;\n- Centralising oversight over a large number of AI systems built on general-purpose AI models or embedded in very large online platforms and very large search engines with the AI Office;\n- facilitating compliance with the data protection laws by allowing providers and deployers of all AI systems and models to process special categories of personal data for ensuring bias detection and correction, with the appropriate safeguards;\n- a broader use of AI regulatory sandboxes and real-world testing , that will benefit European key industries such as the automotive industry, and facilitating an EU-level AI regulatory sandbox which the AI Office will set up as from 2028;\n- targeted  changes  clarifying  the interplay  between  the  AI  Act  and  other  EU legislation and adjusting the AI Act's procedures to improve its overall implementation and operation.\n\nBeyond the legislative measures,  the  Commission  is  taking further measures to facilitate compliance  with  the  AI  Act  and  address  the  concerns  raised by  stakeholders.  Further guidance is under preparation, focusing on offering clear and practical instructions to apply the AI Act in parallel with other EU legislation. This includes:\n\n- Guidelines on the practical application of the high-risk classification;\n- Guidelines  on  the  practical  application  of  the  transparency  requirements  under Article 50 AI Act;\n- Guidance on the reporting of serious incidents by providers of high-risk AI systems;\n- Guidelines on the practical application of the high-risk requirements;\n\n<!-- image -->\n\n<!-- image -->\n\n- Guidelines on the practical application of the obligations for providers and deployers of high-risk AI systems;\n- Guidelines with a template for the fundamental rights impact assessment;\n- Guidelines on the practical application of rules for responsibilities along the AI value chain;\n- Guidelines  on  the  practical  application  of  the  provisions  related  to  substantial modification;\n- Guidelines on the post-market monitoring of high-risk AI systems;\n- Gudelines  on  the  elements  of  the  quality  management  system  which  SMEs  and SMCs may comply with in a simplified manner;\n- Guidelines on the AI Act's interplay with other Union legislation, for example joint guidelines of the Commission and European Data Protection Board on the interplay of the AI Act and EU data protection law, guidelines on the interplay between the AI Act and the Cyber Resilience Act, and guidelines on the interplay between the AI Act and the Machinery Regulation;\n- Guidelines on the competences and designation procedure for conformity assessment bodies to be designated under the AI Act.\n\nIn  particular,  stakeholder  consultations  reveal  the  need  to  offer guidance on the practical application of the AI Act's research exemptions under Article 2(6) and (8), including how they apply in sectoral contexts like in the pre-clinical research and product development in the field  of  medicinal  products  or  medical  devices,  which  the  Commission  will  work  on  with priority.\n\nThese  simplification  efforts  will  help  to  ensure  that  the  implementation  of  the  AI  Act  is smooth, predictable, and innovation-friendly, enabling Europe to strengthen its position as the AI continent and to pursue an AI-first approach safely.\n\n## Â· Consistency with existing policy provisions in the policy area\n\nThe proposal is part of a broader Digital Package on Simplification composed of measures to reduce the administrative costs of compliance for businesses and administrations in the EU, which  applies  to  several  regulations  of  the  EU's  digital  acquis  without  compromising  the objectives of the underlying rules. The proposal builds on Regulation (EU) 2024/1689 and is aligned with existing policies to make the EU a global leader in AI, to make the EU an AI continent and to promote the uptake of human-centric and trustworthy a AI.\n\n## Â· Consistency with other Union policies\n\nThe proposal is part of a series of simplification packages.\n\n## 2. LEGAL BASIS, SUBSIDIARITY AND PROPORTIONALITY\n\n## Â· Legal basis\n\nThe  legal  basis  for  this  proposal  is  Article  114  of  the  Treaty  on  the  Functioning  of  the European Union (TFEU) in line with the original legal basis for the adoption of the legal acts which this proposal aims to amend.\n\n<!-- image -->\n\n<!-- image -->\n\n## Â· Subsidiarity (for non-exclusive competence)\n\nRegulation  (EU)  2024/1689  was  adopted  at  EU  level.  Accordingly,  amendments  to  that Regulation need to be made at EU level.\n\n## Â· Proportionality\n\nThe initiative does not go beyond what is necessary to achieve the objectives of simplification and burden reduction without lowering the protection of health, safety and fundamental rights.\n\n## Â· Choice of the instrument\n\nThe proposal amends Regulation (EU) 2024/1689 adopted by ordinary legislative procedure. Therefore,  the  amendments  to  that  Regulation  also  must  be  adopted  by  regulation  in accordance with the ordinary legislative procedure.\n\n## 3. RESULTS OF EX-POST EVALUATIONS, STAKEHOLDER CONSULTATIONS AND IMPACT ASSESSMENTS\n\n## Â· Ex-post evaluations/fitness checks of existing legislation\n\nThe  proposal  is  accompanied  by  a  Commission  staff  working  document  that  provides  a detailed  overview  of  the  impact  of  the  proposed  amendments  to  certain  provisions  of Regulation  (EU)  2024/1689.  It  also  provides  an  analysis  of  the  positive  impacts  of  the proposed  measures.  The  analysis  is  based  on  existing  data,  information  gathered  through consultations and during a reality check and through written stakeholder feedback through a call for evidence.\n\n## Â· Stakeholder consultations\n\nSeveral consultations were carried out in the context of the proposal. They all complemented one another, addressing either different topical issues or stakeholder groups concerned by the initiative.\n\nIn the initial scoping phase of the Digital Package on Simplification, three public consultations and calls for evidence were published on the key strands of the proposal in the spring of 2025. A consultation was held on the Apply AI Strategy from 9 April to 4 June 2025 ( 5 ), another on the revision of the Cybersecurity Act from 11 April to 20 June 2025 ( 6 ), and finally another on the European Data Union Strategy from 23 May to 20 July 2025 ( 7 ). Each consultation included a questionnaire with a section (or at times multiple) on implementation and  simplification  concerns,  directly  related  to  the  reflections  on  the  Digital  Package  on Simplification. Taken together, 718 responses were received as part of this first consultation exercise.\n\n5 European  Commission  (2025) Call  for  evidence  on  the  Apply  AI  Strategy .  Available  at:  Apply  AI Strategy - strengthening the AI continent\n\n6 European Commission (2025) Call for evidence on the revision of the Cybersecurity Act. Available at: The EU Cybersecurity Act\n\n7 European Commission (2025) Call for evidence on the European Data Union Strategy .  Available at: European Data Union Strategy\n\n<!-- image -->\n\n<!-- image -->\n\nFrom  16  September  to  14  October  2025,  a  call  for  evidence  on  the  Digital  Package  on Simplification was further published ( 8 ). Its aim was to cover the whole scope of the initiative and give an opportunity to stakeholders to comment on a more targeted set of proposals in one go. A total of 513 responses were received, by a wide range of stakeholders.\n\nWith a view to raising awareness on the Digital Package on Simplification among small and medium-sized enterprises (SMEs), and collecting their feedback, a dedicated SME Panel was organised  through  the  Enterprise  Europe  Network  (EEN)  between  4  September  and  16 October 2025. The EEN is the world's largest support network for SMEs and is implemented by the Commission's European Innovation Council and SMEs Executive Agency (EISMEA). SME Panels are a way to consult stakeholders falling under this framework. SMEs have the opportunity to contribute their views to upcoming policy initiatives. In addition to the online written  consultation  (where  106  SMEs'  responses  were  received),  the  Commission  also presented the Digital Package on Simplification to SME associations part of the EEN, in a meeting that took place on 1 October 2025.\n\nA  large  number  of  bilateral  meetings  were  organised  by  the  Commission  services  with stakeholders in 2025, to address specific concerns. Discussions were also held with Member States. In addition to bilateral exchanges, specific agenda points on the Digital Simplification Package were discussed at Council Working Parties in June and September 2025, where the Commission presented the current situation and asked Member States' to express their views.\n\nOverall, stakeholder feedback converged on the need for a simplified application of some of the  digital  rules.  Better  coherence,  and  a  focus  on  optimisation  of  compliance  costs,  was largely  supported  by  a  cross-section  of  stakeholders.  Some  differences  in  opinion  were expressed regarding some of the more tailored measures. A more detailed overview of these stakeholder consultations, and how they were reflected in the proposal can be found in the staff working document accompanying the Digital Package on Simplification.\n\n## Â· Collection and use of expertise\n\nIn  addition  to  the  consultation  outlined  above,  the  Commission  mainly  relied  on  its  own internal analysis for the purpose of this proposal.\n\n## Â· Impact assessment\n\nThe amendments put forward in the proposal are technical in nature. They are designed to ensure  a  more  efficient  implementation  of  rules  that  were  already  agreed  at  political  level. There  are  no  policy  options  that  could  meaningfully  be  tested  and  compared  in  an  impact assessment report.\n\nThe accompanying staff working document examines the reasoning behind the amendments and  outlines  the  views  of  stakeholders  on  the  different  measures.  It  also  presents  the  costs savings and other types of impacts the proposal may entail. In many cases, it builds on the impact assessments that was originally carried out for the Regulation (EU) 2024/1689.\n\n8 European  Commission  (2025) Call  for  evidence  on  the  digital  package  and  omnibus .  Available  at: Simplification - digital package and omnibus\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\nThe  staff  working  document  therefore  serves  as  a  reference  point  to  inform  the  European Parliament  and  the  Council's  debate  on  the  proposal,  as  well  as  the  public,  in  a  clear  and engaged way.\n\n## Â· Regulatory fitness and simplification\n\nThe proposal aims to produce a significant reduction in administrative burden for businesses, national administrations, and the public at large. Initial estimates project possible savings of â‰ˆ EUR 297.2 to 433.2 million .  Non-quantifiable benefits are also expected, notably due to a streamlined set of rules which will ease compliance and enforcement thereof.\n\nSMEs already  benefit  from  regulatory  privileges  under  Regulation  (EU)  2024/1689.  Some regulatory privileges already afforded to SMEs are extended to small mid-caps (SMCs). Since SMEs  and  SMCs  are  disproportionality  more  impacted  by  the  compliance  burden,  it  is expected that they will particularly benefit from these simplification measures.\n\nThe  proposal  is  consistent  with  the  Commission's  'Digital  Fitness  Check  for  the  digital rulebook',  which  aims  to  ensure  properly  aligned  policy  proposals  with  real-world  digital environments (see Chapter 4 on Legislative and Financial Digital Statement).\n\n## Â· Fundamental rights\n\nRegulation (EU) 2024/1689 is expected to promote the protection of a number of fundamental rights and freedoms set out in the EU Charter of Fundamental Rights ( 9 ), as well as positively impacting the rights of a number of special groups ( 10 ). At the same time, the Regulation (EU) 2024/1689  imposes  some  restrictions  on  certain  rights  and  freedoms  ( 11 ),  which  are proportionate and limited to the minimum necessary. The proposal is not expected to modify the impact of the Regulation (EU) 2024/1689 on fundamental rights since the targeted nature of  envisaged  amendments  do  not  affect  the  scope  of  the  regulated  AI  systems  or  on  the substantive requirements applicable to those systems.\n\n## 4. BUDGETARY IMPLICATIONS\n\nThe proposal amends the supervision and enforcement system of Regulation (EU) 2024/1689, whereby oversight over certain AI systems will be transferred to the Commission's AI Office. In addition, to facilitate compliance by operators, the AI Office should set up an EU-level AI regulatory  sandbox.  To  implement  these  new  tasks,  to  the  Commission  will  need  the appropriate resources, which is estimated to stand at 53 FTE, of which 15 FTE can be covered\n\n9 In detail: the right to human dignity (Article 1), respect for private life and protection of personal data (Articles 7 and 8), non-discrimination (Article 21) and equality between women and men (Article 23), freedom of expression (Article 11) and freedom of assembly (Article 12), right to an effective remedy and to a fair trial, the rights of defence, and the presumption of innocence (Articles 47 and 48), right to a high level of environmental protection and the improvement of the quality of the environment (Article 37).\n\n10 In  detail:  workers'  rights  to  fair  and  just  working  conditions  (Article  31),  a  high  level  of  consumer protection (Article 28), the rights of the child (Article 24) and the integration of persons with disabilities (Article 26).\n\n11 In detail: the freedom to conduct business (Article 16) and the freedom of art and science (Article 13).\n\n<!-- image -->\n\n<!-- image -->\n\nby internal redeployment. These implications have to be considered against the backdrop of reduced budgetary implications  for  the  Member States  which  no  longer  have  to  ensure  the oversight  for  those  certain  AI  systems.  A  detailed  overview  of  the  costs  involved  in  this transfer  of  competences  is  provided  in  the  'Legislative  and  Financial  Digital  Statement' accompanying this proposal.\n\n## 5. OTHER ELEMENTS\n\n## Â· Implementation plans and monitoring, evaluation and reporting arrangements\n\nThe Commission will monitor the implementation, application, and compliance with the new provisions.  Furthermore,  the  Regulation  that  is  amended  by  this  proposal  is  regularly evaluated for its efficiency, effectiveness in reaching its objectives, relevance, coherence and value added in line with the EU's better regulation principles. This proposal does not require an implementation plan.\n\n## Â· Explanatory documents (for directives)\n\nNot applicable.\n\n## Â· Detailed explanation of the specific provisions of the proposal\n\nArticle 1 amends Regulation (EU) 2024/1689 ('AI Act'). In particular,\n\n- Paragraph 1 adds a reference to SMCs in the subject matter of the AI Act.\n- Paragraph 2 is a technical change that is necessary to enable extending the real-world testing  to  high-risk  AI  systems  embedded  in  products  covered  under  Section  B  of Annex I AI Act.\n- Paragraph 3 adds legal definitions for SME and SMC to the definitions in Article 3 of the AI Act.\n- Paragraph 4 transforms the obligation for providers and deployers of AI systems with regards to AI literacy in Article 4 AI Act to an obligation on the Commission and the Member States to foster AI literacy.\n- Paragraph  5  introduces  a  new  Article  4a,  replacing  Article  10(5)  AI  Act,  which provides a legal basis for providers and deployers of AI systems and AI models to exceptionally process special categories of personal data for the purpose of ensuring bias detection and correction under certain conditions.\n- Paragraphs  6,  14  and  32  refer  to  the  deletion  of  the  obligation  for  providers  to register AI systems in the EU database for high-risk systems under Annex III, where they have been exempted from classification as high-risk under Article 6(3) AI Act, because they are for instance only used for preparatory tasks.\n- Paragraph 7 contains editorial follow-up changes to amendments made by paragraph 4.\n- Paragraphs 8 and 9 extend existing regulatory privileges of the AI Act for SMEs to SMCs on technical documentation and putting in place a quality management system that takes into account their size.\n- Paragraph  10  introduces  a  new  procedure  in  Article  28  AI  Act,  whereby  Member States  are  required  to  ensure  that  a  conformity  assessment  body  that  applies  for designation both under this Regulation and Union harmonization legislation listed in\n\n<!-- image -->\n\n<!-- image -->\n\nSection A of Annex I AI Act shall be provided with the possibility to submit a single application and undergo a single assessment procedure to be designated.\n\n- Paragraph 11 proposes to replace paragraph 4 of Article 29 AI Act which requires conformity assessment bodies to submit a single application  in the  cases  to  which reference is made in that paragraph.\n- Paragraph 12 amends Article 30 AI Act by requiring conformity assessment bodies which  apply  to  be  designated  as  notified  bodies  to  make  that  application  in accordance  with  the  codes,  categories,  and  corresponding  types  of  AI  systems referred to in a new Annex XIV for the Commission's New Approach Notified and Designated  Organisations  ('NANDO')  information  system,  and  empowers  the Commission to amend these codes, categories, and corresponding types in light of technological developments.\n- Paragraph 13 clarifies the conformity assessment procedure laid down in Article 43 AI Act where a high-risk AI system is covered by Union harmonisation legislation listed under Section A of Annex I to the AI Act and where an AI system is classified as high-risk both under Annex I and Annex III to the AI Act.\n- Paragraphs 15 and 16 remove the Commission empowerments in Articles 50 and 56 AI Act to adopt implementing acts to give codes of practice for general purpose AI models and transparency obligations for certain AI  systems general  validity  in  the Union.\n- Paragraph  17  introduces  amendments  to  the  rules  on  AI  regulatory  sandboxes  in Article  57  AI  Act,  inter  alia,  by  providing  the  legal  basis  for  the  AI  Office  to introduce  an  AI  regulatory  sandbox  on  EU  level  for  certain  AI  systems  within  its exclusive competence of supervision and require Member States to strengthen crossborder cooperation of their sandboxes.\n- Paragraph 18 specifies the empowerment of the Commission to adopt implementing acts  specifying  the  detailed  arrangements  for  the  establishment,  development, implementation, operation, governance and supervision of AI regulatory sandboxes.\n- Paragraph 19 introduces changes to the testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes as governed by Article 60 AI Act, inter alia  extending  this  opportunity  to  high-risk  AI  systems  covered  by  Section  A  of Annex I.\n- Paragraph 20 creates an additional legal basis for interested Member States and the Commission, on voluntary basis, to enter into written agreements to test high-risk AI systems referred to in Section B of Annex I in real world-conditions.\n- Paragraph 21 extends the derogation from micro-enterprises to SMEs to comply with certain elements of the quality management system required by Articleâ€¯17 AI Act in aâ€¯simplified manner .\n- Paragraph 22 removes an empowerment of the Commission in Article 69 AI Act to adopt  an  implementing  act  in  relation  to  the  reimbursement  of  experts  of  the scientific panel when called upon by Member States, to simplify the procedure.\n- Paragraph 23 extends the focus of guidance which national authorities may provide from SMEs to SMCs.\n- Paragraph 24 replaces the empowerment of the Commission in Article 72 AI Act to adopt an implementing act with regard to the post-market monitoring plan.\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\n- Paragraph 25 makes amendments to the supervision and enforcement of certain AI systems in Article 75 AI Act:\n- Point (a) changes the heading.\n- Point (b) reinforces the competence of the AI Office for the supervision and enforcement  of  certain  AI  systems,  that  are  based  on  a  general-purpose  AI model, where the model and the system are provided by the same provider. At the  same  time,  the  provision  clarifies  that  AI  systems  related  to  products covered  under  Annex  I  are  not  included  in  that  supervision.  Moreover,  it  is clarified that the supervision and enforcement of the compliance of AI systems embedded in designated very large online platforms or very large online search engines should fall under the competence of the AI Office.\n- Point (c) introduces several new paragraphs, empowering the Commission to adopt implementing acts to define the enforcement powers and the procedures for  the  exercise  of  those  powers  of  the  AI  Office,  introducing  a  reference  to Regulation  (EU)  2019/1020  ensuring  certain  procedural  safeguards  apply  to providers  covered  and  empowering  the  Commission  to  carry  out  conformity assessments of AI systems within the scope of Article 75.\n- Paragraph  26  amends  Article  77  AI  Act  as  regards  the  powers  of  authorities  or bodies  protecting  fundamental  rights  and  cooperation  with  market  surveillance authorities.\n- Paragraphs  27  and  28  extends  provisions  in  Articles  95  and  96  that  require  that voluntary support tools should take into account the needs of SMEs to SMCs.\n- Paragraph 29 extends existing regulatory privileges in Article 99 AI Act on penalties for SMEs to SMCs.\n- Paragraph  30  contains  amendments  to  Article  111  AI  Act  which  result  from amendments made in paragraph 30 and introduces a transitional period of 6 months for providers who need to retroactively include technical solutions in their generative AI systems, to make them machine readable and detectable as artificially generated or manipulated.\n- Paragraph 31 introduces changes to the entry into application of certain provisions of the AI Act:\n- For  the  obligations  for  high-risk  AI  systems  in  Chapter  III,  a  mechanism  is introduced that links the entry into application to the availability of measures in support of compliance with the AI Act's high-risk rules, such as harmonised standards, common specifications, and Commission guidelines. This availability will be confirmed by the Commission by decision, following which the rules for high-risk AI systems start to apply after an appropriate transition period.  However,  this  flexibility  should  apply  only  for  a  limited  time  and  a definite date by which the rules apply in any case should be set. Moreover, it is appropriate to distinguish between the two types of AI systems that classify as high-risk and extend a longer transition period to AI systems that classify as high-risk pursuant to Article 6(1) and Annex I to the AI Act.\n- It is clarified that  the  amendments  necessary  to  integrate  the  high-risk requirements into sectoral law listed in Section B of Annex I apply with the Digital Omnibus' entry into force.\n\n<!-- image -->\n\n<!-- image -->\n\n- Paragraph 33 is related to the change in paragraph 11 and introduces a new Annex XIV setting out codes, categories, and corresponding types of AI systems referred to in a new Annex XIV for the Commission's New Approach Notified and Designated Organisations ('NANDO') information system.\n\nArticle 2 makes amendments with regards to Regulation (EU) 2018/1139, to allow a smooth integration of the AI Act's high-risk requirements into that Regulation.\n\nArticle 3 provides the rule of entry into force and the binding nature of this Regulation.\n\n<!-- image -->\n\n<!-- image -->\n\n## 2025/0359 (COD)\n\n## Proposal for a\n\n## REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL\n\namending Regulations (EU) 2024/1689 and (EU) 2018/1139 as regards the simplification of the implementation of harmonised rules on artificial intelligence (Digital Omnibus on AI)\n\n(Text with EEA relevance)\n\n## THE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION,\n\nHaving  regard  to  the  Treaty  on  the  Functioning  of  the  European  Union,  and  in  particular Article 114 thereof,\n\nHaving regard to the proposal from the European Commission,\n\nAfter transmission of the draft legislative act to the national parliaments,\n\nHaving regard to the opinion of the European Economic and Social Committee 1 ,\n\nHaving regard to the opinion of the Committee of the Regions 2 ,\n\nActing in accordance with the ordinary legislative procedure,\n\nWhereas:\n\n- (1) Regulation (EU) 2024/1689 of the European Parliament and of the Council 3 lays down harmonised rules on artificial intelligence (AI) and aims to improve the functioning of the internal market, to promote the uptake of human-centric and trustworthy artificial intelligence, while ensuring a high level of protection of health, safety and fundamental  rights,  and  supporting  innovation.  Regulation  (EU)  2024/1689  entered into  force  on  1  August  2024.  Its  provisions  enter  into  application  in  a  staggered manner, with all rules entering into application by 2 August 2027.\n- (2) The experience gathered in implementing the parts of Regulation (EU) 2024/1689 that have already entered into application can inform the implementation of those parts that are yet to apply. In this context, the delayed preparation of standards, which should provide technical solutions for providers of high-risk AI systems to ensure compliance with their obligations under  that regulation, and  the delayed  establishment  of\n\n1 OJ C , , p. .\n\n2 OJ C , , p. .\n\n3 Regulation  (EU)  2024/1689  of  the  European  Parliament  and  of  the  Council  of  13  June  2024  laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013,  (EU)  No  168/2013,  (EU)  2018/858,  (EU)  2018/1139  and  (EU)  2019/2144  and  Directives 2014/90/EU,  (EU)  2016/797  and  (EU)  2020/1828  (Artificial  Intelligence  Act)  (OJ  L,  2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).\n\n<!-- image -->\n\n<!-- image -->\n\nthe governance and the conformity assessment frameworks at national level result in a compliance  burden  that  is  heavier  than  expected.  In  addition,  consultations  of stakeholders have revealed the need for additional measures that facilitate and provide clarification  on  the  implementation  and  compliance,  without  reducing  the  level  of protection for health, safety and fundamental rights from AI-related risks that the rules of Regulation (EU) 2024/1689 seek to achieve.\n\n- (3) Consequently, targeted amendments to Regulation (EU) 2024/1689 are necessary to address certain implementation challenges, with a view to the effective application of the relevant rules.\n- (4) Enterprises  outgrowing  the  micro,  small  and  medium-sized  enterprises  ('SME') definition - the 'small mid-cap enterprises' ('SMCs') - play a vital role in the Union's economy. Compared to SMEs, SMCs tend to demonstrate a higher pace of growth, and level of innovation and digitisation. Nevertheless, they face challenges similar to SMEs in relation to administrative burden, leading to a need for proportionality in the implementation of Regulation (EU) 2024/1689 and for targeted support. To enable the smooth transition of enterprises from SMEs into SMCs, it is important to address in a coherent  manner  the  effect  that  regulation  may  have  on  their  activity  once  those enterprises outgrow the segment of SMEs and are faced with rules that apply to large enterprises. Regulation (EU) 2024/1689 provides for several measures for small-scale providers,  which  should  be  extended  to  SMCs.  In  order  to  clarify  the  treatment  of SMEs  and  SMCs  in  Regulation  (EU)  2024/1689,  it  is  necessary  to  introduce definitions for SMEs and SMCs, which should correspond to the definition set out in the Annex to Commission Recommendation 2003/361/EC 4  and Annex to Commission Recommendation 2025/3500/EC 5 .\n- (5) Article  4  of  Regulation  (EU)  2024/1689  currently  imposes  an  obligation  on  all providers and deployers of AI systems to ensure AI literacy of their staff. AI literacy development starting from education and training and continuing in a lifelong learning manner is  crucial  to  equip  providers,  deployers  and  other  affected  persons  with  the necessary  notions  to  make  informed  decisions  regarding  AI  systems  deployment. However, experience shared by stakeholders reveals that a one-size-fits-all solution is not suitable for all types of providers and deployers in relation to the promotion of AI literacy, rendering such a horizontal obligation ineffective in achieving the objective pursued by this provision. Moreover, data indicate that imposing such an obligation creates an additional compliance burden, particularly for smaller enterprises, whereas AI  literacy  should  be  a  strategic  priority,  regardless  of  regulatory  obligations  and potential sanctions. In light of that, Article 4 of Regulation (EU) 2024/1689 should be amended to require the Member States and the Commission, without prejudice to their respective competences, to individually, collectively and in cooperation with relevant stakeholders  encourage  providers  and  deployers  to  provide  a  sufficient  level  of  AI literacy  of  their  staff  and  other  persons  dealing  with  the  operation  and  use  of  AI\n\n4 Commission Recommendation of 6 May 2003 concerning the definition of micro, small and mediumsized enterprises (OJ L 124, 20.5.2003, pp. 36-41, ELI: http://data.europa.eu/eli/reco/2003/361/oj).\n\n5 Commission  Recommendation  (EU)  2025/1099  of  21  May  2025  on  the  definition  of  small  mid-cap enterprises (OJ L, 2025/1099, 28.5.2025, ELI: http://data.europa.eu/eli/reco/2025/1099/oj).\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\nsystems on their behalf,  including  through  offering  training  opportunities,  providing informational  resources,  and  allowing  exchange  of  good  practices  and  other  nonlegally binding initiatives. The European Artificial Intelligence Board ('Board') will ensure recurrent exchange between the Commission and Member States on the topic, while  the  Apply  AI  Alliance  will  allow  discussion  with  the  wider  community.  This amendment is without prejudice to the broader measures taken by the Commission and the Member States to promote AI literacy and competences for the wider population, including  learners,  students,  and  citizens  at  different  ages  and  in  particular  through education and training systems.\n\n- (6) Bias  detection  and  correction  constitute  a  substantial  public  interest  because  they protect natural persons from biases' adverse effects, including discrimination. Discrimination  might  result  from  the  bias  in  AI  models  and  AI  systems  other  than high-risk AI systems for which of Regulation (EU) 2024/1689 already provides a legal basis  authorising  the  processing  of  special  categories  of  personal  data  under  Article 9(2), point (g), of Regulation (EU) 2016/679 of the European Parliament and of the Council 6 . Given that discrimination might result also from those other AI systems and models, it is therefore appropriate that Regulation (EU) 2024/1689 should provide for a legal basis for the processing of special categories of personal data also by providers and deployers of other AI systems and AI models as well as deployers of high-risk AI systems. The legal basis is established in compliance with Article 9(2), point (g) of Regulation (EU) 2016/679 Article 10(2), point (g) of Regulation (EU) 2018/1725 of the  European  Parliament  and  of  the  Council 7 and  Article  10,  point  (a)  of  Directive (EU) 2016/680 of the European Parliament and of the Council 8 provides a legal basis allowing,  where  necessary  for  the  detection  and  removal  of  bias,  the  processing  of special categories of personal data by providers and deployers of all AI systems and models, subject to appropriate safeguards that complement Regulations (EU) 2016/679, Regulation (EU) 2018/1725 and Directive (EU) 2016/680, as applicable.\n- (7) In order to ensure consistency, avoid duplication and minimise administrative burdens in  relation  to  the  procedure  for  designating  notified  bodies  under  Regulation  (EU) 2024/1689, while maintaining the same level of scrutiny, a single application and a single  assessment  procedure  should  be  available  for  new  conformity  assessment\n\n6 Regulation  (EU)  2016/679  of  the  European  Parliament  and  of  the  Council  of  27  April  2016  on  the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1, ELI: http://data.europa.eu/eli/reg/2016/679/oj).\n\n7 Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of natural persons with regard to the processing of personal data by the Union institutions, bodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No 45/2001 and Decision No 1247/2002/EC (OJ L 295, 21.11.2018, p. 39, ELI: http://data.europa.eu/eli/reg/2018/1725/oj).\n\n8 Directive  (EU)  2016/680  of  the  European  Parliament  and  of  the  Council  of  27  April  2016  on  the protection of natural persons with regard to the processing of personal data by competent authorities for the  purposes  of  the  prevention,  investigation,  detection  or  prosecution  of  criminal  offences  or  the execution  of  criminal  penalties,  and  on  the  free  movement  of  such  data,  and  repealing  Council Framework Decision 2008/977/JHA (OJ L 119, 4.5.2016, pp. 89-131, ELI: http://data.europa.eu/eli/dir/2016/680/oj).\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\nEN\n\nbodies  and  notified  bodies  which  are  designated  under  the  Union  harmonisation legislation listed in Section A of Annex I to Regulation (EU) 2024/1689, such as under Regulations (EU) 2017/745 9 and (EU) 2017/746 10 of the European Parliament and of the  Council,  where  such  a  procedure  is  established  under  that  Union  harmonisation legislation.  The  single  application  and  assessment  procedure  aims  at  facilitating, supporting and expediting the designation procedure under Regulation (EU) 2024/1689,  while  ensuring  compliance  with  the  requirements  applicable  to  notified bodies under that Regulation and the Union harmonisation legislation listed in Section A of Annex I thereto.\n\n- (8) With a view to ensuring the smooth application and consistency of Regulation (EU) 2024/1689, amendments should be made to it. A technical correction to Article 43(3), first  subparagraph,  of  Regulation  (EU)  2024/1689  should  be  added  to  align  the conformity assessment requirements with the requirements of providers of high-risk AI  systems  in  Article  16  of  that  Regulation.  Moreover,  it  should  be  clarified  that where  a  provider  of  a  high-risk  AI  system  is  subject  to  the  conformity  assessment procedure  under  Union  harmonisation  legislation  listed  in  Section  A  of  Annex  I  to Regulation (EU) 2024/1689, and the conformity assessment extends to compliance of the quality management system of that Regulation and of such Union harmonisation legislation, the provider should be able to include aspects related to quality management systems under that Regulation as part of the quality management systems under such Union harmonisation legislation, in line with Article 17(3) of Regulation (EU)  2024/1689.  Article  43(3),  second  subparagraph,  should  be  amended  to  clarify that  notified  bodies  which  have  been  notified  under  the  Union  harmonisation legislation listed in  Section  A  of  Annex  I to  Regulation  (EU)  2024/1689 and which aim  to  assess  high-risk  AI  systems  covered  by  the  Union  harmonisation  legislation listed in Section A of Annex I to that Regulation, should apply for the designation as a notified body under that Regulation within 18 months from [the entry into application of this Regulation]. This amendment is without prejudice to Article 28 of Regulation (EU) 2024/1689. Moreover, Regulation (EU) 2024/1689 should be amended to clarify that  where  a  high-risk  AI  system  is  both  covered  by  the  Union  harmonisation legislation  listed  in  Section  A  of  Annex  I  to  Regulation  (EU)  2024/1689  and  falls within one of the use-cases listed in Annex III to that Regulation, the provider should follow the relevant conformity assessment procedure as required under that relevant harmonisation legislation.\n- (9) To  streamline  compliance  and  reduce  the  associated  costs,  providers  of  AI  systems should not be required to register AI systems referred to in Article 6(3) of Regulation (EU)  2024/1689  in  the  EU  database  pursuant  to  Article  49(2)  of  that  Regulation. Given that such systems are not considered high-risk under certain conditions where\n\n9 Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices,  amending  Directive  2001/83/EC,  Regulation  (EC)  No  178/2002  and  Regulation  (EC)  No 1223/2009 and repealing  Council  Directives  90/385/EEC  and  93/42/EEC  (OJ  L  117,  5.5.2017,  p.  1, ELI: http://data.europa.eu/eli/reg/2017/745/oj).\n\n10 Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro diagnostic medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176, ELI: http://data.europa.eu/eli/reg/2017/746/oj).\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\nthey do not pose significant risk of harm to the health, safety or fundamental rights of persons, imposing  registration requirements  would  constitute a disproportionate compliance  burden.  Nevertheless,  a  provider  who  considers  that  an  AI  system  falls under Article 6(3) remains obligated to document its assessment before that system is placed on the market or put into service. This assessment may be requested by national competent authorities.\n\n- (10) Articles 57, 58 and 60 of Regulation (EU) 2024/1689 should be amended to strengthen further  cooperation  at  Union  level  of  AI  regulatory  sandboxes,  foster  clarity  and consistency in the governance of AI regulatory sandboxes, and to extend the scope of real-world testing outside AI regulatory sandboxes to high-risk AI systems covered by the Union harmonisation legislation listed in Annex I to that Regulation. In particular, to allow procedural simplification, where applicable, in the projects supervised in the AI  regulatory  sandboxes  that  include  also  real-world  testing,  the  real-world  testing plan should be integrated in the sandbox plan agreed by the providers or prospective providers  and  the  competent  authority  in  a  single  document.  In  addition,  it  is appropriate to provide for the possibility of the AI Office to establish an AI regulatory sandbox at Union level for AI systems that are covered by Article 75(1) of Regulation (EU)  2024/1689.  By  leveraging  these  infrastructures  and  facilitating  cross-border collaboration,  coordination  would  be  better  streamlined  and  resources  optimally utilised.\n- (11) To foster innovation, it is  also appropriate  to  extend  the  scope  of  real-world  testing outside  AI  regulatory  sandboxes  in  Article  60  of  Regulation  (EU)  2024/1689, currently applicable to high-risk AI systems listed in Annex III to that Regulation, and allow  providers  and  prospective  providers  of  high-risk  AI  systems  covered  by  the Union harmonisation legislation listed in Annex I to that Regulation to also test such systems in real-world conditions. This is without prejudice to other Union or national law on the testing in real-world conditions of high-risk AI systems related to products covered by that Union harmonisation legislation. To address the specific situation of high-risk AI systems covered the Union harmonisation legislation listed in Section B of  Annex  I  to  that  Regulation,  it  is  necessary  to  allow  the  conclusion  of  voluntary agreements  between  the  Commission  and  Member  States  to  enable  testing  of  such high-risk AI systems in real-world conditions.\n- (12) Article 63 of Regulation (EU) 2024/1689 offers microenterprises who are providers of high-risk AI systems the possibility to benefit from a simplified way to comply with the obligation to establish a quality management system. With a view to facilitating compliance  for  more  innovators,  that  possibility  should  be  extended  to  all  SMEs, including start-ups.\n- (13) Article  69  of  Regulation  (EU)  2024/1689  should  be  amended  to  simplify  the  fee structure of the scientific panel. If Member States call upon the panel's expertise, the fees they may be required to pay the experts should be equivalent to the remuneration the Commission is obliged to pay in similar circumstances. Furthermore, to reduce the procedural  complexity,  Member  States  should  be  able  to  consult  the  experts  of  the scientific panel directly, without involvement of the Commission.\n- (14) In order to strengthen the governance system for AI systems based on general-purpose AI  models,  it  is  necessary  to  clarify  the  role  of  the  AI  Office  in  monitoring  and supervising compliance of such AI systems with Regulation (EU) 2024/1689, while excluding  AI  systems  related  to  products  covered  by  the  Union  harmonisation legislation listed in Annex I to that Regulation. While sectoral authorities continue to\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\nremain responsible for the supervision of AI systems related to products covered by that Union harmonisation legislation, Article 75(1) Regulation (EU) 2024/1689 should be modified to bring all AI systems based on general-purpose AI models developed by the  same  provider  within  the  scope  of  the  AI  Office's  supervision.  This  does  not include  AI  systems  placed  on  the  market,  put  into  service  or  used  by  Union institutions,  bodies,  offices  or  agencies,  which  are  under  the  supervision  of  the European  Data  Protection  Supervisor  pursuant  to  Article  74(9)  of  Regulation  (EU) 2024/1689. To ensure effective supervision for those AI systems in accordance with the tasks and  responsibilities  assigned  to  market  surveillance  authorities  under Regulation  (EU)  2024/1689,  the  AI  Office  should  be  empowered  to  take  the appropriate measures and decisions to adequately exercise its powers provided for in that  Section  and  Regulation  (EU) 2019/1020 of the European Parliament and of the Council 11 .  Article  14  of  Regulation (EU) 2019/1020 should apply mutatis mutandis. Furthermore, to ensure effective enforcement, the authorities involved in the application of Regulation (EU) 2024/1689 should cooperate actively in the exercise of those powers, in particular where enforcement actions need to be taken in the territory of a Member State.\n\n- (15) Considering the existing supervisory and enforcement system under Regulation (EU) 2022/2065 of the European Parliament and of the Council 12 , it is appropriate to grant the  Commission  the  powers  of  a  competent  market  surveillance  authority  under Regulation  (EU)  2024/1689  where  an  AI  system  qualifies  as  a  very  large  online platform or a very large online search engine within the meaning of Regulation (EU) 2022/2065, or where it is embedded in such a platform or search engine. This should contribute to ensuring that the  exercise  of  the  Commission's  supervision  and enforcement powers under Regulation (EU) 2024/1689 and Regulation (EU) 2022/2065, as well as those applicable to general-purpose AI models integrated into such platforms or search engines, are carried out in a coherent manner. In the case of AI  systems  embedded  in  or  qualifying  as  a  very  large  online  platform  or  search engine,  the  first  point  of  entry  for  the  assessment  of  the  AI  systems  are  the  risk assessment, mitigating measures and audit obligations prescribed by Articles 34, 35 and 37 of Regulation (EU) 2022/2065, without prejudice to the AI Office's powers to investigate and enforce ex post non-compliance with the rules of this Regulation. In the context of the analysis of this risk assessment, mitigating measures and audits, the Commission services responsible for the enforcement of Regulation (EU) 2022/2065 may seek the opinion of the AI Office on the outcome of a potential earlier or parallel risk assessment carried out under this Regulation and the applicability of prohibitions under this Regulation. In addition, the AI Office and the competent national authorities under (EU) 2024/1689 should coordinate their enforcement efforts with the authorities\n\n11 Regulation (EU) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market surveillance and compliance of products and amending Directive 2004/42/EC and Regulations (EC) No 765/2008 and (EU) No 305/2011 (OJ L 169, 25.6.2019, p. 1, ELI: http://data.europa.eu/eli/reg/2019/1020/oj).\n\n12 Regulation (EU) 2022/2065 of the European Parliament and of the Council of 19 October 2022 on a Single Market For Digital Services and amending Directive 2000/31/EC (Digital Services Act) (OJ L 277, 27.10.2022, p. 1, ELI: http://data.europa.eu/eli/reg/2022/2065/oj).\n\n<!-- image -->\n\n<!-- image -->\n\ncompetent  for  the  supervision  and  enforcement  of  Regulation  (EU)  2022/2065, including the Commission, in order to ensure that the principles of loyal cooperation, proportionality and non bis in idem are respected, while information obtained under the  respective  other  Regulation  would  be  used  for  the  purposes  of  supervision  and enforcement  of  the  other  only  provided  the  undertaking  agrees.  In  particular,  those authorities should exchange views regularly and take into account, in their respective areas  of  competence,  any  fines  and  penalties  imposed  on  the  same  provider  for  the same conduct through a final decision in proceedings relating to an infringement of other  Union  or  national  rules,  so  as  to  ensure  that  the  overall  fines  and  penalties imposed  are  proportionate  and  correspond  to  the  seriousness  of  the  infringements committed.\n\n- (16) To  further  operationalise  the  AI  Office's  supervision  and  enforcement  set  out  in Article 75(1) of Regulation (EU) 2024/1689, it is necessary to further define the which of the powers listed in Article 14 of Regulation (EU) 2019/1020 should be conferred upon  the  AI  Office.  The  Commission  should  therefore  be  empowered  to  adopt implementing acts to specify those powers, including the ability to impose penalties, such as fines or other administrative sanctions, in accordance with the conditions and ceilings referred to in Article 99, and applicable procedures. This should ensure that the AI Office has the necessary tools to effectively monitor and supervise compliance with Regulation (EU) 2024/1689.\n- (17) Additionally,  it  is  essential  to  ensure  that  effective  procedural  safeguards  apply  to providers of AI systems subject to monitoring and supervision by the AI Office. To that  end,  the  procedural  rights  provided  for  in  Article  18  of  Regulation  (EU) 2019/1020  should  apply  mutatis  mutandis  to  providers  of  AI  systems,  without prejudice to more  specific procedural rights provided  for in  Regulation  (EU) 2024/1689.\n- (18) To enable access to Union market for AI systems which are under the supervision by the AI Office pursuant to Article 75 of Regulation (EU) 2024/1689 and subject to third party  conformity  assessment,  the  Commission  should  be  enabled  to  carry  out  premarket conformity assessments of those systems.\n- (19) Article  77  and  related  provisions  of  Regulation  (EU)  2024/1689  constitute  an important  governance  mechanism,  as  they  aim  to  enable  authorities  or  bodies responsible  for  enforcing  or  supervising  Union  law  intended  to  protect  fundamental rights to fulfil their mandate under specific conditions and to foster cooperation with market surveillance authorities responsible for the supervision and enforcement of that Regulation. It is necessary to clarify the scope of such cooperation, as well as to clarify which  public  authorities  or  bodies  benefit  from  it.  With  a  view  to  reinforcing  the cooperation, it should be clarified that requests to access information and documentation should be made to the competent market surveillance authority, which should  respond  to  such  requests,  and  that  the  involved  authorities  or  bodies  should have a mutual obligation to cooperate.\n- (20) To allow sufficient time for providers of generative AI systems subject to the marking obligations  laid  down  in  Article  50(2)  of  Regulation  (EU)  2024/1689  to  adapt  their practices within a reasonable time without disrupting the market, it is appropriate to introduce a transitional period of 6 months for providers who have already placed their systems on the market before the 2 August 2026.\n- (21) To  provide  sufficient  time  for  providers  of  high-risk  AI  systems  and  to  clarify applicable  rules  to  the  AI  systems  already  placed  on  the  market  or  put  into  service\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\nbefore  the  entry  into  application  of  relevant  provisions  of  the  Regulation  (EU) 2024/1689,  it  is  appropriate  to  clarify  the  application  of  a  grace  period  provided  in Article 111(2) of that Regulation. The grace period, for the purpose of Article 111(2), should apply to a type and model of AI systems already placed in the market. This means that if at least one individual unit of the high-risk AI system has been lawfully placed on the market or put into service before the date specified in Article 111(2), other individual units of the same type and model of high-risk AI system are subject to the grace period provided in Article 111(2) and thus may continue to be placed on the market, made available or put into service on the Union market without any additional obligations, requirements or the need for additional certification, as long as the design of that high-risk AI system remains unchanged. For the purposes of application of the grace period provided in Article 111(2), the decisive factor is the date on which the first unit of that type and model of high-risk AI system was placed on the market or put into service on the Union market for the first time. Any significant change to the design of that AI system after the date specified in Article 111(2) should trigger the obligation  of  the  provider  to  comply  fully  with  all  relevant  provisions  of  this Regulation  applicable  to  high-risk  AI  systems,  including  the  conformity  assessment requirements.\n\n- (22) Article 113 of Regulation (EU) 2024/1689 establishes the dates of entry into force and application of that Regulation, notably that the general date of application is 2 August 2026. For the obligations related to high-risk AI systems laid down in Sections 1, 2 and  3  of  Chapter  III  of  Regulation  (EU)  2024/1689,  the  delayed  availability  of standards, common specifications, and alternative guidance and the delayed establishment of national competent authorities lead to challenges that jeopardise those obligation's  effective  entry  into  application  and  that  risk  to  significantly  increase implementation costs in  a  way  that  does  not  justify  maintaining  their  initial  date  of application, namely 2 August 2026. Building on experience, it is appropriate to put in place a mechanism that links the entry into application to the availability of measures in support of compliance with Chapter III, which may include harmonised standards, common specifications, and Commission guidelines. This should be confirmed by the Commission  by  decision,  following  which  the  rules  obligations  for  high-risk  AI systems  should  apply  after  6  months  as  regards  AI  systems  classified  as  high-risk pursuant  to  Article  6(2)  and  Annex  III  and  after  12  months  as  regards  AI  systems classified  as  high-risk  pursuant  to  Article  6(1)  and  Annex  I  to  Regulation  (EU) 2024/1689. However, this flexibility should only be extended until 2 December 2027 as  regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III and  until  2  August  2028  as  regards  AI  systems  classified  as  high-risk  pursuant  to Article 6(1) and Annex I to that Regulation, by which dates those rules should enter into application in any case. The distinction between the entry into application of the rules as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III  and  Article  6(1)  and  Annex I to that Regulation is consistent with the difference between the initial dates of application envisaged in Regulation (EU) 2024/1689 and aims  to  provide  the  necessary  time  for  adaptation  and  implementation  of  the corresponding obligations.\n- (23) In light of the objective to reduce implementation challenges for citizens, businesses and public administrations, it is essential that harmonised conditions for the implementation  of  certain  rules  are  adopted  only  where  strictly  necessary.  For  that purpose, it is appropriate to remove  certain empowerments  bestowed  on  the Commission to adopt such harmonised conditions by means of implementing acts in cases where those conditions are not met. Regulation (EU) 2024/1689 should therefore\n\n<!-- image -->\n\n<!-- image -->\n\nbe  amended  to  remove  the  empowerments  conferred  on  the  Commission  in  Article 50(7),  Article  56(6),  and  Article  72(3)  thereof  to  adopt  implementing  acts.  The removal  of  the  empowerment  to  adopt  a  harmonised  template  for  a  post-market monitoring plan in Article 72(3) of Regulation (EU) 2024/1689 has as an additional benefit that it will offer more flexibility for providers of high-risk AI systems to put in place a system for post-market monitoring that is tailored to their organisation. At the same time, recognising the need to offer clarity how providers of high-risk AI systems are required to comply, the Commission should be required to publish guidance.\n\n- (24) Conformity  assessment  of  high-risk  AI  systems  under  Regulation  (EU)  2024/1689 may require involvement of conformity assessment bodies. Only conformity assessment  bodies  that  have  been  designated  under  that  Regulation  may  carry  out conformity assessments and only for the activities related to the categories and types of AI systems concerned. To enable the specification of the scope of the designation of conformity assessment bodies notified under Article 30 of Regulation (EU) 2024/1689, it is necessary to draw up a list of codes, categories, and corresponding types of AI systems. The list of codes should take into account whether the AI system is  a  component of a product or itself a product covered by the Union harmonisation legislation  listed  in  Annex  I  (referred  to  as  'AIP  codes',  for  AI  systems  covered  by product legislation) or a system referred in Annex III of Regulation (EU) 2024/1689, which currently concerns only biometric AI systems referred to in point (1) of Annex III  (referred to as 'AIB codes', for biometric AI systems). Both AIP codes and AIB codes are vertical codes. The AIP codes are reference codes to provide a link to the Union harmonisation legislation  listed  in  Section  A  of  Annex  I  of  Regulation  (EU) 2024/1689. The AIB codes are new codes specific to Regulation (EU) 2024/1689 to identify biometric AI systems referred in paragraph 1 of Annex III of that Regulation. The  list  of  codes  should  also  take  into  account  specific  types  and  underlying technologies  of  AI  systems  (referred  to  as  'AIH  codes',  for  horizontal  AI  system codes). The AIH codes are new AI technology-specific codes and can be applied in conjunction  with  AIP  or  AIB  vertical  codes.  The  AIH  codes  cover  AI  systems' underlying types and technologies. The list of codes, including three categories, should provide for a multi-dimensional typology of AI systems which ensures that conformity assessment bodies designated as notified bodies are fully competent for the AI systems they are required to assess.\n- (25) Regulation (EU) 2018/1139 of the European Parliament and the Council 13 lays down common rules in the field of civil aviation. Article 108 of Regulation (EU) 2024/1689 sets  out  amendments  to  Regulation  (EU)  2018/1139  to  ensure  that  the  Commission takes into account, on the basis of the technical and regulatory specificities of the civil aviation sector, and without interfering with existing governance, conformity\n\n13 Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules  in  the  field  of  civil  aviation  and  establishing  a  European  Union  Aviation  Safety  Agency,  and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and  Directives  2014/30/EU  and  2014/53/EU  of  the  European  Parliament  and  of  the  Council,  and repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the Council and Council Regulation (EEC) No  3922/91(OJ L 212, 22.8.2018, pp. 1-122, ELI: http://data.europa.eu/eli/reg/2018/1139/oj).\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\nassessment  and  enforcement  mechanisms  and  authorities  established  therein,  the mandatory  requirements  for  high-risk  AI  systems  laid  down  in  Regulation  (EU) 2024/1689 when adopting any relevant delegated or implementing acts on the basis of that  act.  A  technical  correction  extending  specific  articles  of  Regulation  (EU) 2018/1139 is necessary to ensure that those mandatory requirements for high-risk AI systems  laid  down  in  Regulation  (EU)  2024/1689  are  fully  covered  when  adopting relevant delegated or implementing acts on the basis of Regulation (EU) 2018/1139.\n\n- (26) In  order  to  ensure  legal  certainty  as  soon  as  possible,  with  a  view  to  the  imminent general application of Regulation (EU) 2024/1689, this Regulation should enter into force as a matter of urgency,\n\n## HAVE ADOPTED THIS REGULATION:\n\n## Article 1\n\n## Amendments to Regulation (EU) 2024/1689\n\nRegulation (EU) 2024/1689 is amended as follows:\n\n- (1) in Article 1(2), point (g) is replaced by the following:\n2. '(g) measures to support innovation, with a particular focus on small mid-cap enterprises  (SMCs)  and  small  and  medium-sized  enterprises  (SMEs), including start-ups.';\n- (2) in Article 2, paragraph 2 is replaced by the following:\n4. '2.  For  AI  systems  classified  as  high-risk  AI  systems  in  accordance  with Article  6(1)  related  to  products  covered  by  the  Union  harmonisation legislation listed in Section B of Annex I, only Article 6(1), Article 60a, Articles 102 to 109 and Articles 111 and 112 shall apply. Article 57 shall apply only in so far as the requirements for high-risk AI systems under this Regulation have  been integrated in that  Union  harmonisation legislation.;\n- (3) in Article 3, the following points (14a) and (14b) are inserted:\n6. '(14a) micro, small and medium-sized enterprise ('SME') means a micro, small or  medium-sized  enterprise  as  defined  in  Article  2  of  the  Annex  to Commission Recommendation 2003/361/EC;\n7. (14b) small mid-cap enterprise ('SMC') means a small mid-cap enterprise as defined in point (2) of the Annex to Commission Recommendation (EU) 2025/1099';\n- (4) Article 4 is replaced by the following:\n\n'\n\nArticle 4\n\n## AI literacy\n\n'The Commission and Member States shall encourage providers and deployers of AI systems to take measures to ensure a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems  on  their  behalf,  taking  into  account  their  technical  knowledge, experience,  level  of  education  and  training  and  the  context  the  AI\n\n<!-- image -->\n\n<!-- image -->\n\nsystems  are  to  be  used  in,  and  considering  the  persons  or  groups  of persons on whom the AI systems are to be used.';\n\n- (5) the following Article 4a is inserted in Chapter I:\n\n## ' Article 4a\n\n## Processing of special categories of personal data for bias detection and mitigation\n\n1. To the extent necessary to ensure bias detection and correction in relation to high-risk AI systems in accordance with Article 10 (2), points (f) and (g), of this Regulation, providers of such systems may exceptionally process special categories of personal data, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons. In addition to the safeguards  set  out  in  Regulations  (EU)  2016/679  and  (EU)  2018/1725 and Directive (EU) 2016/680, as applicable, all the following conditions shall be met in order for such processing to occur:\n2. (a) the bias detection and  correction  cannot be effectively fulfilled by processing other data, including synthetic or anonymised data;\n3. (b) the special categories of personal data are subject to technical limitations on the re-use of the personal data, and state-of-the-art security and privacypreserving measures, including pseudonymisation;\n4. (c) the special categories of personal data are subject to measures to ensure that the  personal  data  processed  are  secured,  protected,  subject  to  suitable safeguards, including strict controls and documentation of the access, to avoid  misuse  and  ensure  that  only  authorised  persons  have  access  to those personal data with appropriate confidentiality obligations;\n5. (d)  the  special  categories  of  personal  data  are  not  transmitted,  transferred  or otherwise accessed by other parties;\n6. (e)  the  special  categories  of  personal  data  are  deleted  once the  bias  has  been corrected or the personal data has reached the end of its retention period, whichever comes first;\n7. (f) the records of processing activities pursuant to Regulations (EU) 2016/679 and  (EU)  2018/1725  and  Directive  (EU)  2016/680  include  the  reasons why the processing of special categories of personal data was necessary to detect and correct biases, and why that objective could not be achieved by processing other data.\n2. Paragraph 1 may apply to providers and deployers of other AI systems and models  and  deployers  of  high-risk  AI  systems  where  necessary  and proportionate if the processing occurs for the purposes set out therein and provided that the conditions set out under the safeguards set out in this paragraph.;\n- (6) in Article 6(4), paragraph 4 is replaced by the following:\n10. '4. A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessment before that system is placed on the  market  or  put  into  service.  Upon  request  of  national  competent authorities, the provider shall provide the documentation of the assessment.';\n\n<!-- image -->\n\n<!-- image -->\n\n- (7) Article 10 is amended as follows:\n2. (a) paragraph 1 is replaced by the following:\n3. '1. High-risk AI systems which make use of techniques involving the training of  AI  models  with  data  shall  be  developed  on  the  basis  of  training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2, 3 and 4 of this Article and in Article 4a(1) whenever such data sets are used.';\n4. (b) paragraph 5 is deleted;\n5. (c) paragraph 6 is replaced by the following:\n6. '6. For the development of high-risk AI systems not using techniques involving the  training  of  AI  models,  paragraphs  2,  3  and  4  of  this  Article  and Article 4a(1) shall apply only to the testing data sets.';\n- (8) in Article 11(1), the second subparagraph is replaced by the following:\n8. 'That  technical  documentation  shall  be  drawn  up  in  such  a  way  as  to demonstrate that the high-risk AI system complies with the requirements set out in this Section and to provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive  form  to  assess  the  compliance  of  the  AI  system  with those requirements. It shall contain, at a minimum, the elements set out in Annex  IV.  SMCs  and  SMEs,  including  start-ups,  may  provide  the elements  of  the  technical  documentation  specified  in  Annex  IV  in  a simplified  manner.  To  that  end,  the  Commission  shall  establish  a simplified technical documentation form targeted at the needs of SMCs and SMEs, including start-ups. Where an SMC or SME, including a startup, opts to provide the information required in Annex IV in a simplified manner,  it  shall  use  the  form  referred  to  in  this  paragraph.  Notified bodies shall accept the form  for the purposes of the conformity assessment.';\n- (9) in Article 17, paragraph 2 is replaced by the following:\n10. '2.  The  implementation  of  the  aspects  referred  to  in  paragraph  1  shall  be proportionate to the size of the provider's organisation, in particular, if the provider is an SMC or an SME, including a start-up. Providers shall, in  any  event,  respect  the  degree  of  rigour  and  the  level  of  protection required to ensure the compliance of their high-risk AI systems with this Regulation.';\n- (10) in Article 28, the following paragraph 8 is added:\n12. '8.  Notifying  authorities  designated  under  this  Regulation  responsible  for  AI systems covered by the Union harmonisation legislation listed in Section A of Annex I shall be established, organised and operated in such a way that  ensures that the  conformity  assessment  body  that  applies  for designation  both  under  this  Regulation  and  the  Union  harmonisation legislation  listed  in  Section  A  of  Annex  I  shall  be  provided  with  the possibility to submit a single application and undergo a single assessment procedure to be designated under this Regulation and Union harmonisation  legislation  listed  in  Section  A  of  Annex  I,  where  the\n\n<!-- image -->\n\n<!-- image -->\n\n- relevant Union harmonisation legislation provides for such single application and single assessment procedure.\n- The  single  application  and  single  assessment  procedure  referred  to  in  this paragraph  shall also be made  available  to  notified bodies  already designated under the Union harmonisation legislation listed in Section A of Annex I, when those notified bodies apply for designation under this Regulation,  provided  that  the  relevant  Union  harmonisation  legislation provides for such a procedure.\n- The  single  application  and  single  assessment  procedure  shall  avoid  any unnecessary duplications, build on the existing procedures for designation under the Union harmonisation legislation listed in Section A of Annex I and ensure compliance with the requirements both relating to notified bodies under this Regulation and the relevant Union harmonisation legislation.';\n- (11) in Article 29, paragraph 4 is replaced by the following:\n- '4. For notified bodies which are designated under any other Union harmonisation legislation, all documents and certificates linked to those designations  may  be  used  to  support  and  expedite  their  designation procedure under this Regulation, as appropriate.\n- Notified bodies, which are designated under any of the Union harmonisation legislation listed in Section A of Annex I and which apply for the single assessment referred to in Article 28(8), shall submit the single application for  assessment to the notifying authority designated in accordance with that Union harmonisation legislation.\n- The notified body shall update the documentation referred to in paragraphsâ€¯2 and 3 of this Article whenever relevant changes occur, in order to enable the  authority  responsible  for  notified  bodies  to  monitor  and  verify continuous compliance with all the requirements laid down in Articleâ€¯31. ';\n- (12) in Article 30, paragraph 2 is replaced by the following:\n- '2.  Notifying  authorities  shall  notify  the  Commission  and  the  other  Member States, based on the list of codes, categories, and corresponding types of AI systems referred to in Annex  XIV,  and using the electronic notification  tool  developed  and  managed  by  the  Commission,  of  each conformity assessment body referred to in paragraph 1.\n- The  Commission  is  empowered  to  adopt  delegated  acts  in  accordance  with Article  97  to  amend  Annex  XIV,  in  the  light  of  technical  progress, advances in knowledge or new scientific evidence by adding to the list of codes, categories, and corresponding types of AI systems a new code, a category or a type of AI system, withdrawing an existing code, category or  a  type  of  AI  system  from  that  list  or  moving  a  code  or  type  of  AI system from one category to another.';\n- (13) in Article 43, paragraph 3 is replaced by the following:\n- 'For  high-risk  AI  systems  covered  by  the  Union  harmonisation  legislation listed in Section A of Annex I, the provider of the system shall follow the relevant conformity assessment procedure as required under the relevant\n\n<!-- image -->\n\n<!-- image -->\n\nUnion harmonisation legislation. The requirements set out in Section 2 of this Chapter shall apply to those high-risk AI systems and shall be part of that assessment. Assessment of the quality management system set out in Article 17 and Annex VII shall also apply.\n\n- For  the  purposes  of  that  conformity  assessment,  notified  bodies  which  have been notified under the Union harmonisation legislation listed in Section A of Annex I shall have the power to assess the conformity of high-risk AI systems with the requirements set out in Section 2, provided that the compliance of those notified bodies with the requirements laid down in Article 31(4), (5), (10) and (11) has been assessed in the context of the notification procedure under the relevant Union harmonisation legislation. Without prejudice to Article 28, such notified bodies which have been notified under the Union harmonisation legislation in Section A of Annex I, shall apply for designation in accordance with Section 4 at the latest [18 months from the entry into application of this Regulation].\n- Where Union harmonisation legislation listed in Section A of Annex I provides the  product  manufacturer  with  an  option  to  opt  out  from  a  third-party conformity  assessment,  provided  that  that  manufacturer  has  applied harmonised standards covering all the relevant requirements, that manufacturer may use that option only if it has also applied harmonised standards  or,  where  applicable,  common  specifications  referred  to  in Article 41, covering all requirements set out in Section 2 of this Chapter.\n- Where  a  high-risk  AI  system  is  both  covered  by  the  Union  harmonisation legislation listed in Section A of Annex I and it falls within one of the categories listed in Annex III, the provider of the system shall follow the relevant conformity assessment procedure as required under the relevant Union harmonisation legislation listed in Section A of Annex I.';\n- (14) in Article 49, paragraph 2 is deleted;\n- (15) in Article 50, paragraph 7 is replaced by the following:\n- '7.  The  AI  Office  shall  encourage  and  facilitate  the  drawing  up  of  codes  of practice  at  Union  level  to  facilitate  the  effective  implementation  of  the obligations regarding the detection, marking and labelling of artificially generated or manipulated content. The Commission may assess whether adherence  to  those  codes  of  practice  is  adequate  to  ensure  compliance with  the  obligation  laid  down  in  paragraph  2,  in  accordance  with  the procedure laid down in Article 56(6), first subparagraph. If it deems the code  is  not  adequate,  the  Commission  may  adopt  an  implementing  act specifying common rules for the implementation of those obligations in accordance with the examination procedure laid down in Article 98(2).';\n- (16) in Article 56(6), the first subparagraph is replaced by the following:\n- '6.  The  Commission  and  the  Board  shall  regularly  monitor  and  evaluate  the achievement of the objectives of the codes of practice by the participants and their  contribution  to  the  proper  application  of  this  Regulation.  The Commission,  taking  utmost  account  of  the  opinion  of  the  Board,  shall assess whether the codes of practice cover the obligations provided for in Articles 53  and  55, and  shall regularly  monitor  and  evaluate  the\n\n<!-- image -->\n\n<!-- image -->\n\nachievement  of  their  objectives.  The  Commission  shall  publish  its assessment of the adequacy of the codes of practice.';\n\n- (17) Article 57 is amended as follows:\n2. (a) the following paragraph 3a is inserted:\n3. 'The AI Office may also establish an AI regulatory sandbox at Union level for AI  systems  covered  by  Article  75(1).  Such  an  AI  regulatory  sandbox shall  be  implemented  in  close  cooperation  with  relevant  competent authorities, in particular when Union legislation other than this Regulation is supervised in the AI regulatory sandbox, and shall provide priority access to SMEs.';\n4. (b) paragraph 5 is replaced by the following:\n5. '5.  AI regulatory sandboxes established under this Article shall provide for a controlled environment that fosters innovation and facilitates the development,  training,  testing  and  validation  of  innovative  AI  systems for  a  limited  time  before  their  being  placed  on  the  market  or  put  into service pursuant to a specific sandbox plan agreed between the providers or  prospective  providers  and  the  competent  authority,  ensuring  that appropriate safeguards are in place. Such sandboxes may include testing in real world conditions supervised therein. When  applicable, the sandbox  plan  shall  incorporate  in  a  single  document  the  real-world testing plan.';\n6. (c) paragraph 9, point (e) is replaced by the following:\n7. '(e) facilitating and accelerating access to the Union market for AI systems, in particular when provided by SMCs and SMEs, including start-ups.';\n8. (d) paragraph 13 is replaced by the following:\n9. '13. The AI regulatory sandboxes shall be designed and implemented in such a way  that they facilitate cross-border cooperation between  national competent authorities.';\n10. (e) paragraph 14 is replaced by the following:\n11. '14. National competent  authorities shall coordinate their activities and cooperate  within  the  framework  of  the  Board.  They  shall  support  the joint establishment and operation of AI regulatory sandboxes, including in different sectors.';\n- (18) Article 58, paragraph 1, is replaced by the following:\n13. '1.  In  order  to  avoid  fragmentation  across  the  Union,  the  Commission  shall adopt  implementing  acts  specifying  the  detailed  arrangements  for  the establishment, development, implementation, operation, governance, and supervision of the AI regulatory sandboxes. The implementing acts shall include common principles on the following issues:\n14. (a)  eligibility  and  selection  criteria  for  participation  in  the  AI  regulatory sandbox;\n15. (b) procedures for the application, participation, monitoring, exiting from and termination of the AI regulatory sandbox, including the sandbox plan and the exit report;\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\n- (c) the terms and conditions applicable to the participants;\n- (d) the detailed rules applicable to the governance of AI regulatory sandboxes covered under Article 57, including as regards the exercise of the tasks of the competent  authorities and  the coordination and  cooperation  at national and EU level.';\n- (19) Article 60 is amended as follows:\n- (a) in paragraph 1, the first subparagraph is replaced by the following:\n- 'Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes  may  be  conducted  by  providers  or  prospective  providers  of high-risk AI systems listed in Annex III or covered by Union harmonisation legislation listed in Section A of Annex I, in accordance with this Article and the real-world testing plan referred to in this Article, without prejudice to the prohibitions under Article 5.';\n- (b) paragraph 2 is replaced by the following:\n- '2.  Providers  or  prospective  providers  may  conduct  testing  of  high-risk  AI systems  referred  to  in  Annex  III  or  covered  by  Union  harmonisation legislation listed in Section A of Annex I in real world conditions at any time before the placing on the market or the putting into service of the AI system  on  their  own  or  in  partnership  with  one  or  more  deployers  or prospective deployers.';\n- (20) the following Article 60a is inserted:\n\n## ' Article 60a\n\n## Testing of high-risk AI systems covered by Union harmonisation legislation listed in Section B of Annex I in real-world conditions outside AI regulatory sandboxes\n\n1. Testing  of  high-risk  AI  systems  in  real  world  conditions  outside  AI regulatory  sandboxes  may  be  conducted  by  providers  or  prospective providers  of  AI  enabled  products  covered  by  Union  harmonisation legislation listed in Section B of Annex I, in accordance with this Article and  a  voluntary  real-world  testing  agreement,  without  prejudice  to  the prohibitions under Article 5.\n2. The  voluntary  real-world  testing  agreement  referred  to  in  paragraph  1 shall be concluded in writing between interested Member States and the Commission.  It  shall  set  the  requirements  for  the  testing  of  those  AIenabled  products  covered  by  Union  harmonisation  legislation  listed  in Section B of Annex I in real-world conditions.\n3. Member  States,  the  Commission,  market  surveillance  authorities  and public  authorities  responsible  for  the  management  and  operation  of infrastructure  and  products  covered  by  Union  harmonisation  legislation listed in Section B of Annex I shall cooperate closely with each other and in  good  faith,  and  shall  remove  any  practical  obstacles,  including  on procedural rules providing access to physical public infrastructure, where this  is  necessary,  to  successfully  implement  the  voluntary  real-world testing  agreement  and  test  AI-enabled  products  covered  by  Union harmonisation legislation listed in Section B of Annex.\n\n<!-- image -->\n\n<!-- image -->\n\n4. The  signatories  of  the  voluntary  real-world  testing  agreement,  shall specify  conditions  of  the  testing  in  real  world  conditions  and  establish detailed elements of the real-world testing plan for AI systems covered by Union harmonisation legislation listed in Section B of Annex I.\n5. Article 60(2), (5) and (9) shall apply.';\n- (21) Article 63(1) is replaced by the following:\n4. '1. SMEs, including start-ups, may comply with certain elements of the quality management system required by Article 17 in a simplified manner. For that purpose, the Commission shall develop guidelines on the elements of the quality management  system  which  may  be  complied  with  in a simplified manner considering the needs of SMEs, without affecting the level of protection or the need for compliance with the requirements in respect of high-risk AI systems.';\n- (22) Article 69 is amended as follows:\n6. (a) paragraph 2 is replaced by the following:\n7. '2. The Member States may be required to pay fees for the advice and support provided  by  the  experts  at  a  rate  equivalent  to  the  remuneration  fees applicable to the Commission pursuant to the implementing act referred to in Article 68(1).';\n8. (b) paragraph 3 is deleted.\n- (23) in Article 70, paragraph 8 is replaced by the following:\n10. '8.  National  competent  authorities  may  provide  guidance  and  advice  on  the implementation  of  this  Regulation,  in  particular  to  SMCs  and  SMEs, including  start-ups,  taking  into  account  the  guidance  and  advice  of  the Board and the Commission, as appropriate. Whenever national competent authorities intend to provide guidance and advice with regard to  an  AI  system  in  areas  covered  by  other  Union  law,  the  national competent  authorities  under  that  Union  law  shall  be  consulted,  as appropriate.';\n- (24) in Article 72, paragraph 3 is replaced by the following:\n12. '3.  The  post-market  monitoring  system  shall  be  based  on  a  post-market monitoring  plan.  The  post-market  monitoring  plan  shall  be  part  of  the technical documentation referred to in Annex IV. The Commission shall adopt guidance on the post-market monitoring plan.';\n- (25) Article 75 is amended as follows:\n14. (a) the heading of Article 75 is replaced by the following:\n\n## ' Market surveillance and control of AI systems and mutual assistance ';\n\n- (b) paragraph 1 is replaced by the following:\n- '1.  Where  an  AI  system  is  based  on  a  general-purpose  AI  model,  with  the exclusion  of  AI  systems  related  to  products  covered  by  the  Union harmonisation  legislation  listed  in  Annex  I,  and  that  model  and  that system  are  developed  by  the  same  provider,  the  AI  Office  shall  be exclusively competent for the supervision and enforcement of that system with the obligations of this Regulation in accordance with the tasks and\n\n<!-- image -->\n\n<!-- image -->\n\nresponsibilities assigned by it to market surveillance authorities. The AI Office  shall  also  be  exclusively  competent  for  the  supervision  and enforcement  of  the  obligations  under  this  Regulation  in  relation  to  AI system that constitute or that are integrated into a designated very large online platform or very large online search engine within the meaning of Regulation (EU) 2022/2065.\n\n- When  exercising  its  tasks  of  supervision  and  enforcement  under  the  first subparagraph,  the  AI  Office  shall  have  all  the  powers  of  a  market surveillance authority provided for in this Section and in Regulation (EU) 2019/1020.  The  AI  Office  shall  be  empowered  to  take  appropriate measures  and  decisions  to  adequately  exercise  its  supervisory  and enforcement  powers.  Article  14  of  Regulation  (EU)  2019/1020  shall apply mutatis mutandis.\n- The authorities involved in the application of this Regulation shall cooperate actively in the exercise of these powers, in particular where enforcement actions need to be taken in the territory of a Member State.';\n- (c) the following paragraphs 1a to 1c are inserted:\n- '1a. The Commission shall adopt an implementing act to define the enforcement powers and the procedures for the exercise of those powers of the AI Office, including its ability to impose penalties, such as fines or other  administrative  sanctions,  in  accordance  with  the  conditions  and ceilings identified in Article 99, in relation to AI systems referenced to in paragraphs 1 and 1a of this Article that  are  found  to  be  non-compliant with  this  Regulation,  in  the  context  of  its  monitoring  and  supervision tasks under this Article.'\n- '1b. Article 18 of Regulation (EU) 2019/1020 shall apply mutatis mutandis to providers of AI systems referred to in paragraph 1, without prejudice to more specific procedural rights provided for in this Regulation.'\n- '1c.  The  Commission  shall  organise  and  carry  out  pre-market  conformity assessments and tests of AI systems referred to in paragraph 1 that are classified  as  high-risk  and  subject  to  third-party  conformity  assessment under Article 43 before such AI systems are placed on the market or put into  service.  These  tests  and  assessments  shall  verify  that  the  systems comply  with  the  relevant  requirements  of  this  Regulation  and  may  be placed on the market or put into service in the Union in accordance with this Regulation. The Commission may entrust the performance of these tests or assessments to notified bodies designated under this Regulation, in which case the notified body shall act on behalf of the Commission. Article  34(1)  and  (2)  shall  apply mutatis  mutandis to  the  Commission when exercising its powers under this paragraph.\n- The fees for testing and assessment activities shall be levied on the provider of a  high-risk  AI  system  who  has  applied  for  third-party  conformity assessment to the Commission. The costs related to the services entrusted by the Commission to the notified bodies in accordance with this Article shall be directly paid by the provider to the notified body.';\n- (26) Article 77 is amended as follows:\n- (a) the heading is replaced by the following:\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\n## ' Powers of authorities protecting fundamental rights and cooperation with market surveillance authorities '\n\n- (b) paragraph 1 is replaced by the following:\n- '1. National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights, including the right to non-discrimination, shall have the power to make a request and access any information or documentation created or maintained from the  relevant  market  surveillance  authority  under  this  Regulation  in accessible  language  and  format  where  access  to  that  information  or documentation  is  necessary  for  effectively  fulfilling  their  mandates within the limits of their jurisdiction.';\n- (c) the following paragraph 1a and 1b are inserted:\n- '1a. Subject to the conditions specified in this Article, the market surveillance authority shall grant the relevant public authority or body referred to in paragraph 1 access to such information or documentation, including by requesting such information or documentation from the provider or the deployer, where necessary.'\n- '1b. Market surveillance authorities and public authorities or bodies referred to in  paragraph  1  shall  cooperate  closely  and  provide  each  other  with mutual assistance necessary for fulfilling their respective mandates, with a view to ensuring coherent application of this Regulation and Union law protecting  fundamental  rights  and  streamlining  procedures.  This  shall include,  in  particular,  exchange  of  information  where  necessary  for  the effective supervision or enforcement of this Regulation and the respective other Union legislation.';\n- (27) Article 95, paragraph 4 is replaced by the following:\n- '4. The AI Office and the Member States shall take into account the specific interests  and  needs  of  SMCs  and  SMEs,  including  start-ups,  when encouraging and facilitating the drawing up of codes of conduct.';\n- (28) in Article 96(1), the second subparagraph is replaced by the following:\n- 'When issuing such guidelines, the Commission shall pay particular attention to  the  needs  of  SMCs  and  SMEs  including  start-ups,  of  local  public authorities  and of the sectors most  likely to be affected by this Regulation.';\n- (29) Article 99 is amended as follows:\n- (a) paragraph 1 is replaced by the following:\n- '1. In accordance with the terms and conditions laid down in this Regulation, Member  States shall lay down  the rules on penalties and other enforcement  measures,  which  may  also  include  warnings  and  nonmonetary  measures,  applicable  to  infringements  of  this  Regulation  by operators, and shall take all measures necessary to ensure that they are properly  and  effectively  implemented,  thereby  taking  into  account  the guidelines  issued  by  the  Commission  pursuant  to  Article  96.  The penalties  provided  for  shall  be  effective,  proportionate  and  dissuasive. The  Member  States  shall  take  into  account  the  interests  of  SMCs  and\n\n<!-- image -->\n\n<!-- image -->\n\nSMEs, including start-ups, and their economic viability when imposing penalties.';\n\n- (b) paragraph 6 is replaced by the following:\n- '6. In the case of SMCs and SMEs, including start-ups, each fine referred to in this  Article  shall  be  up  to  the  percentages  or  amount  referred  to  in paragraphs 3, 4 and 5, whichever thereof is lower.';\n- (30) Article 111 is amended as follows:\n- (a) paragraph 2 is replaced by the following:\n- '2.  Without  prejudice  to  the  application  of  Article  5  as  referred  to  in  Article 113(3), third paragraph, point (a), this Regulation shall apply to operators of high-risk AI systems, other than the systems referred to in paragraph 1 of  this  Article,  that  have  been  placed  on  the  market  or  put  into  service before the date of application of Chapter III and corresponding obligations  referred  to  in  Article  113,  only  if,  as  from  that  date,  those systems are subject to significant changes in their designs. In any case, the providers and deployers of high-risk AI systems intended to be used by  public  authorities  shall  take  the  necessary  steps  to  comply  with  the requirements and obligations laid down in this Regulation by 2 August 2030.';\n- (b) the following paragraph 4 is added:\n- '4. Providers of AI systems, including general-purpose AI systems, generating synthetic audio, image, video or text content, that have been placed on the market before 2 August 2026 shall take the necessary steps in order to comply with Article 50(2) by 2 February 2027.';\n- (31) Article 113 is amended as follows:\n- (a) in the third paragraph, point (d) is added:\n- '(d) Chapter III, Sections 1, 2, and 3, shall apply following the adoption of a decision  of  the  Commission  confirming  that  adequate  measures  in support of compliance with Chapter III are available, from the following dates:\n- (i) 6 months after the adoption of that decision as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III, and\n- (ii)  12  months  after  the  adoption  of  the  decision  as  regards  AI  systems classified as high-risk pursuant to Article 6(1) and Annex I.\n- In  the  absence  of  the  adoption  of  the  decision  within  the  meaning  of subparagraph  1,  or  where  the  dates  below  are  earlier  than  those  that follow  the  adoption  of  that  decision,  Chapter  III,  Sections  1,  2,  and  3, shall apply:\n- (i) on 2 December 2027 as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III, and\n- (ii) on 2 August 2028 as regards AI systems classified as high-risk pursuant to Article 6(1) and Annex I.';\n- (b) in the third paragraph, point (e) is added:\n\n<!-- image -->\n\n<!-- image -->\n\n- ' 3. Articles 102 to 110 shall apply from [the date of entry into application of this Regulation].';\n- (32) in Annex VIII, section B is deleted;\n- (33) the following Annex XIV is added:\n\n## 'Annex XIV\n\n## The list of codes, categories and corresponding types of AI systems for the purpose of the notification procedure referred to in Article 30 specifying the scope of the designation as notified bodies\n\n## 1. Introduction\n\nConformity  assessment  of  high-risk  AI  systems  under  this  Regulation  may  require involvement  of  conformity  assessment  bodies.  Only  conformity  assessment  bodies  that have  been  designated  in  accordance  with  this  Regulation  may  carry  out  conformity assessments and only for the activities related to the types of AI systems concerned. The list  of  codes,  categories,  and  corresponding  types  of  AI  systems  sets  the  scope  of  the designation of conformity assessment bodies notified under Article 30 of this Regulation.\n\n2. List of Codes, categories, and corresponding AI systems\n\n## 1. AI systems subject to Annex I of the AI Act\n\n| AIA Code                                           |          |\n|----------------------------------------------------|----------|\n| AI systems subject to Annex I.A.1. of the AI Act.  | AIP 0101 |\n| AI systems subject to Annex I.A.2. of the AI Act.  | AIP 0102 |\n| AI systems subject to Annex I.A.3. of the AI Act.  | AIP 0103 |\n| AI systems subject to Annex I.A.4. of the AI Act.  | AIP 0104 |\n| AI systems subject to Annex I.A.5. of the AI Act.  | AIP 0105 |\n| AI systems subject to Annex I.A.6. of the AI Act.  | AIP 0106 |\n| AI systems subject to Annex I.A.7. of the AI Act.  | AIP 0107 |\n| AI systems subject to Annex I.A.8. of the AI Act.  | AIP 0108 |\n| AI systems subject to Annex I.A.9. of the AI Act.  | AIP 0109 |\n| AI systems subject to Annex I.A.10. of the AI Act. | AIP 0110 |\n| AI systems subject to Annex I.A.11. of the AI Act. | AIP 0111 |\n| AI systems subject to Annex I.A.12. of the AI Act. | AIP 0112 |\n\n## 2. AI systems subject to Annex III.1 of the AI Act\n\n<!-- image -->\n\n<!-- image -->\n\n| AIA Code   |                                                                                                                                                                   |\n|------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AIB 0201   | Remote biometric identification systems under Annex III.1.a. of the AI Act intended to be put into service by Union institutions, bodies, offices or agencies.    |\n| AIB 0202   | Biometric categorisation AI systems under Annex III.1.b. of the AI Act intended to be put into service by Union institutions, bodies, offices or agencies.        |\n| AIB 0203   | Emotion recognition AI systems under Annex III.1.c. of the AI Act intended to be put into service by Union institutions, bodies, offices or agencies.             |\n| AIB 0204   | Remote biometric identification systems under Annex III.1.a. of the AI Act intended to be put into service by law enforcement, immigration or asylum authorities. |\n| AIB 0205   | Biometric categorisation AI systems under Annex III.1.b. of the AI Act intended to be put into service by law enforcement, immigration or asylum authorities.     |\n| AIB 0206   | Emotion recognition AI systems under Annex III.1.c. of the AI Act intended to be put into service by law enforcement, immigration or asylum authorities.          |\n| AIB 0207   | Remote biometric identification systems under Annex III.1.a. of the AI Act (general).                                                                             |\n| AIB 0208   | Biometric categorisation AI systems under Annex III.1.b. of the AI Act (general).                                                                                 |\n| AIB 0209   | Emotion recognition AI systems under Annex III.1.c. of the AI Act (general).                                                                                      |\n\n## 3. AI technology-specific codes\n\n## a) Symbolic AI, expert systems and mathematical optimization\n\n| AIA Code   |                                                                                                                    |\n|------------|--------------------------------------------------------------------------------------------------------------------|\n| AIH 0101   | Logic- and knowledge-based AI systems that infer from encoded knowledge or symbolic representation, expert systems |\n| AIH 0102   | Logic-based AI systems, excluding basic data processing                                                            |\n\n## b) Machine learning, excluding GPAI and single modality generative AI\n\n| AIA Code                                      |          |\n|-----------------------------------------------|----------|\n| AI systems that process structured data       | AIH 0201 |\n| AI systems that process signal and audio data | AIH 0202 |\n| AI systems that process text data             | AIH 0203 |\n\n<!-- image -->\n\n<!-- image -->\n\n| AIH 0204   | AI systems that process image and video                            |\n|------------|--------------------------------------------------------------------|\n| AIH 0205   | AI systems that learn from their environment, excluding agentic AI |\n\n## c) AI systems based on GPAI or single modality generative AI\n\n| AIA Code   |                                                                             |\n|------------|-----------------------------------------------------------------------------|\n| AIH 0301   | Single modality generative AI systems                                       |\n| AIH 0302   | Multimodal generative AI systems, including AI systems based on GPAI models |\n\n## d) Agentic AI\n\n| AIA Code   |            |\n|------------|------------|\n| AIH 0401   | Agentic AI |\n\n## 3. Application for designation\n\nConformity assessment bodies shall use the lists of codes, categories and corresponding types of AI systems set out in this Annex when specifying the types of AI  systems  in  the  application  for  designation  referred  to  in  Article  29  of  this Regulation.'.\n\n## Article 2\n\n## Amendments to Regulation (EU) 2018/1139\n\nRegulation (EU) 2018/1139 is amended as follows:\n\n- (1) in Article 27, the following paragraph is added:\n2. '3.  Without  prejudice  to  paragraph  2,  when  adopting  implementing  acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council 14 , the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.';\n- (2) in Article 31, the following paragraph is added:\n\n14 Regulation  (EU)  2024/1689  of  the  European  Parliament  and  of  the  Council  of  13  June  2024  laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013,  (EU)  No  168/2013,  (EU)  2018/858,  (EU)  2018/1139  and  (EU)  2019/2144  and  Directives 2014/90/EU,  (EU)  2016/797  and  (EU)  2020/1828  (Artificial  Intelligence  Act)  (OJ  L,  2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).\n\n<!-- image -->\n\n<!-- image -->\n\n- '3.  Without  prejudice  to  paragraph  2,  when  adopting  implementing  acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.';\n- (3) in Article 32, the following paragraph is added:\n- '3. When adopting delegated acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation  (EU)  2024/1689  of  the  European  Parliament  and  of  the Council  (*),  the  requirements  set  out  in  Chapter  III,  Section  2,  of  that Regulation shall be taken into account.';\n- (4) in Article 36, the following paragraph is added:\n- '3.  Without  prejudice  to  paragraph  2,  when  adopting  implementing  acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.';\n- (5) in Article 39 the following paragraph is added:\n- '3. When adopting delegated acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation  (EU)  2024/1689  of  the  European  Parliament  and  of  the Council,  the  requirements  set  out  in  Chapter  III,  Section  2,  of  that Regulation shall be taken into account.';\n- (6) in Article 50, the following paragraph is added:\n- '3.  Without  prejudice  to  paragraph  2,  when  adopting  implementing  acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.';\n- (7) in Article 53, the following paragraph is added:\n- '3.  Without  prejudice  to  paragraph  2,  when  adopting  implementing  acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.'.\n\n## Article 3\n\n## Entry into force and application\n\nThis Regulation shall enter into force on the third day following that of its publication in the Official Journal of the European Union .\n\n<!-- image -->\n\n<!-- image -->\n\nThis Regulation shall be binding in its entirety and directly applicable in all Member States. Done at Brussels,\n\nFor the European Parliament The President\n\nFor the Council The President\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\n| 1.          | LEGISLATIVE FINANCIAL AND DIGITAL STATEMENT FRAMEWORK OF THE PROPOSAL/INITIATIVE................................................. 3                                                                                                                                                                                                                                                                                                                    |\n|-------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1.1.        | Title of the proposal/initiative...................................................................................... 3                                                                                                                                                                                                                                                                                                                               |\n| 1.2.        | Policy area(s) concerned .............................................................................................. 3                                                                                                                                                                                                                                                                                                                              |\n| 1.3.        | Objective(s).................................................................................................................. 3                                                                                                                                                                                                                                                                                                                       |\n| 1.3.1.      | General objective(s) ..................................................................................................... 3                                                                                                                                                                                                                                                                                                                           |\n| 1.3.2.      | Specific objective(s)..................................................................................................... 3                                                                                                                                                                                                                                                                                                                           |\n| 1.3.3.      | Expected result(s) and impact...................................................................................... 3                                                                                                                                                                                                                                                                                                                                  |\n| 1.3.4.      | Indicators of performance ............................................................................................ 3                                                                                                                                                                                                                                                                                                                               |\n| 1.4.        | The proposal/initiative relates to:................................................................................. 4                                                                                                                                                                                                                                                                                                                                 |\n| 1.5.        | Grounds for the proposal/initiative .............................................................................. 4                                                                                                                                                                                                                                                                                                                                   |\n| 1.5.1.      | Requirement(s) to be met in the short or long term including a detailed timeline for roll-out of the implementation of the initiative............................................................ 4                                                                                                                                                                                                                                                    |\n| 1.5.2.      | Added value of EU involvement (it may result from different factors, e.g. coordination gains, legal certainty, greater effectiveness or complementarities). For the purposes of this section 'added value of EU involvement' is the value resulting from EU action, that is additional to the value that would have been otherwise created by Member States alone. ................................................................................. 4 |\n| 1.5.3.      | Lessons learned from similar experiences in the past.................................................. 4                                                                                                                                                                                                                                                                                                                                               |\n| 1.5.4.      | Compatibility with the multiannual financial framework and possible synergies with other appropriate instruments....................................................................................... 5                                                                                                                                                                                                                                              |\n| 1.5.5.      | Assessment of the different available financing options, including scope for redeployment................................................................................................................ 5                                                                                                                                                                                                                                            |\n| 1.6.        | Duration of the proposal/initiative and of its financial impact .................................... 6                                                                                                                                                                                                                                                                                                                                                 |\n|             | MEASURES .................................................................................. 8                                                                                                                                                                                                                                                                                                                                                          |\n| 2.          | MANAGEMENT                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| 2.1.        | Monitoring and reporting rules .................................................................................... 8                                                                                                                                                                                                                                                                                                                                  |\n| 2.2. 2.2.1. | Management and control system(s) ............................................................................. 8 Justification of the budget implementation method(s), the funding implementation mechanism(s), the payment modalities and the control strategy proposed.................. 8                                                                                                                                                           |\n| 2.2.2.      | Information concerning the risks identified and the internal control system(s) set up to mitigate them............................................................................................................ 8                                                                                                                                                                                                                                   |\n| 2.2.3.      | Estimation and justification of the cost-effectiveness of the controls (ratio between                                                                                                                                                                                                                                                                                                                                                                  |\n|             | the control costs and the value of the related funds managed), and assessment of the expected levels of risk of error (at payment &at closure)........................................... 8                                                                                                                                                                                                                                                            |\n| 3. 3.1.     | ESTIMATED FINANCIAL IMPACT OF THE PROPOSAL/INITIATIVE............ 10                                                                                                                                                                                                                                                                                                                                                                                   |\n|             | Heading(s) of the multiannual financial framework and expenditure budget line(s) affected....................................................................................................................... 10                                                                                                                                                                                                                                    |\n\n<!-- image -->\n\n<!-- image -->\n\n| 3.2.     | Estimated financial impact of the proposal on appropriations...................................                                   |   12 |\n|----------|-----------------------------------------------------------------------------------------------------------------------------------|------|\n| 3.2.1.   | Summary of estimated impact on operational appropriations....................................                                     |   12 |\n| 3.2.1.1. | Appropriations from voted budget.............................................................................                     |   12 |\n| 3.2.1.2. | Appropriations from external assigned revenues.......................................................                             |   17 |\n| 3.2.2.   | Estimated output funded from operational appropriations.........................................                                  |   22 |\n| 3.2.3.   | Summary of estimated impact on administrative appropriations...............................                                       |   24 |\n| 3.2.3.1. | Appropriations from voted budget ..............................................................................                   |   24 |\n| 3.2.3.2. | Appropriations from external assigned revenues.......................................................                             |   24 |\n| 3.2.3.3. | Total appropriations ...................................................................................................          |   24 |\n| 3.2.4.   | Estimated requirements of human resources..............................................................                           |   25 |\n| 3.2.4.1. | Financed from voted budget.......................................................................................                 |   25 |\n| 3.2.4.2. | Financed from external assigned revenues ................................................................                         |   26 |\n| 3.2.4.3. | Total requirements of human resources.....................................................................                        |   26 |\n| 3.2.5.   | Overview of estimated impact on digital technology-related investments................                                            |   28 |\n| 3.2.6.   | Compatibility with the current multiannual financial framework..............................                                      |   28 |\n| 3.2.7.   | Third-party contributions ...........................................................................................             |   28 |\n| 3.3.     | Estimated impact on revenue .....................................................................................                 |   29 |\n| 4.       | DIGITAL DIMENSIONS..........................................................................................                      |   29 |\n| 4.1.     | Requirements of digital relevance..............................................................................                   |   30 |\n| 4.2.     | Data ............................................................................................................................ |   30 |\n| 4.3.     | Digital solutions .........................................................................................................       |   31 |\n| 4.4.     | Interoperability assessment........................................................................................               |   31 |\n| 4.5.     | Measures to support digital implementation..............................................................                          |   32 |\n\n<!-- image -->\n\n<!-- image -->\n\n## 1. FRAMEWORK OF THE PROPOSAL/INITIATIVE\n\n## 1.1. Title of the proposal/initiative\n\nProposal for a Regulation of the European Parliament and of the Council amending Regulations (EU) 2024/1689 and (EU) 2018/1139 as regards the simplification of the implementation  of  harmonised  rules  on  artificial  intelligence  (Digital  Omnibus  on AI)\n\n## 1.2. Policy area(s) concerned\n\nCommunications Networks, Content and Technology;\n\nInternal Market, Industry, Entrepreneurship and SMEs\n\nThe budgetary impact concerns the new tasks entrusted with the AI Office.\n\n## 1.3. Objective(s)\n\n## 1.3.1. General objective(s)\n\n1. To strengthen the monitoring and supervision of certain categories of AI systems by the AI Office.\n2.  To  facilitate  the  development  and  testing  at  EU  level  of  innovative  AI  systems under  strict  regulatory  oversight  before  these  systems  are  placed  on  the  market  or otherwise put into service.\n\n## 1.3.2. Specific objective(s)\n\n## Specific objective No 1\n\nTo enhance governance and effective enforcement of the AI Act rules related to AI systems by reinforcing the powers and procedures applicable as well as by providing for new resources for the AI Office in charge of the enforcement.\n\n## Specific objective No 2\n\nTo  provide  for  the  establishment  of  a  sandbox  at  EU  level,  enabling  cross  border activities and testing.\n\n## 1.3.3. Expected result(s) and impact\n\nSpecify the effects which the proposal/initiative should have on the beneficiaries/groups targeted.\n\nAI providers should benefit from a centralised level of governance and the access to an  EU-level  sandbox  for  certain  categories  of  AI  systems,  avoiding  duplication  of procedures and costs.\n\n## 1.3.4. Indicators of performance\n\nSpecify the indicators for monitoring progress and achievements.\n\n## Indicator 1\n\nNumber of  AI  systems  falling  under  the  scope  of  the  monitoring  and  supervision tasks to be carried out by the AI Office.\n\n## Indicator 2\n\nNumber of providers and prospective providers requesting access to the sandbox at EU level.\n\n<!-- image -->\n\n<!-- image -->\n\n## 1.4. The proposal/initiative relates to:\n\nï‚¨ a new action\n\n- ï‚¨ a new action following a pilot project / preparatory action 26\n- ï¸ the extension of an existing action\n- ï‚¨ a merger or redirection of one or more actions towards another/a new action\n\n## 1.5. Grounds for the proposal/initiative\n\n## 1.5.1. Requirement(s) to be met in the short or long term including a detailed timeline for roll-out of the implementation of the initiative\n\nThe additional elements relevant for the enhancement of the governance structure of the AI Office should be in place before the entry into application of the provisions applicable to AI systems.\n\nThe  first  EU  sandbox  is  expected  to  be  operational  in  2028,  although  some  key setting details should be established beforehand.\n\n- 1.5.2. Added value of EU  involvement (it may  result from different factors, e.g. coordination gains, legal certainty, greater effectiveness or complementarities). For the purposes of this section 'added value of EU involvement' is the value resulting from  EU  action,  that  is  additional  to  the  value  that  would  have  been  otherwise created by Member States alone.\n\nThe AI Office will have the power to monitor and supervise the compliance of all AI systems  based  on  general-purpose  AI  (GPAI)  models,  where  the  model  and  the system are developed by the same provider, as well as all AI systems embedded in or constituting  very  large  online  platforms  or  search  engines,  even  if  the  system  and GPAI model provider are different. The tasks that the AI Office would need to carry out for this vast range of AI systems include requesting full access to documentation, training/validation/testing datasets, and, when necessary, the source code of high-risk AI systems, supervising real-world testing, identifying and evaluating risks, dealing with  serious  incidents,  taking  preventive  and  corrective  measures  while  ensuring cooperation with  national  market  surveillance  authorities,  dealing  with  AI  systems classified  as  not  high-risk  by  the  provider,  dealing  with  complaints  of  noncompliance,  and  imposing  penalties.  Moreover,  to  allow  market  access  for  AI systems  in  the  scope  of  this  provision  which  are  also  subject  to  pre-market  thirdparty conformity assessment under the AI Act, the AI Office will be the responsible body to carry out conformity assessments. All these actions require resources and a set  of  enforcement  procedures  to  be  developed  and  implemented,  as  well  as  the appropriate technical support to assess and evaluate systems.\n\nThe AI Office's role in ensuring compliance would also involve ensuring synergies with  the  evaluation  of  the  GPAI  models,  which  would  strengthen  the  overall evaluations of models and systems provided by the same provider. This would enable a  more  comprehensive understanding of the AI systems and their associated risks,\n\n26 As referred to in Article 58(2), point (a) or (b) of the Financial Regulation.\n\n<!-- image -->\n\nallowing  for  more  effective  monitoring  and  enforcement.  The  AI  Office  will  also need  to  consider  the  unique  challenges  posed  by  agentic  AI,  which  can  operate autonomously  and  make  decisions  that  may  have  significant  consequences,  and develop strategies to address these risks in line with Commission policies.\n\nThe enhancement of the AI Office's governance would bring numerous benefits to the  regulation  of  AI  systems  in  the  EU.  One  of  the  primary  advantages  is  the consistency and coherence it would ensure in the application of the AI Act across the EU. By having a single  authority  overseeing  the  implementation  of  the  AI  Act  in relation to certain categories of AI systems, the risk of conflicting interpretations and decisions would  be  significantly reduced, providing clarity and certainty for companies operating in the EU.\n\nFurthermore,  this  would  simplify  the  regulatory  landscape  for  companies,  as  they would only need to deal with one regulator, rather than multiple national authorities. This  would  reduce  the  complexity  and  administrative  burden  associated  with navigating different regulatory frameworks, allowing companies to focus on innovation and growth. The centralised approach would also enable the development within  the  Commission  of  specialised  expertise  in  AI  systems  and  GPAI  models, enabling more effective monitoring and enforcement of the AI Act.\n\nThis approach would avoid diverging national enforcement actions on the AI systems concerned  that  may  lead  to  the  fragmentation  of  the  internal  market  and  decrease legal  certainty  for  operators.  This  would  also  address  the  challenges  faced  by Member States in securing specialised resources to staff their authorities responsible for implementing the AI Act and overseeing AI systems within their territories. By centralizing  market  surveillance  authorities'  powers  within  the  AI  Office,  this scenario  would  enable  the  AI  Office  to  assume  responsibility  for  evaluating  and monitoring complex AI systems provided by the same model provider, as well as AI systems constituting or embedded into platforms, thereby alleviating the burden on national  authorities.  This  would  leverage  the  AI  Office's  existing  expertise  in evaluating  GPAI  models  and  monitoring  their  compliance,  creating  a  unique concentration of specialized knowledge and capabilities. As a result, the AI Office would be well-positioned  to  provide  consistent  and  effective  oversight,  while  also supporting  Member  States  in  their  efforts  to  implement  the  AI  Act  and  ensure  a harmonized  regulatory  environment  across  the  EU.  With  the  AI  Office  handling these  additional  tasks,  national  authorities  could  focus  more  on  their  enforcement actions under the AI Act, allowing for a more efficient allocation of resources and a more effective implementation of the AI Act across the EU.\n\n## 1.5.3. Lessons learned from similar experiences in the past\n\nThe European Commission's experience in enforcing the Digital Services Act (DSA) provides valuable lessons that can be applied to the enforcement of the AI Act. In particular,  the  establishment  of  a  robust  and  transparent  enforcement  framework, which sets out clear procedures for investigating and addressing breaches of the DSA and  the  close  cooperation  with  national  authorities,  to  ensure  that  enforcement actions are coordinated and effective, represent relevant elements in this context.\n\nThe Commission's experience with DSA enforcement has shown that this approach can be effective in promoting compliance and protecting users' rights. For example, the  Commission  has  already  taken  action  against  several  online  platforms  for breaches of the DSA, and has worked with national authorities to develop guidance and best practices for compliance.\n\nBy  building  on  the  lessons  learned  from  DSA  enforcement,  the  Commission  can develop an effective enforcement framework for the AI Act that promotes compliance,  and  supports  the  development  of  a  trustworthy  and  innovative  AI ecosystem in the EU. This will involve enhancing the AI Office enforcement role to duly monitor and supervision certain categories of AI systems, and working closely with  national  authorities  to  ensure  that  the  AI  Act  is  enforced  in  a  consistent  and effective manner.\n\nThe possibility to provide for an EU-level sandbox should be seen as complementing the sandboxes established at national level and should be implemented in a way to facilitate cross-border cooperation between national competent authorities.\n\n## 1.5.4. Compatibility with the multiannual financial framework and possible synergies with other appropriate instruments\n\nThe  amendments  proposed  to  the  AI  Act  within  this  initiative  would  result  in  a significant  increase  in  the  number  of  AI  systems  subject  to  the  monitoring  and supervision  of  the  AI  Office,  with  a  corresponding  rise  in  the  number  of  systems potentially eligible to participate in an EU-level sandbox. To effectively manage this expansion,  it  is  essential  to  strengthen  the  European  regulatory  and  coordination function,  as  proposed  in  this  initiative.  This  reinforcement  would  enable  the  AI Office to efficiently oversee the growing number of AI systems, ensure compliance with the regulatory framework, and provide a supportive environment for innovation and testing through the EU-level sandbox.\n\n## 1.5.5. Assessment of the different available financing options, including scope for redeployment\n\nThe AI Office will make an effort in order to redeploy part of the staff allocated but could do it only partially (15 FTEs) as the staff is currently fully allocated to tasks directly linked to ensuring a timely and correct implementation of the AI Act. New resources will be needed (estimated in 38 additional FTEs) to efficiently exercise the new enforcement tasks.\n\nIn  particular,  the  AI  Office  plans  to  identify  colleagues  with  legal  and  procedural expertise who can take on part of the upcoming new enforcement tasks. At this stage, we  estimate  that  around  5  CAs  with  relevant  profiles  can  be  redeployed  for  this purpose.\n\nIn addition, the AI Office will make an effort to redeploy 5 officials.\n\nThe  AI  Office  envisages  to  make  fully  operational  the  EU-level  sandbox  for  AI systems falling under its monitoring in 2028, which will make  possible a redeployment of 3 CAs needed to set up and run the sandbox. This phased approach would enable to ensure the full operational capacity of the sandbox by 2028, and in particular will also give the AI Office the time to identify the most suitable staff to cover this task and ensure proper project management for facilitating the development, training, testing, and validation of innovative AI systems.\n\nIn addition, the AI Office will explore opportunities to expand the scope of IT tools (currently mostly in development or pre-launch phase) supporting the AI Act to also cover  relevant  new  enforcement  activities  (i.e.  case  handling,  AI  system  registry, monitoring and reporting, exchange of information with authorities). 2 FTEs with IT and administrative profiles will be redeployed to manage these IT tools. This would help to partially cover the management needs related to the new tasks.\n\n<!-- image -->\n\nOverall, these redeployment efforts and synergies will help to address some of the staffing  needs  for  the  new  enforcement  tasks,  while  additional  resources  will  be necessary to ensure the effective implementation of the AI Act.\n\nAdditional staff will be funded under DEP support, given that the objectives of the proposed amendments contribute directly to one key objective of Digital Europe accelerating AI development and deployment in Europe.\n\n<!-- image -->\n\n<!-- image -->\n\n## 1.6. Duration of the proposal/initiative and of its financial impact\n\n- [ ] ï‚¨ limited duration\n\n- [ ] - ï‚¨ in effect from [DD/MM]YYYY to [DD/MM]YYYY\n\n- ï‚¨ financial  impact  from  YYYY  to  YYYY  for  commitment  appropriations  and from YYYY to YYYY for payment appropriations.\n\n## ï¸ unlimited duration\n\n- Implementation with a start-up period from 2026 to 2027,\n\n- followed by full-scale operation.\n\n## 1.7. Method(s) of budget implementation planned\n\n- ï¸ Direct management by the Commission\n\n- [ ] - ï‚¨ by its departments, including by its staff in the Union delegations;\n\n- [ ] - ï‚¨ by the executive agencies\n\n- [ ] ï‚¨ Shared management with the Member States\n\n- [ ] ï‚¨ Indirect management by entrusting budget implementation tasks to:\n\n- [ ] - ï‚¨ third countries or the bodies they have designated\n\n- ï‚¨ international organisations and their agencies (to be specified)\n\n- [ ] - ï‚¨ the European Investment Bank and the European Investment Fund\n\n- ï‚¨ bodies referred to in Articles 70 and 71 of the Financial Regulation\n\n- ï‚¨ public law bodies\n\n- ï‚¨ bodies governed by private law with a public service mission to the extent that they are provided with adequate financial guarantees\n\n- ï‚¨ bodies governed by the private law of a Member State that are entrusted with the  implementation  of  a  public-private  partnership  and  that  are  provided  with adequate financial guarantees\n\n- ï‚¨ bodies or persons entrusted with the implementation of specific actions in the common  foreign  and  security  policy  pursuant  to  Title  V  of  the  Treaty  on European Union, and identified in the relevant basic act\n\n- ï‚¨ï‚  bodies  established  in  a  Member  State,  governed  by  the  private  law  of  a Member  State  or  Union  law  and  eligible  to  be  entrusted,  in  accordance  with sector-specific  rules,  with  the  implementation  of  Union  funds  or  budgetary guarantees, to the extent that such bodies are controlled by public law bodies or by bodies governed by private law with a public service mission, and are provided with adequate financial guarantees in the form of joint and several liability by the controlling bodies or equivalent financial guarantees and which may be, for each action, limited to the maximum amount of the Union support.\n\n<!-- image -->\n\n<!-- image -->\n\n## 2. MANAGEMENT MEASURES\n\n## 2.1. Monitoring and reporting rules\n\nSpecify frequency and conditions.\n\nThe strengthened dispositions will be reviewed and evaluated with the entire AI Act in August 2029. The Commission will report on the findings of the evaluation to the European Parliament, the Council and the European Economic and Social Committee.\n\n## 2.2. Management and control system(s)\n\n- 2.2.1. Justification  of  the  budget  implementation  method(s),  the  funding  implementation mechanism(s), the payment modalities and the control strategy proposed\n\nThe regulation reinforces  the  European  policy with  regard  to  harmonised  rules  for the provision of artificial intelligence systems in the internal market while ensuring the  respect  of  safety  and  fundamental  rights.  The  simplified  single  supervision ensures  consistency  for  the  cross-border  application  of  the  obligations  under  this Regulation.\n\nIn  order  to  face  these  new  tasks,  it  is  necessary  to  appropriately  resource  the Commission's  services.  The  enforcement  of  the  new  regulation  is  estimated  to require 53 FTE.\n\n- 2.2.2. Information concerning the risks identified and the internal control system(s) set up to mitigate them\n\nThe  risks  correspond  to  the  standard  risks  of  Commission  operations  and  are adequately covered by existing standard risk minimising procedures.\n\n- 2.2.3. Estimation and justification of the cost-effectiveness of the controls (ratio between the control costs and the value of the related funds managed), and assessment of the expected levels of risk of error (at payment &amp; at closure)\n\nFor  the  meeting  expenditure,  given  the  low  value  per  transaction  (e.g.  refunding travel  costs  for  a  delegate  for  a  meeting),  standard  control  procedures  seem sufficient.\n\n## 2.3. Measures to prevent fraud and irregularities\n\nSpecify existing or envisaged prevention and protection measures, e.g. from the antifraud strategy.\n\nThe existing fraud prevention measures applicable to the Commission will cover the additional appropriations necessary for this Regulation.\n\n<!-- image -->\n\n<!-- image -->\n\n## 3. ESTIMATED FINANCIAL IMPACT OF THE PROPOSAL/INITIATIVE\n\n## 3.1. Heading(s)  of  the  multiannual  financial  framework  and  expenditure  budget line(s) affected\n\n## Â· Existing budget lines\n\nIn order of multiannual financial framework headings and budget lines.\n\n|                                            | Budget line                                                      | Type of expenditure   | Contribution           | Contribution                                         | Contribution               | Contribution           |\n|--------------------------------------------|------------------------------------------------------------------|-----------------------|------------------------|------------------------------------------------------|----------------------------|------------------------|\n| Heading of multiannual financial framework | Number                                                           | Diff./Non- diff. 27   | from EFTA countries 28 | from candidate countries and potential candidates 29 | From other third countries | other assigned revenue |\n| 7                                          | 20 02 06 Administrative expenditure                              | Nondiff               | No                     |                                                      |                            |                        |\n| 1                                          | 02 04 03 DEP Artificial Intelligence                             | Diff.                 | YES                    | NO                                                   | yes                        | NO                     |\n| 1                                          | 02 01 30 01 Support expenditure for the Digital Europe programme | Nondiff               | yes                    |                                                      | yes                        |                        |\n\n<!-- image -->\n\n## 3.2. Estimated financial impact of the proposal on appropriations\n\n- 3.2.1. Summary of estimated impact on operational appropriations\n- -ï‚¨ The proposal/initiative does not require the use of operational appropriations\n- -ï¸ The proposal/initiative requires the use of operational appropriations, as explained below\n- 3.2.1.1.  Appropriations from voted budget\n\n[\n\nEUR million (to three decimal places)\n\n| Heading of multiannual financial framework                                                   | Heading of multiannual financial framework                                                   | 1                                                                                            |                                                                                              |                                                                                              |                                                                                              |                                                                                              |                                                                                              |                                                                                              |\n|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|\n|                                                                                              |                                                                                              |                                                                                              | Year                                                                                         | Year                                                                                         | Year                                                                                         | Year                                                                                         | After 2027                                                                                   | TOTAL MFF 2021-2027                                                                          |\n| DG:                                                                                          | CNECT                                                                                        |                                                                                              | 2024                                                                                         | 2025                                                                                         | 2026                                                                                         | 2027                                                                                         | After 2027                                                                                   |                                                                                              |\n| Budget                                                                                       | Commitments                                                                                  | (1a)                                                                                         |                                                                                              |                                                                                              | 0,500 30                                                                                     | 0,500 31                                                                                     |                                                                                              | 1,000                                                                                        |\n| line 02                                                                                      | Payments                                                                                     | (2a)                                                                                         |                                                                                              |                                                                                              |                                                                                              | 0,500                                                                                        | 0,500                                                                                        | 1,000                                                                                        |\n| Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes |\n\n<!-- image -->\n\n<!-- image -->\n\n| Budget line 02 01 30 01                                                                      |                                                                                              | (3)                                                                                          |                                                                                              |                                                                                              | 2,642 32                                                                                     | 6,283 33                                                                                     | 7,283                                                                                        | 8,925                                                                                        |\n|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|\n| TOTAL appropriations for DG                                                                  | Commitments                                                                                  | =1a+1b+3                                                                                     |                                                                                              |                                                                                              | 3,142                                                                                        | 6,783                                                                                        | 7,283                                                                                        | 9,925                                                                                        |\n| CNECT                                                                                        | Payments                                                                                     | =2a+2b+3                                                                                     |                                                                                              |                                                                                              | 2,642                                                                                        | 6,783                                                                                        | 7,783                                                                                        | 9,925                                                                                        |\n|                                                                                              |                                                                                              |                                                                                              | Year                                                                                         | Year                                                                                         | Year                                                                                         | Year                                                                                         | After 2027                                                                                   | TOTAL MFF 2021-2027                                                                          |\n| TOTAL                                                                                        | TOTAL                                                                                        | TOTAL                                                                                        | 2024                                                                                         | 2025                                                                                         | 2026                                                                                         | 2027                                                                                         | After 2027                                                                                   | TOTAL MFF 2021-2027                                                                          |\n| Budget line 02 04 03                                                                         | Commitments                                                                                  | (1a)                                                                                         |                                                                                              |                                                                                              | 0,500 34                                                                                     | 0,500 35                                                                                     |                                                                                              | 1,000                                                                                        |\n| Budget line 02 04 03                                                                         | Payments                                                                                     | (2a)                                                                                         |                                                                                              |                                                                                              |                                                                                              | 0,500                                                                                        | 0,500                                                                                        | 1,000                                                                                        |\n| Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes | Appropriations of an administrative nature financed from the envelope of specific programmes |\n\n<!-- image -->\n\n<!-- image -->\n\n| Budget line 02 01 30 01           |             | (3)      | 2,642 36   | 6,283 37   | 7,283   | 8,925   |\n|-----------------------------------|-------------|----------|------------|------------|---------|---------|\n| TOTAL appropriations for DG CNECT | Commitments | =1a+1b+3 | 3,142      | 6,783      | 7,283   | 9,925   |\n| TOTAL appropriations for DG CNECT | Payments    | =2a+2b+3 | 2,642      | 6,783      | 7,783   | 9,925   |\n\n]\n\n[\n\n| Heading of multiannual financial framework   | Heading of multiannual financial framework   | 7   | 'Administrative expenditure'   | 'Administrative expenditure'   | 'Administrative expenditure'   | 'Administrative expenditure'   | 'Administrative expenditure'   |\n|----------------------------------------------|----------------------------------------------|-----|--------------------------------|--------------------------------|--------------------------------|--------------------------------|--------------------------------|\n|                                              |                                              |     |                                | Year                           | Year                           | Year                           | TOTAL MFF                      |\n|                                              | CNECT                                        |     |                                | 2025                           | 2026                           | 2027                           | 2021- 2027                     |\n| ï‚Ÿ Human resources                            |                                              |     |                                |                                | 0,940                          | 0,940                          | 1,880                          |\n| ï‚Ÿ Other administrative expenditure           |                                              |     |                                |                                | 0,025                          | 0,025                          | 0,050                          |\n| TOTAL DG CNECT                               | Appropriations                               |     |                                |                                | 0,965                          | 0,965                          | 1,930                          |\n\n<!-- image -->\n\n<!-- image -->\n\nEN\n\n(Total\n\n| TOTAL appropriations under HEADING 7 of the multiannual financial framework   | commitments = Total payments)   |\n|-------------------------------------------------------------------------------|---------------------------------|\n\n]\n\n## 3.2.2. Estimated output funded from operational appropriations (not to be completed for decentralised agencies)\n\nCommitment appropriations in EUR million (to three decimal places)\n\n| Indicate objectives          |                              |                              | Year 2024                    | Year 2024                    | Year 2025                    | Year 2025                    | Year 2026                    | Year 2026                    | Year 2027                    | Year 2027                    | Enter as many years as necessary to show the duration of the impact (see Section1.6)   | Enter as many years as necessary to show the duration of the impact (see Section1.6)   | Enter as many years as necessary to show the duration of the impact (see Section1.6)   | Enter as many years as necessary to show the duration of the impact (see Section1.6)   | Enter as many years as necessary to show the duration of the impact (see Section1.6)   | Enter as many years as necessary to show the duration of the impact (see Section1.6)   | TOTAL                        | TOTAL                        |\n|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|------------------------------|----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------|------------------------------|------------------------------|\n| and outputs                  | OUTPUTS                      | OUTPUTS                      | OUTPUTS                      | OUTPUTS                      | OUTPUTS                      | OUTPUTS                      | OUTPUTS                      | OUTPUTS                      | OUTPUTS                      | OUTPUTS                      | OUTPUTS                                                                                | OUTPUTS                                                                                | OUTPUTS                                                                                | OUTPUTS                                                                                | OUTPUTS                                                                                | OUTPUTS                                                                                | OUTPUTS                      | OUTPUTS                      |\n| ïƒ²                            | Type 38                      | Avera ge cost                | No                           | Cost                         | No                           | Cost                         | No                           | Cost                         | No                           | Cost                         | No                                                                                     | Cost                                                                                   | No                                                                                     | Cost                                                                                   | No                                                                                     | Cost                                                                                   | Total No                     | Total cost                   |\n| SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦                                                           | SPECIFIC OBJECTIVE No 1 39 â€¦                                                           | SPECIFIC OBJECTIVE No 1 39 â€¦                                                           | SPECIFIC OBJECTIVE No 1 39 â€¦                                                           | SPECIFIC OBJECTIVE No 1 39 â€¦                                                           | SPECIFIC OBJECTIVE No 1 39 â€¦                                                           | SPECIFIC OBJECTIVE No 1 39 â€¦ | SPECIFIC OBJECTIVE No 1 39 â€¦ |\n\n<!-- image -->\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\n| - Output                             |                                      |                                      |          |          |          |          |          |          |          |          |          |          |          |          |          |          |\n|--------------------------------------|--------------------------------------|--------------------------------------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| - Output                             |                                      |                                      |          |          |          |          |          |          |          |          |          |          |          |          |          |          |\n| - Output                             |                                      |                                      |          |          |          |          |          |          |          |          |          |          |          |          |          |          |\n| Subtotal for specific objective No 1 | Subtotal for specific objective No 1 | Subtotal for specific objective No 1 |          |          |          |          |          |          |          |          |          |          |          |          |          |          |\n| OBJECTIVE No 2 ...                   | OBJECTIVE No 2 ...                   | OBJECTIVE No 2 ...                   | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC | SPECIFIC |\n| - Output                             |                                      |                                      |          |          |          |          |          |          |          |          |          |          |          |          |          |          |\n| Subtotal for specific objective No 2 | Subtotal for specific objective No 2 | Subtotal for specific objective No 2 |          |          |          |          |          |          |          |          |          |          |          |          |          |          |\n| TOTALS                               | TOTALS                               | TOTALS                               |          |          |          |          |          |          |          |          |          |          |          |          |          |          |\n\nEN\n\nEN\n\n- 3.2.3. Summary of estimated impact on administrative appropriations\n- -ï‚¨ The proposal/initiative does not require the use of appropriations of an administrative nature\n- -ï¸ The proposal/initiative requires the use of appropriations of an administrative nature, as explained below\n\n## 3.2.3.1. Appropriations from voted budget\n\n[\n\n| VOTED APPROPRIATIONS                          | Year   | Year   | Year   | Year   | TOTAL 2021 - 2027   |\n|-----------------------------------------------|--------|--------|--------|--------|---------------------|\n|                                               | 2024   | 2025   | 2026   | 2027   |                     |\n| HEADING 7                                     |        |        |        |        |                     |\n| Human resources                               |        |        | 0,940  | 0,940  | 1,880               |\n| Other administrative expenditure              |        |        | 0,025  | 0,025  | 0,050               |\n| Subtotal HEADING 7                            |        |        | 0,965  | 0,965  | 1,930               |\n| Outside HEADING 7                             |        |        |        |        |                     |\n| Human resources                               |        |        | 2,429  | 4,858  | 7,287               |\n| Other expenditure of an administrative nature |        |        | 0,213  | 1,425  | 1,638               |\n| Subtotal outside HEADING 7                    |        |        | 2,642  | 6,283  | 8,925               |\n\n]\n\nThe appropriations required for human resources and other expenditure of an administrative nature will be met by appropriations from the DG that are already assigned to management of the action and/or have been redeployed within the DG, together, if necessary, with any additional allocation which may be granted to the managing DG under the annual allocation procedure and in the light of budgetary constraints.\n\n## 3.2.4. Estimated requirements of human resources\n\n- -ï‚¨ The proposal/initiative does not require the use of human resources\n- -ï¸ The proposal/initiative requires the use of human resources, as explained below\n\n<!-- image -->\n\n<!-- image -->\n\n[\n\n| VOTED APPROPRIATIONS                                               | VOTED APPROPRIATIONS                                               | Year 2024   | Year 2025   | Year 2026   | Year 2027   |\n|--------------------------------------------------------------------|--------------------------------------------------------------------|-------------|-------------|-------------|-------------|\n| ï‚Ÿ Establishment plan posts (officials and temporary staff)         | ï‚Ÿ Establishment plan posts (officials and temporary staff)         |             |             |             |             |\n| 20 01 02 01 (Headquarters and Commission's Representation Offices) | 20 01 02 01 (Headquarters and Commission's Representation Offices) | 0           | 0           | 5           | 5           |\n| 20 01 02 03 (EU Delegations)                                       | 20 01 02 03 (EU Delegations)                                       | 0           | 0           | 0           | 0           |\n| 01 01 01 01 (Indirect research)                                    | 01 01 01 01 (Indirect research)                                    | 0           | 0           | 0           | 0           |\n| 01 01 01 11 (Direct research)                                      | 01 01 01 11 (Direct research)                                      | 0           | 0           | 0           | 0           |\n| Other budget lines (specify)                                       | Other budget lines (specify)                                       | 0           | 0           | 0           | 0           |\n| â€¢ External staff (in FTEs)                                         | â€¢ External staff (in FTEs)                                         |             |             |             |             |\n| 20 02 01 (AC, END from the 'global envelope')                      | 20 02 01 (AC, END from the 'global envelope')                      | 0           | 0           | 0           | 0           |\n| 20 02 03 (AC, AL, END and JPD in the EU Delegations)               | 20 02 03 (AC, AL, END and JPD in the EU Delegations)               | 0           | 0           | 0           | 0           |\n| Admin. Support line [XX.01.YY.YY]                                  | - at Headquarters                                                  | 0           | 0           | 0           | 0           |\n| Admin. Support line [XX.01.YY.YY]                                  | - in EU Delegations                                                | 0           | 0           | 0           | 0           |\n| 01 01 01 02 (AC, END - Indirect research)                          | 01 01 01 02 (AC, END - Indirect research)                          | 0           | 0           | 0           | 0           |\n| 01 01 01 12 (AC, END - Direct research)                            | 01 01 01 12 (AC, END - Direct research)                            | 0           | 0           | 0           | 0           |\n| Other budget lines (specify) - Heading 7                           | Other budget lines (specify) - Heading 7                           | 0           | 0           | 0           | 0           |\n| Other budget lines (02 01 30 01) - Outside Heading 7               | Other budget lines (02 01 30 01) - Outside Heading 7               | 0           | 0           | 48          | 48          |\n| TOTAL                                                              | TOTAL                                                              | 0           | 0           | 53          | 53          |\n\n]\n\nThe staff required to implement the proposal (in FTEs):\n\nTo be covered by current staff\n\n<!-- image -->\n\nEN\n\nExceptional additional staff*\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\nEN\n\n## available in the Commission services\n\n|                                |   To be financed under Heading 7 or Research | To be financed from BA line   | To be financed from fees   |\n|--------------------------------|----------------------------------------------|-------------------------------|----------------------------|\n| Establishment plan posts       |                                            5 | N/A                           |                            |\n| External staff (CA, SNEs, INT) |                                           10 | 38                            |                            |\n\n## Description of tasks to be carried out by:\n\n| Officials and temporary staff   | The strengthening of the central supervision by the AI Office will lead to a significant increase in the number of AI systems. These task cannot be carried out by the current staff levels, which are only sufficient for the current scope of supervision.   |\n|---------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| External staff                  |                                                                                                                                                                                                                                                                |\n\n## 3.2.5. Overview of estimated impact on digital technology-related investments\n\nCompulsory: the best estimate of the digital technology-related investments entailed by the proposal/initiative should be included in the table below.\n\nExceptionally, when required for the implementation of the proposal/initiative, the appropriations under Heading 7 should be presented in the designated line.\n\nThe appropriations under Headings 1-6 should be reflected as 'Policy IT expenditure on operational programmes'. This expenditure refers to the operational budget to be used to re-use/ buy/ develop IT platforms/ tools directly linked to the implementation of the initiative and their associated  investments  (e.g.  licences,  studies,  data  storage  etc).  The  information  provided  in  this  table  should  be  consistent  with  details presented under Section 4 'Digital dimensions'.\n\n<!-- image -->\n\nEN\n\n| TOTAL Digital and IT appropriations             | Year 2024   | Year 2025   | Year 2026   | Year 2027   | TOTAL MFF 2021 - 2027   |\n|-------------------------------------------------|-------------|-------------|-------------|-------------|-------------------------|\n| HEADING 7                                       |             |             |             |             |                         |\n| IT expenditure (corporate)                      | 0.000       | 0.000       | 0.000       | 0.000       | 0.000                   |\n| Subtotal HEADING 7                              | 0.000       | 0.000       | 0.000       | 0.000       | 0.000                   |\n| Outside HEADING 7                               |             |             |             |             |                         |\n| Policy IT expenditure on operational programmes | 0.000       | 0.000       | 0.000       | 0.000       | 0.000                   |\n| Subtotal outside HEADING 7                      | 0.000       | 0.000       | 0.000       | 0.000       | 0.000                   |\n| TOTAL                                           | 0.000       | 0.000       | 0.000       | 0.000       | 0.000                   |\n\n## 3.2.6. Compatibility with the current multiannual financial framework\n\nThe proposal/initiative:\n\n- -ï¸ can be fully financed through redeployment within the relevant heading of the multiannual financial framework (MFF)\n\nThe amounts will be redeployed from 02.013001 support expenditure for the Digital Europe Programme for 2026 and from 02.0403 (SO2 artificial intelligence) for 2027.\n\n- -ï‚¨ requires use of the unallocated margin under the relevant heading of the MFF and/or use of the special instruments as defined in the MFF Regulation\n- -ï‚¨ requires a revision of the MFF\n\n## 3.2.7. Third-party contributions\n\nThe proposal/initiative:\n\n- -ï¸ does not provide for co-financing by third parties\n- -ï‚¨ provides for the co-financing by third parties estimated below:\n\n<!-- image -->\n\nAppropriations in EUR million (to three decimal places)\n\n<!-- image -->\n\nEN\n\n|                                  | Year 2024   | Year 2025   | Year 2026   | Year 2027   | Total   |\n|----------------------------------|-------------|-------------|-------------|-------------|---------|\n| Specify the co-financing body    |             |             |             |             |         |\n| TOTAL appropriations co-financed |             |             |             |             |         |\n\n## 3.3. Estimated impact on revenue\n\n- -ï¸ The proposal/initiative has no financial impact on revenue.\n- -ï‚¨ The proposal/initiative has the following financial impact:\n- -ï‚¨ on own resources\n- -ï‚¨ on other revenue\n- -ï‚¨ please indicate, if the revenue is assigned to expenditure lines\n\nEUR million (to three decimal places)\n\n|                      | Appropriations available for the   | Impact of the proposal/initiative 40   | Impact of the proposal/initiative 40   | Impact of the proposal/initiative 40   | Impact of the proposal/initiative 40   |\n|----------------------|------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|\n| Budget revenue line: | current financial year             | Year 2024                              | Year 2025                              | Year 2026                              | Year 2027                              |\n| Articleâ€¦â€¦â€¦â€¦.         |                                    |                                        |                                        |                                        |                                        |\n\nFor assigned revenue, specify the budget expenditure line(s) affected.\n\n40 As regards traditional own resources (customs duties, sugar levies), the amounts indicated must be net amounts, i.e. gross amounts after deduction of 20% for collection costs.\n\n<!-- image -->\n\n<!-- image -->\n\nEN\n\nOther remarks (e.g. method/formula used for calculating the impact on revenue or any other information).\n\n## 4. DIGITAL DIMENSIONS\n\n## 4.1. Requirements of digital relevance\n\n| Reference to the requirement   | Requirement description                                                                                                                                                                                                                                                                                  | Actors affected or concerned by the requirement                                                                                | High-level Processes    | Categories   |\n|--------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|-------------------------|--------------|\n| Article 1(5)                   | Inserting Article 4a : Allowing providers and deployers of AI systems and AI models to exceptionally process special categories of personal data to the extent necessary for the purpose of ensuring bias detection and correction, subject to certain conditions.                                       | Providers and deployers of AI systems and AI models Concerned data subjects                                                    | Data processing         | Data         |\n| Article 1(8)                   | Amending Article 11(1), second subparagraph : Relating to the technical documentation of high-risk AI systems that needs to be drawn up before that system is placed on the market or put into service. SMEs and SMCs are given certain regulatory privileges as concerns this provision of information. | Providers of high-risk AI systems (including SMCs and SMEs) National competent authorities Notified bodies European Commission | Technical documentation | Data         |\n\n<!-- image -->\n\n<!-- image -->\n\nEN\n\n| Article 1(10)   | Amending Article 28, inserting paragraph (1a) : Conformity assessment bodies that apply for a designation may be offered the possibility to submit a single application and undergo a single assessment procedure.                                                                                                                                                                                                                                                                      | Conformity assessment bodies Notifying authorities                                                                                                                            | Application submission   | Data   |\n|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------|--------|\n| Article 1(11)   | Amending Article 29(4 ): Notified bodies which apply for a single assessment shall submit the single application to the notifying authority. The notified body shall update the documentation if relevant changes occur.                                                                                                                                                                                                                                                                | Notified bodies Notifying authority                                                                                                                                           | Application submission   | Data   |\n| Article 1(16)   | Amending Article 56(6) : The Commission shall publish its assessments on the adequacy of the codes of practice.                                                                                                                                                                                                                                                                                                                                                                         | European Commission                                                                                                                                                           | Assessment publication   | Data   |\n| Article 1(26)   | Amending Article 77 : â€¢ Paragraph 1 : National public authorities/bodies which supervise/enforce EU law obligations protecting fundamental rights may make a reasoned request and access any information/documentation from the relevant market surveillance authority â€¢ Paragraph 1a : market surveillance authority shall grant access and, where needed, request the information from the provider/deployer â€¢ Paragraph 1b : where necessary, the aforementioned market surveillance | National public authorities/bodies which supervise/enforce EU law obligations protecting fundamental rights Market surveillance authorities Providers/deployers of AI systems | Information exchange     | Data   |\n\n<!-- image -->\n\n<!-- image -->\n\n| authorities and public authorities/bodies shall exchange information.   |\n|-------------------------------------------------------------------------|\n\n## 4.2. Data\n\nHigh-level description of the data in scope\n\n| Type of data                                                                                       | Reference to the requirement(s)   | Standard and/or specification (if applicable)                                                                                                                                                            |\n|----------------------------------------------------------------------------------------------------|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Special categories of personal data (where the processing is needed for bias detection/correction) | Article 1(5)                      | //                                                                                                                                                                                                       |\n| Technical documentation for high-risk AI systems                                                   | Article 1(8)                      | Technical documentation shall contain, at a minimum, the elements set out in Annex IV of the AI Act. The Commission shall establish a simplified technical documentation form targeted at SMCs and SMEs. |\n| Applications of conformity assessment bodies for designation                                       | Article 1(10)                     | //                                                                                                                                                                                                       |\n| Applications of a conformity assessment bodies for notification                                    | Article 1(11)                     | The notified body shall update the relevant documentation whenever relevant changes occur.                                                                                                               |\n| Commission assessment of the adequacy of the codes of practice                                     | Article 1(16)                     | //                                                                                                                                                                                                       |\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\nEN\n\n| Request for access to information on AI systems                                                                                                 | Article 1(26)   | //                                                |\n|-------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|---------------------------------------------------|\n| Information or documentation requested by national public authorities/bodies which supervise/enforce obligations relating to fundamental rights | Article 1(26)   | To be provided in accessible language and format. |\n\n## Alignment with the European Data Strategy\n\nExplanation of how the requirement(s) are aligned with the European Data Strategy\n\nArticle 1(4) establishes that the processing of special categories of personal data shall be subject to appropriate safeguards for fundamental rights and freedoms of natural persons. This is in alignment with Regulations (EU) 2016/679 (GDPR) and (EU) 2018/1725 (EUDPR).\n\n## Alignment with the once-only principle\n\nExplanation of how the once-only principle has been considered and how the possibility to reuse existing data has been explored\n\nArticle 1(10) states that conformity assessment bodies may be provided the possibility to submit a single application and undergo a single assessment procedure.\n\nExplanation of how newly created data is findable, accessible, interoperable and reusable, and meets high-quality standards\n\nData flows\n\nHigh-level description of the data flows\n\n| Type of data   | Reference(s) to the   | Actors who provide the data   | Actors who receive the data   | Trigger for the data exchange   | Frequency (if applicable)   |\n|----------------|-----------------------|-------------------------------|-------------------------------|---------------------------------|-----------------------------|\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\nEN\n\n|                                                                 | requirement(s)   |                                                                                                                                           |                                                                                                                  |                                                                                              |           |\n|-----------------------------------------------------------------|------------------|-------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|-----------|\n| Applications of a conformity assessment bodies for notification | Article 1(11)    | Notified bodies which are designated under Union harmonisation legislation listed in Section A of Annex I                                 | Notifying authority designated in accordance with Union harmonisation legislation listed in Section A of Annex I | Application being made for single assessment                                                 | //        |\n| Commission assessment of the adequacy of the codes of practice  | Article 1(16)    | European Commission                                                                                                                       | General Public                                                                                                   | Performance of an assessment as regards the codes of practice                                | Regularly |\n| Request for access to information on AI systems                 | Article 1(26)    | National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights | Market surveillance authority                                                                                    | National public authorities/bodies require the information in order to fulfil their mandates | //        |\n| Information or documentation requested by national public       | Article 1(26)    | Market surveillance                                                                                                                       | National public authorities or                                                                                   | Submission of a reasoned request to                                                          | //        |\n\n<!-- image -->\n\nEN\n\n<!-- image -->\n\n| authorities/bodies which supervise/enforce obligations relating to fundamental rights                                                                                                |               | authority                                                   | bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights   | access information                                                                                                                                                                                   |    |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|-------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----|\n| Information or documentation requested by market surveillance authorities                                                                                                            | Article 1(26) | Market surveillance authorities                             | Providers/ deployers of AI systems                                                                           | Market surveillance authority is in need of the information so as to answer to a request from national public authorities/bodies which supervise/enforce obligations relating to fundamental rights) | // |\n| Information exchanges as part of the cooperation of market surveillance authorities and public authorities/bodies which supervise/enforce obligations relating to fundamental rights | Article 1(26) | Market surveillance authorities / Public authorities/bodies | Market surveillance authorities / Public authorities/bodies                                                  | Information exchange need identified in the course of cooperation and mutual assistance                                                                                                              | // |\n\n## 4.3. Digital solutions\n\nHigh-level description of digital solutions\n\n<!-- image -->\n\n<!-- image -->\n\n| Digital solution                                                                                  | Reference(s) to the requirement(s)   | Main mandated functionalities   | Responsible body   | How is accessibility catered for?   | How is reusability considered?   | Use of AI technologies (if applicable)   |\n|---------------------------------------------------------------------------------------------------|--------------------------------------|---------------------------------|--------------------|-------------------------------------|----------------------------------|------------------------------------------|\n| N.A. (the proposed amendments to the AI Act do not foresee the adoption of new digital solutions) |                                      |                                 |                    |                                     |                                  |                                          |\n\nFor each digital solution, explanation of how the digital solution complies with applicable digital policies and legislative enactments\n\n## Digital Solution #1\n\n| Digital and/or sectorial policy (when these are applicable)   | Explanation on how it aligns   |\n|---------------------------------------------------------------|--------------------------------|\n| AI Act                                                        |                                |\n| EU Cybersecurity framework                                    |                                |\n| eIDAS                                                         |                                |\n| Single Digital Gateway and IMI                                |                                |\n| Others                                                        |                                |\n\n<!-- image -->\n\n## 4.4. Interoperability assessment\n\nHigh-level description of the digital public service(s) affected by the requirements\n\n| Digital public service or category of digital public services                      | Description   | Reference(s) to the requirement(s)   | Interoperable Europe Solution(s) (NOT APPLICABLE)   | Other interoperability solution(s)   |\n|------------------------------------------------------------------------------------|---------------|--------------------------------------|-----------------------------------------------------|--------------------------------------|\n| N.A. (the proposed amendments to the AI Act do not affect digital public services) |               |                                      |                                                     |                                      |\n\nImpact of the requirement(s) as per digital public service on cross-border interoperability\n\n## Digital Public Service #1\n\n| Assessment                                                                                                                  | Measure(s)   | Potential remaining barriers (if applicable)   |\n|-----------------------------------------------------------------------------------------------------------------------------|--------------|------------------------------------------------|\n| Alignment with existing digital and sectorial policies Please list the applicable digital and sectorial policies identified |              |                                                |\n\n<!-- image -->\n\n<!-- image -->\n\n| Organisational measures for a smooth cross-border digital public services delivery Please list the governance measures foreseen   |\n|-----------------------------------------------------------------------------------------------------------------------------------|\n| Measures taken to ensure a shared understanding of the data Please list such measures                                             |\n| Use of commonly agreed open technical specifications and standards Please list such measures                                      |\n\n## 4.5. Measures to support digital implementation\n\nHigh-level description of measures supporting digital implementation\n\n| Description of the measure   | Reference(s) to the requirement(s)   | Commission role (if applicable)   | Actors to be involved (if applicable)   | Expected timeline (if applicable)   |\n|------------------------------|--------------------------------------|-----------------------------------|-----------------------------------------|-------------------------------------|\n| N.A.                         |                                      |                                   |                                         |                                     |\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:07:57Z", "sha256": "5f6a24febe2c3503b299509802c73059a084a7fccb2e965b10c1c02e1f28fa66", "meta": {"file_name": "Proposal Digital Omnibus.pdf", "file_size": 731943, "mtime": 1766949659, "docling_errors": []}}
{"doc_id": "pdf-pdfs-singapore-governance-for-agentic-ai-107524b18879", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Singapore - Governance for Agentic AI.pdf", "title": "Singapore - Governance for Agentic AI", "text": "## MODEL AI GOVERNANCE FRAMEWORK FOR AGENTIC AI\n\nVersion 1.0 | Published 22 January 2026\n\nPublished on 19 January 2026 by:\n\n<!-- image -->\n\n## Contents\n\n| Executive Summary........................................................................................................... 1     | Executive Summary........................................................................................................... 1     | Executive Summary........................................................................................................... 1   |\n|------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|\n| Introduction to Agentic AI ........................................................................................... 3           | Introduction to Agentic AI ........................................................................................... 3           | Introduction to Agentic AI ........................................................................................... 3         |\n| 1.1                                                                                                                                | What is Agentic AI? ............................................................................................. 3                | What is Agentic AI? ............................................................................................. 3              |\n| 1.1.1 Core components of an agent ...................................................................................3             | 1.1.1 Core components of an agent ...................................................................................3             |                                                                                                                                  |\n| 1.1.2 Multi-agent setups....................................................................................................4      | 1.1.2 Multi-agent setups....................................................................................................4      |                                                                                                                                  |\n| 1.1.3 Howagent design affects the limits and capabilities of each agent.............................4                              | 1.1.3 Howagent design affects the limits and capabilities of each agent.............................4                              |                                                                                                                                  |\n| 1.2 Risks of Agentic AI............................................................................................... 6           | 1.2 Risks of Agentic AI............................................................................................... 6           | 1.2 Risks of Agentic AI............................................................................................... 6         |\n| 1.2.1 Sources of risk..........................................................................................................6   | 1.2.1 Sources of risk..........................................................................................................6   |                                                                                                                                  |\n| 1.2.2 Types of risk .............................................................................................................7 | 1.2.2 Types of risk .............................................................................................................7 |                                                                                                                                  |\n| 2 Model AI Governance Framework for Agentic AI ........................................................... 8                       | 2 Model AI Governance Framework for Agentic AI ........................................................... 8                       | 2 Model AI Governance Framework for Agentic AI ........................................................... 8                     |\n| 2.1 Assess and bound the risks upfront......................................................................                       | 2.1 Assess and bound the risks upfront......................................................................                       | 9                                                                                                                                |\n| 2.1.1 Determine suitable use cases for agent deployment ..................................................9                        | 2.1.1 Determine suitable use cases for agent deployment ..................................................9                        |                                                                                                                                  |\n| 2.1.2 Bound risks through design by defining agents limits and                                                                     | 2.1.2 Bound risks through design by defining agents limits and                                                                     | permissions......................11                                                                                              |\n| 2.2 Makehumansmeaningfully accountable ............................................................13                              | 2.2 Makehumansmeaningfully accountable ............................................................13                              |                                                                                                                                  |\n| 2.2.1 Clear allocation of responsibilities within and outside the organisation                                                     | 2.2.1 Clear allocation of responsibilities within and outside the organisation                                                     | ....................13                                                                                                           |\n| 2.2.2 Design for meaningfulhuman oversight...................................................................16                    | 2.2.2 Design for meaningfulhuman oversight...................................................................16                    |                                                                                                                                  |\n| 2.3 Implement technical controls and processes                                                                                     | 2.3 Implement technical controls and processes                                                                                     | ......................................................18                                                                         |\n| 2.3.1 During design and development, use technical controls...........................................18                           | 2.3.1 During design and development, use technical controls...........................................18                           |                                                                                                                                  |\n| 2.3.2 Before deploying, test agents ..................................................................................19           | 2.3.2 Before deploying, test agents ..................................................................................19           |                                                                                                                                  |\n| 2.3.3 Whendeploying, continuously monitor and test.......................................................20                        | 2.3.3 Whendeploying, continuously monitor and test.......................................................20                        |                                                                                                                                  |\n| 2.4 Enable end-user responsibility ...........................................................................22                   | 2.4 Enable end-user responsibility ...........................................................................22                   |                                                                                                                                  |\n| 2.4.1 Different users, different needs...............................................................................22            | 2.4.1 Different users, different needs...............................................................................22            |                                                                                                                                  |\n| 2.4.2 Users who interact with agents................................................................................23             | 2.4.2 Users who interact with agents................................................................................23             |                                                                                                                                  |\n|                                                                                                                                    |                                                                                                                                    | ..............................................23                                                                                 |\n| 2.4.3 Users who integrate agents into their work processes A: Further                                                              | 2.4.3 Users who integrate agents into their work processes A: Further                                                              |                                                                                                                                  |\n| Annex resources...............................................................................................25                   | Annex resources...............................................................................................25                   | Annex resources...............................................................................................25                 |\n| Annex B: Call for feedback and case studies                                                                                        | Annex B: Call for feedback and case studies                                                                                        | ......................................................................27                                                         |\n\n## Executive Summary\n\nAgentic AI is the next evolution of AI , holding transformative potential for users and businesses. Compared to generative AI, AI agents can take actions, adapt to new information, and interact with other  agents  and  systems  to  complete  tasks  on  behalf  of  humans.  While  use  cases  are  rapidly evolving,  agents  are  already  transforming  the  workplace  through  coding  assistants,  customer service agents, and automating enterprise productivity workflows.\n\nThese greater capabilities also bring forth new risks . Agents' access to sensitive data and ability to make changes to their environment, such as updating a customer database or making a payment, are double-edged swords. As we move towards deploying multiple agents with complex interactions, outcomes also become more unpredictable.\n\nHumans must remain accountable and properly manage these risks. While existing governance principles for trusted AI such as transparency, accountability and fairness continue to apply, they need to be translated in practice for agents. Meaningful human control and oversight need to be integrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous human oversight over all agent workflows becomes impractical at scale.\n\nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured overview of the risks of agentic AI and emerging best practices in managing these risks. If risks are  properly  managed,  organisations  can  adopt  agentic  AI  with  greater  confidence.  The  MGF  is targeted at organisations looking to deploy agentic AI, whether by developing AI agents in-house or using third-party agentic solutions. Building on our previous model governance frameworks, we have outlined key considerations for organisations in four areas when it comes to agents:\n\n## 1. Assess and bound the risks upfront\n\nOrganisations should adapt their internal structures and processes to account for new risks from agents. Key to this is first understanding the risks posed by the agent's actions, which depend on factors such as the scope of actions the agent can take, the reversibility of those actions, and the agent's level of autonomy.\n\nTo manage these risks early, organisations could limit the scope of impact of their agents by designing appropriate boundaries at the planning stage, such as limiting the agent's access to tools and external systems. They could also ensure that the agent's actions are traceable and controllable through establishing robust identity management and access controls for agents.\n\n## 2. Make humans meaningfully accountable\n\nOnce the 'green light' is given for agentic AI deployment, an organisation should take steps to  ensure  human  accountability.  However,  the  autonomy  of  agents  may  complicate traditional responsibility assignments which are tied to static workflows. Multiple actors may also  be  involved  in  different  parts  of  the  agent  lifecycle,  diffusing  accountability.  It  is therefore  important  to  clearly  define  the  responsibilities  of  different  stakeholders,  both\n\nwithin the organisation and with external vendors, while emphasising adaptive governance, so that the organisation is set up to quickly understand new developments and update its approach as the technology evolves.\n\nSpecifically, 'human -in-theloop' has to be adapted to address automation bias, which has become a bigger concern with increasingly capable agents. This includes defining significant checkpoints in the agentic workflow that require human approval, such as high-stakes or irreversible actions, and regularly auditing human oversight to check that it remains effective over time.\n\n## 3. Implement technical controls and processes\n\nOrganisations  should  ensure  the  safe  and  reliable  operationalisation  of  AI  agents  by implementing technical measures  across  the agent lifecycle. During development, organisations should incorporate technical controls for new agentic components such as planning,  tools  and  still-maturing  protocols,  to  address  increased  risks  from  these  new attack surfaces.\n\nBefore  deployment,  organisations  should  test  agents  for  baseline  safety  and  reliability, including new dimensions such as overall execution accuracy, policy adherence, and tool use. New testing approaches will be needed to evaluate agents.\n\nDuring and after deployment, as agents interact dynamically with their environment and not all risks can be anticipated upfront, it is recommended to gradually roll out agents alongside continuous monitoring after deployment.\n\n## 4. Enable end-user responsibility\n\nTrustworthy deployment of agents does not rely solely on developers, but also on end-users using them responsibly. To enable responsible use, as a baseline, users should be informed of  the  agent's  range  of  actions,  access  to  data,  and  the  user's  own  responsibilities. Organisations should consider layering on training to equip employees with the knowledge required  to  manage  human-agent  interactions  and  exercise  effective  oversight,  while maintaining their tradecraft and foundational skills.\n\nThis is a living document. We have worked with government agencies and leading companies to collate current best practices, but this is a fast-developing space, and best practices will evolve. This framework will need to be continuously updated to keep pace with new developments. We invite feedback  to  refine  the  framework,  and  case  studies  demonstrating  how  the  framework  can  be applied for responsible agentic deployment.\n\n## 1 Introduction to Agentic AI\n\n## 1.1 What is Agentic AI?\n\nAgentic  AI  systems  are  systems  that  can  plan  across  multiple  steps  to  achieve  specified objectives, using AI agents . 1 There is no consensus on what defines an agent, but there are certain common features -agents usually possess some degree of independent planning and action taking (e.g. searching the web or creating files) over multiple steps to achieve a user-defined goal. 2\n\nIn  this  framework,  we  focus  on  agents  built  on  language  models,  which  are  increasingly  being adopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) as its brain to make decisions and complete tasks. However, it is worth noting that software agents are not a new concept and other types of agents exist, such as those which use deterministic rules, or other neural networks, to make decisions. 3\n\n## 1.1.1 Core components of an agent\n\nCore components of a simple agent 4\n\n<!-- image -->\n\nAs agents are built on top of language models, it is helpful to start with the core components of a simple LLM-based app.\n\n1. Model : an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or the  'brain'  of  the  agent.  It  processes  instructions,  interprets  user  inputs,  and  generates contextually appropriate responses.\n\n1 Adapted from Cyber Security Agency of Singapore (CSA), Draft Addendum on Securing Agentic AI.\n\n2 See International AI Safety Report.\n\n3 See World Economic Forum (WEF), AI Agents in Action: Foundations for Evaluation and Governance.\n\n4 Adapted  from  GovTech  Singapore,  Agentic  Risk  &amp;  Capability  Framework,  CSA  Singapore,  Draft Addendum on Securing Agentic AI and Anthropic, Building Effective Agents).\n\n2. Instructions :  Natural  language  commands  that  define  an  agent's  role,  capabilities,  and behavioural constraints e.g. a system prompt for an LLM.\n3. Memory : Information that is stored and accessible to the LLM, either in short or long-term storage.  Sometimes  added  to  allow  the  model  to  obtain  information  from  previous  user interactions or external knowledge sources.\n\nAn agent uses the model, instructions and memory in similar ways as an LLM-based app. In addition, it has other components that enable it to complete more complex tasks:\n\n4. Planning and reasoning :  The model is usually trained to reason and plan, meaning that it can output a series of steps needed for a task.\n6. Protocols: This is a standardised way for agents to communicate with tools and other agents. For  example,  the  Model  Context  Protocol  (MCP)  has  been  developed  for  agents  to communicate with tools, 5 whereas the Agent2Agent Protocol (A2A) defines a standard for agents to communicate with each other. 6\n5. Tools: Tools enable the agent to take actions and interact with other systems, such as writing to files and databases, controlling devices, or performing transactions. The model calls tools to complete a task.\n\n## 1.1.2 Multi-agent setups\n\nIn an agentic system, it is common for multiple agents to be set up to work together. This can sometimes improve performance, by allowing each agent to specialise in a certain function or task and work in parallel. 7\n\nThree common design patterns for multi-agent systems are : 8\n\n- Sequential :  Agents  work  one  after  another  in  a  linear  workflow.  Each  agent's  output becomes the next agent's input.\n- Supervisor : One supervising agent coordinates specialised agents under it.\n- Swarm : Agents work at the same time, handing off to another agent when needed\n\n## 1.1.3 How agent design affects the limits and capabilities of each agent\n\nWhile each agent may have the same core components, the design of each component can significantly  affect  what  the  agent  can  do .  It  is  generally  helpful  to  distinguish  between  two concepts when considering what an agent can do: 9\n\n- Action-space (or  authority, capabilities): Range of actions the agent is permitted to take, determined by the tools it is allowed to use, transactions it can execute, etc.\n\n5 See Anthropic, Model Context Protocol.\n\n6 See Google, Agent2Agent Protocol.\n\n7 See LangChain, Benchmarking Multi-Agent Architectures.\n\n8 Adapted from AWS, Multi-Agent Collaboration Patterns with Strands Agents and Amazon Nova.\n\n9 See WEF, AI Agents in Action: Foundations for Evaluation and Governance.\n\n- Autonomy (or decision-making): Degree to which an agent can decide when and how to act towards  a  goal,  such  as  by  defining  the  steps  to  be  taken  in  a  workflow.  This  can  be determined by its instructions and level of human involvement.\n\n## Action-space\n\n## An agent's action -space mainly depends on the tools it has access to, which can affect:\n\n- Systems it can access:\n- o Internal systems: Tools internal to the organisation, such as being able to search and update the organisation's databases\n- o Sandboxes only: Sandboxed tools (e.g. for code execution, data analysis) that cannot affect any other system\n- o External systems: Tools that enable the agent to access external services, such as retrieving and updating data through third-party pre-defined APIs.\n- Actions it can take in relation to the system it can access:\n- o Read vs write: An agent may only be able to read and retrieve information from a system, rather than write to and modify data within the system.\n\nAn emerging modality of agentic AI is a computer use agent, whose primary tool is access to a computer and browser. This means that it can take any action that a human can take with a computer and browser without having to rely on specifically defined tools and APIs. This significantly increases what the agent can access and do.\n\n## Autonomy\n\n## An agent's autonomy mainly depends on its instructions component and the level of human involvement in the agentic system.\n\nIn terms of instructions, an agent can be given differing level of instructions:\n\n- Detailed instructions and SOP: An agent instructed to follow a detailed SOP to complete a task would be limited in the decisions it can make at each stage.\n- Using its own judgment: An agent instructed to use its own judgment to complete a task would have more freedom to define its plan and workflow.\n\nAnother relevant factor is the level of human involvement. When interacting with an agent, a human can be involved to different levels: 10\n\n- Agent proposes, human operates: The human directs and approves every step taken by an agent.\n- Agent and human collaborate: The  human and agent work together. The agent requires human  approval  at  significant  steps,  such  as  before  writing  to  a  database  or  making  a payment. However, the human can intervene anytime by taking over the agent's work or pausing the agent and requesting a change.\n\n10 See Knight First Amendment Institute at Columbia University, Levels of Autonomy for AI Agents.\n\n- Agent operates, human approves: The agent requires human approval only at critical steps or failures, such as deleting a database or making a payment above a predefined amount.\n- Agent  operates,  human  observes: The  agent  does  not  require  human  approval  as  it completes its task, though its actions may be audited after the fact.\n\n## 1.2 Risks of Agentic AI\n\n## 1.2.1 Sources of risk\n\nThe new components of an agent constitute new sources of risks . 11 The risks themselves are familiar -fundamentally,  agents  are  software  systems  built  on  LLMs.  They  inherit  traditional software vulnerabilities (such as SQL injection) and LLM-specific risks (such as hallucination, bias, data leakage and adversarial prompt injections). 12\n\nHowever, the risks can manifest differently through the different components. For example:\n\n- Planning and reasoning: An agent can hallucinate and make a wrong plan to complete a task.\n- Tools: An agent can hallucinate by calling non-existent tools or calling tools with the wrong input, or calling tools in a biased manner. As tools connect the agent to external systems, prompt  or  code  injections  can  also  manipulate  the  agent  to  exfiltrate  or  otherwise manipulate the data it has access to.\n- Protocols: Finally, as new protocols emerge to handle agent communication, they can also be poorly deployed or compromised e.g. an untrusted MCP server deployed with code to exfiltrate the user's data.\n\nAs components within an agent or multiple agents interact, risks can also arise at the system level. 13 For example:\n\n- Cascading effect: A mistake by one agent can quickly escalate as its outputs are passed onto other agents. For example, in supply chain management, a hallucinated inventory figure from  one  agent  could  potentially  cause  downstream  agents  to  reorder  excessive  or insufficient stock.\n- Unpredictable  outcomes: Agents  working  together  can  also  compete  or  coordinate  in unintended  ways.  For  example,  in  manufacturing,  different  agents  may  be  involved  in managing machines and inventory. While coordinating to meet production goals, the agents might  interact  unpredictably  due  to  complex  optimisation  algorithms  and  over  or  underprioritise one resource or machine, leading to unexpected bottlenecks.\n\n11 BCG highlighted examples of new risks from agents e.g. agents that optimize their own goals locally may create instability across the system, flawed behaviour by one agent may spread to other agents (see What Happens When AI Stops Asking Permission?)\n\n12 Adapted from CSA, Draft Addendum on Securing Agentic AI.\n\n13 See WEF, AI Agents in Action: Foundations for Evaluation and Governance, which highlighted a new class  of  failure  modes,  linked  to  potentially  misaligned  interactions  in  multi-agent  systems  e.g. orchestration drift, semantic misalignment, interconnectedness and cascading effects.\n\n## 1.2.2 Types of risk\n\n<!-- image -->\n\nBecause agents take actions in the real world, when they malfunction, it can lead to harmful real-world impact . Organisations should be aware of these negative outcomes:\n\n- Erroneous actions :  Incorrect actions such as an agent fixing appointments on the wrong date  or  producing  flawed  code.  The  exact  harmful  outcome  depends  on  the  action  in question, e.g. flawed code can lead to exploited security vulnerabilities, and wrong medical appointment s may affect a patient's health outcomes.\n- Unauthorised actions : Actions taken by the agent outside its permitted scope or authority, such as taking an action without escalating it for human approval based on a company policy or standard operating procedure.\n- Biased or unfair actions : Actions that lead to unfair outcomes, especially when dealing with groups  of  different  profiles  and  demographics,  such  as  biased  vendor  selection  in procurement, disbursements of grants, and/or hiring decisions.\n- Data breaches : Actions that lead to the exposure or manipulation of sensitive data. Such data may be personally identifiable information or confidential information e.g. customer details, trade secrets, and/or internal communications. This can be due to a security breach, where attackers exploit agents to reveal private information, or an agent disclosing sensitive data due to a failure to recognise it as sensitive.\n- Disruption to connected systems : As agents interact with other systems, they can cause disruption to connected systems when they are compromised or malfunction e.g. deleting a production codebase, or overwhelming external systems with requests.\n\n## 2 Model AI Governance Framework for Agentic AI\n\nFour dimensions of the MGF for Agentic AI\n\n<!-- image -->\n\nThe  MGF  for  Agentic  AI  builds  on  the  responsible  AI  practices  for  organisations  set  out  in  MGF (2020) 14 by highlighting emerging best practices to address new concerns from agentic AI. This is so that organisations can develop and use agentic AI with the requisite knowledge and judgment.\n\nThe  framework  begins  with  helping  organisations  to assess  and  bound  the  risks  upfront . It highlights new risks that should be considered during risk assessment, and design considerations at the planning stage to limit the potential scope of impact of the agents, as well as ensure that agents are traceable and controllable.\n\nWhile agents may act autonomously, human responsibility continues to apply. Once the 'green light' is  given  to  deploy  agentic  AI,  an  organisation  should  take  immediate  steps  to make  humans meaningfully  accountable .  This  includes  clearly  defining  responsibility  across  multiple  actors within and outside the organisation involved in the agent lifecycle; and taking measures to ensure that human-in-the-loop remains effective over time notwithstanding automation bias.\n\nTo ensure safe and reliable operationalisation of agents, an organisation should adopt technical controls  and  processes across  the AI lifecycle.  During  development,  guardrails  for  new components in AI agents such as planning and tools should be implemented. Before deployment, agents  should  be  tested  for  baseline  safety  and  reliability.  After  deployment,  agents  should  be continuously monitored as they interact dynamically with their environment.\n\nFinally, trustworthy deployment of agents does not rest solely on developers, but also on end-users. Organisations  are  responsible  for enabling  end-user  responsibility by  equipping  them  with essential information to use agents appropriately and exercise effective oversight, while maintaining their tradecraft and foundational skills.\n\n14 See Model AI Governance Framework (2 nd Ed).\n\n## 2.1 Assess and bound the risks upfront\n\nAgents  bring  new  risks,  especially  in  their  access  to  sensitive  data  and  ability  to  change  their environment  through  action-taking.  Their  adaptive,  autonomous  and  multi-step  nature  also increases the potential for unexpected actions, emergent risks and cascading impacts. Organisations should consider these new dimensions as part of risk assessment, and limit the scope of impact of their agents by designing appropriate boundaries at an early stage.\n\nWhen planning for the use of agentic AI, organisations should consider:\n\n- Determining  suitable  use  cases  for  agent  deployment by  considering  agent-specific factors that can affect the likelihood and impact of the risk.\n- Design choices to bound the risks upfront by applying limits on agent's access to tools and systems and defining a robust identity and permissions framework.\n\n## 2.1.1 Determine suitable use cases for agent deployment\n\nRisk identification and assessment is the first step when considering if an agentic use case is suitable for development or deployment .  Risk  is  a  function  of  likelihood (probability of the risk manifesting) and impact (severity of impact if the risk manifests).\n\nThe following non-exhaustive factors affect the level of risk of an agentic use case:\n\n| Factors affecting impact                             | Factors affecting impact                                                                       | Factors affecting impact                                                                                                                                                                                                 |\n|------------------------------------------------------|------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Factor                                               | Description                                                                                    | Illustration                                                                                                                                                                                                             |\n| Domain and use case in which agent is being deployed | Level of tolerance of error in the domain and use case in which the agent is being deployed to | Agent executing financial transactions which require a high degree of accuracy, vs agent that summarises internal meetings                                                                                               |\n| Agent's access to sensitive data                     | Whether the agent can access sensitive data, such as personal information or confidential data | Agent that requires access to personal customer data gives rise to the risk of leaking such data, vs agentwho only has access to publicly available information                                                          |\n| Agent's access to external systems                   | Whether the agent can access external systems                                                  | Agent that sends data to third-party APIs can leak data to these third parties, or disrupt these systems by making too many requests, vs agent that only has access to sandboxed or internal tools                       |\n| Scope of agent's actions                             | Whether an agent can only read from or modify the data and systems it has access to            | Read vs write: Agent that can only read from a database vs being able to write to it Many tools vs a few: Agent that can only choose from a few pre-defined tools, vs an agent whohas unlimited access to a browser tool |\n\n| Reversibility of agent's actions   | If the agent can modify data and systems, whether such modifications are easily reversed   | Agent that schedules meetings vs agent that sends email communications to external parties   |\n|------------------------------------|--------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|\n\n| Factors affecting likelihood       | Factors affecting likelihood                                                                                                                                                                                                       | Factors affecting likelihood                                                                                                                                                         |\n|------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Factor                             | Description                                                                                                                                                                                                                        | Illustration                                                                                                                                                                         |\n| Agent's level of autonomy          | Whether the agent can define the entire workflow or must follow a well-defined procedure. A higher level of autonomy can result in higher unpredictability, increasing likelihood of error.                                        | Agent is provided withaSOPand instructed to follow it when carrying out a task, vs agent is instructed to use its best judgment to select and execute every step                     |\n| Task complexity                    | Howcomplex the task is, in relation to the number of steps required to complete it and the level of analysis required at each step. A higher level of complexity similarly increases unpredictability and the likelihood of error. | Agent is required to extract key action points from a meeting transcript, vs agent is tasked to follow a nuanced data sharing policy when handling external requests for information |\n| Agent's access to external systems | Whether the agent is exposed to external systems, andwho maintains these systems. A higher level of exposure makes the agent more vulnerable to prompt injections and cyberattacks.                                                | Agent can only access an internal knowledge base which is maintained by trusted internal teams, vs an agent who can access the web containing untrusted data                         |\n\nThreat modelling also makes risk assessment more rigorous by systematically identifying specific ways in  which an attacker may take to compromise the system. Common security threats to agentic systems include memory poisoning, tool misuse, and privilege compromise. 15 As  agentic  systems  (especially  multiagent systems) can become very complex, it is often useful to use a method called taint tracing to map out all the  workflows and  interactions to track  how untrusted data can move through the system. For more information on how to perform threat modelling and taint  tracing  for  agentic  systems,  organisations  may refer to CSA's Draft Addendum on Securing Agentic AI .\n\n## The  relationship  between  threat  modelling and risk assessment\n\nThreat modelling augments the risk assessment process  by generating contextualised threat events with well-described sequence of actions, activities  and  scenarios  that  the  attacker  may take  to  compromise  the  system.  With  more relevant threat events, risk assessments will be more  rigorous  and  robust,  resulting  in  more targeted controls and effective layered defence. Since risk assessment is continuous, the threat model should be regularly updated.\n\nAdapted  from CSA, Guide to Cyber Threat Modelling\n\n15 For a more comprehensive coverage of potential security threats to agentic AI systems, see OWASP, Agentic AI -Threats and Mitigations.\n\n## 2.1.2 Bound risks through design by defining agents limits and permissions\n\nHaving selected an appropriate agent use case, organisations can further bound the risks by defining appropriate limits and permission policies for each agent.\n\n## Agent limits\n\nOrganisations should consider defining limits on:\n\n- Agent's access to tools and systems: Define policies that give agents only the minimum tools and data access needed for it to complete its task. 16 For example, a coding assistant may not require access to a web search tool, especially if it already has curated access to the latest software documentation.\n- Agent's autonomy: For process-driven tasks, SOPs and protocols are frequently used to improve  consistency  and  reduce  unpredictability. 17 Define  similar  SOPs  for  agentic workflows that an agent is constrained to follow, rather than giving the agent the freedom to define every step of the workflow.\n- Agent's area of impact: Design mechanisms and procedures to take agents offline and limit their potential scope of impact when they malfunction. This can include running agents in self-contained environments with limited network and data access, particularly when they are carrying out high-risk tasks such as code execution. 18\n\n## Agent identity\n\nIdentity management and access control is one of the key means in which organisations enable traceability and accountability today for humans. As agents become more autonomous, identity management has to be extended to agents as well to track individual agent behaviour and establish who holds accountability for each agent.\n\nThis is an evolving space, and gaps exist today in terms of handling agent identity robustly . For example,  current  authorisation  systems  typically  have  pre-defined,  static  scopes.  However,  to operate safely in more complex scenarios, agents require fine-grained permissions that may change dynamically  depending  on  the  context,  risk  levels,  and  task  objectives.  Current  authentication systems  are  also  typically  based  on  a  single,  unique  individual.  Such  systems  face  difficulty  in handling complex agent setups, such as when agents act for multiple human users with different permissions, or recursive delegation scenarios where agents spin up multiple sub-agents. 19\n\n16 See PwC, The rise -and risks -of agentic AI.\n\n17 Grab introduced an LLM agent framework leveraging on Standard Operating Procedures (SOPs) to guide AI-driven execution (see Introducing the SOP-driven LLM agent frameworks).\n\n18 See McKinsey, Deploying agentic AI with safety and security: A playbook for technology leaders.\n\n19 For  a  more  comprehensive  treatment  of  how  current  identity  systems  may  face  challenges  when catering to agentic AI, see OpenID, Identity Management for Agentic AI.\n\nSolutions  are  being  developed to  address  these  issues,  such  as  integrating  well-established standards like OAuth 2.0 into MCP. 20 The industry is also developing new standards and solutions for agents, such as decentralised identity management and dynamic access control. 21\n\n## In the interim, organisations should consider these best practices to enable agent control and traceability:\n\n- Identification: An agent should have its own unique identity, such that it can identify itself to the organisation, its human user, or other agents. However, an agent's identity may need to be  tied  to  a  supervising  agent,  a  human  user,  or  an  organisational  department  for accountability and tracking. Additionally, the different capacities in which an agent acts (e.g. independently or on behalf of a specified human user) should also be recorded.\n- Authorisation: An agent can have pre-defined permissions based on its role or the task at hand,  or  its  permissions  may  be  dynamically  set  by  its  authorising  human  user,  or  a combination  of  both.  As  a  rule  of  thumb,  the  human  user  should  not  be  able  to  set permissions for the agent greater than what the human user is himself authorised to do. Such delegations of authority should be clearly recorded.\n\n## Evaluating the residual risks\n\nResidual risk is the risk that remains after mitigation measures have been applied. It is important to note that there will always be a level of risk remaining, even after efforts are taken to identify appropriate agentic use cases and define limits on any agents, especially given how quickly agentic AI is evolving. Ultimately, organisations  should  evaluate  and  determine  if  the  residual  risk  for  their  agentic  deployment  is  of  a tolerable level and can be accepted.\n\n20 See MCP specifications for Authentication support , Authorisation support.\n\n21 See proposed framework for agentic identity by Cloud Security Alliance, Agentic AI Identity &amp; Access Management: A New Approach.\n\n## 2.2 Make humans meaningfully accountable\n\nThe organisations that deploy agents and the humans who oversee them remain accountable for the agents' behaviours and actions. But it can be challenging to fulfil this accountability when agent actions  emerge  dynamically  and  adaptively  from  interactions  instead  of  fixed  logic.  Multiple stakeholders may also be involved in different parts of the agent lifecycle, diffusing accountability. Finally, automation bias, or the tendency to over-trust an automated system, especially when it has performed reliably in the past, becomes a bigger concern as humans supervise increasingly capable agents.\n\nTo address these challenges to human accountability, organisations should consider:\n\n- Clear allocation of responsibilities within and outside the organisation , by establishing chains  of  accountability  across  the  agent  value  chain  and  lifecycle,  while  emphasising adaptive  governance,  so  that  the  organisation  is  set  up  to  quickly  understand  new developments and update their approach as the technology evolves.\n- Measures to enable meaningful human oversight of agents ,  such  as  requiring  human approval  at  significant  checkpoints,  auditing  the  effectiveness  of  human  approvals,  and complementing these measures with automated monitoring.\n\n## 2.2.1 Clear allocation of responsibilities within and outside the organisation\n\nAs deployers, organisations and humans remain accountable for the decisions and actions of agents. However, as with AI, the value chain for agentic AI involves multiple actors. Organisations should consider the allocation of responsibility both within their organisation, and vis-Ã -vis other organisations along the value chain.\n\n<!-- image -->\n\nSimplified agentic AI value chain\n\n22\n\n22 For a more comprehensive list of potential stakeholders involved in the agentic AI ecosystem, see CSA and FAR.AI, Securing Agentic AI: A Discussion Paper.\n\n## Within the organisation\n\nWithin  the  organisation,  organisations  should  allocate  responsibilities  for  different  teams across the agent lifecycle. While each organisation is structured differently, this is an illustration of how such responsibilities may be allocated across different teams:\n\n<!-- image -->\n\n<!-- image -->\n\nProduct teams\n\n<!-- image -->\n\nWho: Leaders who define strategic decisions and high-level policies for  the  organisation  e.g.  board  members,  C-suite  executives, managing directors, or department leaders.\n\n## Key responsibilities can include:\n\n- Setting high-level goals for use of agents\n- Defining permitted operational use cases for agents, including limits on agent's data access\n- Setting  the  overall  governance  approach,  including  risk management frameworks and escalation processes\n\nWho: These roles oversee the translation of stakeholder needs or business  goals  into  a  technical  agentic  solution  e.g.  Product Managers, UI / UX Designers, AI Engineers, Software Engineers\n\n## Key responsibilities can include:\n\n- Defining the design and requirements for agents, as well as any feature controls or phased rollouts\n- Reliable  implementation  of  agents  i.e.  development,  predeployment testing and post-deployment monitoring across the agent lifecycle\n- Educating users on responsible use of agentic product\n\nWho: These roles oversee the protection of agentic systems from cyber threats, by implementing and managing security measures, identifying  vulnerabilities,  and  responding  to  incidents  e.g.  Chief Security Officer, Cyber Security Specialist, Penetration Tester\n\n## Key responsibilities can include:\n\n- Defining baseline security guardrails and secure-by-design templates that technical teams should implement or adapt to the agentic system being deployed\n- Conducting regular red teaming and threat modelling\n\nUsers\n\n<!-- image -->\n\nWho: Any individual who utilises the output of the agents to contribute to an  organisational  goal  e.g.  company  employees  making  decisions  or automating workflows and practices.\n\n## Key responsibilities can include:\n\n- Ethical and responsible usage of agents\n- Attending  required  training,  complying  with  usage  policies, timely reporting of bugs or issues with agents\n\n## Developing internal capabilities for adaptive governance\n\nAll  teams  involved  in  the  agentic  AI  lifecycle  should  also  develop  internal  capabilities  to understand agentic AI. As the technology is quickly evolving, being aware of the improvements and limitations of new agentic developments, such as new modalities like computer use agents, or new evaluation frameworks for agents, allow organisations to quickly adapt their governance approach to new developments.\n\n## Outside the organisation\n\nOrganisations may also need to work with external parties when deploying agents e.g. model developers, agentic AI providers, or hosts of external MCP servers or tools.\n\nIn these cases, organisations should similarly ensure that there are measures in place to fulfil its own accountability. Some agent-specific considerations are:\n\n- Clarify distribution of obligations in  any terms and conditions or contracts between the organisation and the external party. In particular, organisations should consider provisions to  address  any  security  arrangements,  performance  guarantees,  or  data  protection  and confidentiality.  Where  there  are  gaps,  the  organisation  should  reassess  if  the  agentic deployment meets its risk tolerance.\n- Features to maintain security and control. Organisations should consider if the external party's product offers features for the organisation to maintain a sufficient level of security or control. This includes strong authentication measures such as scoped API keys, per-agent identity tokens, and robust observability such as the logging of tool calls and access history. Where  such  features  are  lacking,  organisations  should  consider  alternative  or  in-house solutions, or scoping down the agentic use case, such as restricting access to sensitive data.\n\n## End users\n\nOrganisations  may  deploy  agents  to  users  within  or  outside  their  organisation. In  doing  so, organisations should ensure that users are provided sufficient information to hold the organisation accountable, as well as any information relating to the user's own responsibilities. More information can be found in Enabling end-user responsibility below.\n\n## 2.2.2 Design for meaningful human oversight\n\nDefine significant checkpoints or action boundaries that require human approval\n\nTrain humans to evaluate these requests for approval effectively, and audit these approvals\n\nComplement this with automated monitoring mechanisms and predefined alert thresholds\n\nSetting up a system for effective human supervision\n\nOrganisations should define significant checkpoints or action boundaries that require human approval , especially before sensitive actions are executed. This can include: 23\n\n- High-stakes actions and decisions e.g. editing of sensitive data, final decisions in high-risk domains (such as healthcare or legal), actions that may trigger liability\n- Irreversible  actions e.g.  permanently  deleting  data,  sending  communications,  making payments\n- Outlier or atypical behaviour e.g. when agent accesses a system or database outside of its work scope, when agent selects a delivery route that is twice as long as the median distance\n- User-defined . Agents may act on behalf of users who have different risk appetites. Beyond organisation-defined  boundaries,  users  may  be  given  the  option  to  define  their  own boundaries e.g. requiring approval for purchases above a certain amount\n\nApart from considering when approvals are required, organisations should also consider what form approvals should take. These considerations include:\n\n- Keep approval requests contextual and digestible. When  asking  humans for  approval, keep the request short and clear, instead of providing long logs or raw data that may be challenging to decipher and understand.\n- Consider the form of human input required. For straightforward actions such as accessing a database, the human user can simply approve or reject. For more complex cases, such as reviewing an agent's plan before execution, it may be more productive for the human to edit the plan before giving the agent the go-ahead.\n\nOrganisations  should  implement  measures  to  ensure  continued  effectiveness  of  human oversight , particularly as humans remain susceptible to alert fatigue and automation bias. These measures can include:\n\n23 For  further  examples  of  where  human  involvement  may  be  considered,  see  Partnership  on  AI, Prioritising real-time failure detection in AI agents).\n\n- Training  humans to identify common failure modes e.g.  inconsistent  agent  reasoning, agents referring to outdated policies\n- Regularly auditing the effectiveness of human oversight\n\nFinally,  human oversight  should  be complemented with  automated real-time  monitoring  to escalate any unexpected or anomalous behaviour . This can be done by implementing alerts for certain logged events (e.g. attempted unauthorised access or multiple failed attempts to call a tool), using data science techniques to identify anomalous agent trajectories, or using agents to monitor other agents. For more information, see Continuous testing and monitoring below.\n\n## 2.3 Implement technical controls and processes\n\nThe agentic components that differentiate agents from simple LLM-based applications necessitate additional controls during the key stages of the implementation lifecycle.\n\nOrganisations should consider:\n\n- During  design  and  development,  design  and  implement  technical  controls .  The  new components  and  capabilities  of  agents  also  necessitate  new  and  tailored  controls. Depending  on  the  agent  design,  implement  controls  such  as  tool  guardrails  and  plan reflections. Further, limit the agent's impact on the external environment by enf orcing leastprivilege access to tools and data.\n- Pre-deployment, test agents for safety and security. As with all software, testing before deployment ensures that the system behaves as expected. Specifically for agents, test for new dimensions such as overall task execution, policy adherence and tool use accuracy, and test at different levels and across varied datasets to capture the full spectrum of agent behaviour.\n- When deploying, gradually roll out agents and continuously monitor them in production. The autonomous nature of agents and the changing environment makes it challenging to account for and test all possible outcomes before deployment. Hence it is recommended to roll  out agents gradually, supported with real-time monitoring post-deployment to ensure that agents function safely.\n\n## 2.3.1 During design and development, use technical controls\n\nOrganisations  should  design  and  implement  technical  controls  in  the  agentic  AI  system  to mitigate identified risks. For agents specifically, in addition to baseline software and LLM controls, consider adding controls for:\n\n- New agentic components, such as planning and reasoning and tools\n- Increased security concerns from the larger attack surface and new protocols\n\nFor  illustration,  these  are  some  sample  controls  for  agents. For  a  more  comprehensive  list, organisations can refer to CSA's Draft Addendum on Securing Agentic AI and GovTech's Agentic Risk and Capability Framework.\n\n| Planning   | â€¢ Prompt agent to reflect on whether its plan adheres to user instructions â€¢ Prompt the agent to summarise its understanding and request clarification from the user before proceeding â€¢ Log the agent's plan and reasoning for the user to evaluate and verify                                                 |\n|------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Tools      | â€¢ Configure tools to require strict input formats â€¢ Apply the principle of least privilege to limit tools available to each agent, enforced through robust authentication and authorisation â€¢ For data-related tools: o Donotgrant agent write access to tables in sensitive databases unless strictly required |\n\n|           | o Configure agent to let user take over control when keying in sensitive data (e.g. passwords, API keys)                                                                                                                                                                  |\n|-----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Protocols | â€¢ Use standardised protocols where applicable (e.g. agentic commerce protocols when agent is handling a financial transaction) â€¢ For MCPservers: o Whitelist trusted servers and only allow agent to interact with servers on that whitelist o Sandbox any code execution |\n\n## 2.3.2 Before deploying, test agents\n\nOrganisations  should  test  agents  for  safety  and  security  before  deployment. This  provides confidence that the agents work as expected and controls are effective. Best practices on software and LLM testing are still relevant, such as unit and integration testing for software systems, as well as selecting representative datasets, and  useful metrics and evaluators for LLM  testing. Organisations can refer to previous guidance, such as the Starter Kit for testing of LLM-based apps for safety and reliability.\n\nHowever, organisations should adapt their testing approaches for agents. Some considerations include:\n\n- Testing  for  new  risks: Beyond  producing  incorrect  outputs,  agents  can  take  unsafe  or unintended actions through tools. Organisations can consider testing for: 24\n- o Policy compliance : Whether an agent follows defined SOPs and routes for human approval when required\n- o Overall task execution : Whether agent can complete task accurately\n- o Tool calling : Whether an agent calls the right tools, with the right permissions, with the right inputs and in the right order\n- o Robustness : As agents are expected to react and adapt to real-world situations, test for their response to errors and edge cases\n- Testing entire agent workflows: Agents can take multiple steps in sequence without human involvement. Thus, beyond testing an agent's final output, agents should be tested across their entire workflow, including reasoning and tool calling.\n- Testing  agents  individually  and  together: Beyond  individual  agents,  testing  should  be carried out at the multi-agent system level, to understand any emergent risks and behaviours when agents collaborate, such as competitive behaviours or the impact on other agents when one agent has been compromised.\n- Testing in real or realistic environments: As agents may be expected to navigate real-world situations, testing should occur in a properly configured execution environment that mirrors production  as  closely  as  possible,  such  as  using  tool  integrations,  external  APIs,  and sandboxes  that  behave  as  they  would  in  deployment.  However,  organisations  should\n\n24 For an example of new agentic aspects to test for, see Microsoft Foundry,  Agent evaluators.\n\ncalibrate the need for realism against the risk of prematurely allowing agents to access tools that affect the real world.\n\n- Testing repeatedly and across varied datasets: Agent behaviour is inherently stochastic and context-dependent. Testing should thus be done at scale and across varied datasets to observe any unexpected low-probability behaviours, especially if they are high-impact. This requires generating test datasets that cover different conditions that agents may encounter and running these tests multiple times, including minor perturbations where needed.\n- Evaluating  test  results  at  scale: Reliably  evaluating  test  results  at  scale  is  a  known challenge for LLM testing. Agents add a further layer of complexity as their workflows can be long and contain unstructured information that cannot be easily processed by humans or automated  scripts.  Organisations  may  consider  using  different  evaluation  methods  for different parts of the agentic workflow (e.g. deterministic tests for structured tool calls vs LLM or human evaluation for unstructured agent reasoning). However, there is still a need to evaluate agents holistically, so that agent patterns across steps can be evaluated. Current industry solutions thus include defining LLMs or agents to evaluate other agents. 25\n\n## 2.3.3 When deploying, continuously monitor and test\n\nAs agents are adaptive and autonomous, organisations should consider mechanisms to respond to unexpected or emergent risks when deploying agents.\n\n## Gradual deployment of agents\n\nOrganisations  should  consider  gradually  rolling  out  agents  into  production  to  control  the amount of risk exposure . Such rollouts can be controlled based on:\n\n- Users of agents e.g. rolling out to trained or experienced users first\n- Tools and protocols available to agent e.g. restricting agents to more secure, whitelisted MCP servers first\n- Systems exposed to agent e.g. using agents in lower-risk internal systems first\n\n## Continuous testing and monitoring\n\nOrganisations  should  continuously  monitor  and  log  agent  behaviour  post-deployment,  and establish reporting and failsafe mechanisms for agent failures or unexpected behaviours . This allows the organisation to:\n\n- Intervene  in  real-time :  When  potential  failures  are  detected,  stop  agent  workflow  and escalate to a human supervisor e.g. if agent attempts unauthorised access\n- Debug when incidents happen :  Logging and tracing each step of an agent workflow and agent-to-agent interactions help to identify points of failure\n- Audit at regular intervals: This ensures that the system is performing as expected.\n\n25 For an example of agent evaluation solutions, see AWS Labs, Agent Evaluation.\n\nMonitoring and observability are not new concepts, but agents introduce some challenges. As agents  execute  multiple  actions  at  machine  speed,  organisations  face  the  issue  of  extracting meaningful insights from the voluminous logs generated by monitoring systems. This becomes more difficult when high-risk anomalies are expected to be detected in real-time and surfaced as early as possible.\n\n## Key considerations when setting up a monitoring system include:\n\n- What to log: Organisations should determine their objectives for monitoring (e.g. real-time intervention, debugging, integration between components) to identify what to log. In doing so,  prioritise  monitoring  for  high-risk  activities  such  as  updating  database  records  or financial transactions.\n- How to effectively monitor logs: Organisations can consider approaches such as:\n- o Defining alert thresholds:\n- Outlier / anomaly detection : Use data science or deep learning techniques to process agent signals and identify anomalous behaviour that may indicate malfunctions.\n- Programmatic, threshold-based : Define alerts when agents trigger thresholds  e.g.  agent  attempts  unauthorised  access  or  makes  too  many repeated tool calls within a specified timeframe.\n- Agents monitoring other agents : Design agents to monitor other agents in real-time, flagging any anomalies or inconsistencies.\n- o Defining  specific  interventions :  For  each  alert  type,  consider  what  the  level  of intervention  should  be.  Some  degree  of  human  review  should  be  incorporated, proportionate to the risk level. For example, lower-priority alerts can be flagged for review at a scheduled time, whereas higher-priority ones might require temporarily halting  agent  execution  until  a  human  reviewer  can  assess.  In  the  event  of catastrophic agentic malfunction or compromise, commensurate measures such as termination and fallback solutions should be considered.\n\nFinally, continuously test the agentic system even post-deployment to ensure that it works as expected and is not affected by model drift or other changes in the environment.\n\n## 2.4 Enable end-user responsibility\n\nUltimately, end users are the ones who use and rely on agents, and human accountability also extends  to  these  users. Organisations  should  provide  sufficient  information  to  end  users  to promote trust and enable responsible use.\n\nOrganisations should consider:\n\n- Transparency :  Users should be informed of the agents' capabilities (e.g. scope of agent's access to user's data, actions the agent can take) and the contact points whom users can escalate to if the agent malfunctions.\n- Education : Users should be educated on proper use and oversight of agents (e.g. training should be provided on an agent's range of actions, common failure modes like hallucinations, usage policies for data), as well as the potential loss of trade craft i.e. as agents take over more  functions,  basic  operational  knowledge  could  be  eroded.  Hence  sufficient  training (especially in areas where agents are prevalent) should be provided to ensure that humans retain core skills.\n\n## 2.4.1 Different users, different needs\n\nOrganisations should cater to different users with different information needs, to enable such users to use AI responsibly. Broadly, there  are  two  main  archetypes  of  end-users -those  who interact with agents, and those who integrate agents into their work processes or oversee them.\n\n<!-- image -->\n\n## 2.4.2 Users who interact with agents\n\nSuch users usually interact with agents that act on behalf of the organisation, e.g. customer service or sales agents. These agents tend to be external facing, although they can also be deployed within  the  organisation  e.g.  a  human  resource  agent  that  interacts  with  other  users  in  the organisation.\n\nFor  these  users,  focus  on  transparency .  Organisations  should  share  pertinent  information  to foster trust and facilitate proper usage of agents. Such information can include:\n\n- User's responsibilities : Clearly define the user's responsibilities, such as asking the user to double-check all information provided by the agent.\n- Interaction : Declare upfront that the users are interacting with agents.\n- Agent s' range  of  actions  and  decisions :  Inform  the  users  on  the  range  of  actions  and decisions that the agent is authorised to perform and make.\n- Data: Be clear on how user data is collected, stored, and used by the agents, in accordance with the organisation's data privacy policies. Where necessary, obtain explicit consent from users before collecting or using their data for the agents.\n- Human accountability and escalation : Provide users with the respective human contact points who are responsible for the agents, whom the users can alert if the agents malfunction or if they are dissatisfied with a decision.\n\n## 2.4.3 Users who integrate agents into their work processes\n\nSuch users typically utilise agents as part of their internal  workflows e.g.  coding  assistants, automation of enterprise processes. The agent acts for and on behalf of the user.\n\nFor these users, in addition to the information in the previous section, layer on education and training so that users can use the agents responsibly . Key aspects include education and training on:\n\n- Foundational knowledge on agents\n- o Instructing the agents e.g. general best practices in prompting, glossary of keywords to elicit specific responses\n- o Relevant use cases , so that the users understand how to best integrate the agents into their  day-to-day  work,  and  the  scenarios  under  which  the  use  of  agents  should  be restricted (e.g. do not use an agent for confidential data)\n- o Agents' range of actions ,  so  that the user is aware of their capabilities and potential impact\n- Effective oversight of agents\n- o Ongoing support ,  such  as  regular  refreshers  to  update  users  on  latest  features  and common user mistakes\n- o Common agent failure modes , such as hallucinations, getting stuck in loops after errors, so that the user can identify and flag out issues.\n- Potential impact on tradecraft\n\n- o As agents take over entry level tasks, which typically serve as the training ground for new staff, this could lead to loss of basic operational knowledge for the users.\n- o Organisations should identify core capabilities of each job and provide sufficient training and work exposure so that users retain foundational skills.\n\n## Annex A: Further resources\n\n## 1. Introduction to Agentic AI\n\n| What is Agentic AI?   | â€¢ AWS, Agentic AI Security Scoping Matrix: A framework for securing autonomous AI systems â€¢ WEF, AI Agents in Action: Foundations for Evaluation and Governance â€¢ Anthropic, Building effective agents â€¢ IBM, The 2026 Guide to AI Agents â€¢ McKinsey, What is an AI agent?   |\n|-----------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Risks of Agentic AI   | â€¢ GovTech, Agentic Risk &Capability Framework â€¢ CSA, Draft Addendum on Securing Agentic AI â€¢ OWASP, Multi-Agentic System Threat Modelling Guide â€¢ IBM, AI agents: Opportunities, risks, and mitigations â€¢ Infosys, Agentic AI risks to the enterprise, and its mitigations   |\n\n## 2. MGF for Agentic AI\n\n| Assess and bound the risks upfront   | Agentic governance in general â€¢ EY, Building a risk framework for Agentic AI â€¢ McKinsey,DeployingagenticAI with safetyandsecurity:Aplaybook for technology leaders â€¢ Bain, Building the Foundation for Agentic AI â€¢ OWASP, State of Agentic AI Security and Governance 1.0 Risk assessment and threat modelling â€¢ OWASP, Agentic AI - Threats &Mitigations â€¢ OWASP, Multi-Agentic System Threat Modelling Guide â€¢ Cloud Security Alliance, Agentic AI: Understanding Its Evolution, Risks, and Security Challenges â€¢ EY, Building a risk framework for Agentic AI Agent limits and agent identity â€¢ Meta, Agents Rule of Two: APractical Approach to AI Agent Security â€¢ OpenID, Identity Management for Agentic AI   |\n|--------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Makehumans meaningfully accountable  | Allocating responsibility within and outside an organisation â€¢ Carnegie Mellon University, The 'Who', 'What', and 'How' of Responsible AI Governance â€¢ CSAandFAR.AI, Securing Agentic AI: ADiscussion Paper â€¢ McKinsey, Accountability by design in the agentic organization Designing for meaningful humanoversight â€¢ PartnershiponAI, Prioritizing real-time failure detection in AI agents                                                                                                                                                                                                                                                                                                                         |\n\n|                                            | â€¢ Permit.IO, Human-in-the-Loop for AI Agents: Best Practices, Frameworks, Use Cases,andDemo                                                                                                                                                                                                                                                                                                  |\n|--------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Implement technical controls and processes | Technical controls â€¢ GovTech, Agentic Risk &Capability Framework â€¢ CSA, Draft Addendum on Securing Agentic AI Testing and evaluation â€¢ Microsoft, Microsoft Agent Evaluators â€¢ AWS, AWSAgent Evaluation â€¢ Anthropic, Demystifying evals for AI agents â€¢ IBM, What is AI Agent Evaluation? Monitoring and observability â€¢ Microsoft, Top 5 agent observability best practices for reliable AI |\n| Enabling end-user responsibility           | â€¢ Zendesk, What is AI transparency? A comprehensive guide â€¢ HR Brew, Salesforce's head of talent growth and development shares how the tech giant is training its 72,000 employees on agentic AI â€¢ Harvard Business Review, The Perils of Using AI to Replace Entry- Level Jobs                                                                                                              |\n\n## Annex B: Call for feedback and case studies\n\nCall for feedback: This is a living document, and we invite suggestions on how the framework can be updated or refined. The following questions can be used as a guide:\n\n- Introduction to Agentic AI : Are the descriptions of agentic AI systems accurate and sufficiently comprehensive for readers to obtain a clear overview of the governance challenges of agentic AI? Are there other risks that should be included?\n- Proposed Model Governance Framework : Are the four dimensions of the framework practical and applicable? Are there any other dimensions that should be included? For each dimension, are there specific governance and technical challenges and best practices that should be included?\n\nCall  for  case  studies: We  also  invite  organisations  to  submit  their  own  agentic  governance experiences as case studies on how specific aspects of the framework can be implemented, to serve as practical examples of responsible deployment that other organisations can refer to. Case studies should ideally involve an organisation's deployment of an agentic use case that demonstrates one of the dimensions of the framework. While not exhaustive, we are specifically interested in case studies that demonstrate good practices in:\n\n| Dimension                                  | Example case studies                                                                                                                                                                                                                                                                                        |\n|--------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Assess and bound the risks upfront         | â€¢ Defining use cases to reduce risk but maximise benefits of agents â€¢ Defining limits on agent's autonomy through defined SOPs and workflows â€¢ Defining limits on agent's access to tools and systems â€¢ Howidentity is implemented for agents, and howit interacts with human identities in an organisation |\n| Make humans meaningfully accountable       | â€¢ Allocating responsibility across the organisation for agentic deployment â€¢ Assessing when human approvals are required in an agentic use case, and howrequests for such approvals are implemented                                                                                                         |\n| Implement technical controls and processes | â€¢ Designing and implementing technical controls for agents â€¢ Howagentic safety testing is carried out â€¢ Howmonitoring and observability mechanisms are set up, including defining alert thresholds and processing large volumes of agent-related data                                                       |\n| Enable end-user responsibility             | â€¢ Making information available to internal and external stakeholders who interact with and use agents â€¢ Training humanoverseers to exercise effective oversight                                                                                                                                             |\n\nFor  an example of what a case study may look like, please refer to those in our previous Model Governance Framework for AI.\n\nPlease note that any feedback and case studies may be incorporated into an updated version of the framework, and contributors will be acknowledged accordingly. Please submit your feedback and case studies at this link: https://go.gov.sg/mgfagentic-feedback.", "fetched_at_utc": "2026-02-09T14:08:56Z", "sha256": "107524b188794effe3f8110638fa4e9d9ad64ef5ccc155a7a4ddb36968a07fc4", "meta": {"file_name": "Singapore - Governance for Agentic AI.pdf", "file_size": 1078313, "mtime": 1770063233, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-blueprint-for-agentic-ai-in-industral-operations-c9c7b9a92ce1", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The Blueprint for Agentic AI in Industral Operations.pdf", "title": "The Blueprint for Agentic AI in Industral Operations", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:10:09Z", "sha256": "c9c7b9a92ce1a420066deec1d10ede0d5127818e99ccd934022c0f5af0151a66", "meta": {"file_name": "The Blueprint for Agentic AI in Industral Operations.pdf", "file_size": 8294789, "mtime": 1769422290, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-72-europe-s-competitiveness-2a158c852ebb", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #72 - Europe's Competitiveness.pdf", "title": "The EU AI Act Newsletter #72 - Europe's Competitiveness", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:10:23Z", "sha256": "2a158c852ebb06906b554e7b25dc2a4990628ca3548ea5fefae75c4075432675", "meta": {"file_name": "The EU AI Act Newsletter #72 - Europe's Competitiveness.pdf", "file_size": 1213566, "mtime": 1770639419, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-74-human-rights-are-not-optional-5a7784a711e8", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #74 - Human Rights Are Not Optional.pdf", "title": "The EU AI Act Newsletter #74 - Human Rights Are Not Optional", "text": "<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:10:36Z", "sha256": "5a7784a711e89a32919aefc7993fb951126abe87dd147da00664bdd965c27d72", "meta": {"file_name": "The EU AI Act Newsletter #74 - Human Rights Are Not Optional.pdf", "file_size": 1080834, "mtime": 1770639365, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-82-gpai-code-of-practice-goes-live-5a7784a711e8", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #82 - GPAI Code of Practice Goes Live.pdf", "title": "The EU AI Act Newsletter #82 - GPAI Code of Practice Goes Live", "text": "<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:10:51Z", "sha256": "5a7784a711e89a32919aefc7993fb951126abe87dd147da00664bdd965c27d72", "meta": {"file_name": "The EU AI Act Newsletter #82 - GPAI Code of Practice Goes Live.pdf", "file_size": 1148982, "mtime": 1770639175, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-83-gpai-rules-now-apply-2a158c852ebb", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #83 - GPAI Rules Now Apply.pdf", "title": "The EU AI Act Newsletter #83 - GPAI Rules Now Apply", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:11:04Z", "sha256": "2a158c852ebb06906b554e7b25dc2a4990628ca3548ea5fefae75c4075432675", "meta": {"file_name": "The EU AI Act Newsletter #83 - GPAI Rules Now Apply.pdf", "file_size": 960419, "mtime": 1770639138, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-84-trump-vs-global-regulation-2a158c852ebb", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #84 - Trump vs Global Regulation.pdf", "title": "The EU AI Act Newsletter #84 - Trump vs Global Regulation", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:11:19Z", "sha256": "2a158c852ebb06906b554e7b25dc2a4990628ca3548ea5fefae75c4075432675", "meta": {"file_name": "The EU AI Act Newsletter #84 - Trump vs Global Regulation.pdf", "file_size": 1175544, "mtime": 1770639099, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-85-concerns-over-chatbots-and-relationships-a85849a3f44a", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #85 - Concerns Over Chatbots and Relationships.pdf", "title": "The EU AI Act Newsletter #85 - Concerns Over Chatbots and Relationships", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:11:37Z", "sha256": "a85849a3f44a15f6cf47da0ebc5760bbb465bc58c840894da72cb473eae8212d", "meta": {"file_name": "The EU AI Act Newsletter #85 - Concerns Over Chatbots and Relationships.pdf", "file_size": 1362238, "mtime": 1770639043, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-86-concerns-around-gpt-5-compliance-5a7784a711e8", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #86 - Concerns Around GPT-5 Compliance.pdf", "title": "The EU AI Act Newsletter #86 - Concerns Around GPT-5 Compliance", "text": "<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:11:50Z", "sha256": "5a7784a711e89a32919aefc7993fb951126abe87dd147da00664bdd965c27d72", "meta": {"file_name": "The EU AI Act Newsletter #86 - Concerns Around GPT-5 Compliance.pdf", "file_size": 1001680, "mtime": 1770638999, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-87-digital-simplification-consultation-launches-2a158c852ebb", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #87 - Digital Simplification Consultation Launches.pdf", "title": "The EU AI Act Newsletter #87 - Digital Simplification Consultation Launches", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:12:06Z", "sha256": "2a158c852ebb06906b554e7b25dc2a4990628ca3548ea5fefae75c4075432675", "meta": {"file_name": "The EU AI Act Newsletter #87 - Digital Simplification Consultation Launches.pdf", "file_size": 1277284, "mtime": 1770638956, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-88-resources-to-support-implementation-5a7784a711e8", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #88 - Resources to Support Implementation.pdf", "title": "The EU AI Act Newsletter #88 - Resources to Support Implementation", "text": "<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:12:21Z", "sha256": "5a7784a711e89a32919aefc7993fb951126abe87dd147da00664bdd965c27d72", "meta": {"file_name": "The EU AI Act Newsletter #88 - Resources to Support Implementation.pdf", "file_size": 1268109, "mtime": 1770639925, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-89-ai-standards-acceleration-updates-2a158c852ebb", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #89 - AI Standards Acceleration Updates.pdf", "title": "The EU AI Act Newsletter #89 - AI Standards Acceleration Updates", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:12:36Z", "sha256": "2a158c852ebb06906b554e7b25dc2a4990628ca3548ea5fefae75c4075432675", "meta": {"file_name": "The EU AI Act Newsletter #89 - AI Standards Acceleration Updates.pdf", "file_size": 1264631, "mtime": 1770639888, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-90-digital-simplification-package-imminent-2a158c852ebb", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #90 - Digital Simplification Package Imminent.pdf", "title": "The EU AI Act Newsletter #90 - Digital Simplification Package Imminent", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:12:53Z", "sha256": "2a158c852ebb06906b554e7b25dc2a4990628ca3548ea5fefae75c4075432675", "meta": {"file_name": "The EU AI Act Newsletter #90 - Digital Simplification Package Imminent.pdf", "file_size": 2767006, "mtime": 1770639837, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-91-whistleblower-tool-launch-2a158c852ebb", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #91 - Whistleblower Tool Launch.pdf", "title": "The EU AI Act Newsletter #91 - Whistleblower Tool Launch", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:13:10Z", "sha256": "2a158c852ebb06906b554e7b25dc2a4990628ca3548ea5fefae75c4075432675", "meta": {"file_name": "The EU AI Act Newsletter #91 - Whistleblower Tool Launch.pdf", "file_size": 1304196, "mtime": 1770639724, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-92-ai-sandboxes-consultation-open-5a7784a711e8", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #92 - AI Sandboxes Consultation Open.pdf", "title": "The EU AI Act Newsletter #92 - AI Sandboxes Consultation Open", "text": "<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:13:25Z", "sha256": "5a7784a711e89a32919aefc7993fb951126abe87dd147da00664bdd965c27d72", "meta": {"file_name": "The EU AI Act Newsletter #92 - AI Sandboxes Consultation Open.pdf", "file_size": 1190617, "mtime": 1770639678, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-93-transparency-code-of-practice-first-draft-2a158c852ebb", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #93 - Transparency Code of Practice First Draft.pdf", "title": "The EU AI Act Newsletter #93 - Transparency Code of Practice First Draft", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:13:40Z", "sha256": "2a158c852ebb06906b554e7b25dc2a4990628ca3548ea5fefae75c4075432675", "meta": {"file_name": "The EU AI Act Newsletter #93 - Transparency Code of Practice First Draft.pdf", "file_size": 2076456, "mtime": 1770639626, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-94-grok-nudification-scandal-5a7784a711e8", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #94 - Grok Nudification Scandal.pdf", "title": "The EU AI Act Newsletter #94 - Grok Nudification Scandal", "text": "<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:13:55Z", "sha256": "5a7784a711e89a32919aefc7993fb951126abe87dd147da00664bdd965c27d72", "meta": {"file_name": "The EU AI Act Newsletter #94 - Grok Nudification Scandal.pdf", "file_size": 1196653, "mtime": 1770639563, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-eu-ai-act-newsletter-95-one-law-or-a-hundred-5a7784a711e8", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The EU AI Act Newsletter #95 - One Law or a Hundred.pdf", "title": "The EU AI Act Newsletter #95 - One Law or a Hundred", "text": "<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:14:09Z", "sha256": "5a7784a711e89a32919aefc7993fb951126abe87dd147da00664bdd965c27d72", "meta": {"file_name": "The EU AI Act Newsletter #95 - One Law or a Hundred.pdf", "file_size": 1245498, "mtime": 1770639533, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-language-of-trustworthy-ai-glossary-nist-dfa16a5f31a8", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The Language of Trustworthy AI Glossary - NIST.pdf", "title": "The Language of Trustworthy AI Glossary - NIST", "text": "| Terms Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Citation 2                                                                                                               | Definition 3                                                                                                                                              | Citation                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                | Citation 3 Definition 4 4 Definition 5 Citation 5 Related terms and synonyms Legal definition                                                                                                                     | Citation 3 Definition 4 4 Definition 5 Citation 5 Related terms and synonyms Legal definition                                                                                                                     | Citation 3 Definition 4 4 Definition 5 Citation 5 Related terms and synonyms Legal definition                                                                                                                     | Citation 3 Definition 4 4 Definition 5 Citation 5 Related terms and synonyms Legal definition                                                                                                                     | Citation 3 Definition 4 4 Definition 5 Citation 5 Related terms and synonyms Legal definition                                                                                                                     |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| relates to an allocated responsibility. The responsibility can be based on regulation or agreement or through assignment as part of delegation; 2) For systems, a property that ensures that actions of an entity can be traced uniquely the entity; 3) In a governance context, the obligation of an individual or organization to account for its activities, for completion of a deliverable or task, accept the responsibility for those activities, deliverables or tasks, and to disclose results in a transparent manner. ISO/IEC_TS_ the exact or true values that the OECD A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 5723Ê¼2022(en) \"accountable\" (adjective vs. noun): answerable for actions, performance qualitative assessment of correctness or freedom from                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | decisions, and ISO/IEC_TS_ 5723Ê¼2022(en) error.                                                                          |                                                                                                                                                           | The accuracy of a machine learning system is measured as the percentage of correct predictions or classifications made by the model over a specific data set. It is typically estimated using a test or \"hold out\" sample, other than the one(s) Raynor | measure of closeness of results of observations, computations, or estimates to the true values or the values accepted as being true                                                                                                                                                                                                                                                                            | [2]                                                                                                                                                                                                               | accountability 1) to the accuracy Closeness of computations or estimates to                                                                                                                                       | [2]                                                                                                                                                                                                               | [2]                                                                                                                                                                                                               | [2]                                                                                                                                                                                                               |\n| method for modifying machine learning algorithms by allowing them specify test regions to improve their accuracy. At any point, the algorithm can a new point x, observe the output and incorporate the new (x, y) pair into training base. It has been applied to neural networks, prediction functions, Raynor                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Active learning (also called 'query learning,' or sometimes 'optimal design' in the statistics literature) is a subfield of machine learning and, more generally, artificial intelligence. The key hypothesis is that, if the learning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                          | FDA_Glossary The measure of an instrument's capability to approach a true or absolute value. It is a function of precision and bias. FDA_Glossary         | used to construct the model. Its complement, the error rate, is the proportion of incorrect predictions on the same data.                                                                                                                               | ISO/IEC_TS_ 5723Ê¼2022(en)                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                   | statistics were intended to measure. active learning                                                                                                                                                              |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n| A proposed to choose its and clustering functions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | experimental algorithm is allowed to choose the data from which it learns-to be 'curious,' if you will-it will perform better with less training.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | _2009                                                                                                                    | settles_active the process of learning through activities and/or discussion in class, as opposed to passively listening to an expert. Freeman_et_a l_2014 |                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                | passive learning agent                                                                                                                                                                                            | passive learning agent                                                                                                                                                                                            | passive learning agent                                                                                                                                                                                            | passive learning agent                                                                                                                                                                                            | passive learning agent                                                                                                                                                                                            |\n| [a machine learning algorithm that can] decide what actions to take [with regards to its training data, in contrast to a passive learning agent, which is limited to a fixed policy]. Russell_and_N orvig Work that an organization performs using business processes; can be singular or compound. IEEE_Guide_I PA agent takes advantage of the learning the transition model Markov decision process their operation to react to applying a small but such that the perturbed answer. in substantially the amount or application unless the creditor amount or on other terms) and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Set of cohesive tasks of a process. an                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | CSRC observations that then optimal policy; makes imposed through the Russell_and_N orvig imperceptible Zhang, _Yonggang |                                                                                                                                                           |                                                                                                                                                                                                                                                         | that using Russell_and_N orvig A means of learning a model and a reward function from uses value or policy iteration to obtain the utilities or optimal use of the local constraints on utilities of states neighborhood structure of the environment. concept Gama,_Joao intentionally input NISTIR_8269_ Draft Samples generated from real samples with carefully designed perturbations on makes a the ECOA | adversarial perturbation disparate impact ratio, relative risk ratio garey_comput                                                                                                                                 | adversarial perturbation disparate impact ratio, relative risk ratio garey_comput                                                                                                                                 | adversarial perturbation disparate impact ratio, relative risk ratio garey_comput                                                                                                                                 | adversarial perturbation disparate impact ratio, relative risk ratio garey_comput                                                                                                                                 | adversarial perturbation disparate impact ratio, relative risk ratio garey_comput                                                                                                                                 |\n| terms of an account that does not affect accounts or iii) A refusal to an applicant who has made an different outcomes irrespective of of the decision-making procedure. ratio = ð‘ƒ ( ð‘¦Ì‚ ( ð‘‹ ) = fav âˆ£âˆ£ ð‘ = unpr )/ ð‘ƒ ( ð‘¦Ì‚ ( ð‘‹ ) favorable label, ( ð‘ = priv) is the unprivileged group. Varshney, _Kush in increments by following the Development. Gartner A to norm that guides AI development, The OECD] identifies five the responsible stewardship of promote and implement them: inclusive growth, sustainable development and well-being; human-centred values and fairness; transparency and explainability; robustness, security and safety; and accountability. OECD_CAI_re commendation A set of computational rules to be followed to solve a mathematical problem. More recently, the term has been adopted to refer to a process to be followed, often by a computer. Comptroller_O ffice aversion biased assessment of an algorithm which manifests in negative behaviours and attitudes towards the algorithm compared to a human agent. ensur[ing] that powerful AI is properly aligned with human values. ... The challenge of alignment has two parts. The first part is technical and focuses on how to formally encode values or principles in artificial agents so that they reliably do what they ought to do. ... The second part of the value alignment question is normative . It asks what values or principles, if any, we ought to encode in artificial agents. | philosophy and methodology used to describe the develop and deliver software and other digital technologies. requirements and feedback inform incremental development developers. Let [construct space] ð‘Œ â€² and [prediction space] ð‘Œ Ë† be categorical. Then, a model exhibits disparity amplification if ð‘‘ tv ( ð‘Œ Ë† &#124; ð‘ =0, ð‘Œ Ë† &#124; ð‘ =1) > ð‘‘ tv ( ð‘Œ â€² &#124; ð‘ =0, ð‘Œ â€² &#124; ð‘ =1). dtv is the total variation distance defined as follows. Let ð‘Œ 0 and ð‘Œ 1 becategorical random variables with finite supports Y0 and Y1. Then,the total variation distance between ð‘Œ 0 and ð‘Œ 1 is ð‘‘ tv ( ð‘Œ 0, ð‘Œ 1) =12 Î£ ð‘¦ âˆˆ Y0 âˆª Y1 Pr[ ð‘Œ 0= ð‘¦ ] - Pr[ ð‘Œ 1= ð‘¦ ] .In the special case where ð‘Œ 0, ð‘Œ 1 âˆˆ {0, 1}, the total variation distancecan also be expressed as &#124; Pr[ ð‘Œ 0=1] - Pr[ ð‘Œ 1=1] &#124;. | continuous, iterative process User and delivery by yeom_avoiding _2021                                                   | NSCAI                                                                                                                                                     | precise rules for transforming specified inputs into specified outputs in a number of steps                                                                                                                                                             | Ekaterina_et_ al_2020 Gabriel_2020 Merriam- Webster_ampl ify & informs_analyt                                                                                                                                                                                                                                                                                                                                  | finite knuth_art_198 1 algorithms are step-by-step procedures for solving problems. For concreteness, we can think of them simply as being computed programs, written in some precise computer languages ers_1979 | finite knuth_art_198 1 algorithms are step-by-step procedures for solving problems. For concreteness, we can think of them simply as being computed programs, written in some precise computer languages ers_1979 | finite knuth_art_198 1 algorithms are step-by-step procedures for solving problems. For concreteness, we can think of them simply as being computed programs, written in some precise computer languages ers_1979 | finite knuth_art_198 1 algorithms are step-by-step procedures for solving problems. For concreteness, we can think of them simply as being computed programs, written in some precise computer languages ers_1979 | finite knuth_art_198 1 algorithms are step-by-step procedures for solving problems. For concreteness, we can think of them simply as being computed programs, written in some precise computer languages ers_1979 |\n| [an act of amplifying, which is] to make larger or greater (as in amount, importance, or intensity). Analytics is the application of scientific & mathematical methods to the study analysis of problems involving complex systems. There are three distinct types analytics: * Descriptive Analytics gives insight into past events, using historical data. * Predictive Analytics provides insight on what will happen in the future. * Prescriptive Analytics helps with decision making by providing actionable advice. Further documentation accompanying a requirement.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | [the act of] mak[ing] or furnish[ing] critical or explanatory notes or comment                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Merriam- tate                                                                                                            |                                                                                                                                                           |                                                                                                                                                                                                                                                         | of ics_2022 IEEE_Soft_Vo cab                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                   | analytics                                                                                                                                                                                                         |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n| Anything observed in the documentation or operation of a system that                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | cab Condition that deviates from expectations, based on requirements design documents, user documents, or standards, or from someone's or experiences. IAPP_Privacy_ process that removes the association between the identifying dataset and the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Webster_anno SP800-160 CSRC                                                                                              |                                                                                                                                                           |                                                                                                                                                                                                                                                         | deviates IEEE_Soft_Vo                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                   | annotation anomaly                                                                                                                                                                                                |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n| from expectations based on previously verified system, software, or products or reference documents. The process in which individually identifiable data is altered in such a way no longer can be related back to a given individual. Among many techniques, there are three primary ways that data is anonymized. Suppression is the basic version of anonymization and it simply removes some identifying from data to reduce its identifiability. Generalization takes specific values and makes them broader, such as changing a specific age (18) to an                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | specifications, perceptions data subject                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         | and set. Glossary                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                   | anonymization                                                                                                                                                                                                     |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n| hardware that it most values identifying age range (18-24). Noise addition takes identifying values from a given data set switches them with identifying values from another individual in that data Note that all of these processes will not guarantee that data is no longer identifiable and have to be performed in such a way that does not harm the usability of the data. the attribution of distinctively human-like feelings, mental states, and behavioral                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         | hism_in_AI_2 020 that goes beyond what is directly observable SP800-37 A hardware/software system implemented to satisfy requirements.                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n| characteristics to inanimate objects, animals, and in general to natural                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | a particular human-like interpretation of existing physical features and behaviors                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Anthropomorp hism_in_AI_2 020 CSRC                                                                                       | software or a program that is specific tothe solution of an application                                                                                   | aime_measure                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                   | anthropomorphism phenomena and supernatural entities                                                                                                                                                              |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n| system. a software contract between the application and client, expressed as a of methods or functions. . . it defines the available functions you can the intermediary interface between the client and the application. an engineered or machine-based system that can, for a given set of generate outputs such as predictions, recommendations, or decisions real or virtual environments. AI systems are designed to operate with levels of autonomy                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Anthropomorp a particular set of                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                          |                                                                                                                                                           | problem ISO/IEC                                                                                                                                                                                                                                         | collection . . . Hands- On_Smart_Co ntract_Dev NIST AI RMF                                                                                                                                                                                                                                                                                                                                                     | ment_2022 citing TR 24030                                                                                                                                                                                         | ment_2022 citing TR 24030                                                                                                                                                                                         | ment_2022 citing TR 24030                                                                                                                                                                                         | ment_2022 citing TR 24030                                                                                                                                                                                         | ment_2022 citing TR 24030                                                                                                                                                                                         |\n| interface execute;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | (Adapted from: OECD Recommendati on on AIÊ¼2019; ISO/IEC                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         | 22989Ê¼2022). IEEE_Guide_I                                                                                                                                                                                                                                                                                                                                                                                      | weak intelligence; applied intelligence                                                                                                                                                                           | weak intelligence; applied intelligence                                                                                                                                                                           | weak intelligence; applied intelligence                                                                                                                                                                           | weak intelligence; applied intelligence                                                                                                                                                                           | weak intelligence; applied intelligence                                                                                                                                                                           |\n| The ingestion of a corpus, application of semantic mapping, and relevant ontology of structured and/or unstructured data that yields inference and correlation leading to the creation of useful conclusive or predictive in a given knowledge domain. Strong AI learning also includes the capability creating unique hypotheses, attributing data relevance, processing data relationships, and updating its own lines of inquiry to further the usefulness purpose. [an AI system that] is designed to accomplish a specific problem-solving or                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | its PA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         | capabilities of of                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | OECD_Artifici al_Intelligence _in_Society                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n| reasoning task.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         | objectives, influencing varying                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n| (ANI)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n| intelligence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                   | learning                                                                                                                                                                                                          |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                          |                                                                                                                                                           |                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                   | artificial artificial narrow intelligence                                                                                                                                                                         | artificial artificial narrow intelligence                                                                                                                                                                         |                                                                                                                                                                                                                   |                                                                                                                                                                                                                   |\n\n<!-- image -->\n\n|                                  | Terms Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Citation 2                                                | Definition 3                                                                                                                                                                                                                                                                                                                                               | Citation 3 Definition 4 Citation 4                                                                                                                                                                                                                                                                                                                                |                         | Related terms and synonyms Legal definition   |\n|----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|-----------------------------------------------|\n| artificial networks              | computing system, made up of a number of simple, highly interconnected processing elements, which processes information by its dynamic state response external inputs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Reznik,_Leon Definition 1. A directed graph is called an Artificial Neural Network (ANN) if it has x at least one start node (or Start Element; SE), x at least one end node (or End Element; EE), x at least one Processing Element (PE), x all the nodes used must Processing Elements (PEs), except start nodes and end nodes, x a state variable associated with each node i, x a real valued weight wki associated with each link (ki) from node k to node i, x a real valued bias bi associated with each node i, x at least two of the multiple PEs connected in parallel, x a learning algorithm that helps to model the desired output for given input. x a flow on each link (ki) from node k to node i, that carries exactly the same flow which equals to nk caused by the output of node k , x each start node is connected to at least one end node, | be ni                                                     |                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                   | neural A to             | [2] applicable                                |\n| attack                           | Action of applying specific documented criteria to a specific software module, package or product for the purpose of determining acceptance or release of the software module, package or product. IEEE_Soft_Vo cab Action targeting a learning system to cause malfunction. NISTIR_8269_ Draft Property associated with a a set of real or abstract things that is some characteristic of interest. IEEE_Soft_Vo cab                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | (each link (ki) from node k to node i is unique). the action or an instance of making a judgment about something : the act of assessing something : APPRAISAL Any kind of malicious activity that attempts to collect, disrupt, deny, degrade, or destroy information system resources or the information itself. property or characteristic of an object that can be distinguished quantitatively or qualitatively by human or automated means                                                                                                                                                                                                                                                                                                                                                                                                                    | Merriam- Webster_asses sment CSRC aime_measure ment_2022, |                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                   | assessment attribute    |                                               |\n| audit                            | Systematic, independent, documented process for obtaining records, statements of fact, or other relevant information and assessing them objectively, to determine the extent to which specified requirements are fulfilled. IEEE_Soft_Vo cab                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | To conduct an independent review and examination of system records and activities in order to test the adequacy and effectiveness of data security and data integrity procedures, to ensure compliance with established policy and operational procedures, and to recommend any necessary changes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | TR 24029-1 FDA_Glossary                                   | Independent examination of a software product, software process, or set of software processes to assess compliance with specifications, standards, contractual agreements, or other criteria andards                                                                                                                                                       | NASA_Soft_St Independent review conducted to compare the various aspects of the laboratory' s performance with a standard for that performance. Also defined as a systematic, independent and documented process for obtaining audit evidence and evaluating it objectively to determine the extent to which audit criteria are fulfilled. UNODC_Gloss ary_QA_GLP |                         |                                               |\n| audit log authenticity           | A chronological record of system activities, including records of system accesses and operations performed in a given period. property that an entity is what it claims to be Independent machine-managed choreography of the operation of one or more                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | SP800-37 ISO/IEC_TS_ 5723Ê¼2022(en) IEEE_Guide_I conversion of processes or equipment to automatic operation, or the results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | of IEEE_Soft_Vo                                           | The                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                   |                         |                                               |\n| automation                       | digital systems. PA over-relying on the outputs of AI systems                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | the conversion                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | cab                                                       | system functions with no/little human operator involvement; however, the system performance is limited to the specific actions it has been designed to do. Typically these are well-defined tasks that have predetermined responses (i.e., simple rule-based responses).                                                                                   | DOD_TEVV                                                                                                                                                                                                                                                                                                                                                          | automation bias         |                                               |\n| autonomic                        | s A monitor-analyze-plan-execute (MAPE) computer system capable of sensing environments, interpreting policy, accessing knowledge (data --- information --- knowledge), making decisions, and initiating dynamically assembled routines of choreographed activity to both complete a process and update the set of environmental variables that enables the autonomic system to self-manage its own operation and the processes it oversees. An autonomic system is identified by eight characteristics: a) Knows the resources to which it has access, what its capabilities and limitations are, and how and why it is connected to other systems. b) Is able to configure and reconfigure itself depending on the changing computing environment. c) Is able to optimize its performance to ensure the most efficient computing process. d) Is able to work around encountered problems either by repairing itself or routing functions away from the trouble. e) Is able to detect, identify, and protect itself against various types of attacks to maintain overall system PA | David_Leslie_ Morgan_Brigg IEEE_Guide_I                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                           |                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                   |                         |                                               |\n| vehicle autonomy                 | [an] automobile, bus, tractor, combine, boat, forklift, etc. . . . capable of sensing its environment and moving safely with little or no human input. A system's level of independence from human involvement and ability to operate without human intervention. [Different AI systems have different levels of autonomy.] An autonomous system has a set of learning, adaptive and analytical capabilities to respond to situations that were not pre-programmed or anticipated (i.e., decision-based responses) prior to system deployment. Autonomous or semi- autonomous AI systems can be characterised as \"human-in-the-loop\", \"human- on-the-loop\", or \"human-out-of-the loop\" systems depending on their level of meaningful involvement of human beings. gy                                                                                                                                                                                                                                                                                                               | Introduction_t o_Information _Systems TTC6_Taxono my_Terminolo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                           |                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                   | autonomous              |                                               |\n| availability back-testing        | Ensuring timely and reliable access to and use of information. A form of outcomes analysis that involves the comparison of actual outcomes with modeled forecasts during a development sample time period (in-sample back-testing) and during a sample period not used in model development (out- of-time back-testing), and at an observation frequency that matches the forecast horizon or performance window of the model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | The property that data or information is accessible and usable upon an authorized person.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | NIST_SP_800                                               | property of being accessible and usable on demand by an authorized entity ISO/IEC_TS_ 5723Ê¼2022(en)                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                   |                         |                                               |\n| batched benchmark                | Process automation execution of intentionally segregated work processes that are able to be processed irrespective of their contextual placement within a service. PA Standard against which results can be measured or assessed; Procedure, problem, or test that can be used to compare systems or components to each cab                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | SP800-37 demand by Comptroller_O ffice IEEE_Guide_I IEEE_Soft_Vo An alternative prediction or approach used to compare a model's inputs and outputs to estimates from alternative internal or external data or models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Comptroller_O ffice                                       | The term benchmarking is used in machine learning (ML) to refer to the evaluationand comparison of ML methods regarding their ability to learn in 'benchmark'datasets that have been applied as 'standards'. could be thought of simplyas a sanity check to confirm that a new successfully runs as expected and canreliably find simple patterns that 017 | olson_pmlb_2                                                                                                                                                                                                                                                                                                                                                      | automation              |                                               |\n| bias bias algorithm bias testing | other or to a standard. A systematic error. In the context of fairness, we are concerned with unwanted bias that places privileged groups at systematic advantage and unprivileged groups at systematic disadvantage. 60 A procedure for reducing unwanted bias in training data or models. 60 As it relates to disparate impact, courts and regulators have utilized or considered as acceptable various statistical tests to evaluate evidence of disparate impact. Traditional methods of statistical bias testing look at differences in predictions across protected classes, such as race or sex. In                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | AI_Fairness_3 (computational bias) An effect which deprives a statistical result of representativeness by systematically distorting it, as distinct from a random error which may distort on any one occasion but balances out on the average. AI_Fairness_3 SP1270                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | OECD                                                      | patterns Benchmarking method existing methods are known to identify. (systemic bias) systematic difference in treatment of certain objects, people or groups in comparison to others                                                                                                                                                                       | measurement_ iso22989_2022 (mathematical) A point estimator \\theta_hat is said to be an unbiased estimator fo \\theta if E(\\theta_hat) = \\theta for every possible value of \\theta. If \\theta_hat is not unbiased, the difference E(\\theta_hat) - \\theta is called the bias of \\theta devore_probab ility_2004                                                     | mitigation              |                                               |\n| big data binning                 | particular, courts have looked to statistical significance testing to assess whether the challenged practice likely caused the disparity and was not the result of chance or a nondiscriminatory factor. consists of extensive datasets primarily in the characteristics of volume, variety, velocity, and/or variability GLYPH<c=0,font=/MUFUZY+TimesNewRomanPSMT> that require a scalable architecture for efficient storage, manipulation, and analysis a technique of lumping small ranges of values together into categories, or \"bins,\" for the purpose of reducing the variability (removing some of the fine structure) in a data set.                                                                                                                                                                                                                                                                                                                                                                                                                                      | NIST_1500 Pyle, _Dorian_Data _Preparation_ as_a_Process GDPR an individual's physiological, biological, or behavioral characteristics, including information pertaining to an individual's deoxyribonucleic acid (DNA), that is or is intended to be used singly or in combination with each other or with other identifying data, to establish individual identity. Biometric information includes,                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                           |                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                   |                         |                                               |\n| biometric                        | personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person, such as facial images or dactyloscopic data;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | used but is not limited to, imagery of the iris, retina, fingerprint, face, hand, palm, vein patterns, and voice recordings, from which an identifier template, such as a faceprint, a minutiae template, or a voiceprint, can be extracted, and keystroke                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | data CCPA A                                               | measurable physical characteristic or personal behavioral trait used to recognize the identity, or verify the claimed identity, of an applicant. Facial images, fingerprints, and iris scan samples are all examples of biometrics.                                                                                                                        | SP800-12                                                                                                                                                                                                                                                                                                                                                          | data                    | personal data; processing                     |\n| boosting                         | A machine learning technique that iteratively combines a set of simple and not very accurate classifiers (referred to as \"weak\" classifiers) into a classifier with high accuracy (a \"strong\" classifier) by upweighting the examples that the model is currently misclassifying                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | that contain identifying information. aime_measure ment_2022, citing Machine Learning Glossary by Google                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                           |                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                   |                         |                                               |\n| breach                           | The loss of control, compromise, unauthorized disclosure, unauthorized acquisition, or any similar occurrence where: a person other than an authorized user accesses or potentially accesses personally identifiable information; or an authorized user accesses personally identifiable information for another than authorized purpose. CSRC Complex, computational, cognitive automation system capable of providing descriptive, predictive, prescriptive, and limited deductive analytics with relevance and accuracy exceeding human expertise in a broad, logically related PA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                           |                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                   | broad artificial (broad |                                               |\n| intelligence                     | AI)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | IEEE_Guide_I                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                           |                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                   |                         |                                               |\n\n<!-- image -->\n\n|                                                                                                            | Terms Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Citation 2                                                                                   | Definition 3                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Citation 3 Definition 4                    | Citation 4 Definition                                                                                                                                            | Related terms and synonyms Legal definition   |\n|------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n| built-in test bug-bounty business process business process management business rule calibration capability | Equipment or software embedded in the operational components or systems, as opposed to external support units, which perform a test or sequence of tests to verify mechanical or electrical continuity of hardware, or the proper automatic sequencing, data processing, and readout of hardware or software systems. SP1011 Reward given to independent security researchers, penetrations testers, and white hat hackers for discovering exploitable software vulnerabilities and sharing this knowledge with the operator of a particular bug-bounty program (BBP). Kuehn, _Andreas A defined set of business activities that represent the steps or tasks required to achieve a business objective, including the flow and use of information, participants, and human or digital resources. PA Discipline involving any combination of modeling, automation, execution, control, measurement and optimization of business activity flows, in support of enterprise goals, spanning systems, employees, customers, and partners within and beyond the enterprise boundaries. PA Definition, constraint, dependency, or decision criteria that determine the method of execution of a task or tasks, or influences the order of execution of a task or tasks. Business rules assert control, or influence the behavior, of a business process within computing systems. PA A comparison between a device under test and an established standard, such as UTC(NIST). When the calibration is finished, it should be possible to state the estimated time offset and/or frequency offset of the device under test with respect to the standard, as well as the measurement uncertainty. CSRC measure of capacity and the ability of an entity, person or organization to achieve                   | IEEE_Guide_I IEEE_Guide_I IEEE_Guide_I operation that, under specified conditions, in a first step, establishes a relation between the quantity values with measurement uncertainties provided by measurement standards and corresponding indications with associated measurement uncertainties and, in a second step, uses this information to establish a relation for obtaining a measurement result from an indication ISO/IEC_TS_                                                                                                                                                                                                                                                                                          | aime_measure ment_2022, citing ISO/IEC Guide 99                                              | Set of operations that establish, under specified conditions, the relationship between values indicated by a measuring instrument or measuring system, or values represented by a material measure, and the corresponding known values of a measurand .                                                                                                                                                                                                           | UNODC_Gloss ary_QA_GLP                     |                                                                                                                                                                  |                                               |\n| case chatbot choreography classification classifier clustering cognitive automation                        | its objectives Single entry, single exit multiple way branch that defines a control expression, specifies the processing to be performed for each value of the control expression, and returns control in all instances to the statement immediately following the overall construct. cab Conversational agent that dialogues with its user (for example: empathic robots available to patients, or automated conversation services in customer relations). ary An ordered sequence of system-to-system message exchanges between two or more participants. In choreography, there is no central controller, responsible entity, or observer of the process. PA When the output is one of a finite set of values (such as sunny, cloudy or rainy), the learning problem is called classification, and is called Boolean or binary classification if there are only two values. AIMA A model that predicts categorical labels from features. 60 Detecting potentially useful clusters of input examples. AIMA The identification, assessment, and application of available machine learning algorithms for the purpose of leveraging domain knowledge and reasoning to further automate the machine learning already present in a manner that may be thought of as cognitive. With cognitive automation, the system performs corrective actions driven by knowledge of the underlying analytics tool itself, iterates its own automation approaches and algorithms for more expansive or more thorough analysis, and is thereby able to fulfill its purpose. The automation of the cognitive process refines itself and dynamically generates novel hypotheses that it can likewise assess against its existing corpus and other information resources. PA Complex computational systems designed to | 5723Ê¼2022(en) IEEE_Soft_Vo COE_AI_Gloss IEEE_Guide_I task of assigning collected data to target categories or The basic problem of clustering may be stated as follows: Given a set of data points, partition them into a set of groups which are as similar as possible. ering_2013 IEEE_Guide_I                                                                                                                                                                                                                                                                                                                                                                                                                               | classes. aime_measure ment_2022, citing ISO/IEC TR 24030                                     | aggarwal_clust the tendency for items to be consistently grouped together in the course of recall. This grouping typically occurs for related items. It is readily apparent in memory tasks in which items from the same category, such as nonhuman animals, are recalled together. g                                                                                                                                                                             | APA_clusterin                              |                                                                                                                                                                  |                                               |\n| cognitive column computer concept drift                                                                    | - Sense (perceive the world and collect data); - Comprehend (analyze and understand the information collected); - Act (make informed decisions and provide guidance based on this analysis in an independent way); and - Adapt (adapt capabilities based on experience) in ways comparable to the human brain. PA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | AI_Fairness_3 IEEE_Guide_I                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                            |                                                                                                                                                                  |                                               |\n| computing                                                                                                  | In the context of relational databases, a column is a set of data values, all of a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | techopedia_co lumn_2022 An image understanding task that automatically builds a description not only of the image itself, but of the three dimensional scene that it depicts. NBSIR_82- 2582 an online supervised learning scenario when the relation between the input data and the target variable changes over time. Gama,_Joao Preserving authorized restrictions on information access and disclosure, including means for protecting personal privacy and proprietary information. CSRC legal guardian, a person who has power of attorney, or a conservator for the consumer, including by a statement action, signifies agreement to the processing of personal the consumer for a narrowly defined particular purpose. | person acting as a or by a clear affirmative information relating to Acceptance of a general |                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | NIST_SP_800 A they                         |                                                                                                                                                                  |                                               |\n| vision confidentiality confusion matrix consent                                                            | single type, in a table. The digital process of perceiving and learning visual tasks in order to interpret and understand the world through cameras and sensors. NSCAI Use of a system outside the planned domain of application, and a common cause of performance gaps between laboratory settings and the real world. SP1270 Data confidentiality is a property of data, usually resulting from legislative measures, which prevents it from unauthorized disclosure. OECD A matrix showing the predicted and actual classifications. A confusion matrix is of size LxL, where L is the number of different label values 'Consent' of the data subject means any freely given, specific, informed and unambiguous indication of the data subject's wishes by which he or she, by a GDPR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Kohavi,_Ron 'Consent' means any freely given, specific, informed, and unambiguous indication of the consumer's wishes by which the consumer, or the consumer's CCPA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Systems intrusion are tracking                                                               | that classify or predict a concept (e.g., credit ratings or computer monitors) over time can suffer performance loss when the changes. This is referred to as concept drift. This can either natural process that occurs without a reference to the system, or an active process, where others are reacting to the system (e.g., virus detection). The property that data or information is not made available or disclosed to unauthorized persons or processes. | concept they be a Raynor                   | property that information is not disclosed to users, processes, or devices unless have been authorized to access the information. CISA                           |                                               |\n| constituent system constraint                                                                              | independent system that forms part of a system of systems (SoS) (note: Constituent systems can be part of one or more SoS. Each constituent system is a useful system by itself, having its own development, management, utilization, goals, and resources, but interacts within the SoS to provide the unique capability of the SoS). Specification of what may be contained in a data or metadata set in terms of the content or, for data only, in terms of the set of key combinations to which specific attributes (defined by the data structure) may be attached. OECD the degree to which the application of constructs to phenomena is warranted                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | or broad terms of use, or similar document, that contains descriptions of personal information processing along with other, unrelated information, does not constitute consent. Hovering over, muting, pausing, or closing a given piece of content does not constitute consent. Likewise, agreement obtained through use of dark patterns does not constitute consent. ISO/IEC_TS_ 5723Ê¼2022(en) A limitation or implied requirement that constrains the design solution or implementation of the systems engineering process and is not changeable by the enterprise cab Construct validation is involved whenever a test is to be interpreted as a measure                                                                   | IEEE_Soft_Vo problem test cronbach_con struct_1955 to how well Modern APA_content_ validity  | Established experimentally to demonstrate that a survey distinguishes between people who do and do not have certain characteristics. It is usually established experimentally.                                                                                                                                                                                                                                                                                    | fink_survey_2 010 the measurements useful. | Establishing construct validity means demonstrating, in a variety of ways, that obtained from measurement model are both meaningful and jacobs_measur ement_2023 | personal data                                 |\n| construct validity content validity                                                                        | with respect to the research goals and questions. Wieringa, _Roel_J. Refers to the extent to which a measure thoroughly and appropriately assesses the skills or characteristics it is intended to measure. 010                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | of some attribute or quality which is not 'operationally defined.' The faced by the investigator is, 'What constructs account for variance in performance?' fink_survey_2 the extent to which a test measures a representative sample of the subject matter or behavior under investigation. For example, if a test is designed survey arithmetic skills at a third-grade level, content validity indicates it represents the range of arithmetic operations possible at that level. approaches to determining content validity involve the use of exploratory factor analysis and other multivariate statistical procedures.                                                                                                   | in a diagram) IEEE_Soft_Vo cab the                                                           | interrelated conditions in which something exists or occurs.                                                                                                                                                                                                                                                                                                                                                                                                      | Merriam- Webster_cont ext                  |                                                                                                                                                                  |                                               |\n| context contextual                                                                                         | The context is the circumstances, purpose, and perspective under which an object is defined or used. OECD A computing system with sufficient knowledge regarding its purpose that it understands the source, relevance, and utility of data and inputs. The Context of Use is the actual conditions under which a given artifact/software product is used, or will be used in a normal day to day situation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | The immediate environment in which a function (or set of operates comprises a combination of users, goals, tasks, resources, and the technical, physical and social, cultural and organizational environments in which a system, product or service is used[; ...] can include the interactions and interdependencies between the object of interest and other systems, products or 2018                                                                                                                                                                                                                                                                                                                                        | functions ISO_9241-11Ê¼                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                            |                                                                                                                                                                  |                                               |\n| learning context-of-use                                                                                    | working                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | services.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                            |                                                                                                                                                                  | data;                                         |\n|                                                                                                            | IEEE_Guide_I PA interaction_co ntext_2023 property of a system that allows a human or another external agent to intervene in the system's functioning; such a system is heteronomous. ISO/IEC_TS_                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                            |                                                                                                                                                                  | personal processor                            |\n| controllability control class controller                                                                   | (control group) the set of observations in an experiment or prospective study that do not receive the experimental treatment(s). These observations serve (a) as a comparison point to evaluate the magnitude and significance of each experimental treatment, (b) as a reality check to compare the current observations with previous observation history, and (c) as a source of data for establishing the natural experimental error. 'Controller' means the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                            |                                                                                                                                                                  |                                               |\n|                                                                                                            | 5723Ê¼2022(en) _2012 GDPR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                            |                                                                                                                                                                  |                                               |\n|                                                                                                            | nist_statistics processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                            |                                                                                                                                                                  |                                               |\n\n| Terms Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Citation 2 Definition                  | Citation 3 Definition 4   | Citation 5 Related terms and synonyms Legal definition      |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------|---------------------------|-------------------------------------------------------------|\n| copilot An artificial intelligence powered software program designed to assist users various tasks and automate features within compatible applications using                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | with A product or service that provides assistance using, incorporating and/or based on artificial intelligence software and artificial intelligence software services                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                        |                           | [2] applicable                                              |\n| interfaces to understand user requests and provide suggestions, summaries, and content generation in response. corpus (corpora) A deliberately assembled collection of knowledge and data (structured and/or unstructured) believed to contain relevant information on a topic or topics to used by software systems for which useful analysis, prediction, or outcome is being sought. correlation In its most general sense correlation denoted the interdependence between quantitative or qualitative data. In this sense it would include the                                                                                                                                                                            | be IEEE_Guide_I PA association of OECD The correlation coefficient of two random variables y_1, and y_2, denoted \\rho                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | box_statistics _2005                   |                           |                                                             |\n| dichotomised attributes and the contingency of multiply-classified attributes. counterfactual explanation Statements taking the form: Score p was returned because variables V had values (v1, v2,...) associated with them. If V instead had values (v1' , v2',...) score p' would have been returned. counterfactual fairness A fairnessmetric that checks whether a classifier produces the same result for one individualas it does for another individual who is identical to the first, withrespect to one or more sensitive attributes. Evaluating a classifier for                                                                                                                                                    | (y_1,y_2) is: \\rho(y_1, y_2) = Cov(y_1, y_2)/\\sqrt{Var(y_1)*Var(y_2)} wachter_coun terfactual_201 8 aime_measure Given a predictive problem with fairness considerations, where A, X and Y                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | kusner_counte rfactual_2017            |                           |                                                             |\n| counterfactualfairness is one method for surfacing potential sources of bias in a model countermeasure Actions, devices, procedures, techniques, or other measures that                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | except ment_2022, citing Machine Learning Glossary by Google represent the protectedattributes, remaining attributes, and output of interest respectively, let us assume that we are given acausal model (U; V; F), where V = A \\cup X. We postulate the following criterion for predictors of Y .Definition 5 (Counterfactual fairness). Predictor ^Y is counterfactually fair if under any context X = x and A = a, P( ^Y_{A <- a} (U) = y &#124; X = x; A = a) = P( ^Y_{A <- a')(U) = y &#124; X = x;A = a); (1) for all y and for any value a' attainable by A. reduce the SP800-37 Actions, devices, procedures, or techniques that meet or oppose (i.e., counters) a                                                               | GWUC                                   |                           |                                                             |\n| vulnerability of a system. Synonymous with security controls and safeguards. criterion validity compares responses to future performance or to those obtained from other, more well-established surveys. Criterion validity is made up two subcategories: predictive and concurrent. Predictive validity refers to the extent to which a survey measure forecasts future performance. A graduate school entry examination that predicts who will do well in graduate school has predictive validity. Concurrent validity is demonstrated when two assessments agree new measure is compared favorably with one that is already considered valid.                                                                              | threat, a vulnerability, or an attack by eliminating or preventing it, by the harm it can cause, or by discovering and reporting it so that corrective action can be taken. fink_survey_2 010 an index of how well a test correlates with an established standard of comparison (i.e., a criterion). Criterion validity is divided into three types: predictive validity, concurrent validity, and retrospective validity. For example, if a measure of criminal behavior is valid, then it should be possible to use it to predict whether an individual (a) will be arrested in the future for a criminal violation, (b) is currently breaking the law, and (c) has a previous criminal record. _validity                              | minimizing APA_criterion               |                           | safeguard; security control                                 |\n| or crowdsource a type of participative online activity in which an individual, an institution, a profit organization, or company proposes to a group of individuals of varying knowledge, heterogeneity, and number, via a flexible open call, the voluntary undertaking of a task. The beneficiary of the execution of an automated task, process, or service. Prevention of damage to, protection of, and restoration of computers, electronic communications systems, electronic communications services, wire communication, including information contained                                                                                                                                                              | a non- Enrique IEEE_Guide_I                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                        |                           | criterion- referenced validity; criterion- related validity |\n| customer cybersecurity communication, and electronic therein, to ensure its availability, integrity, authentication, confidentiality, and nonrepudiation. dark pattern 'Dark pattern' means a user interface designed or manipulated with the substantial effect of subverting or impairing user autonomy, decisionmaking,                                                                                                                                                                                                                                                                                                                                                                                                    | PA SP800-37 or CCPA OECD re-interpretable representation of information ina formalized manner suitable for communication, interpretation or processing                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | aime_measure ment_2022, citing ISO/IEC |                           |                                                             |\n| choice, as further defined by regulation. data Characteristics or information, usually numerical, that are collected through observation. data analytics the process of applying graphical, statistical, or quantitative techniques to a observations or measurements in order to summarize it or to find general patterns. data cleaning Data Cleaning is the process of identifying, correcting, or removing inaccurate corrupt data records data control management oversight of information policies for an organization's information; observing and reporting on how processes are working and managing issues.                                                                                                         | set of APA_data_ana lysis Data analysis is the process of transforming raw data into usable information, often presented in the form of a published analytical article, in order to add value to the statistical output. or Ranschaert, _Erik Egnyte                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | TR 24029-1 OECD                        |                           |                                                             |\n| the analysis of data rather than purely on intuition. data fabric A data corpus, after the application of semantic mapping, relevant ontologies, data seeding sufficient for artificial intelligence (AI) or machine learning algorithms to provide meaningful insight, prediction, and/or data fusion A process in which data, generated by multiple sensory sources, is and/or correlated to create information, knowledge, and/or may to accomplish the tasks. data governance A are formally the authority and the data produced or data mining                                                                                                                                                                           | 2013 and prescription. IEEE_Guide_I PA integrated intelligence that SP1011 The process of combining data from multiple sources to produce more accurate, consistent, and concise information than that provided by any individual data source. by the CSRC refers to a system, including policies, people, practices, and technologies, necessary to ensure data management within an organization from aime_measure ment_2022 citinig ISO/IEC the process of data analysis and information extraction from large amounts of datasets with machine learning, statistical approaches. and many others.                                                                                                                                    | Munir,_Arslan NIST_1500                |                           |                                                             |\n| be displayed for user or be actionable set of processes that ensures that data assets enterprise. A data governance model establishes and decision making parameters related to enterprise. computational process that extracts patternsby analysing different perspectives and dimensions, categorizingthem, and potential relationships and impacts data point a discrete unit of information. data preparation We define data preparation as the set of preprocessing operations performed early stages of a data processing pipeline, i.e., data transformations at the structural and syntactical levels proxy Data that are closely related to and serve in place of data that are either unobservable or immeasurable. | managed throughout management managed quantitative data summarizing 22989 TechTarget_da ta_point in hameed_data_ 2020 Comptroller_O ffice                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Ranschaert, _Erik                      |                           |                                                             |\n| data data quality degree to which the characteristics of data satisfy stated and implied needs used under specified conditions data science Methodology for the synthesis of useful knowledge directly from data through process of discovery or of hypothesis formulation and hypothesis testing. data scientist data seeding                                                                                                                                                                                                                                                                                                                                                                                                | when IEEE_Soft_Vo cab The dimensions of the IMF definition of \"data quality\" are: - integrity; - methodological soundness; - accuracy and reliability; - serviceability; - accessibility. There are a number of prerequisites for quality. These comprise: - legal and institutional environment; - resources; - quality awareness. a NIST_1500 Interdisciplinary science that uses statistics, algorithms, and other methods to extract meaningful and useful patterns from data sets-sometimes known as 'big data.' Today, machine learning is often used in this field. Next to analysis of data, data science is also concerned with the capturing, preparation, and interpretation of data. AI_Ethics_Mar NIST_1500 IEEE_Guide_I PA | OECD                                   |                           |                                                             |\n| A practitioner who has sufficient knowledge in the overlapping regimes of business needs, domain knowledge, analytical skills, and software and systems engineering to manage the end-to-end data processes in the analytics life The intentional introduction of initial state conditions, influencing factors, outcomes (both successful and unsuccessful) in a data fabric to create sufficient data wrangling                                                                                                                                                                                                                                                                                                             | cycle. and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | k_Coeckelberg h                        |                           | artificial intelligence (AI); machine learning (ML)         |\n| machine learning analysis signals to enable encouragement/discouragement enrich deterministic relationships between data elements in a given information domain. process by which the data required by an application is identified, extracted, cleaned and integrated, to yield a data set that is suitable for exploration and analysis. decision decision point                                                                                                                                                                                                                                                                                                                                                            | to Furche,_Tim IEEE_Guide_I Types of statements in which a choice between two or more possible outcomes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | IEEE_Soft_Vo cab APA_decision_ making  |                           |                                                             |\n| reached after consideration of business rules and relevant data given process. within a business process where the process flow can take one of decision tree                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | PA controls which set of actions will result. several PA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                        |                           |                                                             |\n| A conclusion within a A point where every node represents a test the possible outcomes of that test, and the leaves decision-making the among                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | to an                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                        |                           |                                                             |\n| alternative paths, including recursive. Tree-structure resembling a flowchart, attribute, each branch represents represent the class labels. cognitive process several possible irrational. The assumptions of values,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | IEEE_Guide_I Reznik,_Leon the cognitive process of choosing between two or more alternatives, ranging from the relatively clear cut (e.g., ordering a meal at a restaurant) to the complex (e.g., selecting a mate). Psychologists have adopted two converging strategies to                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                        |                           |                                                             |\n| a course of action rational or                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | understand decision making: (a) statistical analysis of multiple decisions involving complex tasks and (b) experimental manipulation of simple decisions, looking at                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                        |                           |                                                             |\n| Every or may not prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | the elements that recur within these decisions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                        |                           |                                                             |\n| resulting in the selection of a belief or alternative options. It could be either decision-making process is a reasoning process based on preferences and beliefs of the decision-maker. decision-making process produces a final choice, which may action.                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Wikipedia_Dec ision-making                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                        |                           |                                                             |\n\n| Terms Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                           | Citation 2 Definition 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Citation 3 Definition 4   | Citation 4 Definition 5                                                                                                                                                                                                                                                                                                                                                                     | Citation 5              | Related terms and synonyms Legal definition                                         | Related terms and synonyms Legal definition                                         | Related terms and synonyms Legal definition                                         |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|\n| a computer program application used to improve a company's decision-making capabilities. It analyzes large amounts of data and presents an organization with the best possible options available[; they] bring together data and knowledge from different areas and sources to provide users with information beyond the usual reports and summaries. This is intended to help people make informed total or partial removal of existing components and their corresponding sub- from Production and any relevant environment, minimizing risks impacts, ensuring policy compliance, and maximizing the financial benefits (i. IG1190M_AIOp s_Decommissi on_v1.0.0                                                                                                                                                                                                                                                                                                                                                            | TechTarget_d ecision_suppo rt_system                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           | [an approach to AI that allows] computers to learn from experience and                                                                                                                                                                                                                                                                                                                      | [2] deductive reasoning | decision support system decommission                                                | decision support system decommission                                                | decision support system decommission                                                |\n| decisions. the components and e., optimizing the cost reduction). Insights, reporting, and information answering the question, \"What would likely happen IFâ€¦?' Deductive analytics evaluates causes and outcomes of possible future events. IEEE_Guide_I PA Deep learning is a broad family of techniques for machine learning in which hypotheses take the form of complex algebraic circuits with tunable connection strengths. The word 'deep' refers to the fact that the circuits are typically organized into many layers, which means that computation paths from inputs to outputs have many steps. Deep learning is currently the most widely used approach for applications such as visual object recognition, machine translation,                                                                                                                                                                                                                                                                                 | Russell_and_N orvig A form of machine learning that uses neural networks with several layers of \"neurons\": simple interconnected processing units that interact.                                                                                                                                                                                                                                                                                                                                                                             | AI_Ethics_Mar k_Coeckelberg h                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           | understand the world in terms of a hierarchy of concepts, with each concept defined through its relation to simpler concepts. By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all the knowledge that the computer needs. The hierarchy of concepts enables the computer to learn complicated concepts by building them out of | deeplearningb ook_intro | deductive analytics deep learning                                                   | deductive analytics deep learning                                                   | deductive analytics deep learning                                                   |\n| operations. (Time-critical may be milliseconds or it maybe hours, depending upon the service provided). <of an item> ability to perform as and when required (note 1Ê¼ includes availability, reliability, recoverability, maintainability, and maintenance support performance, and, in some cases, other characteristics such as durability, safety and security. Note 2Ê¼ used as a collective term for the time-related quality characteristics of an item). Phase of a project in which a system is put into operation and cutover issues are resolved IEEE_Soft_Vo cab Insights, reporting, and information answering the question, 'Why did something happen?' Descriptive analytics determines information useful to understanding cause(s) of an event(s). IEEE_Guide_I PA [that] produces consistent outcomes for a given set of inputs, of how many times the model is recalculated. The mathematical Sourabh_Meht a_deterministi                                                                                    | resources or services. ISO/IEC_TS_ 5723Ê¼2022(en)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                           | other network resources due to the actions of a malicious cyber threat actor. Services affected may include email, websites, online accounts (e.g., banking), or other services that rely on the affected computer or network. A denial-of-service condition is accomplished by flooding the targeted host or network with traffic until the target cannot respond or simply crashes, preventing access for legitimate users. DoS attacks can cost an organization both time and money while their resources and services are inaccessible. |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | dependability deployment descriptive analytics deterministic deterministic          | dependability deployment descriptive analytics deterministic deterministic          | dependability deployment descriptive analytics deterministic deterministic          |\n| the modelling regardless characteristics are known in this case. None of them is random, and each problem has just one set of specified values as well as one answer or solution. The unknown components in a deterministic model are external to the model. It deals with the definitive outcomes as opposed to random results and doesn't make allowances for error. c algorithm An algorithm that, given the same inputs, always produces the same outputs. CSRC A general term that includes developers or manufacturers of systems, system components, or system services; systems integrators; vendors; and product resellers. Development of systems, components, or services can occur internally within organizations or through external entities. SP800-37 Insights, reporting, and information answering the question, 'Why did something happen?' Diagnostic analytics determines information useful to understanding the cause(s) of an event(s). PA _Vocab privacy- hnologies                                  | Individual or organization that performs development requirements analysis, design, testing through software life-cycle process. IEEE_Guide_I IEEE_Software enhancing_tec For two datasets D and D' that differ in at most one element, a randomized algorithm $M$ guarantees \\emph{$(\\epsilon, \\delta)$-differential privacy} any subset of the output $S$ if $M$ satisfies: \\begin{equation} Pr[M(D) \\in S] \\leq exp(\\epsilon)*Pr[M(D') \\in S] + \\delta \\end{equation} Furthermore, when $\\delta = 0$ an {$\\epsilon$-differential privacy} | activities (including acceptance) during the system or IEEE_Soft_Vo cab for algorithm Mis said to guarantee \\emph gong_different ial_2020 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | developer diagnostic analytics diagnostics differential privacy                     | developer diagnostic analytics diagnostics differential privacy                     | developer diagnostic analytics diagnostics differential privacy                     |\n| Pertaining to the detection and isolation of faults or failures Differential privacy is a method for measuring how much information the output of a computation reveals about an individual. It is based on the randomised injection of \"noise\". Noise is a random alteration of data in a dataset so that values such as direct or indirect identifiers of individuals are harder to reveal. An important aspect of differential privacy is the concept of 'epsilon' or É› , which determines the level of added noise. Epsilon is also known as the 'privacy budget' or 'privacy parameter'. Differential validity states that the validities in two applicant populations are unequal, that is, pi != pa. hunter_differe ntial_1979 Digital automation of information technology systems and/or business processes that successfully delivers work output previously performed by human labor or new work output that would typically or alternatively have been performed by human labor. IEEE_Guide_I PA PA _2022 Shalev- | IEEE_Guide_I wolfram_math Distinct components that a multidimensional construct encompasses                                                                                                                                                                                                                                                                                                                                                                                                                                                  | IEEE_Soft_Vo cab                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | differential validity digital labor digital workforce dimension dimension reduction | differential validity digital labor digital workforce dimension dimension reduction | differential validity digital labor digital workforce dimension dimension reduction |\n| The collective suite of automation technologies delivering existing or new work output as applied in a business; the manifestation of digital labor. The dimension of an object is a topological measure of the size of its covering properties. Roughly speaking, it is the number of coordinates needed to specify a point on the object. Dimensionality reduction is the process of taking data in a high dimensional space and mapping it into a new space whose dimensionality is much smaller For Predictor Y and Sensitive Impact S. Definition 6.2 Disparate Impact (DI) = P[YË† = 1 &#124; S != 1]/P[YË† = 1 &#124; S = 1] Intentional discrimination, including (i) decisions explicitly based on protected characteristics; and (ii) intentional discrimination via proxy variables (e.g literacy tests for voting eligibility). Lipton, Optimizing the predictive accuracy for a whole class of distributions instead of just a single target distribution.                                                         | Shwartz,_Shai friedler_comp arative_2019                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                           | <artificial intelligence> specific field of knowledgeor expertise                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | ment_2022, ISO/IEC        |                                                                                                                                                                                                                                                                                                                                                                                             |                         | disparate impact disparate treatment distributional robustness diversity inclusion  | disparate impact disparate treatment distributional robustness diversity inclusion  | disparate impact disparate treatment distributional robustness diversity inclusion  |\n| the practice of including the many communities, identities, races, ethnicities, backgrounds, abilities, cultures, and beliefs of the American people, including underserved communities. 1 Collection of documents on a given subject; written or pictorial information describing, defining, specifying, reporting, or certifying activities, requirements, procedures, or results. cab Distinct scope, within which common characteristics are exhibited, common rules observed, and over which a distribution transparency is preserved. cab                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | commonality combinations of: (1) roles supported, (2) rules governing their use, and (3) protection needs. McCue_Collee                                                                                                                                                                                                                                                                                                                                                                                                                      | in SP800-160                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | citing 2382               |                                                                                                                                                                                                                                                                                                                                                                                             |                         | documentation domain                                                                | documentation domain                                                                | documentation domain                                                                |\n| the conditions present at the time of execution. PA a problem or situation, especially in computer programming, that only happens at the highest or lowest end of a range of possible values or in extreme situations: tionary_2022 An embedding is a representation of a topological object, manifold, graph, field, etc. in a certain space in such a way that its connectivity or algebraic properties are preserved. For example, a field embedding preserves the algebraic structure of plus and times, an embedding of a topological space preserves open sets, and a graph embedding preserves connectivity. One space X is embedded in another space Y when the properties of Y restricted _2022                                                                                                                                                                                                                                                                                                                      | ding_2022                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | shift dogfooding, eating your own dogfood embedding                                 | shift dogfooding, eating your own dogfood embedding                                 | shift dogfooding, eating your own dogfood embedding                                 |\n| cambridge_dic wolfram_math                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         |                                                                                     |                                                                                     |                                                                                     |\n| to X are the same as the properties of X.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         |                                                                                     |                                                                                     |                                                                                     |\n| The use of a data processing system to imitate another data processing system, so that the imitating system accepts the same data, executes the same programs, and achieves the same results as the imitated system. IEEE_Soft_Vo cab An activity, task, or output that describes or defines the conclusion of a process. IEEE_Guide_I PA n. 3a: a designer or builder of engines; b: a person who is trained in or follows as a profession a branch of engineering; c: a person who carries through an enterprise Merriam- Webster_engi                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         |                                                                                     |                                                                                     |                                                                                     |\n| by skillful or artful contrivance; 4Ê¼ a person who runs or supervises an engine or an apparatus. neer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         |                                                                                     |                                                                                     |                                                                                     |\n| engineer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         |                                                                                     |                                                                                     |                                                                                     |\n| The practice in which tech workers use their own product consistently to see                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | drinking your own                                                                   | drinking your own                                                                   | drinking your own                                                                   |\n| Domain expertise implies knowledge and understanding of the essential aspects of a specific field of inquiry. n Differences between the source and target domain data                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | _Zachary Meinshausen, _Nicolai EO_DEIA_202                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | domain expertise domain shift                                                       | domain expertise domain shift                                                       | domain expertise domain shift                                                       |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | IEEE_Soft_Vo A set of elements, data, resources, and functions that share a                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         |                                                                                     |                                                                                     |                                                                                     |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | IEEE_Soft_Vo Stacke,_Karin                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | distributional                                                                      | distributional                                                                      | distributional                                                                      |\n| how well it works and where improvements can be made.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | kelley_dogfoo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | champagne                                                                           | champagne                                                                           | champagne                                                                           |\n| The process in which one or more paths are defined and may be utilized based on                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | IEEE_Guide_I                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | dynamic process                                                                     | dynamic process                                                                     | dynamic process                                                                     |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | edge case                                                                           | edge case                                                                           | edge case                                                                           |\n| event                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         |                                                                                     |                                                                                     |                                                                                     |\n| emulation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         |                                                                                     |                                                                                     |                                                                                     |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         | end engineer                                                                        |                                                                                     |                                                                                     |\n| v. 1Ê¼ to lay out, construct, or manage as an                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         |                                                                                     |                                                                                     |                                                                                     |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                           | aime_measure                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                           |                                                                                                                                                                                                                                                                                                                                                                                             |                         |                                                                                     |                                                                                     |                                                                                     |\n\n<!-- image -->\n\n|                                                       | Terms Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Citation 2                                                                | Definition 3                                                                                                                                                                                              | Citation 3              | Definition 4 Citation 4                                                                                                                                         | Definition 5 Citation 5                                                                                                                                                                                                                                                                                                                                                                                                                                 | Related terms and synonyms Legal definition          |\n|-------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|\n| ensemble environment equality of opportunity error    | a machine learning paradigm where multiple models (often called 'weak learners') are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models. ethods to on Y. with respect to A and Y if Pr{bY = 1 &#124; A = 0; Y = 1} = Pr{bY = 1 &#124; A = 1; Y = 1}. _2016 The difference between the observed value of an index and its 'true' value. Errors maybe random or systematic. Random errors are generally referred to as 'errors'. Systematic errors are called 'biases'. OECD                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Joseph_Rocca _Ensemble_m words, the protected and unprotected groups should have equal true positive rates. Difference between a computed, observed, or measured value or condition the true, specified, or theoretically correct value or condition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | and                                                                       | quantity value minus a reference quantity value                                                                                                                                                           | citing ISO/IEC Guide 99 |                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                         | [2] applicable                                       |\n| equality of odds error propagation ethics             | Anything affecting a subject system or affected by a subject system through interactions with it, or anything sharing an interpretation of interactions with a subject system IEEE_Soft_Vo cab (Equalized odds). We say that a predictor bY satisfies equalized odds with respect protected attribute A and outcome Y, if bY and A are independent conditional _2016 (Equal opportunity). We say that a binary predictor bY satisfies equal opportunity the way in which uncertainties in the variables affect the uncertainty in the calculated results. Merriam- An approach to technology ethics and a key component of responsible innovation that aims to integrate ethics in the design and development stage of the technology. Sometimes formulated as \"embedding values in design.\" Similar h citing 24765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | hardt_equality The probability of a person in the positive class being correctly assigned a positive outcome and the probability of a person in a negative class being incorrectly assigned a positive outcome should both be the same for the protected and unprotected group members. In other words, the protected and unprotected groups should have equal rates for true positives and false positives. Mehrabi, _Ninareh hardt_equality The probability of a person in positive class being assigned to a positive outcome should be equal for both protected and unprotected group members. In other Mehrabi, _Ninareh Dorf_2018 that investigators report results and findings honestly. See code of ethics; professional ethics; research ethics. -ethical adj. AI_Ethics_Mar k_Coeckelberg aime_measrue ment_2022, ISO/IEC                                                                                                                                                                                                                                                                                                                                                                                                                               | IEEE_Soft_Vo cab                                                          | measured aime_measure ment_2022,                                                                                                                                                                          |                         | system that provides for expertly solving problems in a given field or area by drawing inferences from a knowledge base developed from IEEE_Soft_Vo cab         | ability of a human expert of domain-specific if-then rules rather than NSCAI                                                                                                                                                                                                                                                                                                                                                                            | propgation of uncertainty; proprgation of error      |\n| ethics by design evaluation example exception execute | definition 1a: \"a set of moral principles : a theory or system of moral values\"; definition 1b: \"the principles of conduct governing an individual or a group\"; definition 1c: \"a consciousness of moral importance\"; definition 1d: \"a guiding philosophy\"; definition 2Ê¼ \"a set of moral issues or aspects (such as rightness)\"; definition 3Ê¼ \"the discipline dealing with what is good and bad and with moral duty and obligation\" Merriam- ple An event that occurs during the performance of the process that causes a diversion from the normal flow of the process. Exceptions are generated by an unanticipated event within a process due to an undefined or unknown input, undefined or unexpected outcome, or unforeseen sequencing of a task or event. PA To carry out a plan, a task command, or another instruction SP1011                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Webster_ethic n. 1. the branch of philosophy that investigates both the content of moral judgments (i.e., what is right and what is wrong) and their nature (i.e., whether such judgments should be considered objective or subjective). The study of the first type of question is sometimes termed normative ethics and that of the second metaethics. Also called moral philosophy. 2. the principles of morally right conduct accepted by a person or a group or considered appropriate to a specific field. In psychological research, for example, proper ethics requires that participants be treated fairly and without harm and APA_ethics Webster_exam IEEE_Guide_I To carry out an instruction, process, or computer program; directing,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | managing, IEEE_Soft_Vo                                                    | An expert system is an intelligent computer program that uses knowledge and inference procedures to solve problems that are difficult enough to require significant human expertise for their solution.   | OECD                    | Computer application human expertise.                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Test, Evaluation, Verification and Validation (TEVV) |\n| experiment expert system                              | terms are \"value-sensitive design\" and \"ethically aligned design.\" (1) systematic determination of the extent to which an entity meets its specified criteria; (2) action that assesses the value of something definition 1Ê¼ \"one that serves as a pattern to be imitated or not to be imitated\"; definition 3Ê¼ \"one that is representative of all of a group or type\"; definition 4Ê¼ \"a parallel or closely similar case especially when serving as a precedent or model\"; definition 5Ê¼ \"an instance (such as a problem to be solved) serving to illustrate a rule or precept or to act as an exercise in the application of a rule\" a series of observations conducted under controlled conditions to study a relationship with the purpose of drawing causal inferences about that relationship. An experiment involves the manipulation of an independent variable, the measurement of a dependent variable, and the exposure of various participants to one or more of the conditions being studied. Random selection of participants and their random assignment to conditions also are necessary in experiments. nt_2023 A form of AI that attempts to replicate a human's expertise in an area, such as medical diagnosis. It combines a knowledge base with a set of hand-coded rules for applying that knowledge. Machine-learning techniques are increasingly Hutson, | performing, and accomplishing the project work, providing the deliverables, providing work performance information. apa_experime A study of a fundamental physical process by the use of one or more simulators. Like empirical experiments, input variables (factors) are systematically changed to assess their impact upon simulator outputs (responses). Unlike empirical experiments, the simulator responses are deterministic, and this has implications: Computer experiments can appropriately have their factors with intermediate levels and the scope, especially the number of runs, can be more ambitious. Further, modeling methods based on interpolators (especially kriging) emerge as a viable Good practice is to use Latin hypercubes for computer experiments, and advanced nonparametric modeling methods such as kriging, neural networks, and multivariate adaptive regression splines (MARS) in the data analysis stage. Important applications of computer experimental methods are for process optima and for evaluating process tolerances. _Matthew Intelligent computer program that uses knowledge and inference procedures to solve problems that are difficult enough to require significant human expertise for their solution. | and cab computer approach. determining nist_statistics _2012 Reznik,_Leon | characteristic of an AI system in which there is provision of accompanying or reasons for system output in a manner that is meaningful or to individual users (as well as to developers and auditors) and |                         |                                                                                                                                                                 | computer system emulating the decision-making through the use of reasoning, leveraging an encoding knowledge most commonly represented by sets of procedural code. The term 'expert system' was used largely during the 1970s and amidst great enthusiasm about the power and promise of rule-based systems relied on a 'knowledge base' of domain-specific rules and rule-chaining procedures that map observations to conclusions or recommendations. |                                                      |\n| expertise explainability                              | replacing hand coding. The accumulation of specialized knowledge is often called expertise . Passive expertise is a type of knowledge-based specialization that arises from experiences in life and one's position in a society or culture. Formal expertise is the result of a self-selection of a domain of knowledge that is mastered deliberately and for which there are clear benchmarks of success. The ability to provide a human interpretable explanation for a machine learning prediction and produce insights about the causes of decisions, potentially to line up with human reasoning. Draft                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Schneider_Mc Grew_in_Flan agan_McDono ugh_2018 NISTIR_8269_ Within the context of AI, the extent to which AI decisioning processes and outcomes are reasonably understood.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Comptroller_O ffice                                                       | A evidence understandable reflects the system's process for generating the output (e.g., what alternatives considered, but not proposed, and why not).                                                    | NSCAI                   |                                                                                                                                                                 | '80s that                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                      |\n| explainer explanation exploratory                     | Functionality for providing details on or causes for fairness metric results. 60 Systems deliver accompanying evidence or reason(s) for all outputs. Draft Exploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to 1. maximize insight into a data set; 2. uncover underlying structure; 3. extract important variables; 4. detect outliers and anomalies; 5. test underlying assumptions; 6. develop parsimonious models; and 7. determine optimal factor settings. the extent to which the results of research or testing can be generalized beyond                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | AI_Fairness_3 NISTIR_8269_ The explanation principle obligates AI systems to supply evidence, support, or reasoning for each output. nist_statistics _2012 APA_external_ validity                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | NISTIR_8312                                                               | were                                                                                                                                                                                                      |                         |                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                         | interpretability                                     |\n| external validity facial recognition                  | the sample that generated them. The more specialized the sample, the less likely will it be that the results are highly generalizable to other individuals, situations, and time periods. (FR) Face recognition algorithms, however, have no built-in notion of a particular person. They are not built to identify particular people; instead they include a face detector followed by a feature extraction algorithm that converts one or more images of a person into a vector of values that relate to the identity of the person. The extractor typically consists of a neural network that has been trained on ID-labeled images available to the developer. In operations, they act as generic extractors of identity-related information from photos of persons they have usually never seen before. Recognition proceeds as a differential operator: Algorithms compare two feature vectors and emit a similarity score. This is a vendor-defined numeric value expressing how similar the parent faces are. It is compared to a threshold value to decide whether two samples are from, or represent, the same person or not. Thus, recognition is mediated by identity information stored in a feature vector (or 'template'). A quantification of unwanted bias in training data or models.                                                                           | NISTIR_8280 AI_Fairness_3 A mathematical definition of 'fairness' that is measurable. Some commonly used fairness metrics include: equalized odds predictive parity counterfactual fairness demographic parity Many fairness metrics are mutually exclusive; see incompatibility of fairness metrics. an outcome where the model incorrectly predicts the negative class.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | google_glossar y_2023                                                     |                                                                                                                                                                                                           | Varshney,               |                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Type II error (in statistics)                        |\n| fairness metric                                       | persistent 60                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                           | false negative is denying an applicant who should be approved                                                                                                                                             |                         | 1. An instance in which a security tool intended to detect a particular threat fails to do so. 2. Incorrectly classifying malicious activity as benign. egative |                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                      |\n| false negative                                        | An example in which the predictive model mistakenly classifies an item as in the negative class. NSCAI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | assification- true-false-                                                 | google_dev_cl A                                                                                                                                                                                           | _Kush                   | CSRC_false_n                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                      |\n\n<!-- image -->\n\n| Terms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Citation 1 [1]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Definition 2                                                                                                                                                | Citation 2 Definition 3                                                                                                                                                                                                                                                                                                                                                     | Citation 3 Definition 4 Definition 5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Citation 4           | Related terms and synonyms Legal definition   |     |                 | Citation   | 5   |              |                                                                                       |      |\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------|-----------------------------------------------|-----|-----------------|------------|-----|--------------|---------------------------------------------------------------------------------------|------|\n| false positive                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | An example in which the model mistakenly classifies an item as in the positive class NSCAI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | an outcome where the model incorrectly predicts the positive                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | class. google_dev_cl assification- true-false- positive- negative                                                                                           | A false positive is approving an applicant who should be denied                                                                                                                                                                                                                                                                                                             | Varshney, _Kush 1. An alert that incorrectly indicates that a vulnerability is present. 2. An alert that incorrectly indicates that malicious activity is occurring. 3. An instance in which a security tool incorrectly classifies benign content as malicious. 4. Incorrectly classifying benign activity as malicious. 5. An erroneous acceptance of the hypothesis that a statistically significant event has been observed. This is also referred to as a type 1 error. This is also referred to as a type 1 error. When 'health-testing' the components of a device, it often refers to a declaration that a component has malfunctioned - based on some statistical test(s) - despite the fact that the component was actually working correctly. | CSRC_false_p ositive | applicable Type I error (in statistics)       | [2] |                 |            |     |              |                                                                                       |      |\n| fault tolerance favorable label feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | The ability of a system or component to continue normal operation despite the presence of hardware or software faults SP1011 A label whose value corresponds to an outcome that provides an advantage to the recipient. The opposite is an unfavorable label. AI_Fairness_3 60 An attribute containing information for predicting the label. 60 a more general method in which one tries to develop a transformation of the khalid_feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | AI_Fairness_3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              | feature extraction                                                                    |      |\n| feature importance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | input space onto the lowdimensional subspace that preserves most of the relevant information _2014 how important the feature was for the classification performance of the model; a measure of the individual contribution of the corresponding feature for a particular classifier, regardless of the shape (e.g., linear or nonlinear relationship) or direction of the feature effect saarela_featur e_2021 Unlike joint distribution shift detection, which cannot localize which features caused the shift, we define a new hypothesis test for each feature individually. NaÃ¯vely, the simplest test would be to check if the marginal distributions have changed for each feature (as explored by [25]); however, the marginal distribution would be easy for an adversary to simulate (e.g., by looping the sensor values from a previous day). Thus, marginal tests are not sufficient for our purpose. Therefore, we propose to use conditional distribution tests. More formally, our null and alternative hypothesis for the j-th feature is that its full conditional                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              |                                                                                       |      |\n| feature shift learning learning systems have sharing parameters. describes the process of end-user actions in order generated output (predictions final decision (for example, model, allowing it to learn Fitting is the process of specified interval. Computer programs and (ROM) or programmable data cannot be dynamically programs. Estimate or prediction of information and knowledge is based on the project's includes information that at completion and estimate Monitoring the behavior avoid undesirable behavior. Accomplishes its assigned intervention while adapting | distribution (i.e., its distribution given all other features) has not shifted for all values of the other features. kulinski_featur e_2020 An approach to machine learning which addresses problems of data governance and privacy by training algorithms collaboratively without transferring the data to a central location. Each federated device trains on data locally and shares its local model parameters instead of sharing the training data. Different federated different topologies that involve different ways of TTC6_Taxono my_Terminolo gy leveraging the output of an AI system and corresponding to retrain and improve models over time. The AI- or recommendations) are compared against the to perform work or not) and provides feedback to the from its mistakes. C3. ai_feedback_l oop verifying whether the data item value is in the previously OECD data stored in hardware - typically in read-only memory read-only memory (PROM) - such that the programs and written or modified during execution of the SP800-37 conditions and events in the project's future based on available at the time of the forecast. The information past performance and expected future performance, and could impact the project in the future, such as estimate to complete. IEEE_Soft_Vo cab of populations of users in order to estimate, detect, or Kou,_Yufeng mission, within a defined scope, without human to operational and environmental conditions SP1011 | Combination of a hardware device and computer instructions or computer data that reside as read only software on the hardware device. detecting and recognizing fraudulent activities them to a system manager.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | IEEE_Soft_Vo cab as they enter systems and report Behdad                                                                                                    | _Matthew Generative adversarial networks (GANs) consist of two competing neural networks-a generator network that tries to create fake outputs (such as pictures), and a discriminator network that tries to determine whether the outputs are real or fake. A major advantage of this structure is that GANs can learn from less data than other deep learning algorithms. | An approach to training AI models useful for applications like data synthesis, augmentation, and compression where two neural networks are trained in tandem: one is designed to be a generative network (the forger) and the other a discriminative network (the forgery detector). The objective is for each network to train and better itself off the other, reducing the need for big labeled training data.                                                                                                                                                                                                                                                                                                                                        | NSCAI                | closed-loop learning                          |     |                 |            |     |              | federated feedback loop fitting firmware Forecasting fraud detection fully autonomous |      |\n| adversarial Generative Adversarial generative modeling using networks. Generative learning that involves patterns in input data in output new examples that global governance graph                                                                                                                                                                                                                                                                                                                                                                                                    | Networks, or GANs for short, are an approach to deep learning methods, such as convolutional neural modeling is an unsupervised learning task in machine automatically discovering and learning the regularities or such a way that the model can be used to generate or plausibly could have been drawn from the original dataset. Brownlee, _Jason A global explanation produces a model that approximates the non-interpretable model. Full The actions to ensure stakeholder needs, conditions, and options are evaluated to determine balanced, agreed-upon enterprise objectives; setting direction through prioritization and decision-making; and monitoring performance and ompliance against agreed-upon directions and objectives. AI governance may include policies on the nature of AI applications developed and deployed versus those limited or withheld. NSCAI Diagram that represents the variation of a variable in comparison with that of or more other variables. Diagram or other representation consisting of a set of nodes and internode connections called edges or arcs. cab                                                                                                                                                                                                                                                                                                                                                                            | A pair of jointly trained neural networks that improves through competition. One net creates say) as the other tries to detect the fakes. NISTIR_8312_ A system of laws, policies, frameworks, practices and processes at international, national and organizational levels. AI governance helps various stakeholders implement, manage, oversee and regulate the development, deployment and of AI technology. It also helps manage associated risks to ensure AI aligns with stakeholders' objectives, is developed and used responsibly and ethically, and complies with applicable legal and regulatory requirements. IEEE_Soft_Vo A graph (sometimes called an undirected graph to distinguish it from a directed graph, or a simple graph to distinguish it from a multigraph) is a pair G = (V, E), | generates realistic new data and new examples (fake Picassos, Hutson, use IAPP_Governa nce_Terms vertex), and E is a links or lines). wikipedia_gra ph_2023 |                                                                                                                                                                                                                                                                                                                                                                             | CRS_AI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                      |                                               |     |                 |            |     |              | generative network (GAN)                                                              |      |\n| graphical unit (GPU) ground group hacker                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | processing A specialized chip capable of highly parallel processing. GPUs are running machine learning and deep learning algorithms. GPUs were first developed for efficient parallel processing of arrays of values used in graphics. Modern-day GPUs are designed to be optimized for machine NSCAI information provided by direct observation as opposed to information by inference                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | where V is a set whose elements are called vertices (singular: set of paired vertices, whose elements are called edges (sometimes value of the target variable for a particular item of labelledinput data                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                             | aime_measure ISO/IEC                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              |                                                                                       |      |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | one finite well-suited for computer learning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Technically sophisticated computer enthusiast who uses his or her and means to gain unauthorized access to protected resources.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | ment_2022, citing 22989 knowledge cab                                                                                                                       | IEEE_Soft_Vo                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      | human-AI teaming                              |     |                 |            |     |              |                                                                                       |      |\n| hardware harm harmful                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | provided Collins_Dictio nary_ground_ truth The goal of groups defined by protected attributes receiving similar treatments or outcomes. AI_Fairness_3 60 Unauthorized user who attempts to or gains access to an information system. Reznik,_Leon Physical equipment used to process, store, or transmit computer programs or data IEEE_Soft_Vo cab An undesired outcome [whose] cost exceeds some threshold[; ...] the key points in the definition of safety are that: costs have to be sufficiently high in some human sense for events to be harmful, and that safety involves reducing both the probability of expected harms and the possibility of unexpected harms. Engineering_s ine_learning Harmful bias can be either conscious or unconscious. Unconscious, also known as implicit bias, involves associations outside conscious awareness that lead to a negative evaluation of a person on the basis of characteristics such as race, gender, sexual orientation, or physical ability. Discrimination is behavior; discriminatory actions perpetrated by individuals or institutions refer to inequitable treatment of members of certain social groups that results in social advantages or disadvantages The type of human-robot-interaction that that refers to situations during which human interactions are needed at the level of detail of task plans, i.e., during the execution of a task SP1011 Poore_Lawren                                               | afety_in_mach humphrey_add ressing_2020                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                             | Poore_Lawren ce_ARLIS_202                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              |                                                                                       |      |\n| human-enabled machine learning human-in-the-loop                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | The ability of humans and AI systems to work together to undertake complex, evolving tasks in a variety of environments with seamless handoff both ways between human and AI team members. Areas of effort include developing effective policies for controlling human and machine initiatives, computing methods that ideally complement people, methods that optimize goals of teamwork, and designs that enhance human-AI interaction. The need for human interaction in a normally fully autonomous behavior due                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | methods and approaches for coordinating the functions and actions of autonomous machine capabilities and human users, which are granted                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | (semi) equal 3-01                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              |                                                                                       |      |\n| teaming (HMT)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | weighting.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              |                                                                                       |      |\n| truth fairness                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | methods and approaches for designing and architecting user interfaces and the interactions between humans and computer (or information) technology. ce_ARLIS_202 3-01 Human-cognitive biases relate to how an individual or group perceives AI system information to make a decision or fill in missing information, or how humans think about purposes and functions of an AI system. Human biases are omnipresent in decision-making processes across the AI lifecycle and system use, including the design, implementation, operation, and maintenance of AI. NIST_AI_RMF _1.0 Detection, correlation, and pattern recognition generated through machine- based observation of human operation of software systems capturing successful or unsuccessful operations to enable the creation of a useful predictive analytics capability. IEEE_Guide_I PA An AI system that requires human interaction. DOD_Modelin g_and_Simula tion_Glossary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              |                                                                                       |      |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | circumstances.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              |                                                                                       |      |\n| bias human-assisted human-computer interaction (HCI) human-cognitive bias                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Systematic error in judgment and decision-making which can be due to cognitive limitations, adaptations to natural environments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | common to all human beings motivational factors, and/or                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              |                                                                                       |      |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | NSCAI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              |                                                                                       |      |\n| human-machine                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     |                 |            |     |              |                                                                                       |      |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | to SP1011                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                      |                                               |     | human-operator- |            |     | intervention | extenuating                                                                           | some |\n\n<!-- image -->\n\n| Terms                                                                                                                                                         | Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Citation 2                                                  | Definition 3                                                                                                                                                                                                                                                   | Citation                                              | Citation                                                                                                                                                                      | 4 Definition 5                                                                                             | Related terms and synonyms Legal definition   |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|-----------------------------------------------|\n| human subjects hyperparameters hypothesis testing impact assessment impersonation                                                                             | a living individual about whom an investigator (whether professional or student) conducting research: (i) Obtains information or biospecimens through intervention or interaction with the individual, and uses, studies, or analyzes the information or biospecimens; or (ii) Obtains, uses, studies, analyzes, or generates 018_Requirem ents_ (2018_Commo n_Rule) network) or to specify the algorithm used to minimize the loss function (e.g., the activation function and optimizer types in a neural network, and the kernel type in a support vector machine). zation A term used generally to refer to testing significance when specific alternatives to the null hypothesis are considered. OECD a risk management tool that seeks to ensure an organization has sufficiently considered a system's relative benefits and costs before implementation. In the context of AI, an impact assessment helps to answer a simple question: alongside this system's intended use, for whom could it fail? nts A malicious individual is able to impersonate a legitimate data subject to the data controller. The adversary forges a valid access request and goes through the identity verification enforced by the data controller. The data controller sends to the adversary the data of a legitimate data subject. Defeating impersonation is | 45_CFR_46_2 Bipartisan_Poli cy_Center_im pact_assessme An evaluation process designed to identify, understand, document and the potential ethical, legal, economic and societal implications of an AI a specific use case. Security_Analy sis_of_Subject _Access                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | mitigate system in IAPP_Governa nce_Terms                   | an alleged harm or near harm event to people, property, or the environment where an AI system is implicated. In simple terms, inclusion is getting the mix to work together. A fairness metricthat checks whether similar individuals are classified similarly | ditors aime_measure ment_2022 citing Machine Learning | 3 Definition 4                                                                                                                                                                |                                                                                                            | [2] applicable participant                    |\n| human system integration (HSI) incident incident response                                                                                                     | identifiable private information or identifiable biospecimens. methods and approaches for testing and optimizing all human-related considerations from a 'whole-system' or 'system-of-systems' level. the parameters that are used to either configure a ML model (e.g., the penalty parameter C in a support vector machine, and the learning rate to train the primary objective of any authentication protocol. The result of this attack is a data breach (e.g. blaggers [sic] pretend to be someone they are not in order to wheedle out the information they are seeking obtaining information illegaly which they then sell for a specified price). Techniques that modify the algorithms in order to mitigate bias during model training. Model training processes could incorporate changes to the objective (cost) function or impose a new optimization constraint. SP1270 A bias mitigation algorithm that is applied to a model during its training. a situation in which AI systems caused, or nearly caused, real-world harm. a public official response to an incident ... from an entity (i.e. company,                                                                                                                                                                                                                               | Poore_Lawren ce_ARLIS_202 3-01 a neural On_Hyperpara meter_Optimi Techniques that try to modify and change state-of-the-art learning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | algorithms to Mehrabi, _Ninareh FBPML_Wiki                  |                                                                                                                                                                                                                                                                |                                                       | Adverse event(s) in a computer system or networks caused by a failure of a security mechanism, or an attempted or threatened breach of these mechanisms.                      |                                                                                                            |                                               |\n| in-processing in-processing algorithm independence individual fairness inference                                                                              | organization, individual) allegedly responsible for developing or deploying the AI or AI system involved in said incident. Of software quality assurance (SQA), situation in which SQA is free from technical, managerial, and financial influences, intentional or unintentional The goal of similar individuals receiving similar treatments or outcomes. 60 The stage of ML in which a model is applied to a task. For example, a classifier model produces the classification of a test sample. Draft One of the three components of a model. This component delivers assumptions and data to the model. ffice preservation of confidentiality, integrity and availability of information; in addition, other properties, such as authenticity, accountability, and reliability can also be involved. Data received from an external source cab                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | remove discrimination during the model training process. AI_Fairness_3 60 AI_Incident_D atabase the occurrence of a technical event that affects the integrity of a Product and/or Model. AIID_incident _response IEEE_Soft_Vo cab Two events are independent if the occurrence of one event does not affect the chances of the occurrence of the other event. The mathematical formulation of the independence of events A and B is the probability of the occurrence of both A and B being equal to the product of the probabilities of A and B (i.e., P(A and B) = P(A)P(B)) AI_Fairness_3 Give similar predictions to similar individuals NISTIR_8269_ Comptroller_O ISO/IEC_TS_ 5723Ê¼2022(en) IEEE_Soft_Vo IEEE_Caught_ in_the_Act | nist_800_2010 Mehrabi, _Ninareh                             |                                                                                                                                                                                                                                                                | AI_Incident_E Glossary by Google                      |                                                                                                                                                                               | Hasan,_Raza                                                                                                | testing                                       |\n| information input component information security input insider attack in silico instance instance weight integrity intelligent process automation interaction | non-repudiation, Those who are within [an] organisation may have authorised access to vast amounts of sensitive company records that are essential for maintaining competitiveness and market position, and knowledge of information services and procedures that are crucial for daily operations. . . .[and] should an individual choose to act against the organisation, then with their privileged access and their extensive knowledge, they are well positioned to cause serious damage. carrying out some experiment by means of a computer simulation co Discrete, bounded thing with an intrinsic, immutable, and unique identity. Individual occurrence of a type cab A numerical value that multiplies the contribution of a data point in a model. 60 Degree to which a system, product, or component prevents unauthorized access to, or modification of, computer programs or data. cab A preconfigured software instance that combines business rules, experience- based context determination logic, and decision criteria to initiate and execute multiple interrelated human and automated processes in a dynamic context. The goal is to complete the execution of a combination of processes, activities, and tasks in one or more unrelated software systems that deliver a result or service PA                                  | World_Wide_ Words_In_sili IEEE_Soft_Vo A single object of the world from which a model will be learned, or on model will be used (e.g., for prediction). AI_Fairness_3 IEEE_Soft_Vo Guarding against improper information modification or destruction, and ensuring information non-repudiation and authenticity. IEEE_Guide_I Degree to which two or more systems, products or components can exchange information and use the information that has been exchanged. The ability to explain or to present an ML model's reasoning in understandable terms to a human                                                                                                                                                                    | which a Kohavi,_Ron includes CSRC                           | The property whereby information, an information system, or a component of a system has not been modified or destroyed in an unauthorized manner.                                                                                                              | CISA                                                  | <data> property whereby data have not been altered in an unauthorized manner since they were created, transmitted, or stored; <systems> property of accuracy and completeness | ISO/IEC_TS_ 5723Ê¼2022(en) the quality of moral consistency, honesty, and truthfulness with oneself others. | computer simulation                           |\n| internal validity interoperability interpretability interpretable model intervenability                                                                       | with minimal or no human intervention. Action that takes place with the participation of the environment of the object. IEEE_Soft_Vo cab the degree to which a study or experiment is free from flaws in its internal structure and its results can therefore be taken to represent the true nature of the phenomenon. In other words, internal validity pertains to the soundness of results obtained within the controlled conditions of a particular study, specifically with respect to whether one can draw reasonable conclusions about cause-and-effect relationships among variables. APA_internal_ validity The ability of software or hardware systems or components to operate together successfully with minimal effort by end user SP1011 The ability to understand the value and accuracy of system output. Interpretability refers to the extent to which a cause and effect can be observed within a system or to which what is going to happen given a change in input or algorithmic parameters can be predicted. NSCAI                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | cab aime_measure ment_2022, citing Learning Glossary Google | IEEE_Soft_Vo The ability for tools to work together in execution, communication, and data exchange under specific conditions. NIST_1500 Machine by                                                                                                             |                                                       |                                                                                                                                                                               |                                                                                                            | explainability                                |\n| knowledge label label shift                                                                                                                                   | An interpretable machine learning model obeys a domain-specific set of constraints to allow it (or its predictions, or the data) to be more easily understood by humans. These constraints can differ dramatically depending on the domain. rudin_interpre table_2022 the property that intervention is possible concerning all ongoing or planned privacy relevant data processing[; ...] the data subjects themselves should be able to intervene with regards to the processing of their own data ... [to ensure] that data subjects have the ability to control how their data is processed and by whom. Covert_et_al The sum of all information derived from diagnostic, descriptive, predictive, and prescriptive analytics embedded in or available to or from a cognitive computing system. IEEE_Guide_I PA A value corresponding to an outcome. AI_Fairness_3 60 Under label shift, the label distribution p(y) might change but the class-                                                                                                                                                                                                                                                                                                                                                                                                   | <artificial intelligence> abstracted informationabout objects, events, concepts rules, their relationships and properties, organizedfor goal-oriented systematic use target variable assigned to a sample saurabh_label                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | or ment_2022, citinig 22989 ment_2022, citing               | aime_measure ISO/IEC aime_measure ISO/IEC 22989                                                                                                                                                                                                                |                                                       |                                                                                                                                                                               |                                                                                                            | language model                                |\n| large language model (LLM)                                                                                                                                    | conditional distributions p(x&#124;y) do not. ... We work with the label shift assumption, i.e., ps(x&#124;y) = pt(x&#124;y) _2020 a class of language models that use deep-learning algorithms and are trained on extremely large textual datasets that can be multiple terabytes in size. LLMs can be classed into two types: generative or discriminatory. Generative LLMs are models that output text, such as the answer to a question or even writing an essay on a specific topic. They are typically unsupervised or semi-supervised learning models that predict what the response is for a given task. Discriminatory LLMs are supervised learning models that usually focus on classifying text, such as determining whether a text was made by a human or AI. 2022                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                             |                                                                                                                                                                                                                                                                |                                                       |                                                                                                                                                                               |                                                                                                            |                                               |\n| language model                                                                                                                                                | AI_Assurance_ A language model is an approximative description that captures patterns and regularities present in natural language and is used for making assumptions on Gustavii,_Ebba                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | the acquisition of novel information, behaviors, or abilities after practice, observation, or other experiences, as evidenced by change in behavior, knowledge, or brain function. Learning involves consciously or nonconsciously attending to relevant aspects of incoming information, mentally organizing the                                                                                                                                                                                                                                                                                                                                                                                                                       | APA_learning                                                |                                                                                                                                                                                                                                                                |                                                       |                                                                                                                                                                               |                                                                                                            | model (LLM)                                   |\n| learning                                                                                                                                                      | previously unseen language fragments. A procedure in artificial intelligence by which an artificial intelligence program improves its performance by gaining knowledge. Dennis_Merca dal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | information into a coherent cognitive representation, and integrating it with                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                             |                                                                                                                                                                                                                                                                |                                                       |                                                                                                                                                                               |                                                                                                            | large language                                |\n|                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | relevant existing knowledge activated from long-term memory.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                             |                                                                                                                                                                                                                                                                |                                                       |                                                                                                                                                                               |                                                                                                            |                                               |\n\n| Terms Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Citation 1 [1]                                                                                                                                                               | Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Citation 2                                                                        | Definition 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Citation 3 Definition 4                                                                                             |                                                                                                                                                                                                                                                          | Citation 4 Definition 5                                                                                                                                                                                                                                                                                                                                                                                    | Citation 5 Related terms   | and synonyms Legal definition     | and synonyms Legal definition                                                                                                                                          | and synonyms Legal definition                  | and synonyms Legal definition   | and synonyms Legal definition                                                                                                                                                                                                                                                                                          | and synonyms Legal definition                             | and synonyms Legal definition   | and synonyms Legal definition   | and synonyms Legal definition   | and synonyms Legal definition   | and synonyms Legal definition   | and synonyms Legal definition   | and synonyms Legal definition   | and synonyms Legal definition   | and synonyms Legal definition   | and synonyms Legal definition   |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|-----------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|---------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|---------------------------------|\n| The principle that a security architecture should be designed so that each is granted the minimum system resources and authorizations that the entity needs to perform its function. forms of a word so                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | The security objective of granting users only those accesses perform their official duties. Artasanchez_J in natural language processing[, ...] working with words according | they need to to their                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | SP-800-12                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            | applicable                        | entity CSRC they                                                                                                                                                       | applicable                                     | applicable                      | applicable                                                                                                                                                                                                                                                                                                             | privilege lemmatization                                   | least                           | applicable                      | applicable                      | applicable                      | applicable                      | applicable                      | applicable                      | applicable                      | applicable                      | applicable                      |\n| the process of grouping together the different inflected can be analyzed as a single item. [a supervised learning algorithm that uses] a simple formula to find points.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | oshi_AI_with_ Python lexical line dataiku_ML_a nd_linear_mo dels (linear) An and scalar                                                                                      | components operator L^~ is said to be linear if, for every pair of functions f and t,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | g wolfram_math world_2022                                                         | grouping together words with the same root or lemma but with different inflections or derivatives of meaning so they can be analyzed as one item.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | Techslang_lem matization                                                                                            | root Techopedia_le mmatization                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   | a best-fit                                                                                                                                                             |                                                |                                 | through a set of data                                                                                                                                                                                                                                                                                                  | linear model                                              |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| A local explanation explains a subset of decisions or is a per-decision explanation. Creation of a national or specific regional version of a product. (logistic equation) The continuous version of the logistic model is described by parameter (rate of maximum population capacity (i.e., the maximum sustainable K and defining x=N/K then gives the differential logistic equation and has solution sometimes known as the sigmoid function. Intelligence (AI) that focuses on the development from data to perform a task without being that task. Learning refers to the process through computational techniques such that the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | and                                                                                                                                                                          | L^~(f+g)=L^~f+L^~g L^~(tf)=tL^~f.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          | A subcategory of artificial intelligence; a method of designing a sequence of actions to solve a problem that optimizes automatically through experience and with limited or no human intervention. ffice                                                                                                                                                                                                  | Comptroller_O              |                                   | NISTIR_8312_ Full IEEE_Soft_Vo cab growth) and K population). equation wolfram_math world_2022 of systems explicitly of optimizing model's TTC6_Taxono my_Terminolo gy |                                                |                                 | the differential equation (dN)/(dt)=(rN(K-N))/K, (1) where r is the Malthusian is the so-called carrying Dividing both sides by (dx)/(dt)=rx(1-x), (2) which is known as the x(t)=1/(1+(1/(x_0)-1)e^(-rt)). (3) The function x(t) is A branch of Artificial capable of learning programmed to perform model parameters | localization logistic model machine learning              | local                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| Machine detection and interpretation of relevant and meaningful events and conditions that impact operation of the computer system itself or other dependent mechanisms or processes essential to the purpose of the system. Hardware, firmware, or software that is intentionally included or inserted in a system for a harmful purpose. Refers to the significance of a matter in relation to a set of financial or performance information. If a matter is material to the set of information, then it is likely to be of significance to a user of that information presum[ing] that (A) quantitative models of reality are always more accurate than other models; (B) the quantitative measurements that can be made most easily must be the most relevant; and (C) factors other than those currently being used in quantitative metrics must either not exist or not have a significant influence on success. Also known as the quantitative fallacy. (Quantitative) (1) act or process of assigning a number or category to an entity to describe an attribute of that entity; (2) assignment of numbers to way to represent properties of the object; value (e.g., a number or category) from a scale (4) set of operations having the object of | PA Reznik,_Leon OECD                                                                                                                                                         | Software that compromises the operation of a system by performing an unauthorized function or process.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | CISA Leavy_OHQR_ Intro                                                            | Qualitative measurement engages research methods and techniques to provide information about the nature of phenomenon. Qualitative methods are designed for systematic collection, organization, description and interpretation of non- numeric (textual, verbal or visual) data (Hammarberg et. al, 2016). Qualitative measurement generally answers questions about why, for whom, when, and how something is (or is not) observed, whereas quantitative measurement answers questions about what is observed. Elements assessed using qualitative measurement may include contextual norms or meaning, socio-cultural dynamics, individual or collective beliefs, and complex multi-component interactions or interventions (Busetto et. al, 2020). | Hammarberg_ 2016_Busetto_ Documentation qualitative based on the appropriately corroboration or for complementarity | assumptions and methods used is a foundational element of Russell_2003_                                                                                                                                                                                  | of measurement, as the choice of single or combined methods is made phenomenon and its context (Russell & Gregory, 2003). When paired, qualitative and quantitative measurement can provide elaboration, demonstrate use cases, and/or identify conditions or contradiction (Brannen, 2005). Brannen_2005                                                                                                  | trojan horse               | quantitative fallacy              | quantitative fallacy                                                                                                                                                   | quantitative fallacy                           | quantitative fallacy            | objects in a systematic a metric to assign a attribute of an entity;                                                                                                                                                                                                                                                   | quantitative fallacy                                      | quantitative fallacy            | quantitative fallacy            | quantitative fallacy            | quantitative fallacy            | quantitative fallacy            | quantitative fallacy            | quantitative fallacy            | quantitative fallacy            | quantitative fallacy            | quantitative fallacy            |\n| value of a measure; (5) assignment of values and labels to aspects of software engineering work products, processes, and resources plus the models that are derived from them, whether these models are developed using statistical or other techniques; (6) figure, extent, or amount obtained by measuring generic description of a logical organization of operations used in a measurement The initial confirmatory factory analysis (CFA) model that underlies the structural                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 24765 aime_measure ment_2022, citing ISO/IEC Guide 99 Little_2013 A statistical latent variables,                                                                            | aime_measure ISO/IEC (Qualitative) (1) a way of learning about social reality [...][that uses] approaches [...] to explore, describe, or explain social phenomen[a]; unpack the meaning people ascribe to activities, situations, events, or [artifacts]; build a depth of understanding about some aspect of social life; build \"thick descriptions\" (see Clifford Geertz, 1973) of people in naturalistic settings; explore new or underresearched areas; or make micro-macro links (illuminate connections between individuals-groups and institutional and/or cultural contexts). (2) [approaches that] can make visible and unpick the mechanisms which link particular variables, by looking at the explanations, or accounts, provided by those involved. logical sequence of operations, described generically, usedin quantifying an attribute with respect to a specified scale theoretical constructs, properties-i.e., data about the world | aime_measure ment_2022, citing ISO/IEC 24765 operationalized jackman_oxfor d_2008 | 2020                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                     | cab                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   | as is ISO/IEC_TS_ 5723Ê¼2022(en)                                                                                                                                        | model that links and                           | unobservable observable         | method model                                                                                                                                                                                                                                                                                                           | measurement measurement measurability membership metadata |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| model [that] tests the adequacy (as indexed by model fit) of the specified relations whereby indicators are linked to their underlying construct. ability to assess an attribute of an entity against a metric (note 1Ê¼ \"measurable\" the adjective form of \"measurability\") inference given a machine learning model and a record, determining whether the record was used as part of the model's training dataset or not. Metadata is data that defines and describes other data. method and measurement scale                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | OECD ISO/IEC_TS_                                                                                                                                                             | Data that describe other data. (1) quantitative measure of the degree to which a system, component, or possesses a given attribute; (2) defined measurement method and the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | IEEE_Soft_Vo aime_measure ment_2022,                                              | Data employed to annotate other data with descriptive information, possibly including their data descriptions, data about data ownership, access paths, access rights, and data volatility.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                     | process measure in this section above citing ISO/IEC 24765                                                                                                                                                                                               | A quantitative method, system, or approach that applies statistical, economic, financial, or mathematical theories, techniques, and assumptions to process input data into quantitative estimates. A model consists of three components: an information input component, which delivers assumptions and data to the model; a processing component, which transforms inputs into estimates; and a reporting | Comptroller_O ffice        |                                   | 5723Ê¼2022(en)                                                                                                                                                          |                                                |                                 | defined measurement                                                                                                                                                                                                                                                                                                    | metric minimization                                       |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| (Part of the ICO framework for auditing AI) AI systems generally require large amounts of data. However, organisations must comply with the minimisation principle under data protection law if using personal data. This means ensuring that any personal data is adequate, relevant and limited to what is necessary the purposes for which it is processed. [â€¦] The default approach of data scientists in designing and building AI systems will not necessarily take into account any data minimisation constraints. Organisations must therefore have place risk management practices to ensure that data minimisation requirements, and all relevant minimisation techniques, are fully considered from the design phase, or, if AI systems are bought or operated by third parties, as part of the procurement process due diligence In mixed methods, the researcher collects and analyzes both qualitative and quantitative data rigorously in response to research questions and hypotheses; integrates the two forms of data and their results; organizes these procedures                                                                                                                                                                      | for in ICO_data_min imisation research in integrates the quantitative                                                                                                        | measurement scale; c.f., a data controller should limit the collection of personal information to what directly relevant and necessary to accomplish a specified purpose. They should also retain the data only for as long as is necessary to fulfil that purpose. In words, data controllers should collect only the personal data they really need, and should keep it only for as long as they need it. The data minimisation principle is expressed in Article 5(1)(c) of the GDPR and Article 4(1)(c) of Regulation (EU) 2018/1725, which provide that personal data must be \"adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed\". investigator collects and analyzes data, inferences using both qualitative and a single study or a program of                                                                                                                                        | is other EDPS_data_mi nimization study. Lisa_M. _Given_SAGE                       | A core component of an AI system used to make inferences from inputs in order to produce outputs. A model characterizes an input-to-output transformation intended to perform a core computational task of the AI system (e.g., classifying an image, predicting the next word for a sequence, or selecting a robot's next                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                     | expression of a theory or the causal situation which is generated observed data. In statistical analysis the model is in symbols, that is to say in a mathematical form, but are also found. The word has recently become very popular over-worked. OECD | component, which translates the estimates into useful business information.                                                                                                                                                                                                                                                                                                                                |                            |                                   |                                                                                                                                                                        | which the inquirer findings, and approaches or | or draws methods in             |                                                                                                                                                                                                                                                                                                                        | mixed methods                                             |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| into specific research designs that provide the logic and procedures for conducting the study; and frames these procedures within theory and philosophy. MLOps (machine learning operations) stands for the collection of techniques tools for the deployment of ML models in production. A function that takes features as input and predicts labels as output.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Creswell_Clark _mixed_meth ods and symeonidis_M LOps_2022 AI_Fairness_3 60                                                                                                   | A model is a formalised regarded as having expressed models                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                   | action given its state and goals). gy                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | TTC6_Taxono my_Terminolo                                                                                            | about an AI model, like explanations and benchmarked evaluation in various cultures, demographics or race.                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                            |                            | stealing                          | stealing                                                                                                                                                               | stealing                                       | stealing                        | stealing                                                                                                                                                                                                                                                                                                               | stealing                                                  | stealing                        | stealing                        | stealing                        | stealing                        | stealing                        | stealing                        | stealing                        | stealing                        | stealing                        | stealing                        |\n| Model assertions are arbitrary functions over a model's input and output that indicate when errors may be occurring short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. [They] also disclose the context in which models are intended to be used, of the performance evaluation procedures, and other relevant information. Model debugging aims to diagnose a model's failures.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Kang,_Daniel sex, Model_Cards_ for_Model_Re porting A                                                                                                                        | possibly somewhat document that discloses information intended use, performance metrics such as across different                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | IAPP_Governa nce_Terms                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            | model inversion;                  | model inversion;                                                                                                                                                       | model inversion;                               | model inversion;                | model inversion;                                                                                                                                                                                                                                                                                                       | model inversion;                                          | model inversion;                | model inversion;                | model inversion;                | model inversion;                | model inversion;                | model inversion;                | model inversion;                | model inversion;                | model inversion;                | model inversion;                |\n| details Model decay depicts that the performance of the model is degrading over time An area of research that aims to enable fast, data-efficient updates to a pre- trained base model's behavior for only a small region of the domain, without                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Jain_Saachi Nayak,_Pragati                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        | model debugging model decay                               |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface. Model Governance is the name for the overall internal framework of a firm or organization that controls the processes for Model Development, Model Validation and Model Usage, assign responsibilities and roles etc. 2 in the context of Risk Management, [...] a database/[management information                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Chandrasekara n,_Varun                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            | model                      |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        |                                                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| system] developed for the purpose of aggregating quantitative model related information that is in use by a firm or organization. nventory                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | More                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        | model governance                                          |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| or qualitative adjustments to model inputs or outputs to compensate model, data, or other known limitations. A model overlay is a type of override. ffice                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | open_risk_202                                                                                                                                                                | brief about conditions,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   | Mitchell,_Eric                                                                                                                                                         |                                                |                                 |                                                                                                                                                                                                                                                                                                                        | model editing                                             |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| performance on other inputs of interest exploiting the query interface to steal the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | such                                                                                                                                                                         | generally diagrammatic and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   | model.                                                                                                                                                                 |                                                |                                 | damaging model Adversaries maliciously                                                                                                                                                                                                                                                                                 | model extraction                                          |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | controls Fed_Reserve                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        |                                                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| Judgmental for                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Comptroller_O                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        |                                                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| model risk management encompasses governance and control mechanisms                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        |                                                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| as board and senior management oversight, policies and procedures, and compliance, and an appropriate incentive and organizational structure                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        |                                                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        | model suite                                               |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| A group of models that work together.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        |                                                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Comptroller_O                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        |                                                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | ORM_model_i                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        |                                                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n| inventory overlay                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            |                                   |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        |                                                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                     |                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                            |                            | model model model risk management |                                                                                                                                                                        |                                                |                                 |                                                                                                                                                                                                                                                                                                                        |                                                           |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |                                 |\n\n| Terms                                                                                                    | Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                         | Citation 2                                                         | Definition 3 Citation                                                                                                                                                                                                                               | 3 Definition 4                                                                                         | Citation 4                                | Definition 5   | Related terms and synonyms Legal definition         |\n|----------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|-------------------------------------------|----------------|-----------------------------------------------------|\n| model training                                                                                           | the phase in the data science development lifecycle where practitioners try to the best combination of weights and bias to a machine learning algorithm to minimize a loss function over the prediction range                                                                                                                                                                                                                                                                                                                                                                                                                                 | fit C3. ai_Model_Trai ning process to determine or to improve the parameters of a machine learning based on a machine learning algorithm, by using training data                                                                                                                                                                                                                    | model, aime_measure ment_2022, citing ISO/IEC 22989                |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                | [2] applicable                                      |\n| model validation                                                                                         | the set of processes and activities intended to verify that models are as expected.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | performing yields. io_model_vali dation the set of principles, practices and organizational arrangements supporting rigorous (audited) model development and validation cycle.                                                                                                                                                                                                      | a Open_Risk_M anual_model_                                         |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| monitoring                                                                                               | Examination of the status of the activities of a supplier and of their results by acquirer or a third party. The capacity for moral action, reasoning, judgment, and decision making, as                                                                                                                                                                                                                                                                                                                                                                                                                                                      | the IEEE_Soft_Vo cab Continual checking, supervising, critically observing or determining the status in order to identify change from the performance level required or expected. AI_Ethics_Mar                                                                                                                                                                                     | validation SP800-160                                               |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| moral agency                                                                                             | opposed to merely having moral consequences.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | k_Coeckelberg h                                                                                                                                                                                                                                                                                                                                                                     |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| naive Bayes                                                                                              | The naive Bayes classifier is a Bayesian learning method that has been found to be useful in many practical applications. It is called \"naive\" because it incorporates the simplifying assumption that attribute values are conditionally independent, given the classification of the instance. The naive Bayes classifier applies to learning tasks where each instance x is described by a conjunction of attribute values and where the target function f(x) can take on any value from some finite set V.                                                                                                                                | h Mitchell,_Tom                                                                                                                                                                                                                                                                                                                                                                     |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| natural language processing neural network                                                               | The field concerned with machines capable of processing, analysing, and generating human language, either spoken, written or signed. A model that, taking inspiration from the brain, is composed of layers (at least one of which is hidden) consisting of simple connected units or neurons followed by nonlinearities                                                                                                                                                                                                                                                                                                                      | TTC6_Taxono my_Terminolo gy aime_measure ment_2022, citing Machine Learnign Glossary by Google                                                                                                                                                                                                                                                                                      |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| nondiscrimination                                                                                        | the practice of treating people, companies, countries, etc. in the same way as others in order to be fair:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Cambridge Dictionary In the context of machine learning non-discrimination can be defined as follows: (1) people that are similar in terms non-protected characteristics should receive similar predictions, and (2) differences in predictions across groups of people can only be as large as justified by non-protected characteristics. Ä—                                       | Å½liobaitÄ—_Indr                                                     |                                                                                                                                                                                                                                                     | the practice of treating people, companies, countries, etc. in the same way others in order to be fair | Cambridge_Di ctionary_non- discrimination |                |                                                     |\n| normal flow normalization                                                                                | The intended flow of a process originating from a start event, continuing through all defined activities, and concluding successfully to its defined end event. Conceptual procedure in database design that removes redundancy in a complex database by establishing dependencies and relationships between database entities. Normalization reduces storage requirements and avoids database inconsistencies.                                                                                                                                                                                                                               | IEEE_Guide_I PA OECD The process of convertingan actual range of values into a standard range of values, typically -1to +1 or 0 to 1                                                                                                                                                                                                                                                | aime_measure ment_2022, citing Machine Learning Glossary by Google |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| objective evidence observation                                                                           | data supporting the existence or verity of something (note: can be obtained through observation, measurement, test, or other means). a piece of information received online from users, sensors, or other knowledge sources                                                                                                                                                                                                                                                                                                                                                                                                                   | ISO/IEC_TS_ 5723Ê¼2022(en) poole_mackwo rth_observatio n the careful, close examination of an object, process, or other phenomenon for purpose of collecting data about it or drawing conclusions.                                                                                                                                                                                   | the APA_observati on                                               |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| offline learning online learning ontology                                                                | implies ... a static dataset that [one] know[s] from the start and the parameters of [one's] machine learning algorithm are adjusted to the whole dataset at once often loading the whole dataset into memory or in batches. fitting [one's] model incrementally as the data flows in (streaming data). A set of concepts and categories in a subject area or knowledge domain that shows their properties and the relationships among them to enable                                                                                                                                                                                         | Ben_Auffarth_ 2021 Ben_Auffarth_ 2021 IEEE_Guide_I PA                                                                                                                                                                                                                                                                                                                               |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| opacity operationalization operator                                                                      | interoperability among disparate elements and systems and specify interfaces to independent, knowledge-based services for the purpose of enabling certain kinds of automated reasoning. The nature of some AI techniques whereby the inferential operations are complex, hidden, or otherwise opaque to their developers and end users in terms of providing an understanding of how classifications, recommendations, or actions are generated and what overall performance will be. Putting AI systems or related concepts into use so they can be measured. A role assumed by the person performing remote control or teleoperation, semi- | NSCAI A description of some deep learning systems [that] take an input and provide an output, but the calculations that occur in between are not easy for humans to interpret. SP1011 Individual or organization that performs the operations of a system.                                                                                                                          | Hutson, _Matthew                                                   | When one or more features of an AI system, such as processes, the provenance of datasets, functions, output or behaviour are unavailable or incomprehensible to all stakeholders - usually an antonym for transparency. TTC6_Taxono my_Terminolo gy |                                                                                                        |                                           |                | black box; unexplainable                            |\n| autonomous operations, opt-in an individual makes an signaling a desire to share opt-out outcome outlier | or other human-in-the-loop types of operations active affirmative indication of choice via a user interface their information with third parties. an individual makes an active affirmative indication of choice via a user interface signaling a desire not to share their information with third parties. something that follows as a result or consequence An outlier is a data point that is far from other points.                                                                                                                                                                                                                       | IAPP_Privacy_ Glossary IAPP_Privacy_ Glossary merriam_webs ter_outcome Russell_and_N orvig An outlier is a data value that lies in the tail of the statistical distribution of a set of data values.                                                                                                                                                                                | IEEE_Soft_Vo cab OECD Values are                                   | Individual or organization that performs the operations of a system. SP800-160 distant from mostother values. In machine learning, any of the following outliers:â€¢ Weights with high absolute valuesâ€¢ Predicted values relatively far               | aime_measure ment_2022                                                                                 |                                           |                | privacy; consent; opt- out privacy; consent; opt-in |\n| output                                                                                                   | Data transmitted to an external destination Given a hypothesis space H, a hypothesis h element of H is said to training data if there exists some alternative hypothesis h' element of that h has smaller error than h' over the training examples, but h' has a                                                                                                                                                                                                                                                                                                                                                                              | IEEE_Soft_Vo cab Process by which an information processing system, or any of its parts, transfers data outside of that system or part Mitchell,_Tom                                                                                                                                                                                                                                | IEEE_Soft_Vo cab                                                   | Google                                                                                                                                                                                                                                              |                                                                                                        |                                           |                |                                                     |\n| overfitting                                                                                              | overfit the H, such smaller error than h over the entire distribution of instance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                     |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| package                                                                                                  | a folder with all the code and metadata needed to train and serve a machine learning model. A learning model that summarizes data with a set of parameters of                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | about_ML_pa ckages                                                                                                                                                                                                                                                                                                                                                                  |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| parametric                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Russell_and_N                                                                                                                                                                                                                                                                                                                                                                       |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| fixed size (independent parent process                                                                   | of the number of training examples) A process that may contain one or more sub-processes, activities, and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | orvig tasks. IEEE_Guide_I PA                                                                                                                                                                                                                                                                                                                                                        |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| parity                                                                                                   | Bit(s) used to determine whether a block of data has been altered. Rationale: Term has been replaced by the term 'parity bit'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | NIST_CSRC_p arity the quality or state of being equal or equivalent Sloane_et_al_                                                                                                                                                                                                                                                                                                   | Merriam- Webster_parit y                                           |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| participation participant                                                                                | engag[ing] multiple stakeholders in deliberative processes in order to consensus. A computer system, data, input, business rule, human intervention, and contributor to the flow of a process.                                                                                                                                                                                                                                                                                                                                                                                                                                                | 2020 IEEE_Guide_I PA a living individual about whom an investigator (whether professional or student) conducting research: (i) Obtains information or biospecimens through intervention or interaction with the individual, and uses, studies, or analyzes the information or biospecimens; or (ii) Obtains, uses, studies, analyzes, or generates identifiable private information | 45_CFR_46_2 018_Requirem ents_ (2018_Commo n_Rule)                 |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                | human subject                                       |\n|                                                                                                          | achieve other                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                     |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n| passive learning agent                                                                                   | A passive learning agent has a fixed policy that determines its behavior. An                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | or identifiable biospecimens.                                                                                                                                                                                                                                                                                                                                                       |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                | active learning agent                               |\n| active learning agent gets to decide what actions to take. Russell_and_N orvig                           | active learning agent gets to decide what actions to take. Russell_and_N orvig                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | active learning agent gets to decide what actions to take. Russell_and_N orvig                                                                                                                                                                                                                                                                                                      |                                                                    |                                                                                                                                                                                                                                                     |                                                                                                        |                                           |                |                                                     |\n\n|                                                                              | Terms Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Citation 2                                                                      | Definition 3                                                                                                                                                                                                                                                                                                  | Citation 3 Definition                                                                                                                    | 4 Citation 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Related terms and synonyms Legal definition                                             |\n|------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n| personal data                                                                | 'Personal data' means any information relating to an identified or identifiable natural person ('data subject'); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. GDPR                                                                                                                                                                                                                                                                                                                       | (1) 'Personal information' means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular consumer or household. Personal information includes, but is not limited to, the following if it identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular consumer or household: (A) Identifiers such as a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, social security number, driver's license number, passport number, or other similar identifiers. (B) Any personal information described in subdivision (e) of Section 1798.80. (C) Characteristics of protected classifications under California or federal law. (D) Commercial information, including records of personal property, products or services purchased, obtained, or considered, or other purchasing or consuming histories or tendencies. (E) Biometric information. (F) Internet or other electronic network activity information, including, but not limited to, browsing history, search history, and information regarding a consumer's interaction with an internet website application, or advertisement. (G) Geolocation data. (H) Audio, electronic, visual, thermal, olfactory, or similar information. (I) Professional or employment-related information. (J) Education information, defined as information that is not publicly available personally identifiable information as defined in the Family Educational Rights and Privacy Act (20 U.S.C. Sec. 1232g; 34 C.F.R. Part 99). (K) Inferences drawn from any of the information identified in this subdivision to create a profile about a consumer reflecting the consumer's preferences, characteristics, psychological trends, predispositions, behavior, attitudes, intelligence, abilities, and aptitudes. (L) Sensitive personal information. (2) 'Personal information' does not include publicly available information or lawfully obtained, truthful information that is a matter of public concern. For purposes of this paragraph, 'publicly available' means: information that is made available from federal, state, or local government records, or information that a business has a reasonable basis to believe is lawfully made available to the general public by the consumer or from widely distributed media; or information made available by a person to whom the consumer has disclosed the information if the consumer has not restricted the information to a specific 'Publicly available' does not mean biometric information collected about a consumer without the consumer's knowledge. | lawfully audience. by a business CCPA                                           |                                                                                                                                                                                                                                                                                                               |                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | [2] applicable                                                                          |\n| policy positionality                                                         | The general principles by which a government is guided in its management of public affairs, or the legislature in its measures. This term, as applied to a law, ordinance, or rule of law, denotes its general purpose or tendency considered as directed to the POLICY 23 the researcher's starting points and standpoints before and during inquiry, as                                                                                                                                                                                                                                                                                                                                                                                                                                                    | deidentified or aggregate consumer information. law_policy_20 A policy defines the learning agent's way of behaving at a given time Charmaz_Hen                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | sutton_reinfor cement_2018                                                      |                                                                                                                                                                                                                                                                                                               |                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                         |\n| post-hoc                                                                     | well as the conditions shaping the research situation, process, and product. wood Post-hoc explainability targets models that are not readily interpretable by design by resorting to diverse means to enhance their interpretability, such as text explanations, visual explanations, local explanations, explanations by example, explanations by simplification and feature relevance explanations techniques. Each of these techniques covers one of the most common ways humans explain systems and processes by themselves. Full                                                                                                                                                                                                                                                                       | NISTIR_8312_ Post-hoc explainability targets models that are not readily inter- pretable design by resorting to diverse means to enhance their in- terpretability,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | barredo_explai nable_2020                                                       |                                                                                                                                                                                                                                                                                                               |                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | reflexivity                                                                             |\n| explanation post-processing post-processing algorithm practical significance | Typically performed with the help of a holdout dataset (data not used in the training of the model). Here, the learned model is treated as a black box and its predictions are altered by a function during the post-processing phase. The function is deduced from the performance of the black box model on the holdout dataset. SP1270 A bias mitigation algorithm that is applied to predicted labels. 60 a conceptual framework for evaluating discrimination cases developed primarily on statistical evidence that is the subject of increasing interest and discussion by some in the equal employment opportunity (EEO) field.                                                                                                                                                                      | by such as text explanations, visual explanations, local expla- nations, explanations by example, explanations by simplification and feature relevance explanations techniques. Each of these techniques covers one of the most common ways humans explain systems and processes by themselves. Performed after training by accessing a holdout set that was not involved during the training of the model. If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-processing can be used in which the labels assigned by the black- box model initially get reassigned based on a function during the post-processing phase. AI_Fairness_3 DOL_Practical _Significance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Mehrabi, _Ninareh                                                               | Steps performed after a machine learning model has been run to adjust its output. This can include adjusting a model's outputs or using a holdout dataset- data not used in the training of the model -to create a function run on the model's predictions to improve fairness or meet business requirements. | IAPP_Governa nce_Terms                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | statistical significance (often paired in contrast to                                   |\n| pre-processing algorithm precision                                           | A bias mitigation algorithm that is applied to training data. 60 A metric for classification models. Precision identifies the frequency with which a model was correct when classifying the positive class. NSCAI Forecasting quantitative or qualitative outputs through function approximation, NSCAI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | AI_Fairness_3 closeness of agreement between indications or measuredquantity values obtained by replicate measurements on the same or similarobjects under specified conditions primary output of an AI system when provided with input data or                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | ment_2022, citing Guide 99 information                                          | aime_measure ISO/IEC A metric for classification models.Precision model was correct when predictingthe positive Positive/(True Positive + False Positive)                                                                                                                                                     | identifies the frequency with which a class. That is:Precision = True aime_measure ment_2022, citing Machine Learning Glossary by Google | Closeness of agreement between independent test results obtained under prescribed conditions. It is generally dependent on analyte concentration, and this dependence should be determined and documented. The measure of precision is usually expressed in terms of imprecision and computed as a standard deviation of the test results. Higher imprecision is reflected by a larger standard deviation. Independent test results means results obtained in a manner not influenced by any previous results on the same or similar material. Precision covers repeatability and reproducibility [19]. Alternatively, precision is a measure for the reproducibility of measurements within a set, that is, of the scatter or dispersion of a set about its central value. Precision depends only on the distribution of random errors and does not relate to the true value or specified value. UNODC_Gloss ary_QA_GLP | (synonym)                                                                               |\n| prediction predictive analysis predictive analytics                          | applied on input data or measurements. The organization of analyses of structured and unstructured data for inference and correlation that provides a useful predictive capability to new circumstances or data. PA Insights, reporting, and information answering the question, \"What is likely to happen?\" Predictive analytics support high confidence foretelling of future event (s). PA Transforming the data so that the underlying discrimination is mitigated. This method can be used if a modeling pipeline is allowed to modify the training data. SP1270 Insights, reporting, and information answering the question, 'What should I do about it?\" Prescriptive analytics determines information that provides high confidence actions necessary to recover from an event or fulfill a need. PA | IEEE_Guide_I IEEE_Guide_I IEEE_Guide_I ISO/IEC_TS_ 5723Ê¼2022(en) freedom from intrusion into the private lifeor affairs of an intrusion results from undue or illegalgathering and use of data individual                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | ment_2022, citing 22989 individual when that about that aime_measure ment_2022, | aime_measure ISO/IEC                                                                                                                                                                                                                                                                                          |                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | data- protection-by- design (def: https:/ /eur- lex.europa. eu/legal- XT/? uri=CELEX% - |\n| preprocessing prescriptive analytics privacy privacy-by-design               | freedom from intrusion into the private life or affairs of an individual Embedding privacy measures and privacy enhancing technologies directly into the design of information technologies and systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | ENISA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | citing ISO/IEC TR 24029-1                                                       |                                                                                                                                                                                                                                                                                                               |                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | content/EN/T 3A02016R0679 20160504&qid= 1532348683434 )                                 |\n| privacy-enhancing technology privileged protected attribute procedure        | A coherent system of ICT (Information and Communications Technology) measures that protects privacy by eliminating or reducing personal data or by preventing unnecessary and/or undesired processing of personal data, all without losing the functionality of the information system. k A value of a protected attribute indicating a group that has historically been at systematic advantage. 60 Information item that presents an ordered series of steps to perform a process, activity, or task. cab                                                                                                                                                                                                                                                                                                  | PET_Handboo AI_Fairness_3 IEEE_Soft_Vo                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                 |                                                                                                                                                                                                                                                                                                               |                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                         |\n\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Citation 2                                                                                                             | Definition 3                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                           | Citation 4 Definition 5 Citation 5 Related terms and synonyms Legal definition   |         | Citation 3 Definition 4   | Terms                                           |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|---------|---------------------------|-------------------------------------------------|\n| 'Processing' means personal data or on or such as collection, alteration, retrieval, by or otherwise making                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | which is performed on not by automated means, storage, adaptation or transmission, dissemination restriction, erasure or GDPR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 'Processing' means any operation or set of operations that are performed on personal information or on sets of personal information, whether or not by automated means.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | CCPA                                                                                                                   |                                                                                                                        | processing any operation or set of operations sets of personal data, whether recording, organisation, structuring, consultation, use, disclosure available, alignment or combination, destruction.                                                                                                                                                                                                                                        | [2] applicable personal data; processing                                         |         |                           |                                                 |\n| environment 'Processor' means a natural or legal person, public authority, agency or body which processes personal data on behalf of the controller.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | rocessing_envi ronment other GDPR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 'Processing' means any operation or set of operations that are performed on personal information or on sets of personal information, whether or not by automated means.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | CCPA                                                                                                                   |                                                                                                                        | processor product manager a specialized product management professional whose                                                                                                                                                                                                                                                                                                                                                             | personal data; processing; controller                                            |         |                           |                                                 |\n| job is to manage the planning, development, launch, and success of products/solutions powered by AI, machine learning, and deep learning technologies. [person who is] focused on providing direction and prioritization for the cross- functional AI team, ensuring everyone remains focused on the overall vision and road map. This role is responsible for unifying individuals with diverse skills and                                                                                                                                                                                                                                                                                                                                                                                                                               | productmanag erHQ_Josh_Fe chter Forbes_Tracy _Kemp                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  | product |                           |                                                 |\n| owner                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | product,\" ready to be Towards_Prod uctizing personal data consisting of aspects relating to a natural that natural personal preferences, GDPR 'Profiling' means any form of automated processing of personal further defined by regulations pursuant to paragraph (16) of subdivision Section 1798.185, to evaluate certain personal aspects relating to a and in particular to analyze or predict aspects concerning that natural performance at work, economic situation, health, personal preferences, reliability, behavior, location, or movements.                                                                                                                                                                                                                                                                                                                                                                                                            | information, as (a) of natural person person's interests,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | CCPA                                                                                                                   | Measuring the characteristics of expected activity so that changes to it can be more easily identified. CSRC           | backgrounds toward a common goal. productization [turning the best performing model] into an actual \"data used in live services. profiling 'Profiling' means any form of automated processing of the use of personal data to evaluate certain personal person, in particular to analyse or predict aspects concerning person's performance at work, economic situation, health, interests, reliability, behaviour, location or movements. | personal data; processing                                                        |         |                           |                                                 |\n| into groups caste, and specific. basis for decisions of organizational national constructed to include all the technical characteristics and performances of the new product. The granting of access rights and executional privilege to an agent (human machine) within an application(s) or system(s). A variable that can stand in for another, usually not directly observable measurable, variable. 'Pseudonymisation' means the processing of personal data in such a the personal data can no longer be attributed to a specific data subject the use of additional information, provided that such additional information kept separately and is subject to technical and organisational measures that the personal data are not attributed to an identified or identifiable person; characteristics of a product or service that | whose outcomes should religion. Protected 60 [and] could be chosen values. Some common origin, gender, marital status, MIT_Protecte d_Attributes OECD or IEEE_Guide_I PA or SP1270 manner that without is to ensure natural GDPR 'Pseudonymize' or 'Pseudonymization' means the processing of personal information in a manner that renders the personal information no longer attributable to a specific consumer without the use of additional information, provided that the additional information is kept separately and is subject to technical and organizational measures to ensure that the personal information is not attributed to an identified or identifiable consumer. OECD <data> degree to which the characteristics of data satisfy stated and implied needs when used under specified conditions; <system> degree to which a set of inherent characteristics of an object fulfils requirements (an object can be a product, process or service) | AI_Fairness_3 A group of people with a common characteristic who are legally protected from [...] discrimination on the basis of that characteristic. Protected classes are created by both federal and state law. Practical_Law _protected_cl ass                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | CCPA A                                                                                                                 |                                                                                                                        | protected attribute An attribute that partitions a population have parity. Examples include race, gender, attributes are not universal, but are application protected class [a feature] that may not be used as the because of legal mandates or because protected [classes] include race, religion, age, and socioeconomic status. prototype A prototype is an original model provisioning proxy pseudo-anonymization                    |                                                                                  |         |                           |                                                 |\n| quality The totality of features and ability to satisfy stated or implied needs. racialized A socio-political process by which groups are ascribed or not members of the group self-identify as such ranking a type of machine learning that sorts data in a relevant companies] to optimize search and recommendations. recall A metric for classification models; identifies the frequency classifies the true positive items.                                                                                                                                                                                                                                                                                                                                                                                                          | bear on its a racial identity, whether AAAS_AI_and _Bias_2022- 09 order[; often used by DEV_ranking position, order, or standing within a group : RANK with which a model NSCAI A metric for classification modelsthat answers the following the possible positive labels,how many did the model correctly Recall = True Positive/(True Positive + false Negative) Pattern_Recog a sense of awareness and familiarity experienced when one                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | ISO/IEC_TS_ Merriam-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 5723Ê¼2022(en) Webster_ranki ng question: Out of all identify? That is: aime_measure ment_2022, citing Machine Learning | data management technique to strip identifiers linking data to an individual. NSCAI                                    | (pseudonymization)                                                                                                                                                                                                                                                                                                                                                                                                                        | personal data; processing                                                        |         |                           | correctly                                       |\n| recognition automatic discovery of regularities in data through and with the use of these regularities to the data into different categories. recommendation system techniques that provide suggestion appropriate thing for them by on the user's preferences from a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | the use computer take nition_and_M achine_Learni ng events, or objects that have been material that has been learned in the past. based filtering large Das,_Debashis A subclass of information filtering system 'preference' that a user would give to an social element (e.g. people or group) they built from the characteristics of an item                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | encountered before or when one comes upon on that seek to predict 'rating' or item (such as music, books or movies) or had not yet considered, using a model (content based approaches) or the user's social environment (collaborative filtering approaches)                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Glossary by Google encounters people, APA_recogniti to transfer prior learning or retrieve and reproduce               | past experience to current consciousness: that is, to information; to remember. APA_recall                             | taste to discover new information based rectification An individual's business or other red-team A group of people attack or Red Team's the impacts of defenders (i.e., the Cyber Red Team. reference class A class which is Its instances, called                                                                                                                                                                                        |                                                                                  |         |                           | the algorithms classifying                      |\n| data about it is inaccurate. organized to against an enterprise and by                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | of actions such as on the customer's personalized volume of information                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Sharma,_Lalita                                                                                                         |                                                                                                                        | A software tool and                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                  |         |                           |                                                 |\n| an operational structure and passed by-value by substituting for some primitive reference. of critical thinking that prompts us to consider critically questioning the utility, ethics, and study is a process of predicting the value to a yes continuous spectrum of input values, subcategory                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | them corrected or amended by a IAPP_Privacy_ Glossary emulate a potential adversary's enterprise's security posture. The cybersecurity by demonstrating demonstrating what works for the environment. Also known as CSRC behavior of object identifiers. and indirectly represent IGI_Global_ref erence_class the 'whys' and 'hows' of value of what, whom, and Jamieson_Gov aart_Pownall in qualitative researcher or no label provided it falls of supervised learning. Ranschaert, _Erik                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | research, the self-referential quality of a reflects on the assumptions behind the study influence of his or her own motives, history, and biases on its conduct. the prediction of an exact value using a given set of data learning (RL) is a subset of machine learning that allows an system (sometimes referred to as an agent) in a given environment to its behaviour. Agents learn from feedback signals received as a result actions, such as rewards or punishments, with the aim of maximizing the reward. Such signals are computed based on a given reward function, constitutes an abstract representation of the system's goal. The goal for example, to earn a high video game score or to minimize idle worker | study in which the and especially the APA_reflexivity Saleh_Alkhalifa _ML_in_Biote                                     |                                                                                                                        | right to have personal organization if authorized and exploitation capabilities objective is to improve successful attacks Blue Team) in intended to describe references, are                                                                                                                                                                                                                                                             | positionality                                                                    |         |                           | objects A form research, how we Regression on a |\n| reflexivity regression reinforcement of training algorithms to make behavior over the course of its such as of the initial                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | suitable actions by maximizing actions. This type of learning can take game-playing, which reduces the need NSCAI Reinforcement artificial optimize their received which be, in a factory estimated value(s) to the OECD ability of an under given can be operating Note 2 to reliability, such and organizing, and migrating it to a safe CPO_Magazin                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | of could time item to perform as required, without failure, for a given time interval, conditions. Note 1 to <system> definition: The time interval duration expressed in units appropriate to the item concerned (e.g. calendar time, cycles, distance run, etc.) and the units should always be clearly stated. <system> definition: Given conditions include aspects that affect as: mode of operation, stress levels, environmental conditions,                                                                                                                                                                                                                                                                             | ch TTC6_Taxono my_Terminolo gy ISO/IEC_TS_ 5723Ê¼2022(en)                                                               | property of consistentintended behaviour and results aime_measure ment_2022, citing ISO/IEC 22989                      | place in simulated environments, for real-world data. reliability Reliability refers to the closeness subsequent estimated values. remediation The process of treating data by cleaning, and secure environment for optimized Generally [understood] as a process data. However, the actual process including replacing, updating, or                                                                                                     |                                                                                  |         |                           | learning A method rewarded                      |\n| usage is called involving deleting . . . is very detailed modifying data along getting reproducibility of requirement                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | data remediation. unnecessary or unused and includes several steps, with cleaning it, organizing e_Amar_Kana garaj rement from the model residuals represent MathWorks_R esidual and withstand and to withstand and threats or adverse NISTIR_8269_ Draft <governance> ability to anticipate potentially disruptive event, of a system to maintain its external change, and to degrade and values. This NSCAI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | maintenance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                        | of a system to recover operational conditionquickly following an incident aime_measure ment_2022, citing ISO/IEC 22989 | residual Residuals are differences and the measured output from the portion of the validation resilience The ability to prepare for and recover rapidly from recover from deliberate attacks, incidents. The ability of a system conditions. responsible AI An AI system that aligns includes developing and fielding                                                                                                                     |                                                                                  |         |                           | it, and Closeness measurand something CONDITION |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | from and whether functions and that is consistent                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | adapt to, resist, or quickly recover from a natural or man-made; <system> capability structure in the face of internal and gracefully when this is necessary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                        |                                                                                                                        | data not adapt to disruptions. to development AI                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                  |         |                           |                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | ISO/IEC_TS_ 5723Ê¼2022(en)                                                                                              |                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  |         |                           |                                                 |\n| result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  |         |                           |                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        | with democratic values. The consequential                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                  |         |                           |                                                 |\n| retention limit                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | recover to goals in a manner IEEE_Guide_I PA long-term, and can be Industrial_Net                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        | outcome of completing refers to the amount of information that is                                                                                                                                                                                                                                                                                                                                                                         |                                                                                  |         |                           |                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | technology a process. stored size of the total collected logs in bytes) and time (the are stored for). work_Security _2011                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  |         |                           |                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | (the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  |         |                           |                                                 |\n| measured number of                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  |         |                           |                                                 |\n| in volume months or years that logs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  |         |                           |                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | accidents, or naturally occurring adapt to and and behavior                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  |         |                           |                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Webster_requi between the one-step-predicted output the validation data set. Thus, explained by the model. changing conditions Resilience includes the ability                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  |         |                           |                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | rid of unnecessary data. the agreement between the results of measurements of the same carried out under changed conditions of measurement. IEEE_Soft_Vo cab essential to the existence or occurrence of something else : Merriam-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        |                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  |         |                           |                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                        | ability                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                  |         |                           |                                                 |\n\n<!-- image -->\n\n| Terms Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                              | Citation 2                                                              | 3                                                                             | Citation 3 Definition 4                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 5 Related terms and synonyms Legal definition   |                 |                                 |          |           |         |       |                 |                   |                         |                  |                         |       |        |           |             |             | Definition            |               |                       |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------|------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|-----------------|---------------------------------|----------|-----------|---------|-------|-----------------|-------------------|-------------------------|------------------|-------------------------|-------|--------|-----------|-------------|-------------|-----------------------|---------------|-----------------------|\n| composite measure of an event's probability of occurring and the magnitude degree of the consequences of the corresponding event. The impacts, or of AI systems can be positive, negative, or both and can result in or threats (Adapted from: iso 31000Ê¼2018 ) NIST_AI_RMF _1.0 A measure of the extent to which an entity is threatened by circumstance or event, and typically a function of: (i) the would arise if the circumstance or event occurs; and (ii) the occurrence. at the design, implementation, and evaluation stages [that can be into consideration when developing responsible AI for organizations that security risks (cyber intrusion risks, privacy risks, and open source risk), economic risks (e.g., job displacement risks), and performance risk of errors and bias and risk of black box, and risk of explainability). Toward_an_u nderstanding_ of_responsible _artificial_inte lligence_practi                                                                                                                                                                                                                                                                                                                                                                                                                                                    | a potential adverse impacts that likelihood of                                                                                                                                                                                                                                                                                                                                                                                           | SP800-12 An on                                                          | or condition that, if it occurs, has a positive or negative effect objectives | IEEE_Soft_Vo cab effect of uncertainty on objectives | The or                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | [2] applicable                                  |                 |                                 |          |           |         | event | control         |                   |                         |                  |                         |       |        | risk risk |             |             | uncertain a project's |               |                       |\n| consequences, opportunities mechanisms taken] includes software risks (e.g., ces actor's ... readiness to bear PA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | the or NIST_AI_RMF _1.0 IEEE_Guide_I                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                         |                                                                               |                                                      | risk Risk tolerance refers to the organization's or AI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |                                                 | desktop process | (RDA) (RPA)                     |          | tolerance |         |       |                 |                   | robotic robotic         |                  |                         |       |        |           |             |             |                       |               | automation automation |\n| risk in order to achieve its objectives. Risk tolerance can be influenced by legal regulatoryrequirements. The computer application that makes available to a human operator a suite of predefined activity choreography to complete the execution of processes, activities, transactions, and tasks in one or more unrelated software systems to deliver a result or service in the course of human-initiated or -managed workflow. A preconfigured software instance that uses business rules and predefined activity choreography to complete the autonomous execution of a combination of processes, activities, transactions, and tasks in one or more unrelated systems to deliver a result or service with human exception management. An AI system that is resilient in real-world settings, such as an in lighting. The phrase on AI components. under a variety 5723Ê¼2022(en) reliable adversarially of the mean squared error estimator of the parameter t, then expected error of the estimator. the estimator. Glossary_of_S tatistical_Ter ms a frequently population columns describe properties you have, the more examples Machine_Lear ning_Mastery _Jason_Brown                                                                                                                                                                                                 | software IEEE_Guide_I PA Software to help in the automation of tasks, especially those that are tedious and repetitive. NSCAI object-recognition also refers NSCAI of ISO/IEC_TS_ The ability of a machine learning model/algorithm to maintain correct and performance under different conditions (e.g., unseen, manipulated data). used measure of the differences between values values) predicted by a model or an estimator and the | noisy, or NISTIR_8269_ Draft (sample or values observed Wikipedia_RM SD | aime_measure ment_2022,                                                       |                                                      | application that is robust to significant changes to resilience when it comes to adversarial attacks ability of a system to maintain its level of performance circumstances root-mean-square deviation (RMSD) of an estimator of a parameter[; ...] the square-root (MSE) of the estimator. In symbols, if X is an RMSE(X) = ( E( (X-t)2 ) )Â½. The RMSE of an estimator is a measure of the The units of RMSE are the same as the units of row describes a single entity or observation and the about that entity or observation. The more rows from the problem domain that you have. | root-mean- square error (RMSE)                  |                 |                                 |          |           |         |       | AI              |                   | robust                  |                  |                         |       |        |           |             |             |                       |               | robustness            |\n| lee defined conditions, lead to a environment is endangered; expected harms and the ISO/IEC_TS_ 5723Ê¼2022(en) freedom from resources required to execute a varying volume of tasks, processes, or services. IEEE_Guide_I A continuous value output from a classifier. Applying a threshold to a score results in a predicted label. Screen-out discrimination occurs when 'a disability prevents a job applicant or employee from meeting-or lowers their performance on-a selection criterion, and the applicant or employee loses a job opportunity as a result.'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | risk which is not tolerable PA AI_Fairness_3 60 EEOC_ADA_AI ISO/IEC_TS_ degree to which a product or system (3.38)protects information (3.20) and                                                                                                                                                                                                                                                                                        | citinig ISO/IEC TR 24029-1 aime_measure                                 |                                                                               |                                                      | safety property of a system such that it does not, under state in which human life, health, property, or the [safety involves reducing both the probability of possibility of unexpected harms]. scalability The ability to increase or decrease the computational                                                                                                                                                                                                                                                                                                                     |                                                 |                 |                                 |          | out       |         |       |                 | security          | screen                  |                  |                         | score |        |           |             |             |                       |               |                       |\n| resistance to intentional, unauthorized act(s) designed to cause harm or damage to a system The process of identifying homogeneous subgroups within a data table. A computing platform imbued with sufficient knowledge and analytic capability to make useful conclusions about its inputs, its own processing, and the use of its improvement consistent with its information from sensors, and results to other devices SP1011 operating correctly and, adjustments to restore itself to IEEE_Guide_I PA IEEE_Guide_I PA                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 5723Ê¼2022(en) data (3.11) so that persons or other productsor systems have the degree of data access appropriate to their types and levelsof authorization Raynor IEEE_Guide_I PA                                                                                                                                                                                                                                                        | ment_2022, citing ISO/IEC TR 24029-1                                    |                                                                               |                                                      | segmentation self-aware output so that it is capable of self- judgment and purpose. self-diagnosis Ability of a system to adequately take measurement validate the data, and communicate the processes self-healing A computing system able to perceive that it is not without human intervention, make the necessary normalcy.                                                                                                                                                                                                                                                        |                                                 |                 | system system mapping           |          |           |         |       |                 | semantic          |                         |                  |                         |       |        |           |             |             |                       |               |                       |\n| A strategic schema or framework of metadata labels applied to all data, data groups, data fields, data types, or data content used to introduce new or raw data into a corpus or data fabric to give machine learning algorithms direction for investigating known or potential relationships between data. A semantic map provides a structure for the introduction of new data, information, or knowledge A 'what-if' type of analysis to determine the sensitivity of the outcomes to changes in parameters. If a small change in a parameter results in relatively large changes in the outcomes, the outcomes are said to be sensitive to that parameter. OECD The conversion of typically analog or human sensory perception (e.g., vision, speech) to a digital format useful for machine-to-human interaction or machine processing of traditionally analog sensory information [e.g., optical character recognition (OCR)]. IEEE_Guide_I PA A collection of coordinated processes that takes one or more kinds of input, performs a value-added transformation, and creates an output that fulfills the needs of a customer [or shareholder]. IEEE_Guide_I PA a framework for interpreting data from experiments in which accuracy is Signal_Detecti on_Theory extraction from learning itself. environment and a comprehension of the future status of with that status. | Reznik,_Leon SP800-160 SP1270 system that includes a combination of technical and human or natural elements IEEE_Soft_Vo                                                                                                                                                                                                                                                                                                                 | ISO/IEC_TS_ 5723Ê¼2022(en)                                               |                                                                               |                                                      | theory measured. Techniques that separate the process of feature Perception of elements in the system and/or of their meaning, which could include a projection perceived elements and the uncertainty associated                                                                                                                                                                                                                                                                                                                                                                      |                                                 | detection       | analysis digitization awareness | learning |           | testing |       | socio-technical | software          | sensory service shallow |                  | sensitivity situational |       | signal |           |             |             |                       |               |                       |\n| system how humans interact with technology within the broader societal context NIST Activity in which a system or component is executed under specified conditions, the results are observed or recorded, and an evaluation is made of some aspect of the system or component. cab refers to a matrix of numbers that includes many zeros or values that will not significantly impact a calculation. Dave_Salvator _sparsity A document that specifies, in a complete, precise, verifiable manner, the requirements, design, behavior, or other characteristics of a system or component and often the procedures for determining whether these provisions have been satisfied. SP800-37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | IEEE_Soft_Vo any individual, group, or organization that can affect, be affected by, or perceive itself to be affected by a decision or activity                                                                                                                                                                                                                                                                                         | ISO/IEC_TS_ 5723Ê¼2022(en)                                               |                                                                               |                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                 |                 | deviation                       |          |           |         |       |                 | sparsity standard |                         |                  |                         |       |        |           |             | stakeholder |                       | specification |                       |\n| Individual or organization having a right, share, claim, or interest in a system or in its possession of characteristics that meet their needs and expectations. An individual, group, or organization who may affect, be affected by, or perceive itself to be affected by a decision, activity, or outcome of a project. cab The most widely used measure of dispersion of a frequency distribution introduced by K. Pearson (1893). It is equal to the positive square root of the variance. The standard deviation should not be confused with the root mean square deviation. OECD An activity, task, or input that describes or defines the beginning of a process. IEEE_Guide_I PA to be above or below their as opposed to random error. partiality, or discriminatory SP1270                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                         |                                                                               |                                                      | A systematic tendency for estimates or measurements true values. Statistical biases arise from systematic Statistical bias can occur in the absence of prejudice, intent.                                                                                                                                                                                                                                                                                                                                                                                                              |                                                 |                 |                                 | bias     |           | parity  |       | event           |                   |                         |                  |                         |       |        | start     | statistical |             |                       |               |                       |\n| The independence between the protected attribute and the outcome of the decision rule Besse, _Philippe When the probability of obtaining a statistic of a given size due strictly to random sampling error, or chance, is less than the selected alpha level [or the probability of a type I error]; also represents a rejection of the null hypothesis. Statistics_in_P lain_English data relating to an aggregate of individuals; the science of collecting, and interpreting such data OECD set of cognitive generalizations (e.g., beliefs, expectations) about the qualities characteristics of the members of a group or social category. Stereotypes, schemas, simplify and expedite perceptions and judgments, but they are exaggerated, negative rather than positive, and resistant to revision even perceivers encounter individuals with qualities that are not congruent with stereotype. e adjective 'stochastic' implies the presence of a random variable; e.g. variation is variation in which at least one of the elements is a variate OECD                                                                                                                                                                                                                                                                                                                     | refers to whether a relationship between two or more variables exists beyond a probability expected by chance                                                                                                                                                                                                                                                                                                                            | The_SAGE_En cyclopedia_of _Communicati on_Research_ Methods             |                                                                               |                                                      | statistical statistical                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |                                                 |                 | significance                    |          |           |         |       |                 |                   |                         |                  |                         |       |        |           |             |             |                       |               |                       |\n| Numerical APA_stereotyp a stochastic process is one wherein the system incorporates an element of as opposed to a deterministic system. IEEE_Guide_I                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Contemporary social psychology typically defines stereotypes as mental                                                                                                                                                                                                                                                                                                                                                                   |                                                                         |                                                                               |                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                 |                 |                                 |          |           |         |       |                 |                   |                         |                  |                         |       |        |           |             |             |                       |               |                       |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | representations of a group and its members, and stereotyping as the activity of treating individual elements in terms of higher level categorial properties                                                                                                                                                                                                                                                                              |                                                                         |                                                                               |                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                 |                 |                                 |          |           |         |       |                 |                   |                         |                  |                         |       |        |           |             |             |                       |               |                       |\n| analysing a and like often when the The stochastic and randomness                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                         |                                                                               |                                                      | stereotype stochastic                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                 |                 |                                 |          |           |         |       |                 |                   |                         |                  |                         |       |        |           |             |             |                       |               |                       |\n| The successful execution of a service, process, or transaction performed entirely                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                         |                                                                               |                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                 |                 |                                 |          |           |         |       |                 |                   |                         | straight-through |                         |       |        |           |             |             |                       |               |                       |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                          | Walker_1998                                                             |                                                                               |                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                 |                 |                                 |          |           |         |       |                 |                   |                         |                  |                         |       |        |           |             |             |                       |               |                       |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                         |                                                                               |                                                      | statistics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                 |                 |                                 |          |           |         |       |                 |                   |                         |                  |                         |       |        |           |             |             |                       |               |                       |\n|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                          | Augoustinos_                                                            |                                                                               |                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                 |                 |                                 |          |           |         |       |                 |                   |                         |                  |                         |       |        |           |             |             |                       |               |                       |\n| cognitive                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                         |                                                                               |                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |                                                 |                 |                                 |          |           |         |       |                 |                   |                         |                  |                         |       |        |           |             |             |                       |               |                       |\n\n|                                                                          | Terms Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Citation 1 [1] Definition 2                                                                                                                                                                                     | Citation 2                                              | Definition 3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                   | Citation 3 Definition 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Citation 4                      |                                                                                         | Citation 5                     | Related terms and synonyms Legal definition   |\n|--------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------|-----------------------------------------------------------------------------------------|--------------------------------|-----------------------------------------------|\n| strawperson                                                              | a fallacious argument which irrelevantly attacks a position that appears similar to, but is actually different from, an opponent's position, and concludes that the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Hughes_Laver                                                                                                                                                                                                    |                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 |                                                                                         | [2]                            | applicable                                    |\n|                                                                          | opponent's real position has thereby been refuted. Type of performance efficiency testing conducted to evaluate a test item's                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | y_Critical_Thi nking IEEE_Soft_Vo cab                                                                                                                                                                           |                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 | stress test                                                                             |                                |                                               |\n| structured sub-process                                                   | behavior under conditions of loading above anticipated or specified capacity requirements, or of resource availability below minimum specified requirements Data that has a predefined data model or is organized in a predefined way. A subordinate process that can be included within a parent process. It can be present and/or repeated within other parent processes. A type of machine learning in which the algorithm compares its outputs with the correct outputs during training. In unsupervised learning, the algorithm merely looks for patterns in a set of data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | NIST_1500 IEEE_Guide_I PA Hutson, which develop a mathematical model from the                                                                                                                                   | data and known Reznik,_Leon                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | is                              | data supervised learning                                                                |                                |                                               |\n|                                                                          | A supervised machine learning model for data classification and regression                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | _Matthew Algorithms, desired outputs. Ranschaert,                                                                                                                                                               | input                                                   | For a computer to process a set of data whose attributes have been divided into two groups and derive a relationship between the values of one and the values the other. These two groups are sometimes called predictor and targets, respectively. In statistical terminology, they are called independent and dependent variables. Respectively. The learning Is \"supervised because the distinction between the predictors and the target variables is chosen by the investigator or some other outside agency. | of Raynor                                                                                                         | a general subset of machine learning in which data, like its associated labels, used to train models that can learn or generalize from the data to make predictions, preferably with a high degree of certainty.                                                                                                                                                                                                                                                                                                         | Saleh_Alkhalifa _ML_in_Biote ch | support vector                                                                          |                                |                                               |\n| system                                                                   | analysis. One of the most used classifiers in machine learning. It optimizes the width of the gap between the points of separate categories in feature space. combination of interacting elements organized to achieve one or more stated purposes Systemic biases result from procedures and practices of particular institutions that operate in ways which result in certain social groups being advantaged or favored and others being disadvantaged or devalued. This need not be the result of any conscious prejudice or discrimination but rather of the majority following existing rules or norms.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | _Erik ISO/IEC_TS_ 5723Ê¼2022(en) D. Chandler and R. Munday, A Dictionary of Media and Communicatio n. Oxford University Press, Jan. 2011, publication Title: A Dictionary of                                     |                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 | machines systemic bias                                                                  |                                |                                               |\n|                                                                          | set of systems and system elements that interact to provide a unique capability that none of the constituent systems can accomplish on its own (note: can be necessary to facilitate interaction of the constituent systems in the system systems) a method for solving a problem that an AI algorithm parses its training data                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | ISO/IEC_TS_ 5723Ê¼2022(en)                                                                                                                                                                                       |                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 | system of systems target                                                                | target target                  | variable, value                               |\n|                                                                          | of to find. Once an algorithm finds its target function, that function can be used to predict results (predictive analysis). The function can then be used to find output data related to inputs for real problems where, unlike training sets, outputs are not included.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | TechTarget_ta rget_function                                                                                                                                                                                     |                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 |                                                                                         |                                |                                               |\n|                                                                          | The performance of a discrete activity with a defined start, stop, and outcome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                 |                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 |                                                                                         |                                |                                               |\n| task                                                                     | that cannot be broken down to a finer level of detail.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | IEEE_Guide_I PA Required, recommended, or permissible action, intended to achievement of one or more outcomes of a process                                                                                      | contribute to the IEEE_Soft_Vo cab                      | set of activities undertaken in order to achieve a specific goal                                                                                                                                                                                                                                                                                                                                                                                                                                                   | aime_measure ment_2022, citing ISO/IEC TR 24030                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 |                                                                                         |                                |                                               |\n| technical                                                                | among types and their subtypes. Security controls (i.e., safeguards or countermeasures) for an information that are primarily implemented and executed by the information system mechanisms contained in the hardware, software, or firmware components of the system. The belief that technology is always the solution                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | NIST_SP_800- 30_Rev_1 M. Broussard, Artificial Unintelligence: How Computers                                                                                                                                    |                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 | taxonomy                                                                                |                                |                                               |\n|                                                                          | Taxonomy refers to classification according to presumed natural relationships system through                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | OECD                                                                                                                                                                                                            |                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 | control technochauvinism                                                                |                                | techno- solutionism                           |\n| Test and Verification Validation third party three lines of traceability | requirements, and that it is sufficient for its intended use. an entity that is involved in some way in an interaction that is primarily between two other entities. [Please see note, especially regarding NIST CSRC terms that we might incorporate into this definition.] Most financial institutions follow a three-lines-of-defense model, which separates front line groups, which are generally accountable for business risks (the First Line), from other risk oversight and independent challenge groups (the Second Line) and assurance (the Third Line) Ability to trace the history, application or location of an entity by means of recorded identification. [\"Chain of custody\" is a related term.] Alternatively, traceability is a property of the result of a measurement or the value of a standard whereby it can be related with a stated uncertainty, to stated references, usually national or international standards, i.e. through an unbroken chain of comparisons. In this context, The standards referred to here are measurement standards rather than written standards. A dataset from which a model is learned. | ird_party AIRS_Penn UNODC_Gloss A characteristic of an AI system enabling a person to understand the AI_Fairness_3 60 samples for training used to fit a machine learningmodel                                  | design                                                  | is executedunder conditions, the results are observed or recorded, and an some aspect of the system or component; (2) to conduct anactivity of one or more test cases and procedures.                                                                                                                                                                                                                                                                                                                              | ment_2022, citing 24765                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | is 22                           | test Evaluation,                                                                        | Test, Evaluation, Verification |                                               |\n|                                                                          | Technical operation to determine one or more characteristics of or to evaluate the performance of a given product, material, equipment, organism, physical phenomenon, process or service according to a specified procedure . framework for assessing, incorporating methods and metrics to determine that a technology or system satisfactorily meets its design specifications and                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Misunderstand the World. MIT Press, 2018. UNODC_Gloss ary_QA_GLP any activity aimed at evaluating an attribute or capability of and deteermining that it meets its required results. NSCAI_Report TechTarget_th | a program or system l                                   | William_Hetze (1) activity in which a system or component                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                   | the process of executing a program with the intent of finding errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                    | The_Art_of_S oftware_Testi ng   | and (TEVV) A                                                                            | and (TEVV)                     | Validation                                    |\n|                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | ary_QA_GLP development processes, and operational capabilities (e.g., with transparent auditable methodologies along with documented data sources and procedures).                                              | technology, and NSCAI                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | specified evaluationis made of as in (1); (3) set aime_measure ISO/IEC                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 | defense                                                                                 |                                |                                               |\n| training data transaction                                                | Enactment of a process represented by a set of coordinated activities carried out multiple systems and/or participants in accordance with defined This coordination leads to an intentional, consistent, and verifiable across all participants. technique in machine learning in which an algorithm learns to perform one such as recognizing cars, and builds on that knowledge when learning a but related task, such as recognizing cats. procedure that modifies a dataset. open, comprehensive, accessible, clear and understandable of information; <systems> property of a system or process to imply and accountability                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | IEEE_Guide_I PA Hutson, _Matthew AI_Fairness_3 60 ISO/IEC_TS_ 5723Ê¼2022(en) Understanding the working logic of the model.                                                                                       | aime_measure ment_2022, citing 22989 NISTIR_8269_ Draft | ISO/IEC <organization> property of an organization that decisions are communicated to relevant accessible and understandable manner Note 1 to entry: Inappropriate violate security, privacy or                                                                                                                                                                                                                                                                                                                    | appropriate activities and stakeholders (3.5.13) in a comprehensive, activities and decisions can iso_22989_20 22 | <system> property of a system that appropriate information about the system made available to relevant stakeholders (3.5.13) Note 1 to entry: Appropriate information for system transparency can include aspects such as features, performance, limitations, components, procedures, measures, design goals, design choices and assumptions, data sources and labelling protocols. Note 2 to entry: Inappropriate disclosure of some aspects of a system can violate security, privacy or confidentiality requirements. | iso_22989_20                    | by relationships. result transfer learning A task, different transformer A transparency |                                |                                               |\n| true                                                                     | outcome where the model correctly predicts the negative class. outcome where the model correctly predicts the positive class.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | google_dev_cl assification- true-false- positive- negative google_dev_cl                                                                                                                                        |                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | communication of confidentiality requirements.                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 | <information> presentation openness negative                                            |                                |                                               |\n| true                                                                     | the system status in the mind of human beings based on their perception of experience with the system; concerns the attitude that a person or will help achieve specific goals in a situation characterized by uncertainty and vulnerability. The degree to which an information system (including the information technology components that are used to build the system) can be expected to                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | assification- true-false- positive- negative                                                                                                                                                                    | that a                                                  |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 | positive an                                                                             |                                |                                               |\n| trust                                                                    | and technology preserve the confidentiality, integrity, and availability of the information being                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | DOD_TEVV degree to which a user or other stakeholder has confidence system will behave as intended                                                                                                              | product or aime_measure ment_2022, citing               | ISO/IEC                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 |                                                                                         |                                |                                               |\n|                                                                          | processed, stored, or transmitted by the system across the full range of threats and individuals' privacy.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | SP800-37 Worthy of being trusted to fulfill whatever critical requirements may be for a particular component, subsystem, system, network, application, enterprise, or other entity.                             | TR 24029-1 needed mission, SP800-160                    | ability to meet stakeholders' expectations in a verifiable can be applied to services, products, technology, data and to organizations.                                                                                                                                                                                                                                                                                                                                                                            | ISO/IEC_TS_                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 | trustworthiness                                                                         |                                |                                               |\n|                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                 |                                                         | way; an attribute that information as well as                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 5723Ê¼2022(en)                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                 |                                                                                         |                                |                                               |\n\n<!-- image -->\n\n| Terms                     | Definition 1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Citation 1 [1] Definition 2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Citation 2                                     | Definition 3 Citation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 3 Definition 4                                                                                     | Definition 5   | Citation 5 Related terms and synonyms                    |\n|---------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|----------------|----------------------------------------------------------|\n| trustworthy AI            | Characteristics of trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. _1.0                                                                                                                                                                                                                                                                                                                                                                                                                                            | NIST_AI_RMF Trustworthy AI has three components: (1) it should be lawful, ensuring compliance with all applicable laws and regulations (2) it should be ethical, demonstrating respect for, and ensure adherence to, ethical principles and values and (3) it should be robust, both from a technical and social perspective, since, even with good intentions, AI systems can cause unintentional harm. Trustworthy AI concerns not only the trustworthiness of the AI system itself but also comprises the trustworthiness of all processes and actors that are part of the system's life cycle. | european_ethi cs_2019                          | Trustworthy AI has three components: (1) it should be lawful, ensuring compliance with all applicable laws and regulations (2) it should be ethical, demonstrating respect for, and ensure adherence to, ethical principles and values and (3) it should be robust, both from a technical and social perspective, since, even with good intentions, AI systems can cause unintentional harm. Characteristics of Trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. Trustworthy AI concerns not only the trustworthiness of the AI system itself but also comprises the trustworthiness of all processes and actors that are part of the AI system's life cycle. Trustworthy AI is based on respect for human rights and democratic TTC6_Taxonom y_Terminology |                                                                                                    |                | [2]                                                      |\n| type I error              | The null hypothesis H0 is rejected, even though it is [true]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | berthold_guid e_2020 false positive rate                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | james_statistic al_2014                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| type II error             | The null hypothesis H0 is accepted, even though it is [false]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | berthold_guid true positive rate                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | james_statistic                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| uncertainty underfitting  | Result of not having accurate or sufficient knowledge of a situation; state, even partial, of deficiency of information related to understanding or knowledge of event, its consequence, or likelihood Underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data.                                                                                                                                                                                                                                                                                                                                                  | an IEEE_Soft_Vo cab Ranschaert, _Erik                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| underrepresentation       | inadequately represented. (See note.) impossibility of providing an explanation for certain                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Merriam- Webster_unde rrepresented when members of discernible groups are not consistently present in representative bodies and among measures of well-being in numbers proportionate to their numbers within the population. Roman_V. _Yampolskiy_ Unexplainabilit y                                                                                                                                                                                                                                                                                                                              | roughly Encyclopedia. com_underrep resentation |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| unexplainable             | decisions made by an intelligent system which is both 100% accurate and comprehensible.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                | black box; opacity                                       |\n| unstructured data         | Data that does not have a predefined data model or is not organized in a predefined way                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| unsupervised              | learning A learning strategy that consists in observing and analyzing different entities and determining that some of their subsets can be grouped into certain classes, without any correctness test being performed on acquired knowledge through feedback from external knowledge sources. Note 1 to entry: Once a concept is formed, it is given a name that may be used in subsequent learning of other concepts.                                                                                                                                                                                                                                               | iso_2382_1997                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| usability                 | extent to which a system product or service can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use (note 1Ê¼ The 'specified' users, goals and context of use refer to the particular combination of users, goals and context of use for which usability is being considered; note 2Ê¼ used as a qualifier to refer to the design knowledge, competencies, activities and design attributes that contribute to usability, such as usability expertise, usability professional, usability engineering, usability method, usability evaluation, usability heuristic). [See also: ISO/IEC | ISO/IEC_TS_ 5723Ê¼2022(en)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| usability testing         | refers to evaluating a product or service by testing it with representative users. Typically, during a test, participants will try to complete typical tasks while observers watch, listen and takes notes. The goal is to identify any usability problems, collect qualitative and quantitative data and determine the participant's satisfaction with the product.                                                                                                                                                                                                                                                                                                 | Usabilitygov                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| user user-centered design | individual or group that interacts with a system or benefits from a system during its utilization the practice of the following principles, the active involvement of users for a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | IEEE_Soft_Vo cab A person, organization, or other entity which requests access to and uses the resources of a computer system or network. clear and Vredenburg, _Karel Approach to system design and development that aims to make interactive systems more usable by focusing on the use of the system; applying human                                                                                                                                                                                                                                                                            | CSRC                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| validation                | understanding of user and task requirements, iterative design and evaluation, a multi-disciplinary approach Confirmation by examination and provision of objective evidence that the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | factors, ergonomics and usability knowledge and techniques. IEEE_Soft_Vo cab UNODC_Gloss Confirmation, through the provision of objective evidence, that the requirements IEEE_Soft_Vo                                                                                                                                                                                                                                                                                                                                                                                                             |                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| design                    | particular requirements for a specific intended use are fulfilled. for A variable is a characteristic of a unit being observed that may assume one of a set of values to which a numerical measure or a category from a classification can be assigned.                                                                                                                                                                                                                                                                                                                                                                                                              | ary_QA_GLP for a specific intended use or application have been fulfilled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | cab                                            | provides objective evidence that the capability provided by the system complies with stakeholder performance requirements, achieving its use in its intended operational environment; answers the question, \"Is it the right solution to the problem?\" [C]onsists of evaluating the operational effectiveness, operational suitability, sustainability, and survivability of the system or system elements under operationally realistic conditions.                                                                                                                                                                                                                                                                                                                                                                                                                                                         | DOD_TEVV A continuous monitoring of the process of compilation and of the results of this process. |                | Test and Evaluation, Verification, and Validation (TEVV) |\n| value sensitive           | a theoretically grounded approach to the design of technology that accounts human values in a principled and systematic manner throughout the design process.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Friedman_et_ al_2017                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| variable                  | more than                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | OECD Quantity or data item whose value can change                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | IEEE_Soft_Vo cab                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| variance                  | The variance is the mean square deviation of the variable around the average value. It reflects the dispersion of the empirical values around its                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | OECD A quantifiable deviation, departure, or divergence away from a known baseline expected value ISO/IEC_TS_ 5723Ê¼2022(en) provides evidence that the system or system element performs its intended functions and meets all performance requirements listed in the system                                                                                                                                                                                                                                                                                                                        | or IEEE_Soft_Vo cab                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n| verifiable                | mean. can be checked for correctness by a person or tool                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | performance specification and functional and allocated baselines; answers the question, \"Did you build the system correctly?\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | DOD_TEVV                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                | Test and Evaluation, Verification and Validation (TEVV)  |\n| word embedding            | a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. . . . A word embedding, trained on word co-occurrence in text corpora, represents each word (or common phrase) was a d-dimensional word vector w~ 2 Rd. It serves as a dictionary of sorts for computer programs that would like to use word meaning. First, words with similar semantic meanings tend to have vectors that are close together. Second, the vector differences between words in embeddings have ings                                                                                                       | Bolukbasi_et_ al_Debiasing_ Word_Embedd                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                    |                |                                                          |\n\n| ID                                          | Title of article, chapter, or page                                                                                                                                                                                                              | Author(s) and/or Editor(s) with                                                                                                           | Publication or website (either the main domain or major subdomain)                                                                            | Volume Issue   | Page(s) Year URL                                                                                                                                                                                                                                                                                                                                    |\n|---------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| GDPR CCPA                                   | regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Protection Regulation)                                                                                               | Data                                                                                                                                      |                                                                                                                                               |                | https:/ /eur-lex.europa.eu/eli/reg/2016/679/oj https:/ /leginfo.legislature.ca.gov/faces/codes_displayText.xhtml?division=3.&part=4.&lawCode=CIV&title=1.81.5                                                                                                                                                                                       |\n| AI_Incident_Database                        | California Consumer Privacy Act of 2018 What is an AI incident?                                                                                                                                                                                 | AI Incident Database                                                                                                                      | AI Incident Database                                                                                                                          |                | 2022 https:/ /incidentdatabase.ai/research/1-criteria                                                                                                                                                                                                                                                                                               |\n| Shubendhu_and_Vijay                         | Applicability of Artificial Intelligence in Different Fields of Life                                                                                                                                                                            | Shubhendu, Shukla S. and Jaiswal Vijay                                                                                                    | International Journal of Scientific Engineering and Research (IJSER)                                                                          | 1              | 28-35 2013 https:/ /www.ijser.in/archives/v1i1/MDExMzA5MTU=.pdf                                                                                                                                                                                                                                                                                     |\n| Raynor AI_Fairness_360                      | Glossary of Computer System Software Development Terminology Glossary                                                                                                                                                                           | Raynor, William J., Jr. AI Fairness 360                                                                                                   | The International Dictionary of Artificial Intelligence Fairness 360                                                                          |                | 1999 https:/ /archive.org/details/internationaldic0000rayn https:/ /aif360.mybluemix.net/resources#glossary                                                                                                                                                                                                                                         |\n| Mitchell,_Tom                               | Machine Learning                                                                                                                                                                                                                                | Mitchell, Tom                                                                                                                             | AI Machine Learning                                                                                                                           |                | 1997 http:/ /www.cs.cmu.edu/~tom/mlbook.html                                                                                                                                                                                                                                                                                                        |\n| Brookings_Institution Brownlee,_Jason       | The Brookings glossary of AI and emerging technologies A Gentle Introduction to Generative Adversarial Networks (GANs)                                                                                                                          | Allen, John R. and Darrell M. West                                                                                                        | Brookings Institution                                                                                                                         |                | 2021 https:/ /www.brookings.edu/blog/techtank/2020/07/13/the-brookings-glossary-of-ai-and-emerging-technologies/ 2019 https:/ /machinelearningmastery.com/what-are-generative-adversarial-networks-gans/                                                                                                                                            |\n| Pyle_and_San_JosÃ©                           | An executive's guide to machine learning                                                                                                                                                                                                        | Brownlee, Jason Pyle, Dorian and Cristina San JosÃ©                                                                                        | Machine Learning Mastery                                                                                                                      |                | 2015 https:/ /www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/an-executives-guide-to-machine-learning                                                                                                                                                                                                              |\n| Hutson,_Matthew                             | AI Glossary: Artificial intelligence, in so many words                                                                                                                                                                                          | Hutson, Matthew                                                                                                                           | McKinsey Quarterly Science                                                                                                                    | 357 6346       | 19 2017 https:/ /www.science.org/doi/10.1126/science.357.6346.19                                                                                                                                                                                                                                                                                    |\n| FBPML_Wiki                                  | Definitions                                                                                                                                                                                                                                     | Foundation for Best Practices in Machine Learning                                                                                         | FBPML Wiki                                                                                                                                    |                | https:/ /wiki.fbpml.org/wiki/Definitions                                                                                                                                                                                                                                                                                                            |\n| IAPP_Privacy_Glossary IAPP_Governance_Terms | Glossary of Privacy Terms Glossary of Governance Terms                                                                                                                                                                                          |                                                                                                                                           |                                                                                                                                               |                | https:/ /iapp.org/resources/glossary/                                                                                                                                                                                                                                                                                                               |\n| Reznik,_Leon                                | Introduction I.5 Glossary of Basic Terms                                                                                                                                                                                                        | Reznik, Leon                                                                                                                              | Intelligent Security Systems: How Artificial Intelligence, Machine Learning and Science Work for and Against Computer Security                |                | xv-xxiv 2022                                                                                                                                                                                                                                                                                                                                        |\n| IEEE_Guide_IPA Russell_and_Norvig           | IEEE Guide for Terms and Concepts in Intelligent Process Automation                                                                                                                                                                             | IEEE Standards Association Stuart Russell and Peter Norvig                                                                                | IEEE Guide for Terms and Concepts in Intelligent Process Automation Artificial Intelligence: A Modern Approach (Fourth Edition)               |                | 2021                                                                                                                                                                                                                                                                                                                                                |\n|                                             |                                                                                                                                                                                                                                                 | Schwartz, Reva; Apostol Vassilev; Kristen Greene; Lori Perine; Burt; Patrick Hall                                                         | Andrew                                                                                                                                        |                |                                                                                                                                                                                                                                                                                                                                                     |\n| SP1270 SP1011                               | Towards a Standard for Identifying and Managing Bias in Artificial Intelligence Autonomy Levels for Unmanned Systems (ALFUS) Framework                                                                                                          | Autonomy Levels for Unmanned Systems Working Group Participants                                                                           | NIST Special Publication 1270 NIST Special Publication 1011                                                                                   |                | https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf 2008 https:/ /www.nist.gov/system/files/documents/el/isd/ks/NISTSP_1011-I-2-0.pdf                                                                                                                                                                                           |\n| Gartner Varshney,_Kush                      | Gartner Glossary Trustworthy Machine Learning                                                                                                                                                                                                   | Gartner Group Varshney, Kush R.                                                                                                           |                                                                                                                                               |                | https:/ /www.gartner.com/en/glossary/all-terms                                                                                                                                                                                                                                                                                                      |\n| Munir,_Arslan                               | Artificial Intelligence and Data Fusion at the Edge                                                                                                                                                                                             | Munir, Arslan, Erik Blasch, Jisu Kwon, Joonho Kong, and Alexander Aved                                                                    | IEEE A&E SYSTEMS MAGAZINE                                                                                                                     | 36 7           | 62-78 2021 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9475883                                                                                                                                                                                                                                                                        |\n| Wallace,_Brian                              | Introduction to Artificial Intelligence for Security Professionals                                                                                                                                                                              | Wallace, Brian; Sepehr Akhavan-Masouleh; Andrew Davis; Mike Wojnowicz; John H. Brock                                                      |                                                                                                                                               |                | 2017 http:/ /book.itep.ru/depository/AI/IntroductionToArtificialIntelligenceForSecurityProfessionals_Cylance.pdf                                                                                                                                                                                                                                    |\n| NSCAI                                       | National Security Commission on Artificial Intelligence: The Final Report                                                                                                                                                                       | National Security Commission on Artificial Intelligence Organisation for Economic Co-operation and Development                            | National Security Commission on Artificial Intelligence Final Report                                                                          |                | 2021 https:/ /www.nscai.gov/2021-final-report/ /stats.oecd.org/glossary/                                                                                                                                                                                                                                                                            |\n| OECD                                        | Glossary of Statistical Terms                                                                                                                                                                                                                   |                                                                                                                                           |                                                                                                                                               |                | 2007 https:/ /ec.europa.eu/eurostat/ramon/coded_files/OECD_glossary_stat_terms.pdf / https:/                                                                                                                                                                                                                                                        |\n| OECD_CAI_recommendati on                    | Recommendation of the Council on Artificial Intelligence                                                                                                                                                                                        | OECD                                                                                                                                      | OECD Legal Instruments                                                                                                                        |                | 2019 https:/ /legalinstruments.oecd.org/en/instruments/oecd-legal-0449                                                                                                                                                                                                                                                                              |\n| NISTIR_8269_Draft                           | A Taxonomy and Terminology of Adversarial Machine Learning                                                                                                                                                                                      | Tabassi, Elham;Kevin J. Burns; Michael Hadjimichael; Andres D. Markham; Julian T. Sexton                                                  | Molina- Draft NISTIR 8269                                                                                                                     |                | 2019 https:/ /nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8269-draft.pdf                                                                                                                                                                                                                                                                              |\n| SP800-37 IEEE_Soft_Vocab                    | Risk Management Framework for Information Systems and Organizations: A System Life Cycle Approach for Security and Systems and software engineering -Vocabulary                                                                                 | Privacy Joint Task Force Interagency Working Group                                                                                        | NIST Special Publication 800-37 Revision 2 ISO/IEC/IEEE 24765                                                                                 | 2 (revision    | 2) 2018 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-37r2.pdf 2017 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8016712                                                                                                                                                                                          |\n| Kohavi,_Ron                                 | Glossary of Terms: Special Issue on Applications of Machine Learning and the Knowledge Discovery Process                                                                                                                                        | Kohavi, Ron; Foster Provost                                                                                                               | Machine Learning                                                                                                                              | 30             | 271-274 1998 http:/ /robotics.stanford.edu/~ronnyk/glossary.html                                                                                                                                                                                                                                                                                    |\n| Mitchell,_Tom                               | Machine Learning                                                                                                                                                                                                                                | Mitchell, Tom M.                                                                                                                          | McGraw-Hill Science/Engineering/Math                                                                                                          |                | 1997 https:/ /www.cin.ufpe.br/~cavmj/Machine%20-%20Learning%20-%20Tom%20Mitchell.pdf                                                                                                                                                                                                                                                                |\n| Cyber_Guide CSRC                            | Cyber Security Planning Guide Information Technology Laboratory Computer Security Resource Center Glossary                                                                                                                                      | Federal Communications Commision                                                                                                          | NIST                                                                                                                                          |                | https:/ /www.fcc.gov/sites/default/files/cyberplanner.pdf https:/ /csrc.nist.gov/glossary                                                                                                                                                                                                                                                           |\n| AIMA Breiman_Leo                            | Artificial Inelligence: A Modern Approach Bagging Predictors                                                                                                                                                                                    | Russell, Stuart; Peter Norvig Breiman, Leo                                                                                                | Pearson Machine Learning                                                                                                                      | 24             | 2010 https:/ /zoo.cs.yale.edu/classes/cs470/materials/aima2010.pdf 123-140 1996 https:/ /link.springer.com/content/pdf/10.1007/BF00058655.pdf                                                                                                                                                                                                       |\n| NISTIR_8312                                 | Four Principles of Explainable Artificial 3 Intelligence                                                                                                                                                                                        | Phillips, P. Jonathon; Carina A. Hahn; Peter C. Fontana; David A. Broniatowski; Mark A. Przybocki                                         | Draft NISTIR 8312                                                                                                                             |                | 2020 https:/ /nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8312-draft.pdf                                                                                                                                                                                                                                                                              |\n| SP800-12                                    | An Introductin to Information Security Certified Defenses for Data Poisoning Attacks                                                                                                                                                            | Nieles, Michael; Kelley Dempsey; Victoria Yan Pillitteri                                                                                  | NIST SP 800-12                                                                                                                                |                | 2017 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-12r1.pdf                                                                                                                                                                                                                                                                    |\n| Steinhardt,_Jacob Ranschaert,_Erik          | Artificial Intelligence in Medical Imaging: Opportunities, Applications and Risks                                                                                                                                                               | Steinhardt_Jacob; Pang Wei Koh; Percy Liang Ranschaert, Erik R.; Sergey Morozov; Paul R. Algra                                            | 31st Conference on Neural Information Processing Systems                                                                                      |                | 2017 https:/ /proceedings.neurips.cc/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf 2019 https:/ /link.springer.com/content/pdf/10.1007/978-3-319-94878-2.pdf                                                                                                                                                                           |\n| Blank,_Abagayle_Lee                         | Computer Vision Machine Learning and Future-Oriented Ethics                                                                                                                                                                                     | Blank, Abagayle Lee                                                                                                                       | Springer Seattle Pacific University                                                                                                           |                | https:/ /digitalcommons.spu.edu/cgi/viewcontent.cgi?article=1100&context=honorsprojects                                                                                                                                                                                                                                                             |\n| Crawford,_Kate                              | The Trouble with Bias                                                                                                                                                                                                                           | Crawford, Kate                                                                                                                            | Neural Information Processing Systems, Long Beach                                                                                             |                | 2017 https:/ /www.youtube.com/watch?v=fMym_BKWQzk https:/ /www.coe.int/en/web/artificial-intelligence/glossary                                                                                                                                                                                                                                      |\n| COE_AI_Glossary Kuehn,_Andreas              | Artificial Intelligence Glossary Analyzing Bug Bounty Programs: An Institutional Perspective on the Economics of Software Vulnerabilities                                                                                                       | Kuehn, Andreas; Milton Mueller                                                                                                            | Council of Europe 2014 TPRC Conference Paper                                                                                                  |                | 2014 https:/ /papers.ssrn.com/sol3/papers.cfm?abstract_id=2418812                                                                                                                                                                                                                                                                                   |\n| Kang,_Daniel MathWorks_Residual             | Model Assertions for Monitoring and Improving ML Models                                                                                                                                                                                         | Kang, Daniel; Deepti Raghavan; Peter Baili; Matei Zaharia                                                                                 | 3rd MLSys Conference                                                                                                                          |                | 2020 https:/ /arxiv.org/pdf/2003.01668.pdf                                                                                                                                                                                                                                                                                                          |\n|                                             | What Is Residual Analysis?                                                                                                                                                                                                                      |                                                                                                                                           | MathWorks                                                                                                                                     |                | https:/ /www.mathworks.com/help/ident/ug/what-is-residual-analysis.html                                                                                                                                                                                                                                                                             |\n| Nayak,_Pragati                              | Concept Drift and Model Decay Detection using Machine Learning Algorithm                                                                                                                                                                        | Nayak, Pragati Aravind; Pavithra Sriganesh; Rakshitha K.M; Manoj Kumar M.V; Prashanth B S; Sneha H R                                      | 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)                                                 | Management     | 2021 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9580110                                                                                                                                                                                                                                                                              |\n| Hasan,_Raza                                 | Artificial Intelligence Based Model for Incident Response                                                                                                                                                                                       | Hasan, Raza; Salman Mahmood; Akshyadeep Raghav; M. Asim                                                                                   | Hasan 2011 International Conference on Information Management, Innovation and Industrial Engineering                                          |                | 2011 https:/ /ieeexplore.ieee.org/abstract/document/6114714                                                                                                                                                                                                                                                                                         |\n| McCue_Colleen                               | Data Mining and Predictive Analysis: Intelligence Gathering and Crime Analysis                                                                                                                                                                  | McCue, Colleen Besse, Philippe; Eustasio del Barrio; Paula Gordaliza;                                                                     | Butterworth-Heinemann                                                                                                                         |                | 2007 https:/ /www.sciencedirect.com/topics/computer-science/domain-expertise#:~:text=2.1%20Domain%20Expertise,need%20to%20know%20your% 20stuff.                                                                                                                                                                                                     |\n|                                             | A Survey of Bias in Machine Learning Through the Prism of Statistical Parity                                                                                                                                                                    | Jean-Michel Loubes; Laurent Risser                                                                                                        |                                                                                                                                               | 76 2           | 188-198 2021 https:/ /www.tandfonline.com/doi/full/10.1080/00031305.2021.1952897                                                                                                                                                                                                                                                                    |\n| Besse,_Philippe                             |                                                                                                                                                                                                                                                 | Muller, Michael; Christine T. Wolf; Josh Andres; Michael Desmond; Narendra Nath Joshi; Zahra Ashktorab; Aabhas Sharma; Kristina           | The American Statistician                                                                                                                     |                |                                                                                                                                                                                                                                                                                                                                                     |\n| Muller,_Michael Shalev-Shwartz,_Shai        | Designing Ground Truth and the Social Life of Labels Understanding Machine Learning: From Theory to Algorithms                                                                                                                                  | Brimijoin; Qian Pan; Evelyn Duesterwald; Casey Dugan Shalev-Shwartz, Shai; Shai Ben-David                                                 | Proceedings of the 2021 CHI Conference on Human Factors in Computing System Cambridge Unversity Press                                         |                | 1-16 2021 https:/ /doi.org/10.1145/3411764.3445402 2014 https:/ /www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf                                                                                                                                                                        |\n| SP800-160                                   | Engineering Trustworthy Secure Systems Hard choices in artificial intelligence,                                                                                                                                                                 | Ross, Ron; Mark Winstead; Michael McEvilley Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz,                                             | NIST SP 800-160 https:/ /www.sciencedirect.com/science/article/pii/S0004370221001065?via%                                                     |                | 2022 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-160v1r1.fpd.pdf                                                                                                                                                                                                                                                             |\n| Khanna,_Anirudh                             |                                                                                                                                                                                                                                                 | Khanna, Anirudh; Bishwajeet, Pandey; Kushagra, Vashishta;                                                                                 | 3Dihub                                                                                                                                        |                |                                                                                                                                                                                                                                                                                                                                                     |\n| NBSIR_82-2582                               | A Study of Today's A.I. through Chatbots and Rediscovery of Machine Intelligence An Overview of Computer Vision                                                                                                                                 | Kalia; Bhale, Pradeepkumar; Teerath, Das Gevarter, William B.                                                                             | Kartik, International Journal of u-and e-Service, Science and Technology                                                                      |                | 277-284 2015 http:/ /article.nadiapub.com/IJUNESST/vol8_no7/28.pdf                                                                                                                                                                                                                                                                                  |\n| Kou,_Yufeng                                 |                                                                                                                                                                                                                                                 |                                                                                                                                           |                                                                                                                                               | 8              | https:/ /nvlpubs.nist.gov/nistpubs/Legacy/IR/nbsir82-2582.pdf                                                                                                                                                                                                                                                                                       |\n|                                             | Survey of fraud detection techniques                                                                                                                                                                                                            | Kou, Yufeng; Chang-Tien, Lu; Sirirat, Sirwongwattana; Yo-Ping,                                                                            | NIST 82-2582 Huang IEEE International Conference on Networking, Sensing and Control                                                           | 2              | 1982 749-754 2004 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1297040                                                                                                                                                                                                                                                                 |\n| Merritt,_Linda CRS_AI                       | Human capital management: More than HR with a new name. Artificial Intelligence: Background, Selected Issues, and Policy Considerations                                                                                                         | Merritt, Linda Harris, Laurie A.                                                                                                          | People and Strategy Congressional Research Service                                                                                            | 30 6 7         | 14-16 2007 https:/ /www.proquest.com/docview/224596559?pq-origsite=gscholar&fromopenview=true 2021 https:/ /crsreports.congress.gov/product/pdf/R/R46795 2019 https:/ /www.academia.edu/41261685/A_Review_of_the_Role_of_Marketing_in_Recruitment_and_Talent_Acquisition?from=cover_page                                                            |\n| IJMAE Das,_Debashis                         | A Review of the Role of Marketing in Recruitment and Talent Acquisition. A survey on recommendation system.                                                                                                                                     | Alashmawy, Ahmad; Rashad, Yazdanifard Das, Debashis; Laxman, Sahoo; Sujoy, Datta.                                                         | International Journal of Management, Accounting and Economics International Journal of Computer Applications                                  | 160 7          | 2017 https:/ /www.researchgate.net/profile/Debashis-Das- 17/publication/313787463_A_Survey_on_Recommendation_System/links/5d7de0474585155f1e4de908/A-Survey-on-Recommendation-System.pdf                                                                                                                                                            |\n| Hyndman,_Rob                                | Forecasting: principles and practice Actionable Recourse in Linear Classification                                                                                                                                                               | Hyndman, Rob J.; George, Athanasopoulos Voight,_Paul; von_dem_Bussche,_Axel                                                               | Association of Computing Machinery                                                                                                            |                | 14 2018 https:/ /books.google.com/books? hl=en&lr=&id=_bBhDwAAQBAJ&oi=fnd&pg=PA7&dq=Forecasting&ots=Tij_xmXIMJ&sig=c8LjAcmbLDC5QeH_xQno2l_gTr0#v=onepage&q=Forecasting&f=f alse                                                                                                                                                                     |\n| Ustun,_Berk Voight,_Paul                    | The EU General Data Protection Regulation: A Practical Guide                                                                                                                                                                                    | Ustun, Berk; Spangher, Alexander; Liu, Yang                                                                                               | Springer                                                                                                                                      |                |                                                                                                                                                                                                                                                                                                                                                     |\n| ECOA                                        | 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B)                                                                                                                                                                                  | Consumer Financial Protection Bureau                                                                                                      |                                                                                                                                               |                |                                                                                                                                                                                                                                                                                                                                                     |\n| Gill,_Navdeep ISO_IEC_38507                 | A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrimination Information Technology - Governance of IT - Governance implications of the use of artificial intelligence by organizations | Testing Gill, Navdeep; Hall, Patrick; Montgomery, Kim; Schmidt, Nicholas ISO/IEC                                                          | MDPI ISO                                                                                                                                      |                | 2022 https:/ /www.iso.org/obp/ui/#iso:std:iso-iecÊ¼38507Ê¼ed-1Ê¼v1Ê¼en                                                                                                                                                                                                                                                                                  |\n| Darnell_Coss_Hall                           | The Future of Analytics Guidance on Model Risk Management                                                                                                                                                                                       | Dan Darnell, Rafael Coss, Patrick Hall                                                                                                    | O'Reilly Media Inc.                                                                                                                           |                | Ch. 4 2020 https:/ /www.oreilly.com/library/view/the-future-of/9781492091769/ch04.html 2011 https:/ /www.federalreserve.gov/supervisionreg/srletters/sr1107a1.pdf                                                                                                                                                                                   |\n| Fed_Reserve Jennifer,_Hill                  | Inference: Overview                                                                                                                                                                                                                             | Patrick M. Parkinson Jennifer Hill, Elizabeth                                                                                             | The Federal Reserve International Encyclopedia of the Social & Behavioral Sciences (Second Edition)                                           |                | 255-260 2015 https:/ /www.sciencedirect.com/science/article/pii/B9780080970868420957 2022 https:/ /ai.wharton.upenn.edu/artificial-intelligence-risk-governance/                                                                                                                                                                                    |\n| AIRS_Penn                                   | Causal Artificial Intelligence Risk &                                                                                                                                                                                                           | A. Stuart AIRS                                                                                                                            | The Warton School, University of Pennslyvania                                                                                                 |                |                                                                                                                                                                                                                                                                                                                                                     |\n| ENISA                                       | Governance Privacy by Design                                                                                                                                                                                                                    |                                                                                                                                           | European Union Agency for Cybersecurity International                                                                                         |                | https:/ /www.enisa.europa.eu/topics/data-protection/privacy-by-design                                                                                                                                                                                                                                                                               |\n| Furche,_Tim Enrique                         | Data wrangling for big data: Challenges and opportunities. Towards an integrated crowdsourcing definition.                                                                                                                                      | Furche, Tim; George, Gottlob; Leonid, Libkin; Giorgio, Orsi; Norman, Paton EstellÃ©s-Arolas, Enrique; Fernando, GonzÃ¡lez-LadrÃ³n-de-Guevara | Advances in Database Technology-EDBT 2016Ê¼ Proceedings of the 19th Conference on Extending Database Technology Journal of Information science | 38 2           | 473-478 2016 https:/ /www.research.manchester.ac.uk/portal/files/50447231/paper_94_1_.pdf 189-200 2012 https:/ /journals.sagepub.com/doi/full/10.1177/0165551512437638                                                                                                                                                                              |\n| Behdad                                      | Nature-inspired techniques in the context of fraud detection.                                                                                                                                                                                   | Behdad, Mohammad; Luigi, Barone; Mohammed, Bennamoun; Tim, French. Woodward Jr, John D., Christopher Horn, Julius Gatune, and Aryn        | EEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)                                                          | 42 6           | 1273-1290 2012 https:/ /ieeexplore.ieee.org/abstract/document/6392447?casa_token=-mPJh2Do05MAAAAA: c5Eyg4__i64XlBowloZbwwwKNe0KnqAQvjg5romygO6ymRKDl70np3DiDL0mipMXmdmeboDj1hGwa9I                                                                                                                                                                  |\n| Woodward                                    | Biometrics: A look at facial recognition.                                                                                                                                                                                                       | Thomas                                                                                                                                    | RAND CORP SANTA MONICA CA                                                                                                                     |                | 2003 https:/ /apps.dtic.mil/sti/citations/ADA414520 https:/                                                                                                                                                                                                                                                                                         |\n|                                             |                                                                                                                                                                                                                                                 |                                                                                                                                           |                                                                                                                                               |                | /d1wqtxts1xzle7.cloudfront.net/38584474/IJETT-V4I5P132_1_-with-cover-page-v2.pdf? Expires=1656295756&Signature=QAnv0QzAEvSK2TSOMFygnk1AGUQrGWtz7PD7~bal6kX9SbiWAwd18feH3nB0kH4EQDA0rEPGrm3V9E38s4eY2C5i52NHA- jY0h2zBYAnLETt5PB1cAMjSZf4qaqNWZCPvQtOpaxTkhV6YB1MLDTmaFCAjKcqQfqm6WPQoax5VLh0hIbF- hlc8p5wXiW7fE9z~OeYpmTJld4doW94bG3OrDUY75EN9cptH- |\n| Sharma,_Lalita Å½liobaitÄ—_IndrÄ—              | A survey of recommendation system: Research challenges. A survey on measuring indirect discrimination in machine learning                                                                                                                       | Sharma, Lalita; Anju, Gera Å½liobaitÄ—, IndrÄ—                                                                                               | International Journal of Engineering Trends and Technology (IJETT) CoRR                                                                       | 4 5            | 1989-1992 2013 IHlldQRs~jF1POAkJUO5PL91PKVJatYAjeWQn7eNchB2TszrfnDbs6pjCyPXIIvi1WdnQzkKVN0F6N-CA~YtZ6yGKIxTjDuxdcJh0AtfD2cjxLDvNFCA__&Key-Pair- Id=APKAJLOHF5GGSLRBV4ZA 2018 https:/ /arxiv.org/abs/1511.00148 https:/ /www.morganclaypool.com/doi/abs/10.2200/S00861ED1V01Y201806AIM039?casa_token=bcr3UzYRz6AAAAAA:M-                             |\n| Vorobeychik                                 | Adversarial machine learning                                                                                                                                                                                                                    | Vorobeychik, Yevgeniy; Murat, Kantarcioglu                                                                                                | Synthesis Lectures on Artificial Intelligence and Machine Learning                                                                            | 12 3           | 2018 sh4ANwQuRFYcXk18O4x_x6zu7Qq3P5ZC12MrCgTckeNFm9sXOCAkAFvHtMce7t1A3lpUt4MFA                                                                                                                                                                                                                                                                      |\n| Zhang,_Yonggang                             | Principal component adversarial example                                                                                                                                                                                                         | Zhang,_Yonggang; Xinmei, Tian; Ya, Li; Xinchao, Wang; Dacheng, Scholl, Matthew A.; Kevin M. Stine; Joan Hash; Pauline Bowen; L.           | Tao IEEE Transactions on Image Processing 29                                                                                                  |                | 4804-4815 2020 https:/ /ieeexplore.ieee.org/abstract/document/9018372?casa_token=MFIULJJsFBEAAAAA:GVCQa1Ccivt9ETPlYlBmLvK74iIDNzODP- 6mo2B4o4nkZG69ORI-HliXK0iXb3GG-Q8csl2vxtBorQ                                                                                                                                                                   |\n|                                             | Sp 800-66 rev. 1. an introductory resource for implementing the health insurance portability and accountability act                                                                                                                             | (hipaa) Johnson; Carla Dancy Smith; Daniel I. Steinberg                                                                                   | Arnold                                                                                                                                        |                | 2008 https:/ /dl.acm.org/doi/pdf/10.5555/2206281                                                                                                                                                                                                                                                                                                    |\n| NIST_SP_800                                 | guide security rule                                                                                                                                                                                                                             |                                                                                                                                           | National Institute of Standards & Technology                                                                                                  |                |                                                                                                                                                                                                                                                                                                                                                     |\n\n| Title of article, chapter, or page                                                                                                                                           | Author(s) and/or Editor(s) Jain, Saachi; Hadi Salman; Eric Wong; Pengchuan Zhang; Vibhav                           | Publication or website (either the main domain or major subdomain)                                                                                                                                                                                                                                                 | Page(s) Year URL                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Jain_Saachi Missingness Bias in Model Debugging                                                                                                                              | Sai Vemprala; Aleksander Madry Kristina Lerman;                                                                    | Vineet; ICLR                                                                                                                                                                                                                                                                                                       | 2022 https:/ /arxiv.org/abs/2204.08945                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Mehrabi,_Ninareh A Survey on Bias and Fairness in Machine Learning                                                                                                           | Mehrabi, Ninareh ; Fred Morstatter; Nripsuta Saxena; Aram Galstyan                                                 | ACM Computing Surveys                                                                                                                                                                                                                                                                                              | 1-35 2022 https:/ /dl.acm.org/doi/abs/10.1145/3457607                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Lipton,_Zachary Does mitigating ML's impact disparity require treatment disparity?                                                                                           | Lipton, Zachary C.; Alexandra Chouldechova; Julian McAuley                                                         | 32nd Conference on Neural Information Processing Systems                                                                                                                                                                                                                                                           | 2018 https:/ /doi.org/10.48550/arXiv.1711.07076                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Gustavii,_Ebba A Swedish Grammar for Word Prediction                                                                                                                         | Gustavii, Ebba; Eva Pettersson                                                                                     | Uppsala University - Department of Linguistics                                                                                                                                                                                                                                                                     | 2003 https:/ /www.researchgate.net/publication/2838153_A_Swedish_Grammar_for_Word_Prediction/link/00b4951a5f2645ca02000000/download https:/ /www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-                                                                                                                                                                                                                                                                        |\n| Comptroller_Office Comptroller's Handbook: Model Risk Management, Version 1.0                                                                                                | Office of the Comptroller of the Currency                                                                          | Comptroller's Handbook: Model Risk Management, Version 1.0                                                                                                                                                                                                                                                         | 2021 management.html                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| A Glossary of Common Cybersecurity Words and Phrases Cyber Glossary                                                                                                          | National Initiative for Cybersecurity Careers and Studies The George Washington University                         | Natural Security Archive - The Cyber Vault Project                                                                                                                                                                                                                                                                 | 2022 https:/ /niccs.cisa.gov/cybersecurity-career-resources/glossary#D 2018 https:/ /nsarchive.gwu.edu/cyber-glossary-b                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Sp 800-12. an introduction to computer security: The nist handbook.                                                                                                          | Guttman, Barbara; Edward A. Roback                                                                                 |                                                                                                                                                                                                                                                                                                                    | 1995 https:/ /dl.acm.org/doi/pdf/10.5555/2206203                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Thomas_Edgar Research Methods for Cyber Security                                                                                                                             | Thomas W. Edgar; David O. Manz                                                                                     |                                                                                                                                                                                                                                                                                                                    | 367-392 2017 https:/ /www.sciencedirect.com/science/article/pii/B9780128053492000157                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Understanding Denial-of-Service Attacks Chandrasekaran,_Varun                                                                                                                | Chandrasekaran,_Varun; Kamalika, Chaudhuri; Irene, Giacomelli; Somesh, Jha; Songbai, Yan                           | CISA 29th USENIX Security Symposium (USENIX Security 20)                                                                                                                                                                                                                                                           | https:/ /www.cisa.gov/uscert/ncas/tips/ST04-015 1309-1326 2020 https:/ /www.usenix.org/conference/usenixsecurity20/presentation/chandrasekaran https:/ /www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/inspection-guides/glossary-computer-system-software-                                                                                                                                                                                                                                                     |\n| FDA_Glossary Glossary of Computer System Software Development Terminology                                                                                                    | Meinshausen,                                                                                                       | FDA IEEE Data Science Workshop (DSW)                                                                                                                                                                                                                                                                               | 1995 development-terminology-895#_top https:/ /ieeexplore.ieee.org/abstract/document/8439889?casa_token=8tga5hcskjQAAAAA:                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| Meinshausen,_Nicolai Causality from a distributional robustness point of view                                                                                                | Nicolai                                                                                                            | 2018                                                                                                                                                                                                                                                                                                               | 6-10 2018 PlpvmJtJNGKSNXiBdghGmy670MxN91PXc3ekZRVEXwOcLoJkh0sxDuWseVzr0EowGRCf8WR-eYLWeU4 https:/ /ieeexplore.ieee.org/abstract/document/9234592?casa_token=RIQcYxte8lIAAAAA:                                                                                                                                                                                                                                                                                                                                                               |\n| Stacke,_Karin Measuring domain shift for deep learning in histopathology.                                                                                                    | Stacke, Karin; Gabriel, Eilertsen; Jonas, Unger; Claes, LundstrÃ¶m                                                  | IEEE journal of biomedical and health informatics NASA Standard                                                                                                                                                                                                                                                    | 325-336 2020 4t6IwNomEw95f3c1ir73BRReG7OzKecUzpVQS_Bk5zIEWA5R75uG-66g9irlblzDDVwu7ut4jAo2i_8                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| NASA_Soft_Standards Software Assurance and Software Safety Standard PET_Handbook Handbook of privacy and privacy-enhancing technologies.                                     | Van Blarkom, G. W.; John J. Borking; JG Eddy Olk                                                                   | Privacy Incorporated Software Agent (PISA) Consortium, The Hague 198                                                                                                                                                                                                                                               | 2020 https:/ /standards.nasa.gov/standard/nasa/nasa-std-87398 2003 https:/ /andrewpatrick.ca/pisa/handbook/Handbook_Privacy_and_PET_final.pdf                                                                                                                                                                                                                                                                                                                                                                                               |\n| NIST_1500 NIST Big Data Interoperability Framework Microsoft_Azure_documen                                                                                                   | Wo L. Chang; Nancy Grady                                                                                           | NIST                                                                                                                                                                                                                                                                                                               | 2019 https:/ /www.nist.gov/publications/nist-big-data-interoperability-framework-volume-1-definitions?pub_id=918927                                                                                                                                                                                                                                                                                                                                                                                                                         |\n| tation_Detect_data_drift Detect data drift (preview) on datasets Egnyte                                                                                                      | Microsoft Egnyte                                                                                                   | Azure documentation Data Control: Definition and Benefits                                                                                                                                                                                                                                                          | 2022 https:/ /docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-datasets?tabs=python 2022 https:/ /www.egnyte.com/guides/governance/data-control                                                                                                                                                                                                                                                                                                                                                                               |\n| IG1190M_AIOps_Decommis sion_v1.0.0 IG1190M AIOps Decommission v1.0.0                                                                                                         | AI Operations                                                                                                      | IG1190M AIOps Decommission v1.0.0                                                                                                                                                                                                                                                                                  | 2022 https:/ /www.tmforum.org/resources/reference/ig1190m-aiops-decommission-v1-0-0/                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| AI decision making: the future of business intelligence Hochreiter,_Sepp Toward a Broad AI                                                                                   | Peak.AI Hochreiter, Sepp                                                                                           | AI decision making: the future of business intelligence Communications of the ACM                                                                                                                                                                                                                                  | 2022 https:/ /peak.ai/hub/blog/ai-decision-making-the-future-of-business-intelligence/ 56-57 2022 https:/ /cacm.acm.org/magazines/2022/4/259402-toward-a-broad-ai/fulltext                                                                                                                                                                                                                                                                                                                                                                  |\n| Mitchell,_Eric Memory-Based Model Editing at Scale.                                                                                                                          | Mitchell, Eric; Charles, Lin; Antoine, Bosselut; Christopher D., Chelsea, Finn                                     | Manning; International Conference on Machine Learning, PMLR                                                                                                                                                                                                                                                        | 15817-15831 2022 https:/ /proceedings.mlr.press/v162/mitchell22a.html                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| ISO/IEC_TS_5723Ê¼2022(en) ISO/IEC TS 5723Ê¼2022(en) Trustworthiness -Vocabulary                                                                                                | ISO/IEC H20.ai                                                                                                     | ISO/IEC H20.ai                                                                                                                                                                                                                                                                                                     | 2022 https:/ /www.iso.org/obp/ui/#iso:std:iso-iec:tsÊ¼5723Ê¼ed-1Ê¼v1Ê¼en 2022 https:/ /docs.h2o.ai/h2o/latest-stable/h2o-docs/glossary.html                                                                                                                                                                                                                                                                                                                                                                                                     |\n| H20.ai_glossary H20.ai Glossary                                                                                                                                              |                                                                                                                    |                                                                                                                                                                                                                                                                                                                    | 2017 https:/ /www.techtarget.com/searchenterpriseai/definition/AI-washing                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| TechTarget_Ivy_Wigmore AI Washing                                                                                                                                            | Ivy Wigmore                                                                                                        | TechTarget                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| _Yampolskiy_Unexplainabil Unexplainability and Incomprehensibility of Artificial Intelligence Remember 'Cloud Washing'? It's Happening In RegTech                            | Roman V. Kayvan Alikhani Brian D. Ripley                                                                           | PhilArchive Forbes                                                                                                                                                                                                                                                                                                 | 2019 https:/ /philarchive.org/archive/YAMUAI 2019 https:/ 1996 https:/                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Forbes_Kayvan_Alikhani Ripley,_Brian Introduction and Examples                                                                                                               | Yampokskiy                                                                                                         | Pattern Recognition and Neural                                                                                                                                                                                                                                                                                     | /www.forbes.com/sites/forbestechcouncil/2019/10/14/remember-cloud-washing-its-happening-in-regtech/?sh=2a52bd3a796c 6 /archive.org/details/patternrecogniti0000ripl/page/6/mode/2up?q=training&view=theater                                                                                                                                                                                                                                                                                                                                 |\n| C3.ai_Model_Training Model Training                                                                                                                                          | C3.ai                                                                                                              | Networks C3.ai Glossary                                                                                                                                                                                                                                                                                            | https:/ /c3.ai/glossary/data-science/model-training/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| _Dorian_Data_Preparation _as_a_Process Data Preparation as a Process                                                                                                         | Dorian Pyle                                                                                                        | Data Preparation for Data Mining                                                                                                                                                                                                                                                                                   | 89-124 1999 https:/ /www.google.com/books/edition/Data_Preparation_for_Data_Mining/hhdVr9F-JfAC?hl=en&gbpv=1&dq=Binning%20is%20a% 20technique&pg=PA110&printsec=frontcover https:/ /www.google.com/books/edition/Design_Science_Methodology_for_Informati/xLKLBQAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                                                                                    |\n| Wieringa,_Roel_J. Conceptual Frameworks                                                                                                                                      | Roel J.                                                                                                            | Design Science Methodology for Information Systems and Software                                                                                                                                                                                                                                                    | 73-92 2014 22Construct+validity+is+defined+by+Shadish+et+al+24+p+506+as+the+degree+to+which+inferences+from+phenomena+to+constructs+are+warranted% 22&pg=PA87&printsec=frontcover 2022 https:/ /www.internetsociety.org/blog/2022/03/what-is-the-splinternet-and-why-you-should-be-paying-attention/                                                                                                                                                                                                                                        |\n| _Dan_Internet_Society What Is the Splinternet? And Why You Should Be Paying Attention                                                                                        | Wieringa Dan York                                                                                                  | Engineering Internet Society                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Splinternets Key Definitions                                                                                                                                                 | European Parliament, Directorate-General for Parliamentary Services, Perarnaud, C., Rossi, J., Musiani, F., et al. | Research 'Splinternets': Addressing the Renewed Debate on Internet Fragmentation                                                                                                                                                                                                                                   | 2022 https:/ /op.europa.eu/en/publication-detail/-/publication/5a5bfaed-0d52-11ed-b11c-01aa75ed71a1/language-en 2016 https:/ /www.google.com/books/edition/Deep_Learning/omivDQAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                                                                                                                                                                     |\n| Regularization_for_Deep_ Regularization for Deep Learning                                                                                                                    | Ian Goodfellow, Yoshua Bengio, and Aaron Courville                                                                 |                                                                                                                                                                                                                                                                                                                    | 221-266                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Statistics_in_Plain_English                                                                                                                                                  | Timothy C. Urdan                                                                                                   | Deep Learning Statistics in Plain English                                                                                                                                                                                                                                                                          | 22Sparsity+in+this+context+refers+to+the+fact+that+some+parameters+have+an+optimal+value+of+zero%22&pg=PA229&printsec=frontcover 43-56 2001 https:/ /www.google.com/books/edition/Statistics_in_Plain_English/MwCK52n_wBEC?hl=en&gbpv=1&dq=% 22Statistical+significance+When+the+probability+of+obtaining+a+statistic+of+a+given+size+due+strictly+to%22&pg=PA55&printsec=frontcover                                                                                                                                                        |\n| Statistical Significance and Effect Size The_SAGE_Encyclopedia_ ch_Methods Between Variables                                                                                 | Mike Allen, ed.                                                                                                    | The SAGE Encyclopedia of Communication Research Methods, Volume 1                                                                                                                                                                                                                                                  | 2017 https:/ /www.google.com/books/edition/The_SAGE_Encyclopedia_of_Communication_R/4GFCDgAAQBAJ?hl=en&gbpv=1&dq=% 22Statistical+significance+refers+to+whether+a+relationship+between+two+or+more+variables+exists+beyond+a+probability+expected+by+chance% 22&pg=PA1413&printsec=frontcover https:/ /www.google.com/books/edition/The_Science_of_Algorithmic_Trading_and_P/FKPND2zz9OoC?hl=en&gbpv=1&dq=% 22Back+testing+is+the+quantitative+evaluation+of+a+model%E2%80%99s+performance+both+from+a+statistical+and+trading+perspective% |\n| of_Communication_Resear Relationships                                                                                                                                        |                                                                                                                    |                                                                                                                                                                                                                                                                                                                    | 2013 22&pg=PA446&printsec=frontcover                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| The_Science_of_Algorithm ic_Trading_and_Portfolio_ Management Evaluation                                                                                                     | Robert Kissell                                                                                                     | The Science of Algorithmic Trading and Portfolio Management                                                                                                                                                                                                                                                        | https:/ /www.google.com/books/edition/Introduction_to_Information_Systems/Y75VEAAAQBAJ?hl=en&gbpv=1&dq=% 22An+autonomous+vehicle+automobile+bus+tractor+combine+boat+forklift+etc+is+a+vehicle+capable+of+sensing+its+environment+and+moving+safely+wi                                                                                                                                                                                                                                                                                      |\n| on_Systems Artificial Intelligence OED_snake_oil Snake oil What Is Rounding? What Is Stochastic Rounding?                                                                    | R. Kelly Rainer, Reiner R. Kelly, and Brad Prince Oxford English Dictionary Nick Higham                            | Introduction to Information Systems: International Adaptation Oxford English Dictionary Nick Higham                                                                                                                                                                                                                | 410-450 2022 th+little+or+no+human+input%22&pg=PA437&printsec=frontcover 2022 https:/ /www-oed-com.proxygw.wrlc.org/view/Entry/95490133?redirectedFrom=%22snake+oil%22 2020 https:/ /nhigham.com/2020/04/28/what-is-rounding/                                                                                                                                                                                                                                                                                                               |\n| Introduction_to_Informati                                                                                                                                                    |                                                                                                                    |                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Nick_Higham_1 pwc_Model_Risk_Manage                                                                                                                                          | Nick Higham                                                                                                        | Nick Higham                                                                                                                                                                                                                                                                                                        | 2020 https:/ /nhigham.com/2020/07/07/what-is-stochastic-rounding/                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| Nick_Higham_2 of AI and Machine                                                                                                                                              |                                                                                                                    |                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| ment_of_AI_and_ML_Syst Model Learning Systems                                                                                                                                | PricewaterhouseCoopers                                                                                             | Model Risk Management of AI and Machine Learning Systems In: Bui, T.X., (ed.) Proceedings of the 53rd Hawaii International Conference on System Sciences. Hawaii International Conference on System Sciences (HICSS 2020), 20202- 01-07 - 2020-01-10, Maui, Hawaii, USA. Hawaii International Conference on System | 2020 https:/ /www.pwc.co.uk/data-analytics/documents/model-risk-management-of-ai-machine-learning-systems.pdf                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| Risk Management Toward_an_understanding _of_responsible_artificial_ intelligence_practices understanding of responsible artificial intelligence practices                    | Yichuan Wang, Mengran Xiong, and Hossein G. T.                                                                     | Sciences (HICSS) Comparing scores and reason codes in credit scoring systems: NeuroDecision vs.                                                                                                                                                                                                                    | 2020 https:/ /eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf 2020 https:/ /assets.equifax.com/marketing/US/assets/comparing_scores_whitepaper.pdf                                                                                                                                                                                                                                                                                                              |\n| Toward an Comparing_scores_and_re ason_codes Comparing scores and reason codes in credit scoring systems: NeuroDecision vs. unconstrained neural networks                    | Olya Equifax                                                                                                       | unconstrained neural networks                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Machine_Learning_Interpr Learning Interpretability with H20 Driverless AI                                                                                                    | Patrick Hall, Navdeep Gill, Megan Kurka, and Wen edited Angela Bartz                                               | by                                                                                                                                                                                                                                                                                                                 | 2022 https:/ /docs.h2o.ai/driverless-ai/latest-stable/docs/booklets/MLIBooklet.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n|                                                                                                                                                                              | Phan; Christopher M. Bishop                                                                                        | Machine Learning Interpretability with H20 Driverless AI                                                                                                                                                                                                                                                           | 2006                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| etability_with_H20_Driverl Machine Pattern_Recognition_and_ Machine_Learning Introduction DOD_TEVV Technology Investment Strategy 2015-2018                                  | United States Department of Defense's Test and Evaluation, Verification and Validation (TEVV) Working Group        | Pattern Recognition and Machine Learning Technology Investment Strategy 2015-2018                                                                                                                                                                                                                                  | 1-66 2015 https:/ /defenseinnovationmarketplace.dtic.mil/wp-content/uploads/2018/02/OSD_ATEVV_STRAT_DIST_A_SIGNED.pdf 216-250 2016 https:/ 22Audit+logs+Defined+events+that+provide+additional+input+to+audit+activities%22&pg=PA233&printsec=frontcover                                                                                                                                                                                                                                                                                    |\n| Fundamentals_of_Informat ion_Systems_Security Auditing, Testing, and Monitoring AI_Ethics_Mark_Coeckelb Glossary                                                             | David Kim and Michael G. Solomon                                                                                   | Fundamentals of Information Systems Security                                                                                                                                                                                                                                                                       | /www.google.com/books/edition/Fundamentals_of_Information_Systems_Secu/Yb4eDQAAQBAJ?hl=en&gbpv=1&dq=% https:/                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|                                                                                                                                                                              |                                                                                                                    | AI Ethics                                                                                                                                                                                                                                                                                                          | 203-206 2020 /www.google.com/books/edition/AI_Ethics/Gs_XDwAAQBAJ?hl=en&gbpv=1&dq=%22Trustworthy+AI+Al+that+can+be+trusted+by+humans% 22&pg=PA206&printsec=frontcover                                                                                                                                                                                                                                                                                                                                                                       |\n| of Blockchain                                                                                                                                                                | Mark Coeckelbergh Matt Zand, Xun (Brian) Wu, and Mark                                                              |                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Concepts function                                                                                                                                                            | Morris                                                                                                             | Hands-On Smart Contract Development with Hyperledger V2                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| On_Smart_Contract_Dev Fundamental TechTarget_target_functio target IGI_Global_reference_clas What is Reference Class                                                         | Anthony TechTarget                                                                                                 | Fabric TechTarget IGI Global                                                                                                                                                                                                                                                                                       | 2018 https:/ /www.techtarget.com/whatis/definition/target-function https:/ /www.igi-global.com/dictionary/reference-class/35564                                                                                                                                                                                                                                                                                                                                                                                                             |\n| CPO_Magazine_Amar_Kan Data Remediation and Its Role in Data Security and Privacy learning (ML) applications: ranking                                                         | IGI Global CPO Magazine                                                                                            | CPO Magazine DEV Community                                                                                                                                                                                                                                                                                         | 2022 https:/ 2022 https:/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| DEV_ranking Do? Intelligence and Big Data                                                                                                                                    | DEV                                                                                                                |                                                                                                                                                                                                                                                                                                                    | /dev.to/mage_ai/machine-learning-ml-applications-ranking-238d                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| of Artificial                                                                                                                                                                | Community Product Manager HQ                                                                                       | Product Manager HQ                                                                                                                                                                                                                                                                                                 | /www.cpomagazine.com/data-protection/data-remediation-and-its-role-in-data-security-and-privacy/ https:/ /productmanagerhq.com/ai-product-manager/                                                                                                                                                                                                                                                                                                                                                                                          |\n| Machine productmanagerHQ_Josh_ What Does an AI Product Manager Proxy Discrimination in the Age Four Skills Every Successful AI Product Owner Should Possess underrepresented | Anya E. R. Prince; Daniel Tracy Kemp                                                                               | Iowa Law Review Forbes                                                                                                                                                                                                                                                                                             | 2020 https:/ /ilr.law.uiowa.edu/print/volume-105-issue-3/proxy-discrimination-in-the-age-of-artificial-intelligence-and-big-data 2021 https:/ /www.forbes.com/sites/forbestechcouncil/2021/08/24/four-skills-every-successful-ai-product-owner-should-possess/?sh=65dbfe423d3d                                                                                                                                                                                                                                                              |\n| Proxy_Discrimination Forbes_Tracy_Kemp Webster_underrepresented HBR_Andrew_Burt_how_t                                                                                        | Schwarcz Merriam-Webster                                                                                           | Merriam-Webster Dictionary                                                                                                                                                                                                                                                                                         | https:/ /www.merriam-webster.com/dictionary/underrepresented                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| How to Ensure Your AI Doesn't Discriminate Understanding and Avoiding Adverse Impact in Employment Practices                                                                 | Andrew Burt                                                                                                        | Harvard Business Review Cadient                                                                                                                                                                                                                                                                                    | https:/ /hbr.org/2020/08/how-to-ensure-your-ai-doesnt-discriminate https:/ /cadienttalent.com/resources/understanding-and-avoiding-adverse-impact                                                                                                                                                                                                                                                                                                                                                                                           |\n| Algorithm-in-the-Loop Decision Making                                                                                                                                        | Michael Baysinger; Kristin                                                                                         | The Thirty-Fourth AAAI Conference on Artificial                                                                                                                                                                                                                                                                    | https:/ /ojs.aaai.org/index.php/AAAI/article/view/7115                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Cadient_EEOC                                                                                                                                                                 | Worrell Ben Green; Yiling Chen                                                                                     | Intelligence (AAAI-20)                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Ben_Green_Yiling_Chen Personal data vs Sensitive Data: What's the Difference? Stereotype                                                                                     | Luke Irwin                                                                                                         | IT Governance Blog Oxford English Dictionary                                                                                                                                                                                                                                                                       | https:/ https:/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| IT_Governance_Blog_Luke OED_stereotype Machine Learning Terminology from Statistics and Computer Science                                                                     | Oxford English                                                                                                     |                                                                                                                                                                                                                                                                                                                    | /www.itgovernance.co.uk/blog/the-gdpr-do-you-know-the-difference-between-personal-data-and-sensitive-data /www-oed-com.proxygw.wrlc.org/view/Entry/189956?rskey=JmZ8YE&result=1#eid https:/                                                                                                                                                                                                                                                                                                                                                 |\n| Machine_Learning_Master                                                                                                                                                      |                                                                                                                    | Learning Mastery                                                                                                                                                                                                                                                                                                   | 2016 /machinelearningmastery.com/data-terminology-in-machine-learning/#:~:text=Row%3A%20A%20row%20describes%20a,problem%20domain% 20that%20you%20have.                                                                                                                                                                                                                                                                                                                                                                                      |\n| y_Jason_Brownlee Testimony and Statement for the Record of Christopher Gilliard, PhD. Hearing on \"Banking on Your                                                            | Dictionary Jason Brownlee                                                                                          | Machine Prepared Testimony and Statement for the Record of Christopher Gilliard, PhD.                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Data:                                                                                                                                                                        | of Big                                                                                                             | Hearing on \"Banking on Your Data: the Role of Big Data in Financial Services\" before the House Financial Services Committee Task Force on Financial Technology                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| the Financial Services\" before the House Financial Services Committee Task Force on Financial Technology                                                                     | Christopher Gilliard                                                                                               |                                                                                                                                                                                                                                                                                                                    | https:/ /www.congress.gov/116/meeting/house/110251/witnesses/HHRG-116-BA00-Wstate-GillardC-20191121.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Prepared Data in                                                                                                                                                             |                                                                                                                    |                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|                                                                                                                                                                              | Role                                                                                                               |                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Banking_on_Your_Data_C hristopher_Gilliard                                                                                                                                   |                                                                                                                    |                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n\n| ID Title of article, chapter, or page Merriam-                                                                                                                                           | Author(s) and/or Editor(s)                                                                                                                                   | Publication or website (either the main domain or major subdomain)                                                               | Issue Page(s) Year URL                                                                                                                                                                                                                                                                                                                                         |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Webster_pseudoscience pseudoscience                                                                                                                                                      | Merriam-Webster                                                                                                                                              | Merriam-Webster Dictionary                                                                                                       | https:/ /www.merriam-webster.com/dictionary/pseudoscience https:/ /www.google.com/books/edition/Cost_Management/HhQcEAAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                                                 |\n| Cost_Management_ch15 Lean Accounting and Productivity Measurement                                                                                                                        | Don R. Hansen; Maryanne M. Mowen; Dan L. Heitger                                                                                                             | Cost Management                                                                                                                  | 2021 22Velocity+has+to+do+with+how+fast+a+product+can+be+delivered+to+the+market+and+quality+is+concerned+with+providing+a+nondefective+product+ with+the+desired+features+to+customers%22&pg=PA783&printsec=frontcover                                                                                                                                        |\n| Towards_Productizing Towards Productizing AI/ML Models: An Industry Perspective from Data Scientists                                                                                     | Filippo Lanubile, Fabio Calefato, Luigi Quaranta, Maddalena Fabio Fumarola, and Michele Filannino                                                            | Amoruso, arXiv                                                                                                                   | https:/ /arxiv.org/abs/2103.10548                                                                                                                                                                                                                                                                                                                              |\n| The_Art_of_Software_Tes Program Testing                                                                                                                                                  |                                                                                                                                                              |                                                                                                                                  | 1979 https:/ /archive.org/details/artofsoftwaretes0000myer/page/4/mode/2up                                                                                                                                                                                                                                                                                     |\n| ting The Psychology and Economics of William_Hetzel An Introduction                                                                                                                      | Glenford J. Myers William C. Hetzel                                                                                                                          | The Art of Software Testing The Complete Guide to Software Testing , 2nd edition                                                 | 1988 https:/ /archive.org/details/completeguidetos0000hetz/page/6/mode/2up?view=theater                                                                                                                                                                                                                                                                        |\n| On_Hyperparameter_Opti mization On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice                                                                       | Li Yang and Abdallah Shami                                                                                                                                   | arXiv                                                                                                                            | https:/ /arxiv.org/pdf/2007.15745.pdf                                                                                                                                                                                                                                                                                                                          |\n| Security_Analysis_of_Subj Security Analysis of Subject Access Request Procedures: How to Authenticate Data Subjects Safely When                                                          | Their Coline Boniface, Imane Fouad, Nataliia Bielova, CÃ©dric                                                                                                 | Lauradoux, and Privacy Technologies and Policy (7th Annual Privacy Forum, APF 2019, Rome, Italy,                                 | https:/ /www.google.com/books/edition/Privacy_Technologies_and_Policy/SW2cDwAAQBAJ?hl=en&gbpv=1&dq=% 22i+Impersonation+data+breach+A+malicious+individual+is+able+to+impersonate+a+legitimate+data+subject+to+the+data+controller%                                                                                                                             |\n| ect_Access Data Threat                                                                                                                                                                   | Cristiana Santos Philip A. Legg, Oliver Buckley, Michael Goldsmith, and Sadie                                                                                | 13-14, 2019, Proceedings) Homeland Security (HST                                                                                 | 2019 22&pg=PA186&printsec=frontcover 2015 https:/ /ieeexplore-ieee-org.proxygw.wrlc.org/stamp/stamp.jsp?tp=&arnumber=7446229                                                                                                                                                                                                                                   |\n| IEEE_Caught_in_the_Act Caught in the Act of an Insider Attack: Detection and Assessment of Insider Moradi_Samwald Post-hoc explanation of black-box classifiers using confident itemsets | Milad Moradi; Matthias Samwald                                                                                                                               | Creese 2015 IEEE International Symposium on Technologies for Expert Systems with Applications                                    | 2021 https:/ /reader.elsevier.com/reader/sd/pii/S0957417420307302? token=858E09A321B5727ECF889090AFB0B943768A26AFE15FB920B20176BD22F82449CE42841047FD99C51A01659C2ECE9695&originRegion=us-east- 1&originCreation=20220918190335#b0035                                                                                                                          |\n| Mind_on_Statistics Chapter 4                                                                                                                                                             | Jessica M. Utts and Robert F. Heckard                                                                                                                        |                                                                                                                                  | 2021 https:/ /www.google.com/books/edition/Mind_on_Statistics/npQMEAAAQBAJ?hl=en&gbpv=1&dq=% 22Practical+Versus+Statistical+Significance+Statistical+significance+does+not+necessarily+mean+that+the+relationship+between+the+two+variables+has+ practical+significance%22&pg=PA138&printsec=frontcover                                                        |\n| Jenna_Burrell How the machine 'thinks': Understanding opacity in machine learning algorithms                                                                                             | Jenna Burrell                                                                                                                                                | Mind on Statistics (6th Edition) Big Data & Society                                                                              | 1-12 2016 https:/ /journals.sagepub.com/doi/pdf/10.1177/2053951715622512 https:/ /www.google.com/books/edition/Critical_Thinking_Concise_Edition/k0idCgAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                |\n| Hughes_Lavery_Critical_T hinking Glossary                                                                                                                                                | William Hughes and Jonathan Lavery                                                                                                                           | Critical Thinking - Concise Edition                                                                                              | 2015 222+7+2+straw+man+a+fallacious+argument+which+irrelevantly+attacks+a+position+that+appears+similar+to+but+is+actually+different+from+an+opponen t%E2%80%99s+position+and+concludes+that+the+opponent%E2%80%99s+real+position+has+thereby+been+refuted%22&pg=PA271&printsec=frontcover                                                                     |\n| NIST_SP_800-30_Rev_1 NIST Special Publication 800-30 Revision 1Ê¼ Guide for Conducting Risk Assessments TechTarget_third_party third party                                                | NIST TechTarget                                                                                                                                              | NIST Special Publication 800-30 Revision 1Ê¼ Guide for Conducting Risk Assessments TechTarget Glossary                            | 2012 https:/ /nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-30r1.pdf 2014 https:/ /www.techtarget.com/whatis/definition/third-party                                                                                                                                                                                                            |\n| Law_Insider_processing_e                                                                                                                                                                 | Insider Law                                                                                                                                                  |                                                                                                                                  | https:/ /www.lawinsider.com/dictionary/processing-environment                                                                                                                                                                                                                                                                                                  |\n| nvironment Processing Environment wachter_counterfactual_2 018 'Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR.'                            | Wachter, S., B. D. M. Mittelstadt, and C. Russell.                                                                                                           | Harvard Journal of Law and Technology                                                                                            | 2 2018 https:/ /ora.ox.ac.uk/objects/uuidÊ¼86dfcdac-10b5-4314-bbd1-08e6e78b9094                                                                                                                                                                                                                                                                                 |\n| mills_study_2010 Proposed Internet Congestion Control Mechanisms.                                                                                                                        | Mills, Kevin L, James J Filliben, Dong Yeon Cho, Edward Schwartz, Daniel Genin.                                                                              | and NIST SP 500-282.                                                                                                             | 2010 https:/ /doi.org/10.6028/NIST.SP.500-282                                                                                                                                                                                                                                                                                                                  |\n| friedman_additive_2000 Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors)                                                     | Friedman, Jerome, Trevor Hastie, and Robert Tibshirani.                                                                                                      | The Annals of Statistics                                                                                                         | 2 337-407 2000 https:/ /doi.org/10.1214/aos/1016218223                                                                                                                                                                                                                                                                                                         |\n| kusner_counterfactual_201 7 Counterfactual Fairness                                                                                                                                      | Kusner, Matt J, Joshua Loftus, Chris Russell, and Ricardo Silva.                                                                                             | Advances in Neural Information Processing Systems (NIPS) Psychological Bulletin                                                  | 2017 https:/ /proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html                                                                                                                                                                                                                                                            |\n| cronbach_construct_1955 Construct Validity in Psychological Tests.                                                                                                                       | Cronbach, Lee J., and Paul E. Meehl.                                                                                                                         | International Encyclopedia of Education (Third Edition)                                                                          | 281-302 1955 https:/ /doi.org/10.1037/h0040957                                                                                                                                                                                                                                                                                                                 |\n| fink_survey_2010 Survey Research Methods bordens_research_2010 Research Design and Methods: A Process Approach                                                                           | Fink, A. Kenneth S. Bordens, Bruce B. Abbott                                                                                                                 | Book (Eighth Edition)                                                                                                            | 152-160 2010 https:/ /doi.org/10.1016/B978-0-08-044894-7.00296-7 2011 https:/ /www.amazon.com/Research-Design-Methods-Process-Approach/dp/B008BLHYQ8                                                                                                                                                                                                           |\n| nist_statistics_2012 NIST/SEMATECH e-Handbook of Statistical Methods                                                                                                                     | Symeonidis, Georgios, Evangelos Nerantzis, Apostolos Kazakis,                                                                                                | and In 2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC)                                          | https:/ /doi.org/10.18434/M32189                                                                                                                                                                                                                                                                                                                               |\n| symeonidis_MLOps_2022 MLOps - Definitions, Tools and Challenges hardt_equality_2016 Equality of Opportunity in Supervised Learning                                                       | George A. Papakostas Hardt, Moritz, Eric Price, and Nati and Srebro                                                                                          | Advances in Neural Information Processing Systems (NIPS) Questions and Answers to Clarify and Provide a Common Interpretation of | 453-460 2022 https:/ /ieeexplore.ieee.org/document/9720902 3315-3323 2016 http:/ /papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf                                                                                                                                                                                                 |\n| EEOC_Q&A_Employee_Sel ection Questions and Answers to Clarify and Provide a Common Interpretation of the Uniform Guidelines on Procedures                                                | Equal Employment Opportunity Commission                                                                                                                      | the Uniform Guidelines on Employee Selection Procedures                                                                          | https:/ /www.eeoc.gov/laws/guidance/questions-and-answers-clarify-and-provide-common-interpretation-uniform-guidelines                                                                                                                                                                                                                                         |\n| Engineering_safety_in_ma chine_learning Engineering safety in machine learning                                                                                                           | Kush R. Varshney                                                                                                                                             | Information Theory and Applications Workshop (ITA), 2016                                                                         | 2016 https:/ /ieeexplore-ieee-org.proxygw.wrlc.org/document/7888195                                                                                                                                                                                                                                                                                            |\n| DOD_Modeling_and_Simul                                                                                                                                                                   |                                                                                                                                                              | DoD Modeling and Simulation (M&S) Glossary                                                                                       | 1998 https:/ /web.archive.org/web/20070710104756/http:/ /www.dtic.mil/whs/directives/corres/pdf/500059m.pdf                                                                                                                                                                                                                                                    |\n| ation_Glossary DoD Modeling and Simulation (M&S) Glossary                                                                                                                                | United States Department of Defense Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah | Lucy Raji, arXiv Explaining Decisions Made with AI: A Workbook (Use Case 1Êµ AI-Assisted Recruitment                              | https:/ /arxiv.org/abs/1810.03993                                                                                                                                                                                                                                                                                                                              |\n| Model_Cards_for_Model_ Reporting Model Cards for Model Reporting David_Leslie_Morgan_Brig AI-Assisted Recruitment Tool)                                                                  | Timnit Gebru David Leslie; Morgan Briggs                                                                                                                     | Tool)                                                                                                                            | 2021 https:/ /arxiv.org/ftp/arxiv/papers/2104/2104.03906.pdf                                                                                                                                                                                                                                                                                                   |\n| gs Explaining Decisions Made with AI: A Workbook (Use Case 1Êµ deeplearningbook_intro Introduction                                                                                        | Ian Goodfellow, Yoshua Bengio; Aaron Courville Office                                                                                                        | Deep Learning                                                                                                                    | 2016 https:/ /www.deeplearningbook.org/contents/intro.html                                                                                                                                                                                                                                                                                                     |\n| privacy- enhancing_technologies Chapter 5Ê¼ Privacy-enhancing technologies (PETs) Joseph_Rocca_Ensemble_                                                                                  | UK Information Commissioner's                                                                                                                                | DRAFT Anonymisation, pseudonymisation and privacy enhancing technologies guidance                                                | 2022 https:/ /ico.org.uk/media/about-the-ico/consultations/4021464/chapter-5-anonymisation-pets.pdf                                                                                                                                                                                                                                                            |\n| methods Ensemble methods: bagging, boosting and stacking                                                                                                                                 | Joseph Rocca                                                                                                                                                 |                                                                                                                                  | 2019 https:/ /towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205                                                                                                                                                                                                                                                               |\n| google_dev_classification-                                                                                                                                                               |                                                                                                                                                              | Towards Data Science                                                                                                             | https:/ /developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative                                                                                                                                                                                                                                                       |\n| true-false-positive-negative Classification: True vs. False and Positive vs. Negative                                                                                                    | Google                                                                                                                                                       | Google Machine Learning Education Foundational Courses                                                                           | https:/ /www.google.com/books/edition/Public_Health_and_Informatics/81A2EAAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                                                                                             |\n| Public_Health_and_Inform atics_MIE_2021 A Preliminary Scoping Study of Federated Learning for the Internet of Medical Things                                                             | Arshad Farhad; Sandra I. Woolley; Peter Andras                                                                                                               | Public Health and Informatics: Proceedings of MIE 2021                                                                           | 504-505 2021 22Federated+learning+1+2+is+a+learning+model+which+addresses+the+problem+of+data+governance+and+privacy+by+training+algorithms+collaboratively +without+transferring+the+data+to+another+location%22&pg=PA504&printsec=frontcover                                                                                                                 |\n| Black's_Law_Dictionary_h arm harm                                                                                                                                                        | The Law Dictionary / Black's Law Dictionary Second Edition                                                                                                   | The Law Dictionary / Black's Law Dictionary Second Edition                                                                       | https:/ /thelawdictionary.org/harm/                                                                                                                                                                                                                                                                                                                            |\n| dataiku_ML_and_linear_m odels Machine Learning and Models: How They Work (In Plain English)                                                                                              | Katie Gross                                                                                                                                                  | Dataiku                                                                                                                          | 2020 https:/ /blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1                                                                                                                                                                                                                                                                |\n| Linear ORM_model_inventory Model Inventory yields.io_model_validation What Is Model Validation?                                                                                          | Open Risk Manual Eimee V                                                                                                                                     | Yields.io                                                                                                                        | https:/ /www.openriskmanual.org/wiki/Model_Inventory 2020 https:/ /www.yields.io/blog/what-is-model-validation/                                                                                                                                                                                                                                                |\n| Open_Risk_Manual_model _validation Model Validation misuse_of_public_data Implicit data crimes: Machine                                                                                  | Open Risk Manual                                                                                                                                             | Open Risk Manual                                                                                                                 | https:/ /www.openriskmanual.org/wiki/Model_Validation                                                                                                                                                                                                                                                                                                          |\n| learning bias arising from misuse of public data                                                                                                                                         | Efrat Shimron; Jonathan I. Tamir; Ke Wang; Michael Lustig                                                                                                    | PNAS                                                                                                                             | 13 2022 https:/ /www.pnas.org/doi/full/10.1073/pnas.2117203119 https:/                                                                                                                                                                                                                                                                                         |\n| MIT_Protected_Attributes Module 3Ê¼ Pedagogical Framework for Addressing Ethical Challenges - Protected Attritbutes and \"Fairness Practical_Law_protected_                                | Unawareness\" MIT Open Courseware                                                                                                                             | Exploring Fairness in Machine Learning for International Development                                                             | 2020 /ocw.mit.edu/courses/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/pages/module-three- framework/protected-attributes/ https:/ /content.next.westlaw.com/practical-law/document/Ibb0a38daef0511e28578f7ccc38dcbee/Protected-Class?                                                                          |\n| class Protected Class Dave_Salvator_sparsity How Sparsity Adds Umph to AI Inference                                                                                                      | Thomson Reuters Practical Dave Salvator                                                                                                                      | Thomson Reuters Practical Law                                                                                                    | viewType=FullText&transitionType=Default&contextData=(sc.Default)&firstPage=true0law. 2020 https:/ /blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/                                                                                                                                                                                                    |\n| saurabh_label_2020 A Unified View of Label Shift                                                                                                                                         | Law Garg, Saurabh, Yifan Wu,                                                                                                                                 | NVIDIA Blogs Zachary                                                                                                             | 2020 https:/ /proceedings.neurips.cc/paper/2020/hash/219e052492f4008818b8adb6366c7ed6-Abstract.html                                                                                                                                                                                                                                                            |\n| Estimation provost_data_2013 Data Science and its Relationship to Big Data and Data-Driven hameed_data_2020 Data Preparation: A Survey of Commercial Tools                               | Sivaraman Balakrishnan, and Provost, Foster and Tom Fawcett                                                                                                  | Lipton Advances in Neural Information Processing Systems Big Data                                                                | 51-59 2013 https:/ /www.liebertpub.com/doi/full/10.1089/big.2013.1508                                                                                                                                                                                                                                                                                          |\n| Decision Making                                                                                                                                                                          | Hameed, Mazhar and Felix Naumann                                                                                                                             | ACM SIGMOD Record                                                                                                                | 1 3 18-29 2020 https:/ /doi.org/10.1145/3444831.3444835                                                                                                                                                                                                                                                                                                        |\n| Merriam-Webster_ranking ranking hunter_differential_1979 Differential validity of employment tests by race: A comprehensive review and                                                   | Merriam-Webster                                                                                                                                              | Merriam-Webster Dictionary                                                                                                       | https:/ /www.merriam-webster.com/dictionary/ranking 1979 https:/ /psycnet.apa.org/record/1979-30107-001                                                                                                                                                                                                                                                        |\n| analysis kelley_dogfooding_2022 'Dogfooding'                                                                                                                                             | Hunter, John E. and Frank L. Schmidt and Ronda Hunter                                                                                                        | Psychological Bulletin                                                                                                           | 721-735 2022 https:/ /www.nytimes.com/2022/11/14/business/dogfooding.html 2022 https:/ /mathworld.wolfram.com/                                                                                                                                                                                                                                                 |\n| wolfram_math_2022 Wolfram MathWorld: The Web's Most Extensive Mathematics Resource aivodji_fairwashing_2019 Fairwashing: the risk of rationalization                                     | Kelley, Lora Aivodji, Ulrich, Hiromi Arai, Olivier Fortineau, SÃ©bastien                                                                                      | The New York Times Gambs, Satoshi Proceedings of the 36th International Conference on Machine                                    | 2019 https:/ /proceedings.mlr.press/v97/aivodji19a.html 372-378 2014 https:/ /ieeexplore.ieee.org/abstract/document/6918213                                                                                                                                                                                                                                    |\n| khalid_feature_2014 A survey of feature selection and feature extraction techniques in machine learning                                                                                  | Hara, and Alain Tapp Khalid, Samina, Tehmina Khalil, and Shamila Nasreen.                                                                                    | Learning 2014 Science and Information Conference Molina-                                                                         |                                                                                                                                                                                                                                                                                                                                                                |\n| tabassi_adversarial_2019 A Taxonomy and Terminology of Adversarial Machine Learning                                                                                                      | Tabassi, Elham, Kevin Burns, Michael Hadjimichael, Andres                                                                                                    | NIST Internal or Interagency Report (NISTIR) 8269 (Draft)                                                                        | 2019 https:/ /doi.org/10.6028/NIST.IR.8269-draft                                                                                                                                                                                                                                                                                                               |\n| measurement_iso22989_20 22 ISO/IEC 22989Ê¼2022 Information technology -Artificial intelligence -Artificial                                                                                | Markham, and Julian Sexton.                                                                                                                                  | ISO/IEC 22989Ê¼2022                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                |\n| intelligence concepts aime_measruement_2022 Notes on Measurement                                                                                                                         | NIST AIME Team                                                                                                                                               | Unpublished Manuscript SN Applied Sciences                                                                                       | 2022 https:/ /www.iso.org/standard/74296.html 2022 <None>                                                                                                                                                                                                                                                                                                      |\n| saarela_feature_2021 Comparison of feature importance measures as explanations for classification models poole_mackworth_observa tion 5.3.1 Background Knowledge and Observations        | Mirka Saarela and Susanne Jauhiainen David Poole and Alan Mackworth                                                                                          | Artificial Intelligence: Foundations of                                                                                          | 2021 https:/ /link.springer.com/article/10.1007/s42452-021-04148-9 2010 https:/ /artint.info/html/ArtInt_112.html                                                                                                                                                                                                                                              |\n| kathleen_walch_operationa lization Operationalizing AI                                                                                                                                   | Kathleen Walch                                                                                                                                               | Computational Agents                                                                                                             | 2020 https:/                                                                                                                                                                                                                                                                                                                                                   |\n| about_ML_packages About ML Packages                                                                                                                                                      | UiPath                                                                                                                                                       | Forbes UiPath AI Center Guide                                                                                                    | 2021 https:/ /docs.uipath.com/ai-fabric/v0/docs/about-ml-packages                                                                                                                                                                                                                                                                                              |\n| TechTarget_data_point data point                                                                                                                                                         | Katie Terrell Hanna and Ivy                                                                                                                                  | TechTarget                                                                                                                       | /www.forbes.com/sites/cognitiveworld/2020/01/26/operationalizing-ai/?sh=42ea4b2733df 2022 https:/ /www.techtarget.com/whatis/definition/data-point 2022 https:/ /www.vproexpert.com/what-is-a-data-point-in-a-machine-learning-model/                                                                                                                          |\n| Morris_John_data_point What is a data point in a machine learning model? Artasanchez_Joshi_AI_wit                                                                                        | Wigmore John Morris                                                                                                                                          | VProexpert Artificial Intelligence with Python: Your Complete Guide to Building                                                  | 351-378 2020 https:/ 22&pg=PA356&printsec=frontcover                                                                                                                                                                                                                                                                                                           |\n| h_Python Natural Language Processing Techopedia_lemmatization Lemmatization                                                                                                              | Alberto Artasanchez and Prateek Joshi Techopedia                                                                                                             | Intelligent Apps Using Python 3.x, 2nd Edition Techopedia                                                                        | /www.google.com/books/edition/Artificial_Intelligence_with_Python/P0fODwAAQBAJ?hl=en&gbpv=1&dq=% 22Lemmatization+is+the+process+of+grouping+together+the+different+inflected+forms+of+a+word+so+they+can+be+analyzed+as+a+single+item% https:/ /www.techopedia.com/definition/33256/lemmatization https:/ /www.techslang.com/definition/what-is-lemmatization/ |\n| Techslang_lemmatization What is Lemmatization?                                                                                                                                           | Techslang                                                                                                                                                    |                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                |\n| TechTarget_lemmatization Lemmatization                                                                                                                                                   | TechTarget contributor                                                                                                                                       | Techslang TechTarget                                                                                                             | 2018 https:/                                                                                                                                                                                                                                                                                                                                                   |\n| Lim_Swee_Kiat_harms Understanding Bias Part I                                                                                                                                            | Lim Sweet Kiat                                                                                                                                               | Machines Gone Wrong                                                                                                              | 2019 https:/ /machinesgonewrong.com/bias_i/#two-types-of-harms                                                                                                                                                                                                                                                                                                 |\n| ICO_data_minimisation Data minimisation and privacy-preserving techniques in AI                                                                                                          | Information Commissioner's                                                                                                                                   | ICO AI Blog                                                                                                                      | /www.techtarget.com/searchenterpriseai/definition/lemmatization 2022 https:/                                                                                                                                                                                                                                                                                   |\n| systems EDPS_data_minimization Data Minimization                                                                                                                                         | Office European Data Protection Supervisor                                                                                                                   | Data Protection Glossary                                                                                                         | /ico.org.uk/about-the-ico/media-centre/ai-blog-data-minimisation-and-privacy-preserving-techniques-in-ai-systems/ https:/ /edps.europa.eu/data-protection/data-protection/glossary/d_en                                                                                                                                                                        |\n| Arjun_Subramonian_bias_ mitigation An Introduction to Fairness and Bias Mitigation with AllenNLP                                                                                         | Arjun Subramonian Nicole Chi, Keming Gao, Joanne Ma                                                                                                          | AI2Blog                                                                                                                          | 2021 https:/ /blog.allenai.org/an-introduction-to-fairness-and-bias-mitigation-with-allennlp-d1b478d44d4c                                                                                                                                                                                                                                                      |\n| Chi,_Gao,_Ma Tik Tok Unwrapped                                                                                                                                                           |                                                                                                                                                              | Tik Tok Unwrapped                                                                                                                | 2022 https:/ /www.ischool.berkeley.edu/sites/default/files/sproject_attachments/tiktok_unwrapped_zine_final.pdf                                                                                                                                                                                                                                                |\n\n| ID Title of article, chapter, or page TechTarget_decision_supp                                                                                                                                                                              | Author(s) and/or Editor(s)                                                                                                    | Publication or website (either the main domain or major subdomain)                                                             | Volume        | Issue Page(s)   | Year URL                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|---------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ort_system decision support system                                                                                                                                                                                                          | TechTarget                                                                                                                    | TechTarget Support Systems 1Êµ Basic Themes                                                                                     |               |                 | 2021 https:/ /www.techtarget.com/searchcio/definition/decision-support-system 2008 https:/ /www.google.com/books/edition/Handbook_on_Decision_Support_Systems_1/q_3sRkRKZQwC?hl=en&gbpv=0                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| Burstein_Holsapple Spreadsheet-Based Decision Support Systems Sourabh_Mehta_determini                                                                                                                                                       | Frada Burstein and Clyde W. Holsapple                                                                                         | Handbook on Decision                                                                                                           |               |                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| stic Deterministic vs Stochastic Machine Learning jansen_graphical_1998 The Graphical User Interface.                                                                                                                                       | Sourabh Mehta Jansen, Bernard J.                                                                                              | Analytics India Magazine ACM SIGCHI Bulletin                                                                                   | 30            | 2 22-26         | 2022 https:/ /analyticsindiamag.com/deterministic-vs-stochastic-machine-learning/ 1998 https:/ /doi.org/10.1145/279044.279051                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| rudin_interpretable_2022 Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges.                                                                                                                                    | Rudin, Cynthia, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong.                                       | Statistics Surveys                                                                                                             | 16            | 1-85            | 2022 https:/ /doi.org/10.1214/21-SS133                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| NISTIR_8312_Full Four Principles of Explainable Artificial Intelligence                                                                                                                                                                     | Phillips, P. Jonathon; Carina A. Hahn; Peter C. Fontana; David Broniatowski; Mark A.                                          | A. NISTIR 8312                                                                                                                 |               |                 | 2021 https:/ /nvlpubs.nist.gov/nistpubs/ir/2021/NIST.IR.8312.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n| arun_opportunities_2020 Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey. open_risk_2022 Open Risk Manual: Midel Governancde                                                                             | Przybocki Das, Arun, and Paul Rad.                                                                                            | Arxiv Open Risk Manual                                                                                                         |               |                 | 2020 https:/ /doi.org/10.48550/arXiv.2006.11371 2022 https:/ /www.openriskmanual.org/wiki/Model_Governance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| merriam_webster_outcom                                                                                                                                                                                                                      | Merriam-Webster                                                                                                               | Merriam-Webster Dictionary                                                                                                     |               |                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| e outcome sutton_reinforcement_201                                                                                                                                                                                                          |                                                                                                                               | Book, Published by MIT Press                                                                                                   |               |                 | 2022 https:/ /www.merriam-webster.com/dictionary/outcome                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| 8 Reinforcement Learning: An Introduction                                                                                                                                                                                                   | Sutton, Richard, and Andrew Barto and Kevin Streff                                                                            | Proceedings of the 53rd Hawaii International Conference on System Sciences                                                     |               |                 | 2018 2020 https:/ /scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/5486a250-cc3c-4227-a752-7d08378afbdf/content                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n| Covert_et_al Towards a Triad for Data Privacy Cami_Rosso The Human Bias in the AI Machine: How artificial intelligence is subject to cognitive bias                                                                                         | Quentin Covert, Mary Francis, Dustin Steinhagen, Cami Rosso                                                                   | Psychology Today                                                                                                               |               |                 | 2018 https:/ /www.psychologytoday.com/us/blog/the-future-brain/201802/the-human-bias-in-the-ai-machine                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| NIST_CSRC_man-in-the- middle_attack man-in-the-middle attack (MitM)                                                                                                                                                                         | NIST Computer Security Resource Center                                                                                        | NIST Computer Security Resource Center                                                                                         |               |                 | https:/ /csrc.nist.gov/glossary/term/man_in_the_middle_attack                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| cambridge_dictionary_202 2 Cambridge Dictionary                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                |               |                 | 2022 https:/ /dictionary.cambridge.org/us/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| settles_active_2009 Active Learning Literature Survey informs_analytics_2022 Operations Research & Analytics                                                                                                                                |                                                                                                                               | Technical Report, University of Wisconsim-Madison, Department of Computer Sciences                                             |               |                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|                                                                                                                                                                                                                                             | Burr Settles                                                                                                                  |                                                                                                                                |               |                 | 2009 https:/ /minds.wisconsin.edu/handle/1793/60660                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| guresen_definition_2011 Definition of Artificial Neural Networks with Comparison to Other Networks                                                                                                                                          | INFORMS Guresen, Erkam, and Gulgun Kayakutlu                                                                                  | Procedia Computer Science, World Conference on Information Technology                                                          | 3             |                 | 2022 https:/ /www.informs.org/Explore/Operations-Research-Analytics 2011 https:/ /doi.org/10.1016/j.procs.2010.12.071                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| olson_pmlb_2017 PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison.                                                                                                                                               | Olson, Randal S., William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore                                 | BioData Mining                                                                                                                 |               | 1               | 2017 https:/ /doi.org/10.1186/s13040-017-0154-4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| devore_probability_2004 Probability and Statistics for Engineering and the Sciences Applications                                                                                                                                            | Jay L. Devore                                                                                                                 | Book, Published by Thompson, Brooks/Cole, Sixth Edition                                                                        | 10            |                 | 2004                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| aggarwal_clustering_2013 Data Clustering: Algorithms and                                                                                                                                                                                    | Aggarwal, Charu C., and Chandan K. Reddy                                                                                      | Book, Published by Chapman & Hall/CRC, First Edition                                                                           |               |                 | 2013 https:/ /www.techopedia.com/definition/8/database-column#:~:text=In%20the%20context%20of%20relational,documents%20or%20even%20video%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| techopedia_column_2022 Database Column box_statistics_2005 Statistics for Experimenters: Design, Innovation,                                                                                                                                | Techopedia George E. P. Box, J. Stuart Hunter, William G. Hunter                                                              | Book, Published by Wiley, Second Edition                                                                                       |               |                 | 2022 20clips. 2005                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| and Discovery james_statistical_2014 An Introduction to Statistical Learning: With Applications in R                                                                                                                                        | James, Gareth, Daniela Witten, Trevor Hastie, and Robert Friedler, Sorelle A., Carlos Scheidegger, Suresh Venkatasubramanian, | Tibshirani Book, Published by Springer In Proceedings of the Conference on Fairness, Accountability, and                       |               |                 | 2014 http:/ /www-bcf.usc.edu/~gareth/ISL/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| friedler_comparative_2019 A Comparative Study of Fairness-Enhancing Interventions in Machine                                                                                                                                                | Sonam Choudhary, Evan P. Hamilton, and Derek Roth Gong, Maoguo, Yu Xie, Ke Pan, Kaiyuan Feng, and A.K. Qin                    | Transparency IEEE Computational Intelligence Magazine                                                                          |               |                 | 2019 https:/ /doi.org/10.1145/3287560.3287589                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Learning gong_differential_2020 A Survey on Differentially Private Machine Learning [Review Article]. Bolukbasi_et_al_Debiasing                                                                                                             | Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh                                                                          | and 30th Conference on Neural Information Processing Systems                                                                   | 15 Barcelona, | 2               | 2020 https:/ /doi.org/10.1109/MCI.2020.2976185                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| _Word_Embeddings Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings google_glossary_2023 Machine Learning Glossary                                                                                           | Adam Kalai Google                                                                                                             | Saligrama, (NIPS 2016), Spain                                                                                                  |               |                 | 2016 https:/ /papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| wikipedia_graph_2023 Graph (discrete                                                                                                                                                                                                        | Wikipedia                                                                                                                     |                                                                                                                                |               |                 | 2023 https:/ /developers.google.com/machine-learning/glossary https:/ /en.wikipedia.org/wiki/Graph_(discrete_mathematics)#Definitions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| mathematics)                                                                                                                                                                                                                                |                                                                                                                               | Wikipedia                                                                                                                      |               |                 | 2023 https:/ /www.google.com/books/edition/CompTIA_CySA+_Guide_to_Cybersecurity_Ana/NwpIEAAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| Mark_Ciampa_2021 Utilizing Threat Data and Intelligence David_Lyon_2007 Security, Suspicion, Social Sorting                                                                                                                                 | Mark Ciampa David Lyon                                                                                                        | CompTIA CySA+ Guide to Cybersecurity Analyst (CS0-002) Surveillance Studies: An Overview                                       |               |                 | 2021 22In+cybersecurity+a+threat+actor+is+a+term+used+to+describe+individuals+or+entities+who+are+responsible+for+cyber+incidents+against+enterprises+ governments+and+users%22&pg=PA29&printsec=frontcover 2007 https:/ /www.google.com/books/edition/Surveillance_Studies/_dTHJgh3-f0C?hl=en&gbpv=1&bsq=surveillance%20is                                                                                                                                                                                                                                                                                                                         |\n| Hartley_and_Zisserman_2 003 Projective Geometry and Transformations of 2D                                                                                                                                                                   | Richard Hartley; Andrew Zisserman Mona Sloane, Emanuel Moss, Olaitan                                                          | Multiple View Geometry in Computer Vision Proceedings of the 37th International Conference on Machine Learning, PMLR           |               |                 | 2003 https:/ /www.google.com/books/edition/Multiple_View_Geometry_in_Computer_Visio/si3R3Pfa98QC?hl=en&gbpv=1&dq=% 22Projection+along+rays+through+a+common+point+the+centre+of+pro+jection+defines+a+mapping+from+one+plane+to+another% 22&pg=PA34&printsec=frontcover 2020 https:/ /arxiv.org/ftp/arxiv/papers/2007/2007.02423.pdf https:/ /csrc.nist.gov/glossary/term/parity                                                                                                                                                                                                                                                                    |\n| Sloane_et_al_2020 Participation is not a Design Fix for Machine Learning NIST_CSRC_parity parity                                                                                                                                            | Awomolo, Laura Forlano NIST Computer Security Resource Center                                                                 | NIST Computer Security Resource Center                                                                                         |               |                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Dennis_Mercadal L Ekaterina_et_al_2020 Why Are We Averse towards Algorithms? A Comprehensive Literature Review on Algorithmic Aversion                                                                                                      | Dennis Mercadal Ekaterina Jussupow, Izak Benbasat, and Armin Heinzl                                                           | Dictionary of Artificial Intelligence Proceedings of the 28th European Conference on Information Systems                       | An            |                 | 1990 https:/ /archive.org/details/dictionaryofarti0000merc/page/162/mode/2up?view=theater https:/ /aisel.aisnet.org/ecis2020_rp/168/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| Gabriel_2020 Artificial Intelligence, Values,                                                                                                                                                                                               | Iason Gabriel Rukhin, Andrew, Juan Soto, James Nechvatal, Miles                                                               | (ECIS), Online AIS Conference Minds and Machines                                                                               | 30            |                 | 2020 2020 https:/ /link.springer.com/article/10.1007/s11023-020-09539-2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| and Alignment nist_800_2010 A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications.                                                                                                          | Smid, Elaine Stefan Leigh, Mark Levenson, et al. ISO                                                                          | Barker, NIST SP-800-22ra                                                                                                       |               |                 | 2010 https:/ /doi.org/10.6028/NIST.SP.800-22r1a 1997                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| iso_2382_1997 Information technology -Vocabulary -Part 31Ê¼ Artificial intelligence -Machine learning Information technology -Artificial intelligence -Artificial intelligence concepts and terminology Ethics Guidelines for Trustworthy AI | ISO                                                                                                                           | ISO/IEC 2382-31 ISO/IEC 22989                                                                                                  |               |                 | 2022 https:/ /www.iso.org/standard/74296.html 2019 https:/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n| iso_22989_2022 european_ethics_2019 Friedman_et_al_2017 A Survey of Value Sensitive Design Methods                                                                                                                                          | High-Level Expert Group on Artificial Intelligence                                                                            |                                                                                                                                |               |                 | /ec.europa.eu/futurium/en/ai-alliance-consultation.1.html 2017 https:/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| Bipartisan_Policy_Center_i mpact_assessments Explainer: Impact Assessments for Artificial Intelligence                                                                                                                                      | Batya Friedman, David G. Hendry, and Alan Borning                                                                             | Draft Report Foundations and TrendsÂ® in Human-Computer Interaction                                                             |               |                 | /www.nowpublishers.com/article/Details/HCI-015                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n|                                                                                                                                                                                                                                             | Sean Long, Jeremy Pesner, and Tom Romanoff                                                                                    | Bipartisan Policy Center                                                                                                       |               |                 | 2022 https:/ /bipartisanpolicy.org/blog/impact-assessments-for-ai/ https:/ /www.google.com/books/edition/Knowledge_Management_in_Theory_and_Pract/_MrxCwAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n|                                                                                                                                                                                                                                             | Kimiz Dalkir                                                                                                                  | Knowledge Management in Theory and Practice                                                                                    |               |                 | 2011 22Qualitative+measures+provide+more+context+and+details+about+the+value+e+g+perceptions+which+are+often+difficult+to+measure+quantitatively% 22&pg=PA343&printsec=frontcover https:/ /www.google.com/books/edition/Cost_Management/HhQcEAAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                                                                                                                                                                                                              |\n| Kimiz_Dalkir_2011 The Value of Knowledge Management Cost_Management_ch2 Basic Cost Management Concepts                                                                                                                                      |                                                                                                                               |                                                                                                                                |               |                 | 2021 22Qualitative+measurement+implies+the+use+of+data+expressed+in+categories+such+as+customer+reviews+of+new+model+jet+skis% 22&pg=PA38&printsec=frontcover                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Virginia_Dignum_Responsi                                                                                                                                                                                                                    | Don R. Hansen; Maryanne M. Mowen; Dan L. Heitger                                                                              | Cost Management                                                                                                                |               |                 | https:/ /www.google.com/books/edition/The_Oxford_Handbook_of_Ethics_of_AI/8PQTEAAAQBAJ?hl=en&gbpv=1&dq=% 22Understanding+the+values+behind+the+technology+and+deciding+on+how+we+want+our+values+to+be+incorporated+in+AI+systems+requires+that+we                                                                                                                                                                                                                                                                                                                                                                                                  |\n| bility_and_Artificial_Intelli gence Responsibility and Artificial Intelligence                                                                                                                                                              | Virginia Dignum Yeom, Samuel, and Michael                                                                                     | The Oxford Handbook of Ethics of AI FAccT 2021Ê¼ In Proceedings of the 2021 ACM Conference on Fairness, and Transparency        |               |                 | 215-232 2020 +are+also+able+to+decide+on+how+and+what+we+want+AI+to+mean+in+our+societies%22&pg=PA221&printsec=frontcover 273-283                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| yeom_avoiding_2021 Avoiding Disparity Amplification under Different Worldviews Merriam-Webster_context context                                                                                                                              | Carl Tschantz Merriam-Webster                                                                                                 | Accountability, Merriam-Webster Dictionary                                                                                     |               |                 | 2021 https:/ /doi.org/10.1145/3442188.3445892 https:/ /www.merriam-webster.com/dictionary/context                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| jacobs_measurement_2023 Measurement and Fairness                                                                                                                                                                                            | Jacobs, Abigail Z., and Hanna Wallach                                                                                         | FAccT 2021Ê¼ In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency                            |               |                 | 2021 https:/ /doi.org/10.1145/3442188.3445901                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n| Merriam-Webster_impact impact                                                                                                                                                                                                               | Merriam-Webster                                                                                                               | Merriam-Webster                                                                                                                |               |                 | https:/ /www.merriam-webster.com/dictionary/impact https:/ /www.google.com/books/edition/The_SAGE_Encyclopedia_of_Qualitative_Res/byh1AwAAQBAJ?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|                                                                                                                                                                                                                                             | Lisa M. Given NIST                                                                                                            | The SAGE Encyclopedia of Qualitative Research Methods                                                                          |               |                 | 491-538 2008 hl=en&gbpv=1&dq=Mixed+methods+is+defined+as+research+in+which+the+inquirer+or+investigator+collects+and+analyzes+data,+integrates+the+findings, +and+draws+inferences+using+both+qualitative+and+quantitative+approaches+or+methods+in+a+single+study+or+a+program+of+study. &pg=PT584&printsec=frontcover 2023 https:/ /nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf                                                                                                                                                                                                                                                                |\n| Lisa_M._Given_SAGE M: Mixed Methods Research NIST_AI_RMF_1.0 NIST AI RMF 1.0 45_CFR_46_2018_Require ments_ (2018_Common_Rule) 2018 Requirements (2018                                                                                       |                                                                                                                               | NIST AI RMF 1.0                                                                                                                |               |                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Common Rule) cambridge_causative_2023 Cambridge Dictionary: causative lyons_contestability_2021 Conceptualising Contestability: Perspectives on cambridge_contestable_20                                                                    | United States Department of Health and Human Services Cambridge Lyons, Henrietta, Eduardo Velloso, and Tim Miller             | (HHS) 45 CFR 46 Cambridge Dictionary                                                                                           |               |                 | 2018 https:/ /www.hhs.gov/ohrp/regulations-and-policy/regulations/45-cfr-46/revised-common-rule-regulatory-text/index.html 2023 https:/ /dictionary.cambridge.org/us/dictionary/english/causative 2021 https:/ /doi.org/10.1145/3449180                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Contesting Algorithmic Decisions 23 Contestable                                                                                                                                                                                             |                                                                                                                               | Proceedings of the ACM on Human-Computer                                                                                       |               |                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Saleh_Alkhalifa_ML_in_Bi                                                                                                                                                                                                                    |                                                                                                                               | Interaction Cambridge Dictionary                                                                                               |               |                 | 2023 https:/ /dictionary.cambridge.org/us/dictionary/english/contestable https:/ /www.google.com/books/edition/Machine_Learning_in_Biotechnology_and_Li/KUVWEAAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n| otech Supervised Machine Learning                                                                                                                                                                                                           | Saleh Alkhalifa W. Joel Schneider and Kevin S. McGrew; edited by Dawn P. and Erin M. McDonough                                | Machine Learning in Biotechnology and Life Sciences: Build Machine Learning Using Python and Deploy Them on the Cloud Flanagan | Models        |                 | 168-233 2022 22We+can+define+supervised+learning+as+a+general+subset+of+machine+learning+in+which+data+like+its+associated+labels+is+used+to+train+models+th at+can+learn+or+generalize+from+the+data+to+make+predictions+preferably+with+a+high+degree+of+certainty%22&pg=PA168&printsec=frontcover 73-163 2018 https:/ /www.google.com/books/edition/Contemporary_Intellectual_Assessment/JA1mDwAAQBAJ?hl=en&gbpv=1&dq=% 22Formal+expertise+is+the+result+of+a+selfselection+of+a+domain+of+knowledge+that+is+mastered+deliberately+and+for+which+there+are+clear+bench marks+of+success+Fisher+Keil+2016%22&pg=PA117&printsec=frontcover https:/ |\n| Schneider_McGrew_in_Fla nagan_McDonough_2018 The Cattell-Horn-Carroll Theory of Cognitive Abilities Little_2013 The Measurement Model Merriam-                                                                                              | Todd D. Little Merriam-Webster                                                                                                | Contemporary Intellectual Assessment: Theories, Tests, and Issues Longitudinal Structural Equation Modeling                    |               |                 | 71-105 2013 /www.google.com/books/edition/Longitudinal_Structural_Equation_Modelin/gzeCu3FjZf4C?hl=en&gbpv=1&dq=%22Measurement+model% 22&pg=PA103&printsec=frontcover https:/ /www.merriam-webster.com/dictionary/executive                                                                                                                                                                                                                                                                                                                                                                                                                         |\n| Webster_executive executive                                                                                                                                                                                                                 |                                                                                                                               | Merriam-Webster Dictionary                                                                                                     |               |                 | https:/ /www.google.com/books/edition/The_Engineering_Handbook/l_TLBQAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n|                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                |               |                 | 22The+propagation+of+uncertainty+is+defined+as+the+way+in+which+uncertainties+in+the+variables+affect+the+uncertainty+in+the+calculated+results%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n|                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                |               |                 | 151-160 2018 22&pg=SA99-PA711&printsec=frontcover                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| Dorf_2018 Measurement and Instrumentation Merriam-Webster_example example Merriam-Webster_ethic ethic                                                                                                                                       | Richard C. Dorf Merriam-Webster Merriam-Webster                                                                               | The Engineering Handbook Merriam-Webster Dictionary                                                                            |               |                 | https:/ /www.merriam-webster.com/dictionary/example https:/ /www.merriam-webster.com/dictionary/ethic                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| Merriam-                                                                                                                                                                                                                                    |                                                                                                                               | Merriam-Webster Dictionary                                                                                                     |               |                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Webster_anthropomorphis m anthropomorphism                                                                                                                                                                                                  | Merriam-Webster                                                                                                               |                                                                                                                                |               |                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|                                                                                                                                                                                                                                             | Berthold, Michael R., Christian Borgelt, Frank HÃ¶ppner, Frank and Rosaria Silipo                                              | Merriam-Webster Dictionary                                                                                                     |               |                 | https:/ /www.merriam-webster.com/dictionary/anthropomorphism 2020 https:/ /doi.org/10.1007/978-3-030-45574-3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| berthold_guide_2020 Guide to Intelligent Data Science: How to Intelligently Make Use of Real Data alon-barkat_human_2023 Human-AI Interactions in Public Sector Decision Making: 'Automation Bias' and 'Selective                           | Alon-Barkat, Saar, and Madalina Busuioc Stephen                                                                               | Klawonn, Springer International Publishing Journal of Public Administration Research and Theory                                |               |                 | 2023 https:/ /doi.org/10.1093/jopart/muac007                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Adherence' to humphrey_addressing_202                                                                                                                                                                                                       | Advice. Humphrey, Holly J., Dana Levinson, Marc A. Nivet, and                                                                 |                                                                                                                                |               | 1               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| Algorithmic 0 Addressing Harmful Bias and Eliminating Discrimination in Health Professions Learning Environments: An Urgent                                                                                                                 | Schoenbaum                                                                                                                    | C.                                                                                                                             | 33            | 153-169 12S     | 2020 https:/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| knuth_art_1981 The Art of Computer Programming, Volume 2Ê¼ Seminumerical Algorithms garey_computers_1979 Computers and Intractability: A Guide to the Theory of NP-Completeness                                                              | Challenge. Donald Knuth Michael Garey and David Johnson                                                                       | Academic Medicine Addison-Wesley                                                                                               | 95 2          |                 | /doi.org/10.1097/ACM.0000000000003679 1981 1979                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| cambridge_impact_2023 Impact                                                                                                                                                                                                                |                                                                                                                               | W. H. Freeman Cambridge Dictionary                                                                                             |               |                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                |               |                 | 2023 https:/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                |               |                 | /dictionary.cambridge.org/us/dictionary/english/impact                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n\n| ID                                                                                                                                                                           | Title of article, chapter, or page                                                                                                                                  | Author(s) and/or Editor(s)                                                                                                                                                              | Publication or website (either the main domain or major subdomain) Volume Issue           | Page(s) Year URL                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| law_policy_2023 Policy fernandez_residual_1992 Residual Analysis and Data Transformations: Important Tools in Statistical                                                    | Analysis Fernandez, George C.                                                                                                                                       | The Law Dictionary J. HortScience                                                                                                                                                       | 27 4                                                                                      | 2023 https:/ /thelawdictionary.org/policy/#:~:text=Definition%20%26%20Citations%3A,as%20directed%20to%20the%20POLICY 297-300 1992 https:/ /journals.ashs.org/hortsci/view/journals/hortsci/27/4/article-p297.xml                                                                                                                                                                                                                                                 |\n| Leavy_OHQR_Intro Introduction                                                                                                                                                | Patricia Leavy                                                                                                                                                      | The Oxford Handbook of Qualitative Research                                                                                                                                             |                                                                                           | 1-13 2014 https:/ /academic.oup.com/edited-volume/38166/chapter-abstract/332997092?redirectedFrom=fulltext 2008;                                                                                                                                                                                                                                                                                                                                                 |\n| Barbour_2014 The scope and contribution of qualitative research                                                                                                              | Rosaline S. Barbour                                                                                                                                                 | Introducing Qualitative Research: A Student's Guide, Second                                                                                                                             | Edition                                                                                   | 11-27 2014 https:/ /archive.org/details/introducingquali0000barb_j9h1/page/12/mode/2up?view=theater                                                                                                                                                                                                                                                                                                                                                              |\n| Merriam- Webster_engineer engineer                                                                                                                                           | Merriam-Webster                                                                                                                                                     | Merriam-Webster Dictionary                                                                                                                                                              |                                                                                           | https:/ /www.merriam-webster.com/dictionary/engineer                                                                                                                                                                                                                                                                                                                                                                                                             |\n| AI_Incident_Editors Editor's Guide                                                                                                                                           | AI Incident Database Interaction Design: The                                                                                                                        | AI Incident Database Computer                                                                                                                                                           |                                                                                           | 2023 https:/ /incidentdatabase.ai/editors-guide/ 2023 https:/ /www.interaction-design.org/literature/book/the-glossary-of-human-computer-interaction/context-of-use                                                                                                                                                                                                                                                                                              |\n| interaction_context_2023 Context of Use AAAS_AI_and_Bias_2022-                                                                                                               | Glossary of Human                                                                                                                                                   | Interaction Interaction Design Foundation                                                                                                                                               |                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| 09 Artificial Intelligence and Bias - An Evaluation Seth_Boden_2020 Start Here: A Primer on Diversity and Inclusion (Part 1 of 2)                                            | M. Karanicolas and M. Knodel Seth Boden                                                                                                                             | Artificial Intelligence and the Courts: Materials for Judges Harvard Business Publishing                                                                                                |                                                                                           | 2022 https:/ /doi.org/10.1126/aaas.adf0788 2020 https:/ /www.harvardbusiness.org/start-here-a-primer-on-diversity-and-inclusion-part-1-of-2/                                                                                                                                                                                                                                                                                                                     |\n| GWU_diversity_and_inclus                                                                                                                                                     | George Washington University                                                                                                                                        | George Washington University Office for Diversity, Equity and Engagement                                                                                                                | Community                                                                                 | https:/ /diversity.gwu.edu/diversity-and-inclusion-defined                                                                                                                                                                                                                                                                                                                                                                                                       |\n| ion Diversity and Inclusion Defined HUD_diversity_and_inclus                                                                                                                 | U.S. Department of Housing and Urban Development                                                                                                                    | U.S. Department of Housing and Urban Development                                                                                                                                        |                                                                                           | https:/ /www.hud.gov/program_offices/administration/admabout/diversity_inclusion/definitions                                                                                                                                                                                                                                                                                                                                                                     |\n| ion Diversity and Inclusion Definitions Jamieson_Govaart_Pownall Reflexivity in quantitative research: A rationale and beginner's guide                                      | Michelle K. Jamieson, Gisela H. Govaart, and Madeleine                                                                                                              | Pownall Social and Personality Psychology Compass                                                                                                                                       | e12735                                                                                    | 2023 https:/ /doi.org/10.1111/spc3.12735 https:/ /www.google.com/books/edition/Industrial_Network_Security/PlOtqouTwaUC?hl=en&gbpv=1&dq=% 22Data+retention+refers+to+the+amount+of+information+that+is+stored+long-term,+and+can+be+measured+in+volume+                                                                                                                                                                                                          |\n| Industrial_Network_Securi ty_2011 Monitoring Enclaves                                                                                                                        | Eric D. Knapp and Joel Langill                                                                                                                                      | Industrial Network Security: Securing SCADA, and Other Industrial Control                                                                                                               | Critical Infrastructure Networks for Smart Grid, Systems                                  | 2011 (the+size+of+the+total+collected+logs+in+bytes)+and+time+(the+number+of+months+or+years+that+logs+are+stored+for).% 22&pg=PA243&printsec=frontcover                                                                                                                                                                                                                                                                                                         |\n| ChatGPT ChatGPT Merriam-Webster_parity Parity                                                                                                                                | OpenAI                                                                                                                                                              | Merriam-Webster Dictionary                                                                                                                                                              |                                                                                           | https:/ /chat.openai.com/chat 2023 https:/ /www.merriam-webster.com/dictionary/parity                                                                                                                                                                                                                                                                                                                                                                            |\n| barredo_explainable_2020 Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities                                                                      | and Challenges toward Responsible AI Barredo Arrieta, Alejandro, Natalia DÃ­az-RodrÃ­guez, Javier Del Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, | Ser,                                                                                                                                                                                    |                                                                                           | 82-115 2020                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| jackman_oxford_2008 Measurement                                                                                                                                              | Simon Jackman 1. K. Hammarberg, M. Kirkman, and S. de Lacey; 2.                                                                                                     | et al. Information Fusion The Oxford Handbook of Political Methodology                                                                                                                  | 58                                                                                        | https:/ /doi.org/10.1016/j.inffus.2019.12.012 2008                                                                                                                                                                                                                                                                                                                                                                                                               |\n| Hammarberg_2016_Busett o_2020 1. Qualitative research methods: when to use them and how to judge them; 2. How to use and assess qualitative research methods Approaches into | Loraine Busetto, Wolfgang Wick, and Christoph Gumbinger                                                                                                             | 1. Human Reproduction ; 2. Neurological Research and Practice                                                                                                                           | 1. 31                                                                                     | 1. 3; 2. 14 1. 2016; 2. 2020 1. https:/ /academic.oup.com/humrep/article/31/3/498/2384737; 2. https:/ /neurolrespract.biomedcentral.com/articles/10.1186/s42466-020-00059-z 36-40;                                                                                                                                                                                                                                                                               |\n| Russell_2003_Brannen_20 05 1. Evaluation of qualitative research studies; 2. Mixing Methods: The Entry of Qualitative and Quantitative Research Process                      | the 1. Cynthia K. Russell and David M. Gregory; 2. Julia Brannen Pamela Goh. Edited by Majeed Khader, Loo Seng Neo, and                                             | 1. Evidence-Based Nursing ; 2. International Journal of Social Research                                                                                                                 | Methodology 1. 6; 2. 8 2. 8                                                               | 1. 2. 173-184 1. 2003; 2. 2007 1. http:/ /dx.doi.org/10.1136/ebn.6.2.36; 2. https:/ /www.tandfonline.com/doi/full/10.1080/13645570500154642                                                                                                                                                                                                                                                                                                                      |\n| Pamela_Goh_2021 Humans as the Weakest Link in Maintaining Cybersecurity: Building Cyber Resilience in                                                                        | Humans Xiau Ting Chai                                                                                                                                               | Whistine Introduction to Cyber Forensic Psychology: Understanding the Mind of the Cyber Deviant Perpetrators The Pentagon's Brain: An Uncensored History of DARPA, America's Top-Secret |                                                                                           | 2021 https:/ /www.worldscientific.com/doi/abs/10.1142/9789811232411_0014                                                                                                                                                                                                                                                                                                                                                                                         |\n| Annie_Jacobsen_2015 Total Information                                                                                                                                        | Annie Jacobsen                                                                                                                                                      | Research Agency                                                                                                                                                                         | Military                                                                                  | 287-305 336-352 2015 https:/ /archive.org/details/pentagonsbrainun0000jaco_c8o3/page/342/mode/2up?q=%22a+role- playing+exercise+in+which+a+problem+is+examined+from+an+adversary%E2%80%99s+or+enemy%E2%80%99s+perspective.%22                                                                                                                                                                                                                                    |\n| Awareness Ben_Auffarth_2021 Online Learning for Time-Series                                                                                                                  | Ben Auffarth                                                                                                                                                        | Machine Learning for                                                                                                                                                                    | Time-Series with Python: Forecast, Predict, and Detect Anomalies Methods                  | 209-259 2021 https:/ /www.google.com/books/edition/Machine_Learning_for_Time_Series_with_Py/a7tLEAAAQBAJ?hl=en&gbpv=1&dq=% 22On+the+other+hand+offline+learning+the+more+commonly+known+approach+implies+that+you+have+a+static+dataset+that+you+know+from+the+start +and+the+parameters+of+your+machine+learning+algorithm+are+adjusted+to+the+whole+dataset+at+once+often+loading+the+whole+dataset+into+memo ry+or+in+batches%22&pg=PA210&printsec=frontcover |\n| FWS_062_FW_1 062 FW 1, Affirmative Employment Program and Plans Merriam- Webster_assessment assessment                                                                       | Office for Human Resources of the U.S. Fish & Wildlife Service Merriam-Webster                                                                                      | with State-of-the-art Machine Learning U.S. Fish & Wildlife Service                                                                                                                     |                                                                                           | 1996 https:/ /www.fws.gov/policy/062fw1.html                                                                                                                                                                                                                                                                                                                                                                                                                     |\n|                                                                                                                                                                              |                                                                                                                                                                     | Merriam-Webster Dictionary                                                                                                                                                              |                                                                                           | https:/ /www.merriam-webster.com/dictionary/assessment                                                                                                                                                                                                                                                                                                                                                                                                           |\n| Anthropomorphism_in_AI_                                                                                                                                                      | and Michele Farisco                                                                                                                                                 | AJOB Neuroscience                                                                                                                                                                       |                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| 2020 anthropomorphism                                                                                                                                                        | Arleen Salles, Kathinka Evers,                                                                                                                                      |                                                                                                                                                                                         | 2                                                                                         | 88-95 2020 https:/ /doi.org/10.1080/21507740.2020.1740350 https:/ /www.google.com/books/edition/Artificial_Intelligence_in_Society/eRmdDwAAQBAJ?hl=en&gbpv=1&dq=% 22Artificial+narrow+intelligence+ANI+or+applied+AI+is+designed+to+accomplish+a+specific+problem+solving+or+reasoning+task%                                                                                                                                                                     |\n| OECD_Artificial_Intelligenc e_in_Society The technical landscape AI_in_Medical_Imaging_gl                                                                                    | OECD Erik R. Ranschaert, Sergey                                                                                                                                     | Artificial Intelligence in Society Morozov, and Paul R. Algra, eds. Artificial Intelligence in Medical Imaging:                                                                         | Applications and Risks                                                                    | 19-34 2019 22&pg=PA22&printsec=frontcover 349-364 2019 https:/ /www.google.com/books/edition/Artificial_Intelligence_in_Medical_Imagi/ss6FDwAAQBAJ?hl=en&gbpv=1&dq=% 22The+definition+of+artificial+narrow+intelligence+is+in+contrast+to+that+of+strong+AI+or+artificial+general+intelligence+which+aims+at+providing+a+sy stem+with+consciousness+or+the+ability+to+solve+any+problems%22&pg=PA350&printsec=frontcover                                         |\n| ossary Glossary DOL_Practical_Significanc e Practical Significance in EEO Asked Questions                                                                                    | U.S. Department of Labor Programs                                                                                                                                   | Opportunities, Office of Federal Contract Compliance U.S. Department of Labor Office of Federal                                                                                         | Compliance Programs                                                                       | 2021 https:/ /www.dol.gov/agencies/ofccp/faqs/practical-significance                                                                                                                                                                                                                                                                                                                                                                                             |\n| Analysis Frequently Cambridge_Dictionary_no n-discrimination non-discrimination                                                                                              | Cambridge Dictionary                                                                                                                                                | Contract Cambridge Dictionary                                                                                                                                                           |                                                                                           | https:/ /dictionary.cambridge.org/us/dictionary/english/non-discrimination https:/ /doi.org/10.1016/B0-08-043076-7/00677-X                                                                                                                                                                                                                                                                                                                                       |\n| Signal_Detection_Theory Signal Detection Theory                                                                                                                              | N.A.                                                                                                                                                                | International Encyclopedia of the Social & Behavioral                                                                                                                                   | Sciences                                                                                  | 14075-14078 2001                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| Techopedia_kill_switch Kill Switch                                                                                                                                           | Macmillan Techopedia                                                                                                                                                | Techopedia                                                                                                                                                                              |                                                                                           | 2019 https:/ /www.techopedia.com/definition/4001/kill-switch https:/                                                                                                                                                                                                                                                                                                                                                                                             |\n| Batya_Friedman_VSD_Intr oduction Introduction C3.ai_feedback_loop What Is a Feedback Loop?                                                                                   | Batya Friedman and David C3.ai                                                                                                                                      | G. Hendry Value Sensitive Design: Shaping Technology C3.ai                                                                                                                              | with Moral Imagination                                                                    | 1-17 2019 /doi-org.proxy.library.georgetown.edu/10.7551/mitpress/7585.003.0002 https:/ /c3.ai/glossary/features/feedback-loop/                                                                                                                                                                                                                                                                                                                                   |\n| Collins_Dictionary_ground                                                                                                                                                    |                                                                                                                                                                     |                                                                                                                                                                                         |                                                                                           | https:/ /www.collinsdictionary.com/us/dictionary/english/ground-truth                                                                                                                                                                                                                                                                                                                                                                                            |\n| _truth ground truth Wikipedia_Decision-                                                                                                                                      | Collins Dictionary                                                                                                                                                  | Collins Dictionary Wikipedia                                                                                                                                                            |                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| making Decision-making Shevlin_et_al_2019 The limits of machine                                                                                                              | Wikipedia Henry Shevlin, Karina Vold, Matthew Crosby, and                                                                                                           | Halina EMBO Reports                                                                                                                                                                     | 20 10                                                                                     | https:/ /en.wikipedia.org/wiki/Decision-making e49177 2019 https:/ /www.ncbi.nlm.nih.gov/pmc/articles/PMC6776890/                                                                                                                                                                                                                                                                                                                                                |\n| intelligence The Americans with Disabilities Act and the Use of Software, Algorithms,                                                                                        | Marta and Artificial Intelligence to Assess Job Applicants and U.S. Equal Employment Opportunity Commission                                                         | U.S. Equal Employment Opportunity                                                                                                                                                       |                                                                                           | 2022 https:/ /www.eeoc.gov/laws/guidance/americans-disabilities-act-and-use-software-algorithms-and-artificial-intelligence                                                                                                                                                                                                                                                                                                                                      |\n| EEOC_ADA_AI Employees Merriam-                                                                                                                                               |                                                                                                                                                                     | Commission                                                                                                                                                                              |                                                                                           | https:/ /www.merriam-webster.com/dictionary/screen%20out                                                                                                                                                                                                                                                                                                                                                                                                         |\n| Webster_screen_out screen out apa_experiment_2023 experiment                                                                                                                 | Merriam-Webster American Psychological Association (APA)                                                                                                            | Merriam-Webster Dictionary APA Dictionary of Psychology                                                                                                                                 |                                                                                           | https:/ /dictionary.apa.org/experiment                                                                                                                                                                                                                                                                                                                                                                                                                           |\n| APA_DoP_laboratory_rese laboratory research                                                                                                                                  | American Psychological Association (APA)                                                                                                                            | APA Dictionary of Psychology                                                                                                                                                            |                                                                                           | 2023 https:/ /dictionary.apa.org/laboratory-research                                                                                                                                                                                                                                                                                                                                                                                                             |\n| arch UNODC_Glossary_QA_GL Glossary of Terms for Quality                                                                                                                      | Laboratory and Scientific Section of the United Nations Drugs and Crime                                                                                             | Office on Glossary of Terms for Quality Assurance and Good Laboratory                                                                                                                   |                                                                                           | 2009 https:/ /www.unodc.org/documents/scientific/ST_NAR_26_E.pdf                                                                                                                                                                                                                                                                                                                                                                                                 |\n| P Assurance and Good Laboratory Practices World_Wide_Words_In_si In silico                                                                                                   | World Wide Words                                                                                                                                                    | World Wide Words                                                                                                                                                                        | Practices                                                                                 | http:/ /www.worldwidewords.org/weirdwords/ww-ins1.htm                                                                                                                                                                                                                                                                                                                                                                                                            |\n| lico Bassiouni_Baffes_Evrard An Appraisal of Human Experimentation in International Law and Practice: Experimentation                                                        | The Need for International Regulation of Human                                                                                                                      |                                                                                                                                                                                         | 72                                                                                        | 1981 https:/ /scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=6276&context=jclc                                                                                                                                                                                                                                                                                                                                                                |\n| Merriam-Webster_amplify amplify                                                                                                                                              | M. Cheriff Bassiouni, Thomas G. Baffes, and John T. Evrard                                                                                                          | Journal of Criminal Law and Criminology                                                                                                                                                 | 4                                                                                         | 1597-1666 https:/ /www.merriam-webster.com/dictionary/amplify                                                                                                                                                                                                                                                                                                                                                                                                    |\n| Gupta_et_al_HAR_2022 Human activity recognition in artificial intelligence framework: a narrative Merriam-                                                                   | Merriam-Webster review Neha Gupta, Suneet K. Gupta, Rajesh K. Pathak, Vanita Jain, Rashidi, and Jasjit S. Suri                                                      | Merriam-Webster Dictionary Parisa Artificial Intelligence Review                                                                                                                        | 55                                                                                        | 4755-4808 2022 https:/ /link.springer.com/article/10.1007/s10462-021-10116-x                                                                                                                                                                                                                                                                                                                                                                                     |\n| Webster_annotate annotate                                                                                                                                                    | Merriam-Webster Scott Freeman, Sarah L. Eddy, Miles McDonough,                                                                                                      | Merriam-Webster Dictionary                                                                                                                                                              |                                                                                           | https:/ /www.merriam-webster.com/dictionary/annotate                                                                                                                                                                                                                                                                                                                                                                                                             |\n| Freeman_et_al_2014 Active learning increases student                                                                                                                         | Michelle Nnadozie Okoroafor, Hannah Jordt, and Mary Pat                                                                                                             | Smith, PNAS                                                                                                                                                                             | 111                                                                                       | https:/ /www.pnas.org/doi/full/10.1073/pnas.1319030111                                                                                                                                                                                                                                                                                                                                                                                                           |\n| performance in science, engineering, AI_Assurance_2022 Assuring AI methods for economic policymaking                                                                         | and mathematics Anderson Monken, William Ampeh, Flora Haberkorn, Uma                                                                                                | K. Wenderoth                                                                                                                                                                            | 23                                                                                        | 8410-8415 2014 https:/ /www.google.com/books/edition/AI_Assurance/dch6EAAAQBAJ?hl=en&gbpv=1&dq=%                                                                                                                                                                                                                                                                                                                                                                 |\n| Poore_Lawrence_ARLIS_2 023-01 AI Engineering: An                                                                                                                             | Krishnaswamy, and Feras A. Batarseh                                                                                                                                 | AI Assurance: Towards Trustworthy, Explainable,                                                                                                                                         | Ethical AI                                                                                | 371-428 2022 22Large+language+models+LLMs+are+a+class+of+language+models+that+use+deep+learning+algorithms+and+are+trained+on+extremely+large+textual+da tasets+that+can+be+multiple+terabytes+in+size%22&pg=PA376&printsec=frontcover                                                                                                                                                                                                                           |\n| Academic Research Roadmap Survey_of_Hallucination_i Generation                                                                                                               | Joshua Poore and Craig Lawrence Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su,                                                                            | Safe, and Applied Research Laboratory for Intelligence and Security                                                                                                                     | (ARLIS)                                                                                   | 2023 2023                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n| n_NLG Survey of Hallucination in Natural Language                                                                                                                            | Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung                                                                                                                | Yan Xu, Etsuko ACM Computing Surveys                                                                                                                                                    | 55 12                                                                                     | 1-38 https:/ /dl.acm.org/doi/10.1145/3571730 https:/ /dictionary.apa.org/clustering                                                                                                                                                                                                                                                                                                                                                                              |\n| APA_clustering clustering APA_content_validity content ISO                                                                                                                   | American Psychological Association (APA) American Psychological Association (APA) ISO                                                                               | APA Dictionary of APA Dictionary of ISO Online Browsing                                                                                                                                 | Psychology Psychology Platform                                                            | https:/ /dictionary.apa.org/content-validity https:/                                                                                                                                                                                                                                                                                                                                                                                                             |\n| validity Ergonomics of interaction -Part 11Ê¼ Usability: Definitions criterion                                                                                                | and concepts American Psychological Association (APA)                                                                                                               | APA Dictionary of APA Dictionary of                                                                                                                                                     | Psychology Psychology                                                                     | 2018 https:/ /dictionary.apa.org/criterion-validity https:/ /dictionary.apa.org/data-analysis                                                                                                                                                                                                                                                                                                                                                                    |\n| ISO_9241-11Ê¼2018 9241-11Ê¼2018(en) human-system APA_criterion_validity validity data analysis                                                                                 | American Psychological Association (APA)                                                                                                                            | Association (APA)                                                                                                                                                                       |                                                                                           | /www.iso.org/obp/ui/#iso:std:isoÊ¼9241Ê¼-11Ê¼ed-2Ê¼v1Ê¼en https:/ /dictionary.apa.org/decision-making                                                                                                                                                                                                                                                                                                                                                                 |\n| decision                                                                                                                                                                     | American Psychological                                                                                                                                              | APA Dictionary of Handbook of Human Factors                                                                                                                                             | Psychology                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| APA_data_analysis Support, and Problem Solving                                                                                                                               | Mark R. Lehto and Gaurav Nanda Jonathan Baron                                                                                                                       | and Thinking and Deciding                                                                                                                                                               | Ergonomics , Fifth Edition                                                                | 2021 https:/ 2008                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| APA_decision_making making Lehto_Nanda_2021 Decision-Making Models, Baron_Thinking_and_Deci ding                                                                             |                                                                                                                                                                     | The                                                                                                                                                                                     |                                                                                           | 159-202 /www.wiley.com/en-us/Handbook+of+Human+Factors+and+Ergonomics%2C+5th+Edition-p-9781119636083 https:/                                                                                                                                                                                                                                                                                                                                                     |\n| Decision EO_DEIA_2021 Executive Order on Diversity, Equity, Inclusion, and Accessibility in the APA_ethics ethics                                                            | Federal Workforce Joseph R. Biden Jr.                                                                                                                               | Association (APA)                                                                                                                                                                       | House                                                                                     | 2021 /www.whitehouse.gov/briefing-room/presidential-actions/2021/06/25/executive-order-on-diversity-equity-inclusion-and-accessibility-in- the-federal-workforce/ https:/ /dictionary.apa.org/ethics https:/ /dictionary.apa.org/external-validity                                                                                                                                                                                                               |\n| APA_external_validity external                                                                                                                                               | American Psychological American Psychological Association                                                                                                           | (APA)                                                                                                                                                                                   | White of Psychology of Psychology                                                         | https:/ /csrc.nist.gov/glossary/term/false_negative                                                                                                                                                                                                                                                                                                                                                                                                              |\n| False False                                                                                                                                                                  | NIST CSRC NIST CSRC                                                                                                                                                 | APA Dictionary APA Dictionary Information Information                                                                                                                                   | Laboratory Computer Security Resource Center Laboratory Computer Security Resource Center | https:/ /csrc.nist.gov/glossary/term/false_positive                                                                                                                                                                                                                                                                                                                                                                                                              |\n| validity CSRC_false_negative Negative CSRC_false_positive Wilke_Mata_2012 Cognitive                                                                                          | A. Wilke and R. Mata Sean McGregor                                                                                                                                  | Encyclopedia of Artificial                                                                                                                                                              | Technology Technology Human Behavior                                                      | https:/ https:/ /incidentdatabase.ai/research/5-response/                                                                                                                                                                                                                                                                                                                                                                                                        |\n| Positive Bias Defining an \"AI Incident Response\"                                                                                                                             | American Psychological                                                                                                                                              | Intelligence Incident APA Dictionary of Psychology                                                                                                                                      | Glossary Glossary 1 Database                                                              | 531-535 2012 /s3.amazonaws.com/arena-attachments/557491/b16d97da35ed37a0a022e806cc931a0d.pdf https:/ /dictionary.apa.org/integrity                                                                                                                                                                                                                                                                                                                               |\n| integrity internal validity                                                                                                                                                  | Association American Psychological Association                                                                                                                      | APA Dictionary of APA Dictionary of                                                                                                                                                     |                                                                                           | https:/ /dictionary.apa.org/internal-validity https:/ /dictionary.apa.org/learning                                                                                                                                                                                                                                                                                                                                                                               |\n| AIID_incident_response APA_integrity APA_internal_validity APA_learning learning                                                                                             | (APA) American Psychological Association                                                                                                                            |                                                                                                                                                                                         |                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| McNamara_Fallacy The                                                                                                                                                         | (APA) (APA) Jonathan Cook                                                                                                                                           | Psychology Psychology The McNamara Fallacy                                                                                                                                              |                                                                                           | https:/ /mcnamarafallacy.com/                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n| McNamara Fallacy Creswell_Clark_mixed_me thods                                                                                                                               | John W. Creswell and Vicki L. Plano Clark                                                                                                                           | Designing and Conducting Mixed Methods                                                                                                                                                  | Research , Third Edition                                                                  | 2023                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|                                                                                                                                                                              | American Psychological Association (APA)                                                                                                                            | APA Dictionary of Psychology                                                                                                                                                            |                                                                                           | 2017 https:/ /dictionary.apa.org/observation                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| APA_observation                                                                                                                                                              |                                                                                                                                                                     |                                                                                                                                                                                         |                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n| observation                                                                                                                                                                  |                                                                                                                                                                     |                                                                                                                                                                                         |                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n\n<!-- image -->\n\n| ID                                               | Title of article, chapter, or page                                                          | Author(s) and/or Editor(s)                                     | Publication or website (either the main domain or major subdomain)     | Volume Issue               | Page(s) Year               | URL                                                                                                                                                                               |                            |\n|--------------------------------------------------|---------------------------------------------------------------------------------------------|----------------------------------------------------------------|------------------------------------------------------------------------|----------------------------|----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------|\n| Glossary_of_Statistical_Te                       | Glossary_of_Statistical_Te                                                                  | Glossary_of_Statistical_Te                                     | Glossary_of_Statistical_Te                                             | Glossary_of_Statistical_Te | Glossary_of_Statistical_Te | Glossary_of_Statistical_Te                                                                                                                                                        | Glossary_of_Statistical_Te |\n| rms                                              | Glossary of Statistical Terms                                                               | Philip B. Stark                                                | SticiGui                                                               |                            |                            | 2019 https:/ /www.stat.berkeley.edu/~stark/SticiGui/Text/gloss.htm                                                                                                                |                            |\n| Wikipedia_RMSD                                   | Root-mean-square-deviation                                                                  | Wikipedia                                                      | Wikipedia                                                              |                            |                            | https:/ /en.wikipedia.org/wiki/Root-mean-square_deviation                                                                                                                         |                            |\n| APA_recognition                                  | recognition                                                                                 | American Psychological Association (APA)                       | APA Dictionary of Psychology                                           |                            |                            | https:/ /dictionary.apa.org/recognition                                                                                                                                           |                            |\n| APA_recall                                       | recall                                                                                      | American Psychological Association (APA)                       | APA Dictionary of Psychology                                           |                            |                            | https:/ /dictionary.apa.org/recall                                                                                                                                                |                            |\n| APA_stereotype                                   | stereotype                                                                                  | American Psychological Association (APA)                       | APA Dictionary of Psychology                                           |                            |                            | https:/ /dictionary.apa.org/stereotype                                                                                                                                            |                            |\n| Augoustinos_Walker_1998                          | The Construction of Stereotypes within Social Psychology: From Social Cognition to Ideology | Martha Augoustinos and Iain Walker                             | Theory & Psychology                                                    | 8 5                        | 629-652                    | 1998 https:/ /doi.org/10.1177/0959354398085003                                                                                                                                    |                            |\n| APA_autonomy                                     | autonomy                                                                                    | American Psychological Association (APA)                       | APA Dictionary of Psychology                                           |                            |                            | https:/ /dictionary.apa.org/autonomy                                                                                                                                              |                            |\n| Charmaz_Henwood                                  | Grounded Theory Methods for Qualitative Psychology                                          | Kathy Charmaz and Karen Henwood                                | The SAGE Handbook of Qualitative Research in Psychology                |                            | 238-255                    | 2017 https:/ /doi.org/10.4135/9781526405555                                                                                                                                       |                            |\n| APA_reflexivity                                  | reflexivity                                                                                 | American Psychological Association (APA)                       | APA Dictionary of Psychology                                           |                            |                            | https:/ /dictionary.apa.org/reflexivity                                                                                                                                           |                            |\n| Lee_See_2004                                     | Trust in Automation: Designing for Appropriate Reliance                                     | John D. Lee and Katrina A. See                                 | Human Factors: The Journal of the Human Factors and Ergonomics Society | 46 1                       | 50-80                      | 2004 https:/ /doi.org/10.1518/hfes.46.1.50_30392                                                                                                                                  |                            |\n| Mayer_Davis_Schoorman_                           | Mayer_Davis_Schoorman_                                                                      | Mayer_Davis_Schoorman_                                         | Mayer_Davis_Schoorman_                                                 | Mayer_Davis_Schoorman_     | Mayer_Davis_Schoorman_     | Mayer_Davis_Schoorman_                                                                                                                                                            | Mayer_Davis_Schoorman_     |\n| 1995                                             | An Integrative Model of Organizational Trust                                                | Roger C. Mayer, James H. Davis, and F. David Schoorman         | The Academy of Management Review                                       | 20 3                       | 709-734 1995               | https:/ /doi.org/10.2307/258792                                                                                                                                                   |                            |\n| NISTIR_8280                                      | NISTIR 8280. Face Recognition Vendor Test (FRVT). Part 3Ê¼ Demographic Effects               | Patrick Grother, Mei Ngan, and Kayee Hanaoka                   | NIST                                                                   |                            |                            | 2019 https:/ /nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8280.pdf                                                                                                                  |                            |\n| Usabilitygov                                     | Usability Testing                                                                           | Usability.gov                                                  | Usabillity.gov                                                         |                            |                            | https:/ /www.usability.gov/how-to-and-tools/methods/usability-testing.html                                                                                                        |                            |\n| Encyclopedia.                                    | Encyclopedia.                                                                               | Encyclopedia.                                                  | Encyclopedia.                                                          | Encyclopedia.              | Encyclopedia.              | Encyclopedia.                                                                                                                                                                     | Encyclopedia.              |\n| com_underrepresentation                          | Underrepresentation                                                                         | Encyclopedia.com                                               | Encyclopedia.com                                                       |                            |                            | https:/ /www.encyclopedia.com/social-sciences/applied-and-social-sciences-magazines/underrepresentation                                                                           |                            |\n| Arham_Islam_History_202 3 McKinsey_generative_AI | A History of Generative AI: From GAN to GPT-4 What is generative AI?                        | Arham Islam McKinsey & Company                                 | MarkTechPost McKinsey & Company                                        |                            | 2023                       | https:/ /www.marktechpost.com/2023/03/21/a-history-of-generative-ai-from-gan-to-gpt-4/ 2023 https:/ /www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai |                            |\n| Seshia_et_al_2022                                | Toward Verified Artificial Intelligence                                                     | Sanjit A. Seshia, Dorsa Sadigh, and S. Shankar Sastry          | Communications of the ACM                                              | 65 7                       | 46-55                      | 2022 https:/ /cacm.acm.org/magazines/2022/7/262079-toward-verified-artificial-intelligence/fulltext                                                                               |                            |\n| Liam_Tung_2022_Meta_h                            | Liam_Tung_2022_Meta_h                                                                       | Liam_Tung_2022_Meta_h                                          | Liam_Tung_2022_Meta_h                                                  | Liam_Tung_2022_Meta_h      | Liam_Tung_2022_Meta_h      | Liam_Tung_2022_Meta_h                                                                                                                                                             | Liam_Tung_2022_Meta_h      |\n| allucination                                     | Meta warns its new chatbot may forget that it's a bot                                       | Liam Tung                                                      | ZDNet                                                                  |                            |                            | 2022 https:/ /www.zdnet.com/article/meta-warns-its-new-chatbot-may-not-tell-you-the-truth/                                                                                        |                            |\n| Merriam-                                         | Merriam-                                                                                    | Merriam-                                                       | Merriam-                                                               | Merriam-                   | Merriam-                   | Merriam-                                                                                                                                                                          | Merriam-                   |\n| Webster_requirement                              | requirement                                                                                 | Merriam-Webster                                                | Merriam-Webster Dictionary                                             |                            |                            | https:/ /www.merriam-webster.com/dictionary/requirement                                                                                                                           |                            |\n| TTC6_Taxonomy_Terminol                           | TTC6_Taxonomy_Terminol                                                                      | TTC6_Taxonomy_Terminol                                         | TTC6_Taxonomy_Terminol                                                 | TTC6_Taxonomy_Terminol     | TTC6_Taxonomy_Terminol     | TTC6_Taxonomy_Terminol                                                                                                                                                            | TTC6_Taxonomy_Terminol     |\n| ogy                                              | EU-U.S. Terminology and Taxonomy for Artificial Intelligence - Second Edition               | EU-US Trade and Technology Council (TTC) Working Group 1 (WG1) |                                                                        |                            |                            |                                                                                                                                                                                   |                            |\n\n[1] Add citation to citations sheet and only list ID in these columns\n\n- [2] Make sure the spelling matches another term (value in A column)", "fetched_at_utc": "2026-02-09T14:33:48Z", "sha256": "dfa16a5f31a8ba4008bce8c3464dab22d954121f59f0a0936e67a17a5d1b0a23", "meta": {"file_name": "The Language of Trustworthy AI Glossary - NIST.pdf", "file_size": 640715, "mtime": 1766930323, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-protect-framework-managing-data-risks-in-the-ai-era-oliver-patel-cda79ebd0a7b", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel.pdf", "title": "The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel", "text": "<!-- image -->\n\nHey\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis week's newsletter presents, for the first time, my PROTECT Framework for Managing Data Risks in the AI Era Â© 2025.\n\nLast week I published Part 1 of my 2-part series on the Top 10 Challenges for AI Governance Leaders in 2025. It focused on how the democratisation of AI, coupled with the sheer volume and rapid velocity of AI initiatives, is putting serious strain on the enterprise AI governance function. It outlined how, in response to these challenges, AI governance leaders must refine and update their risk-based approach and narrow the focus of their teams' work, to avoid being overwhelmed, distracted, or neglecting the most serious risks.\n\nAlthough I promised Part 2 this week, I ended up writing a full article on the fourth challenge: protecting confidential business data. This is a hugely important topic and there was simply too much to say. The beauty of having your own Substack is that you can follow whichever creative or intellectual direction most appeals to you, rather than rigidly sticking to prior plans. I hope that you will indulge my partial detour and that you find value in this article for your work. The 'real' Part 2, covering challenges 510, will have to wait another week.\n\n## Challenge 4. Protecting Confidential Business Data\n\nHow can enterprises protect confidential business data when there is immense hunger to\n\nexperiment with and use the latest AI applications that are released on the market?\n\nThe generative AI boom has amplified and exacerbated a plethora of data risks that all enterprises are exposed to. It has never been more important to ensure that your AI governance, cybersecurity, and information security frameworks are designed to, and capable of, mitigating these risks.\n\nHowever, designing, implementing, and scaling robust controls that actually protect your organisation's data and intellectual property requires precise understanding of what these risks are and how they are impacted by AI. That's where my PROTECT Framework comes in.\n\nThe PROTECT Framework empowers you to understand, map, and mitigate the most pertinent data risks that are fuelled by widespread adoption of generative AI. Below is a high-level summary of the framework, followed by a detailed breakdown of each of the 7 themes.\n\n## PROTECT: Managing Data Risks in the AI Era\n\nThe PROTECT Framework focuses primarily on protecting ( no surprises there ) confidential business data from exposure, disclosure, and misuse-as well as associated data privacy and security risks fuelled by AI. It also outlines how\n\norganisations can use data in a compliant way, in the context of AI development, deployment, and use.\n\n- P - Public AI Tool Usage\n- R - Rogue Internal AI Projects\n- O - Opportunistic Vendors\n- T - Technical Attacks and Vulnerabilities\n- E - Embedded Assistants and Agents\n- C - Compliance, Copyright, and Contractual Breaches\n- T - Transfer Violations\n\nThe rest of the article breaks down each of the seven themes, highlighting both the core risks that enterprises are exposed to, as well as practical mitigations that can be implemented to manage and control these risks.\n\nMy forthcoming book, Fundamentals of AI Governance, provides a comprehensive visual overview of the PROTECT Framework, as well as a deep-dive on the Top 10 AI\n\nRisks impacting the modern enterprise. To secure a 25% discount during the prelaunch period, sign up at the link below.\n\n## P - Public AI Tool Usage\n\nEnterprise risks: Use of publicly available AI tools is arguably the most severe data risk, because of how easy it is to do and how difficult it is to prevent. Simply put, there are thousands of publicly available AI tools that anyone can access via the internet, most of which are free or cheap. As well as mainstream generative AI chatbots like Claude, Gemini and ChatGPT, there are countless AI tools for creating presentations, managing email inboxes, transcribing meetings, and generating videos.\n\nNo matter how mature your enterprise AI capabilities are-and even if you are a frontier AI company- it is not going to be feasible to keep up with the latest and greatest AI tools and AI models that are released on the market each day, whilst also performing robust due diligence on AI vendors. Even with enterprise-grade licenses to mainstream generative AI services, you will not always get immediate access to the latest features included in the consumer version. This fuels immense hunger for employees to experiment with and use the most cutting-edge AI tools, irrespective of whether they are 'internal' and approved or 'publicly available' and unapproved.\n\nThis inability to keep pace with the market, coupled with a lack of awareness regarding the risks of using publicly available AI tools and the pressure that employees and teams are under to become 'AI-first', exacerbates the risks. Many employees may not understand the difference, from a data risk perspective, between using internal AI tools and public AI tools. But the risk is real. For example, when enterprise data is shared with publicly available AI tools, the organisation no longer has any control over what happens to it. This confidential business data could be used to train the AI models that power publicly available AI tools and in turn be disclosed, via future AI outputs, to competitors or malicious actors. It may even end up on the public internet-as we saw when various shared AI chat logs were indexed and publicly accessible online-or be retained indefinitely, as a result of court orders like the one OpenAI faced from the New York Court. Simply put, if you want to be in control of how your data and intellectual property is used, who has access to it, and for how long it is retained, your employees should avoid using publicly available AI tools.\n\nPractical mitigations: Although shadow AI use is ubiquitous, there are various controls you can implement to mitigate this risk. My 3 Gs for Governing AI Democratisation offers a useful starting point:\n\nGuidance: educate and train the workforce on the risks of using publicly available AI tools and the importance of protecting confidential business data.\n\nGreenlight: provide access to secure, best-in-class AI tools, platforms, and capabilities that are approved for internal use and can process confidential business data.\n\nGuardrails: implement guardrails-including technical and legal mitigations-to mitigate outstanding data risks that the use of internally approved AI tools entails. This can include scanning and blocking certain types of data from being uploaded as an input or generated as an output.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n## R - Rogue Internal AI Projects\n\nEnterprise risks: Bypassing governance, especially when it happens at scale, creates compliance blind spots. Various risks emerge, and are difficult to manage, when teams develop and deploy AI systems, or procure AI solutions from vendors, without adhering to the mandatory AI governance, privacy, and cyber security processes.\n\nIn such scenarios, there is unlikely to have been any legal or compliance review, privacy assessment, or security evaluation. In turn, this means that genuine risks are\n\nunlikely to be understood, required documentation and artefacts may not have been produced, and robust mitigations will not be in place to address any important risks.\n\nThis increases organisational technical debt, which can lead to costly and burdensome efforts to retrospectively re-engineer non-compliant AI systems that are already deployed in production. Finally, it increases the likelihood that data is used without authorisation, and in a manner that constitutes a contractual breach or potential compliance violation.\n\nIn most cases, this does not happen because of malicious internal actors or intentional rule-breaking. Rather, it is more likely due to enthusiasm, competitive internal pressures and competing priorities, or a lack of awareness of internal governance processes and how to navigate them.\n\nPractical mitigations: To mitigate the risk of rogue internal AI projects bypassing compliance checks, a proportionate degree of oversight must be applied to all AI projects. The level of governance scrutiny and oversight should flex in relation to the type of data being used. The use of sensitive personal data, confidential business data, or copyright protected material should entail more rigorous oversight, both at the project outset and throughout the AI lifecycle. Oversight should also be more rigorous for scenarios that involve sharing data with external vendors and the applications they provide.\n\nThe most important thing you can do is to make your AI and digital governance processes as easy to navigate as possible, by integrating different processes where possible, providing accessible guidance and support, and using automation to streamline processes and improve the user experience.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n## O - Opportunistic Vendors\n\nEnterprise risks: Almost all enterprises must work with, and procure from, external organisations to progress with their AI ambitions. In the generative AI era, the trend is from build to buy. Whether it is leveraging pre-trained foundation models and generative AI chatbots, or working with vendors that provide bespoke AI products, exposure to third-party AI risk is unavoidable. In some cases, AI vendors and service providers may seek to use your confidential business data to train, develop, and improve their AI models and services-potentially without your explicit knowledge or consent.\n\nThe terms of service for many AI platforms and products are ambiguous or difficult to understand, and grant vendors broad rights over customer data. The key risk is whether your organisation's data is used to train AI models that other customers can access and use. If so, competitors (or any other organisation) using that same vendor's products and services may benefit from insights derived from your data. The risk is lower-or potentially fully mitigated-if your data is only used to train AI models and services that only your organisation has access to. This can enable you to benefit from feature improvements and customisation whilst mitigating data exposure and leakage risks.\n\nPractical mitigations: Consider contractually prohibiting AI vendors from using your data (e.g., prompt, input, log, and output data), to train AI models and improve services that are accessible to other customers. This requires robust vendor due diligence, and a specific AI governance process pathway for AI procurement. It also requires clear guidance and training on third-party AI risks, acceptable data use terms, as well as template contracts and addendums that can be used across the business. Although the demand for AI vendors (and the products they provide) is high, the presence of opportunistic or shady operators in the market-and the immense value of the data they can obtain from enterprise customers-makes rigorous due diligence and contractual safeguards essential.\n\n## T - Technical Attacks and Vulnerabilities\n\nEnterprise risks: AI systems introduce novel attack vectors that are linked to the distinct vulnerabilities of these systems. In particular, generative AI and agentic AI systems can be compromised and exploited, leading to confidential data-that was part of the AI model's training, input, or output data-being extracted and stolen. Such data exfiltration is a known and widely documented AI vulnerability and can be caused by various attack methods. Prompt injection attacks, for example, are when an AI model is provided with malicious inputs (during inference) that are designed to manipulate and steer the outputs it generates, by jailbreaking model guardrails.\n\nSimply put, the goal of prompt injection is to make the AI model do something that it is not supposed to. This includes, but is not limited to, data exfiltration, as well as reconstruction of training data. This risk of prompt injection is amplified with agentic AI, given the ability of AI agents to use tools and execute actions that can have a material impact (rather than 'just' generate outputs for consumption).\n\nPractical Mitigations: Although there are no foolproof mitigations against prompt injection attacks, there are nonetheless important steps you can take. Consider implementing AI system-level security controls and guardrails including input\n\nvalidation, prompt sanitisation, output filtering, and incident monitoring and detection. Also, for high risk applications, conduct red-teaming and adversarial testing. Cybersecurity best practice emphasises the importance of multiple overlapping security layers. However, no technical controls can fully prevent prompt injection or data exfiltration. Therefore, carefully control who has access to sensitive AI systems, what data they can access, and what actions agentic AI can execute.\n\n## E - Embedded Assistants and Agents\n\nEnterprise risks: The AI assistants and agents that are increasingly embedded in the core workplace software we all use pose novel data risks. In particular, these embedded AI tools can inappropriately disclose and disseminate data to people and groups that were not supposed to have access to it. For example, a personalised AI assistant that summarises your emails and daily tasks can analyse wider organisational data that you have access to, such as document libraries and shared calendars. If that data has not been protected with appropriate file sharing permissions-and moreover has erroneously been made available to the entire organisation-then elements of it may be surfaced to you via your handy AI assistant.\n\nAlthough the root cause of this is often inappropriate or inadequate file sharing\n\npermissions, AI significantly increases the likelihood of such data being shared with the wrong person. It also provides malicious internal actors with a powerful tool for mischief.\n\nAI meeting assistants and note-taking applications are of particular concern. It is commonplace to join calls with external organisations, only to find that a random AI bot is also on the call, recording and transcribing everything that is said. From an enterprise perspective, this is akin to uploading all of this information into a publicly available AI tool (which poses similar risks to those outlined in 'Public AI Tool Usage '), unless you have assurances from the external organisation regarding the technology they are using and how it processes your data. Furthermore, be wary of individuals having access to AI meeting recordings and transcripts of parts of the discussion they were not part of.\n\nIncreasingly autonomous agentic AI systems exacerbate these risks. In order to effectively determine the best course of action and use tools to execute tasks, LLMbased AI agents will need to mine, retrieve from, and synthesise myriad enterprise data sources. Establishing appropriate access controls, and maintaining AI agent audit trails, will become increasingly complex yet important.\n\nPractical mitigations: Robust data governance and data risk management is critical to ensuring your increasingly autonomous AI tools do not cause havoc. Ensuring\n\nappropriate file sharing permissions for sensitive and critical data sources is paramount, given the ways in which AI agents can mine through your document libraries and other repositories, as well as the obvious value this capability provides. Also, when deploying agentic AI, start with lower risk use cases, applications, and data sources. Furthermore, apply the principle of least privilege, to ensure that AI agents only have access to data that is necessary for their tasks.\n\n## C - Compliance, Copyright, and Contractual Breaches\n\nEnterprise risks: Data science, AI, and business teams are under significant pressure to leverage AI to deliver value for the business. To do so, they require seamless access to vast amounts of high-quality, business-critical data. However, the increasingly stringent data and AI regulatory landscape-particularly in the EU-creates numerous compliance risks when using data for AI activities. It is therefore crucial to have robust controls in place to prevent unauthorised or non-compliant use of data. The most important regulatory and legal domains to consider are:\n\nPrivacy and data protection: Privacy and data protection laws restrict the way in which personal data-in particular sensitive personal data-can be used. For example, under the EU's GDPR, you must have a lawful basis to process personal data. Personal\n\ndata processing is therefore only lawful if at least one of the following lawful bases apply: i) consent, ii) performance of a contract, iii) legal compliance, iv) protection of vital interests, v) performance of a task in the public interest, or vi) legitimate interests. Therefore, just because you have access to personal data, does not mean you are permitted by default to use it to develop or deploy AI.\n\nCopyright and intellectual property: Organisations must be cautious when using external data for AI development and deployment, as it is often copyright protected. Different data sources come with different licenses, terms, and conditions. This cautiousness must extend to 'everyday' employee use of generative AI tools-in particular their prompts and document uploads.\n\nContracts: Your organisation may have access to data that another organisation provided in the course of an engagement, such as the use of a product or service you provide, that is governed by a bespoke legal agreement. Therefore, you must protect and handle that data in accordance with the applicable legal agreement.\n\nAI-specific laws: Finally, AI regulations like the EU AI Act typically include provisions that stipulate how data should be used in the context of AI activities. For example, the AI Act requires providers of high-risk AI systems to use high quality, accurate, and 'representative' training, validation, and testing data sets, in order to mitigate bias risks and promote reliable AI performance.\n\nPractical mitigations: The legal and regulatory domains outlined above are vast; comprehensive risk mitigation across all of them is beyond the scope of the PROTECT Framework. However, the overarching principle is that you must embed proportionate governance and oversight throughout the AI lifecycle, to prevent non-compliant or unauthorised data use. This means legal and compliance review must occur at critical stages, including before data is sourced, before AI models are trained, and before AI systems are deployed in production or released on the market. As ever, this governance must be complemented and reinforced by company-wide and rolespecific training. When these governance checkpoints are bypassed-as discussed in the 'Rogue Internal AI Projects' theme-the aforementioned data-related compliance and legal risks materialise.\n\n## T - Transfer Violations\n\nEnterprise risks: Leveraging cloud-based AI services, such as platforms for accessing and using foundation models, almost always involves international data transfers. Given that AI processing is rarely confined to one jurisdiction, navigating international data transfer compliance is an important part of AI governance.\n\nPrivacy and data protection regimes worldwide restrict the way in which personal data can be transferred internationally. Under the GDPR, organisations can transfer personal data freely from the EU to entities in a non-EU country if there is an EU adequacy decision in place. An adequacy decision is the EU's way of ' protecting the rights of its citizens by insisting upon a high standard of data protection in foreign countries where their data is processed '. 15 jurisdictions are recognised as 'adequate' by the EU. This includes the U.S.-but it only applies to commercial organisations participating in and certified under the EU-U.S. Data Privacy Framework. If there is no EU adequacy decision, alternative legal safeguards must be put in place (e.g., Standard Contractual Clauses), before personal data can be transferred from the EU to the non-EU jurisdiction.\n\nOn a separate note, the U.S. 'Bulk Data Transfer Rule' prohibits or restricts organisations from transferring 'U.S. sensitive personal data' and 'government-related data' to 'countries of concern', including China, Cuba, Iran, North Korea, Russia, and Venezuela. The Rule was issued by the Department of Justice in January 2025.\n\nPractical mitigations: Although the above was far from an exhaustive overview of international data transfer regulations, the key point is that you must implement specific mitigations-as required by the applicable law-prior to transferring personal data across borders. For example, before onboarding U.S. AI vendors and service", "fetched_at_utc": "2026-02-09T14:34:18Z", "sha256": "cda79ebd0a7b3e04702ca480922554cb42a7bdbd2a5c2c58631cf28b7a3f72de", "meta": {"file_name": "The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel.pdf", "file_size": 931949, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-singapore-consensus-on-global-ai-safety-research-priorities-6a7795a9bc09", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The Singapore Consensus on Global AI Safety Research Priorities.pdf", "title": "The Singapore Consensus on Global AI Safety Research Priorities", "text": "<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:35:53Z", "sha256": "6a7795a9bc09cc8229d7af48a1ba159402a1652fc76a43104b8c86bd600cc107", "meta": {"file_name": "The Singapore Consensus on Global AI Safety Research Priorities.pdf", "file_size": 14759899, "mtime": 1767346293, "docling_errors": []}}
{"doc_id": "pdf-pdfs-the-ultimate-guide-to-ai-literacy-oliver-patel-c43706585b13", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The Ultimate Guide to AI Literacy - Oliver Patel.pdf", "title": "The Ultimate Guide to AI Literacy - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable, and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement, and master AI governance.\n\nIf you haven't already, subscribe below and share it with your colleagues and friends. Thank you for your support!\n\nThis week's edition is a practical guide to implementing and scaling an impactful AI literacy programme. It covers:\n\n- âœ… What is AI literacy and why does it matter?\n- âœ… What does the EU AI Act require?\n- âœ… The 4 Layers of AI Literacy (the 'what')\n- âœ… 8 Practical Tips for AI Literacy Success (the 'how')\n- âœ… The 3 Es for Impact: Educate, Engage, Empower (the 'why')\n- âœ… AI Literacy Cheat Sheet scroll to the end to download the high-res pdf!\n\nPlease note that this analysis is geared towards larger organisations in the private and public sector, as opposed to educational institutions like schools and universities, which have a unique set of challenges and considerations.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week.\n\n## What is AI literacy and why does it matter?\n\nAI literacy is a crucial part of modern business in 2025.\n\nAny organisation investing in AI technology and implementation will struggle to maximise the value of AI without embedding AI knowledge, skills, and understanding across its workforce.\n\nIf people don't know how to use AI, you are not going to achieve ROI. According to McKinsey, 48% of employees rate training as the most important factor driving their adoption of generative AI. However, many feel that they receive inadequate support.\n\nAI literacy means educating and upskilling employees on AI. A comprehensive, dual approach to AI literacy focuses on both AI risks and opportunities. Specifically, AI literacy deepens understanding of how to anticipate and mitigate AI risks, avoid harms, comply with regulations, and use AI in a safe and responsible way. It also advances\n\nunderstanding of what AI is, what it can and cannot do, how the technology is developing, and how it can be used in a positive, impactful, and transformative way.\n\nAI literacy matters because AI is far more sophisticated and complex than most other tools and technologies we use. AI can mimic, replicate, and even surpass the cognitive, intellectual, and creative abilities of human experts across an infinite number of tasks and topics, from coding to poetry. Across hundreds of thousands of years of human history, this has never before been possible.\n\nTherefore, unlike many other tools and technologies we use each day, safe, responsible, effective, and impactful use of AI is far from obvious or intuitive. It requires insight, understanding, and the curiousity to keep up with the latest developments.\n\n## According to DataCamp:\n\n- 62% of leaders believe AI literacy is important for their teams' daily tasks. Bear in mind this covers leaders working across many business functions, not just AI and data.\n- 'Basic understanding of AI concepts' and 'AI ethics and responsible AI best practices' are ranked by leaders (70% and 69% respectively) as the two most important AI skills for their teams.\n\n## What does the EU AI Act require?\n\nArticle 4 of the EU AI Act is simply titled 'AI literacy'. This humble article is undeniably a big part of why this has become such a hot topic.\n\nAI literacy is now mandatory. It was one of the first EU AI Act provisions to become applicable, alongside prohibited AI practices, on 2 February 2025.\n\nHere is the full text of Article 4:\n\nProviders and deployers of AI systems shall take measures to ensure, to their best extent, a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, education and training and the context the AI systems are to be used in, and considering the persons or groups of persons on whom the AI systems are to be used.\n\nAI literacy is also defined in Article 3(56) of the EU AI Act:\n\n' AI literacy' means skills, knowledge and understanding that allow providers, deployers and affected persons, taking into account their respective rights and obligations in the context of this Regulation, to make an informed deployment of AI systems, as well as to gain awareness about the opportunities and risks of AI and possible harm it can cause.\n\nHere is a simplified breakdown of what this means and what you need to do.\n\nOrganisations which develop and use AI must ensure their workforce-especially those who develop, use, and operate high-risk AI systems-have the requisite skills, knowledge, and understanding to enable AI risk mitigation, regulatory compliance, and the protection of people from potential harms and other negative impacts of AI.\n\nAlthough organisations will not be fined for failure to comply with the AI literacy provision in Article 4 (as it is not explicitly covered in the penalty and enforcement regime), such non-compliance will certainly not help in the event of investigations, enforcement action, or legal proceedings relating to other EU AI Act provisions.\n\nFurthermore, it will be practically impossible to comply with many other aspects of the EU AI Act without implementing robust AI literacy.\n\nFor example, Article 14 obliges deployers to: assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support .\n\nThis effectively mandates tailored, role-based training for those tasked with oversight of high-risk AI systems. Although AI literacy should be interpreted as applying to the whole organisation, this is a key part of it.\n\n## The 4 Layers of AI Literacy\n\nThe 4 Layers of AI Literacy is a framework which I developed that describes the 'what' of AI literacy.\n\nFor your AI literacy programme to be robust, compliant, and effective, all 4 of these elements must be part of it.\n\n## 1. AI governance fundamentals\n\n- a.  Mandatory training for the entire organisation.\n- b.  Educate the workforce on the key pillars of responsible AI and the AI policies and processes. The high level Do's and Don'ts.\n- c.  Must be straightforward, accessible, and easy for all to understand.\n\n## 2. Generative AI empowerment\n\n- a.  Upskill and empower the workforce to adopt and embrace AI tools and technologies, from generative AI to agentic AI.\n- b.  Incentivise uptake via gamification and leverage external expertise and resources.\n- c.  Must be inspirational, interactive, and hands-on. Provide safe environments for experimentation and discovery.\n\n## 3. Persona and role-based training\n\n- a.  Tailored training for specific personas who build, buy, use, deploy, or govern AI as a core part of their work.\n- b.  Target key personas including AI governance, privacy, procurement, data scientists, and IT business partners.\n- c.  Must be engaging, practical, and relevant for the role, as well as likely scenarios which will arise.\n\n## 4. AI system specific\n\n- a.  Mandatory training for end users and others responsible for operating highrisk AI systems.\n- b.  Bespoke instructions and guidance on implementing human oversight, transparency, and other risk mitigation measures.\n- c.  Must be context-specific, ensuring end users can interpret AI outputs and detect serious incidents.\n- d.  Requires collaboration between providers and deployers.\n\nAI literacy is about much more than generic company-wide training. Although this is important for compliance, it is only the first layer.\n\nFor example, without implementing Layer 4, you cannot comply with the AI Act's human oversight requirements.\n\nThis AI literacy framework is not intended to cover absolutely everything. It should be complementary to relevant educational initiatives focusing on data literacy, cyber security awareness, ethics training etc.\n\n## 8 Practical Tips for AI Literacy Success\n\nNow that you know what your AI literacy programme should consist of, here are 8 practical tips that will help you succeed. These tips are the 'how' of AI literacy.\n\n1. Focus on the 'so what'. Identify the end goal of AI literacy, then reverse engineer it. There is no point in rolling out AI literacy merely for the sake of it. In the planning phase, carefully consider what you are hoping to achieve and design your programme accordingly. Consider the skills, capabilities, and knowledge you need to embed and the type of organisation you want to become. Critical objectives should include:\n2. ensuring all employees have a baseline understanding of responsible and compliant use of AI;\n3. driving understanding regarding what AI is, how to use it effectively, which AI tools and technologies are available, and how to identify promising use cases;\n\n- providing employees with high-quality training and development opportunities, to boost morale, engagement, and retention; and\n- preparing the workforce for the jobs of the future and fostering the skills required to succeed.\n\nA world-class AI literacy programme will be designed and structured to achieve all of these objectives. But if your resources are constrained, start with the first.\n\n2. Diversify your content, formats, and pathways. Combine live, self-paced, virtual, and in-person learning. We have never had access to more high-quality, engaging, and free educational content. Your audience-whose attention you are competing for-has high standards for what keeps them engaged. Therefore, your content needs to be at least as good, if not better than, what can be found externally. The best thing you can do is diversify. Offer a range of different types of content, including live keynotes, panel discussions, workshops, in-person events and conferences, and online modules with videos, infographics, and synthesised materials. Traditional teaching and learning methods are becoming archaic in the era of YouTube, TikTok, DuoLingo, and AI coaching apps; meet your learners where they are.\n3. Leverage internal and external experts. Partner with leading educational institutions. There are thought leaders and experts all over your organisation. Your job is to find them and give them a voice. Your AI upskilling initiatives represent an ideal opportunity to provide a platform to individuals and teams\n\n- leading on impactful and transformative work. Being proactive in convening a broad range of voices is win-win. Your learners will benefit from a richer experience and you will forge strong relationships with key stakeholders across different functions, who will appreciate the opportunity for visibility. You should also mix it up by bringing in external experts, who can offer thought-provoking, challenging, and even divergent views. This could be leading academics and researchers, policymakers and regulators, or industry leaders from other sectors.\n4. Gamify your learning pathways to drive engagement and incentivise uptake. Gamification, defined as 'using game design elements in non-game contexts', is an increasingly popular approach to designing and delivering effective learning experiences, in both educational and professional settings. Gamification covers a broad range of different techniques, including points, badges, and leaderboards (PBL), levels, feedback, challenges, and even missions. Many studies prove that introducing such elements leads to more engaged, motivated, and proactive learners. To keep your learning participants inspired and excited, and to generate momentum across the organisation, offer badges, accreditations, and awards, for a more meaningful and rewarding learning journey.\n5. Hands-on, practical, and interactive learning is always most effective and impactful. If it's not hands on, it will quickly be forgotten. The biggest flaw with most AI trainings is that participants do nothing with AI. This makes no sense. You need to be creative in incorporating practical and scenario-based activities, which\n\n- give people a chance to experiment with AI tools, understand their capabilities and limitations, and tackle realistic problems and challenges. Simply learning how to 'prompt' is not going to cut it in 2025. Get people involved in deeper activities, such as hackathons, AI model red-teaming, reviewing live cases, AI incident response, use case ideation, and product development workshops.\n6. Enable tailored, role-based learning to be provided for every function and business unit. Different people and teams will require specialisation in different aspects of AI and AI governance, based upon the core focus of their role or work. You should offer advanced pathways which cater to different audiences. You should also provide frameworks, tools, and resources which enable every function to develop bespoke, role-based AI literacy programmes, building on the company-wide curriculum. You won't have the bandwidth to create training for every group, but you can enable and empower this work to be done by others (e.g., local L&amp;D teams).\n7. Build a community of engaged learners and create spaces for vibrant discussions. Community building is the best way you can elevate AI literacy beyond a training programme. Content on AI is abundant and never-ending. There is no shortage of educational materials for people to study and consume. What is harder to come by is opportunities for meaningful and in-depth discussions, with people from many different geographies, departments, roles, and backgrounds. Given the concurrently high levels of concern, confusion, and\n\n- excitement about the impact of AI on the future of work, keeping the discussion going and encouraging this collaboration is key.\n8. Provide successful learners with tangible opportunities for impact and career growth. There should be ample opportunities for employees to apply what they have learnt, to benefit their career and the wider organisation. Don't just reward and acknowledge with certificates and badges, without providing a meaningful follow up or next step. Connect AI literacy and upskilling achievements with opportunities for promotion, career advancement, and change. For example, empower people with specific roles or leadership positions-such as mentoring and guiding others, chairing forums, championing AI initiatives, and leading on parts of future AI literacy initiatives-which they can perform alongside their day job. Alongside gamification, providing meaningful opportunities for growth and impact is the best way to keep learners engaged.\n\n## The 3 Es for Impact: Educate, engage, empower\n\nWe've covered the 'what' (The 4 Layers of AI Literacy) and the 'how' (8 Practical Tips for AI Literacy Success). To conclude, we will focus on the 'why'.\n\nAI governance is change management. And AI literacy and upskilling is at the heart of driving that cultural change.\n\nFirst and foremost, AI literacy is about serving others. Your role is to ensure that colleagues across your organisation feel like they are playing an active part in the AI revolution, rather than it being something passively happening to them, or worse, passing them by.\n\nYou can use my 3 Es for Impact framework to shape and guide your approach, to ensure you fulfil this overall objective of serving others.\n\nPrioritising these 3 Es is vital for delivering the meaningful change and impact you hope to achieve, for your organisation and workforce.\n\nEducate: It is challenging to track and keep up with all of the developments in AI and AI governance, even for professionals working at the cutting-edge. It is even harder to cut through the noise and figure out what trends and developments really matter. Therefore, by synthesising and simplifying the vast amount of AI-related educational content which is out there, rendering it digestible, useful, and actionable for your organisation, and outlining the foundational and conceptual basis required for deeper understanding, you are providing immense value.\n\nEngage: Be creative and use all of the tools and resources at your disposal to cultivate buzz and excitement, motivate learning, facilitate community building and, ultimately, engage learners with a diverse range of practical and interactive offerings, delivered by a diverse cast of educators. As an AI governance leader, you cannot bury your head in\n\nthe sand. Being an impactful, visible, and engaged thought leader in your organisation is non negotiable.\n\nEmpower: AI literacy should be an empowerment programme, not a training programme. There is a palpable sense of concern, and even fear, about the impact of AI on the future of careers and work. People are uncertain regarding which jobs will be automated or outsourced, what skills they should develop, and what to prioritise. According to Pew Research, 52% of employed adults are worried about the future impact of AI in the workforce. By providing a safe and inspiring space for people across the organisation to come together and discuss, debate, and collaborate, you are empowering people to shape and take control of their future. This action drives empowerment.\n\n## AI Literacy Cheat Sheet\n\n<!-- image -->", "fetched_at_utc": "2026-02-09T14:36:20Z", "sha256": "c43706585b1399bb24f1c7e67ab8199382b939b6cf44d281aa6ad11c6b1fdf3e", "meta": {"file_name": "The Ultimate Guide to AI Literacy - Oliver Patel.pdf", "file_size": 1523395, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-trust-and-transparency-in-ai-industry-voices-on-data-ethics-and-compliance-9148829b9ca7", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Trust and Transparency in AI - Industry Voices on Data, Ethics and Compliance.pdf", "title": "Trust and Transparency in AI - Industry Voices on Data, Ethics and Compliance", "text": "## Trust and Transparency in AI: Industry Voices on Data, Ethics, and Compliance\n\nLouise McCormack 1* , Diletta Huyskes 2 , Dave Lewis 3 , Malika Bendechache 1\n\n1 ADAPT Research Centre, School of Computer Science, University of Galway, Galway, Ireland.\n\n2\n\nADAPT Research Centre, Trinity College Dublin, Dublin, Ireland.\n\nUniversity of Milan, Milan, Italy. 3\n\n*Corresponding author(s). E-mail(s): louise.mccormack@adaptcentre.ie; Contributing authors: diletta.huyskes@unimi.it; dave.lewis@adaptcentre.ie; Malika.bendechache@adaptcentre.ie;\n\n## Abstract\n\nThe EU Artificial Intelligence (AI) Act directs businesses to assess their AI systems to ensure they are developed in a way that is human-centred and trustworthy. The rapid adoption of AI in the industry has outpaced ethical evaluation frameworks, leading to significant challenges in accountability, governance, data quality, human oversight, technological robustness, and environmental and societal impacts. Through structured interviews with fifteen industry professionals, paired with a literature review conducted on each of the key interview findings, this paper investigates practical approaches and challenges in the development and assessment of Trustworthy AI (TAI). The findings from participants in our study, and the subsequent literature reviews, reveal complications in risk management, compliance and accountability, which are exacerbated by a lack of transparency, unclear regulatory requirements and a rushed implementation of AI. Participants reported concerns that technological robustness and safety could be compromised by model inaccuracies, security vulnerabilities, and an overreliance on AI without proper safeguards in place. Additionally, the negative environmental and societal impacts of AI, including high energy consumption, political radicalisation, loss of culture and reinforcement of social inequalities, are areas of concern. There is a pressing need not just for risk mitigation and TAI evaluation within AI systems but for a wider approach to developing an AI landscape that aligns with the social and cultural values of the countries adopting those technologies.\n\nKeywords: Trustworthy AI, Regulatory Compliance, Data Management, AI Governance, Ethical AI\n\n## 1 Introduction\n\nArtificial Intelligence (AI) is being increasingly adopted by a variety of industries, including essential sectors like healthcare, education, transport,\n\nNote: This work has been accepted for publication in AI and Society .\n\nand climate action, necessitating regulations to ensure the safe, ethical, and effective use of AI systems[1].\n\nIn Europe, existing regulations covering these sectors are being supported by the introduction of the Artificial Intelligence Act (AI Act) ([2]) which are further enhanced by the General Product\n\nSafety Regulation ([3]) and the Product Liability Directive ([4]), which both explicitly include aspects of software. The AI Act seeks to enable seamless access of AI products to the single market while protecting health, safety and fundamental rights, as well as the environment and democracy rights.\n\nPreceding the introduction of the AI Act, recent years have seen intense activity in developing principles and guidelines that claim to support trustworthy AI systems, sometimes under the label of ethical or responsible AI ([5]). Where adopted, these were typically voluntary measures, with little external validation and often appealing to subjective or contested forms of behaviour. This also led to accusations of ethics washing ([6]). In contrast, on entering into law in August 2024, the EU's AI Act([2]) states that it is a comprehensive legally binding set of rules for AI systems that integrates Europe's existing legal protections of fundamental rights (based on the European Charter of Fundamental Rights (CFR)([7]) with its existing framework for harmonised product safety certification across the European single market. The requirement for AI products to address the protection of fundamental rights can be considered the practical transposition of many of the concerns addressed by prior Trustworthy AI initiatives into an enforceable legal framework. However, with the AI Act having entered into law in August 2024 and the enforcement of its different provisions being scheduled for phased introduction over a 36 month period[1], it will be several years before concrete enforcement experience and case law becomes evident.\n\nTrustworthy AI (TAI) in this context therefore refers to an ethical, human-centred approach to artificial intelligence as outlined in the EU's Ethical Guidelines for Trustworthy AI ([8]) published in 2019. These guidelines emphasise the importance of developing AI systems that are lawful, ethical, and robust. TAI focuses on ensuring that AI respects human rights, fosters transparency and accountability, and promotes fairness and inclusivity. By adhering to these principles, TAI aims to mitigate risks and enhance the positive impact of AI on society.\n\nThe seven principles of Trustworthy AI, as outlined by the EU's Ethical Guidelines for Trustworthy AI, are:\n\n1. Human Agency and Oversight\n2. Technical Robustness and Safety\n3. Privacy and Data Governance\n4. Transparency\n5. Diversity, Non-discrimination, and Fairness\n6. Societal and Environmental Well-being\n7. Accountability\n\nThere are several established methods ([9]) and metrics ([10]) to evaluate specific aspects of TAI. However, a significant gap remains in the availability of processes for a comprehensive evaluation of TAI. Existing methods often address isolated components, such as fairness, transparency, or accountability, but fail to provide an integrated framework that covers all ethical and technical dimensions of trustworthy AI systems. This lack of holistic evaluation frameworks presents challenges in ensuring that AI systems align with the full spectrum of TAI principles.\n\nAdditionally, TAI evaluation faces multiple challenges, including a lack of standardisation of metrics and methods, reliance on manual questionnaires, the need for use-case-specific evaluation methods, and fragmented AI development processes which cause issues relating to the accountability of evaluation ([9]). Researchers have also found a lack of suitable industry tools, highlighting the impractical level of human effort required to make existing toolkits work to mitigate bias effectively ([11]), and the difficulties in translating real-world use cases into the quantifiable metrics required by these toolkits ([12]). These challenges hinder the practical implementation of comprehensive TAI evaluation frameworks. AI systems are diverse, and there are many challenges associated with assessing them for trustworthiness. To address these challenges, researchers suggest a need for Standard Developing Organisations (SDOs) mandating ethical disclosures and ensuring minimum standards for testing, documentation, and public reporting ([13]). One such standard is ISO/IEC 42001 ([14]), which offers high-level guidance for compliance with the EU AI Act, in a similar way as ISO27001 guides compliance for information security management in line with the European General Data Protection Regulation(GDPR)[15].To conduct empirical studies into industry preparedness for more strongly enforced trustworthy AI measures, we have conducted this study in line with the EU's\n\nprecursor framework for Trustworthy AI, which benefits from a degree of existing adoption, familiarity and implementation experience. This paper seeks to understand the challenges and trends that industry professionals face when evaluating AI systems for trustworthiness.\n\n## 2 Research Questions\n\nTo guide the investigation into the challenges and trends that industry professionals face when assessing AI systems for trustworthiness, this research aims to answer the following research questions:\n\n(R1:) What are the primary challenges industries face regarding data acquisition, quality, preparation, and provenance in AI systems, and how do these challenges influence the trustworthiness of their AI-based solutions?\n\nData is a key part of AI systems and relates directly to its trustworthiness. Identifying data challenges in AI systems will help in developing strategies to enhance data management practices, thereby improving the trustworthiness of AI systems.\n\n(R2:) How do industry professionals perceive the seven principles of Trustworthy AI? This research question aims to understand how industry professionals perceive each of the seven principles, including identifying their priorities, challenges and comprehension of each.\n\n(R3:) What current practices and challenges exist in assessing compliance with AI-related standards and regulations (such as ISO27001 and GDPR), and how can organisations improve their processes to foster trustworthy AI systems?\n\nCompliance with standards is essential for TAI, however regulating a fast-paced technology such as AI introduces new complications. To help inform the design of assessments for TAI, this question seeks to understand the current processes for evaluating areas such data security which is relevant to the security of AI systems also.\n\n## 3 Methodology\n\nWe invited industry professionals working in AIrelated roles to participate in one-hour structured interviews. These professionals were selected through the research team's existing professional networks to represent a variety of tech industries and job functions. We did not exclude any sectors in our selection, but did target sectors with high AI adoption and focusing on interviewing professionals working across a diversity of applications of AI. The goal of the interviews was to gain insights into the challenges and trends in assessing data use for Trustworthy AI (TAI). Conversations with the 15 professionals were audio recorded, with notes taken during the interviews.\n\nTo ensure compliance with ethics guidelines and GDPR regulations, the University of Galway's Ethics Committee approved the outreach approach, which involved sending invitations via email and LinkedIn. Participants provided informed consent by signing consent forms prior to taking part in the research.\n\nThe insights from these 15 interviews were compiled by the research team and structured into key thematic sections. Empirical evidence suggests that the majority of themes (over 90%) can typically be identified within the first twelve interviews[16] with proposed methods to evaluate if saturation has been reached[17]. Further research emphasises that sample size decisions are inherently situated and cannot be predetermined by saturation rules[18], finding that quality in reflexive thematic analysis derives not from sample size per se, but from transparent, reflexive, and well-justified analytic practice[19]. In this study, the final sample of 15 participants was sufficient to capture diverse perspectives across contexts, with thematic adequacy and depth achieved during analysis. The sections were shaped by identifying common challenges raised in interviews, and aligning those groups with principles of Trustworthy AI from the EU High-Level Expert Group (HLEG), where interviewees provided relevant findings. Sections were combined or created for findings which did not fit into these principles directly. This approach ensured the groupings were data-driven while reflecting established principles. Further research was conducted to explore each challenge, drawing on the latest literature to expand on these findings.\n\n## 3.1 Participants\n\nParticipants came from a variety of industries which is outlined in table 1, with Software as a Service (SAAS) Technology being the most common industry. Participants came from a diverse\n\nset of job functions, with technical functions such as data analytics and IT infrastructure/Cyber security being the leading functions. Participants came from a range of industry levels. All participants were based in Ireland and the UK and were well-experienced in their respective fields. Due to location, the findings reflect perspectives shaped by proximity to the EU AI Act and by local organisational and cultural norms. Although many participants worked for multinational companies, with headquarters in the United States of America, China and Europe, their geographic location likely influenced both regulatory awareness and institutional framing of AI governance. The company sizes varied from 10-50 employees to 100k+ employees. All participants were working in organisations where AI is embedded into core business processes, supported by dedicated infrastructure, governance mechanisms, and ongoing operational use, rather than early-stage or pilot experimentation.\n\nOur sampling strategy deliberately included both technical and non-technical roles to capture a breadth of perspectives across organisational functions. While algorithm engineers, machine learning engineers, and UX researchers were not directly included, our research focus was less on model development and more on governance, compliance, and the organisational embedding of AI. Non-technical roles such as marketing operations and customer support were included to provide additional insights into accountability, user-facing implications, and cross-departmental coordination-critical dimensions of Trustworthy AI implementation. For technically complex issues such as technological robustness or data provenance, findings were interpreted primarily through the lens of participants in data analytics, IT infrastructure, and cyber security roles, whose day-to-day responsibilities directly intersect with these areas. This approach aligns with recommendations in implementation research, where purposeful sampling is used to select participants with relevant knowledge and experience for the study aims, often prioritising variation across roles to capture organisational dynamics rather than technical details of system design [20].\n\n## 4 Detailed Research Findings\n\nThis section organises the findings from our interviews into five sections based on the common themes identified by participants. We extracted the insights provided by participants, highlighting the main points they made, and grouped these insights where they were either consistent or contradictory. These insights were then categorised using the trustworthy AI principles from the HLEG as an initial framework to create highlevel thematic groupings. For example, findings related to fairness and bias were generally associated with compliance, so they were placed within a broader category of Accountability, Governance, and Regulatory Compliance. The section titles have been adjusted slightly from the HLEG principles to better align with the specific feedback in each area. Based on the interview results, a comprehensive literature review was conducted for each key finding within the five sections, providing academic context to support the insights gathered. Table 2 outlines these five sections along with their key findings.\n\nEach subsection includes a table summarising the main findings from the interviews, followed by a discussion of the concerns raised, incorporating both interview insights and academic perspectives. Separate summaries highlight the key takeaways from both sources. Given the focus of this paper on industry perspectives, the findings from industry interviews guided the classification and direction of the subsequent literature review.\n\nTo ensure a structured and thorough analysis, each section in this chapter follows a consistent approach: first presenting interview findings, then reviewing relevant literature, and finally synthesizing both perspectives in a concluding discussion.\n\n## 4.1 Accountability, Governance, and Regulatory Compliance\n\nThis section outlines key challenges and developments in achieving accountability, governance, and regulatory compliance for AI systems. It is divided into three subsections: organisational structure and accountability, which explores internal responsibility gaps; fairness and transparency, which considers biases and the conflict between ethics and business priorities; and challenges with\n\nTable 1 Overview of participants\n\n| ID   | Industry Category       | Company Size   | Job Function                     | Seniority   |\n|------|-------------------------|----------------|----------------------------------|-------------|\n| P:1  | SaaS Technology         | 5k-15k         | Marketing Operations             | Senior      |\n| P:2  | Business Development/BI | 50-500         | Business Development/BI          | Mid-Level   |\n| P:3  | SaaS Technology         | 5k-15k         | Product Design                   | Senior      |\n| P:4  | Other                   | 40k-100k       | IT Infrastructure/Cyber Security | Mid-Level   |\n| P:5  | SaaS Technology         | 1k-5k          | Customer Support                 | Executive   |\n| P:6  | SaaS Technology         | 500-1k         | Data Analytics                   | Senior      |\n| P:7  | SaaS Technology         | 1k-5k          | IT Infrastructure/Cyber Security | Mid-Level   |\n| P:8  | Business Development/BI | 1k-5k          | Business Development/BI          | Mid-Level   |\n| P:9  | Social Media            | 40k-100k       | IT Infrastructure/Cyber Security | Senior      |\n| P:10 | Other                   | 100k+          | IT Infrastructure/Cyber Security | Mid-Level   |\n| P:11 | SaaS Technology         | 10-50          | IT Infrastructure/Cyber Security | Executive   |\n| P:12 | SaaS Technology         | 15k-40k        | Data Analytics                   | Senior      |\n| P:13 | Social Media            | 40k-100k       | Trust & Safety                   | Mid-Level   |\n| P:14 | Other                   | 40k-100k       | IT Infrastructure/Cyber Security | Mid-Level   |\n| P:15 | Other                   | 40k-100k       | Data Analytics                   | Senior      |\n\nTable 2 Detailed Findings - Overview of Categories\n\n| Category                                                | Primary Concerns                                                                                                                                 |\n|---------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|\n| Accountability, Governance, and Regu- latory Compliance | â€¢ Organisational Structure & Accountability â€¢ Fairness & Transparency â€¢ Challenges with Standards and Regulations                                |\n| Data Management and Quality in AI Systems               | â€¢ Data Quality â€¢ Data Provenance, Documentation & Assessment                                                                                     |\n| Human Factors in AI Development and Oversight           | â€¢ Human Oversight and Accountability in AI Sys- tems â€¢ Human Bias & Ethical Implications â€¢ People Management & Organisation Structure Challenges |\n| Technological Robustness and Safety                     | â€¢ Performance, Reliability and Transparency â€¢ Security, Risk & Trust Concerns                                                                    |\n| Environmental and Societal Impact                       | â€¢ Environmental Impact â€¢ Societal and Cultural Impact                                                                                            |\n\nstandards and regulations, which discusses the difficulties organisations face in keeping pace with evolving legal and assessment frameworks.\n\nTable 3 : Detailed Findings -Accountability, Governance, and Regulatory Compliance\n\n| Category                      |              | Details                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|-------------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Organisational Accountability | Structure    | Challenges: â€¢ Siloed Departments and Communication Gaps [P1, P2, P3, P4, P5, P6, P7, P11, P13, P15] â€¢ Need for clearer Lines of responsibility [P1, P2, P3, P5, P6, P7, P8, P11, P12, P13] â€¢ Having to have blind faith in third-party compliance [P1, P2, P3, P4, P5, P7, P8, P9, P13, P14] â€¢ Lack of accountability of companies [P1, P2, P3, P4, P5, P6, P7, P9, P11, P12] Observations: â€¢ Formation of AI Councils and Steering Committees [P6, P8, P13] â€¢ AI provider/product developer is seen as accountable for the system [P4, P13, P14]                                                                                                                                                                                                                                                                                                                                                                     |\n| Fairness &                    | Transparency | Challenges: â€¢ Risk of Embedded Biases [P1, P2, P3, P5, P6, P7, P9, P10, P11, P12, P13, P14] â€¢ Ethical Implications of AI Decisions are unclear [P1, P2, P3, P7, P8, P11, P13] â€¢ Lack of Systematic Bias Testing [P1, P2, P6, P7, P11, P12] â€¢ Resource or Time Constraints for Testing and Mitigating Bias [P1, P2, P6, P8, P11, P13] â€¢ Explainability of AI Models is a key challenge [P1, P3, P4, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15] â€¢ Conflict Between Fairness and Profitability [P2, P5, P6, P7, P9, P13] Observations: â€¢ High Reward, Low Risk Business Environment [P2, P6, P7, P9, P13] â€¢ Awareness of Biases and Cultural Impacts [P1, P2, P3, P5, P6, P7, P8, P9, P10, P11, P12, P14] â€¢ Transparency can increase trust in decisions [P6, P10] â€¢ Fairness is not binary. It is measurable on a scale [P2, P3, P11, P13, P14, P15] â€¢ Inclusive AI design can lead to wider product innovation [P1] |\n\n## (Continued from previous page)\n\n| Category                                  | Details                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|-------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Challenges with Standards and Regulations | Challenges: â€¢ Confusion around the specific impact of the AI Act. [P1, P5, P11, P12, P13] â€¢ Lack of AI-Specific Standards for Trustworthy AI [P1, P4, P6, P7, P8, P9, P11, P12, P13, P14, P15] â€¢ Rush to Implement AI Without Proper Risk Consideration or Mitigation Processes [P1, P2, P3, P4, P5, P6, P7, P8, P9, P11, P12, P13, P14] â€¢ Lack of AI Assessment Tools [P1, P2, P4, P6, P7, P9, P11, P15] â€¢ Financial pressure for auditors to sign off compliance [P4, P9] â€¢ Over reliance on good faith and assumptions [P1, P2, P3, P4, P5, P6, P7, P8, P9, P13, P14] â€¢ Concerns fines are not effective enough or are too low [P2, P3, P5, P7, P9] â€¢ ISO standards are not sufficient anymore, real-time, technology- based assessments of AI systems are required [P9, P11, P12] â€¢ Need for certifications to be quantifiably validated [P7, P14] Observations: â€¢ Belief that regulation is chasing to catch up with technology [P2, P3, P7, P8, P9, P13] â€¢ Belief that fines and stopping activities is at least somewhat helpful to police companies [P1, P2, P3, P5, P7, P9, P13] |\n\n## 4.1.1 Organisational Structure &amp; Accountability\n\nThis section explores how organisational structure influences accountability in AI governance, including the distribution of responsibilities and the effectiveness of internal processes. Interview participants highlighted challenges related to siloed departments, unclear compliance ownership, and the need for interdepartmental collaboration to ensure AI accountability. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.\n\n## Interview findings:\n\nMany organisations operate in silos, hindering cross-functional collaboration and creating gaps in compliance implementation. Compliance teams were often disconnected from operational functions, limiting their ability to enforce effective oversight. To combat this, some organisations have started to form interdepartmental AI steering committees with representatives from multiple departments. These committees have a goal to lead the AI strategy and compliance within the organisations. In some cases, professionals noted that when it came to AI compliance, departments were not interested in being responsible for this within their department, and felt another department should make sure they are compliant, in particular as it related to maintaining compliance documentation. Compliance was seen as an administrative task that distracted team members from the primary goals of the department.\n\nThere were mixed views on who was responsible for the AI systems Trustworthiness. Some participants felt certain that accountability ultimately sat with the AI provider, regardless of where they sourced their data from, or who built various aspects of the AI system. However, some professionals felt that due to the fragmented nature of AI development, there was a blurred line in terms of the overall responsibility of an AI system. There were additional concerns around working with third-party providers, such as who was responsible if data was purchased to train a model which subsequently created issues for the AI system, or how information could be safely shared as part of an inter-organisational development process. Participants felt they were at the mercy of trusting the compliance of third parties, as even going for a site visit, there was no real way to trust their governance completely. Technical participants (particularly in cyber security and data analytics) emphasised system-level risks and vulnerabilities, whereas non-technical participants, such as those in marketing operations or business development, viewed accountability as the responsibility of external providers or compliance departments. These role-based differences highlight the importance of organisational dialogue across professional functions.\n\n## Literature findings:\n\nResearchers argue that mechanisms need to be put in place to allow us to hold decision makers accountable for constitutional and financial consequences including monetary damages or sanctions for companies, as well as broader ethical responsibility ([21]). Legal accountability in AI systems involves assigning responsibility for decisions, managing liability, ensuring transparency, and addressing risks, with evolving frameworks needed to clarify obligations ([22]). AI systems are complex and research highlighted the potential for autonomous decision-making create accountability gaps that traditional legal frameworks struggle to address, reinforcing the need for structured regulatory mechanism ([23]). The findings from the interviews were reiterated in our literature review, which also provided additional insights into the impact of the organisational silos noted by interview participants. These silos are defined as clusters of employees lacking cross-departmental communication, are not necessarily formed due to structural isolation, but instead, interaction patterns are primarily driven by the nature of employees' tasks ([24]). Siloed departments can inhibit the transfer of information, and contribute to problems in organisations by negatively affecting things such as transparency, accountability, and risk management ([25]). These barriers to communication and cooperation can negatively affect organisational efficiency, morale, and innovation. To address these challenges, organisations can adopt a systems approach that emphasises strong leadership, fosters collaboration, and implements structured processes to enhance information flow and teamwork ([26]; [27]).\n\n## Discussion:\n\nBased on the findings of the interviews and literature review, we can conclude that effective organisational structure needs to enable communication between those with in-depth knowledge of functions involved in AI systems, and those responsible for compliance. structure should be designed to ensure that both parties are wellorganised and motivated to collaborate on AI compliance initiatives, fostering alignment and mutual understanding in their efforts to meet regulatory and ethical standards. Stakeholders within departments involved in AI systems such as IT and data departments, should input into the design and maintenance of AI compliance, as those stakeholders have the expertise and knowledge of the systems required to design effective evaluation processes required by the compliance department. Under the EU AI Act, primary responsibility and accountability for an AI system rests with the provider who builds or modifies the system and places it on the market. However, while the legislation specifies this allocation of responsibility, participants in our study reported that accountability processes have not yet been fully adopted or understood within industry, largely due to ongoing uncertainty about the Act's requirements.\n\n## 4.1.2 Fairness and Transparency\n\nThis section examines the role of fairness and transparency in AI, considering how they are defined, implemented, and the challenges associated with ensuring clarity and equity in AI decision-making. Interviews revealed concerns about bias in AI models, the lack of systematic fairness testing, and the difficulty in balancing business priorities with ethical considerations. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion summing up both.\n\n## Interview findings:\n\nAI fairness and transparency emerged as key concerns amongst industry professionals who expressed an increased awareness of biases in models due to the data used for training models. Professionals who worked closely with data and machine learning (ML) models were very aware of how a model can figure out sensitive data, even if it is not explicitly told. There were concerns over risks to embedded bias, with participants noting that biased data was coming out of models and being used to train other models. Additionally, there were concerns over a notable lack of systematic testing for bias. Resource and time constraints contribute to a systematic lack of bias testing and mitigation in organisations. Bias testing was typically informal and initiated by individual interest rather than mandated business practice. Some participants noted that concerns around bias either were not taken or would not be taken seriously due to the conflict with business objectives, primarily profitability. It was also pointed out that fairness concerns a human might pick up on, may not be picked up by a model that's being asked to optimise for profit. However, one senior data analyst pointed out that even when it was spotted, often it was not prioritised. They explained that there are business targets for profit in sectors like finance and insurance and that this influences the bias of the model. The participant explained that profit needs to be traded off with bias, and it's usually around feature selection; going on to explain that options are presented to senior leadership, and they make a selection based on their financial targets and projections, with profitability as the primary motivation. Participants noted that efforts to reduce bias for underrepresented groups can introduce trade-offs affecting majority outcomes, while also expressing concern that the growing reliance on synthetic data may amplify bias and undermine fairness and transparency.\n\nA number of participants highlighted the importance of explainability of the AI systems. Two participants noted that introducing transparency in AI systems enabled senior leaders to trust them more and accept AI results quickly. One participant pointed out that while they had internal explainability for the AI models their users directly engaged with, the model explanation was not shared with the users of the platform. Others highlighted a lack of availability of any explainability, even within their organisations. One participant noted that having the transparency between AI provider and client gives the opportunity for the client to input into the model, to ask for additional human in the loop points, and to make changes to the model where needed so that they can trust it. Another participant explained how transparency algorithms\n\nsuch as SHAP ([28]) and LIME ([29]) are being used to build trust in AI models internally by enabling them to be explained to management, which increased the trust in the model. Participants also called for transparency around what mechanisms were in place to combat bias, where humans were involved in the process, and what fairness metrics were being used.\n\nMost of the participants referred to fairness simply as quantifiable bias, limiting their focus to model behavior or dataset imbalances. In doing this, they overlooked how their own design choices and usage decisions may qualitatively contribute to potential negative impacts or risks. However, one participant noted that AI systems should be designed for inclusivity and diversity. They believed that designing AI systems for inclusivity and diversity had additional benefits to the organisation. They gave positive examples of product innovation, such as automatic doors, which were designed for people with disabilities, and selfopening car boots, which were designed by a female product designer returning from maternity leave. Almost half of the participants referenced that compliance should be viewed as a spectrum as opposed to a binary state. Bias, for example, was not seen as something which could be entirely removed without making ineffective the models which had been trained on biased data. Instead, compliance should be viewed on a usecase basis, with varying metrics and levels of adherence required for each situation.\n\n## Literature findings:\n\nResearchers reported similar findings to the interviews about the relationship between bias and accuracy in models, noting that although in many instances a trade-off must be made between the two, several researchers proposed methods resulting in what they considered an acceptable balance of bias versus accuracy for their use-cases. Bias can arise from human decisions in the design, implementation, and management of AI systems, with its mitigation requiring a continuous and iterative feedback loop ([30]). Lee ([31]) considered bias in the use case of credit lending, proposing considering fairness as variable level of trade-off between competing objectives such as accuracy. Singh et al. ([32]) developed the Alternate World Index (AWI), a universal fairness metric which they proposed a level of trade-off between fairness and accuracy for credit lending. Lee and Floridi ([33]) expanded on the concept of treading fairness not as a binary condition, but instead as a relational trade-off. This transparency allows lenders to justify algorithm choices by balancing financial inclusion and impact on minority groups, while providing regulators and policymakers' with insights to recommend acceptable risk levels. For the use case of recidivism [34], Farayola et al. ([35]). demonstrated a multi-objective optimisation approach to minimizing bias by examining multiple fairness-enhancing techniques across different stages of the ML model and examine their impact on the balance between fairness and accuracy. By introducing techniques such as disparate impact remover, adversarial learning, and equalized odds optimisation, the researchers were able to significantly reduce bias with only a minimal cost to the model's accuracy. McCormack and Bendechache ([10]) classify evaluation criteria for the seven principles of Trustworthy AI originally published by the EU high level expert group, which were also included as non-legally binding guiding principles in the EU AI Act. Their fairness metrics include Group, Individual, Counterfactual, Intersectional, Complex Fairness, and Inclusive Design. They also identify a research gap in AI transparency and propose evaluating systems for model, data, and outcome transparency, emphasizing the importance of visibility into data use and decision-making. The paper stresses the need for standards and processes to address transparency, fairness, and other unknowns in the area of Trustworthy AI. The uncertainty around the future of AI is also echoed by Floridi ([36]) who refers to speculation on the hype of AI as the wild west of 'what if' scenarios. They say this is impacted by media oscillating from an AI utopia to an AI doomsday scenario and propose that AI be viewed as a normal technology rather than a miracle or plague. The paper calls for more philosophical thought into ethics, and consideration around what technology is being developed and its potential impacts. The paper argues that ethical frameworks and principles to underpin AI technology do exist, but states a need for more thought into how AI will fit in the developing human-technology relationship.\n\n## Discussion:\n\nAI is being developed at such a rapid pace that existing AI systems frequently perpetuate bias and discrimination, which can be introduced at various stages of the AI system([30], raising significant concerns about the unknown ethical implications. There is a lack of transparency and established processes for evaluating AI systems' fairness. Moreover, business objectives, particularly profit, are often prioritised over societal and ethical considerations. While technology to enhance transparency and fairness has already been developed, it is not yet sufficiently adopted or audited by regulatory bodies. There is an urgent need for the standardisation and evaluation of fairness and transparency practices in AI systems, particularly concerning the trade-offs between profit and fairness in decision-making processes.\n\n## 4.1.3 Challenges\n\nThis section discusses the complexities of regulating AI, the difficulties in assessing compliance, and the evolving landscape of governance frameworks. Interview participants expressed frustration over unclear regulatory expectations, the reactive nature of compliance measures, and the challenges of holding AI developers accountable. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.\n\n## Interview findings:\n\nProfessionals found several challenges when it came to regulations and assessment for Trustworthy AI. They noted that companies were currently operating with very little accountability for their AI activities, with several participants believing that regulation was struggling to catch up with technology. Professionals referred to the current AI development environment as the 'Wild West' due to the high ratio of reward versus risk for companies. Several professionals felt this was because stealing data or engaging in unethical AI practices did not currently have sufficient repercussions in place. One participant described an attitude of entitlement within organisations when it came to taking data, even illegally for the sake of AI innovation. Although many professionals felt that fines were somewhat effective in holding businesses accountable, ultimately, they noted that fines are often seen as an acceptable risk by companies. Prohibiting companies from engaging in activities due to breaches was suggested by some professionals as a better way to hold organisations accountable, as it would have more impact on profitability. Participants felt that companies would tend to opt for the bare minimum for legal compliance. When it came to implementation, professionals faced challenges in putting Trustworthy AI practices in place. One issue was a lack of tools available to easily assess AI systems against a trustworthy AI standard. While professionals felt that the AI Act would be significant to organisations, they noted that there was confusion around its implications, along with a lack of resources and clarity such as those currently available for GDPR compliance. Some professionals identified bias but felt unable to act due to anticipated resistance or previous dismissal by leadership.\n\nThe industry's reliance on good faith, both in self-regulation and third-party verification, was seen as a key vulnerability in compliance enforcement. Approaches to compliance were typically responsive 'band-aid' solutions designed to meet minimum requirements stipulated by the organisations compliance department. Employees are both pressured and financially incentivised to make sure they assess systems in a way that would pass their compliance processes. They noted that company bonuses were often dependent on achieving company objectives such as compliance certification for their systems. Due to the over-reliance on good faith, doing the bare minimum or even lying to get certification, to achieve this financial reward was both possible and incentivised. This was the case for both internal employees and external auditors, who in some cases would have a prior discussions around compliance for certifications such as ISO27001[37], before the official findings of the report were published. This allowed the company the opportunity to liaise with the auditor before any official report maintained for an audit trail was completed. Paying auditors for certification created perceived conflicts of interest, raising doubts about the objectivity and integrity of compliance processes. The existing processes were largely around maintaining documentation, and less about the quality of the documentation. The ability to make human judgement calls around what documentation was considered sufficient added ambiguity into the audit process. This\n\nproblem was echoed by participants who noted a need for certifications which were able to be quantifiably validated. Participants noted that there has to be technology and tools that go hand in hand with design, development, and deployment that collect data in real-time if possible.\n\n## Literature findings:\n\nThe literature reports similar concerns around several issues relating to regulations and assessment for trustworthy AI. Jobin et al. ([38]) highlighted concerns about lack of accountability in AI development. They found that companies were able to bypass ethical requirements due to insufficient regulatory enforcement or the use of highlevel soft-policy approaches. They describe uncertainty around how ethical principles should be evaluated, as there was no clear process for enforcing oversight. In particular, they note a gap in the ability of regulators to prioritise conflicting ethical principles. They proposed twenty-two approaches to effecting change in TAI in companies under the four high-level classifications of social engagement, soft policy, economic incentives and regulation and audits. A recent paper by Diaz RodrÂ´ Ä±guez et al. ([39]) reports that accountability issues are still prominent, calling for enhanced regulation and oversight mechanisms. The researchers highlighted the importance of developing social and ethical standards that could first be implemented in the design and construction of systems and subsequently used to assess those systems for their conformity. Percy et al. ([40]) highlighted the importance of both external accreditation and internal audits to foster trust and transparency and establish a balanced ecosystem. They argued that reliance solely on high-level ethical principles or external regulations is insufficient for accountability. Using the example of gender bias in gambling, they showed that gender bias can be reduced, albeit with a cost to overall model accuracy. Due to the variability in AI systems, they concluded that industry-specific guidelines were essential for accountability in addition to internal audits and explainability processes. They expressed the concern that without improved governance, companies may continue to bypass ethical responsibilities. The researchers also noted that a variety of tools and techniques have been developed and published to help machine learning developers to implement ethical principles at various stages of the development process, however there is no agreement on how these should be measured or enforced.\n\nEwuga et al. ([41]) investigated the implementation and effectiveness of the ISO27001, a risk based framework for information security management systems prominently used as part data protection in organisations. Their research, which looked at the banking sector, showed many benefits, including improved risk management, incident response, and cultivating a security-aware culture. It uses a Plan-Do-CheckAct approach which requires continuous monitoring and improvement and includes ethical considerations such as customer consent and balancing data subjects' privacy with system security and transparency. The researchers found that banks have dynamic cyber threats which can often lack evaluation metrics and indicators. In their discussion on addressing future challenges with emerging technologies including AI and ML, the researchers call for the development of specific and stringent cybersecurity standards for these evolving threats. Kamil et al. ([42]) found that there was pressure on employees within organisations to maintain compliance with ISO27001 which sometimes led to them becoming bad actors. The pressure felt by employees could result in unintended risks such as bypassing security measures or exploiting system gaps to pass audits. The certification was seen as somewhat devalued as some clients were not accepting this standard alone but required multiple standards to feel assured of their commitment to information security. Fontrodona et al. ([43]) researched the relationship between innovation and ethics, finding that they are closely interconnected. While innovation is about exploring the possibilities, ethics provides a framework to ensure this progress is aligned with high-level societal principles. The authors noted that innovation that is not rooted in ethical considerations can lead to harmful societal or environmental consequences.\n\n## Discussion:\n\nBoth the literature and interviews highlight significant challenges related to accountability in AI systems. Current accountability standards,\n\nwhich rely on risk-based frameworks and manual processes such as checklists, are regarded as insufficient. There is a clear need for quantifiable standards and metrics that can be continuously monitored throughout the lifecycle of AI systems. Regulators often place undue reliance on good faith, with organisations depending heavily on their own auditors or certification bodies. Given that these auditors and certification companies are paid for their services, they may be financially incentivized to act in the best interest of the business, which can undermine their ability to hold organisations accountable.\n\nWithin organisations, employees may face pressure, leading them to take shortcuts or exploit vulnerabilities to pass compliance audits. While tools and techniques exist to integrate ethical principles at various stages of the AI development process, implementing Trusted AI (TAI) is often hindered by conflicting business goals. For example, in sectors such as banking and insurance, machine learning models can be quantitatively assessed for bias, but efforts to reduce bias usually come with a trade-off in accuracy, which negatively impacts profitability and is thus often deprioritised.\n\nThis conflict between business objectives and ethical considerations results in organisations setting acceptable levels of bias in machine learning models, leading to profit-driven models rather than fair ones. The lack of ethical considerations in the innovation and development of AI technology is already contributing to harmful societal consequences. To enable effective ethical accountability, independent third-party accreditation bodies must establish Trustworthy AI evaluation criteria tailored to specific use cases at a sector level, incorporating standardised metrics that must be adhered to.\n\n## 4.2 Data Management and Quality 4.2.1 Data Quality\n\nThis section addresses the importance of data quality in AI systems, including the factors that impact data reliability and the implications of poor data practices. Interviewees frequently cited issues with inconsistent data entry, vendor misrepresentation of data consent, and the risks posed by low-quality or biased data feeding AI models. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.\n\n## Interview findings:\n\nInsights in this section were drawn primarily from participants in technical roles relating more closely to data quality (data analytics, IT infrastructure, cyber security). While non-technical roles provided complementary perspectives, the technical participants' expertise informed the more detailed aspects of these themes. All fifteen participants highlighted issues with data accuracy and reliability. The most mentioned issue was around inaccurate model outputs resulting from low-quality or misunderstood data. Participants emphasised that poor-quality input data leads directly to unreliable model outputs, highlighting a persistent 'rubbish in, rubbish out' issue. There were several ethical concerns flagged around biased data leading to biased outcomes that reflected social prejudices. Participants flagged challenges around inconsistent data entry practices, either from human-entered data or fields that were not common across departments and subsequently misinterpreted. One participant who worked as a lead data scientist explained the benefits of assigning quality thresholds to input data so it could automatically be flagged for changes affecting quality. They explained that the model performance metrics which were also tracked, took some time to pick up the mistakes, versus it being spotted at source. Despite data quality being more critical to model success, it was frequently deprioritised in favour of advancing model development. Additionally, when the error is flagged at the input data, it makes it easier to fix as the source of the issue is immediately identified.\n\nConcerns were raised by four participants around data acquisition and cleaning practices, specifically around data vendors misrepresenting consent. It was pointed out that vendors will claim to be the originator of the data, when they are not. Additionally, it was mentioned that there was no way to verify if sets of data with and without consent were merged and disguised as fully GDPR-compliant data when they were not. The lack of transparency in data sourcing from brokers could lead to problems for purchasers who are\n\nTable 4 Detailed Findings - Data Management and Quality\n\n| Category                                      | Details                                                                                                                                                                                                                                                                                                                             |\n|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Data Quality                                  | Challenges: â€¢ Issues with accuracy and trustworthiness [P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15] â€¢ Inconsistent Data Entry [P1, P2, P3, P4, P7, P8, P13, P15] â€¢ Overreliance, validity or bias concerns around Synthetic Data [P3, P7, P11] â€¢ Concerns with data vendors faking consent [P4, P9, P10, P11] |\n| Data Provenance, Documenta- tion & Assessment | Challenges: â€¢ Challenges in Maintaining Up-to-Date Documentation [P1, P2, P3, P4, P6, P7, P8, P9, P12, P13, P15] Observation: â€¢ Benefits to companies with good Data Lineage Tracking [P1, P2, P8, P9, P13, P15]                                                                                                                    |\n\nrelying on good faith that the data is being represented truthfully. There were questions about how compliant an AI builder could be if they bought data to train their model, which later turned out to have been misrepresented by the data vendor.\n\nThree participants flagged concerns around the increasing use of synthetic data. The primary concern was that the data would have increasing bias and validity issues affecting the model's performance. This cycle was referred to as a chicken and egg problem.\n\n## Literature findings:\n\nThe literature also highlighted that data quality is an essential part of the performance of machine learning models, and that it was also often underprioritised versus model development, along with concerns raised around synthetic data and data provenance. Gupta et al. ([44]) found there are many issues around data collection and processing affecting the reliability and accuracy of models. Researchers highlighted the amount of time spent by data scientists on debugging data, citing the importance of well-defined quality metrics as a way to save time. The paper also proposes a series of quality metrics for various aspects and types of AI systems. Ragineni ([45]) also highlighted the importance of data quality and noted the issues that can occur during the machine learning pipeline. The paper outlines a comprehensive data cleaning process, and also proposes additional data quality dimensions to address challenges around software, bias and legal and ethical aspects. Priestley et al. ([46]) argued that data quality means different things to different people. To ensure that data quality was contextual, they proposed mapping traditional criteria such as accuracy and consistency to specific stages of the ML lifecycle. The paper offers additional practical insights and advice around stakeholder needs, data quality across complex ecosystems and data quality management.\n\nSambasivan et al. ([47]) noted that model development was often overvalued versus data quality, and this led to negative outcomes. They found a need for cross-domain collaboration, which was a challenging task due to poorly maintained documentation, variability in incentives and insufficient domain expertise, which contributed to project failures. The researchers also noted that data collectors and vendors involved in AI projects can be under-resourced or lack proper training. This can lead to conflicting agendas where field workers may either fabricate data or fail to collect adequate data due to a lack of motivation or understanding of the importance. Additionally, Morey et al. ([48]) found a lack of transparency in the collection of personal data which caused concerns with customers. They found that some organisations were exploiting data for shortterm gain but that was not a sustainable approach due to growing customer demand for control of\n\ntheir data. Hug ([49]) discussed the manipulation of data from individuals for profit, noting that companies circulate data without transparent consent from individuals. Whitney and Norman ([50]) explore the area of circumventing consent, where companies take data without consent and manipulate it to sidestep actually obtaining consent. The authors detailed the practice of organisations taking personal data and using it to generate synthetic data, obscuring the data's origin.\n\nJordon et al. ([51]) also found that synthetic data comes with a number of risks, including privacy and bias concerns. While datasets can be augmented to create de-biased synthetic datasets, fixing the inherent bias in the original data, the process comes with its own set of risks to create new problems, which need to be closely monitored. Joshi et al. ([52]) highlight concerns with synthetic data, such as leakage, dataset diversity, and representation fidelity, along with approaches to mitigate these risks.\n\nDelacroix et al. ([53]) highlighted the power imbalance between data controllers and data subjects, in part due to an inability to effectively implement governance measures such as GDPR. They proposed a bottom-up data trust solution in which data subjects could pool their data under a fiduciary structure to offer collective empowerment over their data.\n\n## Discussion:\n\nBoth the interviews and literature showed the importance of data quality, in particular, the benefits of having well-defined data quality metrics to ensure consistent model performance. Unethical concerns were raised around current practices of biased and manipulated data leading to negative outcomes. Synthetic data is an area of growing concern for similar reasons including legalities of consent, prejudice and validity concerns. Looking forward, both traditional data quality dimensions and data cleaning processes need to be updated and embraced by industry to support better data governance-ultimately leading to greater transparency and improved performance of AI systems. There are also calls for changes in the structure of data ownership, with a shift towards increasing control for data subjects over how their data is used.\n\n## 4.2.2 Data Provenance\n\nThis section considers the significance of data provenance, focusing on how organisations track, document, and verify the origins of data used in AI models. Interviewees highlighted difficulties in maintaining up-to-date documentation, the time wasted tracking down data sources, and the negative impact of poor data lineage on AI reliability. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.\n\n## Interview findings:\n\nSix participants mentioned the benefits of maintaining good data lineage within organisations. Participants noted that time is frequently wasted chasing the source or proper meaning of data. This can include just emails or phone calls to colleagues or, on occasion, actually chasing down the people on the floor who collected the data to understand the meaning and context of various fields. Participants noted that models had been hastily built in companies in the AI rush, which subsequently didn't work due to the model builder not understanding or making false assumptions about the data used to train the model. Additional benefits of good data tracking included ensuring data integrity, compliance, improved transparency and improved trust in the data. The benefits of tracking data through the company, including data flow diagrams and real-time accurate documentation, are also highlighted. Many participants noted that a lot of their answers around data could be found in internal repositories. Despite this, they mentioned chasing additional information about data to assist with their projects. This was often due to documentation either not being complete or being out of date. This was particularly the case in dynamic environments where both the data and the AI models are evolving. There was a fragmentation of responsibilities, with people finding the task of continuously updating documentation to be labour-intensive admin work they did not want to do. It was seen as checklist-type work, which often did not keep pace with product deployments. The fragmented development also created challenges in ensuring consistent data oversight, in particular when there was a lack of integrated systems.\n\n## Literature findings:\n\nThe findings in the interviews align with the existing literature on data quality and data provenance in AI systems. Werder et al. ([54])highlighted the importance of data provenance for mitigating bias and making AI systems more responsible. They note that organisations see data provenance as a compliance mandate rather than as part of an organisational commitment to developing responsible AI systems. They found that when organisations developed practices such as the automated ones recommended in their paper, they had better long-term outcomes for the organisation, particularly in data-driven development and AI engineering. Laine et al. [55] also notes the importance of data provenance for mitigating bias and promoting accountability and transparency in AI systems, in particular in relation to decision pipelines. They also note that fragmented documentation with missing information leads to challenges for businesses during fairness and compliance audits. Solomon and Brown [56] found that organisational culture played a significant role in informational security subculture, affecting how employees followed processes such as good data provenance. They found that a top-down approach, including the monitoring of compliance processes and ensuring employees were aware of their accountability in those processes, was important. Additionally, good data provenance processes can offer additional benefits such as also smoothing transitions between employees, in particular in instances such as company downsizing [57].\n\n## Discussion:\n\nData provenance, when implemented as a bandaid style compliance solution, causes friction and creates problems that cascade throughout the organisation. Examples of this include both labour-intensive tasks around chasing data sources and meaning and the building of AI products that don't work due to data misunderstandings. Businesses that fail to create both good data processes and a culture of data compliance will see increasingly negative organisational outcomes as AI becomes more commonplace. Strong data provenance practices that are rooted in best practices, in particular those that include automation, will offer significant benefits to organisations.\n\n## 4.3 Human Agency and Oversight\n\nThis section explores the role of human oversight in AI systems, assessing the balance between automation and human decision-making, as well as the need for accountability. Interviews highlighted concerns about insufficient human oversight, gaps in AI education among professionals, and conflicts of interest in human decisionmaking. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.\n\n## Interview findings:\n\nParticipants had concerns there might not be enough humans in the loop in systems. There were concerns around over-automation of systems, in particular without proper monitoring, robustness and safety being built into them. One participant noted that they expect their organisation's tools not to allow them to do anything that isn't compliant, and that compliance should be inherently built into the system, but this was not always the case. There were also concerns that even when things are GDPR compliant, we still didn't know enough about the potential ethical implications. A number of participants felt there is a need for people to be in the AI process of making decisions at mandatory gates. One participant noted that AI could check AI in the future, but at this stage, humans should be monitoring these checks due to AI verification tools not being available. One participant gave the example of pilots who have certain thresholds and points that they are required to take over, noting that this should be the same to ensure the safety of all AI systems. From a usability perspective, one participant noted that people sometimes don't realise they are dealing with an AI, so you need humans to be available when issues arise. Participants also noted that the AI can miss things due to context or just not being advanced enough, and this is something humans are able to pick up on. One participant noted that humans in the loop should not be incentivised to choose profit, stating that if a human is a shareholder, they will not be able to make an unbiased decision between profit and ethics. Two participants pointed out that humans in the loop can introduce their own biases into the system also. There were concerns about the educational training of humans involved in AI systems in areas such\n\nTable 5 Detailed Findings - Human Agency and Oversight\n\n| Category                 | Details                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n|--------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Human Agency & Oversight | Challenges: â€¢ Not enough humans-in-the-loop in AI systems [P1, P2, P3, P4, P8, P10, P13, P15] â€¢ Risks of over automation [P1, P2, P5, P6, P7, P8, P10, P12, P14, P15] â€¢ Need for ongoing monitoring [P1, P5, P9, P11, P12, P13] â€¢ Humans need to be able to understand model decisions [P1, P3, P4, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15] â€¢ There is a need for human checks at certain points [P1, P3, P4, P5, P8, P13, P14, P15] â€¢ Concerns that human-in-the-loop could introduce bias [P11, P12] â€¢ Human decisions need accountability [P11, P12, P13] â€¢ Organisational structural changes needed for oversight [P1, P2, P11] â€¢ Call for mandated oversight roles in companies [P1, P2] â€¢ Concerns around education of professionals [P1, P2, P8, P10] Observations: â€¢ Humans can help mitigate known and unknown ethical implica- tions in system design or data collection, processing & testing [P1, P4, P5, P6, P7, P10, P15] |\n\nas poor data literacy or professionals having fragmented knowledge of AI systems. One participant commented that issues are arising because a lot of models are being built by engineers who don't understand the full process, how data is collected and processed and how it's going to be used by end users. A second participant noted a similar focus on the model build, stating that data scientists are directed to focus on math and algorithms, leading to a significant gap in education around the data aspects. Another participant highlighted the need for the addition of specific people or tools to ensure compliance with AI systems. A fourth participant commented on the lack of education of employees due to the rush to implement AI in their organisation. They commented that this was happening without educating employees on the risks, including the unknown risks involved, so that they could help mitigate these. A number of participants noted a need for change in organisational structure, with this already starting to happen in many organisations. AI teams or AI councils are being retrospectively set up to look after privacy and governance. Some participants called for new roles or AI compliance departments, such as AI equivalent to the GDPRmandated data protection officer, questioning why this wasn't included in the act. One participant noted that smaller companies who now have access to this powerful technology may not have the resources to ensure compliance with it. There was also a reported disconnect between data controllers and end users. One participant said that instead of replacing departments or roles in companies with AI, the roles should be halved and become AI-assisted with enough human checks in place to mitigate risks. The explainability of models to humans, as discussed in section 4.1.2 in this paper was also flagged as important by participants. One participant highlighted that proper human oversight is only possible when the humans understand what is being done, the input, output and the steps in between. Participants noted that human decisions need accountability. Companies that are incentivised to favour profit over bias need to be made accountable for their decisions. Additionally, humans who could introduce bias into systems interviews, needs to be transparent so that it can be monitored for quality\n\nchecks and auditing also. It was noted that monitoring of AI systems would have require ongoing accountability and monitoring, an AI tool that was originally safe to use could be compromised by a bad actor gaming it or introducing bias. One participant described their organisational GDPR implementation as a big project initially, but once it was in place, ongoing monitoring and training were sufficient.\n\n## Literature findings:\n\nThe literature also found that human oversight in AI systems must be carefully designed, with particular focus on transparency, ongoing monitoring, and the level of training and motivation of those involved. Enqvist [58], noted that human oversight can be designed in many ways, with many outcome variabilities based on the selection of which processes, which person and which time the process is overseen. They also pointed out that the oversight should be designed with attention given to transparency and the mandates and working conditions of the human oversight to more effectively counterbalance the risks they are trying to mitigate. They highlighted that human monitoring should be ongoing to address risks and biases instead of reactive. The paper also discusses the human-centric approach discussed in laws such as the AI Act, noting that this mandated oversight requirement is being written into law with no precedent for what the human-centric approach applies, and a lot of room left for interpretation by AI providers.\n\nAutomation bias leads humans to uncritically rely on automated systems, even when faced with contradictory information, showing that AI can introduce new biases through the way its outputs are interpreted rather than simply eliminating existing ones [59]. Human oversight is a potential safeguard to mitigate some risks in AI systems in the hope that humans may be better at including ethical considerations and adhering to social norms in decision-making processes [60]. However, this paper also noted that humans may come to rely too heavily on the AI outputs and be influenced by them, or else counter them unfairly and introduce their own biases. They noted that any human in the loop would have to be trained with sufficient knowledge about the risk and how to mitigate it, also calling attention to the accountability of having an appropriate person in the loop, stating that effectiveness requires both moral responsibility and fitting intentions. The researchers propose an approach to effective human oversight under three categories: the technical design of the system, individual characteristics of oversight persons, and the environmental circumstances in which oversight occurs.\n\nResearchers discuss what an acceptable standard of care, a norm-based governance with legal implications, would entail for AI systems [61]. They found AI providers need to take reasonable actions and precautions to ensure no resulting harm, tort or regulatory liability, and this includes implementing human oversight. The paper states that human oversight includes both the information to responsibly use an AI agent and control it during operation as opposed to autonomy, which is at the other end of the spectrum. They state that for a use case, such as when a driver should take over driving a vehicle, the appropriate level of oversight versus autonomy must be selected, which reflects the advances in capabilities and safety. They argue that AI systems which pose challenges to effective human oversight could be seen as defective products under the law. The paper states that more research into the humanAI relationship will help determine an appropriate standard of care for specific use cases.\n\n## Discussion:\n\nThere is a clear need for sufficient human oversight in order for AI systems to be considered safe and ethical. The legal implications of what an appropriate level of oversight or standard of care might look like is currently unclear. The literature and regulations in this area state that humans providing oversight or operating AI systems should have sufficient training. However, industry participants noted concerns that sufficient training was not in place currently. There is a shared concern in the literature and industry that overautomation and lack of human control could occur either through lack of human oversight or through human decision-making either being influenced or wrongly influencing AI systems. This could result from a lack of transparency, lack of education, or human oversight by individuals with conflicting agendas regarding the ethical integrity of the system. Despite these concerns, human oversight\n\nis seen as integral to ensuring fairness and safety in AI systems. There are concerns that achieving legal compliance with AI standards will not result in meeting ethical standards. Accountability and transparency of human involvement in AI systems involved in high-risk decision-making is seen as essential. Human agency versus human oversight is a balance that must be struck for each use case. It is likely that significant organisational changes in structure will result from the implementation of AI technology with sufficient human involvement.\n\n## 4.4 Technological Robustness and Safety\n\nThis section focuses on the technical resilience and security of AI systems, considering issues with performance, reliability, transparency, evolving cyber threats, and the need for new standards and risk mitigation measures to maintain trust and safety.\n\n## 4.4.1 Performance, Reliability and Transparency\n\nThis section examines the technical robustness of AI systems, including the factors that influence performance, reliability, and the importance of transparency in AI operations. Interviewees raised concerns about model accuracy, the impact of data quality on performance, and the need for transparency to build trust in AI decisions. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.\n\n## Interview findings:\n\nInsights in this section were drawn primarily from participants in technical roles (data analytics, IT infrastructure, cyber security), whose responsibilities involved direct engagement with system robustness, with non-technical roles providing complementary perspectives. Data accuracy is essential in robust AI systems. Participants commented on the risks of low-quality or poorly structured data, giving examples of how they had gotten inaccurate outputs or poor model performance as a result. Participants highlighted challenges in people not understanding the data, not being data literate, and also having issues chasing back data sources to find out what data really means and where it was acquired so they can use it effectively in models. Participants felt that organisations needed to focus more on model accuracy and robustness. Many participants highlighted the impact of conflicting goals, such as improving bias and having negative outcomes on model performance.\n\nTwo participants highlighted the negative impact of hallucinations, which they said was also a result of poor data or, at times, out-of-date data. One participant commented that the quality of the ML model can be compromised when models are fed chunks of data from various sources that conflict with or confuse the model, causing hallucinations. Another participant noted that hallucinations could frequently occur in specific topics that haven't been well-trained on the model, leading to confident hallucination responses to fill the gaps.\n\nThe positive benefits of introducing automated techniques to improve transparency was highlighted by participants. One participant described how introducing rule-based dashboards for data quality fields such as completeness, validity, reasonableness, and consistency improved both their model performance and gave organisation leaders confidence. Another participant described how the introduction of model transparency techniques gave confidence to senior leaders in AI decisions. Reproducibility was highlighted as important, with participants noting the importance of being able to do things such as going back and checking data when there are variances in figures and being able to revert to the previous state if something goes wrong in processes that are in place. One participant noted that AI assessments should be repeatable by software in heightened security risk situations. Overreliance on AI systems was also listed as a risk by some participants. One participant explained how employees already expect systems to be fully compliant and not allow them to do anything they shouldn't.\n\n## Literature findings:\n\nThe literature shows a number of common themes with research participants' comments in the area of technical robustness and safety. A technical report by the Joint Research Centre and the European Commission's Science and Knowledge Service drew attention to the importance of reproducibility, risks of overreliance on\n\nTable 6 Detailed Findings - Technological Robustness and Safety\n\n| Category                  |                 | Details                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n|---------------------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Performance, Transparency | Reliability and | Challenges: â€¢ Issues with model Reliability and Accuracy [P1, P3, P8, P9, P10, P11, P12, P13] â€¢ Issues with hallucinations [P3, P14] â€¢ Risk of overreliance on AI [P2, P3] Observations: â€¢ Automated data and model transparency initiatives improved organisational trust in AI [P6, P15] â€¢ Importance of reproducibility [P9, P13, P14]                                                                                                                                                                       |\n| Security, Risk &          | Trust Concerns  | Challenges: â€¢ Evolving security needs [P1, P7, P11] â€¢ Sensitive data leaking into model [P3, P9, P10] â€¢ Data Poisoning and Adversarial Attacks [P9, P11, P14] â€¢ Increase in volume of attacks [P1, P2, P9, P11] â€¢ Need for strong risk mitigation [P1, P3, P4, P9, P10] â€¢ Concerns current assessments like ISO27001 were not sufficient for AI [P9, P10] â€¢ Found existing security measures sufficient for at least some aspects of AI systems [P3, P4, P5] Observations: â€¢ Fear of the unknowns [P5, P7, P11] |\n\nAI, and discussed issues with model reliability and accuracy[62]. The report highlights two key aspects of evaluating an AI system's reliability: performance and vulnerabilities. They explain that poor performance would be a model that cannot perform well in a task that is considered normal for humans, and vulnerabilities would be performance that malfunctions in specific conditions, either unintentionally or through intentional malicious provocation. They discuss how model performance can vary when other features are added, such as introducing interpretability, which requires a trade-off with accuracy. They highlight the importance of reproducibility for AI reliability. The report highlights the importance of external data validation to help avoid overfitting and improve model performance in diverse scenarios. The report highlights the risk of blind trust in AI systems, which can lead to overreliance without sufficient knowledge of the system, which could have negative outcomes like errors or abuse of AI systems. The literature also reports a correlation between transparency and user trust. Transparency is required for building trust in Human-Centred Artificial Intelligence (HCAI), stating that the lack of procedures for investigating incidents, along with the lack of specifications of standards for black-box technology is inhibiting trust in those systems ([63]). This paper also proposes automated real-time risk evaluations to provide transparency for stakeholders. Additionally, increasing either transparency or control over decision-making in AI systems correlates positively with user trust in the system ([10]).\n\n## Discussion:\n\nThe insights from the study participants, and the supporting literature emphasise critical role of data accuracy and data management processes in ensuring technical robustness and safety in AI systems. Poor data literacy and lack of transparency or lack of knowledge about AI systems create risks for model performance and hallucinations. The possibility of negative outcomes from overreliance on AI systems was also expressed. Additionally,\n\nethical challenges between who decides the appropriate trade-off between bias and accuracy were highlighted, with an expressed need for standard practices to be developed to increase transparency. Increasing transparency was shown to increase user trust and AI adoption within organisations, giving confidence to leadership in AI models. Alongside transparency, reproducibility was flagged as essential in ensuring the reliability of AI systems.\n\n## 4.4.2 Security, Risk and Trust Concerns\n\nThis section considers the security challenges associated with AI, including emerging risks, vulnerabilities, and the measures required to build trust in AI systems. Interviews revealed growing concerns over adversarial attacks, data leakage, and the inadequacy of existing security frameworks in addressing AI-specific threats. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.\n\n## Interview findings:\n\nParticipants noted there were evolving security measures with AI, which some felt wasn't thought out enough, and that security wasn't moving as quickly as the technology, with new classes of security threats emerging all the time. One participant explained that when they create a model and apply it to an environment, it mathematically makes sense, but it must be validated, which is difficult when data sets fluctuate and need to be aggregated. Another participant said that AI tools are so much more powerful than people realise, and they are being developed without technical robustness and safety being built in. Participants noted risks around malicious attacks. One participant noted that processes for data acquisition, cleaning, and transformation are very light in determining whether that data has malicious intent. Another noted the importance of securing not just the model itself but the risks of injecting malware into data and data poisoning. Another noted that AI systems need to be protected from development pipelines, which can be interfered with, similar to supply chain attacks.\n\nSensitive data leaking through a model was a concern for some participants, particularly one participant who was in the process of incorporating AI into their business intelligence product, which would have sensitive competitor data on it; it was clear that it could not be leaked between clients. Another participant explained that AI can scrub data but that people can convince it to leak that data. Four participants noted that there was an increase in the volume of cyber-attacks. Some noted that this was because as AI lowered the bar to entry for hackers. One participant commented that hackers are no longer required to know an obscure programming language to launch good cyber-attacks; and another participant commented how individuals and companies are using AI to launch an increased volume of attacks. Another reason given by three participants was the increased surface area for attacks, with one participant noting that every time they transmit data, it's a risk for security and another commenting that they open up their models for other APIs and create multiple entry points, making it harder to protect. Data accuracy was cited as important in cyber security by one participant who noted that AI and Large Language models (LLMs) have created a huge new territory in cyber security with new risks.\n\nRisk mitigation was a key concern with participants highlighting the need for proper procedures and techniques in place. One participant said that risk level needs to be evaluated and made relevant to the level of oversight. One participant highlighted the importance of having data breach guidelines and risk mitigation processes in place with the provider to bring back the trust and repair any situation where something went wrong. Internal mistakes from AI builders to other teams were also highlighted as an important area to have proper checks in place. Another participant explained that the key thing for privacy is data minimisation, but with data being so valuable, companies are collecting it without techniques for data privacy being well implemented. They explained that sometimes data privacy is implemented in sections and can later be combined with other data that was purchased. So, in itself, it's anonymous, but when everyone has a piece of that jigsaw, they can put that together. One participant noted the benefits of looking after their own data, explaining they had implemented a very strict system for moving data around the company, which could only be accessed through VPN,\n\nand even then is highly regulated by legal and only on request. Participants highlighted a number of issues with AI assessments. One participant explained that the format of the current ISO standards is not sufficient for evaluating AI systems as they don't assess anything in a mathematically proven way. They also explained that the current ISO27001 standard should not be used for AI as it is a point-in-time standard and framework, and AI is fundamentally not assessable for that control. They said that the standard doesn't address AIspecific threats or threat vectors, that it has not been updated for AI, and cannot be updated in its current form. Another participant explained that the lack of verifiability of systems created trust issues when selecting providers to look after data. Stating that even though they can test their own security measures, the uncertainty generated by back to lack of actual assessments of data centres means that even if they physically go and look at data centres and examine systems, they still have to just trust that it will be secure. Therefore, they cannot assure their own clients their AI systems are secure because they can't actually be assured of vendor security. Three of the fifteen participants commented that existing security measures can serve technical robustness in at least some part of their AI. One participant noted that they have five layers to make sure their internal models work, so that was not something they are worried about as it's one of the easiest ones to secure. Another explained that they have good plans in place for a long time for high availability systems, including backup buildings and risk mitigation processes. A third participant said they hadn't worked in many systems where there were a lot of concerns around cyberattacks. Even in their current role where there's a lot of sensitive data, they felt there's not much cyber risk. Although there was an increased interest in security from customers, their model and security systems hadn't changed. Two of these three participants worked in non-technical roles, and one worked as a cybersecurity analyst.\n\n## Literature findings:\n\nThe literature reported concerns about evolving threats and the pace at which security is keeping up. The area of AI-specific security controls to address evolving threats is a very young but active field of research ([62]). This paper also found that\n\nAI systems being developed did not come close to meeting the minimal requirements of safety and security that would be expected from autonomous systems. Evasion attacks, which are attacks on AI models during the testing phase, are the most prevalent type of attacks on machine learning models [64]. Poisoning attacks, which are attacks during the training phase, are less common but can easily be carried out on applications that use data from untrusted sources to train their models. The researchers present a systematic framework for demonstrating AI attack techniques. However, they explain that unknown unknowns pose a significant risk as the field of AI evolves, and further research is needed in this area to help identify these. AI systems can produce outputs beyond human comprehension, making it impossible to predict the possibilities in terms of safety risks ([65]). The importance of continued research to develop more secure AI systems was echoed by [66], who found that repercussions of successful adversarial attacks can cause harm to public safety and privacy. In addition to discussing data poisoning, exploration, evasion and membership inference, the paper highlights the risks of model inversion, which has the capability to reverse engineer the sensitive data used to train the model from AI model outputs. The rise in adversarial attacks in AI was also reported that research into this field is extremely urgent, including investigating strengthening defence techniques for each individual AI application [67].\n\nIn the area of AI safety governance, researchers have proposed independent audits of AI systems to address assurance challenges based on three 'AAA' governance principles: Assessments, Audit Trails, and Adherence to jurisdictional requirements ([63]). They also stress the importance of interdisciplinary approaches to governance and propose automated real-time assessments of AI systems to increase transparency and reduce risk. There is a need for semi-automated tools for a comprehensive evaluation of AI systems. [9]. The cost of developing effective governance systems could be high; however, smaller, more agile sectorbased regulators could counter the resource costs associated with the necessary proliferation of regulatory bodies ([63]). The approach to governing AI at an industry sector level was echoed by other\n\nresearchers [10], along with calls for the development of context-specific standards [68]; [69]; [70]. Researchers also reported that countries such as South Korea have already issued Trustworthy AI Development Guidebooks at a sector level ([71]. Researchers have pointed to existing audit approaches, such as the audit model used in financial accounting, which is based on the Generally Accepted Accounting Principles (GAAP), safety standards such as ISO-13489:2015 and BS86112016, which includes a risk assessment for the ethical design of robotic systems [63].\n\n## Discussion:\n\nThe field of security for AI systems is not keeping pace with the advances in AI technology, with calls for a collaborative, interdisciplinary approach to developing evaluation and risk mitigation processes for AI systems. The level of unknowns in this space is seen as a high risk, particularly in areas which could result in significant negative societal outcomes in the case of security failure. The rapid pace of technological advances has led to evolving threats such as malicious attacks, data leakage, and increased cyber-attacks. The volume increase is in part due to the ability of AI-powered attacks to move quickly, along with a lower barrier to entry for hackers who can launch cyber-attacks using AI without specialist coding knowledge. Security assessments such as ISO27001 are not seen as fit for purpose for AI systems. Both the interviews and literature question the suitability of point-in-time checklist-based certification, noting that it is too manual and slow to implement and update to be considered a fully comprehensive solution for AI security evaluation. There is a clearly established need for AI technology to be evaluated on an ongoing basis through independent audits, context-specific standards, and real-time automated assessments to enhance transparency and mitigate evolving risks.\n\n## 4.5 Environmental and Societal Impact of AI\n\nThis section explores the wider impact of AI on the environment and society, highlighting concerns about energy consumption, environmental costs, societal inequalities, political radicalisation, and the growing need for ethical governance and cultural sensitivity in AI development.\n\n## 4.5.1 Environmental Impact\n\nThis section explores the environmental implications of AI, focusing on resource consumption, sustainability considerations, and the broader impact of AI infrastructure. Participants voiced concerns about the high energy demands of AI, inefficient data storage practices, and the lack of transparency regarding AI's environmental footprint. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.\n\n## Interview findings:\n\nSeven participants expressed concerns about the high energy consumption of AI data centres and its impact on the environment. Participants worked with AI and were aware of and alarmed by the high energy demands of these facilities, which were described as rivalling or surpassing the consumption of entire countries. Additionally, one participant highlighted major concerns over the extraction of rare metals necessary for GPUs. Two participants called for regulatory measures to mitigate environmental impacts. Suggestions included implementing fines based on energy consumption and enforcing stricter controls to prevent the unnecessary expansion of data centres driven by redundant data generation. One participant gave the example of considering how many pointless screenshots we each have on our phones and pointed out that AI could be generating and storing similarly redundant data at scale without regulations. The inefficiency in data management was also a point of concern for another participant who saw good use cases for AI to help the environment but did not know how to mitigate data management concerns. Two participants highlighted a lack of public awareness and a disconnect between AI usage and the environmental impact. It was pointed out that most people don't associate asking a computer to do something with its impact on the environment. They believed that transparency around the environmental impact of flying gave people choices regarding their travel choices but that the same environmental impact transparency did not exist for AI usage. Another participant pointed out that data centres use more electricity in Ireland than private homes, and with the huge traction AI has gained, this is a real concern. Three participants commented on the\n\nTable 7 Detailed Findings - Environmental and Societal Impact of AI\n\n| Category                     | Details                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Environmental Impact         | Challenges: â€¢ High energy consumption of data centres [P1, P2, P4, P6, P9, P11, P13] â€¢ Need for fines and regulation on environmental impact of AI [P1, P2] â€¢ Concerns About Data Redundancy and Management [P1, P3] â€¢ Lack of public awareness & disconnect from environmental impact of AI [P6, P9] Observations: â€¢ Variance in views on environmental benefits vs environmental costs [P3, P4, P9]                                   |\n| Societal and Cultural Impact | Challenges: â€¢ Positive changes to western workforce, with negative impacts in third-world countries [P7, P9, P10] â€¢ Significant negative influence on society and political radicali- sation [P2, P3, P9, P14] â€¢ Unwanted reinforcement of social inequalities and unrealistic standards [P11, P12, P13] â€¢ Loss of culture [P8, P14] â€¢ Challenges preventing companies implementing AI with nega- tive societal outcomes [P9, P12, P14] |\n\nbenefits of AI versus its impact on the environment. Two participants noted the potential to use AI for the good of the environment, even though it was not being done now. One participant said that none of the main current use cases being prioritised is about helping the environment, but instead, AI is destroying the environment and polluting for sometimes trivial results. The third participant commented that the increased power usage did not concern them currently, as the benefits outweighed their environmental concerns. Literature findings: The literature paints the impact of AI on the environment in different ways. On one side, researchers have cited AI as a solution to create efficiencies in companies that will lower their carbon emissions, but another view reports significant concerns about the impacts of AI systems on the environment. Findings indicate that AI can lower emissions through efficient processes developed through innovation and reduction in resource-intensive labour practices, in particular in big cities, where larger, older, and non-technology-based industries ([72]). AI has the potential to address environmental issues and climate change [73]. However, those researchers noted that historical precedents cautioned that new technologies could lead to negative unforeseen issues for sustainability when risks are not managed properly. There is a reported threshold effect, showing that AI reduces carbon emissions only after reaching a particular level of deployment, showing that in parts of China, this threshold was not reached [74]. They also noted that the effect is notable only in certain industries, particularly labour-intensive ones. Researchers reported that accuracy was prioritised over efficiency, meaning that AI developers were not prioritising carbon emissions when developing models [75]. They proposed developing multi-objective models, which considered training models to consider both the environmental impact and accuracy. Development of standards and trade-offs between accuracy and multiple trustworthy AI principles, including the environmental impact of AI systems, was also proposed by ([10]. Accountability for trade-offs in AI systems requires an increased level of oversight and accountability for businesses that can\n\nhave conflicting agendas when it comes to profitability and ethics [40]; [39]. Training a single LLM can emit 300,000kg of CO2, the equivalent of 125 round-trip flights between New York and Beijing[76]. They also highlighted the focus and prioritisation of companies on model accuracy over energy efficiency, which results in higher emissions, and highlighted a need to balance model performance with environmental impact. The paper also references the negative environmental impact of extracting metals from the earth.\n\n## Discussion:\n\nThe relationship between AI and environmental sustainability is intricate. While there are many potential environmental benefits argued in the literature, the current trajectory of balancing environmental gain with the environmental cost is not positive. The literature shows companies prioritise model accuracy over efficiency, leading to increased carbon emissions. The conflict of interest between prioritising business goals with ethical goals was highlighted, along with concerns about allowing organisations to choose their own trade-offs between environmental impact and business goals like accuracy. Questions around the environmental cost of training and operating AI systems, along with environmental considerations such as the extraction of metals for AI hardware, were highlighted by both the industry participants and researchers. An additional concern was highlighted around the environmental cost of the generation of vast amounts of redundant new data being stored in data centres. The disconnect and misconceptions between AI and its negative carbon footprint raised by participants were also seen in the confusion and conflicts in the literature, which, on the one hand, outlined the potential positive benefits of AI for the environment and, on the other hand, drew attention to the concerns over the carbon emissions from AI systems. The lack of clarity and standards to measure the impact of AI systems on the environment was an additional key concern in the literature.\n\n## 4.5.2 Societal and Cultural Impact\n\nThis section examines how AI affects society and culture, including its influence on employment, social structures, and ethical considerations in its deployment. Interviewees raised concerns about AI-driven job displacement, biases reinforcing social inequalities, and AI's role in shaping political discourse and cultural identity. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.\n\n## Interview findings:\n\nTwo participants pointed out that AI is about enhancing people's jobs, not replacing them, and that AI can give more meaningful jobs to people. One participant said they were implementing AI cautiously in their organisation, with a focus on ensuring employees felt secure and were aware their jobs were not being replaced. They explained that they were delivering the rollout by being sensitive to the feelings of employees who might have worked at a task for twenty years that was now being automated with AI. Another participant suggested taxing AI models in cases where there were significant job displacements. While those comments around changes to the workforce were positive, one participant highlighted the negative impact on those working in third-world countries where dangerous jobs were involved in the mining of rare metals. They described the work done by artisanal miners who mine metals for AI systems with their bare hands in the global south as devastating. Concerns were raised by four participants around the significant influence AI is having on society, with all four drawing attention to AI's ability to influence politics. The existing harm to society and democracy was noted as already being extremely damaging, with further concerns surrounding the escalation of AI. Two participants raised significant concerns about the use of AI to profile users and feed them tailored versions of news content. This was described as leading to radicalisation and the formation of online groups that both believe are right because of how AI has tailored their version of news content. Participants referenced platforms such as X (formerly Twitter), TikTok, and Reddit as places where people are being delivered harmful information that has negative societal effects. Participants described how these platforms influence you through AI personalisation, with one describing the information as being fed through social media, resulting in political radicalisation. One participant explained that AI can influence you very quickly because it has\n\nbuilt a profile on you and knows you. The creation and dissemination of deep fake videos and images online were noted by one participant as having already damaged trust in the news online and undermining democracy, with another referencing that in social settings on trips to the US, they found it is now considered improper social etiquette to discuss politics due to tensions and deep political divides in their society.\n\nThis research also highlighted concerns around unwanted reinforcement of things such as social inequalities and unrealistic standards. One participant noted that vulnerable groups, such as young people, individuals from underprivileged communities, and people of colour, need more transparency so that AI doesn't cement existing inequalities in society. Another participant gave the example of harmful AI filters that are designed to alter body types or enhance beauty, asking about the effects that it has on that child when the filter gets turned off, and they immediately compare themselves to the AI version. They pointed out that social media companies should be investing more in the mental health effects of their platforms and the tools their platforms are having on the young people who use them. They argued that platforms need to work more on improving features on their platforms that contribute to increased rates of depression and suicide in young people. Another participant provided the example of bias in insurance models, asking who gets to decide how much weight your gender or postcode should have on your premium, explaining that bias does exist for things like this. They explained that models know that men are more likely to crash and bias against them, but when you reduce that bias in the model, it gets traded off with accuracy. They posed the question, 'Who gets to decide what's fair and unfair?' and called for more transparency for people in society into how these models work. Two participants flagged loss of cultural identity, beliefs, language and knowledge as a concern. One participant pointed out that there are variances in cultural beliefs and agreements about what is fair and what is discrimination. They explained that when you ask an AI for an answer, people from different parts of the world would have a different right and wrong answer, giving the example of European data holding European beliefs, which wouldn't likely be accepted in other places such as Afghanistan, which has its own set of beliefs. Another participant detailed how the language used for data going into and coming out of AI systems was becoming standardised, resulting in a loss of local language, colloquialisms, and even industry terminology that was leading to the sacrificing of cultural identities. They described a situation where a global data team wanted to rename local placenames so they could be more easily understood by an AI system. As they foresaw the downstream cultural effects, they decided to fight for their sense of Irish identity and argued against the change, noting, however, that these things aren't always considered and that there were already many changes to language they had seen in the short time that AI has been used. Three participants raised the challenge of enforcing governance on companies that implement AI with negative societal outcomes. One participant explicitly stated that we cannot control how data gets produced and used, stating that it was definitely a concern for them. They pointed out that they are given sole responsibility for pricing for a large number of customers, and although nobody enforces fairness, it was important for them to have integrity towards the model, so it is fair and works for everyone. However, they said that in their experience, they have to work towards a target revenue, and if it's not matching, they have to bridge the gap; this involves changes in feature selection and the bias that goes into it. They said that although they would like the model to have a lower level of bias, their company requires them to present multiple options of trade-offs between bias and accuracy so that executives can decide the trade-offs, describing the process of allowing machines to implement bias in these decisions as unfair. Another participant explained that if humans making decisions around the AI process are financially incentivised to choose profit, then they will, and decisions around profit and fairness cannot be made by a person with a financial interest in the outcome. They explained agency is more than just consent; humans should have control over the decisions and the outcomes of the AI system, highlighting that the AI should be for humanity's good rather than the company's profit. Challenges regulating AI internationally were also flagged by two participants, with one calling for international cooperation and transparency around which data\n\nis moved between regions. Another participant described how AI gives companies an edge, so even if regulation is introduced, the countries that do permit unregulated AI will gain advantages, in particular, countries that are already heavily sanctioned for other activities.\n\n## Literature findings:\n\nAmerish et al 2020.[77] defines culture as the ethical, sociological, technological, and ideological features of a society or social group which define their way of life, work and communication. They suggest that culture is a precedent to the process of knowledge creation and thus influences the perception and construction of knowledge systems. They believe that the emergence of AI systems, which have been shaped by the cultural parameters of the 'West' and the current social and economic power structures, is fundamentally shifting the entire social, political and ethical fabric and disrupting the natural environment. They explained that the foundation of modern technological development in the West originates from British social values, namely the utilitarian principle, which seeks to achieve the greatest good for the greatest number and a desire to control the environment for the betterment of human life. In contrast, countries like Japan inherently value group solidarity, social harmony and a reduction of internal conflict, and China holds the traditional cultural values of Confucianism, which stresses the importance of 'formal learning processes' and administrative guidance (by the bureaucratic Mandarin class), and Taoism which emphasises living in harmony with the natural order of the universe. The paper suggests that a fresh perspective on which trajectory the technological process should take is needed, suggesting that the Eastern values of interdependence and universal harmony with nature should be revived and integrated into how we shape the future of technology. Other research argues that variances in cultural values need to be considered when developing evaluation criteria for Trustworthy AI technology [10]. They also highlight a disconnect between what policy makers, AI experts, and a standard non-expert user considers fair, which, along with these differences in culture, show a need for the inclusion of a wide variety of stakeholders to establish norms for deciding what Trustworthy AI looks like. Research suggest that AI is an opportunity to implement sociotechnical change that can help bring about a better, fairer world [78]. They note that while sociologists are partially contributing to the design of AI systems, they need to be involved more to help design technology that is more beneficial for society. A challenge they present is decision-making protocols that favour corporate elites, naming Silicon Valley as a part of the problem leading to the current class, gender and racial inequalities in societies. Silicon Valley and corporate interests have influenced the direction of academic research, noting as a shift towards entrepreneurship in education aimed at fostering new ventures [79]. Additional concerns about conflicts over the ownership of art and a loss of culture were raised by researchers [80].\n\nAdditional societal concerns around the impact of AI on radicalisation were raised by researchers. Researchers reported that AI systems have increased the radicalisation and divide between viewpoints linked to political violence [79]. The paper highlights the research gap in the harmful political and social effects of AI. Although AI can be used for cyber security purposes, they suggest that this term should be broadened to include the harmful effects of social media and AI systems on societies. Persuasion into radicalisation is a process involving the weaponisation of words to convince a target audience of a particular narrative [81]. This paper also discuss the use of counter-narratives to combat these, in particular the ones most likely to lead to violence. They explained how AI was used to distribute 'fake news' on social media while also being used to help identify and censor terrorist material. They highlighted that the volume of harmful content online means that it would be impossible to remove it all or attempt to rebut it with counter-narratives. However, they highlight that countries should attempt to decrease the credibility gap, address the root causes, and use both online and offline approaches to solve the issues of online radicalisation that lead to violent extremism. There is a need for Extremism, Radicalisation and Hate speech (ESH) detection tools to combat the societal risks associated with social media's ability to mobilise extremist communities [82]. To contribute to protecting both free speech and user safety, the paper proposed an ERH\n\ncontext-mining framework which involved ideological isomorphism (radicalisation), morphological mapping (extremism), and outward dissemination (politicised hate speech). They highlighted the need for more research into resolving biases in dataset collection, annotation and algorithmic approaches in this field.\n\nResearchers reported a dual positive and negative effect on the workforce. While it was noted that AI could replace lower-skilled positions, resulting in improved skill level of the workforce [72], researchers also raised concerns about the current underpaid workforce that is keeping AI systems going ([76];[80]. Challenges in implementing sufficient oversight and accountabilities for companies who can release AI systems that have harmful societal outcomes were also highlighted by several researchers who demonstrated a clear need for standards and proper accountability for organisations [38]; [39]; [40].\n\n## Discussion:\n\nWhile there are many arguments around the potential of AI for good, the current economic and social structures have led to the prioritisation and shaping of AI technology that is causing unintended harm to societies. Western cultural values and influential decision-makers in places like Silicon Valley have shaped the development of AI technology. However, the decisions made by those investing in technology conflict with the values held by populations in Western societies, which broadly strive for the greatest good for the greatest number of people. The societal values of controlling and subjugating the natural world for the benefit of humans have spread rapidly throughout Western culture in recent years from Great Britain, directly conflicting with the dominant Eastern values of harmony with the natural world. AI systems, the design and function of which are shaped by the cultural, social and economic structures existing in the global technology industries, can transform societies at scale and result in mass shifts in the fabric of our societies. Adverse societal outcomes are seen from AI systems, in particular, the radicalisation of viewpoints online, the amassing of extremist communities and a rise in political violence. AI is expected to cause significant shifts in the global workforce. After a transition period of job displacement, the Western world anticipates positive outcomes: job functions are expected to become more attractive for employees, with a reduction in unwanted, lower-paid administrative tasks. However, there is already a notable reliance on an underpaid workforce, described by researchers as a 'sweatshop of programmers' in the Eastern world, which sustains AI infrastructure globally. Further research is needed to establish how we can challenge existing social and economic structures to influence the design of AI systems, which can be aligned with the culture and values held by societies using that technology.\n\n## 5 Reflection on the EU AI Act from our Findings\n\nThe EU AI Act [1] seeks to protect fundamental rights by regulating AI systems, mandating risk assessments, accountability mechanisms, and compliance requirements. It also refers to the EU Principles of Trustworthy AI as a guidance for safe AI development. However, the findings from this study highlight significant gaps in how the industry professionals in this study understand and implement trustworthy AI principles, raising questions about whether the Act's provisions will be effective in practice. The section links key industry concerns identified in our research to the regulatory approach taken by the AI Act.\n\n## 5.1 Accountability, Governance, and Regulatory Compliance:\n\nThe AI Act, clearly defines the roles and responsibilities for those involved in AI systems, but accountability in practice remains complex. AI accountability is divided across multiple roles in the Act, including AI providers who build, buy or adapt AI models and place them on the market, and deployers who use them. Additional responsibilities fall on importers, distributors, and supervisory authorities, but ambiguity arises when any distributor, importer, deployer or other third party makes a 'substantial modification' to a high-risk AI system, shifting their classification and compliance requirements. The Act mandates technical documentation and data labelling transparency, yet challenges such as communication\n\naround potentially sensitive data between various stakeholders in the AI development process remain. This includes difficulties coordinating compliance across multiple stakeholders when dealing with third-party AI models ([83]. Findings from industry professionals during our interviews highlighted key concerns with compliance readiness. Accountability within organisations is fragmented, with unclear ownership of AI governance and reliance on third-party compliance claims that often lack verification. Many companies approach AI governance reactively, addressing risks only when problems arise, while existing compliance tools are insufficient for AI-specific risks. Financial incentives also influence compliance, with auditors and internal teams sometimes prioritising approvals over rigorous assessments. Unlike GDPR, AI regulations lack widely adopted compliance frameworks, making enforcement inconsistent and challenging. While the AI Act establishes an important foundation for AI governance, the insights reported by participants in this study and subsequent literature review indicate that its effectiveness depends on stronger enforcement mechanisms. Real-time compliance monitoring, sector-specific regulatory frameworks, and independent auditing are necessary to prevent the Act from becoming another bureaucratic hurdle rather than a meaningful tool for ensuring Trustworthy AI.\n\n## 5.2 Data Management and Quality in AI Systems:\n\nThe AI Act includes provisions for data governance, requiring that AI models be trained on high-quality, well-documented datasets. However, our interview findings show that poor data quality, provenance issues, and synthetic data manipulation are widespread industry concerns. Many AI professionals noted that their organisations struggle to ensure data integrity, with some questioning whether data brokers provide truthful representations of consent and compliance. The lack of transparency in third-party data sourcing is a direct challenge to the AI Act's goal of ensuring fairness and accountability. Additionally, the growing reliance on synthetic data raises concerns about bias amplification and the loss of real-world validity in AI models. While the AI Act touches on data quality, it does not yet include specific provisions to regulate synthetic data generation, which may become a loophole that weakens compliance efforts. Given the risks associated with data quality failures, including biased decision-making and unreliable AI outputs, the Act may need to introduce stricter requirements for provenance tracking and independent data audits.\n\n## 5.3 Human Factors in AI Development and Oversight:\n\nThe AI Act mandates human oversight in highrisk AI systems, requiring human intervention in decision-making processes. However, industry professionals highlight several challenges with how human oversight is implemented in practice. Some noted that organisational structures do not currently support effective AI governance, as compliance teams often lack the technical expertise to assess AI risks, and AI engineers may not fully understand ethical and regulatory requirements. Another key issue is conflicts of interest in human decision-making. Several professionals pointed out that when humans are included in AI oversight, their incentives may prioritise business interests over ethical concerns. For example, when AI bias is detected, professionals reported that leadership often dismisses these concerns due to the financial trade-offs associated with bias mitigation. The AI Act does not currently specify the qualifications, independence, or ethical responsibilities of human overseers, leaving significant room for organisations to self-regulate oversight in ways that may not be effective. Additionally, there is an emerging risk of over-reliance on AI, where professionals assume that AI-driven compliance tools will inherently prevent unethical or illegal behaviour. This 'blind trust' in AI automation contradicts the AI Act's aim to ensure meaningful human control, highlighting a gap between regulatory intent and industry practice.\n\n## 5.4 Technological Robustness and Safety:\n\nWhile the AI Act introduces requirements for technological robustness and security, our findings reveal that AI security risks are evolving\n\nfaster than regulatory frameworks. Industry professionals expressed concerns about data poisoning, adversarial attacks, and sensitive data leakage in AI systems, but noted that security compliance efforts often rely on reactive measures rather than proactive risk management. A number of participants specifically criticised the use of static certifications like ISO27001 for AI security, arguing that these frameworks were not designed for real-time AI monitoring. The Act's provisions on security could be strengthened by incorporating continuous evaluation mechanisms rather than relying solely on documentation and predeployment risk assessments. Without this, companies may remain vulnerable to dynamic security threats that emerge after AI systems are deployed.\n\n## 5.4.1 Environmental and Societal Impact:\n\nThe AI Act includes fairness and societal wellbeing as core principles but does not yet introduce concrete enforcement mechanisms for evaluating AI's societal and environmental impact. Our research finds that AI professionals are increasingly aware of AI's negative societal consequences, including reinforcement of social inequalities, political radicalisation, and cultural loss. However, companies currently lack clear guidelines on how to measure and mitigate these risks in practice. Similarly, AI's high energy consumption remains an industry concern, with some professionals calling for environmental impact assessments and potential fines for AI-driven carbon footprints. While the AI Act does acknowledge the need for responsible AI development, it does not yet introduce binding sustainability requirements. As AI adoption continues to grow, the regulatory framework may need to evolve to include mandatory environmental impact assessments for AI models, particularly those requiring large-scale computational resources.\n\n## 6 Practical Checklist for Operationalisation of Trustworthy AI\n\nWhile this study has identified critical challenges in achieving trustworthy AI across governance, data quality, regulatory compliance, fairness, and human oversight, it is evident that the industry lacks a consolidated, actionable pathway for addressing these issues in practice. Many of the principles outlined in the EU's Ethical Guidelines for Trustworthy AI, though widely accepted in theory, remain difficult to implement. To bridge this gap between theory and application, we propose a practical governance checklist with a list of actions designed to assist organisations in operationalizing the challenges identified in this paper for the implementation of Trustworthy AI. This is detailed in table 8 Although standards such as ISO42001[14] offer structured frameworks for AI Management Systems, they are often high level and lack the focus of the specific challenges which were identified through this research. This checklist translates the thematic findings of our interviews into concrete organisational actions that can be implemented across organisations, as a supplement to existing certifications and standards such as ISO42001. While this checklist is designed for practitioner use, we acknowledge that its application may vary considerably across sectors and organisations and we therefore frame it as a flexible guide rather than a prescriptive standard.\n\n## 7 Discussion and Future Directions\n\nAI technologies present both opportunities and challenges in ensuring the development and deployment of TAI. Addressing the identified industry challenges requires an independent and sector level approach involving policy reforms, standardisation of industry best practices, and collaborative efforts including all relevant stakeholders. This section outlines future directions for the improvement and adoption of TAI principles including recommendations for policymakers, industry and academia. All three areas should involve diverse stakeholders, including underrepresented communities, in their decision-making process to ensure AI systems are developed with a broader perspective, mitigating biases and promoting fairness across different segments of society. Efforts should be made to prevent the loss of cultural identities due to AI standardisation. This can be achieved by incorporating local languages, customs, and values into not just developing AI\n\nTable 8 Key Challenges and Recommended Organisational Actions\n\n| Key Challenge                                  | Recommended Organisational Action                                                                            |\n|------------------------------------------------|--------------------------------------------------------------------------------------------------------------|\n| Siloed departments & communication gaps        | Establish cross-functional AI steering committees to improve col- laboration and compliance integration.     |\n| Unclear lines of responsibility                | Clearly define and document roles and responsibilities for AI com- pliance and accountability.               |\n| Blind trust in third-party compliance          | Conduct thorough due diligence and regular audits of third-party providers.                                  |\n| Lack of systematic bias testing                | Implement regular and mandatory bias audits and fairness evalu- ations.                                      |\n| Conflict between fairness and prof- itability  | Incorporate fairness metrics into business KPIs and decision- making frameworks.                             |\n| Confusion around AI Act implications           | Provide targeted training on regulatory requirements and impli- cations for different teams.                 |\n| Lack of AI-specific standards                  | Adopt and contribute to the development of sector-specific AI standards.                                     |\n| Rush to implement AI without risk mit- igation | Mandate risk assessments before deployment of AI systems.                                                    |\n| Lack of AI assessment tools                    | Invest in or develop AI-specific compliance and assessment tools.                                            |\n| Poor data quality                              | Introduce automated data quality checks and establish quality thresholds.                                    |\n| Vendor misrepresentation of data con- sent     | Implement stringent data provenance checks and vendor contracts with accountability clauses.                 |\n| Overreliance on synthetic data                 | Use synthetic data cautiously, ensuring validation and bias moni- toring processes.                          |\n| Inadequate data documentation                  | Develop automated and regularly updated data lineage documen- tation systems.                                |\n| Insufficient human oversight                   | Define human-in-the-loop checkpoints and mandate oversight at critical decision points.                      |\n| Lack of explainability                         | Use explainable AI methods and ensure model transparency is available to internal and external stakeholders. |\n| Model hallucinations                           | Ensure high-quality, up-to-date data is used and monitor model outputs for anomalies.                        |\n| Evolving cyber threats                         | Establish continuous AI security assessment protocols and update cybersecurity training.                     |\n| Increased surface area for attacks             | Limit unnecessary data exposure and API access points through stringent access controls.                     |\n| Unethical data practices                       | Enforce ethical data collection policies and increase internal audit- ing of data usage.                     |\n\nsystems, but at the level of prioritising which technology gets researched, developed and implemented into societies. The use of AI in social media should have sufficient oversight to prevent the spread of misinformation and radicalisation, whilst also respecting and preserving cultural diversity of the societies they introduce their systems into. Implementing AI models that detect and mitigate harmful content can help protect societal well-being, but due to the speed of which AI can be rolled out, the cultural impact should also be prioritised.\n\n## 7.1 Policymakers\n\nThis study's findings indicate that the AI Act alone may not fully address the practical challenges of AI implementation. Industry professionals face uncertainty regarding compliance,\n\nstruggle with data quality and oversight, and report that security risks are escalating beyond what current regulations can manage. Without stronger enforcement mechanisms, sector-specific governance frameworks, and real-time AI evaluation tools, the Act may prove insufficient in ensuring genuinely Trustworthy AI. To close the gap between policy and practice, regulators should clarify accountability structures to ensure independent and enforceable AI oversight roles, introduce dynamic compliance mechanisms that go beyond static certifications to incorporate realtime AI system monitoring, and enhance security provisions by requiring ongoing adversarial testing and AI-specific risk mitigation measures. Additionally, regulating synthetic data usage through stricter standards for provenance and bias mitigation, as well as incorporating environmental accountability by mandating carbon footprint assessments for AI models, could strengthen the Act's effectiveness. By addressing these gaps, the AI Act could evolve from a broad regulatory framework into a more effective instrument for ensuring AI accountability, security, and ethical alignment in real-world applications.\n\nPolicymakers should work towards providing clearer and more detailed regulatory guidelines at an industry sector level to eliminate ambiguities surrounding the EU AI Act and other related regulations. The creation of AI-specific standards and certifications, audited by independent bodies who are not being paid by the company they are auditing, is crucial for establishing uniform and fair evaluation. Standard-setting organisations, in collaboration with industry experts and academics, should develop quantifiable metrics and real-time assessment tools tailored to different AI use cases and sectors. These standards should address all seven TAI principles. To enhance accountability, regulators could introduce independent auditing bodies to oversee AI compliance at a sector level. These bodies should operate without conflicts of interest to ensure impartial evaluations. In addition to developing quantifiable standards, enforcing stricter penalties for non-compliance, such as higher fines and restrictions on operations is required to ensure that companies are sufficiently deterred from bypassing ethical considerations in favour of profitability. Given the global nature of AI development, international cooperation is essential for harmonising regulations and standards. Policymakers should engage in cross-border dialogues to address challenges related ensuring that AI systems are developed safely, and in alignment with the values of the societies that they are impacting.\n\n## 7.2 Industry\n\nIn addition to the recommended actions provided in table 8, organisations should embed ethical principles into every stage of the AI development lifecycle. This includes conducting thorough bias testing, ensuring data quality, and incorporating fairness and transparency metrics. Companies can build more trustworthy AI systems by prioritising ethical considerations alongside technical performance. Forming interdisciplinary teams or AI steering committees can break down departmental silos and foster collaboration between technical experts, compliance officers, and business leaders. These teams should oversee AI strategy, compliance, and risk management, ensuring that accountability is clearly defined within the organisation. Continuous education and training programmes can enhance employees' understanding of AI ethics, data literacy, and regulatory requirements. By equipping staff with the necessary knowledge and skills, organisations can improve human oversight and reduce risks associated with over-automation and poor decisionmaking. Transparency in AI systems can build trust among stakeholders. Organisations should adopt tools and methodologies that explain AI decision-making processes, making them accessible to both internal teams and external users. Transparent communication about data sources, model decision making and limitations, and ethical considerations can also mitigate misunderstandings and increase user trust and adoption of AI systems.\n\n## 7.3 Academia\n\nAcademic institutions and research organisations should encourage multidisciplinary studies that combine insights from areas such as computer science, ethics, sociology, and law. These collaborations can lead to the development of more holistic approaches to TAI design and evaluation and help to address complex challenges like cultural biases and societal impacts. There is a\n\npressing need for semi-automated and real-time assessment tools capable of evaluating AI systems comprehensively. These tools should align with regulations such as the AI act, and include a number of metrics for TAI principles, which can be adopted for multiple use cases. Research efforts should focus on creating solutions that can continuously monitor AI performance and help inform decisions around trade-offs for TAI principles and company goals such as profitability. This will increase transparency and aid in reducing the reliance on manual checklists and point-intime assessments. Educational initiatives aimed at the general public can bridge the disconnect between AI usage and its societal and environmental impacts, as well as increase knowledge about how these systems work and their biases and limitations. By increasing awareness, individuals can make informed decisions and advocate for responsible AI practices, contributing to a culture that values TAI principles. Collaborative efforts should be directed towards researching energy-efficient AI models and promoting sustainable practices. This includes optimising algorithms for lower energy consumption, utilising renewable energy sources for data centres, and developing standards to measure and report the environmental impact of AI systems. Thought should be put into what aspects of AI academia wants to help further knowledge in. Current economic structures, including corporate interests, should not continue to increase their influence in directing which aspects of knowledge is furthered by academic research.\n\n## 7.4 Limitations\n\nThe study has several limitations. First, the sample was confined to the UK and Ireland, where regulatory proximity to the EU AI Act may shape perspectives in ways that differ from those in other jurisdictions. Second, while we sought diversity in job functions, certain roles central to AI development (e.g., algorithm engineers, UX researchers, in-house legal experts) were not included. Their absence limits the generalisability of our findings, though the focus on deployment and governance roles remains aligned with our research objectives. Finally, with fifteen participants, our qualitative approach prioritised thematic depth over breadth;\n\nas such, findings should be interpreted as illustrative of lived industry perspectives rather than definitive of the sector as a whole.\n\n## 7.5 Proposed Next Steps\n\nWhile this paper prioritises empirical insight from industry voices, we recognise that further work could benefit from deeper integration with established theoretical frameworks such as institutional theory, organisational ethics, or sociotechnical systems thinking. These lenses offer valuable ways of understanding how organisational norms, structures, and power dynamics shape the adoption of Trustworthy AI. However, given the emergent and still-contested nature of Trustworthy AI as a concept, and the intention of this paper to centre lived experience and practical barriers, we have chosen not to impose a singular theoretical framing. Future research could productively explore how these perspectives might complement the findings presented here, particularly in understanding how ethical AI commitments are shaped, constrained, or enabled by institutional context.\n\nAs a next step, we are focusing on developing a follow-up project that builds on the findings of this paper to provide more practical guidance for industry. This work will focus on designing a governance evaluation framework to help organisations align with both the Trustworthy AI principles and the requirements of the EU AI Act. It will translate high-level ethical and regulatory expectations into a practical tool that can be used across different stages of the AI lifecycle to assess systems for both regulatory compliance and adherence to the seven Trustworthy AI principles. Informed by the challenges identified through our interviews, the proposed framework will aim to bridge the gap between abstract principles and day-to-day industry implementation. Future work could more explicitly integrate theoretical frameworks, such as institutional theory, to situate these findings within broader organisational research traditions. In this study, however, our priority was to focus on the industry participants' lived experiences in practice.\n\n## 8 Conclusion\n\nThis paper contributes significantly to academia by providing empirical insights into the challenges faced by industry professionals who participated in this study in implementing Trustworthy AI. Through structured interviews with fifteen experts across various sectors, we bridge the gap between academic and regulatory knowledge and real-world TAI practices of the participants interviewed. The study highlights ambiguities in accountability and governance, revealing a 'wild west' mentality where both AI adoption and enforceable regulations are not incorporating thorough ethical considerations. By documenting issues raised by participants such as data quality problems, inherent biases, and overreliance on AI without proper safeguards, the research emphasises the urgent need for standardised metrics, automated assessment tools, and interdisciplinary oversight. Academically, it fills a critical gap by offering qualitative data on industry practices, underscoring discrepancies between policy intentions and industry realities. It also highlights a need for societal and environmental impacts to be included in safety assessments of AI technology. In essence, this study advances academic knowledge by providing empirical evidence of industry challenges in the development and assessment of TAI, highlighting the need for more tangible regulations and standards, and emphasising the important role of human oversight and accountability. The findings from the participants in this study illustrate how practitioners in our sample perceive and navigate the challenges of Trustworthy AI. While the insights are not generalisable to the industry as a whole, they nonetheless highlight pressing issues in governance, accountability, and evaluation that are likely to resonate across a range of organisational contexts. These insights lay a foundation for future research to develop effective strategies and policies that align AI development with societal values and ethical principles.\n\nAuthors and Affiliations. All authors have reviewed and consented to the publication of the manuscript as presented. This research received partial support from the Research Ireland under grants 13/RC/2106 Â¶ 2 (ADAPT) and is co-funded by the European Regional Development Fund (ERDF).\n\nEthics Declaration. The data employed in this review are sourced from one to one interviews with fifteen professionals following the University of Galway Ethics Process. The research was approved by the University of Galway's Research Ethics Committee (REC). The ethics protocol covered the outreach approach, which included contacting participants via email and LinkedIn. Informed consent was obtained from all participants via signed consent forms prior to their involvement in the structured interviews. This ethical process ensured compliance with GDPR regulations and ethical standards for research involving human participants.\n\nAdditionally research was collected through publicly available materials, including published research articles, ISO standards, books, and openly accessible databases and industry publications. All sources are duly cited and listed in the reference section of this paper.\n\n## Appendix A Structured Interview Questions\n\n## Data Acquisition\n\n- Do you know how data is acquired for use in AI typically, and if so, can you tell me about it?\n- Do you have concerns around data acquisition for AI?\n- In your opinion, what are the most important aspects of data acquisition for AI?\n\n## Data Quality\n\n- What do you know about data quality and its importance in relation to AI?\n- Do you have concerns around data quality for AI?\n- In your opinion, what are the most important aspects of data quality for AI?\n\n## Data Preparation\n\n- Do you know how data is prepared for use with AI? Tell me about it if so.\n- Do you have concerns around data preparation for AI?\n- In your opinion, what are the most important aspects of data preparation for AI?\n\n## Data Provenance\n\n- Do you know if documentation is generally kept on data as it moves through the various stages of the AI system?\n- In your opinion, what are the most important aspects of data provenance for AI?\n- If you wanted to understand aspects of data usage in the company, how would you find out where the data came from and how it was transformed or used?\n- Do you have concerns around data provenance for AI?\n\n## Trustworthy AI\n\nDo you have any additional comments, concerns or insights in relation to AI, specifically in relation to each of the seven key principles:\n\n1. Human agency and oversight\n2. Technical robustness and safety\n3. Privacy and data governance\n4. Transparency\n5. Diversity, non-discrimination and fairness\n6. Environmental and societal well-being\n7. Accountability\n\n## Assessment of Standards and Regulations\n\n- How do organisations you have worked with generally assess for compliance with standards like ISO27001 and ISO27701 or regulations like GDPR?\n- In your experience have you seen any challenges with assessment?\n- Can you tell me any methods or frameworks you are aware of?\n- In relation to self-assessment for AI compliance in the five aspects of data we previously discussed, can you foresee any potential challenges or obstacles?\n- Are there any issues you see in relation to documenting these assessments?\n- How does your organisation document assessment with standards and regulations?\n- Who are the key stakeholders involved in the assessment of standards and regulations like GDPR and ISO27001?\n- Do you know what processes these stakeholders follow or how they engage with the wider organisation on this work?\n\n- Based on your knowledge of ISO27001 and ISO27701 (if any), what are the key aspects you think will need to be extended to include AI-specific aspects of these ISOs?\n\n## References\n\n- [1] European Union: Regulation (EU) 2024/0138 of the European Parliament and of the Council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union Legislative Acts. https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX: 52021PC0206. COM(2021)0206 - C9-0146/2021 - 2021/0106(COD) (2024). https://eur-lex.europa. eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206\n- [2] European Commission: Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act). Official Journal of the European Union. COM(2021) 206 final, updated text as provisionally agreed in 2024 (2024). https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206\n- [3] European Parliament and Council: Regulation (EU) 2023/988 of the European Parliament and of the Council of 10 May 2023 on general product safety and repealing Directive 2001/95/EC and Council Directive 87/357/EEC. Official Journal of the European Union, L 135, 23.5.2023, p. 1-48. General Product Safety Regulation (GPSR) (2023). https://eur-lex.europa.eu/eli/reg/2023/988/oj\n- [4] European Parliament and Council: Directive (EU) 2024/2853 of the European Parliament and of the Council of 23 October 2024 on liability for defective products and repealing Council Directive 85/374/EEC. Official Journal of the European Union (2024)\n- [5] CorrË† ea, N.K., Camila GalvËœ ao, C., Santos, J.W., Del Pino, C., Pontes Pinto, E., Barbosa, C., Massmann, D., Mambrini, R., GalvËœ ao, L., Terem, E., Oliveira, N.: Worldwide ai ethics: A review of 200 guidelines and recommendations for ai governance. Patterns 4 (10) (2023) https://doi.org/10.1016/ j.patter.2023.100857\n- [6] Schultz, M.D., Conti, L.G., Seele, P.: Digital ethicswashing: a systematic review and a processperception-outcome framework. AI and Ethics, 1-14 (2024)\n- [7] Charter of Fundamental Rights of the European Union. Official Journal of the European Union, 2012/C 326/02. Accessed: 2024-11-22 (2012). https://eur-lex.europa.eu/legal-content/EN/TXT/ ?uri=CELEX%3A12012P%2FTXT\n- [8] European Commission High-Level Expert Group on Artificial Intelligence: Ethics Guidelines for Trustworthy AI. European Commission. Accessed: 2025-03-25 (2019). https://ec.europa.eu/ futurium/en/ai-alliance-consultation\n- [9] McCormack, L., Bendechache, M.: Ethical ai governance: Methods for evaluating trustworthy ai. arXiv preprint arXiv:2409.07473 (2024)\n- [10] McCormack, L., Bendechache, M.: A comprehensive survey and classification of evaluation criteria for trustworthy artificial intelligence. AI and Ethics, 1-22 (2024)\n- [11] Harris, C.: Mitigating age biases in resume screening ai models. In: The International FLAIRS Conference Proceedings, vol. 36 (2023)\n- [12] Deng, W.H., Nagireddy, M., Lee, M.S.A., Singh, J., Wu, Z.S., Holstein, K., Zhu, H.: Exploring how machine learning practitioners (try to) use fairness toolkits. In: Proceedings of the 2022 ACM\n\n- Conference on Fairness, Accountability, and Transparency, pp. 473-484 (2022)\n- [13] Laux, J., Wachter, S., Mittelstadt, B.: Three pathways for standardisation and ethical disclosure by default under the european union artificial intelligence act. Computer Law &amp; Security Review 53 , 105957 (2024)\n- [14] Standardization, I.O., International Electrotechnical Commission: ISO/IEC 42001:2023, Information technology -Artificial intelligence -Management system. International Organization for Standardization and the International Electrotechnical Commission, Geneva, CH (2023)\n- [15] Lopes, I.M., Guarda, T., Oliveira, P.: Implementation of iso 27001 standards as gdpr compliance facilitator. Journal of information systems engineering &amp; management 4 (2), 1-8 (2019)\n- [16] Guest, G., Bunce, A., Johnson, L.: How many interviews are enough? an experiment with data saturation and variability. Field methods 18 (1), 59-82 (2006)\n- [17] Guest, G., Namey, E., Chen, M.: A simple method to assess and report thematic saturation in qualitative research. PloS one 15 (5), 0232076 (2020)\n- [18] Braun, V., Clarke, V.: One size fits all? what counts as quality practice in (reflexive) thematic analysis? Qualitative research in psychology 18 (3), 328-352 (2021)\n- [19] Braun, V., Clarke, V.: To saturate or not to saturate? questioning data saturation as a useful concept for thematic analysis and sample-size rationales. Qualitative research in sport, exercise and health 13 (2), 201-216 (2021)\n- [20] Palinkas, L.A., Horwitz, S.M., Green, C.A., Wisdom, J.P., Duan, N., Hoagwood, K.: Purposeful sampling for qualitative data collection and analysis in mixed method implementation research. Administration and policy in mental health and mental health services research 42 (5), 533-544 (2015)\n- [21] Jensen, L.S., Kennedy, S.S.: Public ethics, legal accountability, and the new governance. In: Ethics in Public Management, pp. 228-248. Routledge, ??? (2016)\n- [22] Uzougbo, N.S., Ikegwu, C.G., Adewusi, A.O.: Legal accountability and ethical considerations of ai in financial services. GSC Advanced Research and Reviews 19 (2), 130-142 (2024)\n- [23] Novelli, C.: Ai and legal personhood: a theoretical survey (2022)\n- [24] Vantaggiato, F.P., Kassim, H., Connolly, S.: Breaking out of silos: explaining cross-departmental interactions in two european bureaucracies. Journal of European Public Policy 28 (9), 1432-1452 (2021)\n- [25] Sheaff, M.: Constructing accounts of organisational failure: Policy, power and concealment. Critical Social Policy 37 (4), 520-539 (2017)\n- [26] Bento, F., Tagliabue, M., Lorenzo, F.: Organizational silos: A scoping review informed by a behavioral perspective on systems and networks. Societies 10 (3), 56 (2020)\n- [27] Drake, A., Keller, P., Pietropaoli, I., Puri, A., Maniatis, S., Tomlinson, J., Maxwell, J., Fussey, P., Pagliari, C., Smethurst, H., et al. : Legal contestation of artificial intelligence-related decision-making in the united kingdom: reflections for policy. International Review of Law, Computers &amp; Technology 36 (2), 251-285 (2022)\n\n- [28] Lundberg, S.M., Lee, S.-I.: A unified approach to interpreting model predictions. In: Advances in Neural Information Processing Systems, pp. 4765-4774 (2017)\n- [29] Ribeiro, M.T., Singh, S., Guestrin, C.: Why should i trust you? explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135-1144. ACM, ??? (2016)\n- [30] Suresh, H., Guttag, J.: A framework for understanding sources of harm throughout the machine learning life cycle. In: Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, pp. 1-9 (2021)\n- [31] Lee, M.S.A.: Context-conscious fairness in using machine learning to make decisions. AI Matters 5 (2), 23-29 (2019)\n- [32] Singh, J., Singh, A., Khan, A., Gupta, A.: Developing a novel fair-loan-predictor through a multisensitive debiasing pipeline: Dualfair. arXiv preprint arXiv:2110.08944 (2021)\n- [33] Lee, M.S.A., Floridi, L.: Algorithmic fairness in mortgage lending: from absolute conditions to relational trade-offs. Minds and Machines 31 (1), 165-191 (2021)\n- [34] Farayola, M.M., Tal, I., Connolly, R., Saber, T., Bendechache, M.: Ethics and trustworthiness of ai for predicting the risk of recidivism: A systematic literature review. Information 14 (8), 426 (2023)\n- [35] Farayola, M.M., Bendechache, M., Saber, T., Connolly, R., Tal, I.: Enhancing algorithmic fairness: Integrative approaches and multi-objective optimization application in recidivism models. In: Proceedings of the 19th International Conference on Availability, Reliability and Security, pp. 1-10 (2024)\n- [36] Floridi, L.: Ai and its new winter: From myths to realities. Philosophy &amp; Technology 33 , 1-3 (2020)\n- [37] ISO/IEC 27001:2022 Information Technology - Security Techniques - Information Security Management Systems - Requirements. International Organization for Standardization. Available at: https://www.iso.org/standard/82875.html\n- [38] Jobin, A., Ienca, M., Vayena, E.: The global landscape of ai ethics guidelines. Nature machine intelligence 1 (9), 389-399 (2019)\n- [39] DÂ´ Ä±az-RodrÂ´ Ä±guez, N., Del Ser, J., Coeckelbergh, M., Prado, M.L., Herrera-Viedma, E., Herrera, F.: Connecting the dots in trustworthy artificial intelligence: From ai principles, ethics, and key requirements to responsible ai systems and regulation. Information Fusion 99 , 101896 (2023)\n- [40] Percy, C., Dragicevic, S., Sarkar, S., Garcez, A.: Accountability in ai: From principles to industryspecific accreditation. AI Communications 34 (3), 181-196 (2021)\n- [41] Ewuga, S.K., Egieya, Z.E., Omotosho, A., Adegbite, A.O.: Iso 27001 in banking: An evaluation of its implementation and effectiveness in enhancing information security. Finance &amp; Accounting Research Journal 5 (12), 405-425 (2023)\n- [42] Kamil, Y., Lund, S., Islam, M.S.: Information security objectives and the output legitimacy of iso/iec 27001: stakeholders' perspective on expectations in private organizations in sweden. Information Systems and e-Business Management 21 (3), 699-722 (2023)\n- [43] Fontrodona, J.: The relation between ethics and innovation. In: Social Innovation: Solutions for a\n\n- Sustainable Future, pp. 23-33. Springer, ??? (2013)\n- [44] Gupta, N., Mujumdar, S., Patel, H., Masuda, S., Panwar, N., Bandyopadhyay, S., Mehta, S., Guttula, S., Afzal, S., Sharma Mittal, R., et al. : Data quality for machine learning tasks. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, pp. 4040-4041 (2021)\n- [45] Rangineni, S.: An analysis of data quality requirements for machine learning development pipelines frameworks. International Journal of Computer Trends and Technology 71 (9), 16-27 (2023)\n- [46] Priestley, M., O'donnell, F., Simperl, E.: A survey of data quality requirements that matter in ml development pipelines. ACM Journal of Data and Information Quality 15 (2), 1-39 (2023)\n- [47] Sambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P., Aroyo, L.M.: 'everyone wants to do the model work, not the data work': Data cascades in high-stakes ai. In: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-15 (2021)\n- [48] Morey, T., Forbath, T., Schoop, A.: Customer data: Designing for transparency and trust. Harvard Business Review 93 (5), 96-105 (2015)\n- [49] Huq, A.Z.: The public trust in data. Geo. LJ 110 , 333 (2021)\n- [50] Whitney, C.D., Norman, J.: Real risks of fake data: Synthetic data, diversity-washing and consent circumvention. In: The 2024 ACM Conference on Fairness, Accountability, and Transparency, pp. 1733-1744 (2024)\n- [51] Jordon, J., Szpruch, L., Houssiau, F., Bottarelli, M., Cherubin, G., Maple, C., Cohen, S.N., Weller, A.: Synthetic data-what, why and how? arXiv preprint arXiv:2205.03257 (2022)\n- [52] Joshi, I., Grimmer, M., Rathgeb, C., Busch, C., Bremond, F., Dantcheva, A.: Synthetic data in human analysis: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024)\n- [53] Delacroix, S., Lawrence, N.D.: Bottom-up data trusts: Disturbing the 'one size fits all'approach to data governance. International data privacy law 9 (4), 236-252 (2019)\n- [54] Werder, K., Ramesh, B., Zhang, R.: Establishing data provenance for responsible artificial intelligence systems. ACM Transactions on Management Information Systems (TMIS) 13 (2), 1-23 (2022)\n- [55] Laine, J., Minkkinen, M., MÂ¨ antymÂ¨ aki, M.: Ethics-based ai auditing: A systematic literature review on conceptualizations of ethical principles and knowledge contributions to stakeholders. Information &amp; Management, 103969 (2024)\n- [56] Solomon, G., Brown, I.: The influence of organisational culture and information security culture on employee compliance behaviour. Journal of Enterprise Information Management 34 (4), 1203-1228 (2021)\n- [57] McLachlan, C.J.: Developing a framework for responsible downsizing through best fit: the importance of regulatory, procedural, communication and employment responsibilities. The InTernaTIonal Journal of human resource managemenT 33 (1), 16-44 (2022)\n- [58] Enqvist, L.: 'human oversight'in the eu artificial intelligence act: what, when and by whom? Law, Innovation and Technology 15 (2), 508-535 (2023)\n- [59] Alon-Barkat, S., Busuioc, M.: Human-ai interactions in public sector decision making:'automation\n\n- bias' and 'selective adherence' to algorithmic advice. Journal of Public Administration Research and Theory 33 (1), 153-169 (2023)\n- [60] Sterz, S., Baum, K., Biewer, S., Hermanns, H., Lauber-RÂ¨ onsberg, A., Meinel, P., Langer, M.: On the quest for effectiveness in human oversight: Interdisciplinary perspectives. In: The 2024 ACM Conference on Fairness, Accountability, and Transparency, pp. 2495-2507 (2024)\n- [61] Cihon, P.: Chilling autonomy: Policy enforcement for human oversight of ai agents. In: 41st International Conference on Machine Learning, Workshop on Generative AI and Law (2024)\n- [62] Hamon, R., Junklewitz, H., Sanchez, I., et al. : Robustness and explainability of artificial intelligence. Publications Office of the European Union 207 , 2020 (2020)\n- [63] Falco, G., Shneiderman, B., Badger, J., Carrier, R., Dahbura, A., Danks, D., Eling, M., Goodloe, A., Gupta, J., Hart, C., et al. : Governing ai safety through independent audits. Nature Machine Intelligence 3 (7), 566-571 (2021)\n- [64] Oseni, A., Moustafa, N., Janicke, H., Liu, P., Tari, Z., Vasilakos, A.: Security and privacy for artificial intelligence: Opportunities and challenges. arXiv preprint arXiv:2102.04661 (2021)\n- [65] Yampolskiy, R.V.: On monitorability of ai. AI and Ethics, 1-19 (2024)\n- [66] Hossain, M.T., Afrin, R., Biswas, M.A.-A.: A review on attacks against artificial intelligence (ai) and their defence image recognition and generation machine learning, artificial intelligence. Control Systems and Optimization Letters 2 (1), 52-59 (2024)\n- [67] Kong, Z., Xue, J., Wang, Y., Huang, L., Niu, Z., Li, F.: A survey on adversarial attack in the age of artificial intelligence. Wireless Communications and Mobile Computing 2021 (1), 4907754 (2021)\n- [68] Lee, M.S.A.: Context-conscious fairness in using machine learning to make decisions. AI Matters 5 (2), 23-29 (2019)\n- [69] Almeida, P.G.R., Santos, C.D., Farias, J.S.: Artificial intelligence regulation: a framework for governance. Ethics and Information Technology 23 (3), 505-525 (2021)\n- [70] Ojewale, V., Steed, R., Vecchione, B., Birhane, A., Raji, I.D.: Towards ai accountability infrastructure: Gaps and opportunities in ai audit tooling. arXiv preprint arXiv:2402.17861 (2024)\n- [71] Park, D.H., Cho, E., Lim, Y.: A tough balancing act-the evolving ai governance in korea. East Asian Science, Technology and Society: An International Journal 18 (2), 135-154 (2024)\n- [72] Shang, Y., Zhou, S., Zhuang, D., Ë™ Zywioglyph[suppress] lek, J., Dincer, H.: The impact of artificial intelligence application on enterprise environmental performance: Evidence from microenterprises. Gondwana Research 131 , 181-195 (2024)\n- [73] Nishant, R., Kennedy, M., Corbett, J.: Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda. International Journal of Information Management 53 , 102104 (2020)\n- [74] Wang, L., Chen, Q., Dong, Z., Cheng, L.: The role of industrial intelligence in peaking carbon emissions in china. Technological Forecasting and Social Change 199 , 123005 (2024)\n- [75] Spelda, P., Stritecky, V.: The future of human-artificial intelligence nexus and its environmental\n\ncosts. Futures 117 , 102531 (2020)\n\n- [76] Dhar, P.: The carbon impact of artificial intelligence. Nature Machine Intelligence 2 , 423-425 (2020) https://doi.org/10.1038/s42256-020-0219-9\n- [77] Amershi, B.: Culture, the process of knowledge, perception of the world and emergence of ai. AI &amp; SOCIETY 35 (2), 417-430 (2020)\n- [78] Joyce, K., Smith-Doerr, L., Alegria, S., Bell, S., Cruz, T., Hoffman, S.G., Noble, S.U., Shestakofsky, B.: Toward a sociology of artificial intelligence: A call for research on inequalities and structural change. Socius 7 , 2378023121999581 (2021)\n- [79] PiquÂ´ e, J.M., Berbegal-Mirabent, J., Etzkowitz, H.: The role of universities in shaping the evolution of silicon valley's ecosystem of innovation. Triple Helix 7 (2-3), 277-321 (2020)\n- [80] KÂ¨ ose, U.: Are we safe enough in the future of artificial intelligence? a discussion on machine ethics and artificial intelligence safety. BRAIN. Broad Research in Artificial Intelligence and Neuroscience 9 (2), 184-197 (2018)\n- [81] Bamsey, O., Montasari, R.: The role of the internet in radicalisation to violent extremism. In: Digital Transformation in Policing: The Promise, Perils and Solutions, pp. 119-135. Springer, ??? (2023)\n- [82] Govers, J., Feldman, P., Dant, A., Patros, P.: Down the rabbit hole: Detecting online extremism, radicalisation, and politicised hate speech. ACM Computing Surveys 55 (14s), 1-35 (2023)\n- [83] Golpayegani, D., Hupont, I., Panigutti, C., Pandit, H.J., Schade, S., O'Sullivan, D., Lewis, D.: Ai cards: Towards an applied framework for machine-readable ai and risk documentation inspired by the eu ai act. arXiv preprint arXiv:2406.18211 (2024)", "fetched_at_utc": "2026-02-09T14:37:33Z", "sha256": "9148829b9ca7bd2749c6703f54d7b5fb35b9251d86d70e41c1bb4916a5539ea3", "meta": {"file_name": "Trust and Transparency in AI - Industry Voices on Data, Ethics and Compliance.pdf", "file_size": 454483, "mtime": 1770643245, "docling_errors": []}}
{"doc_id": "pdf-pdfs-u-s-ai-law-policy-explained-oliver-patel-f7bc9f242e09", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\U.S. AI Law & Policy Explained - Oliver Patel.pdf", "title": "U.S. AI Law & Policy Explained - Oliver Patel", "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable, and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement, and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nA lot has changed with respect to U.S. AI policy in recent months; and it can be hard for AI governance professionals to keep up.\n\nTo help navigate these recent shifts, this article provides a comprehensive, up-to-date, and accessible overview of U.S. federal AI law and policy.\n\nIt does not cover U.S. state AI laws and initiatives, which will be the focus of next week's edition of Enterprise AI Governance.\n\nDespite lacking comprehensive EU-style regulation, the U.S. does have several important AI laws. In fact, there have been dozens of federal laws, regulations, and initiatives on AI, many of which will be discussed below.\n\nAside from President Biden's (ultimately unsuccessful) attempts to initiate private sector AI regulation, the U.S. government's core focus in recent years has been on maintaining and promoting U.S. AI leadership, restricting the export of AI-related technologies, and encouraging responsible and innovative federal government use of AI.\n\nThe Trump administration, which is primarily concerned with strengthening 'U.S. global AI dominance', is pivoting away from the Biden administration's AI governance and safety agenda. However, this does not mean that AI governance is completely off the menu. Its importance was stressed in a recent memorandum published by the White House's Office of Management and Budget, which described effective AI governance as 'key to accelerated innovation'.\n\n## This overview covers:\n\n- âœ… U.S. global AI leadership and the race with China\n- âœ… Biden's AI governance agenda\n- âœ… Trump's agenda: what 'America First' means for AI\n- âœ… AI export controls and investment restrictions\n- âœ… Federal government use and acquisition of AI\n- âœ… NIST and the U.S. AI Safety Institute\n- âœ… What's coming next?\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week.\n\n## U.S. global AI leadership and the race with China\n\nThe U.S. is the undisputed global AI leader, by nearly every metric. The global AI industry is dominated by U.S. companies, AI models, and hardware. Here are some stats from Stanford's 2025 AI Index Report, to illustrate the point:\n\n- In 2024, U.S. private investment in AI was $109 billion. In contrast, it was $9.3 billion in China and $4.5 billion in the UK.\n- U.S. organisations produced 40 'notable' AI models in 2024, significantly more than China's 15 and Europe's 3.\n\nHowever, in some areas, China is quickly catching up and the U.S. is taking nothing for granted. For example, although U.S. AI model production output is higher, China's advanced AI models are getting closer to U.S. models in terms of quality ( see chart below ).\n\n<!-- image -->\n\nSource: Stanford AI Index Report 2025 [original]\n\nDeepSeek recently demonstrated that it can develop AI models, which have similar performance capabilities to leading U.S. models, at a fraction of the cost. This prompted a sell-off in U.S. tech stocks, with the S&amp;P 500 falling 1.5% on the day DeepSeek released its open-source R1 model.\n\nFurthermore, China is charging ahead in AI talent, research, and patent filing. Increasing numbers of 'top-tier' AI researchers originate from China ( see chart below ), with the share originating from the U.S. declining in recent years. Also, 300,510 AIrelated patents were filed in China in 2024, compared with 67,773 in the U.S.\n\n<!-- image -->\n\nImage source: Information Technology &amp; Innovation Foundation, 2025 [original]\n\nThese trends explain one of the core drivers behind much of the U.S. AI policy agenda in recent years, from AI export controls and investment restrictions, to substantial support for AI infrastructure funding.\n\nAlthough there is no comprehensive federal AI law, like the EU AI Act, there have been a number of federal laws and initiatives, across the past few administrations, which seek to maintain and strengthen the U.S. position of global leadership.\n\nSome of the most relevant laws include:\n\n## Executive Order 14141: Advancing United States Leadership in AI Infrastructure\n\n- This Executive Order was signed by President Biden in January 2025. At the time of writing, it has not been revoked by President Trump.\n- The purpose of this Executive Order is to promote and encourage domestic AI infrastructure development, to 'protect U.S. national security' and 'advance U.S. economic competitiveness'. This includes using federal sites to build data centres for AI and prioritising clean energy techniques.\n\nCHIPS and Science Act of 2022\n\n- Enacted in August 2022, the CHIPS Act ('Creating Helpful Incentives to Produce Semiconductors') was a key pillar of Biden's AI policy.\n- The headline impact of this Act was to authorise and release approximately $280 billion in spending on the hardware components and infrastructure most critical for AI development.\n\n## National AI Initiative Act of 2020\n\n- This law was enacted in January 2021 as part of the National Defense Authorization Act (NDAA) for Fiscal Year 2021.\n- It provided over $6 billion in funding for AI R&amp;D, education, and standards development, with the ultimate goal of strengthening U.S. AI leadership. This included a mandate which led to NIST developing the NIST AI Risk Management Framework ( discussed below ).\n- The Act also established the National AI Advisory Committee, a high-level group of experts which advise the President on AI policy matters.\n\n## Executive Order 13859: Maintaining American Leadership in AI\n\n- This Executive Order was signed by President Trump in February 2019. It remains in force today.\n\n- The purpose of this Executive Order is to promote investment and use of AI across the federal government, as well as to 'facilitate AI R&amp;D' and the development of 'breakthrough technology'.\n\n## Biden's AI governance agenda\n\nDuring the Biden administration, AI governance and safety was a top policy priority. Although this appeared to signal the beginning of a shift away from the historically free market approach to technology regulation adopted by previous U.S. administrations, it did not last for too long.\n\nPresident Biden attempted to balance promoting U.S. global AI leadership with upholding civil liberties and protecting citizens from unfair and harmful practices. As described throughout this article, various federal initiatives designed to strengthen the U.S.' global position-such as on AI export controls, investment restrictions, and AI infrastructure and manufacturing spending-were complemented with initial efforts to regulate private sector AI activities and promote responsible AI.\n\nThe Blueprint for an AI Bill of Rights, for example, was developed by the White House Office of Science and Technology Policy. It outlined Biden's AI policy vision. The\n\nBlueprint was defined by five core principles for AI development and use: i) Safe &amp; Effective Systems, ii) Algorithmic Discrimination Protections, iii) Data Privacy, iv) Notice &amp; Explanation and v) Human alternatives &amp; Fallback.\n\nThe Blueprint argued that AI systems used in healthcare have 'proven unsafe, ineffective, or biased' and algorithms used in recruitment and credit scoring 'reflect and reproduce existing unwanted inequities or embed new harmful bias and discrimination'.\n\nPresident Biden also secured 'Voluntary Commitments' from 15 leading AI companies. These included conducting AI model security testing and sharing and publishing information on AI safety. The core purpose of these commitments was to ensure that advanced AI models were safe before being released.\n\nBuilding on these initiatives, Executive Order 14110: Safe, Secure, and Trustworthy Development and Use of AI was signed by President Biden in October 2023. This represented the most comprehensive U.S. federal AI governance initiative to date. It mandated a major programme of work, entailing over 100 specific actions across over 50 federal entities.\n\nTangible resulting actions included the establishment of the U.S. AI Safety Institute and the publication of various NIST AI safety standards, guidelines, and toolkits ( discussed below ). Also, developers of the most powerful AI models were obliged to perform\n\nsafety and security testing, and report results back to the U.S. government. However, this AI model evaluation regime was never fully operationalised.\n\n## Trump's agenda: what 'America First' means for AI\n\nPresident Trump's 'America First' mantra is not just about tariffs, defence spending, and immigration; it is also relevant for AI.\n\nThe AI policy ambition of Trump's second term is to strengthen U.S. global AI leadership and dominance, promote AI innovation, and advance deregulation.\n\nTwo decisive actions were taken by President Trump within days of his second term commencing.\n\nOn his first day in office, 20 January 2025, President Trump signed Executive Order 14148: Initial Rescissions of Harmful Executive Orders and Actions .\n\nThe purpose of this was simple: to revoke dozens of Executive Orders and Presidential Memorandums issued by President Biden. This included revocation of Executive Order 14110: Safe, Secure, and Trustworthy Development and Use of AI , which was the cornerstone of Biden's AI governance agenda.\n\nThe second decisive move came 3 days later, when President Trump signed Executive Order 14179: Removing Barriers to American Leadership in AI. Doubling down on the revocation of Biden's AI Executive Order, this announcement deemed the previous administration's wider AI policy agenda as a 'barrier to American AI innovation'.\n\nThe stated policy of the new administration is to 'sustain and enhance America's global AI dominance in order to promote human flourishing, economic competitiveness, and national security'.\n\nTrump's Executive Order mandates formulation of an 'AI Action Plan', by July 2025, which can achieve this policy objective. Top U.S. officials are now working on this.\n\nAs part of this, the federal government will run an exercise to identify, halt, and shut down any AI-related government activities or initiatives which are deemed as contrary to achieving the policy objective stated above. This may include AI governance and safety related initiatives which were pursued following the mandate from Biden's AI Executive Order.\n\nThe development of the AI Action Plan has received significant interest, with the public consultation (which has now closed) receiving 8,755 comments in under 2 months.\n\nDespite the policy shift, it is important to note that the Trump administration has not dismantled Biden's entire AI policy agenda, nor has it abandoned the concept of AI governance and risk management.\n\nFor example, much of what was previously implemented on AI-related export controls and investment restrictions, as well as Biden's Executive Order on AI Infrastructure, remains in force. Furthermore, both of the recent memoranda on federal agency use and acquisition of AI, published by the Office of Management and Budget (OMB), emphasise the importance of responsible AI adoption and sound AI governance and risk management practices.\n\nWhat is clear, however, is that the Trump administration has no intention to impose any major AI governance related regulations or restrictions on private sector AI development and deployment.\n\n## AI export controls and investment restrictions\n\nOne potential point of relative harmony between the Biden and Trump administrations is the stance on AI-related export controls and investment restrictions.\n\nThe U.S. deems the development of AI in 'countries of concern' as a 'national security threat'.\n\nConcerted attempts to control how, where, and at what pace AI capabilities are developed has become a fundamental element of U.S. federal AI policy and the broader mission to sustain U.S. leadership in this foundational technology.\n\nVarious laws and regulations were passed during Biden's presidency which significantly restrict which countries U.S. advanced AI computing chips and AI model weights can be exported to, as well as which countries' AI industries U.S. persons can invest in.\n\nThe key laws are summarised below:\n\n## Framework for AI Diffusion\n\n- An interim final rule issued by the U.S. Department of Commerce's Bureau of Industry and Security (BIS). This is the most comprehensive U.S. regulation restricting the export of AI-related technologies.\n- It became effective on 13 January 2025, but most requirements are not applicable until 15 May 2025, when the consultation period closes.\n- The purpose of this rule is twofold: to ensure that a) model weights of the most advanced 'closed' (i.e., not open-source) U.S. AI models are only stored outside of", "fetched_at_utc": "2026-02-09T14:37:58Z", "sha256": "f7bc9f242e096b8133aa39f45aa33f2d370bde5512c0bfd306c0f36330c1cb32", "meta": {"file_name": "U.S. AI Law & Policy Explained - Oliver Patel.pdf", "file_size": 981207, "mtime": 1767775345, "docling_errors": []}}
{"doc_id": "pdf-pdfs-what-is-president-trump-s-ai-policy-by-oliver-patel-3b1b363b61c0", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\What is President Trump_s AI policy - by Oliver Patel.pdf", "title": "What is President Trump_s AI policy - by Oliver Patel", "text": "<!-- image -->\n\nHey\n\nðŸ‘‹\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nOn Thursday 11 December 2025, President Trump issued an Executive Order on Ensuring a National Policy Framework for AI. This represents the administration's latest attempt at blocking and constraining how U.S. states regulate AI. This article summarises the new Executive Order, situates it in the wider context of Trump's AI policy, assesses the challenges the administration faces in preempting state AI laws, and reflects on what companies should do next.\n\nFor a detailed, up-to-date, and visual guide to U.S. AI law and policy (covering the federal and state levels), as well as U.S, China, and EU comparison charts, sign up to secure a 25% discount for my forthcoming book, Fundamentals of AI Governance (2026).\n\n## What just happened?\n\nOn 11 December 2025, President Trump issued a new Executive Order that attempts to block states from enforcing existing AI regulations and deter states from enacting new AI laws.\n\nJust five months after the Senate rejected the proposed 10-year moratorium on state AI laws by 99 votes to 1, the administration is having another bite of the cherry-this\n\ntime through litigation, funding restrictions, and federal preemption, potentially via new legislation.\n\nHowever, it is important to note that the latest development is an Executive Order (i.e., a presidential directive), not legislation. Meaningfully preempting state laws in this way would require legislation, which requires approval from both the House of Representatives and the Senate. It does not appear that substantive political changes have occurred in the past few months to render this more likely than it was back in the summer.\n\nNonetheless, this latest Executive Order is a powerful signal of intent that highlights the administration is digging in on this particular issue, despite the large-scale opposition to the previous 10-year moratorium proposal.\n\nIndeed, the administration has repeatedly stated that its AI policy objective is to 'sustain and enhance America's global AI dominance'. State AI laws have been in the firing line, as part of the wider focus on pursuing deregulation as a means to propel the U.S. as the world's dominant AI power.\n\nPreemption is a U.S. constitutional principle whereby federal law (i.e., law passed by Congress) takes precedence over state law when there are conflicts between the two. When a federal law 'preempts' a state law, the state law is effectively nullified.\n\nHowever, the controversy regarding state AI law preemption is partly due to the fact that there is no comprehensive federal AI law. And the federal AI laws that are on the books cover narrower domains such as strengthening the U.S. AI industry, support and funding for AI research and infrastructure, and export controls. Where state AI laws focus on responsible AI topics like transparency or bias, there is arguably a lack of federal law to preempt these laws.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n## What is the new Executive Order on AI?\n\nPresident Trump signed the Executive Order: Ensuring a National Policy Framework for Artificial Intelligence on Thursday 11 December 2025,\n\nAn Executive Order is a directive issued by the President to federal agencies and executive branch officials. It is issued by the President unilaterally, does not require Congressional approval and cannot, by itself, override state laws.\n\nThe core argument Trump presents in this Executive Order is that the patchwork of many state AI laws creates compliance burdens and administrative complexity that could undermine U.S. competitiveness. Having 50 different AI regulatory regimes, the Executive Order argues, ' makes compliance challenging, particularly for startups '.\n\nColorado's AI law-Consumer Protections for AI (SB24-205), effective date 30 June 2026-is singled out for criticism, with the Executive Order claiming that such laws tackling 'algorithmic discrimination' may force AI models to produce false results in order to avoid differential treatment of protected groups.\n\nThe declared policy of the administration is to establish a 'minimally burdensome national standard' for AI, as opposed to '50 discordant State ones'. To achieve this, the Executive Order directs a multi-pronged set of actions and broader strategy for the U.S. federal government to pursue:\n\n- First, the Attorney General must establish an AI Litigation Task Force within 30 days (of the Executive Order), with the goal of challenging state AI laws in court. The focus will be on identifying state laws that are inconsistent with the U.S. AI policy of 'sustaining and advancing U.S. global dominance in AI'. The grounds for legal challenge could include 'unconstitutional regulation' of interstate commerce or preemption by existing federal regulations.\n\n- Second, the Secretary of Commerce, in consultation with other government leaders, must publish an evaluation of existing state AI laws within 90 days (of the Executive Order), identifying 'onerous' laws that conflict with the U.S. AI policy objective mentioned above. This evaluation must focus on laws that 'require AI models to alter truthful outputs' or that compel organisations to 'disclose information in a manner that would violate the First Amendment'. This is significant, as it explicitly targets laws relating to AI transparency and bias mitigation, two core threads throughout many existing state AI laws.\n- Third, the Order imposes funding restrictions on states that are deemed to have 'onerous AI laws'. States identified through the evaluation referenced above will be ineligible for certain categories of federal funding under the Broadband Equity Access and Deployment (BEAD) Program. U.S. government executive departments and agencies are also directed to assess whether they can condition discretionary grants on states agreeing not to enforce their existing AI laws and/or not enacting new AI laws. This could result in government agencies withholding certain types of grant funding from states.\n- Fourth, the Federal Communications Commission (FCC), the agency that regulates U.S. communications (e.g., broadcasting, internet, and telecommunications), must initiate work to determine whether to adopt a federal reporting and disclosure standard for AI models that would preempt conflicting state laws. It is important to note that the Executive Order requires the FCC to 'initiate a proceeding'; it\n\n- does not require the FCC to adopt such a standard for AI transparency. However, the implied thinking is that if there is a national standard, this could be argued to supersede state AI laws covering AI reporting and disclosure.\n- Fifth, the FTC must issue a policy statement explaining when state laws requiring the alteration of the 'truthful outputs' are preempted by federal prohibitions on deceptive practices, which largely stem from the FTC Act that prohibits 'unfair and deceptive practices'. Again, the implication is that such guidance would strengthen the case for existing federal laws superseding state AI laws in this domain too.\n- Finally, the administration will prepare a legislative proposal for a uniform federal AI framework that preempts state laws. However, the proposal would be narrow and would not seek to preempt state laws covering any of the following policy areas:\n- child safety;\n- AI compute and data centre infrastructure;\n- state government procurement and use of AI; and\n- other topics to be determined.\n\nThis is significant as it highlights the policy areas which the administration deem to be legitimate domains of autonomous state AI lawmaking, even if the result is regulatory\n\n## divergence within the U.S.\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n## How did Trump attempt to block state AI laws previously?\n\nThis latest Executive Order is not the administration's first attempt to constrain state AI laws. Back in May 2025, the administration proposed adding a 10-year moratorium on state AI laws enforcement to Trump's flagship domestic policy and taxation bill, the 'One Big Beautiful Bill Act' (H.R.1).\n\nI covered the 10-year moratorium in detail in a previous edition of Enterprise AI Governance: Unpacking the 10-year Moratorium on U.S. State AI Laws . Here is a summary of that article.\n\nThe proposed moratorium was designed to prevent U.S. states from being able to enforce 'any law or regulation limiting, restricting, or otherwise regulating AI models, AI systems, or automated decision systems' for a period of 10 years. Although this\n\nwould not technically have prevented states from introducing and passing new AI laws, the broad restriction on enforcement would have rendered doing so pointless.\n\nThis was an attempt by the federal government to preempt state AI laws by blocking states from enforcing laws they had already passed, as well as any new laws they might pass in the future. The stated goal was to halt the 'proliferation of a complex and fragmented patchwork of state AI laws', in support of AI innovation across the country.\n\nThe House of Representatives voted to pass the state AI law moratorium by a narrow margin, largely along party lines, with 215 in favour and 214 against. However, following this, the moratorium was decisively rejected. In July 2025, the Senate voted 99 to 1 to remove it from the bill. This followed a significant bipartisan campaign against the provision. A June 2025 letter, signed by 260 state lawmakers, stated that 'states are laboratories of democracy accountable to their citizens and must maintain the flexibility to respond to new digital concerns'. Several Republican state governors also campaigned against the proposal.\n\nThe main reason the moratorium proved so controversial was not really about AI. The case (against it) centred on the philosophical objection to the federal government constraining states in this way, particularly given that there is no comprehensive federal AI law to take precedence. The argument was that the states' hands were being\n\ntied, without an alternative federal framework on the table. The fundamentals of the situation do not appear to have meaningfully changed in the months that have passed.\n\nHowever, the December 2025 Executive Order represents a pivot in strategy. Rather than seeking blanket preemption through legislation-which requires Congressional approval-the administration is now pursuing litigation, agency action, and funding leverage. These approaches can be initiated by the executive branch alone. However, as noted above, the President's legal authority to actually preempt state laws remains limited without legislation. The administration may also pursue new legislation, but this will likely face similar challenges in Congress.\n\n## What AI laws do states have and will Trump succeed in blocking them?\n\nIn lieu of comprehensive federal AI law, dozens of U.S. states have enacted AI-related laws. 131 such laws were passed between 2016 and 2024, and over 700 AI-related bills were proposed in 2024 alone. The states with the most AI-related laws are California, Colorado, Maryland, Utah, and Virginia. California has been particularly active, enacting dozens of laws that regulate AI in different ways. These laws cover themes including fairness and accountability, transparency, data privacy, deepfakes, and government use of AI.\n\nHowever, the administration faces an uphill task in its efforts to block the enforcement and enactment of these laws. Now that the moratorium has been rejected by the Senate due to concerns regarding preemption and the ways in which the federal government can constrain the states, it is fair to argue that subsequent federal legislative proposals will be met with similar levels of scrutiny and controversy.\n\n## What other AI policy actions has this administration taken?\n\nThe new Executive Order sits within a broader pro-business and pro-innovation AI policy agenda.\n\nTrump has pivoted away from the Biden administration's AI governance and safety agenda. Promoting U.S. AI leadership-which has always been a core federal AI policy objective-now takes centre stage. One of the first actions taken by Trump at the start of this second term, in January 2025, was to revoke various Biden-era executive orders, including the flagship Executive Order on Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\n\nIn July 2025, the administration published Winning the Race: America's AI Action Plan , which outlines how the U.S. can achieve and maintain 'unquestioned and\n\nunchallenged global technological dominance' in AI.\n\nI also covered the administration's AI Action Plan in a previous edition of Enterprise AI Governance: What America's AI Action Plan Means for AI Governance . A summary of that explainer piece is provided below.\n\nThe AI Action Plan contains dozens of policy recommendations across three pillars:\n\n- Pillar 1. Accelerate AI Innovation\n- Pillar 2. Build American AI Infrastructure\n- Pillar 3. Lead in International AI Diplomacy and Security\n\nNotably, under Pillar 1, the AI Action Plan recommended withholding funding for AIrelated initiatives from states with 'burdensome AI regulations'. This core idea has now been operationalised through the new Executive Order and its funding restrictions. Moreover, the rejection of the 10-year moratorium by the Senate did not extinguish the underlying policy objective; rather, it has been repackaged and is now pursued through alternative means.\n\nThe AI Action Plan is significant because it directly links AI regulations with the ability (or lack thereof) of companies to innovate at speed. Put simply, the Trump\n\nadministration believes that AI should not be constrained by regulations and that doing so would impede the U.S. economic and security prospects in a damaging way.\n\nOther recommended policy actions in the AI Action Plan included:\n\n- Taking action to review and remove any existing Federal regulations that impede AI innovation.\n- Ensure the federal government only procures 'unbiased' and 'ideologically neutral' large language models.\n- Fast-track and streamline processes for data centre construction review, approval, and licensing.\n- Advocate for 'pro-innovation' approaches to international AI governance, that reflect 'American values' and shift away from 'burdensome regulations'.\n\nOther notable AI policy measures pursued by this administration include Executive Orders on advancing AI education for American youth (April 2025), accelerating federal permitting for data centre infrastructure (July 2025), preventing 'woke AI' in federal government procurement (July 2025), and promoting the export of the American AI technology stack (July 2025). The TAKE IT DOWN Act, which criminalises the publication of non-consensual intimate deepfakes, was signed into law in May 2025.\n\n## What should companies operating in the U.S. do now?\n\nIf you think AI governance is not relevant for your organisation because the current administration is pro-AI and against restrictive AI regulation, you could be in for a rude awakening if things go wrong.\n\nDeregulation does not mean that AI risks no longer apply to you or that you are not exposed. Indeed, the AI Action Plan itself highlights various AI-related risks that could slow innovation, including interpretability, robustness, and misalignment. Furthermore, all enterprises using generative AI at scale are exposed to a litany of (relatively novel) data-related risks. I outlined these in my article on the PROTECT Framework: Managing Data Risks in the AI Era .\n\nThe PROTECT Framework empowers you to understand, map, and mitigate the most pertinent data risks that are fuelled by widespread adoption of generative AI, covering themes such as public AI tool usage, rogue internal AI projects, opportunistic vendors, and compliance and copyright breaches. These risks cannot be mitigated without a robust approach to AI governance, which serves as a reminder that AI-specific regulatory compliance is not the sole driver for enterprise AI governance.\n\nMoreover, even the White House's Office of Management and Budget (OMB) describes effective AI governance as 'key to accelerated innovation'. Indeed, the AI governance framework that OMB directs federal agencies to implement-covering AI\n\ndevelopment, deployment, procurement, and use-is robust. This suggests that although the U.S. government does not want there to be any regulatory measures getting in the way of U.S. companies' AI activities, it nonetheless recognises that a well-designed and proportionate AI governance framework is important for both risk mitigation and value generation, especially in sensitive domains.\n\nFor U.S. companies, the reality today is the same as it was yesterday. There still exists a complex patchwork of many state AI laws to contend with, as well as federal and state laws that meaningfully regulate or implicate AI in specific ways, such as privacy, copyright, employment, and consumer protection laws. Given the complexity of developing different internal AI governance frameworks for different jurisdictions, I always recommend having a company-wide AI governance framework that promotes and facilitates compliance, risk management, and AI-enablement across all the important jurisdictions you operate in.\n\nThanks for reading! Subscribe below for weekly updates from Enterprise AI Governance.\n\n<!-- image -->\n\n6 Likes", "fetched_at_utc": "2026-02-09T14:38:24Z", "sha256": "3b1b363b61c09264cf8621855135ea3f1daa849fc8af3f7b5301d42b573258f8", "meta": {"file_name": "What is President Trump_s AI policy - by Oliver Patel.pdf", "file_size": 809321, "mtime": 1767775345, "docling_errors": []}}
