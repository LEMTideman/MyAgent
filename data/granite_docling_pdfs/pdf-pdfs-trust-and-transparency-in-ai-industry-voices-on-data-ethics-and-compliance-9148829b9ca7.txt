## Trust and Transparency in AI: Industry Voices on Data, Ethics, and Compliance

Louise McCormack 1* , Diletta Huyskes 2 , Dave Lewis 3 , Malika Bendechache 1

1 ADAPT Research Centre, School of Computer Science, University of Galway, Galway, Ireland.

2

ADAPT Research Centre, Trinity College Dublin, Dublin, Ireland.

University of Milan, Milan, Italy. 3

*Corresponding author(s). E-mail(s): louise.mccormack@adaptcentre.ie; Contributing authors: diletta.huyskes@unimi.it; dave.lewis@adaptcentre.ie; Malika.bendechache@adaptcentre.ie;

## Abstract

The EU Artificial Intelligence (AI) Act directs businesses to assess their AI systems to ensure they are developed in a way that is human-centred and trustworthy. The rapid adoption of AI in the industry has outpaced ethical evaluation frameworks, leading to significant challenges in accountability, governance, data quality, human oversight, technological robustness, and environmental and societal impacts. Through structured interviews with fifteen industry professionals, paired with a literature review conducted on each of the key interview findings, this paper investigates practical approaches and challenges in the development and assessment of Trustworthy AI (TAI). The findings from participants in our study, and the subsequent literature reviews, reveal complications in risk management, compliance and accountability, which are exacerbated by a lack of transparency, unclear regulatory requirements and a rushed implementation of AI. Participants reported concerns that technological robustness and safety could be compromised by model inaccuracies, security vulnerabilities, and an overreliance on AI without proper safeguards in place. Additionally, the negative environmental and societal impacts of AI, including high energy consumption, political radicalisation, loss of culture and reinforcement of social inequalities, are areas of concern. There is a pressing need not just for risk mitigation and TAI evaluation within AI systems but for a wider approach to developing an AI landscape that aligns with the social and cultural values of the countries adopting those technologies.

Keywords: Trustworthy AI, Regulatory Compliance, Data Management, AI Governance, Ethical AI

## 1 Introduction

Artificial Intelligence (AI) is being increasingly adopted by a variety of industries, including essential sectors like healthcare, education, transport,

Note: This work has been accepted for publication in AI and Society .

and climate action, necessitating regulations to ensure the safe, ethical, and effective use of AI systems[1].

In Europe, existing regulations covering these sectors are being supported by the introduction of the Artificial Intelligence Act (AI Act) ([2]) which are further enhanced by the General Product

Safety Regulation ([3]) and the Product Liability Directive ([4]), which both explicitly include aspects of software. The AI Act seeks to enable seamless access of AI products to the single market while protecting health, safety and fundamental rights, as well as the environment and democracy rights.

Preceding the introduction of the AI Act, recent years have seen intense activity in developing principles and guidelines that claim to support trustworthy AI systems, sometimes under the label of ethical or responsible AI ([5]). Where adopted, these were typically voluntary measures, with little external validation and often appealing to subjective or contested forms of behaviour. This also led to accusations of ethics washing ([6]). In contrast, on entering into law in August 2024, the EU's AI Act([2]) states that it is a comprehensive legally binding set of rules for AI systems that integrates Europe's existing legal protections of fundamental rights (based on the European Charter of Fundamental Rights (CFR)([7]) with its existing framework for harmonised product safety certification across the European single market. The requirement for AI products to address the protection of fundamental rights can be considered the practical transposition of many of the concerns addressed by prior Trustworthy AI initiatives into an enforceable legal framework. However, with the AI Act having entered into law in August 2024 and the enforcement of its different provisions being scheduled for phased introduction over a 36 month period[1], it will be several years before concrete enforcement experience and case law becomes evident.

Trustworthy AI (TAI) in this context therefore refers to an ethical, human-centred approach to artificial intelligence as outlined in the EU's Ethical Guidelines for Trustworthy AI ([8]) published in 2019. These guidelines emphasise the importance of developing AI systems that are lawful, ethical, and robust. TAI focuses on ensuring that AI respects human rights, fosters transparency and accountability, and promotes fairness and inclusivity. By adhering to these principles, TAI aims to mitigate risks and enhance the positive impact of AI on society.

The seven principles of Trustworthy AI, as outlined by the EU's Ethical Guidelines for Trustworthy AI, are:

1. Human Agency and Oversight
2. Technical Robustness and Safety
3. Privacy and Data Governance
4. Transparency
5. Diversity, Non-discrimination, and Fairness
6. Societal and Environmental Well-being
7. Accountability

There are several established methods ([9]) and metrics ([10]) to evaluate specific aspects of TAI. However, a significant gap remains in the availability of processes for a comprehensive evaluation of TAI. Existing methods often address isolated components, such as fairness, transparency, or accountability, but fail to provide an integrated framework that covers all ethical and technical dimensions of trustworthy AI systems. This lack of holistic evaluation frameworks presents challenges in ensuring that AI systems align with the full spectrum of TAI principles.

Additionally, TAI evaluation faces multiple challenges, including a lack of standardisation of metrics and methods, reliance on manual questionnaires, the need for use-case-specific evaluation methods, and fragmented AI development processes which cause issues relating to the accountability of evaluation ([9]). Researchers have also found a lack of suitable industry tools, highlighting the impractical level of human effort required to make existing toolkits work to mitigate bias effectively ([11]), and the difficulties in translating real-world use cases into the quantifiable metrics required by these toolkits ([12]). These challenges hinder the practical implementation of comprehensive TAI evaluation frameworks. AI systems are diverse, and there are many challenges associated with assessing them for trustworthiness. To address these challenges, researchers suggest a need for Standard Developing Organisations (SDOs) mandating ethical disclosures and ensuring minimum standards for testing, documentation, and public reporting ([13]). One such standard is ISO/IEC 42001 ([14]), which offers high-level guidance for compliance with the EU AI Act, in a similar way as ISO27001 guides compliance for information security management in line with the European General Data Protection Regulation(GDPR)[15].To conduct empirical studies into industry preparedness for more strongly enforced trustworthy AI measures, we have conducted this study in line with the EU's

precursor framework for Trustworthy AI, which benefits from a degree of existing adoption, familiarity and implementation experience. This paper seeks to understand the challenges and trends that industry professionals face when evaluating AI systems for trustworthiness.

## 2 Research Questions

To guide the investigation into the challenges and trends that industry professionals face when assessing AI systems for trustworthiness, this research aims to answer the following research questions:

(R1:) What are the primary challenges industries face regarding data acquisition, quality, preparation, and provenance in AI systems, and how do these challenges influence the trustworthiness of their AI-based solutions?

Data is a key part of AI systems and relates directly to its trustworthiness. Identifying data challenges in AI systems will help in developing strategies to enhance data management practices, thereby improving the trustworthiness of AI systems.

(R2:) How do industry professionals perceive the seven principles of Trustworthy AI? This research question aims to understand how industry professionals perceive each of the seven principles, including identifying their priorities, challenges and comprehension of each.

(R3:) What current practices and challenges exist in assessing compliance with AI-related standards and regulations (such as ISO27001 and GDPR), and how can organisations improve their processes to foster trustworthy AI systems?

Compliance with standards is essential for TAI, however regulating a fast-paced technology such as AI introduces new complications. To help inform the design of assessments for TAI, this question seeks to understand the current processes for evaluating areas such data security which is relevant to the security of AI systems also.

## 3 Methodology

We invited industry professionals working in AIrelated roles to participate in one-hour structured interviews. These professionals were selected through the research team's existing professional networks to represent a variety of tech industries and job functions. We did not exclude any sectors in our selection, but did target sectors with high AI adoption and focusing on interviewing professionals working across a diversity of applications of AI. The goal of the interviews was to gain insights into the challenges and trends in assessing data use for Trustworthy AI (TAI). Conversations with the 15 professionals were audio recorded, with notes taken during the interviews.

To ensure compliance with ethics guidelines and GDPR regulations, the University of Galway's Ethics Committee approved the outreach approach, which involved sending invitations via email and LinkedIn. Participants provided informed consent by signing consent forms prior to taking part in the research.

The insights from these 15 interviews were compiled by the research team and structured into key thematic sections. Empirical evidence suggests that the majority of themes (over 90%) can typically be identified within the first twelve interviews[16] with proposed methods to evaluate if saturation has been reached[17]. Further research emphasises that sample size decisions are inherently situated and cannot be predetermined by saturation rules[18], finding that quality in reflexive thematic analysis derives not from sample size per se, but from transparent, reflexive, and well-justified analytic practice[19]. In this study, the final sample of 15 participants was sufficient to capture diverse perspectives across contexts, with thematic adequacy and depth achieved during analysis. The sections were shaped by identifying common challenges raised in interviews, and aligning those groups with principles of Trustworthy AI from the EU High-Level Expert Group (HLEG), where interviewees provided relevant findings. Sections were combined or created for findings which did not fit into these principles directly. This approach ensured the groupings were data-driven while reflecting established principles. Further research was conducted to explore each challenge, drawing on the latest literature to expand on these findings.

## 3.1 Participants

Participants came from a variety of industries which is outlined in table 1, with Software as a Service (SAAS) Technology being the most common industry. Participants came from a diverse

set of job functions, with technical functions such as data analytics and IT infrastructure/Cyber security being the leading functions. Participants came from a range of industry levels. All participants were based in Ireland and the UK and were well-experienced in their respective fields. Due to location, the findings reflect perspectives shaped by proximity to the EU AI Act and by local organisational and cultural norms. Although many participants worked for multinational companies, with headquarters in the United States of America, China and Europe, their geographic location likely influenced both regulatory awareness and institutional framing of AI governance. The company sizes varied from 10-50 employees to 100k+ employees. All participants were working in organisations where AI is embedded into core business processes, supported by dedicated infrastructure, governance mechanisms, and ongoing operational use, rather than early-stage or pilot experimentation.

Our sampling strategy deliberately included both technical and non-technical roles to capture a breadth of perspectives across organisational functions. While algorithm engineers, machine learning engineers, and UX researchers were not directly included, our research focus was less on model development and more on governance, compliance, and the organisational embedding of AI. Non-technical roles such as marketing operations and customer support were included to provide additional insights into accountability, user-facing implications, and cross-departmental coordination-critical dimensions of Trustworthy AI implementation. For technically complex issues such as technological robustness or data provenance, findings were interpreted primarily through the lens of participants in data analytics, IT infrastructure, and cyber security roles, whose day-to-day responsibilities directly intersect with these areas. This approach aligns with recommendations in implementation research, where purposeful sampling is used to select participants with relevant knowledge and experience for the study aims, often prioritising variation across roles to capture organisational dynamics rather than technical details of system design [20].

## 4 Detailed Research Findings

This section organises the findings from our interviews into five sections based on the common themes identified by participants. We extracted the insights provided by participants, highlighting the main points they made, and grouped these insights where they were either consistent or contradictory. These insights were then categorised using the trustworthy AI principles from the HLEG as an initial framework to create highlevel thematic groupings. For example, findings related to fairness and bias were generally associated with compliance, so they were placed within a broader category of Accountability, Governance, and Regulatory Compliance. The section titles have been adjusted slightly from the HLEG principles to better align with the specific feedback in each area. Based on the interview results, a comprehensive literature review was conducted for each key finding within the five sections, providing academic context to support the insights gathered. Table 2 outlines these five sections along with their key findings.

Each subsection includes a table summarising the main findings from the interviews, followed by a discussion of the concerns raised, incorporating both interview insights and academic perspectives. Separate summaries highlight the key takeaways from both sources. Given the focus of this paper on industry perspectives, the findings from industry interviews guided the classification and direction of the subsequent literature review.

To ensure a structured and thorough analysis, each section in this chapter follows a consistent approach: first presenting interview findings, then reviewing relevant literature, and finally synthesizing both perspectives in a concluding discussion.

## 4.1 Accountability, Governance, and Regulatory Compliance

This section outlines key challenges and developments in achieving accountability, governance, and regulatory compliance for AI systems. It is divided into three subsections: organisational structure and accountability, which explores internal responsibility gaps; fairness and transparency, which considers biases and the conflict between ethics and business priorities; and challenges with

Table 1 Overview of participants

| ID   | Industry Category       | Company Size   | Job Function                     | Seniority   |
|------|-------------------------|----------------|----------------------------------|-------------|
| P:1  | SaaS Technology         | 5k-15k         | Marketing Operations             | Senior      |
| P:2  | Business Development/BI | 50-500         | Business Development/BI          | Mid-Level   |
| P:3  | SaaS Technology         | 5k-15k         | Product Design                   | Senior      |
| P:4  | Other                   | 40k-100k       | IT Infrastructure/Cyber Security | Mid-Level   |
| P:5  | SaaS Technology         | 1k-5k          | Customer Support                 | Executive   |
| P:6  | SaaS Technology         | 500-1k         | Data Analytics                   | Senior      |
| P:7  | SaaS Technology         | 1k-5k          | IT Infrastructure/Cyber Security | Mid-Level   |
| P:8  | Business Development/BI | 1k-5k          | Business Development/BI          | Mid-Level   |
| P:9  | Social Media            | 40k-100k       | IT Infrastructure/Cyber Security | Senior      |
| P:10 | Other                   | 100k+          | IT Infrastructure/Cyber Security | Mid-Level   |
| P:11 | SaaS Technology         | 10-50          | IT Infrastructure/Cyber Security | Executive   |
| P:12 | SaaS Technology         | 15k-40k        | Data Analytics                   | Senior      |
| P:13 | Social Media            | 40k-100k       | Trust & Safety                   | Mid-Level   |
| P:14 | Other                   | 40k-100k       | IT Infrastructure/Cyber Security | Mid-Level   |
| P:15 | Other                   | 40k-100k       | Data Analytics                   | Senior      |

Table 2 Detailed Findings - Overview of Categories

| Category                                                | Primary Concerns                                                                                                                                 |
|---------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|
| Accountability, Governance, and Regu- latory Compliance | • Organisational Structure & Accountability • Fairness & Transparency • Challenges with Standards and Regulations                                |
| Data Management and Quality in AI Systems               | • Data Quality • Data Provenance, Documentation & Assessment                                                                                     |
| Human Factors in AI Development and Oversight           | • Human Oversight and Accountability in AI Sys- tems • Human Bias & Ethical Implications • People Management & Organisation Structure Challenges |
| Technological Robustness and Safety                     | • Performance, Reliability and Transparency • Security, Risk & Trust Concerns                                                                    |
| Environmental and Societal Impact                       | • Environmental Impact • Societal and Cultural Impact                                                                                            |

standards and regulations, which discusses the difficulties organisations face in keeping pace with evolving legal and assessment frameworks.

Table 3 : Detailed Findings -Accountability, Governance, and Regulatory Compliance

| Category                      |              | Details                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|-------------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Organisational Accountability | Structure    | Challenges: • Siloed Departments and Communication Gaps [P1, P2, P3, P4, P5, P6, P7, P11, P13, P15] • Need for clearer Lines of responsibility [P1, P2, P3, P5, P6, P7, P8, P11, P12, P13] • Having to have blind faith in third-party compliance [P1, P2, P3, P4, P5, P7, P8, P9, P13, P14] • Lack of accountability of companies [P1, P2, P3, P4, P5, P6, P7, P9, P11, P12] Observations: • Formation of AI Councils and Steering Committees [P6, P8, P13] • AI provider/product developer is seen as accountable for the system [P4, P13, P14]                                                                                                                                                                                                                                                                                                                                                                     |
| Fairness &                    | Transparency | Challenges: • Risk of Embedded Biases [P1, P2, P3, P5, P6, P7, P9, P10, P11, P12, P13, P14] • Ethical Implications of AI Decisions are unclear [P1, P2, P3, P7, P8, P11, P13] • Lack of Systematic Bias Testing [P1, P2, P6, P7, P11, P12] • Resource or Time Constraints for Testing and Mitigating Bias [P1, P2, P6, P8, P11, P13] • Explainability of AI Models is a key challenge [P1, P3, P4, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15] • Conflict Between Fairness and Profitability [P2, P5, P6, P7, P9, P13] Observations: • High Reward, Low Risk Business Environment [P2, P6, P7, P9, P13] • Awareness of Biases and Cultural Impacts [P1, P2, P3, P5, P6, P7, P8, P9, P10, P11, P12, P14] • Transparency can increase trust in decisions [P6, P10] • Fairness is not binary. It is measurable on a scale [P2, P3, P11, P13, P14, P15] • Inclusive AI design can lead to wider product innovation [P1] |

## (Continued from previous page)

| Category                                  | Details                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
|-------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Challenges with Standards and Regulations | Challenges: • Confusion around the specific impact of the AI Act. [P1, P5, P11, P12, P13] • Lack of AI-Specific Standards for Trustworthy AI [P1, P4, P6, P7, P8, P9, P11, P12, P13, P14, P15] • Rush to Implement AI Without Proper Risk Consideration or Mitigation Processes [P1, P2, P3, P4, P5, P6, P7, P8, P9, P11, P12, P13, P14] • Lack of AI Assessment Tools [P1, P2, P4, P6, P7, P9, P11, P15] • Financial pressure for auditors to sign off compliance [P4, P9] • Over reliance on good faith and assumptions [P1, P2, P3, P4, P5, P6, P7, P8, P9, P13, P14] • Concerns fines are not effective enough or are too low [P2, P3, P5, P7, P9] • ISO standards are not sufficient anymore, real-time, technology- based assessments of AI systems are required [P9, P11, P12] • Need for certifications to be quantifiably validated [P7, P14] Observations: • Belief that regulation is chasing to catch up with technology [P2, P3, P7, P8, P9, P13] • Belief that fines and stopping activities is at least somewhat helpful to police companies [P1, P2, P3, P5, P7, P9, P13] |

## 4.1.1 Organisational Structure &amp; Accountability

This section explores how organisational structure influences accountability in AI governance, including the distribution of responsibilities and the effectiveness of internal processes. Interview participants highlighted challenges related to siloed departments, unclear compliance ownership, and the need for interdepartmental collaboration to ensure AI accountability. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.

## Interview findings:

Many organisations operate in silos, hindering cross-functional collaboration and creating gaps in compliance implementation. Compliance teams were often disconnected from operational functions, limiting their ability to enforce effective oversight. To combat this, some organisations have started to form interdepartmental AI steering committees with representatives from multiple departments. These committees have a goal to lead the AI strategy and compliance within the organisations. In some cases, professionals noted that when it came to AI compliance, departments were not interested in being responsible for this within their department, and felt another department should make sure they are compliant, in particular as it related to maintaining compliance documentation. Compliance was seen as an administrative task that distracted team members from the primary goals of the department.

There were mixed views on who was responsible for the AI systems Trustworthiness. Some participants felt certain that accountability ultimately sat with the AI provider, regardless of where they sourced their data from, or who built various aspects of the AI system. However, some professionals felt that due to the fragmented nature of AI development, there was a blurred line in terms of the overall responsibility of an AI system. There were additional concerns around working with third-party providers, such as who was responsible if data was purchased to train a model which subsequently created issues for the AI system, or how information could be safely shared as part of an inter-organisational development process. Participants felt they were at the mercy of trusting the compliance of third parties, as even going for a site visit, there was no real way to trust their governance completely. Technical participants (particularly in cyber security and data analytics) emphasised system-level risks and vulnerabilities, whereas non-technical participants, such as those in marketing operations or business development, viewed accountability as the responsibility of external providers or compliance departments. These role-based differences highlight the importance of organisational dialogue across professional functions.

## Literature findings:

Researchers argue that mechanisms need to be put in place to allow us to hold decision makers accountable for constitutional and financial consequences including monetary damages or sanctions for companies, as well as broader ethical responsibility ([21]). Legal accountability in AI systems involves assigning responsibility for decisions, managing liability, ensuring transparency, and addressing risks, with evolving frameworks needed to clarify obligations ([22]). AI systems are complex and research highlighted the potential for autonomous decision-making create accountability gaps that traditional legal frameworks struggle to address, reinforcing the need for structured regulatory mechanism ([23]). The findings from the interviews were reiterated in our literature review, which also provided additional insights into the impact of the organisational silos noted by interview participants. These silos are defined as clusters of employees lacking cross-departmental communication, are not necessarily formed due to structural isolation, but instead, interaction patterns are primarily driven by the nature of employees' tasks ([24]). Siloed departments can inhibit the transfer of information, and contribute to problems in organisations by negatively affecting things such as transparency, accountability, and risk management ([25]). These barriers to communication and cooperation can negatively affect organisational efficiency, morale, and innovation. To address these challenges, organisations can adopt a systems approach that emphasises strong leadership, fosters collaboration, and implements structured processes to enhance information flow and teamwork ([26]; [27]).

## Discussion:

Based on the findings of the interviews and literature review, we can conclude that effective organisational structure needs to enable communication between those with in-depth knowledge of functions involved in AI systems, and those responsible for compliance. structure should be designed to ensure that both parties are wellorganised and motivated to collaborate on AI compliance initiatives, fostering alignment and mutual understanding in their efforts to meet regulatory and ethical standards. Stakeholders within departments involved in AI systems such as IT and data departments, should input into the design and maintenance of AI compliance, as those stakeholders have the expertise and knowledge of the systems required to design effective evaluation processes required by the compliance department. Under the EU AI Act, primary responsibility and accountability for an AI system rests with the provider who builds or modifies the system and places it on the market. However, while the legislation specifies this allocation of responsibility, participants in our study reported that accountability processes have not yet been fully adopted or understood within industry, largely due to ongoing uncertainty about the Act's requirements.

## 4.1.2 Fairness and Transparency

This section examines the role of fairness and transparency in AI, considering how they are defined, implemented, and the challenges associated with ensuring clarity and equity in AI decision-making. Interviews revealed concerns about bias in AI models, the lack of systematic fairness testing, and the difficulty in balancing business priorities with ethical considerations. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion summing up both.

## Interview findings:

AI fairness and transparency emerged as key concerns amongst industry professionals who expressed an increased awareness of biases in models due to the data used for training models. Professionals who worked closely with data and machine learning (ML) models were very aware of how a model can figure out sensitive data, even if it is not explicitly told. There were concerns over risks to embedded bias, with participants noting that biased data was coming out of models and being used to train other models. Additionally, there were concerns over a notable lack of systematic testing for bias. Resource and time constraints contribute to a systematic lack of bias testing and mitigation in organisations. Bias testing was typically informal and initiated by individual interest rather than mandated business practice. Some participants noted that concerns around bias either were not taken or would not be taken seriously due to the conflict with business objectives, primarily profitability. It was also pointed out that fairness concerns a human might pick up on, may not be picked up by a model that's being asked to optimise for profit. However, one senior data analyst pointed out that even when it was spotted, often it was not prioritised. They explained that there are business targets for profit in sectors like finance and insurance and that this influences the bias of the model. The participant explained that profit needs to be traded off with bias, and it's usually around feature selection; going on to explain that options are presented to senior leadership, and they make a selection based on their financial targets and projections, with profitability as the primary motivation. Participants noted that efforts to reduce bias for underrepresented groups can introduce trade-offs affecting majority outcomes, while also expressing concern that the growing reliance on synthetic data may amplify bias and undermine fairness and transparency.

A number of participants highlighted the importance of explainability of the AI systems. Two participants noted that introducing transparency in AI systems enabled senior leaders to trust them more and accept AI results quickly. One participant pointed out that while they had internal explainability for the AI models their users directly engaged with, the model explanation was not shared with the users of the platform. Others highlighted a lack of availability of any explainability, even within their organisations. One participant noted that having the transparency between AI provider and client gives the opportunity for the client to input into the model, to ask for additional human in the loop points, and to make changes to the model where needed so that they can trust it. Another participant explained how transparency algorithms

such as SHAP ([28]) and LIME ([29]) are being used to build trust in AI models internally by enabling them to be explained to management, which increased the trust in the model. Participants also called for transparency around what mechanisms were in place to combat bias, where humans were involved in the process, and what fairness metrics were being used.

Most of the participants referred to fairness simply as quantifiable bias, limiting their focus to model behavior or dataset imbalances. In doing this, they overlooked how their own design choices and usage decisions may qualitatively contribute to potential negative impacts or risks. However, one participant noted that AI systems should be designed for inclusivity and diversity. They believed that designing AI systems for inclusivity and diversity had additional benefits to the organisation. They gave positive examples of product innovation, such as automatic doors, which were designed for people with disabilities, and selfopening car boots, which were designed by a female product designer returning from maternity leave. Almost half of the participants referenced that compliance should be viewed as a spectrum as opposed to a binary state. Bias, for example, was not seen as something which could be entirely removed without making ineffective the models which had been trained on biased data. Instead, compliance should be viewed on a usecase basis, with varying metrics and levels of adherence required for each situation.

## Literature findings:

Researchers reported similar findings to the interviews about the relationship between bias and accuracy in models, noting that although in many instances a trade-off must be made between the two, several researchers proposed methods resulting in what they considered an acceptable balance of bias versus accuracy for their use-cases. Bias can arise from human decisions in the design, implementation, and management of AI systems, with its mitigation requiring a continuous and iterative feedback loop ([30]). Lee ([31]) considered bias in the use case of credit lending, proposing considering fairness as variable level of trade-off between competing objectives such as accuracy. Singh et al. ([32]) developed the Alternate World Index (AWI), a universal fairness metric which they proposed a level of trade-off between fairness and accuracy for credit lending. Lee and Floridi ([33]) expanded on the concept of treading fairness not as a binary condition, but instead as a relational trade-off. This transparency allows lenders to justify algorithm choices by balancing financial inclusion and impact on minority groups, while providing regulators and policymakers' with insights to recommend acceptable risk levels. For the use case of recidivism [34], Farayola et al. ([35]). demonstrated a multi-objective optimisation approach to minimizing bias by examining multiple fairness-enhancing techniques across different stages of the ML model and examine their impact on the balance between fairness and accuracy. By introducing techniques such as disparate impact remover, adversarial learning, and equalized odds optimisation, the researchers were able to significantly reduce bias with only a minimal cost to the model's accuracy. McCormack and Bendechache ([10]) classify evaluation criteria for the seven principles of Trustworthy AI originally published by the EU high level expert group, which were also included as non-legally binding guiding principles in the EU AI Act. Their fairness metrics include Group, Individual, Counterfactual, Intersectional, Complex Fairness, and Inclusive Design. They also identify a research gap in AI transparency and propose evaluating systems for model, data, and outcome transparency, emphasizing the importance of visibility into data use and decision-making. The paper stresses the need for standards and processes to address transparency, fairness, and other unknowns in the area of Trustworthy AI. The uncertainty around the future of AI is also echoed by Floridi ([36]) who refers to speculation on the hype of AI as the wild west of 'what if' scenarios. They say this is impacted by media oscillating from an AI utopia to an AI doomsday scenario and propose that AI be viewed as a normal technology rather than a miracle or plague. The paper calls for more philosophical thought into ethics, and consideration around what technology is being developed and its potential impacts. The paper argues that ethical frameworks and principles to underpin AI technology do exist, but states a need for more thought into how AI will fit in the developing human-technology relationship.

## Discussion:

AI is being developed at such a rapid pace that existing AI systems frequently perpetuate bias and discrimination, which can be introduced at various stages of the AI system([30], raising significant concerns about the unknown ethical implications. There is a lack of transparency and established processes for evaluating AI systems' fairness. Moreover, business objectives, particularly profit, are often prioritised over societal and ethical considerations. While technology to enhance transparency and fairness has already been developed, it is not yet sufficiently adopted or audited by regulatory bodies. There is an urgent need for the standardisation and evaluation of fairness and transparency practices in AI systems, particularly concerning the trade-offs between profit and fairness in decision-making processes.

## 4.1.3 Challenges

This section discusses the complexities of regulating AI, the difficulties in assessing compliance, and the evolving landscape of governance frameworks. Interview participants expressed frustration over unclear regulatory expectations, the reactive nature of compliance measures, and the challenges of holding AI developers accountable. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.

## Interview findings:

Professionals found several challenges when it came to regulations and assessment for Trustworthy AI. They noted that companies were currently operating with very little accountability for their AI activities, with several participants believing that regulation was struggling to catch up with technology. Professionals referred to the current AI development environment as the 'Wild West' due to the high ratio of reward versus risk for companies. Several professionals felt this was because stealing data or engaging in unethical AI practices did not currently have sufficient repercussions in place. One participant described an attitude of entitlement within organisations when it came to taking data, even illegally for the sake of AI innovation. Although many professionals felt that fines were somewhat effective in holding businesses accountable, ultimately, they noted that fines are often seen as an acceptable risk by companies. Prohibiting companies from engaging in activities due to breaches was suggested by some professionals as a better way to hold organisations accountable, as it would have more impact on profitability. Participants felt that companies would tend to opt for the bare minimum for legal compliance. When it came to implementation, professionals faced challenges in putting Trustworthy AI practices in place. One issue was a lack of tools available to easily assess AI systems against a trustworthy AI standard. While professionals felt that the AI Act would be significant to organisations, they noted that there was confusion around its implications, along with a lack of resources and clarity such as those currently available for GDPR compliance. Some professionals identified bias but felt unable to act due to anticipated resistance or previous dismissal by leadership.

The industry's reliance on good faith, both in self-regulation and third-party verification, was seen as a key vulnerability in compliance enforcement. Approaches to compliance were typically responsive 'band-aid' solutions designed to meet minimum requirements stipulated by the organisations compliance department. Employees are both pressured and financially incentivised to make sure they assess systems in a way that would pass their compliance processes. They noted that company bonuses were often dependent on achieving company objectives such as compliance certification for their systems. Due to the over-reliance on good faith, doing the bare minimum or even lying to get certification, to achieve this financial reward was both possible and incentivised. This was the case for both internal employees and external auditors, who in some cases would have a prior discussions around compliance for certifications such as ISO27001[37], before the official findings of the report were published. This allowed the company the opportunity to liaise with the auditor before any official report maintained for an audit trail was completed. Paying auditors for certification created perceived conflicts of interest, raising doubts about the objectivity and integrity of compliance processes. The existing processes were largely around maintaining documentation, and less about the quality of the documentation. The ability to make human judgement calls around what documentation was considered sufficient added ambiguity into the audit process. This

problem was echoed by participants who noted a need for certifications which were able to be quantifiably validated. Participants noted that there has to be technology and tools that go hand in hand with design, development, and deployment that collect data in real-time if possible.

## Literature findings:

The literature reports similar concerns around several issues relating to regulations and assessment for trustworthy AI. Jobin et al. ([38]) highlighted concerns about lack of accountability in AI development. They found that companies were able to bypass ethical requirements due to insufficient regulatory enforcement or the use of highlevel soft-policy approaches. They describe uncertainty around how ethical principles should be evaluated, as there was no clear process for enforcing oversight. In particular, they note a gap in the ability of regulators to prioritise conflicting ethical principles. They proposed twenty-two approaches to effecting change in TAI in companies under the four high-level classifications of social engagement, soft policy, economic incentives and regulation and audits. A recent paper by Diaz Rodr´ ıguez et al. ([39]) reports that accountability issues are still prominent, calling for enhanced regulation and oversight mechanisms. The researchers highlighted the importance of developing social and ethical standards that could first be implemented in the design and construction of systems and subsequently used to assess those systems for their conformity. Percy et al. ([40]) highlighted the importance of both external accreditation and internal audits to foster trust and transparency and establish a balanced ecosystem. They argued that reliance solely on high-level ethical principles or external regulations is insufficient for accountability. Using the example of gender bias in gambling, they showed that gender bias can be reduced, albeit with a cost to overall model accuracy. Due to the variability in AI systems, they concluded that industry-specific guidelines were essential for accountability in addition to internal audits and explainability processes. They expressed the concern that without improved governance, companies may continue to bypass ethical responsibilities. The researchers also noted that a variety of tools and techniques have been developed and published to help machine learning developers to implement ethical principles at various stages of the development process, however there is no agreement on how these should be measured or enforced.

Ewuga et al. ([41]) investigated the implementation and effectiveness of the ISO27001, a risk based framework for information security management systems prominently used as part data protection in organisations. Their research, which looked at the banking sector, showed many benefits, including improved risk management, incident response, and cultivating a security-aware culture. It uses a Plan-Do-CheckAct approach which requires continuous monitoring and improvement and includes ethical considerations such as customer consent and balancing data subjects' privacy with system security and transparency. The researchers found that banks have dynamic cyber threats which can often lack evaluation metrics and indicators. In their discussion on addressing future challenges with emerging technologies including AI and ML, the researchers call for the development of specific and stringent cybersecurity standards for these evolving threats. Kamil et al. ([42]) found that there was pressure on employees within organisations to maintain compliance with ISO27001 which sometimes led to them becoming bad actors. The pressure felt by employees could result in unintended risks such as bypassing security measures or exploiting system gaps to pass audits. The certification was seen as somewhat devalued as some clients were not accepting this standard alone but required multiple standards to feel assured of their commitment to information security. Fontrodona et al. ([43]) researched the relationship between innovation and ethics, finding that they are closely interconnected. While innovation is about exploring the possibilities, ethics provides a framework to ensure this progress is aligned with high-level societal principles. The authors noted that innovation that is not rooted in ethical considerations can lead to harmful societal or environmental consequences.

## Discussion:

Both the literature and interviews highlight significant challenges related to accountability in AI systems. Current accountability standards,

which rely on risk-based frameworks and manual processes such as checklists, are regarded as insufficient. There is a clear need for quantifiable standards and metrics that can be continuously monitored throughout the lifecycle of AI systems. Regulators often place undue reliance on good faith, with organisations depending heavily on their own auditors or certification bodies. Given that these auditors and certification companies are paid for their services, they may be financially incentivized to act in the best interest of the business, which can undermine their ability to hold organisations accountable.

Within organisations, employees may face pressure, leading them to take shortcuts or exploit vulnerabilities to pass compliance audits. While tools and techniques exist to integrate ethical principles at various stages of the AI development process, implementing Trusted AI (TAI) is often hindered by conflicting business goals. For example, in sectors such as banking and insurance, machine learning models can be quantitatively assessed for bias, but efforts to reduce bias usually come with a trade-off in accuracy, which negatively impacts profitability and is thus often deprioritised.

This conflict between business objectives and ethical considerations results in organisations setting acceptable levels of bias in machine learning models, leading to profit-driven models rather than fair ones. The lack of ethical considerations in the innovation and development of AI technology is already contributing to harmful societal consequences. To enable effective ethical accountability, independent third-party accreditation bodies must establish Trustworthy AI evaluation criteria tailored to specific use cases at a sector level, incorporating standardised metrics that must be adhered to.

## 4.2 Data Management and Quality 4.2.1 Data Quality

This section addresses the importance of data quality in AI systems, including the factors that impact data reliability and the implications of poor data practices. Interviewees frequently cited issues with inconsistent data entry, vendor misrepresentation of data consent, and the risks posed by low-quality or biased data feeding AI models. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.

## Interview findings:

Insights in this section were drawn primarily from participants in technical roles relating more closely to data quality (data analytics, IT infrastructure, cyber security). While non-technical roles provided complementary perspectives, the technical participants' expertise informed the more detailed aspects of these themes. All fifteen participants highlighted issues with data accuracy and reliability. The most mentioned issue was around inaccurate model outputs resulting from low-quality or misunderstood data. Participants emphasised that poor-quality input data leads directly to unreliable model outputs, highlighting a persistent 'rubbish in, rubbish out' issue. There were several ethical concerns flagged around biased data leading to biased outcomes that reflected social prejudices. Participants flagged challenges around inconsistent data entry practices, either from human-entered data or fields that were not common across departments and subsequently misinterpreted. One participant who worked as a lead data scientist explained the benefits of assigning quality thresholds to input data so it could automatically be flagged for changes affecting quality. They explained that the model performance metrics which were also tracked, took some time to pick up the mistakes, versus it being spotted at source. Despite data quality being more critical to model success, it was frequently deprioritised in favour of advancing model development. Additionally, when the error is flagged at the input data, it makes it easier to fix as the source of the issue is immediately identified.

Concerns were raised by four participants around data acquisition and cleaning practices, specifically around data vendors misrepresenting consent. It was pointed out that vendors will claim to be the originator of the data, when they are not. Additionally, it was mentioned that there was no way to verify if sets of data with and without consent were merged and disguised as fully GDPR-compliant data when they were not. The lack of transparency in data sourcing from brokers could lead to problems for purchasers who are

Table 4 Detailed Findings - Data Management and Quality

| Category                                      | Details                                                                                                                                                                                                                                                                                                                             |
|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Data Quality                                  | Challenges: • Issues with accuracy and trustworthiness [P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15] • Inconsistent Data Entry [P1, P2, P3, P4, P7, P8, P13, P15] • Overreliance, validity or bias concerns around Synthetic Data [P3, P7, P11] • Concerns with data vendors faking consent [P4, P9, P10, P11] |
| Data Provenance, Documenta- tion & Assessment | Challenges: • Challenges in Maintaining Up-to-Date Documentation [P1, P2, P3, P4, P6, P7, P8, P9, P12, P13, P15] Observation: • Benefits to companies with good Data Lineage Tracking [P1, P2, P8, P9, P13, P15]                                                                                                                    |

relying on good faith that the data is being represented truthfully. There were questions about how compliant an AI builder could be if they bought data to train their model, which later turned out to have been misrepresented by the data vendor.

Three participants flagged concerns around the increasing use of synthetic data. The primary concern was that the data would have increasing bias and validity issues affecting the model's performance. This cycle was referred to as a chicken and egg problem.

## Literature findings:

The literature also highlighted that data quality is an essential part of the performance of machine learning models, and that it was also often underprioritised versus model development, along with concerns raised around synthetic data and data provenance. Gupta et al. ([44]) found there are many issues around data collection and processing affecting the reliability and accuracy of models. Researchers highlighted the amount of time spent by data scientists on debugging data, citing the importance of well-defined quality metrics as a way to save time. The paper also proposes a series of quality metrics for various aspects and types of AI systems. Ragineni ([45]) also highlighted the importance of data quality and noted the issues that can occur during the machine learning pipeline. The paper outlines a comprehensive data cleaning process, and also proposes additional data quality dimensions to address challenges around software, bias and legal and ethical aspects. Priestley et al. ([46]) argued that data quality means different things to different people. To ensure that data quality was contextual, they proposed mapping traditional criteria such as accuracy and consistency to specific stages of the ML lifecycle. The paper offers additional practical insights and advice around stakeholder needs, data quality across complex ecosystems and data quality management.

Sambasivan et al. ([47]) noted that model development was often overvalued versus data quality, and this led to negative outcomes. They found a need for cross-domain collaboration, which was a challenging task due to poorly maintained documentation, variability in incentives and insufficient domain expertise, which contributed to project failures. The researchers also noted that data collectors and vendors involved in AI projects can be under-resourced or lack proper training. This can lead to conflicting agendas where field workers may either fabricate data or fail to collect adequate data due to a lack of motivation or understanding of the importance. Additionally, Morey et al. ([48]) found a lack of transparency in the collection of personal data which caused concerns with customers. They found that some organisations were exploiting data for shortterm gain but that was not a sustainable approach due to growing customer demand for control of

their data. Hug ([49]) discussed the manipulation of data from individuals for profit, noting that companies circulate data without transparent consent from individuals. Whitney and Norman ([50]) explore the area of circumventing consent, where companies take data without consent and manipulate it to sidestep actually obtaining consent. The authors detailed the practice of organisations taking personal data and using it to generate synthetic data, obscuring the data's origin.

Jordon et al. ([51]) also found that synthetic data comes with a number of risks, including privacy and bias concerns. While datasets can be augmented to create de-biased synthetic datasets, fixing the inherent bias in the original data, the process comes with its own set of risks to create new problems, which need to be closely monitored. Joshi et al. ([52]) highlight concerns with synthetic data, such as leakage, dataset diversity, and representation fidelity, along with approaches to mitigate these risks.

Delacroix et al. ([53]) highlighted the power imbalance between data controllers and data subjects, in part due to an inability to effectively implement governance measures such as GDPR. They proposed a bottom-up data trust solution in which data subjects could pool their data under a fiduciary structure to offer collective empowerment over their data.

## Discussion:

Both the interviews and literature showed the importance of data quality, in particular, the benefits of having well-defined data quality metrics to ensure consistent model performance. Unethical concerns were raised around current practices of biased and manipulated data leading to negative outcomes. Synthetic data is an area of growing concern for similar reasons including legalities of consent, prejudice and validity concerns. Looking forward, both traditional data quality dimensions and data cleaning processes need to be updated and embraced by industry to support better data governance-ultimately leading to greater transparency and improved performance of AI systems. There are also calls for changes in the structure of data ownership, with a shift towards increasing control for data subjects over how their data is used.

## 4.2.2 Data Provenance

This section considers the significance of data provenance, focusing on how organisations track, document, and verify the origins of data used in AI models. Interviewees highlighted difficulties in maintaining up-to-date documentation, the time wasted tracking down data sources, and the negative impact of poor data lineage on AI reliability. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.

## Interview findings:

Six participants mentioned the benefits of maintaining good data lineage within organisations. Participants noted that time is frequently wasted chasing the source or proper meaning of data. This can include just emails or phone calls to colleagues or, on occasion, actually chasing down the people on the floor who collected the data to understand the meaning and context of various fields. Participants noted that models had been hastily built in companies in the AI rush, which subsequently didn't work due to the model builder not understanding or making false assumptions about the data used to train the model. Additional benefits of good data tracking included ensuring data integrity, compliance, improved transparency and improved trust in the data. The benefits of tracking data through the company, including data flow diagrams and real-time accurate documentation, are also highlighted. Many participants noted that a lot of their answers around data could be found in internal repositories. Despite this, they mentioned chasing additional information about data to assist with their projects. This was often due to documentation either not being complete or being out of date. This was particularly the case in dynamic environments where both the data and the AI models are evolving. There was a fragmentation of responsibilities, with people finding the task of continuously updating documentation to be labour-intensive admin work they did not want to do. It was seen as checklist-type work, which often did not keep pace with product deployments. The fragmented development also created challenges in ensuring consistent data oversight, in particular when there was a lack of integrated systems.

## Literature findings:

The findings in the interviews align with the existing literature on data quality and data provenance in AI systems. Werder et al. ([54])highlighted the importance of data provenance for mitigating bias and making AI systems more responsible. They note that organisations see data provenance as a compliance mandate rather than as part of an organisational commitment to developing responsible AI systems. They found that when organisations developed practices such as the automated ones recommended in their paper, they had better long-term outcomes for the organisation, particularly in data-driven development and AI engineering. Laine et al. [55] also notes the importance of data provenance for mitigating bias and promoting accountability and transparency in AI systems, in particular in relation to decision pipelines. They also note that fragmented documentation with missing information leads to challenges for businesses during fairness and compliance audits. Solomon and Brown [56] found that organisational culture played a significant role in informational security subculture, affecting how employees followed processes such as good data provenance. They found that a top-down approach, including the monitoring of compliance processes and ensuring employees were aware of their accountability in those processes, was important. Additionally, good data provenance processes can offer additional benefits such as also smoothing transitions between employees, in particular in instances such as company downsizing [57].

## Discussion:

Data provenance, when implemented as a bandaid style compliance solution, causes friction and creates problems that cascade throughout the organisation. Examples of this include both labour-intensive tasks around chasing data sources and meaning and the building of AI products that don't work due to data misunderstandings. Businesses that fail to create both good data processes and a culture of data compliance will see increasingly negative organisational outcomes as AI becomes more commonplace. Strong data provenance practices that are rooted in best practices, in particular those that include automation, will offer significant benefits to organisations.

## 4.3 Human Agency and Oversight

This section explores the role of human oversight in AI systems, assessing the balance between automation and human decision-making, as well as the need for accountability. Interviews highlighted concerns about insufficient human oversight, gaps in AI education among professionals, and conflicts of interest in human decisionmaking. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.

## Interview findings:

Participants had concerns there might not be enough humans in the loop in systems. There were concerns around over-automation of systems, in particular without proper monitoring, robustness and safety being built into them. One participant noted that they expect their organisation's tools not to allow them to do anything that isn't compliant, and that compliance should be inherently built into the system, but this was not always the case. There were also concerns that even when things are GDPR compliant, we still didn't know enough about the potential ethical implications. A number of participants felt there is a need for people to be in the AI process of making decisions at mandatory gates. One participant noted that AI could check AI in the future, but at this stage, humans should be monitoring these checks due to AI verification tools not being available. One participant gave the example of pilots who have certain thresholds and points that they are required to take over, noting that this should be the same to ensure the safety of all AI systems. From a usability perspective, one participant noted that people sometimes don't realise they are dealing with an AI, so you need humans to be available when issues arise. Participants also noted that the AI can miss things due to context or just not being advanced enough, and this is something humans are able to pick up on. One participant noted that humans in the loop should not be incentivised to choose profit, stating that if a human is a shareholder, they will not be able to make an unbiased decision between profit and ethics. Two participants pointed out that humans in the loop can introduce their own biases into the system also. There were concerns about the educational training of humans involved in AI systems in areas such

Table 5 Detailed Findings - Human Agency and Oversight

| Category                 | Details                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
|--------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Human Agency & Oversight | Challenges: • Not enough humans-in-the-loop in AI systems [P1, P2, P3, P4, P8, P10, P13, P15] • Risks of over automation [P1, P2, P5, P6, P7, P8, P10, P12, P14, P15] • Need for ongoing monitoring [P1, P5, P9, P11, P12, P13] • Humans need to be able to understand model decisions [P1, P3, P4, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15] • There is a need for human checks at certain points [P1, P3, P4, P5, P8, P13, P14, P15] • Concerns that human-in-the-loop could introduce bias [P11, P12] • Human decisions need accountability [P11, P12, P13] • Organisational structural changes needed for oversight [P1, P2, P11] • Call for mandated oversight roles in companies [P1, P2] • Concerns around education of professionals [P1, P2, P8, P10] Observations: • Humans can help mitigate known and unknown ethical implica- tions in system design or data collection, processing & testing [P1, P4, P5, P6, P7, P10, P15] |

as poor data literacy or professionals having fragmented knowledge of AI systems. One participant commented that issues are arising because a lot of models are being built by engineers who don't understand the full process, how data is collected and processed and how it's going to be used by end users. A second participant noted a similar focus on the model build, stating that data scientists are directed to focus on math and algorithms, leading to a significant gap in education around the data aspects. Another participant highlighted the need for the addition of specific people or tools to ensure compliance with AI systems. A fourth participant commented on the lack of education of employees due to the rush to implement AI in their organisation. They commented that this was happening without educating employees on the risks, including the unknown risks involved, so that they could help mitigate these. A number of participants noted a need for change in organisational structure, with this already starting to happen in many organisations. AI teams or AI councils are being retrospectively set up to look after privacy and governance. Some participants called for new roles or AI compliance departments, such as AI equivalent to the GDPRmandated data protection officer, questioning why this wasn't included in the act. One participant noted that smaller companies who now have access to this powerful technology may not have the resources to ensure compliance with it. There was also a reported disconnect between data controllers and end users. One participant said that instead of replacing departments or roles in companies with AI, the roles should be halved and become AI-assisted with enough human checks in place to mitigate risks. The explainability of models to humans, as discussed in section 4.1.2 in this paper was also flagged as important by participants. One participant highlighted that proper human oversight is only possible when the humans understand what is being done, the input, output and the steps in between. Participants noted that human decisions need accountability. Companies that are incentivised to favour profit over bias need to be made accountable for their decisions. Additionally, humans who could introduce bias into systems interviews, needs to be transparent so that it can be monitored for quality

checks and auditing also. It was noted that monitoring of AI systems would have require ongoing accountability and monitoring, an AI tool that was originally safe to use could be compromised by a bad actor gaming it or introducing bias. One participant described their organisational GDPR implementation as a big project initially, but once it was in place, ongoing monitoring and training were sufficient.

## Literature findings:

The literature also found that human oversight in AI systems must be carefully designed, with particular focus on transparency, ongoing monitoring, and the level of training and motivation of those involved. Enqvist [58], noted that human oversight can be designed in many ways, with many outcome variabilities based on the selection of which processes, which person and which time the process is overseen. They also pointed out that the oversight should be designed with attention given to transparency and the mandates and working conditions of the human oversight to more effectively counterbalance the risks they are trying to mitigate. They highlighted that human monitoring should be ongoing to address risks and biases instead of reactive. The paper also discusses the human-centric approach discussed in laws such as the AI Act, noting that this mandated oversight requirement is being written into law with no precedent for what the human-centric approach applies, and a lot of room left for interpretation by AI providers.

Automation bias leads humans to uncritically rely on automated systems, even when faced with contradictory information, showing that AI can introduce new biases through the way its outputs are interpreted rather than simply eliminating existing ones [59]. Human oversight is a potential safeguard to mitigate some risks in AI systems in the hope that humans may be better at including ethical considerations and adhering to social norms in decision-making processes [60]. However, this paper also noted that humans may come to rely too heavily on the AI outputs and be influenced by them, or else counter them unfairly and introduce their own biases. They noted that any human in the loop would have to be trained with sufficient knowledge about the risk and how to mitigate it, also calling attention to the accountability of having an appropriate person in the loop, stating that effectiveness requires both moral responsibility and fitting intentions. The researchers propose an approach to effective human oversight under three categories: the technical design of the system, individual characteristics of oversight persons, and the environmental circumstances in which oversight occurs.

Researchers discuss what an acceptable standard of care, a norm-based governance with legal implications, would entail for AI systems [61]. They found AI providers need to take reasonable actions and precautions to ensure no resulting harm, tort or regulatory liability, and this includes implementing human oversight. The paper states that human oversight includes both the information to responsibly use an AI agent and control it during operation as opposed to autonomy, which is at the other end of the spectrum. They state that for a use case, such as when a driver should take over driving a vehicle, the appropriate level of oversight versus autonomy must be selected, which reflects the advances in capabilities and safety. They argue that AI systems which pose challenges to effective human oversight could be seen as defective products under the law. The paper states that more research into the humanAI relationship will help determine an appropriate standard of care for specific use cases.

## Discussion:

There is a clear need for sufficient human oversight in order for AI systems to be considered safe and ethical. The legal implications of what an appropriate level of oversight or standard of care might look like is currently unclear. The literature and regulations in this area state that humans providing oversight or operating AI systems should have sufficient training. However, industry participants noted concerns that sufficient training was not in place currently. There is a shared concern in the literature and industry that overautomation and lack of human control could occur either through lack of human oversight or through human decision-making either being influenced or wrongly influencing AI systems. This could result from a lack of transparency, lack of education, or human oversight by individuals with conflicting agendas regarding the ethical integrity of the system. Despite these concerns, human oversight

is seen as integral to ensuring fairness and safety in AI systems. There are concerns that achieving legal compliance with AI standards will not result in meeting ethical standards. Accountability and transparency of human involvement in AI systems involved in high-risk decision-making is seen as essential. Human agency versus human oversight is a balance that must be struck for each use case. It is likely that significant organisational changes in structure will result from the implementation of AI technology with sufficient human involvement.

## 4.4 Technological Robustness and Safety

This section focuses on the technical resilience and security of AI systems, considering issues with performance, reliability, transparency, evolving cyber threats, and the need for new standards and risk mitigation measures to maintain trust and safety.

## 4.4.1 Performance, Reliability and Transparency

This section examines the technical robustness of AI systems, including the factors that influence performance, reliability, and the importance of transparency in AI operations. Interviewees raised concerns about model accuracy, the impact of data quality on performance, and the need for transparency to build trust in AI decisions. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.

## Interview findings:

Insights in this section were drawn primarily from participants in technical roles (data analytics, IT infrastructure, cyber security), whose responsibilities involved direct engagement with system robustness, with non-technical roles providing complementary perspectives. Data accuracy is essential in robust AI systems. Participants commented on the risks of low-quality or poorly structured data, giving examples of how they had gotten inaccurate outputs or poor model performance as a result. Participants highlighted challenges in people not understanding the data, not being data literate, and also having issues chasing back data sources to find out what data really means and where it was acquired so they can use it effectively in models. Participants felt that organisations needed to focus more on model accuracy and robustness. Many participants highlighted the impact of conflicting goals, such as improving bias and having negative outcomes on model performance.

Two participants highlighted the negative impact of hallucinations, which they said was also a result of poor data or, at times, out-of-date data. One participant commented that the quality of the ML model can be compromised when models are fed chunks of data from various sources that conflict with or confuse the model, causing hallucinations. Another participant noted that hallucinations could frequently occur in specific topics that haven't been well-trained on the model, leading to confident hallucination responses to fill the gaps.

The positive benefits of introducing automated techniques to improve transparency was highlighted by participants. One participant described how introducing rule-based dashboards for data quality fields such as completeness, validity, reasonableness, and consistency improved both their model performance and gave organisation leaders confidence. Another participant described how the introduction of model transparency techniques gave confidence to senior leaders in AI decisions. Reproducibility was highlighted as important, with participants noting the importance of being able to do things such as going back and checking data when there are variances in figures and being able to revert to the previous state if something goes wrong in processes that are in place. One participant noted that AI assessments should be repeatable by software in heightened security risk situations. Overreliance on AI systems was also listed as a risk by some participants. One participant explained how employees already expect systems to be fully compliant and not allow them to do anything they shouldn't.

## Literature findings:

The literature shows a number of common themes with research participants' comments in the area of technical robustness and safety. A technical report by the Joint Research Centre and the European Commission's Science and Knowledge Service drew attention to the importance of reproducibility, risks of overreliance on

Table 6 Detailed Findings - Technological Robustness and Safety

| Category                  |                 | Details                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
|---------------------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Performance, Transparency | Reliability and | Challenges: • Issues with model Reliability and Accuracy [P1, P3, P8, P9, P10, P11, P12, P13] • Issues with hallucinations [P3, P14] • Risk of overreliance on AI [P2, P3] Observations: • Automated data and model transparency initiatives improved organisational trust in AI [P6, P15] • Importance of reproducibility [P9, P13, P14]                                                                                                                                                                       |
| Security, Risk &          | Trust Concerns  | Challenges: • Evolving security needs [P1, P7, P11] • Sensitive data leaking into model [P3, P9, P10] • Data Poisoning and Adversarial Attacks [P9, P11, P14] • Increase in volume of attacks [P1, P2, P9, P11] • Need for strong risk mitigation [P1, P3, P4, P9, P10] • Concerns current assessments like ISO27001 were not sufficient for AI [P9, P10] • Found existing security measures sufficient for at least some aspects of AI systems [P3, P4, P5] Observations: • Fear of the unknowns [P5, P7, P11] |

AI, and discussed issues with model reliability and accuracy[62]. The report highlights two key aspects of evaluating an AI system's reliability: performance and vulnerabilities. They explain that poor performance would be a model that cannot perform well in a task that is considered normal for humans, and vulnerabilities would be performance that malfunctions in specific conditions, either unintentionally or through intentional malicious provocation. They discuss how model performance can vary when other features are added, such as introducing interpretability, which requires a trade-off with accuracy. They highlight the importance of reproducibility for AI reliability. The report highlights the importance of external data validation to help avoid overfitting and improve model performance in diverse scenarios. The report highlights the risk of blind trust in AI systems, which can lead to overreliance without sufficient knowledge of the system, which could have negative outcomes like errors or abuse of AI systems. The literature also reports a correlation between transparency and user trust. Transparency is required for building trust in Human-Centred Artificial Intelligence (HCAI), stating that the lack of procedures for investigating incidents, along with the lack of specifications of standards for black-box technology is inhibiting trust in those systems ([63]). This paper also proposes automated real-time risk evaluations to provide transparency for stakeholders. Additionally, increasing either transparency or control over decision-making in AI systems correlates positively with user trust in the system ([10]).

## Discussion:

The insights from the study participants, and the supporting literature emphasise critical role of data accuracy and data management processes in ensuring technical robustness and safety in AI systems. Poor data literacy and lack of transparency or lack of knowledge about AI systems create risks for model performance and hallucinations. The possibility of negative outcomes from overreliance on AI systems was also expressed. Additionally,

ethical challenges between who decides the appropriate trade-off between bias and accuracy were highlighted, with an expressed need for standard practices to be developed to increase transparency. Increasing transparency was shown to increase user trust and AI adoption within organisations, giving confidence to leadership in AI models. Alongside transparency, reproducibility was flagged as essential in ensuring the reliability of AI systems.

## 4.4.2 Security, Risk and Trust Concerns

This section considers the security challenges associated with AI, including emerging risks, vulnerabilities, and the measures required to build trust in AI systems. Interviews revealed growing concerns over adversarial attacks, data leakage, and the inadequacy of existing security frameworks in addressing AI-specific threats. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.

## Interview findings:

Participants noted there were evolving security measures with AI, which some felt wasn't thought out enough, and that security wasn't moving as quickly as the technology, with new classes of security threats emerging all the time. One participant explained that when they create a model and apply it to an environment, it mathematically makes sense, but it must be validated, which is difficult when data sets fluctuate and need to be aggregated. Another participant said that AI tools are so much more powerful than people realise, and they are being developed without technical robustness and safety being built in. Participants noted risks around malicious attacks. One participant noted that processes for data acquisition, cleaning, and transformation are very light in determining whether that data has malicious intent. Another noted the importance of securing not just the model itself but the risks of injecting malware into data and data poisoning. Another noted that AI systems need to be protected from development pipelines, which can be interfered with, similar to supply chain attacks.

Sensitive data leaking through a model was a concern for some participants, particularly one participant who was in the process of incorporating AI into their business intelligence product, which would have sensitive competitor data on it; it was clear that it could not be leaked between clients. Another participant explained that AI can scrub data but that people can convince it to leak that data. Four participants noted that there was an increase in the volume of cyber-attacks. Some noted that this was because as AI lowered the bar to entry for hackers. One participant commented that hackers are no longer required to know an obscure programming language to launch good cyber-attacks; and another participant commented how individuals and companies are using AI to launch an increased volume of attacks. Another reason given by three participants was the increased surface area for attacks, with one participant noting that every time they transmit data, it's a risk for security and another commenting that they open up their models for other APIs and create multiple entry points, making it harder to protect. Data accuracy was cited as important in cyber security by one participant who noted that AI and Large Language models (LLMs) have created a huge new territory in cyber security with new risks.

Risk mitigation was a key concern with participants highlighting the need for proper procedures and techniques in place. One participant said that risk level needs to be evaluated and made relevant to the level of oversight. One participant highlighted the importance of having data breach guidelines and risk mitigation processes in place with the provider to bring back the trust and repair any situation where something went wrong. Internal mistakes from AI builders to other teams were also highlighted as an important area to have proper checks in place. Another participant explained that the key thing for privacy is data minimisation, but with data being so valuable, companies are collecting it without techniques for data privacy being well implemented. They explained that sometimes data privacy is implemented in sections and can later be combined with other data that was purchased. So, in itself, it's anonymous, but when everyone has a piece of that jigsaw, they can put that together. One participant noted the benefits of looking after their own data, explaining they had implemented a very strict system for moving data around the company, which could only be accessed through VPN,

and even then is highly regulated by legal and only on request. Participants highlighted a number of issues with AI assessments. One participant explained that the format of the current ISO standards is not sufficient for evaluating AI systems as they don't assess anything in a mathematically proven way. They also explained that the current ISO27001 standard should not be used for AI as it is a point-in-time standard and framework, and AI is fundamentally not assessable for that control. They said that the standard doesn't address AIspecific threats or threat vectors, that it has not been updated for AI, and cannot be updated in its current form. Another participant explained that the lack of verifiability of systems created trust issues when selecting providers to look after data. Stating that even though they can test their own security measures, the uncertainty generated by back to lack of actual assessments of data centres means that even if they physically go and look at data centres and examine systems, they still have to just trust that it will be secure. Therefore, they cannot assure their own clients their AI systems are secure because they can't actually be assured of vendor security. Three of the fifteen participants commented that existing security measures can serve technical robustness in at least some part of their AI. One participant noted that they have five layers to make sure their internal models work, so that was not something they are worried about as it's one of the easiest ones to secure. Another explained that they have good plans in place for a long time for high availability systems, including backup buildings and risk mitigation processes. A third participant said they hadn't worked in many systems where there were a lot of concerns around cyberattacks. Even in their current role where there's a lot of sensitive data, they felt there's not much cyber risk. Although there was an increased interest in security from customers, their model and security systems hadn't changed. Two of these three participants worked in non-technical roles, and one worked as a cybersecurity analyst.

## Literature findings:

The literature reported concerns about evolving threats and the pace at which security is keeping up. The area of AI-specific security controls to address evolving threats is a very young but active field of research ([62]). This paper also found that

AI systems being developed did not come close to meeting the minimal requirements of safety and security that would be expected from autonomous systems. Evasion attacks, which are attacks on AI models during the testing phase, are the most prevalent type of attacks on machine learning models [64]. Poisoning attacks, which are attacks during the training phase, are less common but can easily be carried out on applications that use data from untrusted sources to train their models. The researchers present a systematic framework for demonstrating AI attack techniques. However, they explain that unknown unknowns pose a significant risk as the field of AI evolves, and further research is needed in this area to help identify these. AI systems can produce outputs beyond human comprehension, making it impossible to predict the possibilities in terms of safety risks ([65]). The importance of continued research to develop more secure AI systems was echoed by [66], who found that repercussions of successful adversarial attacks can cause harm to public safety and privacy. In addition to discussing data poisoning, exploration, evasion and membership inference, the paper highlights the risks of model inversion, which has the capability to reverse engineer the sensitive data used to train the model from AI model outputs. The rise in adversarial attacks in AI was also reported that research into this field is extremely urgent, including investigating strengthening defence techniques for each individual AI application [67].

In the area of AI safety governance, researchers have proposed independent audits of AI systems to address assurance challenges based on three 'AAA' governance principles: Assessments, Audit Trails, and Adherence to jurisdictional requirements ([63]). They also stress the importance of interdisciplinary approaches to governance and propose automated real-time assessments of AI systems to increase transparency and reduce risk. There is a need for semi-automated tools for a comprehensive evaluation of AI systems. [9]. The cost of developing effective governance systems could be high; however, smaller, more agile sectorbased regulators could counter the resource costs associated with the necessary proliferation of regulatory bodies ([63]). The approach to governing AI at an industry sector level was echoed by other

researchers [10], along with calls for the development of context-specific standards [68]; [69]; [70]. Researchers also reported that countries such as South Korea have already issued Trustworthy AI Development Guidebooks at a sector level ([71]. Researchers have pointed to existing audit approaches, such as the audit model used in financial accounting, which is based on the Generally Accepted Accounting Principles (GAAP), safety standards such as ISO-13489:2015 and BS86112016, which includes a risk assessment for the ethical design of robotic systems [63].

## Discussion:

The field of security for AI systems is not keeping pace with the advances in AI technology, with calls for a collaborative, interdisciplinary approach to developing evaluation and risk mitigation processes for AI systems. The level of unknowns in this space is seen as a high risk, particularly in areas which could result in significant negative societal outcomes in the case of security failure. The rapid pace of technological advances has led to evolving threats such as malicious attacks, data leakage, and increased cyber-attacks. The volume increase is in part due to the ability of AI-powered attacks to move quickly, along with a lower barrier to entry for hackers who can launch cyber-attacks using AI without specialist coding knowledge. Security assessments such as ISO27001 are not seen as fit for purpose for AI systems. Both the interviews and literature question the suitability of point-in-time checklist-based certification, noting that it is too manual and slow to implement and update to be considered a fully comprehensive solution for AI security evaluation. There is a clearly established need for AI technology to be evaluated on an ongoing basis through independent audits, context-specific standards, and real-time automated assessments to enhance transparency and mitigate evolving risks.

## 4.5 Environmental and Societal Impact of AI

This section explores the wider impact of AI on the environment and society, highlighting concerns about energy consumption, environmental costs, societal inequalities, political radicalisation, and the growing need for ethical governance and cultural sensitivity in AI development.

## 4.5.1 Environmental Impact

This section explores the environmental implications of AI, focusing on resource consumption, sustainability considerations, and the broader impact of AI infrastructure. Participants voiced concerns about the high energy demands of AI, inefficient data storage practices, and the lack of transparency regarding AI's environmental footprint. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.

## Interview findings:

Seven participants expressed concerns about the high energy consumption of AI data centres and its impact on the environment. Participants worked with AI and were aware of and alarmed by the high energy demands of these facilities, which were described as rivalling or surpassing the consumption of entire countries. Additionally, one participant highlighted major concerns over the extraction of rare metals necessary for GPUs. Two participants called for regulatory measures to mitigate environmental impacts. Suggestions included implementing fines based on energy consumption and enforcing stricter controls to prevent the unnecessary expansion of data centres driven by redundant data generation. One participant gave the example of considering how many pointless screenshots we each have on our phones and pointed out that AI could be generating and storing similarly redundant data at scale without regulations. The inefficiency in data management was also a point of concern for another participant who saw good use cases for AI to help the environment but did not know how to mitigate data management concerns. Two participants highlighted a lack of public awareness and a disconnect between AI usage and the environmental impact. It was pointed out that most people don't associate asking a computer to do something with its impact on the environment. They believed that transparency around the environmental impact of flying gave people choices regarding their travel choices but that the same environmental impact transparency did not exist for AI usage. Another participant pointed out that data centres use more electricity in Ireland than private homes, and with the huge traction AI has gained, this is a real concern. Three participants commented on the

Table 7 Detailed Findings - Environmental and Societal Impact of AI

| Category                     | Details                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Environmental Impact         | Challenges: • High energy consumption of data centres [P1, P2, P4, P6, P9, P11, P13] • Need for fines and regulation on environmental impact of AI [P1, P2] • Concerns About Data Redundancy and Management [P1, P3] • Lack of public awareness & disconnect from environmental impact of AI [P6, P9] Observations: • Variance in views on environmental benefits vs environmental costs [P3, P4, P9]                                   |
| Societal and Cultural Impact | Challenges: • Positive changes to western workforce, with negative impacts in third-world countries [P7, P9, P10] • Significant negative influence on society and political radicali- sation [P2, P3, P9, P14] • Unwanted reinforcement of social inequalities and unrealistic standards [P11, P12, P13] • Loss of culture [P8, P14] • Challenges preventing companies implementing AI with nega- tive societal outcomes [P9, P12, P14] |

benefits of AI versus its impact on the environment. Two participants noted the potential to use AI for the good of the environment, even though it was not being done now. One participant said that none of the main current use cases being prioritised is about helping the environment, but instead, AI is destroying the environment and polluting for sometimes trivial results. The third participant commented that the increased power usage did not concern them currently, as the benefits outweighed their environmental concerns. Literature findings: The literature paints the impact of AI on the environment in different ways. On one side, researchers have cited AI as a solution to create efficiencies in companies that will lower their carbon emissions, but another view reports significant concerns about the impacts of AI systems on the environment. Findings indicate that AI can lower emissions through efficient processes developed through innovation and reduction in resource-intensive labour practices, in particular in big cities, where larger, older, and non-technology-based industries ([72]). AI has the potential to address environmental issues and climate change [73]. However, those researchers noted that historical precedents cautioned that new technologies could lead to negative unforeseen issues for sustainability when risks are not managed properly. There is a reported threshold effect, showing that AI reduces carbon emissions only after reaching a particular level of deployment, showing that in parts of China, this threshold was not reached [74]. They also noted that the effect is notable only in certain industries, particularly labour-intensive ones. Researchers reported that accuracy was prioritised over efficiency, meaning that AI developers were not prioritising carbon emissions when developing models [75]. They proposed developing multi-objective models, which considered training models to consider both the environmental impact and accuracy. Development of standards and trade-offs between accuracy and multiple trustworthy AI principles, including the environmental impact of AI systems, was also proposed by ([10]. Accountability for trade-offs in AI systems requires an increased level of oversight and accountability for businesses that can

have conflicting agendas when it comes to profitability and ethics [40]; [39]. Training a single LLM can emit 300,000kg of CO2, the equivalent of 125 round-trip flights between New York and Beijing[76]. They also highlighted the focus and prioritisation of companies on model accuracy over energy efficiency, which results in higher emissions, and highlighted a need to balance model performance with environmental impact. The paper also references the negative environmental impact of extracting metals from the earth.

## Discussion:

The relationship between AI and environmental sustainability is intricate. While there are many potential environmental benefits argued in the literature, the current trajectory of balancing environmental gain with the environmental cost is not positive. The literature shows companies prioritise model accuracy over efficiency, leading to increased carbon emissions. The conflict of interest between prioritising business goals with ethical goals was highlighted, along with concerns about allowing organisations to choose their own trade-offs between environmental impact and business goals like accuracy. Questions around the environmental cost of training and operating AI systems, along with environmental considerations such as the extraction of metals for AI hardware, were highlighted by both the industry participants and researchers. An additional concern was highlighted around the environmental cost of the generation of vast amounts of redundant new data being stored in data centres. The disconnect and misconceptions between AI and its negative carbon footprint raised by participants were also seen in the confusion and conflicts in the literature, which, on the one hand, outlined the potential positive benefits of AI for the environment and, on the other hand, drew attention to the concerns over the carbon emissions from AI systems. The lack of clarity and standards to measure the impact of AI systems on the environment was an additional key concern in the literature.

## 4.5.2 Societal and Cultural Impact

This section examines how AI affects society and culture, including its influence on employment, social structures, and ethical considerations in its deployment. Interviewees raised concerns about AI-driven job displacement, biases reinforcing social inequalities, and AI's role in shaping political discourse and cultural identity. Below is a detailed summary of the interview findings, subsequent literature review, and a discussion on both.

## Interview findings:

Two participants pointed out that AI is about enhancing people's jobs, not replacing them, and that AI can give more meaningful jobs to people. One participant said they were implementing AI cautiously in their organisation, with a focus on ensuring employees felt secure and were aware their jobs were not being replaced. They explained that they were delivering the rollout by being sensitive to the feelings of employees who might have worked at a task for twenty years that was now being automated with AI. Another participant suggested taxing AI models in cases where there were significant job displacements. While those comments around changes to the workforce were positive, one participant highlighted the negative impact on those working in third-world countries where dangerous jobs were involved in the mining of rare metals. They described the work done by artisanal miners who mine metals for AI systems with their bare hands in the global south as devastating. Concerns were raised by four participants around the significant influence AI is having on society, with all four drawing attention to AI's ability to influence politics. The existing harm to society and democracy was noted as already being extremely damaging, with further concerns surrounding the escalation of AI. Two participants raised significant concerns about the use of AI to profile users and feed them tailored versions of news content. This was described as leading to radicalisation and the formation of online groups that both believe are right because of how AI has tailored their version of news content. Participants referenced platforms such as X (formerly Twitter), TikTok, and Reddit as places where people are being delivered harmful information that has negative societal effects. Participants described how these platforms influence you through AI personalisation, with one describing the information as being fed through social media, resulting in political radicalisation. One participant explained that AI can influence you very quickly because it has

built a profile on you and knows you. The creation and dissemination of deep fake videos and images online were noted by one participant as having already damaged trust in the news online and undermining democracy, with another referencing that in social settings on trips to the US, they found it is now considered improper social etiquette to discuss politics due to tensions and deep political divides in their society.

This research also highlighted concerns around unwanted reinforcement of things such as social inequalities and unrealistic standards. One participant noted that vulnerable groups, such as young people, individuals from underprivileged communities, and people of colour, need more transparency so that AI doesn't cement existing inequalities in society. Another participant gave the example of harmful AI filters that are designed to alter body types or enhance beauty, asking about the effects that it has on that child when the filter gets turned off, and they immediately compare themselves to the AI version. They pointed out that social media companies should be investing more in the mental health effects of their platforms and the tools their platforms are having on the young people who use them. They argued that platforms need to work more on improving features on their platforms that contribute to increased rates of depression and suicide in young people. Another participant provided the example of bias in insurance models, asking who gets to decide how much weight your gender or postcode should have on your premium, explaining that bias does exist for things like this. They explained that models know that men are more likely to crash and bias against them, but when you reduce that bias in the model, it gets traded off with accuracy. They posed the question, 'Who gets to decide what's fair and unfair?' and called for more transparency for people in society into how these models work. Two participants flagged loss of cultural identity, beliefs, language and knowledge as a concern. One participant pointed out that there are variances in cultural beliefs and agreements about what is fair and what is discrimination. They explained that when you ask an AI for an answer, people from different parts of the world would have a different right and wrong answer, giving the example of European data holding European beliefs, which wouldn't likely be accepted in other places such as Afghanistan, which has its own set of beliefs. Another participant detailed how the language used for data going into and coming out of AI systems was becoming standardised, resulting in a loss of local language, colloquialisms, and even industry terminology that was leading to the sacrificing of cultural identities. They described a situation where a global data team wanted to rename local placenames so they could be more easily understood by an AI system. As they foresaw the downstream cultural effects, they decided to fight for their sense of Irish identity and argued against the change, noting, however, that these things aren't always considered and that there were already many changes to language they had seen in the short time that AI has been used. Three participants raised the challenge of enforcing governance on companies that implement AI with negative societal outcomes. One participant explicitly stated that we cannot control how data gets produced and used, stating that it was definitely a concern for them. They pointed out that they are given sole responsibility for pricing for a large number of customers, and although nobody enforces fairness, it was important for them to have integrity towards the model, so it is fair and works for everyone. However, they said that in their experience, they have to work towards a target revenue, and if it's not matching, they have to bridge the gap; this involves changes in feature selection and the bias that goes into it. They said that although they would like the model to have a lower level of bias, their company requires them to present multiple options of trade-offs between bias and accuracy so that executives can decide the trade-offs, describing the process of allowing machines to implement bias in these decisions as unfair. Another participant explained that if humans making decisions around the AI process are financially incentivised to choose profit, then they will, and decisions around profit and fairness cannot be made by a person with a financial interest in the outcome. They explained agency is more than just consent; humans should have control over the decisions and the outcomes of the AI system, highlighting that the AI should be for humanity's good rather than the company's profit. Challenges regulating AI internationally were also flagged by two participants, with one calling for international cooperation and transparency around which data

is moved between regions. Another participant described how AI gives companies an edge, so even if regulation is introduced, the countries that do permit unregulated AI will gain advantages, in particular, countries that are already heavily sanctioned for other activities.

## Literature findings:

Amerish et al 2020.[77] defines culture as the ethical, sociological, technological, and ideological features of a society or social group which define their way of life, work and communication. They suggest that culture is a precedent to the process of knowledge creation and thus influences the perception and construction of knowledge systems. They believe that the emergence of AI systems, which have been shaped by the cultural parameters of the 'West' and the current social and economic power structures, is fundamentally shifting the entire social, political and ethical fabric and disrupting the natural environment. They explained that the foundation of modern technological development in the West originates from British social values, namely the utilitarian principle, which seeks to achieve the greatest good for the greatest number and a desire to control the environment for the betterment of human life. In contrast, countries like Japan inherently value group solidarity, social harmony and a reduction of internal conflict, and China holds the traditional cultural values of Confucianism, which stresses the importance of 'formal learning processes' and administrative guidance (by the bureaucratic Mandarin class), and Taoism which emphasises living in harmony with the natural order of the universe. The paper suggests that a fresh perspective on which trajectory the technological process should take is needed, suggesting that the Eastern values of interdependence and universal harmony with nature should be revived and integrated into how we shape the future of technology. Other research argues that variances in cultural values need to be considered when developing evaluation criteria for Trustworthy AI technology [10]. They also highlight a disconnect between what policy makers, AI experts, and a standard non-expert user considers fair, which, along with these differences in culture, show a need for the inclusion of a wide variety of stakeholders to establish norms for deciding what Trustworthy AI looks like. Research suggest that AI is an opportunity to implement sociotechnical change that can help bring about a better, fairer world [78]. They note that while sociologists are partially contributing to the design of AI systems, they need to be involved more to help design technology that is more beneficial for society. A challenge they present is decision-making protocols that favour corporate elites, naming Silicon Valley as a part of the problem leading to the current class, gender and racial inequalities in societies. Silicon Valley and corporate interests have influenced the direction of academic research, noting as a shift towards entrepreneurship in education aimed at fostering new ventures [79]. Additional concerns about conflicts over the ownership of art and a loss of culture were raised by researchers [80].

Additional societal concerns around the impact of AI on radicalisation were raised by researchers. Researchers reported that AI systems have increased the radicalisation and divide between viewpoints linked to political violence [79]. The paper highlights the research gap in the harmful political and social effects of AI. Although AI can be used for cyber security purposes, they suggest that this term should be broadened to include the harmful effects of social media and AI systems on societies. Persuasion into radicalisation is a process involving the weaponisation of words to convince a target audience of a particular narrative [81]. This paper also discuss the use of counter-narratives to combat these, in particular the ones most likely to lead to violence. They explained how AI was used to distribute 'fake news' on social media while also being used to help identify and censor terrorist material. They highlighted that the volume of harmful content online means that it would be impossible to remove it all or attempt to rebut it with counter-narratives. However, they highlight that countries should attempt to decrease the credibility gap, address the root causes, and use both online and offline approaches to solve the issues of online radicalisation that lead to violent extremism. There is a need for Extremism, Radicalisation and Hate speech (ESH) detection tools to combat the societal risks associated with social media's ability to mobilise extremist communities [82]. To contribute to protecting both free speech and user safety, the paper proposed an ERH

context-mining framework which involved ideological isomorphism (radicalisation), morphological mapping (extremism), and outward dissemination (politicised hate speech). They highlighted the need for more research into resolving biases in dataset collection, annotation and algorithmic approaches in this field.

Researchers reported a dual positive and negative effect on the workforce. While it was noted that AI could replace lower-skilled positions, resulting in improved skill level of the workforce [72], researchers also raised concerns about the current underpaid workforce that is keeping AI systems going ([76];[80]. Challenges in implementing sufficient oversight and accountabilities for companies who can release AI systems that have harmful societal outcomes were also highlighted by several researchers who demonstrated a clear need for standards and proper accountability for organisations [38]; [39]; [40].

## Discussion:

While there are many arguments around the potential of AI for good, the current economic and social structures have led to the prioritisation and shaping of AI technology that is causing unintended harm to societies. Western cultural values and influential decision-makers in places like Silicon Valley have shaped the development of AI technology. However, the decisions made by those investing in technology conflict with the values held by populations in Western societies, which broadly strive for the greatest good for the greatest number of people. The societal values of controlling and subjugating the natural world for the benefit of humans have spread rapidly throughout Western culture in recent years from Great Britain, directly conflicting with the dominant Eastern values of harmony with the natural world. AI systems, the design and function of which are shaped by the cultural, social and economic structures existing in the global technology industries, can transform societies at scale and result in mass shifts in the fabric of our societies. Adverse societal outcomes are seen from AI systems, in particular, the radicalisation of viewpoints online, the amassing of extremist communities and a rise in political violence. AI is expected to cause significant shifts in the global workforce. After a transition period of job displacement, the Western world anticipates positive outcomes: job functions are expected to become more attractive for employees, with a reduction in unwanted, lower-paid administrative tasks. However, there is already a notable reliance on an underpaid workforce, described by researchers as a 'sweatshop of programmers' in the Eastern world, which sustains AI infrastructure globally. Further research is needed to establish how we can challenge existing social and economic structures to influence the design of AI systems, which can be aligned with the culture and values held by societies using that technology.

## 5 Reflection on the EU AI Act from our Findings

The EU AI Act [1] seeks to protect fundamental rights by regulating AI systems, mandating risk assessments, accountability mechanisms, and compliance requirements. It also refers to the EU Principles of Trustworthy AI as a guidance for safe AI development. However, the findings from this study highlight significant gaps in how the industry professionals in this study understand and implement trustworthy AI principles, raising questions about whether the Act's provisions will be effective in practice. The section links key industry concerns identified in our research to the regulatory approach taken by the AI Act.

## 5.1 Accountability, Governance, and Regulatory Compliance:

The AI Act, clearly defines the roles and responsibilities for those involved in AI systems, but accountability in practice remains complex. AI accountability is divided across multiple roles in the Act, including AI providers who build, buy or adapt AI models and place them on the market, and deployers who use them. Additional responsibilities fall on importers, distributors, and supervisory authorities, but ambiguity arises when any distributor, importer, deployer or other third party makes a 'substantial modification' to a high-risk AI system, shifting their classification and compliance requirements. The Act mandates technical documentation and data labelling transparency, yet challenges such as communication

around potentially sensitive data between various stakeholders in the AI development process remain. This includes difficulties coordinating compliance across multiple stakeholders when dealing with third-party AI models ([83]. Findings from industry professionals during our interviews highlighted key concerns with compliance readiness. Accountability within organisations is fragmented, with unclear ownership of AI governance and reliance on third-party compliance claims that often lack verification. Many companies approach AI governance reactively, addressing risks only when problems arise, while existing compliance tools are insufficient for AI-specific risks. Financial incentives also influence compliance, with auditors and internal teams sometimes prioritising approvals over rigorous assessments. Unlike GDPR, AI regulations lack widely adopted compliance frameworks, making enforcement inconsistent and challenging. While the AI Act establishes an important foundation for AI governance, the insights reported by participants in this study and subsequent literature review indicate that its effectiveness depends on stronger enforcement mechanisms. Real-time compliance monitoring, sector-specific regulatory frameworks, and independent auditing are necessary to prevent the Act from becoming another bureaucratic hurdle rather than a meaningful tool for ensuring Trustworthy AI.

## 5.2 Data Management and Quality in AI Systems:

The AI Act includes provisions for data governance, requiring that AI models be trained on high-quality, well-documented datasets. However, our interview findings show that poor data quality, provenance issues, and synthetic data manipulation are widespread industry concerns. Many AI professionals noted that their organisations struggle to ensure data integrity, with some questioning whether data brokers provide truthful representations of consent and compliance. The lack of transparency in third-party data sourcing is a direct challenge to the AI Act's goal of ensuring fairness and accountability. Additionally, the growing reliance on synthetic data raises concerns about bias amplification and the loss of real-world validity in AI models. While the AI Act touches on data quality, it does not yet include specific provisions to regulate synthetic data generation, which may become a loophole that weakens compliance efforts. Given the risks associated with data quality failures, including biased decision-making and unreliable AI outputs, the Act may need to introduce stricter requirements for provenance tracking and independent data audits.

## 5.3 Human Factors in AI Development and Oversight:

The AI Act mandates human oversight in highrisk AI systems, requiring human intervention in decision-making processes. However, industry professionals highlight several challenges with how human oversight is implemented in practice. Some noted that organisational structures do not currently support effective AI governance, as compliance teams often lack the technical expertise to assess AI risks, and AI engineers may not fully understand ethical and regulatory requirements. Another key issue is conflicts of interest in human decision-making. Several professionals pointed out that when humans are included in AI oversight, their incentives may prioritise business interests over ethical concerns. For example, when AI bias is detected, professionals reported that leadership often dismisses these concerns due to the financial trade-offs associated with bias mitigation. The AI Act does not currently specify the qualifications, independence, or ethical responsibilities of human overseers, leaving significant room for organisations to self-regulate oversight in ways that may not be effective. Additionally, there is an emerging risk of over-reliance on AI, where professionals assume that AI-driven compliance tools will inherently prevent unethical or illegal behaviour. This 'blind trust' in AI automation contradicts the AI Act's aim to ensure meaningful human control, highlighting a gap between regulatory intent and industry practice.

## 5.4 Technological Robustness and Safety:

While the AI Act introduces requirements for technological robustness and security, our findings reveal that AI security risks are evolving

faster than regulatory frameworks. Industry professionals expressed concerns about data poisoning, adversarial attacks, and sensitive data leakage in AI systems, but noted that security compliance efforts often rely on reactive measures rather than proactive risk management. A number of participants specifically criticised the use of static certifications like ISO27001 for AI security, arguing that these frameworks were not designed for real-time AI monitoring. The Act's provisions on security could be strengthened by incorporating continuous evaluation mechanisms rather than relying solely on documentation and predeployment risk assessments. Without this, companies may remain vulnerable to dynamic security threats that emerge after AI systems are deployed.

## 5.4.1 Environmental and Societal Impact:

The AI Act includes fairness and societal wellbeing as core principles but does not yet introduce concrete enforcement mechanisms for evaluating AI's societal and environmental impact. Our research finds that AI professionals are increasingly aware of AI's negative societal consequences, including reinforcement of social inequalities, political radicalisation, and cultural loss. However, companies currently lack clear guidelines on how to measure and mitigate these risks in practice. Similarly, AI's high energy consumption remains an industry concern, with some professionals calling for environmental impact assessments and potential fines for AI-driven carbon footprints. While the AI Act does acknowledge the need for responsible AI development, it does not yet introduce binding sustainability requirements. As AI adoption continues to grow, the regulatory framework may need to evolve to include mandatory environmental impact assessments for AI models, particularly those requiring large-scale computational resources.

## 6 Practical Checklist for Operationalisation of Trustworthy AI

While this study has identified critical challenges in achieving trustworthy AI across governance, data quality, regulatory compliance, fairness, and human oversight, it is evident that the industry lacks a consolidated, actionable pathway for addressing these issues in practice. Many of the principles outlined in the EU's Ethical Guidelines for Trustworthy AI, though widely accepted in theory, remain difficult to implement. To bridge this gap between theory and application, we propose a practical governance checklist with a list of actions designed to assist organisations in operationalizing the challenges identified in this paper for the implementation of Trustworthy AI. This is detailed in table 8 Although standards such as ISO42001[14] offer structured frameworks for AI Management Systems, they are often high level and lack the focus of the specific challenges which were identified through this research. This checklist translates the thematic findings of our interviews into concrete organisational actions that can be implemented across organisations, as a supplement to existing certifications and standards such as ISO42001. While this checklist is designed for practitioner use, we acknowledge that its application may vary considerably across sectors and organisations and we therefore frame it as a flexible guide rather than a prescriptive standard.

## 7 Discussion and Future Directions

AI technologies present both opportunities and challenges in ensuring the development and deployment of TAI. Addressing the identified industry challenges requires an independent and sector level approach involving policy reforms, standardisation of industry best practices, and collaborative efforts including all relevant stakeholders. This section outlines future directions for the improvement and adoption of TAI principles including recommendations for policymakers, industry and academia. All three areas should involve diverse stakeholders, including underrepresented communities, in their decision-making process to ensure AI systems are developed with a broader perspective, mitigating biases and promoting fairness across different segments of society. Efforts should be made to prevent the loss of cultural identities due to AI standardisation. This can be achieved by incorporating local languages, customs, and values into not just developing AI

Table 8 Key Challenges and Recommended Organisational Actions

| Key Challenge                                  | Recommended Organisational Action                                                                            |
|------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| Siloed departments & communication gaps        | Establish cross-functional AI steering committees to improve col- laboration and compliance integration.     |
| Unclear lines of responsibility                | Clearly define and document roles and responsibilities for AI com- pliance and accountability.               |
| Blind trust in third-party compliance          | Conduct thorough due diligence and regular audits of third-party providers.                                  |
| Lack of systematic bias testing                | Implement regular and mandatory bias audits and fairness evalu- ations.                                      |
| Conflict between fairness and prof- itability  | Incorporate fairness metrics into business KPIs and decision- making frameworks.                             |
| Confusion around AI Act implications           | Provide targeted training on regulatory requirements and impli- cations for different teams.                 |
| Lack of AI-specific standards                  | Adopt and contribute to the development of sector-specific AI standards.                                     |
| Rush to implement AI without risk mit- igation | Mandate risk assessments before deployment of AI systems.                                                    |
| Lack of AI assessment tools                    | Invest in or develop AI-specific compliance and assessment tools.                                            |
| Poor data quality                              | Introduce automated data quality checks and establish quality thresholds.                                    |
| Vendor misrepresentation of data con- sent     | Implement stringent data provenance checks and vendor contracts with accountability clauses.                 |
| Overreliance on synthetic data                 | Use synthetic data cautiously, ensuring validation and bias moni- toring processes.                          |
| Inadequate data documentation                  | Develop automated and regularly updated data lineage documen- tation systems.                                |
| Insufficient human oversight                   | Define human-in-the-loop checkpoints and mandate oversight at critical decision points.                      |
| Lack of explainability                         | Use explainable AI methods and ensure model transparency is available to internal and external stakeholders. |
| Model hallucinations                           | Ensure high-quality, up-to-date data is used and monitor model outputs for anomalies.                        |
| Evolving cyber threats                         | Establish continuous AI security assessment protocols and update cybersecurity training.                     |
| Increased surface area for attacks             | Limit unnecessary data exposure and API access points through stringent access controls.                     |
| Unethical data practices                       | Enforce ethical data collection policies and increase internal audit- ing of data usage.                     |

systems, but at the level of prioritising which technology gets researched, developed and implemented into societies. The use of AI in social media should have sufficient oversight to prevent the spread of misinformation and radicalisation, whilst also respecting and preserving cultural diversity of the societies they introduce their systems into. Implementing AI models that detect and mitigate harmful content can help protect societal well-being, but due to the speed of which AI can be rolled out, the cultural impact should also be prioritised.

## 7.1 Policymakers

This study's findings indicate that the AI Act alone may not fully address the practical challenges of AI implementation. Industry professionals face uncertainty regarding compliance,

struggle with data quality and oversight, and report that security risks are escalating beyond what current regulations can manage. Without stronger enforcement mechanisms, sector-specific governance frameworks, and real-time AI evaluation tools, the Act may prove insufficient in ensuring genuinely Trustworthy AI. To close the gap between policy and practice, regulators should clarify accountability structures to ensure independent and enforceable AI oversight roles, introduce dynamic compliance mechanisms that go beyond static certifications to incorporate realtime AI system monitoring, and enhance security provisions by requiring ongoing adversarial testing and AI-specific risk mitigation measures. Additionally, regulating synthetic data usage through stricter standards for provenance and bias mitigation, as well as incorporating environmental accountability by mandating carbon footprint assessments for AI models, could strengthen the Act's effectiveness. By addressing these gaps, the AI Act could evolve from a broad regulatory framework into a more effective instrument for ensuring AI accountability, security, and ethical alignment in real-world applications.

Policymakers should work towards providing clearer and more detailed regulatory guidelines at an industry sector level to eliminate ambiguities surrounding the EU AI Act and other related regulations. The creation of AI-specific standards and certifications, audited by independent bodies who are not being paid by the company they are auditing, is crucial for establishing uniform and fair evaluation. Standard-setting organisations, in collaboration with industry experts and academics, should develop quantifiable metrics and real-time assessment tools tailored to different AI use cases and sectors. These standards should address all seven TAI principles. To enhance accountability, regulators could introduce independent auditing bodies to oversee AI compliance at a sector level. These bodies should operate without conflicts of interest to ensure impartial evaluations. In addition to developing quantifiable standards, enforcing stricter penalties for non-compliance, such as higher fines and restrictions on operations is required to ensure that companies are sufficiently deterred from bypassing ethical considerations in favour of profitability. Given the global nature of AI development, international cooperation is essential for harmonising regulations and standards. Policymakers should engage in cross-border dialogues to address challenges related ensuring that AI systems are developed safely, and in alignment with the values of the societies that they are impacting.

## 7.2 Industry

In addition to the recommended actions provided in table 8, organisations should embed ethical principles into every stage of the AI development lifecycle. This includes conducting thorough bias testing, ensuring data quality, and incorporating fairness and transparency metrics. Companies can build more trustworthy AI systems by prioritising ethical considerations alongside technical performance. Forming interdisciplinary teams or AI steering committees can break down departmental silos and foster collaboration between technical experts, compliance officers, and business leaders. These teams should oversee AI strategy, compliance, and risk management, ensuring that accountability is clearly defined within the organisation. Continuous education and training programmes can enhance employees' understanding of AI ethics, data literacy, and regulatory requirements. By equipping staff with the necessary knowledge and skills, organisations can improve human oversight and reduce risks associated with over-automation and poor decisionmaking. Transparency in AI systems can build trust among stakeholders. Organisations should adopt tools and methodologies that explain AI decision-making processes, making them accessible to both internal teams and external users. Transparent communication about data sources, model decision making and limitations, and ethical considerations can also mitigate misunderstandings and increase user trust and adoption of AI systems.

## 7.3 Academia

Academic institutions and research organisations should encourage multidisciplinary studies that combine insights from areas such as computer science, ethics, sociology, and law. These collaborations can lead to the development of more holistic approaches to TAI design and evaluation and help to address complex challenges like cultural biases and societal impacts. There is a

pressing need for semi-automated and real-time assessment tools capable of evaluating AI systems comprehensively. These tools should align with regulations such as the AI act, and include a number of metrics for TAI principles, which can be adopted for multiple use cases. Research efforts should focus on creating solutions that can continuously monitor AI performance and help inform decisions around trade-offs for TAI principles and company goals such as profitability. This will increase transparency and aid in reducing the reliance on manual checklists and point-intime assessments. Educational initiatives aimed at the general public can bridge the disconnect between AI usage and its societal and environmental impacts, as well as increase knowledge about how these systems work and their biases and limitations. By increasing awareness, individuals can make informed decisions and advocate for responsible AI practices, contributing to a culture that values TAI principles. Collaborative efforts should be directed towards researching energy-efficient AI models and promoting sustainable practices. This includes optimising algorithms for lower energy consumption, utilising renewable energy sources for data centres, and developing standards to measure and report the environmental impact of AI systems. Thought should be put into what aspects of AI academia wants to help further knowledge in. Current economic structures, including corporate interests, should not continue to increase their influence in directing which aspects of knowledge is furthered by academic research.

## 7.4 Limitations

The study has several limitations. First, the sample was confined to the UK and Ireland, where regulatory proximity to the EU AI Act may shape perspectives in ways that differ from those in other jurisdictions. Second, while we sought diversity in job functions, certain roles central to AI development (e.g., algorithm engineers, UX researchers, in-house legal experts) were not included. Their absence limits the generalisability of our findings, though the focus on deployment and governance roles remains aligned with our research objectives. Finally, with fifteen participants, our qualitative approach prioritised thematic depth over breadth;

as such, findings should be interpreted as illustrative of lived industry perspectives rather than definitive of the sector as a whole.

## 7.5 Proposed Next Steps

While this paper prioritises empirical insight from industry voices, we recognise that further work could benefit from deeper integration with established theoretical frameworks such as institutional theory, organisational ethics, or sociotechnical systems thinking. These lenses offer valuable ways of understanding how organisational norms, structures, and power dynamics shape the adoption of Trustworthy AI. However, given the emergent and still-contested nature of Trustworthy AI as a concept, and the intention of this paper to centre lived experience and practical barriers, we have chosen not to impose a singular theoretical framing. Future research could productively explore how these perspectives might complement the findings presented here, particularly in understanding how ethical AI commitments are shaped, constrained, or enabled by institutional context.

As a next step, we are focusing on developing a follow-up project that builds on the findings of this paper to provide more practical guidance for industry. This work will focus on designing a governance evaluation framework to help organisations align with both the Trustworthy AI principles and the requirements of the EU AI Act. It will translate high-level ethical and regulatory expectations into a practical tool that can be used across different stages of the AI lifecycle to assess systems for both regulatory compliance and adherence to the seven Trustworthy AI principles. Informed by the challenges identified through our interviews, the proposed framework will aim to bridge the gap between abstract principles and day-to-day industry implementation. Future work could more explicitly integrate theoretical frameworks, such as institutional theory, to situate these findings within broader organisational research traditions. In this study, however, our priority was to focus on the industry participants' lived experiences in practice.

## 8 Conclusion

This paper contributes significantly to academia by providing empirical insights into the challenges faced by industry professionals who participated in this study in implementing Trustworthy AI. Through structured interviews with fifteen experts across various sectors, we bridge the gap between academic and regulatory knowledge and real-world TAI practices of the participants interviewed. The study highlights ambiguities in accountability and governance, revealing a 'wild west' mentality where both AI adoption and enforceable regulations are not incorporating thorough ethical considerations. By documenting issues raised by participants such as data quality problems, inherent biases, and overreliance on AI without proper safeguards, the research emphasises the urgent need for standardised metrics, automated assessment tools, and interdisciplinary oversight. Academically, it fills a critical gap by offering qualitative data on industry practices, underscoring discrepancies between policy intentions and industry realities. It also highlights a need for societal and environmental impacts to be included in safety assessments of AI technology. In essence, this study advances academic knowledge by providing empirical evidence of industry challenges in the development and assessment of TAI, highlighting the need for more tangible regulations and standards, and emphasising the important role of human oversight and accountability. The findings from the participants in this study illustrate how practitioners in our sample perceive and navigate the challenges of Trustworthy AI. While the insights are not generalisable to the industry as a whole, they nonetheless highlight pressing issues in governance, accountability, and evaluation that are likely to resonate across a range of organisational contexts. These insights lay a foundation for future research to develop effective strategies and policies that align AI development with societal values and ethical principles.

Authors and Affiliations. All authors have reviewed and consented to the publication of the manuscript as presented. This research received partial support from the Research Ireland under grants 13/RC/2106 ¶ 2 (ADAPT) and is co-funded by the European Regional Development Fund (ERDF).

Ethics Declaration. The data employed in this review are sourced from one to one interviews with fifteen professionals following the University of Galway Ethics Process. The research was approved by the University of Galway's Research Ethics Committee (REC). The ethics protocol covered the outreach approach, which included contacting participants via email and LinkedIn. Informed consent was obtained from all participants via signed consent forms prior to their involvement in the structured interviews. This ethical process ensured compliance with GDPR regulations and ethical standards for research involving human participants.

Additionally research was collected through publicly available materials, including published research articles, ISO standards, books, and openly accessible databases and industry publications. All sources are duly cited and listed in the reference section of this paper.

## Appendix A Structured Interview Questions

## Data Acquisition

- Do you know how data is acquired for use in AI typically, and if so, can you tell me about it?
- Do you have concerns around data acquisition for AI?
- In your opinion, what are the most important aspects of data acquisition for AI?

## Data Quality

- What do you know about data quality and its importance in relation to AI?
- Do you have concerns around data quality for AI?
- In your opinion, what are the most important aspects of data quality for AI?

## Data Preparation

- Do you know how data is prepared for use with AI? Tell me about it if so.
- Do you have concerns around data preparation for AI?
- In your opinion, what are the most important aspects of data preparation for AI?

## Data Provenance

- Do you know if documentation is generally kept on data as it moves through the various stages of the AI system?
- In your opinion, what are the most important aspects of data provenance for AI?
- If you wanted to understand aspects of data usage in the company, how would you find out where the data came from and how it was transformed or used?
- Do you have concerns around data provenance for AI?

## Trustworthy AI

Do you have any additional comments, concerns or insights in relation to AI, specifically in relation to each of the seven key principles:

1. Human agency and oversight
2. Technical robustness and safety
3. Privacy and data governance
4. Transparency
5. Diversity, non-discrimination and fairness
6. Environmental and societal well-being
7. Accountability

## Assessment of Standards and Regulations

- How do organisations you have worked with generally assess for compliance with standards like ISO27001 and ISO27701 or regulations like GDPR?
- In your experience have you seen any challenges with assessment?
- Can you tell me any methods or frameworks you are aware of?
- In relation to self-assessment for AI compliance in the five aspects of data we previously discussed, can you foresee any potential challenges or obstacles?
- Are there any issues you see in relation to documenting these assessments?
- How does your organisation document assessment with standards and regulations?
- Who are the key stakeholders involved in the assessment of standards and regulations like GDPR and ISO27001?
- Do you know what processes these stakeholders follow or how they engage with the wider organisation on this work?

- Based on your knowledge of ISO27001 and ISO27701 (if any), what are the key aspects you think will need to be extended to include AI-specific aspects of these ISOs?

## References

- [1] European Union: Regulation (EU) 2024/0138 of the European Parliament and of the Council laying down harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union Legislative Acts. https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX: 52021PC0206. COM(2021)0206 - C9-0146/2021 - 2021/0106(COD) (2024). https://eur-lex.europa. eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206
- [2] European Commission: Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act). Official Journal of the European Union. COM(2021) 206 final, updated text as provisionally agreed in 2024 (2024). https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206
- [3] European Parliament and Council: Regulation (EU) 2023/988 of the European Parliament and of the Council of 10 May 2023 on general product safety and repealing Directive 2001/95/EC and Council Directive 87/357/EEC. Official Journal of the European Union, L 135, 23.5.2023, p. 1-48. General Product Safety Regulation (GPSR) (2023). https://eur-lex.europa.eu/eli/reg/2023/988/oj
- [4] European Parliament and Council: Directive (EU) 2024/2853 of the European Parliament and of the Council of 23 October 2024 on liability for defective products and repealing Council Directive 85/374/EEC. Official Journal of the European Union (2024)
- [5] Corrˆ ea, N.K., Camila Galv˜ ao, C., Santos, J.W., Del Pino, C., Pontes Pinto, E., Barbosa, C., Massmann, D., Mambrini, R., Galv˜ ao, L., Terem, E., Oliveira, N.: Worldwide ai ethics: A review of 200 guidelines and recommendations for ai governance. Patterns 4 (10) (2023) https://doi.org/10.1016/ j.patter.2023.100857
- [6] Schultz, M.D., Conti, L.G., Seele, P.: Digital ethicswashing: a systematic review and a processperception-outcome framework. AI and Ethics, 1-14 (2024)
- [7] Charter of Fundamental Rights of the European Union. Official Journal of the European Union, 2012/C 326/02. Accessed: 2024-11-22 (2012). https://eur-lex.europa.eu/legal-content/EN/TXT/ ?uri=CELEX%3A12012P%2FTXT
- [8] European Commission High-Level Expert Group on Artificial Intelligence: Ethics Guidelines for Trustworthy AI. European Commission. Accessed: 2025-03-25 (2019). https://ec.europa.eu/ futurium/en/ai-alliance-consultation
- [9] McCormack, L., Bendechache, M.: Ethical ai governance: Methods for evaluating trustworthy ai. arXiv preprint arXiv:2409.07473 (2024)
- [10] McCormack, L., Bendechache, M.: A comprehensive survey and classification of evaluation criteria for trustworthy artificial intelligence. AI and Ethics, 1-22 (2024)
- [11] Harris, C.: Mitigating age biases in resume screening ai models. In: The International FLAIRS Conference Proceedings, vol. 36 (2023)
- [12] Deng, W.H., Nagireddy, M., Lee, M.S.A., Singh, J., Wu, Z.S., Holstein, K., Zhu, H.: Exploring how machine learning practitioners (try to) use fairness toolkits. In: Proceedings of the 2022 ACM

- Conference on Fairness, Accountability, and Transparency, pp. 473-484 (2022)
- [13] Laux, J., Wachter, S., Mittelstadt, B.: Three pathways for standardisation and ethical disclosure by default under the european union artificial intelligence act. Computer Law &amp; Security Review 53 , 105957 (2024)
- [14] Standardization, I.O., International Electrotechnical Commission: ISO/IEC 42001:2023, Information technology -Artificial intelligence -Management system. International Organization for Standardization and the International Electrotechnical Commission, Geneva, CH (2023)
- [15] Lopes, I.M., Guarda, T., Oliveira, P.: Implementation of iso 27001 standards as gdpr compliance facilitator. Journal of information systems engineering &amp; management 4 (2), 1-8 (2019)
- [16] Guest, G., Bunce, A., Johnson, L.: How many interviews are enough? an experiment with data saturation and variability. Field methods 18 (1), 59-82 (2006)
- [17] Guest, G., Namey, E., Chen, M.: A simple method to assess and report thematic saturation in qualitative research. PloS one 15 (5), 0232076 (2020)
- [18] Braun, V., Clarke, V.: One size fits all? what counts as quality practice in (reflexive) thematic analysis? Qualitative research in psychology 18 (3), 328-352 (2021)
- [19] Braun, V., Clarke, V.: To saturate or not to saturate? questioning data saturation as a useful concept for thematic analysis and sample-size rationales. Qualitative research in sport, exercise and health 13 (2), 201-216 (2021)
- [20] Palinkas, L.A., Horwitz, S.M., Green, C.A., Wisdom, J.P., Duan, N., Hoagwood, K.: Purposeful sampling for qualitative data collection and analysis in mixed method implementation research. Administration and policy in mental health and mental health services research 42 (5), 533-544 (2015)
- [21] Jensen, L.S., Kennedy, S.S.: Public ethics, legal accountability, and the new governance. In: Ethics in Public Management, pp. 228-248. Routledge, ??? (2016)
- [22] Uzougbo, N.S., Ikegwu, C.G., Adewusi, A.O.: Legal accountability and ethical considerations of ai in financial services. GSC Advanced Research and Reviews 19 (2), 130-142 (2024)
- [23] Novelli, C.: Ai and legal personhood: a theoretical survey (2022)
- [24] Vantaggiato, F.P., Kassim, H., Connolly, S.: Breaking out of silos: explaining cross-departmental interactions in two european bureaucracies. Journal of European Public Policy 28 (9), 1432-1452 (2021)
- [25] Sheaff, M.: Constructing accounts of organisational failure: Policy, power and concealment. Critical Social Policy 37 (4), 520-539 (2017)
- [26] Bento, F., Tagliabue, M., Lorenzo, F.: Organizational silos: A scoping review informed by a behavioral perspective on systems and networks. Societies 10 (3), 56 (2020)
- [27] Drake, A., Keller, P., Pietropaoli, I., Puri, A., Maniatis, S., Tomlinson, J., Maxwell, J., Fussey, P., Pagliari, C., Smethurst, H., et al. : Legal contestation of artificial intelligence-related decision-making in the united kingdom: reflections for policy. International Review of Law, Computers &amp; Technology 36 (2), 251-285 (2022)

- [28] Lundberg, S.M., Lee, S.-I.: A unified approach to interpreting model predictions. In: Advances in Neural Information Processing Systems, pp. 4765-4774 (2017)
- [29] Ribeiro, M.T., Singh, S., Guestrin, C.: Why should i trust you? explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135-1144. ACM, ??? (2016)
- [30] Suresh, H., Guttag, J.: A framework for understanding sources of harm throughout the machine learning life cycle. In: Proceedings of the 1st ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, pp. 1-9 (2021)
- [31] Lee, M.S.A.: Context-conscious fairness in using machine learning to make decisions. AI Matters 5 (2), 23-29 (2019)
- [32] Singh, J., Singh, A., Khan, A., Gupta, A.: Developing a novel fair-loan-predictor through a multisensitive debiasing pipeline: Dualfair. arXiv preprint arXiv:2110.08944 (2021)
- [33] Lee, M.S.A., Floridi, L.: Algorithmic fairness in mortgage lending: from absolute conditions to relational trade-offs. Minds and Machines 31 (1), 165-191 (2021)
- [34] Farayola, M.M., Tal, I., Connolly, R., Saber, T., Bendechache, M.: Ethics and trustworthiness of ai for predicting the risk of recidivism: A systematic literature review. Information 14 (8), 426 (2023)
- [35] Farayola, M.M., Bendechache, M., Saber, T., Connolly, R., Tal, I.: Enhancing algorithmic fairness: Integrative approaches and multi-objective optimization application in recidivism models. In: Proceedings of the 19th International Conference on Availability, Reliability and Security, pp. 1-10 (2024)
- [36] Floridi, L.: Ai and its new winter: From myths to realities. Philosophy &amp; Technology 33 , 1-3 (2020)
- [37] ISO/IEC 27001:2022 Information Technology - Security Techniques - Information Security Management Systems - Requirements. International Organization for Standardization. Available at: https://www.iso.org/standard/82875.html
- [38] Jobin, A., Ienca, M., Vayena, E.: The global landscape of ai ethics guidelines. Nature machine intelligence 1 (9), 389-399 (2019)
- [39] D´ ıaz-Rodr´ ıguez, N., Del Ser, J., Coeckelbergh, M., Prado, M.L., Herrera-Viedma, E., Herrera, F.: Connecting the dots in trustworthy artificial intelligence: From ai principles, ethics, and key requirements to responsible ai systems and regulation. Information Fusion 99 , 101896 (2023)
- [40] Percy, C., Dragicevic, S., Sarkar, S., Garcez, A.: Accountability in ai: From principles to industryspecific accreditation. AI Communications 34 (3), 181-196 (2021)
- [41] Ewuga, S.K., Egieya, Z.E., Omotosho, A., Adegbite, A.O.: Iso 27001 in banking: An evaluation of its implementation and effectiveness in enhancing information security. Finance &amp; Accounting Research Journal 5 (12), 405-425 (2023)
- [42] Kamil, Y., Lund, S., Islam, M.S.: Information security objectives and the output legitimacy of iso/iec 27001: stakeholders' perspective on expectations in private organizations in sweden. Information Systems and e-Business Management 21 (3), 699-722 (2023)
- [43] Fontrodona, J.: The relation between ethics and innovation. In: Social Innovation: Solutions for a

- Sustainable Future, pp. 23-33. Springer, ??? (2013)
- [44] Gupta, N., Mujumdar, S., Patel, H., Masuda, S., Panwar, N., Bandyopadhyay, S., Mehta, S., Guttula, S., Afzal, S., Sharma Mittal, R., et al. : Data quality for machine learning tasks. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, pp. 4040-4041 (2021)
- [45] Rangineni, S.: An analysis of data quality requirements for machine learning development pipelines frameworks. International Journal of Computer Trends and Technology 71 (9), 16-27 (2023)
- [46] Priestley, M., O'donnell, F., Simperl, E.: A survey of data quality requirements that matter in ml development pipelines. ACM Journal of Data and Information Quality 15 (2), 1-39 (2023)
- [47] Sambasivan, N., Kapania, S., Highfill, H., Akrong, D., Paritosh, P., Aroyo, L.M.: 'everyone wants to do the model work, not the data work': Data cascades in high-stakes ai. In: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-15 (2021)
- [48] Morey, T., Forbath, T., Schoop, A.: Customer data: Designing for transparency and trust. Harvard Business Review 93 (5), 96-105 (2015)
- [49] Huq, A.Z.: The public trust in data. Geo. LJ 110 , 333 (2021)
- [50] Whitney, C.D., Norman, J.: Real risks of fake data: Synthetic data, diversity-washing and consent circumvention. In: The 2024 ACM Conference on Fairness, Accountability, and Transparency, pp. 1733-1744 (2024)
- [51] Jordon, J., Szpruch, L., Houssiau, F., Bottarelli, M., Cherubin, G., Maple, C., Cohen, S.N., Weller, A.: Synthetic data-what, why and how? arXiv preprint arXiv:2205.03257 (2022)
- [52] Joshi, I., Grimmer, M., Rathgeb, C., Busch, C., Bremond, F., Dantcheva, A.: Synthetic data in human analysis: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024)
- [53] Delacroix, S., Lawrence, N.D.: Bottom-up data trusts: Disturbing the 'one size fits all'approach to data governance. International data privacy law 9 (4), 236-252 (2019)
- [54] Werder, K., Ramesh, B., Zhang, R.: Establishing data provenance for responsible artificial intelligence systems. ACM Transactions on Management Information Systems (TMIS) 13 (2), 1-23 (2022)
- [55] Laine, J., Minkkinen, M., M¨ antym¨ aki, M.: Ethics-based ai auditing: A systematic literature review on conceptualizations of ethical principles and knowledge contributions to stakeholders. Information &amp; Management, 103969 (2024)
- [56] Solomon, G., Brown, I.: The influence of organisational culture and information security culture on employee compliance behaviour. Journal of Enterprise Information Management 34 (4), 1203-1228 (2021)
- [57] McLachlan, C.J.: Developing a framework for responsible downsizing through best fit: the importance of regulatory, procedural, communication and employment responsibilities. The InTernaTIonal Journal of human resource managemenT 33 (1), 16-44 (2022)
- [58] Enqvist, L.: 'human oversight'in the eu artificial intelligence act: what, when and by whom? Law, Innovation and Technology 15 (2), 508-535 (2023)
- [59] Alon-Barkat, S., Busuioc, M.: Human-ai interactions in public sector decision making:'automation

- bias' and 'selective adherence' to algorithmic advice. Journal of Public Administration Research and Theory 33 (1), 153-169 (2023)
- [60] Sterz, S., Baum, K., Biewer, S., Hermanns, H., Lauber-R¨ onsberg, A., Meinel, P., Langer, M.: On the quest for effectiveness in human oversight: Interdisciplinary perspectives. In: The 2024 ACM Conference on Fairness, Accountability, and Transparency, pp. 2495-2507 (2024)
- [61] Cihon, P.: Chilling autonomy: Policy enforcement for human oversight of ai agents. In: 41st International Conference on Machine Learning, Workshop on Generative AI and Law (2024)
- [62] Hamon, R., Junklewitz, H., Sanchez, I., et al. : Robustness and explainability of artificial intelligence. Publications Office of the European Union 207 , 2020 (2020)
- [63] Falco, G., Shneiderman, B., Badger, J., Carrier, R., Dahbura, A., Danks, D., Eling, M., Goodloe, A., Gupta, J., Hart, C., et al. : Governing ai safety through independent audits. Nature Machine Intelligence 3 (7), 566-571 (2021)
- [64] Oseni, A., Moustafa, N., Janicke, H., Liu, P., Tari, Z., Vasilakos, A.: Security and privacy for artificial intelligence: Opportunities and challenges. arXiv preprint arXiv:2102.04661 (2021)
- [65] Yampolskiy, R.V.: On monitorability of ai. AI and Ethics, 1-19 (2024)
- [66] Hossain, M.T., Afrin, R., Biswas, M.A.-A.: A review on attacks against artificial intelligence (ai) and their defence image recognition and generation machine learning, artificial intelligence. Control Systems and Optimization Letters 2 (1), 52-59 (2024)
- [67] Kong, Z., Xue, J., Wang, Y., Huang, L., Niu, Z., Li, F.: A survey on adversarial attack in the age of artificial intelligence. Wireless Communications and Mobile Computing 2021 (1), 4907754 (2021)
- [68] Lee, M.S.A.: Context-conscious fairness in using machine learning to make decisions. AI Matters 5 (2), 23-29 (2019)
- [69] Almeida, P.G.R., Santos, C.D., Farias, J.S.: Artificial intelligence regulation: a framework for governance. Ethics and Information Technology 23 (3), 505-525 (2021)
- [70] Ojewale, V., Steed, R., Vecchione, B., Birhane, A., Raji, I.D.: Towards ai accountability infrastructure: Gaps and opportunities in ai audit tooling. arXiv preprint arXiv:2402.17861 (2024)
- [71] Park, D.H., Cho, E., Lim, Y.: A tough balancing act-the evolving ai governance in korea. East Asian Science, Technology and Society: An International Journal 18 (2), 135-154 (2024)
- [72] Shang, Y., Zhou, S., Zhuang, D., ˙ Zywioglyph[suppress] lek, J., Dincer, H.: The impact of artificial intelligence application on enterprise environmental performance: Evidence from microenterprises. Gondwana Research 131 , 181-195 (2024)
- [73] Nishant, R., Kennedy, M., Corbett, J.: Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda. International Journal of Information Management 53 , 102104 (2020)
- [74] Wang, L., Chen, Q., Dong, Z., Cheng, L.: The role of industrial intelligence in peaking carbon emissions in china. Technological Forecasting and Social Change 199 , 123005 (2024)
- [75] Spelda, P., Stritecky, V.: The future of human-artificial intelligence nexus and its environmental

costs. Futures 117 , 102531 (2020)

- [76] Dhar, P.: The carbon impact of artificial intelligence. Nature Machine Intelligence 2 , 423-425 (2020) https://doi.org/10.1038/s42256-020-0219-9
- [77] Amershi, B.: Culture, the process of knowledge, perception of the world and emergence of ai. AI &amp; SOCIETY 35 (2), 417-430 (2020)
- [78] Joyce, K., Smith-Doerr, L., Alegria, S., Bell, S., Cruz, T., Hoffman, S.G., Noble, S.U., Shestakofsky, B.: Toward a sociology of artificial intelligence: A call for research on inequalities and structural change. Socius 7 , 2378023121999581 (2021)
- [79] Piqu´ e, J.M., Berbegal-Mirabent, J., Etzkowitz, H.: The role of universities in shaping the evolution of silicon valley's ecosystem of innovation. Triple Helix 7 (2-3), 277-321 (2020)
- [80] K¨ ose, U.: Are we safe enough in the future of artificial intelligence? a discussion on machine ethics and artificial intelligence safety. BRAIN. Broad Research in Artificial Intelligence and Neuroscience 9 (2), 184-197 (2018)
- [81] Bamsey, O., Montasari, R.: The role of the internet in radicalisation to violent extremism. In: Digital Transformation in Policing: The Promise, Perils and Solutions, pp. 119-135. Springer, ??? (2023)
- [82] Govers, J., Feldman, P., Dant, A., Patros, P.: Down the rabbit hole: Detecting online extremism, radicalisation, and politicised hate speech. ACM Computing Surveys 55 (14s), 1-35 (2023)
- [83] Golpayegani, D., Hupont, I., Panigutti, C., Pandit, H.J., Schade, S., O'Sullivan, D., Lewis, D.: Ai cards: Towards an applied framework for machine-readable ai and risk documentation inspired by the eu ai act. arXiv preprint arXiv:2406.18211 (2024)