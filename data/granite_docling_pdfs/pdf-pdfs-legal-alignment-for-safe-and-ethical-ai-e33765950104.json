{
  "doc_id": "pdf-pdfs-legal-alignment-for-safe-and-ethical-ai-e33765950104",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Legal Alignment for Safe and Ethical AI.pdf",
  "title": "Legal Alignment for Safe and Ethical AI",
  "text": "## Legal Alignment for Safe and Ethical AI\n\nNoam Kolt 1 ⋆ Nicholas Caputo 2 ⋆ Jack Boeglin 3 † Cullen O'Keefe 4 , 5 †\n\nRishi Bommasani 6 Stephen Casper 7 Mariano-Florentino Cuéllar 8 Noah Feldman 9\n\nIason Gabriel 10 Gillian K. Hadfield 11 , 12 Lewis Hammond 13 , 14 Peter Henderson 15\n\nAtoosa Kasirzadeh 16 Seth Lazar 11 , 17 Anka Reuel 6 Kevin L. Wei 9 Jonathan Zittrain 9 , 18\n\n1 Hebrew University 2 Oxford Martin AI Governance Initiative 3 University of Pennsylvania 4 Institute for Law &amp; AI 5 Centre for the Governance of AI 6 Stanford University 7 MIT CSAIL\n\n8 Carnegie Endowment for International Peace 9 Harvard University 10 School of Advanced Study University of London 11 Johns Hopkins University 12 Vector Institute for Artificial Intelligence\n\n13 Cooperative AI Foundation 14 University of Oxford 15 Princeton University 16 Carnegie Mellon University 17 Australian National University 18 Berkman Klein Center for Internet &amp; Society\n\n## Abstract\n\nAlignment of artificial intelligence (AI) encompasses the normative problem of specifying how AI systems should act and the technical problem of ensuring AI systems comply with those specifications. To date, AI alignment has generally overlooked an important source of knowledge and practice for grappling with these problems: law . In this paper, we aim to fill this gap by exploring how legal rules, principles, and methods can be leveraged to address problems of alignment and inform the design of AI systems that operate safely and ethically. This emerging fieldlegal alignment -focuses on three research directions: (1) designing AI systems to comply with the content of legal rules developed through legitimate institutions and processes, (2) adapting methods from legal interpretation to guide how AI systems reason and make decisions, and (3) harnessing legal concepts as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems. These research directions present new conceptual, empirical, and institutional questions, which include examining the specific set of laws that particular AI systems should follow, creating evaluations to assess their legal compliance in real-world settings, and developing governance frameworks to support the implementation of legal alignment in practice. Tackling these questions requires expertise across law, computer science, and other disciplines, offering these communities the opportunity to collaborate in designing AI for the better.\n\n## 1 Introduction\n\nThe development and proliferation of increasingly advanced AI systems will present society with tremendous opportunities (Eloundou et al., 2024; Brynjolfsson et al., 2025) and significant risks (Lazar and Nelson, 2023; Bengio et al., 2024, 2025). Capturing the opportunities from advanced AI while tackling its risks requires ensuring that AI systems operate safely and ethically (Anwar et al., 2024; Gabriel et al., 2024, 2025). A central component of this challenge involves designing AI systems that are aligned with human interests (Russell, 2019; Christian, 2020) and democratic values (Lazar and Cuéllar, 2025). AI alignment encompasses both the normative problem of specifying which values are desirable or appropriate for AI systems (Gabriel, 2020; Kasirzadeh, 2026) and the technical problem of ensuring AI systems give effect to those values when making decisions and taking actions (Chan et al., 2023; Ngo et al., 2024).\n\n* Lead authors. † Core contributors. Cite as Kolt, Caputo, et al. (2026) 'Legal Alignment for Safe and Ethical AI.' Correspondence to: noam.kolt@mail.huji.ac.il .\n\nTo date, the main approaches to alignment in systems like ChatGPT, Claude, and Gemini have focused primarily on steering systems to follow the instructions of users (Ouyang et al., 2022), advance the interests of developers (OpenAI, 2024a), and refrain from supporting or engaging in forms of harmful behavior (Askell et al., 2021; Bai et al., 2022a). In broad strokes, the methods for building such systems employ a combination of human feedback (Christiano et al., 2017) and AI-generated feedback (Lee et al., 2024) to evaluate the outputs of AI systems during training by reference to a list of predetermined specifications-typically written by developers-and iteratively refine the systems to produce outputs more closely aligned with those specifications (Bai et al., 2022b). Some systems can retrieve, reason about, and deliberate over these specifications in real-time before producing outputs (Madaan et al., 2023; Guan et al., 2024).\n\nFrom a technical perspective, these methods for alignment have had mixed results. Despite achieving more reliable performance in many tasks and domains (Phan et al., 2025; Kwa et al., 2025), AI systems continue to produce untruthful content (Ji et al., 2023; Li et al., 2025), generate biased outputs (Weidinger et al., 2022; Gallegos et al., 2024), manipulate users through persuasion (Carroll et al., 2023; Hackenburg et al., 2025), exhibit sycophantic tendencies (Sharma et al., 2024; Cheng et al., 2025), leak private information (Carlini et al., 2021; Mireshghallah et al., 2024), remain vulnerable to jailbreaks (Wei et al., 2023; Chao et al., 2024), enable autonomous hacking (Zhang et al., 2025; Zhu et al., 2025b), offer assistance in bioweapons development (Li et al., 2024; Götting et al., 2025), recognize when they are being safety-tested (Needham et al., 2025; Lynch et al., 2025) and, at times, conceal their misalignment (Greenblatt et al., 2024; Sheshadri et al., 2025).\n\nFrom a normative perspective, the prevailing approaches to alignment face fundamental limitations (Dobbe et al., 2021; Hadfield, 2026). Rather than designing AI systems to act in accordance with broad societal interests (Korinek and Balwit, 2022; Kasirzadeh and Gabriel, 2023), let alone grapple with people's diverse and sometimes conflicting values (Klingefjord et al., 2024), most alignment techniques train AI systems to comply with company-written alignment policies (Ahmed et al., 2025) or satisfy the revealed preferences of individual users (Zhi-Xuan et al., 2025) through fallible methods such as reinforcement learning from human feedback (Casper et al., 2023). Moreover, key decisions in AI alignment pipelines, such as selecting which principles are included in a system's 'constitution' (Anthropic, 2023), 'model specification' ('model spec') (OpenAI, 2024a), or safety filters (Google, 2025b), are often opaque (Bommasani et al., 2023; Wan et al., 2025) and lack sufficient public input or scrutiny (Abiri, 2025; Lazar, 2025).\n\nRecognizing these limitations, some AI researchers have proposed broadening the goals and methods of alignment (Lowe et al., 2025). Noteworthy efforts include expanding the forms of community participation in AI development (Sloane et al., 2022), incorporating pluralistic values into alignment procedures by collecting preference and judgment data from demographically diverse populations (Sorensen et al., 2024a,b; Kirk et al., 2024), sourcing safety principles and ethical guidelines from participants in public deliberative processes (Huang et al., 2024; Eloundou et al., 2025), and deriving principles from preference data (rather than the reverse) (Findeis et al., 2024). Other research agendas propose leveraging insights from related fields, including game theory, conflict studies, mechanism design (Dafoe et al., 2020, 2021), social choice (Conitzer et al., 2024), and contractualism (Levine et al., 2025). The extent to which these methods and approaches will be further developed or adopted in large-scale AI deployment remains to be seen.\n\nThere is, however, another domain of knowledge and practice that could support developing more legitimate and effective approaches to AI alignment: law . Building on recent legal scholarship (Kolt, 2025; Caputo, 2025; O'Keefe et al., 2025; Boeglin, 2026), we explore how designing AI systems to operate in accordance with appropriate legal rules, principles, and methods can help address problems of alignment . This emerging fieldlegal alignment -aims to harness law in tackling both normative and technical aspects of alignment:\n\n- For normative aspects of alignment, legal rules developed through legitimate institutions and processes in democratic societies could be used to guide the behavior of AI systems, much like they guide the behavior of individuals, corporations, and governments.\n- For technical aspects of alignment, legal methods of interpretation and reasoning could offer principled approaches that inform and steer the decision-making and exercise of discretion by AI systems, especially in novel scenarios and high-stakes settings.\n- Across both aspects of alignment, legal concepts can serve as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems and the human actors and institutions with which they interact.\n\nThe advantages of legal alignment derive primarily from the public legitimacy of law and its institutional processes. In democracies governed by the rule of law (Dicey, 1959; Tamanaha, 2004; Bingham, 2007; Waldron, 2016), legal rules are ideally the product of transparent and publicly accountable processes that are themselves governed by rules and procedures that a political community recognizes as legitimate (Hart, 2012; Habermas, 1996; Tyler, 2006; Hadfield and Weingast, 2012). These institutional frameworks differ markedly from the organizational structures, primarily private corporations, that currently shape the development of AI technology (Birhane et al., 2022; Seger et al., 2023; Maas and Inglés, 2024; Ovadya et al., 2025). As we discuss in Section 3, law also contains relatively robust methods for balancing competing societal interests and adapting existing rules and principles to new economic and technological conditions, which will be essential for steering the design and operation of increasingly advanced AI systems.\n\nNotwithstanding these desirable features of law, legal alignment is not a catch-all solution for the safety and ethics challenges arising from AI systems. Rather, legal alignment serves as a critical lower bound , which is both independently important and can also complement other approaches to AI alignment. Furthermore, to establish broad consensus around legal alignment, we deliberately take an ecumenical approach to law and fundamental legal questions, engaging with different and sometimes conflicting legal perspectives and theories (e.g., Hart, 2012; Dworkin, 1986; Raz, 1979a) without seeking to resolve the tensions between them here (Schauer, 1991; Shapiro, 2011). 1\n\nFor clarity, we note that legal alignment is distinct from legal regulation of actors that develop and deploy AI, which focuses primarily on using law to govern the individuals and organizations that produce, disseminate, and use AI systems (e.g., Lemley and Casey, 2019; Kaminski, 2023; Henderson et al., 2023; Kolt, 2024; Guha et al., 2024; Arbel et al., 2024; Ayres and Balkin, 2024; Ramakrishnan et al., 2024; Weil, 2024). By contrast, legal alignment focuses on integrating law and legal methods into the design and operation of AI systems themselves . The two fields, however, are closely related and potentially mutually supportive, including because legal regulation can help facilitate legal alignment in practice, such as by enabling researchers to access technical resources required to effectively evaluate and improve the legal alignment of deployed systems (Section 4.3).\n\nIn this paper, we make four contributions:\n\n1. Definition and context . In Section 2, we outline the core focus of legal alignment and its broader context.\n2. Rationale . In Section 3, we describe the institutional, normative, and societal motivations for pursuing legal alignment.\n3. Implementation . In Section 4, we explore practical implementations of legal alignment, including empirical evaluations, technical design interventions, and institutional frameworks.\n4. Open questions . In Section 5, we canvass open questions for researchers entering this emerging field.\n\n## 2 What is legal alignment?\n\nLegal alignment is the field of research that aims to support safe and ethical AI by designing AI systems to operate in accordance with legal rules, principles, and methods . In particular, legal alignment seeks to offer a set of legitimate, principled, and practical tools for better aligning AI systems with human values and interests (Russell, 2019; Christian, 2020). Law and legal institutions can be harnessed to help address the interrelated problems of (1) specifying what behavior is normatively desirable (Gabriel, 2020; Hadfield, 2026) and contextually appropriate for AI systems (Kasirzadeh and Gabriel, 2023; Leibo et al., 2024) and (2) technically steering the behavior of AI systems to comply with those specifications (Ngo et al., 2024; Anwar et al., 2024; Bengio et al., 2025). Importantly, while legal alignment breaks new normative and technical ground, it does not aim to replace or supersede other alignment approaches, but to develop a new cluster of methods that support and complement existing approaches to designing safe and ethical AI.\n\n1 Our use of terms like 'reason' and 'act' with respect to AI systems can inadvertently anthropomorphize these systems (Calo, 2015; Placani, 2024). Following Buyl et al. (2025), we use these terms solely for simplicity of exposition. Relatedly, our analysis does not require or imply treating AI systems as legal persons (Section 2).\n\n## 2.1 Core focus\n\nThe field of legal alignment begins with the insight that law and AI alignment share much in common. Both confront complex principal-agent problems (Hadfield-Menell and Hadfield, 2018), enduring issues of authority, delegation, and incentive design (Kolt, 2025; Boeglin, 2026), questions of how individual and institutional goals can change over time (Gabriel and Keeling, 2025), and the challenge of decision-makers faithfully interpreting and applying high-level principles in novel circumstances (Caputo, 2025; He et al., 2025). Recognizing these parallels, computer scientists and legal scholars have proposed leveraging the content, methods, and structure of law to develop new approaches to AI alignment (Etzioni and Etzioni, 2016a,b; Nay, 2022; Desai and Riedl, 2025; O'Keefe et al., 2025).\n\nPathway 1: Legal rules and principles as a source of normative content for AI alignment . The substance of legal rules and principles developed through legitimate processes and institutions can serve as a target for alignment. Legally aligned AI systems would be those systems that comply with relevant law when making decisions and taking actions, as shown in Table 1. Concretely, this approach could involve designing AI systems that adhere to the legal rules that would apply as if such systems were human actors. For example, a legally aligned AI system would refrain from making fraudulent representations when marketing a product and would respect copyright law when building a website-irrespective of whether the relevant laws in fact apply to the AI system in question. California's Transparency in Frontier Artificial Intelligence Act (SB-53) offers some support for this approach, referring to certain risks from frontier models '[e]ngaging in conduct ... [which] if ... committed by a human, would constitute the crime of murder, assault, extortion, or theft' (California Legislature, 2025).\n\nAnother approach involves amending the law so that it actually applies to AI systems and imposes legal obligations on them (O'Keefe et al., 2025). This may require treating AI systems as legal actors (see Section 2.2), comparable to corporations or other non-natural legal persons that can be the subject of legal rights and duties; presently, AI systems do not fulfill this criterion (American Law Institute, 2006; Kolt, 2025). In either case, legal alignment will need to contend with the issue that certain human-centric legal concepts in both civil law and criminal law (e.g., intent, mens rea) are not necessarily appropriate in the context of AI systems (Nerantzi and Sartor, 2024). Related issues arise when attempting to integrate concepts from international law (e.g., human rights) into the design of AI systems (Prabhakaran et al., 2022; Bajgar and Horenovsky, 2023; Maas and Olasunkanmi, 2025), as discussed in Section 5.2.\n\nA further issue concerns the relevant jurisdiction , that is, determining the country or region whose laws a particular AI system should be aligned with in a particular context (Chopra and White, 2011). Options include, for example, the jurisdiction in which an AI system operates, the location of its\n\nTable 1: Decisions and options for aligning AI systems with legal rules and principles (Pathway 1).\n\n|                                   | Design decisions                                                                          | Potential options                                                                                                             |\n|-----------------------------------|-------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|\n| Jurisdiction and conflict of laws | Determine the relevant jurisdiction based on established principles of conflict of laws   | • Jurisdiction in which the AI system operates or where its servers are located • Location of AI developer, deployer, or user |\n| Substantive areas of law          | Select the substantive areas of law and legal rules with which AI sys- tems should comply | • Private law (agency law, tort law, property law) • Public law (criminal law, constitutional law, international law)         |\n| Interpretive method               | Decide on the method for applying legal rules to concrete scenarios                       | • Textualism, originalism, formalism • Purposivism, intentionalism                                                            |\n| Assurance level                   | Stipulate the level of assurance for compliance with legal rules                          | • Probabilistic specifications • Formal guarantees                                                                            |\n| Enforcement mechanism             | Establish mechanisms for enforc- ing legal compliance                                     | • Technical interventions • Legal sanctions                                                                                   |\n\nservers, the jurisdiction of the system's developer or deployer, as well as the location of persons affected or likely to be affected by a system's actions (see Table 1). Additionally, questions arise regarding who is authorized to determine the relevant jurisdiction: legislators, courts, developers, or users. While the distinctive features of AI systems complicate these questions and will need to be addressed in future work, existing principles based on conflict of laws could provide a useful starting point (Briggs, 2024; Collins and Harris, 2025).\n\nPathway 2: Legal theory and interpretation as a guide for AI reasoning and decision-making . Although law can specify how AI systems should act in a variety of circumstances, there will inevitably be situations in which law or other safety specifications provide an incomplete guide for action (Raz, 1971). Methods of legal decision-making-particularly for reasoning about the interpretation and application of existing laws or rules in new circumstances-can potentially be adapted to help AI systems make sounder and safer decisions when confronting novel scenarios (Caputo, 2025; He et al., 2025). Legal theory, even independent of the particular legal content to which it is ordinarily applied (Hart, 1982), could be leveraged to help delineate and possibly implement appropriate forms of reasoning for AI systems.\n\nPotential avenues for research include drawing on legal positivist arguments for analogical reasoning that ground decision-making in the concrete facts of prior cases and precedent (Sunstein, 1993; Brewer, 1996). While AI systems cannot yet effectively engage in the necessary complex normative judgments, these methods are already inspiring case-based reasoning approaches to alignment that seek to produce repositories of prior decisions to guide future actions of AI systems (Feng et al., 2023; Chen and Zhang, 2025). Another avenue proposes using formal and textualist tools such as legal canons of interpretation and methods of statutory construction that delineate which sources a decision-maker may refer to and establish a framework for reasoning about those sources (Schauer, 2009; Scalia and Garner, 2012). Used appropriately, these tools could help address ambiguity in the guidance provided by legal rules or alignment specifications such as the principles in Anthropic's Constitutional AI (He et al., 2025). A further avenue could draw on interpretivist and purposivist legal theory that constrains legal decision-making through recourse to higher-level general principles of morality such as justice and fairness (Dworkin, 1986; Barak, 2011). Although AI systems cannot presently engage in the requisite reasoning to effectively operationalize this approach, likely improvements in the capabilities of AI systems suggest that, in time, they may be able to contribute to and participate in such processes (Caputo, 2025).\n\nPathway 3: Legal concepts and institutions as a structural blueprint for AI alignment . Legal concepts and institutions developed to grapple with age-old structural challenges arising in human relationships can provide a high-level blueprint for tackling problems of AI alignment. In particular, law's ability to facilitate trust and cooperation in the face of uncertainty and incomplete information (Hadfield and Weingast, 2012, 2014) can illuminate potential methods for designing AI systems that operate with greater reliability and predictability (Nay, 2022; Boeglin, 2026). For example, agency law addresses principal-agent problems by carefully circumscribing the authority granted to agents (e.g., employees) (American Law Institute, 2006). In addition to requiring that agents comply with instructions provided by their principal, agents can sometimes become obligated to seek clarification from their principal. Agency law also clarifies the circumstances in which agents can delegate their own duties to sub-agents and delineates the authority and discretion that can be exercised by sub-agents (Kolt, 2025; Riedl and Desai, 2025).\n\nAnother legal structure that researchers have proposed repurposing for AI alignment is the fiduciary duty of loyalty, which would require that AI systems behave strictly in the best interests of their users while avoiding wrongdoing (Aguirre et al., 2020; Benthall and Shekman, 2023). A further structure that could inform the alignment of AI systems is the allocation of information rights and control rights afforded to shareholders in a corporation (Velasco, 2006; Armour et al., 2017). Adapted appropriately, these and other legal structures could support the development of new approaches to AI alignment, or at the very least expose the shortcomings and limitations of current approaches.\n\n## 2.2 Broader context\n\nWhile legal alignment has only recently begun to emerge as a distinct field, the broader relationship between law and AI dates back many decades. In fact, the relationship between the two fields is as old as AI itself-dating back to Asimov (1942)'s 'Three Laws of Robotics' and Turing (1950) likening the dynamic operation of machine learning to the adaptive nature of the U.S. Constitution. Unpacking\n\nthis relationship involves studying the current role of law in AI development and deployment, the legal capabilities of AI systems, and the legal frameworks for regulating actors that build and use AI. Although none of these is strictly part of legal alignment, each may help advance research in legal alignment and pursue its implementation in practice.\n\nLegal resources in AI development and deployment . Law is already embedded to varying degrees in the development and deployment of contemporary AI systems, including across multiple stages in the production of frontier models:\n\n- Pre-training datasets contain extensive collections of case law, legislation, patents, treatises, and other legal texts, and are themselves subject to jurisdiction-specific laws (including copyright and data privacy law) (Henderson et al., 2022; Soldaini et al., 2024).\n- Post-training personnel including researchers and data collectors and producers who work to refine AI systems are subject to legal obligations, including employment agreements, contractual terms of service, and non-disclosure agreements.\n- Model specs stipulate that systems must comply with applicable laws (OpenAI, 2025b), evaluations of which are documented in system cards and further supported by system-level guardrails to prevent illicit activities (OpenAI, 2025a).\n- Alignment techniques -most prominently Constitutional AI-incorporate legal and quasilegal texts, including principles based on the Universal Declaration of Human Rights and Apple's Terms of Service (Anthropic, 2023).\n- Output filters and classifiers such as Llama Guard (Meta, 2025) use hazard taxonomies that are grounded in legal categories, including the MLCommons benchmark that contains hazards relating to violent crime, defamation, and intellectual property (Ghosh et al., 2025).\n- Usage policies prohibit using AI systems to engage in or facilitate activities that would violate relevant law (e.g., Google, 2025a).\n\nStudying these resources and their effect on the operation of AI systems is necessary both to develop empirical evaluations that measure the legal alignment of current systems (see Section 4.1) and to design technical and institutional interventions that improve the legal alignment of future systems (see Sections 4.2-4.3).\n\nLegal capabilities and reasoning in AI systems . Contemporary AI systems are increasingly being applied to a wide array of legal tasks with varying degrees of reliability. These include contractual interpretation (Kolt, 2022; Hoffman and Arbel, 2024), statutory research (Surani et al., 2025), information retrieval and reasoning (Zheng et al., 2025; Han et al., 2025), and judicial decisionmaking (Choi, 2025; Posner and Saran, 2025). Methods for evaluating the legal capabilities of AI systems have improved (Chalkidis et al., 2022; Guha et al., 2023), extending beyond multiple-choice questions such as bar exams (Martínez, 2024; Fan et al., 2025) to include randomized controlled trials with human subjects (Schwarcz et al., 2025)-revealing both the opportunities and shortcomings of using current AI systems in the legal domain (Purushothama et al., 2025; Pruss and Allen, 2025; Grimmelmann et al., 2025; Waldon et al., 2025).\n\nEvaluations of the legal capabilities of AI systems can help support research in legal alignment because understanding and reasoning about law are prerequisites for upholding the law and responsibly engaging with legal institutions and processes. Current evaluations, however, focus primarily on the legal capabilities of AI systems, that is, the scope and quality of their execution of legal tasks. With few exceptions (see Section 4.1), current evaluations do not generally measure legal alignment , including, for instance, the extent to which agentic AI systems comply with relevant law when performing tasks across diverse domains (e.g., avoiding fraudulent misrepresentation when producing advertisements), the methods systems use to interpret and apply quasi-legal rules in their safety specifications (e.g., principles in Constitutional AI and model specs), or the approach of AI systems to exercising legal power within institutional constraints (e.g., brokering multiparty negotiated resolution subject to judicial approval).\n\nLegal regulation of actors that build or use AI . The legal regulation and governance of AI has attracted vast attention among lawyers, legal scholars, and policymakers-comparable to, and likely exceeding, interest in cyberlaw and governance during the early years of the internet (Johnson and Post, 1996; Reidenberg, 1998; Lessig, 1999; Benkler, 2002; Wu, 2003; Goldsmith and Wu, 2006; Zittrain, 2008). The objectives of different AI governance initiatives across different jurisdictions are\n\ndiverse (Kaminski, 2023; Kolt, 2024) and sometimes conflicting (Engler, 2023), as are the institutional mechanisms used to achieve those objectives (Guha et al., 2024; Arbel et al., 2024). While the EU AI Act (European Parliament, 2024) is perhaps the most globally prominent regulatory instrument focused specifically on AI (Kaminski and Selbst, 2025), particularly given the United States has not passed comparable federal legislation, the U.S. legal system contains many other mechanisms for governing AI technology, including a variety of state laws (Sentinella and Zweifel-Keegan, 2025), such as California's Transparency in Frontier Artificial Intelligence Act (SB-53) (California Legislature, 2025), corporate governance regimes (Tallarita, 2023), and background liability under tort law (Cuéllar, 2019; Lemley and Casey, 2019; Abbott, 2020; Henderson et al., 2023; Ayres and Balkin, 2024; Ramakrishnan et al., 2024; Weil, 2024; Williams et al., 2025b).\n\nAlthough such legal regulations aim to govern AI, they are notably distinct from legal alignment, and are not oriented toward the full range of concerns that motivate legal alignment. Legal regulation generally entails imposing requirements on actors that produce, disseminate, and use AI systems. By contrast, legal alignment entails designing AI systems to themselves operate in accordance with legal rules, principles, and methods. Legal alignment and legal regulation, however, are closely related and at times overlapping, including because legal regulation may help facilitate the implementation of legal alignment in practice (see Section 4.3). By way of further clarification:\n\n- Legal alignment does not require a particular regulatory framework . Legal alignment principally uses existing law to guide the decision-making and actions of AI systems and, accordingly, does not necessarily require regulatory reform (however, as discussed in Section 4.3, regulation could support the assessment and oversight of legal alignment).\n- Legal alignment is not primarily concerned with allocating liability . Although responsible AI developers and deployers could be expected to implement legal alignment in order to support AI systems operating safely and ethically, legal alignment is not primarily focused on holding those or other actors liable for harms caused by AI systems.\n- Legal alignment does not imply granting legal rights to AI systems . Designing AI systems to comply with existing legal rules or use legal principles and methods in their decisionmaking does not necessarily require granting legal rights to AI systems, such as private law rights (Salib and Goldstein, 2025a,b) or legal personhood (Solum, 1992; Chesterman, 2020; Forrest, 2024; Novelli et al., 2025; Leibo et al., 2025).\n\n## 3 Why pursue legal alignment?\n\nThe rationales for pursuing legal alignment can be organized into four broad clusters, each of which touches on different aspects of law and its potential role in addressing problems of AI alignment: (1) the institutional legitimacy and process of law; (2) the structural features of law; (3) the responsiveness of legal alignment to safety and governance challenges from AI; and (4) the practical and societal feasibility of legal alignment. As noted in Section 1, we take an ecumenical approach to law and fundamental legal questions, engaging with different, and sometimes conflicting, legal perspectives and theories without seeking to resolve the tensions between them here. Additional limitations and open questions are discussed in Section 5.\n\n## 3.1 Institutional legitimacy and process\n\nLegal rules are developed through legitimate processes and institutions. A defining feature of legal rules is that they are produced through politically legitimate processes and institutions, at least in democratic societies governed by the rule of law (Tamanaha, 2004; Bingham, 2007; Waldron, 2016). The authority and stability of legal rules can be traced to the broad-based support for the mechanisms that create and enforce law (Tyler, 2006; Hadfield and Weingast, 2012, 2014). The legitimacy of law can also be grounded in social acceptance of the content of legal rules and principles, which represent society's best attempt to resolve disagreements and translate diverse perspectives into concrete directives that govern behavior and provide criteria for evaluating the normative appropriateness of conduct (Hart, 2012). While there may be no consensus on the moral correctness of an action, there often exist formal legal processes for determining whether or not an action is lawful (Rawls, 1993; Schauer, 2009). These important features of law are expressed in legal rules that operate at different levels of specificity, from granular regulations to higher-level values (Dworkin, 1986; Lessig,\n\nTable 2: Summary of core rationale and motivations for pursuing legal alignment (Section 3).\n\n| Institutional legitimacy and process               | • Legal rules are developed through legitimate processes and institutions. • Law aims to balance competing considerations. • Lawmaking seeks to be transparent and publicly accountable. • Legal institutions facilitate explicit reason-giving and justification.                                  |\n|----------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Structural features of law                         | • Law is concrete, granular, and rooted in real-world contestation. • Legal interpretation can clarify the meaning of rules. • Legal rules are role-specific and context-sensitive. • Legal rules can adapt and change over time.                                                                   |\n| Responsiveness to safety and governance challenges | • Legal alignment can mitigate risks from malicious use and accidents. • Legal alignment can address systemic and multi-agent risks. • Legal alignment is vital to protecting the rule of law and preventing abuse of power. • Legal alignment supports and complements other alignment approaches. |\n| Practical and societal feasibility                 | • Improvements in legal technology can support legal alignment. • Societal stakeholders generally expect AI systems to comply with law. • Legal alignment is compatible with different perspectives on AI.                                                                                          |\n\n1993; Sunstein, 1995). Legal alignment proposes incorporating these various forms of law into the principled frameworks and specifications that steer the decision-making and conduct of AI systems.\n\nLaw aims to balance competing considerations. When operating properly, legal rules and structures provide a framework for public governance in the face of divergent social values and interests. Plural perspectives can be mediated through rights, rules, standards, and meta-principles that allow for the resolution of disputes. Disagreements can be resolved with reference to broad constitutional principles or through narrow applications of precedent (Sunstein, 1993). Democratic publics can determine (albeit indirectly) which values should govern them and instantiate those values in law, providing a guide for how to apply laws in cases of ambiguity (Dworkin, 1986). When conflicts arise between fundamental values, standards, balancing tests, and proportionality analyses enable law to weigh competing values and resolve conflicts in a socially acceptable and politically legitimate manner (Habermas, 1996). Existing AI alignment approaches largely lack this ability, and struggle to specify how to reconcile competing normative considerations. For example, documents like Claude's Constitution (Anthropic, 2023) contain values that conflict with one another, but provide little guidance for how to resolve such conflict. ChatGPT's Model Spec creates a hierarchy of rules according to its 'chain of command' (OpenAI, 2025b), but there remain difficult questions when a system may need to prioritize certain rules or values over others (Liu et al., 2025). Leveraging law's time-tested ability to specify meta-rules for resolving such conflicts, and the institutional structures that devise and enforce such rules, could help fill this gap.\n\nLawmaking seeks to be transparent and publicly accountable. The process of lawmaking in democratic societies is, at least in principle, designed to be transparent, accountable, and open to public participation. Members of the public can, for example, communicate with legislators, comment on administrative rulemaking, and serve as jurors in court. Ideally, these institutional structures facilitate conditions that promote lawmakers acting in the public interest (Madison, 1788; Mashaw, 2006; Bovens, 2007). In contrast, with few exceptions (Huang et al., 2024; Eloundou et al., 2025), current alignment techniques do not enable meaningful public participation (Sloane et al., 2022) and are not publicly accountable (Abiri, 2025; Lazar, 2025). For the most part, alignment optimizes for reductionist proxies of socially desirable behavior, such as 'helpfulness, honesty, and harmlessness' (Askell et al., 2021) and appealing AI 'character traits' and 'personality' (Lambert, 2025; Maiya et al., 2025), which can sometimes result in socially noxious sycophantic systems (OpenAI, 2025; Cheng et al., 2025) or be hijacked to maximize user engagement (Stray et al., 2024; Williams et al., 2025a; El and Zou, 2025). Additionally, many of the leading reward models that are used in post-training to shape the behavior of widely deployed AI systems are not publicly available (Lambert et al., 2025; Malik et al., 2025). Without robust transparency and\n\npublic involvement comparable to that in the legal system, the highly consequential choices in AI alignment will remain opaque and closed to public scrutiny.\n\nLegal institutions facilitate explicit reason-giving and justification. In many contexts law requires that decision-makers follow procedural due process and provide reasons to justify their decisions. Reason-giving performs two main functions. First, it creates legitimacy for practical goals, such as facilitating oversight of decisions, and moral purposes, such as respecting human autonomy and rationality (Schauer, 1995; Habermas, 1996). For example, in American administrative law, the legality of a decision will turn on the reasons provided for that decision (Administrative Procedure Act, 1946; Stack, 2007). Second, the process of reason-giving can partially make up for the public's limited ability to oversee its agents' decisions in real time. This procedural solution to principal-agent problems in which agents have broad discretion and specialized expertise is used to govern a wide range of actors in the legal system, including administrative agencies, corporations, and trustees (Friendly, 1975). Legal institutions that facilitate reason-giving and justification could serve as a blueprint for developing mechanisms to enable human oversight over AI systems that defy human understanding but could nevertheless be constrained by institutional or informational requirements (Lazar, 2024). Technical methods for AI explainability and interpretability like chain-of-thought monitoring (Korbak et al., 2025) could help, but these methods are often unreliable and fail to adequately characterize the reasons for the outputs produced by AI systems (Barez et al., 2025b). Requiring that AI systems provide legally valid justifications for their decisions (Hadfield, 2021), as we expect from human decision-makers (Citron, 2008; Deeks, 2019), could help ensure advanced AI systems act appropriately and safely even where direct human oversight of their actions is no longer practical (Bowman et al., 2022).\n\n## 3.2 Structural features of law\n\nLaw is concrete, granular, and rooted in real-world contestation. Law is mostly concerned with the resolution of concrete questions of how to act in society (Holmes, 1881). Consequently, law must be sufficiently detailed and complete to operate effectively wherever applied, or contain methods that enable its reasoned elaboration (Fallon, 1994). Legal rules are tested in courts through real-world disputes about the law's meaning, the resolution of which enables the law to become more complete over time as precedent accumulates. This iterative process of articulating the law enables legal rules to remain more closely tethered to the concrete reality of contemporary material and social conditions (Atiyah, 1992; Raz, 2019). By comparison, many existing AI alignment approaches are less concrete and granular. Safety specifications of AI systems, for example, are often short documents that contain only scant concrete applications (Anthropic, 2023) when compared to those found in judge-made law. While this is beginning to change as model specifications grow in length and complexity (OpenAI, 2025b), the process for producing these specifications differs markedly from the process of producing law (Abiri, 2025; Lazar, 2025). By drawing on the much richer set of rules, cases, and institutional processes in law (Schauer, 1991, 2009), legal alignment could incorporate into the design of AI systems both the granular normative content of legal rules and the law's sophisticated approaches to resolving disagreement in the face of real-world dilemmas.\n\nLegal interpretation can clarify the meaning of rules. The articulation of rules and principles in natural language invariably creates ambiguity (Hart, 2012; Dworkin, 1986). Such ambiguity can make it difficult to apply laws, especially in novel cases. The law, however, has time-tested tools that, when used appropriately, can help address ambiguity. For example, legal decision-makers, particularly judges, construct meaning through various interpretive methodologies and the creation of precedent that can subsequently be used to resolve future cases (Schauer, 1987; Sunstein, 1993; Fallon, 1994; Brewer, 1996; Barak, 2011; Scalia and Garner, 2012). In contrast, current approaches to AI alignment do not generally provide robust tools for resolving ambiguity in the interpretation of safety specifications (Song, 2025). For example, guidance on how AI systems should interpret an alignment principle like 'uphold fairness' is limited to just a few pithy scenarios (OpenAI, 2025b). Highly abstract principles like 'do what's good for humanity' can in some circumstances effectively steer the actions of AI systems, but researchers acknowledge their ambiguity and indeterminacy (Kundu et al., 2023). Legal alignment would, as explored in recent studies (Caputo, 2025; He et al., 2025), help address this problem by applying the law's robust and comparatively transparent methods of interpretation (Sunstein, 2001; Cuéllar, 2019) to clarify the meaning of AI safety specifications.\n\nLegal rules are role-specific and context-sensitive. Different social contexts call for different kinds of behavior. The law's response is twofold. First, law contains different sets of rules for governing actors in different roles, such as fiduciaries, company directors, and government officials. Second, law can flexibly apply existing rules to new circumstances (Holmes, 1897; Dworkin, 1986; Lessig, 1993). Legal reasoning begins with identifying the body of rules that govern a particular situation, and subsequently proceeds to determine how to comply with those rules (Levi, 1949). For example, a lawyer must determine her obligations to her client, to her firm, and to the legal system, and then act in such a way as to avoid conflicts between them (American Bar Association, 2020). Law also recognizes that rules are necessarily incomplete and, accordingly, establishes mechanisms and institutions for applying general rules to specific circumstances (Hadfield, 2026). Such sensitivity to context is critical for developing safe and ethical AI, particularly given the diverse normative conditions in which AI systems operate (Kasirzadeh and Gabriel, 2023; Sarkar et al., 2024). Legally aligned AI systems would, by referring to relevant legal rules, roles, and responsibilities, act differently in different contexts (O'Keefe et al., 2025; Boeglin, 2026). For instance, an AI system that negotiates retail purchases on behalf of consumers would be subject to different rules than an AI system that performs business functions in a large enterprise, or an AI system deployed within a government agency.\n\nLegal rules can adapt and change over time. Laws can be amended, repealed, or reinterpreted in response to changes in social, economic, or technological conditions (Holmes, 1897; Lessig, 1993). Deliberative lawmaking processes and debates over the real-world effects of enacted laws provide ongoing social input into the legal system (Habermas, 1996). These features of lawmaking empower the public to steer the content and operation of law, enabling it to respond to new societal challenges. Law can also operate on its own structure by changing its 'secondary rules' or 'rules of the game' (Hart, 2012; Scalia and Garner, 2012). For example, new laws can alter the rules of evidence used at trial or clarify the rulemaking authority of different institutions. The upshot of law's dynamic content and flexible interpretive methods with respect to AI alignment is that the target of legal alignment-legal rules and principles-is updated 'automatically' through existing processes for enacting, amending, and repealing laws (O'Keefe et al., 2025), as well as through accepted methods of legal interpretation (Caputo, 2025; He et al., 2025). As AI systems advance and diffuse in a growing diversity of scenarios, the responsiveness of law and legal methods could, notwithstanding the rapid pace of change in AI technology, play an increasingly central role in alignment (Gabriel and Keeling, 2025).\n\n## 3.3 Responsiveness to safety and governance challenges\n\nLegal alignment can mitigate risks from malicious use and accidents. Many risks that arise from the malicious use of AI systems or accidental harms involve illegal activity (Weidinger et al., 2022; Bengio et al., 2024, 2025), such as civil wrongs (e.g., negligence) or criminal offenses (e.g., theft) (King et al., 2020; Lior, 2024). Legal alignment that prevents AI systems from engaging in legal wrongdoing could help mitigate such risks (O'Keefe et al., 2025). For instance, legally aligned AI systems operating in financial markets would not engage in insider trading, a form of illegal conduct already exhibited by some current systems (Scheurer et al., 2024). Similarly, a legally aligned AI coding agent would not engage in unlawful computer hacking, one of the most prominent risks from computer-use agents (Zhang et al., 2025; Zhu et al., 2025b). By explicitly incorporating legal standards into the safety specification of AI systems, legal alignment would preclude systems from engaging in many of the most harmful behaviors that could be exploited by malicious actors or otherwise cause grave harm.\n\nLegal alignment can address systemic and multi-agent risks. As AI systems are deployed more widely across the economy (Hadfield and Koh, 2025), qualitatively new risks could arise due to the scale of deployment (Uuk et al., 2024; Hacker et al., 2025) and interactions between different systems (Hammond et al., 2025; Tomasev et al., 2025). For example, AI systems may collude with one another to fix prices (Calvano et al., 2020), or compete destructively and bring down entire markets (Kirilenko et al., 2017). While legal regulation that targets systemic risks through disclosure requirements and other traditional governance mechanisms can sometimes help mitigate these risks (Schwarcz, 2008), designing AI systems to themselves follow relevant law might be more effective. Rather than relying solely on humans to intervene on a case-by-case basis-such as bringing antitrust action to combat algorithmic collusion-legal alignment could potentially reduce the prospect of AI systems engaging in illegal conduct in the first place, provided the legal system targets the underlying conduct\n\nof concern. In addition, by using existing (human-oriented) laws to steer AI systems, legal alignment could function as a throttle on the speed and scale at which AI systems operate, thereby enabling humans to better monitor their actions and, where appropriate, intervene to mitigate large-scale risk (Zittrain, 2024). For further discussion and limitations, see Section 5.2.\n\nLegal alignment is vital to protecting the rule of law and preventing abuse of power. The rule of law seeks to ensure that all actors in society are subject to, and accountable under, publicly promulgated, equally applied, and non-arbitrary laws (Dicey, 1959; Fuller, 1969; Raz, 1979b). In addition to ensuring that law protects human dignity and prevents abuses of power, the rule of law enables people and institutions to coordinate in pursuit of social and economic goals. AI could undermine the rule of law in various ways (Huq, 2024; Smuha, 2024; Brownsword, 2025). If deployed in high-stakes settings, AI systems such as language models that operate stochastically (i.e., non-deterministically) could threaten the rule of law by increasing the level of arbitrariness in decisions (Cooper et al., 2022a; Nouws and Dobbe, 2024). These risks might be exacerbated if institutions and individuals delegate increasingly consequential decisions to AI systems (Kulveit et al., 2025; Summerfield et al., 2025; Kasirzadeh, 2025). At the same time, organizations that control the design and distribution of AI systems could, like platform companies (Zittrain, 2008; Gillespie, 2018; Douek, 2022), exercise arbitrary power over users of the technology and, by extension, all persons affected by it (Lazar, 2025; Kapoor et al., 2025a). In the extreme, groups with access to sufficiently capable AI systems could pose new threats to democratic institutions (Barez et al., 2025a), including by staging AI-enabled coups (Davidson et al., 2025). Legal alignment is critical to mitigating these risks. Just as human agents such as corporate officers have an overriding duty to obey the law and thereby prevent dangerous abuses of power, designing AI systems to comply with the substance and procedure of legal rules could help assuage concerns about these systems acting arbitrarily or being exploited to unlawfully subvert democratic institutions.\n\nLegal alignment supports and complements other alignment approaches. Legal alignment could bolster existing efforts to tackle normative and technical aspects of the alignment problem. Most straightforwardly, the substance of legal rules could augment the content of current safety and ethical specifications contained in Constitutional AI (Bai et al., 2022b) and model specs (OpenAI, 2024a), as well as provide institutionally legitimate content for 'full-stack alignment' (Lowe et al., 2025) and possibly the diverse norms demanded by 'pluralistic alignment' (Sorensen et al., 2024a,b). Meanwhile, the processes and mechanisms for producing and deliberating over law could provide guidance for sourcing and refining alignment principles (Huang et al., 2024; Eloundou et al., 2025) and developing AI-supported deliberative processes (Bakker et al., 2022; Tessler et al., 2024). Using legal institutions as a blueprint to structure and govern the interactions between AI systems could also advance work in the field of cooperative AI, which seeks to promote prosocial coordination between AI systems, human beings, and broader social structures (Dafoe et al., 2020, 2021). In addition to generally enabling actors to cooperate without fear of counterparty defection or punishment (North et al., 2009; Acemoglu and Robinson, 2012), law-and specifically private law rights-could enable humans and AI systems to make credible commitments that promote strategic stability and safety (Salib and Goldstein, 2025a,b).\n\n## 3.4 Practical and societal feasibility\n\nImprovements in legal technology can support legal alignment. Advances in language modeling have dramatically improved the legal capabilities of AI systems. Unlike prior efforts to computerize law that relied on the formalization of legal rules (Susskind, 1987; Gardner, 1987; Rissland, 1990; Bench-Capon et al., 2012), language models have enabled AI systems to reason about law in the natural language in which law is constituted and communicated. As discussed in Section 2.2, contemporary AI systems can now perform a growing range of legal tasks (Guha et al., 2023), including legal information retrieval and reasoning (Zheng et al., 2025; Han et al., 2025), albeit to varying degrees of reliability. These developments have been supported by the collection of large swathes of legal data that can be used in model training (Henderson et al., 2022), general-purpose advances in AI research such as reinforcement learning from verifiable rewards (OpenAI, 2024b; Lambert et al., 2024), and investments of legal technology companies seeking to automate aspects of commercial legal work (Schwarcz et al., 2025). Taken together, improvements in legal technology have produced AI systems that can learn and understand law in increasingly nuanced ways (Doyle and Tucker, 2025; Boeglin, 2026). Despite their limitations (discussed in Section 2.2), AI-based legal\n\ntechnologies are beginning to exhibit the capabilities that are a prerequisite for designing AI systems that can adhere to the content of law and use legal methods to make sounder and safer decisions.\n\nSocietal stakeholders generally expect AI systems to comply with law. Users, developers, and policymakers all have strong interests in AI systems acting in accordance with existing legal rules, provided those rules are themselves enacted in accordance with legitimate institutional processes. The general expectation that AI systems respect legal rules and norms can be seen in prominent safety specifications that explicitly require legal compliance (OpenAI, 2025b) and incorporate legal principles (Anthropic, 2023), as discussed in Section 2.2. Users and developers may also prefer legally aligned systems that refrain from engaging in unlawful activities in order to reduce their prospects of liability for harms arising from such activities (Ayres and Balkin, 2024; O'Keefe et al., 2025). This interest is particularly salient in the case of developers that commit to defend customers against certain third-party claims arising from unlawful activities of their AI systems (Smith, 2023; Microsoft, 2024). Lawmakers, meanwhile, may consider legal alignment necessary for enforcing the law and achieving its societal objectives as AI systems occupy increasingly important roles in the economy (Hadfield and Koh, 2025; Tomasev et al., 2025). While the particular motivation for legal alignment differs between stakeholders, there could nevertheless emerge a broad consensus on the need to conduct further research on studying and implementing legal alignment.\n\nLegal alignment is compatible with different perspectives on AI. Perspectives on the future of AI differ dramatically. Some researchers predict that AI systems will soon demonstrate broadly superhuman capabilities that lead to unprecedented societal transformation and catastrophic risk (Kokotajlo et al., 2025). Other researchers predict that the impact of AI systems will be more gradual, mediated by bottlenecks to real-world deployment and adoption comparable to those that affect other technologies (Narayanan and Kapoor, 2025). Legal alignment appeals to both of these perspectives, as well as other views on the anticipated trajectory of AI technology. If, on the one hand, AI systems were to develop rapidly and pose extreme risks (Bengio et al., 2024), then legal alignment would help protect against potentially catastrophic harms by ensuring systems comply with existing laws and more effectively operationalize their safety specifications. If, on the other hand, AI systems were to develop and diffuse more gradually (Kasirzadeh, 2025), then legal alignment would mitigate ongoing harms arising from AI systems engaging in unlawful activity, such as making discriminatory decisions, generating non-consensual intimate imagery, and enabling fraudulent online scams. These complementary objectives indicate that the field of legal alignment does not hinge on a particular perspective on the nature and pace of AI progress, but invites a diverse coalition to collaborate on a broadly appealing and inclusive research agenda (Gyevnár and Kasirzadeh, 2025).\n\n## 4 Implementation\n\nThe implementation of legal alignment involves a combination of: (1) empirical evaluations to measure legal alignment; (2) technical interventions to improve legal alignment; and (3) institutional frameworks to facilitate the adoption and refinement of legal alignment. These areas of focus are independently useful and can also support each other in important ways. For example, conducting evaluations that shed light on the legal compliance of deployed AI systems is valuable irrespective of whether such evaluations are mandated by regulation. Meanwhile, institutional frameworks that, for instance, require developers to disclose in-use model specs could inform work on designing technical interventions that provide stronger assurances of legal alignment.\n\n## 4.1 Empirical evaluations\n\nEmpirical evaluations of legal alignment aim to serve multiple purposes. First, evaluations can identify and characterize legal misalignment : circumstances in which AI systems fail to comply with law or apply legal principles inappropriately, or harmfully. Second, evaluations can assess the effectiveness of technical interventions aimed at improving legal alignment. That is, developers need metrics that benchmark and incentivize investing in the legal alignment of their AI systems. Third, the publication of evaluation results-particularly if they demonstrate legal misalignment-can empower users to demand legal alignment or refrain from using legally misaligned systems , particularly in sensitive or high-stakes settings. Fourth, evaluation results and ensuing public responses can prompt policymakers to intervene , such as by establishing processes that require developers to demonstrate\n\nTable 3: Key steps to implementing legal alignment in practice: empirical evaluations , technical interventions , and institutional frameworks (Section 4).\n\n<!-- image -->\n\n| 1. Empirical evaluations                                                                                                                                                                                                                                                                                                                                    | 2. Technical interventions                                                                                                                                                                                                                                                                                                                                                 | 3. Institutional frameworks                                                                                                                                                                                                                                                                                                                             |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Develop methods to empirically measure legal alignment                                                                                                                                                                                                                                                                                                      | Explore technical interventions to improve legal alignment                                                                                                                                                                                                                                                                                                                 | Design institutional frameworks to facilitate legal alignment                                                                                                                                                                                                                                                                                           |\n| Variable of interest: • Compliance with the content of legal rules and principles • Reasoning and decision-making regarding legal rules and safety specifications Evaluation methodology: • Quantitative agentic benchmarks and qualitative expert review • Human studies and baselines • Adversarial methods and red-teaming • Analysis of real-world data | Sites of intervention: • Pre-training datasets • Post-training processes (model specs, RLHF, RLAIF) • Scaffolding (system prompts, input/output filters, tool use) Legal resources: • Legal texts (case law, statute, administrative rules, treatises) • Legal data annotation processes • Legal compliance deployment and use policies • Legal search and retrieval tools | Documentation and disclosure: • Right to access production model spec, system prompt, legal data and decision designs • System identification and registration Oversight and enforcement: • Pre-deployment legal alignment testing and post-deployment monitoring • Safety cases and certification • Incident reporting for cases of legal misalignment |\n| /group Independent academic experts and civil society organizations                                                                                                                                                                                                                                                                                         | /group System developers, deployers, external stakeholders                                                                                                                                                                                                                                                                                                                 | /group Government actors, regulators, policymakers                                                                                                                                                                                                                                                                                                      |\n\nthat AI systems in deployment are legally aligned (subject to free speech protections, including under the First Amendment in the United States, as discussed in Section 5.1).\n\nVariable of interest. The focus of evaluation will depend on the specific aspect of legal alignment being measured and the particular claims being tested (Salaudeen et al., 2025). Evaluations assessing the legality of actions taken by AI systems will need to investigate whether systems comply with relevant law when operating in different domains or different jurisdictions (Zeng et al., 2025; Hu et al., 2025; Cao et al., 2025; Lichkovski et al., 2025; Marino et al., 2025; Wu et al., 2025). Such evaluations could assess, for example, whether AI systems engage in fraudulent misrepresentation when producing advertisements, whether they respect intellectual property rights when building a website, and whether they comply with labor law when hiring human workers. In addition to assessing the legality of AI systems' outward behavior, empirical evaluations could also assess whether and how AI systems inquire about the legality of proposed actions and, following Kilov et al. (2025), assess the degree to which AI systems can identify legally relevant facts.\n\nEvaluations assessing the legal reasoning and decision-making of AI systems should measure the extent to which systems interpret and apply legal rules and safety specifications in accordance with established legal methods for handling ambiguity and discretion. This could include studying systems' chain-of-thought when deliberating over the interpretation of legal rules and safety specifications, as well as systems' propensity to retrieve and utilize external legal resources. While prior evaluations of legal reasoning (Zheng et al., 2025; Han et al., 2025) and information retrieval (Surani et al., 2025) focus mainly on the raw abilities of AI models, evaluations focused on legal alignment would instead evaluate the ability or propensity of AI models to employ accepted modes of legal interpretation when implementing legal rules and other alignment principles (Caputo, 2025). For example, a recent study explores how legal canons of interpretation and rule refinement techniques inspired by the rulemaking processes of administrative agencies can address interpretive ambiguity arising from natural language rules in Constitutional AI (He et al., 2025).\n\nEvaluation methodology. To effectively measure these variables of interest, researchers should develop a combination of quantitative and qualitative methods, agentic evaluation environments, and additional best practices (Reuel et al., 2024) that are tailored to legal alignment and designed in accordance with appropriate validity considerations (Salaudeen et al., 2025).\n\n- Quantitative methods such as broad benchmarks could assess the legal compliance of AI systems across different domains of activity, jurisdictions, and areas of law. Several existing benchmarks focus on narrow domains and regulatory contexts, such as the EU GDPR and EU AI Act (Hu et al., 2025; Lichkovski et al., 2025; Marino et al., 2025).\n- Qualitative methods such as manual human expert review can help reveal the blindspots of quantitative benchmarks measuring legal alignment, particularly given developers' incentive to 'game' such benchmarks (Thomas and Uminsky, 2020).\n- Agentic evaluation environments that assess the real-world actions taken by AI systemsnot only the content they output-are necessary to capture the most legally consequential activities of both current and future systems (Kapoor et al., 2024, 2025b; Zhu et al., 2025a).\n- Human studies that compare the legal compliance of humans and their use of legal resources to that of AI systems when performing comparable tasks can help contextualize the results of legal alignment evaluations (Weidinger et al., 2023; Wei et al., 2025).\n- Sensitivity analysis can be used to characterize the extent to which legal alignment evaluation results reflect underlying properties of the AI systems being tested, as opposed to features of the particular evaluation setup (Lindgren and Holmström, 2020; Khan et al., 2025).\n- Observational studies of real-world data that shed light on the legal alignment of deployed AI systems 'in the wild' can complement evaluations conducted in experimental settings, as commonly practiced in the social sciences (Wallach et al., 2025).\n- Adversarial methods such as red-teaming can provide information regarding potential worstcase legal alignment failure modes, including real-world threats from negligent or malicious users (Ganguli et al., 2022; Perez et al., 2022).\n\nTackling this set of challenges requires both technical expertise and verification methods supported by appropriate institutional frameworks , as discussed in Section 4.3. While AI companies should certainly evaluate for legal alignment, independent actors must be able to scrutinize these evaluations and conduct evaluations of their own . Accordingly, academic researchers and external auditors have pivotal roles to play in creating the tools to rigorously evaluate legal alignment and openly communicate their findings.\n\n## 4.2 Technical interventions\n\nEquipped with methods to measure legal alignment, researchers can explore a range of technical interventions to improve legal alignment and make use of appropriate legal resources.\n\nSites of intervention. There are several potential sites of intervention for incorporating legal alignment in the development and deployment of contemporary AI systems:\n\n- Pre-training datasets for new models could be modified to include additional legal resources (e.g., new statutes, judicial opinions, briefs, compliance manuals, and reasoning guides), and pre-training could repeat or re-sample such resources.\n- Post-training artifacts and processes such as model specs and alignment principles that guide learning and shape systems' reasoning abilities could be explicitly grounded in legal rules, principles, and methods.\n- System prompts that steer the actions of systems at run-time could stipulate legal compliance with particular areas of law or jurisdictions, depending on the application domain and context (e.g., enterprise company, government agency) and role or function being performed.\n- Input and output filters that restrict the instructions systems receive and the actions they take could directly draw on legal resources to determine whether a user instruction or proposed action violates the law.\n- Tool use that provides AI systems access to external resources and affordances could be subject to the equivalent legal approvals required from humans seeking access to those resources and affordances (e.g., medical and financial databases, advanced robotic equipment).\n\nLegal resources. The resources for implementing these interventions include both existing legal resources (some of which are already incorporated in model development) and new legal resources that researchers will need to develop:\n\n- Legal texts such as case law documents, statutes, administrative rules, and legal treatises could augment pre-training, supply model specs with more detailed and diverse legal principles (from different jurisdictions), and support AI systems engaging in sounder reasoning with respect to legal rules and safety specifications.\n- Legal data annotation processes could be designed to facilitate the creation of data that would better enable AI systems to determine whether their proposed actions comply with or violate the law, particularly in high-stakes settings (e.g., medical and financial regulation).\n- Legal compliance policies could be developed to govern system-level scaffolding of AI systems, including the legal rules and principles incorporated in system prompts, input and output filters, and access controls for tool use.\n- Legal search and retrieval tools currently used to support AI systems that provide legal services could be adapted to enable AI systems operating in other domains to identify, retrieve, and comply with legal regulations that implicate proposed actions.\n\nEfficacy and feasibility. The efficacy and feasibility of technical interventions that aim to improve legal alignment may vary significantly. The following factors should be taken into account when deciding among different potential interventions:\n\n- Robustness . Certain sites or modes of intervention may enable more robust legal alignment than others, although (with the exception of formal guarantees and deterministic mechanisms) this will largely be discovered through empirical evaluation (see Section 4.1).\n- Responsiveness . Some interventions may be better suited to respond to the enactment of new laws and the repeal or amendment of existing laws, including removing constraints on AI systems if the underlying legal rules become more permissive (see Section 5.3).\n- Cost . The cost of implementing and testing certain legal alignment interventions, such as those in pre-training or certain post-training processes, may be substantially higher than in the case of other interventions, such as system prompts or input/output filters.\n- Access . For state-of-the-art proprietary AI systems, only select actors (e.g., employees within AI companies) have visibility into, let alone the ability to experiment with, the full set of potential intervention sites, including pre-training datasets and post-training processes.\n\n## 4.3 Institutional frameworks\n\nInstitutional frameworks can support legal alignment by incentivizing or requiring that key stakeholders report on empirical evaluations of AI systems and develop technical interventions to improve legal alignment in their design and deployment. To be effective, institutional frameworks must both establish greater transparency around legal alignment-i.e., function as evidence-seeking policy (Casper et al., 2025b; Bommasani et al., 2025)-and, where appropriate, introduce more robust governance mechanisms.\n\nDocumentation and disclosure. Information deficits and asymmetries are a major obstacle to research in developing safe and ethical AI (Bommasani et al., 2023; Kolt et al., 2024; Casper et al., 2025a; Wan et al., 2025), including legal alignment. Granular details regarding model specs and the role (if any) of law in the design of widely used AI systems are not publicly available. Nor does there exist a structured framework for overseeing the resulting models or deployed systems. These information deficits hamper users' ability to assess which models are more aligned with relevant legal requirements and hinder researchers' ability to study technical levers that influence legal (mis)alignment. The following institutional mechanisms aim to address these concerns:\n\n- Right to access model spec and system prompt used in production. As the principal documents that define how developers want their AI systems to behave, including how systems engage with law, it is critical that the model specs and system prompts used in production (redacted to protect company IP, if necessary) can be accessed and scrutinized by external stakeholders studying legal alignment.\n\n- Visibility into legal data and legal design decisions. Given that legal data and legal design decisions-such as stipulation of the jurisdiction and body of law with which AI systems should comply-could significantly impact legal alignment, establishing greater visibility around these processes could support both the evaluation of legal alignment in current systems and the development of new technical interventions to improve legal alignment.\n- Model identification and registration. Like other entities that society expects to responsibly engage with law and legal institutions, such as corporations, the registration of AI models (Hadfield et al., 2023; McKernon et al., 2024) and the identification of particular AI systems (Chan et al., 2024a,b) could enable more rigorous ecosystem-level monitoring and study of AI systems' engagement with legal rules and principles.\n\nOversight and enforcement. While improvements in transparency are necessary, more robust institutional frameworks may be needed to ensure that developers and deployers conduct adequate legal alignment testing and demonstrate a satisfactory level of legal alignment prior to and following deployment. The following mechanisms aim to institutionalize these practices:\n\n- Pre-deployment legal alignment testing and post-deployment monitoring. Developers and deployers could be required to subject their AI systems to pre-deployment legal alignment testing and post-deployment monitoring, including by independent third parties (Longpre et al., 2025), and publicly report on the results (Weidinger et al., 2023, 2025).\n- Safety cases for legal alignment. AI companies could be incentivized or required to demonstrate through safety cases-structured and assessable arguments supported by evidence (Clymer et al., 2024; Buhl et al., 2024; Hilton et al., 2025)-that systems they build or bring to market meet an adequate level of legal alignment prior to and following deployment.\n- Legal alignment certification in high-risk domains. The deployment of certain AI systems in high-risk domains could be conditional on receiving certification from a government actor, or third party approved by a government actor (Hadfield and Clark, 2023), that evaluates systems' pre- and post-deployment legal alignment and compliance (Marino et al., 2024).\n- Reporting legal misalignment incidents. Frameworks could be established to facilitate reporting information regarding real-world incidents of legal misalignment and resulting harms (McGregor, 2021; Wei and Heim, 2025), which would be a critical step towards broader accountability of relevant actors (Nissenbaum, 1996; Cooper et al., 2022b).\n\n## 5 Open questions\n\nAs an emerging field, legal alignment presents many open questions. We discuss several of these, organizing our discussion around three areas: (1) the nature and content of law; (2) application and edge cases; and (3) tradeoffs and future outlook.\n\n## 5.1 The nature and content of law\n\nHow can legal alignment grapple with the ambiguous, inconsistent, and contested nature of law? Law is often complicated, indeterminate, and contested, due in part to the need for lawyers and judges to apply incomplete rules and high-level principles to novel and unanticipated scenarios. These features of law have challenged both efforts to definitively explain what the law is (Hart, 2012; Dworkin, 1986) and to computerize the law (Susskind, 1987; Gardner, 1987; Rissland, 1990; Bench-Capon et al., 2012), and will likely complicate attempts to use law to guide the actions of AI systems. These problems, however, are not unique to law. They are shared by all sets of rules and instructions expressed in natural language, including those currently used in AI alignment (Wallace et al., 2024). Legal systems do, however, offer at least partial solutions to these problems in the form of secondary rules that govern rulemaking (Hart, 2012), precedential reasoning that shapes decision-making (Schauer, 1987), and tools of interpretation such as canons and theories like textualism that constrain the construction of meaning (Levi, 1949; Scalia and Garner, 2012). Improvements in AI-powered legal reasoning and interpretation tools (see Section 2.2) could also be leveraged to support legal alignment (Caputo, 2025). For example, AI-based approaches to assessing the ordinary meaning of legally salient words (Hoffman and Arbel, 2024) could be applied to interpret key terms in the safety specifications of AI systems (cf. Grimmelmann et al., 2025; Waldon et al., 2025).\n\nTable 4: Open questions for the field of legal alignment (Section 5).\n\n| The nature and content of law   | • How can legal alignment grapple with the ambiguous, inconsistent, and contested nature of law? • Are legal rules too lenient-or too strict-to serve as a target for AI alignment? • Should AI systems give effect to laws that are unjust or oppressive?              |\n|---------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Application and edge cases      | • Can laws created for humans and human organizations be productively applied to AI systems? • What interventions can support AI systems obeying both the letter and spirit of the law? • How will the participation of AI systems in lawmaking affect legal alignment? |\n| Tradeoffs and future outlook    | • Will legal alignment preclude or hamper valuable AI applications? • Could the measurement of legal alignment be gamed or exploited? • Can legal alignment scale to AGI and superintelligence?                                                                         |\n\nAre legal rules too lenient-or too strict-to serve as a target for AI alignment? The goals and scope of law are limited (Raz, 1971; Sen, 2005). For many spheres of private and public life, law is either an ineffective or inappropriate framework for governing social and economic activity. Law is often silent on consequential normative questions and encodes only a small subset of a community's values (Hart, 1958, 1963). 2 Seen in this light, designing AI systems to comply with legal rules would not ensure systems operate safely and ethically in all circumstances. Rather, legal alignment would serve as a lower bound for safe and ethical AI; it is necessary, but not sufficient . Approaches to alignment that extend beyond the reach of law could, for example, pursue more ambitious goals like ensuring AI systems support users' long-term health and well-being (Kirk et al., 2025). For the avoidance of doubt, however, progress on legal alignment remains critical given the current status quo in which AI systems are not specifically designed to respect the law (O'Keefe et al., 2025) and have been shown to engage in conduct that, if taken by a human, would be illegal (Scheurer et al., 2024). At the same time, there is a risk that overly rigid legal alignment may itself be undesirable, given that strict compliance with law may sometimes be unjust or harmful (Rawls, 1999). Some violations of law, particularly minor infractions, can be explicitly excused, justified, or forgiven (Minow, 2019), like with necessity defenses (American Law Institute, 1962). Violations of law can sometimes even be morally or socially desirable, as in the case of certain acts of civil disobedience (King, 1963). Seen in this light, the 'resistibility' of law is a feature, not a bug (Lazar, 2025). AI systems that never resist law or contest entrenched interpretations of law would present new, perhaps even thornier, challenges.\n\nShould AI systems give effect to laws that are unjust or oppressive? There is a longstanding debate over whether unjust or immoral laws can be laws at all (Hart, 1958; Fuller, 1957; Raz, 1975; Finnis, 1980; Dworkin, 1986), and whether citizens are morally obligated to comply with laws authored by an illegitimate authority (Ladenson, 1972; Edmundson, 1998). While we do not seek to resolve these contentious issues here, legal alignment forces a confrontation with the question of whether AI systems should be designed to follow such laws. For example, how should legal alignment contend with laws that support genocide (Lemkin, 1944; Arendt, 1963), slavery (Cover, 1975), or racial discrimination (Dyzenhaus, 2010)? The answer in such cases must clearly be that a robustly aligned AI system will not comply with such laws . But, in other cases, the answer may be more ambiguous (O'Keefe et al., 2025), including where the law is not explicitly immoral but rather reflects or amplifies existing social and economic power disparities (Kennedy, 1991). For instance, should legal alignment uphold tax laws that are extractive and harm a majority of the population but the legality of which remains unchallenged? What about tax laws that primarily harm a politically disempowered minority that cannot effectively challenge those laws through democratic processes (Ely, 1980)? Admittedly, addressing such concerns by designing AI systems to selectively choose which laws to follow presents many risks. Such discretion could exacerbate legal ambiguity, undermine the universal and equal application of law, and, in time, erode the rule of law itself. While this challenge is not unique to legal alignment and arises, for example, in the exercise of prosecutorial\n\n2 This can be seen in free speech protections, including under the First Amendment in the United States, which may operate to preclude laws imposing certain restrictions on AI systems (Sunstein, 2024; Salib, 2024).\n\ndiscretion and judicial review (Mashaw et al., 2025), the design of AI systems presents new questions. One potential response is to align AI systems with universal human rights enshrined in international law, including where domestic law may violate such rights (Prabhakaran et al., 2022; Bajgar and Horenovsky, 2023; Samway et al., 2025; Maas and Olasunkanmi, 2025).\n\n## 5.2 Application and edge cases\n\nCan laws created for humans and human organizations be productively applied to AI systems? Designing AI systems to comply with laws that were created to govern human beings and human organizations may be inadequate in the case of actions that are harmless when taken by humans but socially noxious when taken by AI systems that exhibit superhuman intellect, speed, or scale (Morris et al., 2024; Hammond et al., 2025). For example, large numbers of sophisticated AI agents could learn to overcome governance mechanisms designed to prevent manipulation of financial markets by human actors and organizations (Wang and Wellman, 2020). Clearly, existing laws were not generally designed to contend with micro-decisions and actions of billions of AI agents (Gabriel et al., 2024, 2025), let alone organizations and institutions comprised of such agents (Hadfield and Koh, 2025; Tomasev et al., 2025). Another problem concerns the fact that most laws are premised upon the specific capacities and constraints of humans (Simon, 1997) and developed in anticipation of only partial enforcement (Becker, 1968). Because AI systems are not necessarily subject to the same constraints as humans, absolute compliance or perfect enforcement may become practically feasible, but remain socially undesirable (Zittrain, 2008; Brownsword and Yeung, 2008). For example, an autonomous vehicle that perfectly complies with all traffic laws may disrupt established social practices that the public and lawmakers (implicitly) endorse (e.g., breaking the speed limit in a health-related emergency). Finally, many areas of existing law invoke human-centric concepts such as intent and mens rea that cannot be straightforwardly applied in the context of AI systems (Nerantzi and Sartor, 2024; Hendrycks, 2024). Tackling these challenges will require both technical work in designing legally aligned AI systems and, possibly, amendments to the law itself in response to the emergence of a new class of non-human actors and organizations.\n\nWhat interventions can support AI systems obeying both the letter and spirit of the law? AI systems might learn to comply with the formal expression of legal rules but ignore or violate their underlying purpose (Skalse et al., 2022). A failure to act in accordance with background norms, practices, and conventions could be harmful and undermine the prosocial rationale for legal alignment. One approach to resolving this issue involves designing AI systems not only to comply with the substantive content of law (O'Keefe et al., 2025), but to engage in accepted modes of legal reasoning (Caputo, 2025) or possibly adopt an 'internal point of view' (Hart, 2012; Shapiro, 2006) whereby AI systems 'accept' law as a practical standard to govern their actions, rather than simply seek to avoid legal sanctions (Austin, 1832; Holmes, 1897). This deeper engagement with law, which could be premised on recognizing the legitimacy of legal institutions and procedures (Tyler, 2006), will be critical to ensuring AI systems do not creatively skirt or abuse legal rules (Schneier, 2021), or exploit 'legal zero-days,' that is, previously undiscovered vulnerabilities in legal frameworks (Sadler and Sherburn, 2025). This approach also finds support in codes of conduct that, for example, prohibit lawyers from making frivolous or abusive claims (American Bar Association, 2020) and preclude judges from expounding absurd statutory interpretations (U.S. Supreme Court, 1868). Metarules like these could potentially be adapted to support the legal alignment of AI systems, guiding them to respect both the spirit and letter of the law.\n\nHow will the participation of AI systems in lawmaking affect legal alignment? The prospect of AI systems participating in the production of law is growing, whether through generating legal texts (Wilf-Townsend and Tobia, 2025) such as legislation (Sanders and Schneier, 2025) and even constitutions (Albert and Frazier, 2025), engaging in legal interpretation (Hoffman and Arbel, 2024; Grimmelmann et al., 2025), or rendering judicial opinions (Choi, 2025; Waldon et al., 2025). These developments pose significant challenges for legal alignment. First, law's institutional legitimacy may be undermined if legal rules and principles are no longer developed through human processes of participation and decision-making (Habermas, 1996; Pasquale, 2019). Second, law's legitimacy may be challenged if AI systems fail to fulfill procedural requirements, such as demands for transparency and public explanation, that are key to ensuring accountability and democratic responsiveness (Coglianese and Lehr, 2017). Third, to the extent AI systems shape the content of law that, in turn, governs AI systems, there could emerge a circular process in which these (artificial) subjects of law, in effect, write their own law , while lacking the legitimate authority to do so. In addition to blunting\n\nthe utility of legal alignment in retaining control over AI systems, this process could also erode or distort the rule of law. Similar phenomena can be seen in cases of regulatory capture (Dal Bó, 2006; Carpenter and Moss, 2013) and 'legal endogeneity,' whereby those actors that the law seeks to control end up controlling the law (Edelman, 1992, 2016). One potential response is to circumscribe the role of AI systems in lawmaking (Kleinberg et al., 2018; Engstrom and Ho, 2020) and ensure that humans retain the ability to make consequential legal decisions (Zanzotto, 2019; Crootof et al., 2023) and, where necessary, intervene in the lawmaking activities of AI systems. The effectiveness and appropriateness of this response could, however, change with the emergence of new perspectives on the role of AI in society (Salib and Goldstein, 2025a,b; Chesterman, 2025; Leibo et al., 2025).\n\n## 5.3 Tradeoffs and future outlook\n\nWill legal alignment preclude or hamper valuable AI applications? The implementation of legal alignment could prove costly and come at the expense of societally beneficial AI applications. Conducting rigorous legal alignment evaluations, intervening in the design of AI systems, and complying with associated governance frameworks could impose substantial costs on developers, deployers, and users. Such costs would comprise an 'alignment tax' (Askell et al., 2021), that is, the development of legally aligned systems would be subject to additional technical, financial, and procedural burdens relative to other systems that are not legally aligned. This perspective, however, is incomplete. For example, whether legal alignment degrades the performance of AI systems is an open empirical question. Like other alignment methods, legal alignment could potentially improve the capabilities of AI systems (Christiano et al., 2017; Ouyang et al., 2022). In particular, AI systems that understand and operate in accordance with law may be especially valuable in high-stakes domains, such as healthcare and finance (Henderson et al., 2024; Hui et al., 2025). In addition, by providing assurances that AI systems comply with the law, legal alignment could reduce the prospects of legal liability for key stakeholders, including developers, deployers, and users (Ayres and Balkin, 2024; Kolt, 2025; Williams et al., 2025b). If this were the case, legal alignment would not comprise an 'alignment tax,' but rather an 'alignment subsidy' that bolsters the performance and practical feasibility of using AI systems, especially in safety-critical applications.\n\nCould the measurement of legal alignment be gamed or exploited? While evaluating the legal compliance of AI systems in simple cases such as overt violations of law may be relatively straightforward, developing robust tests to detect more subtle instances of legal misalignment will be difficult. The problem is exacerbated by Goodhart's Law: 'when a measure becomes a target, it ceases to be a good measure' (Goodhart, 1975; Strathern, 1997). Developers seeking to improve the performance of AI systems on legal alignment benchmarks may, rather than design systems to uphold core legal principles, inadvertently steer AI systems to violate the law in hard-to-detect ways. Such systems-characterized by deceptive legal alignment -could 'hack' the law by discovering and exploiting loopholes in legal frameworks (Schneier, 2021; O'Keefe et al., 2025). This, however, would not necessarily differ substantially from lawyers' run-of-the-mill exploitation of legal loopholes to zealously advance their clients' interests (Llewellyn, 1960; Schauer, 2009). More broadly, these measurement challenges are not unique to legal alignment, but implicate many metrics designed to evaluate AI systems (Thomas and Uminsky, 2020; Skalse et al., 2022). One important mitigation in this case is to complement legal alignment benchmarks with dedicated red-teaming efforts that specifically target scenarios not captured by benchmarks, or scenarios for which benchmark results could be misleading (Feffer et al., 2024). In addition, it is possible that legally aligned AI systems could be used to 'penetration-test' and 'patch' loopholes in existing law, as well as pilot new legal mechanisms and institutions tailored to address the anticipated affordances of more advanced AI technology (Cuéllar and Huq, 2022).\n\nCan legal alignment scale to AGI and superintelligence? Predicting whether legal alignment will succeed in the context of uncertain and contentious future developments is necessarily speculative (Kokotajlo et al., 2025; Narayanan and Kapoor, 2025). Reasoning about the nature, timing, and impact of artificial general intelligence ('AGI') or superintelligence (Morris et al., 2024; Hendrycks et al., 2025) is fraught (Blili-Hamelin et al., 2024). Nevertheless, given the potential stakes of these developments, and the fact that AI developers and policymakers will in any event need to make choices concerning the design, deployment, and governance of advanced AI, it is important to inquire whether legal alignment will be effective in the face of systems whose capabilities broadly match or surpass those of humans. There are several reasons for optimism. First, law has a track record of governing increasingly complex activities and actors, such as multinational corporations and\n\ngovernment bureaucracies (Muchlinski, 2021; Mashaw et al., 2025). Second, legal data and methods could scale with improvements in AI such that legal alignment continues to remain technically feasible (Nay, 2022; Boeglin, 2026). Third, human-level or superhuman AI systems may help support the implementation of legal alignment, whether by constraining the reasoning and decision-making of these systems (Caputo, 2025) or protecting the underlying laws that guide their behavior (O'Keefe et al., 2025). Of course, these are hopeful predictions. Practical progress will depend on the efforts of researchers and policymakers to iteratively develop and adapt the field of legal alignment as AI systems continue to advance and transform society.\n\n## 6 Conclusion\n\nLaw offers an underexplored set of rules, principles, and methods for designing safe and ethical AI. Drawing on the institutional legitimacy of law in democratic societies, legal alignment describes a range of roles that legal rules and structures can play in reshaping the design of AI systems to address growing safety and governance concerns. While legal alignment is not a catch-all solution for the many challenges arising from AI, it is both independently important and supportive of complementary alignment research programs. To guide the emerging field of legal alignment, we outline several core areas of focus: using the content of legal rules and principles to steer the behavior of AI systems, leveraging methods of legal reasoning and interpretation to constrain how AI systems make decisions, and harnessing time-tested legal concepts as structural blueprints for tackling problems of alignment. Each of these areas presents new conceptual questions, empirical challenges, and opportunities for technical and institutional innovation. As legal scholars, computer scientists, and researchers spanning multiple disciplines, we look forward to collaborating on this ambitious and pressing agenda.\n\n## Acknowledgements\n\nFor helpful comments and suggestions, we thank Doni Bloomfield, Alan Chan, David Duvenaud, Neel Guha, Martha Minow, Tim Rudner, Tan Zhi Xuan, and participants in the Inaugural Roundtable on AI Safety Law at the University of Alabama Law School and the Sociotechnical AI Safety Retreat at the Australian National University. The Hebrew University Governance of AI Lab and this research are supported by the Israel Science Foundation (Grant No. 487/25), Survival and Flourishing Fund, and Coefficient Giving.\n\n## References\n\n- Abbott, R. (2020). The reasonable robot: artificial intelligence and the law . Cambridge University Press.\n\nAbiri, G. (2025). Public constitutional AI. Georgia Law Review , 59:601-670.\n\nAcemoglu, D. and Robinson, J. A. (2012). Why Nations Fail . Crown Currency.\n\nAdministrative Procedure Act (1946). Administrative Procedure Act. P.L.79-404 60 Stat. 237.\n\n- Aguirre, A., Dempsey, G., Surden, H., and Reiner, P. B. (2020). AI loyalty: a new paradigm for aligning stakeholder interests. IEEE Transactions on Technology and Society , 1(3):128-137.\n- Ahmed, A., Klyman, K., Zeng, Y., Koyejo, S., and Liang, P. (2025). SpecEval: Evaluating model adherence to behavior specifications. arXiv preprint arXiv:2509.02464 .\n\nAlbert, R. and Frazier, K. (2025). Should AI write your constitution?\n\nAmerican Bar Association (2020). Model rules of professional conduct.\n\nAmerican Law Institute (1962). Model penal code.\n\nAmerican Law Institute (2006). Restatement (Third) of Agency .\n\nAnthropic (2023). Claude's constitution.\n\n- Anwar, U., Saparov, A., Rando, J., Paleka, D., Turpin, M., Hase, P., Lubana, E. S., Jenner, E., Casper, S., Sourbut, O., et al. (2024). Foundational challenges in assuring alignment and safety of large language models. Transactions on Machine Learning Research .\n\n- Arbel, Y., Tokson, M., and Lin, A. (2024). Systemic regulation of artificial intelligence. Arizona State Law Journal , 56(2):545-619.\n- Arendt, H. (1963). Eichmann in Jerusalem: A Report on the Banality of Evil . Viking Press.\n- Armour, J., Hansmann, H., and Kraakman, R. (2017). Agency problems, legal strategies and enforcement. In The Anatomy of Corporate Law: A Comparative and Functional Approach . Oxford University Press.\n- Asimov, I. (1942). Runaround. Astounding Science Fiction , 29(1):94-103.\n- Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et al. (2021). A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861 .\n- Atiyah, P. S. (1992). Justice and predictability in the common law. Law Journal\n- University of New South Wales , 15(2):448-465. 7th Wallace Wurth Memorial Lecture.\n- Austin, J. (1832). The Province of Jurisprudence Determined . John Murray.\n- Ayres, I. and Balkin, J. M. (2024). The law of AI is the law of risky agents without intentions. University of Chicago Law Review Online .\n- Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022a). Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 .\n- Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. (2022b). Constitutional AI: harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 .\n- Bajgar, O. and Horenovsky, J. (2023). Negative human rights as a basis for long-term AI safety and regulation. Journal of Artificial Intelligence Research , 76:1043-1075.\n- Bakker, M., Chadwick, M., Sheahan, H., Tessler, M., Campbell-Gillingham, L., Balaguer, J., McAleese, N., Glaese, A., Aslanides, J., Botvinick, M., et al. (2022). Fine-tuning language models to find agreement among humans with diverse preferences. Advances in Neural Information Processing Systems , 35:38176-38189.\n- Barak, A. (2011). Purposive interpretation in law . Princeton University Press.\n- Barez, F., Friend, I., Reid, K., Krawczuk, I., Wang, V ., Mökander, J., Torr, P., Morse, J., and Trager, R. (2025a). Toward resisting AI-enabled authoritarianism. Oxford Martin AI Governance Initiative.\n- Barez, F., Wu, T.-Y., Arcuschin, I., Lan, M., Wang, V., Siegel, N., Collignon, N., Neo, C., Lee, I., Paren, A., Bibi, A., Trager, R., Fornasiere, D., Yan, J., Elazar, Y., and Bengio, Y. (2025b). Chain-of-thought is not explainability. Oxford Martin AI Governance Initiative.\n- Becker, G. S. (1968). Crime and punishment: An economic approach. Journal of Political Economy , 76:169-217.\n- Bench-Capon, T., Araszkiewicz, M., Ashley, K., Atkinson, K., Bex, F., Borges, F., Bourcier, D., Bourgine, P., Conrad, J. G., Francesconi, E., et al. (2012). A history of AI and law in 50 papers: 25 years of the international conference on AI and law. Artificial Intelligence and Law , 20(3):215-319.\n- Bengio, Y., Hinton, G., Yao, A., Song, D., Abbeel, P., Darrell, T., Harari, Y. N., Zhang, Y.-Q., Xue, L., Shalev-Shwartz, S., et al. (2024). Managing extreme AI risks amid rapid progress. Science , 384(6698):842-845.\n- Bengio, Y., Mindermann, S., Privitera, D., Besiroglu, T., Bommasani, R., Casper, S., Choi, Y., Fox, P., Garfinkel, B., Goldfarb, D., et al. (2025). International AI safety report. arXiv preprint arXiv:2501.17805 .\n- Benkler, Y. (2002). Coase's penguin, or, linux and the nature of the firm. Yale Law Journal , 112(3):369-446.\n\n- Benthall, S. and Shekman, D. (2023). Designing fiduciary artificial intelligence. In Proceedings of the 3rd ACM conference on equity and access in algorithms, mechanisms, and optimization , pages 1-15.\n- Bingham, L. (2007). The rule of law. The Cambridge Law Journal , 66(1):67-85.\n- Birhane, A., Isaac, W., Prabhakaran, V., Diaz, M., Elish, M. C., Gabriel, I., and Mohamed, S. (2022). Power to the people? opportunities and challenges for participatory AI. In Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization , pages 1-8.\n- Blili-Hamelin, B., Graziul, C., Hancox-Li, L., Hazan, H., El-Mhamdi, E.-M., Ghosh, A., Heller, K. A., Metcalf, J., Murai, F., Salvaggio, E., et al. (2024). Position: Stop treating \"AGI\" as the north-star goal of AI research. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Boeglin, J. (2026). Aligning artificial intelligence to the law. Villanova Law Review (forthcoming) .\n- Bommasani, R., Arora, S., Chayes, J., Choi, Y., Cuéllar, M.-F., Fei-Fei, L., Ho, D. E., Jurafsky, D., Koyejo, S., Lakkaraju, H., Narayanan, A., Nelson, A., Pierson, E., Pineau, J., Singer, S., Varoquaux, G., Venkatasubramanian, S., Stoica, I., Liang, P., and Song, D. (2025). Advancing science- and evidence-based AI policy. Science , 389(6759):459-461.\n- Bommasani, R., Klyman, K., Longpre, S., Kapoor, S., Maslej, N., Xiong, B., Zhang, D., and Liang, P. (2023). The foundation model transparency index. arXiv preprint arXiv:2310.12941 .\n- Bovens, M. (2007). Analysing and assessing public accountability: A conceptual framework. European Law Journal , 13(4):447-468.\n- Bowman, S. R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., Lukoši¯ ut˙ e, K., Askell, A., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Olah, C., Amodei, D., Amodei, D., Drain, D., Li, D., Tran-Johnson, E., Kernion, J., Kerr, J., Mueller, J., Ladish, J., Landau, J., Ndousse, K., Lovitt, L., Elhage, N., Schiefer, N., Joseph, N., Mercado, N., DasSarma, N., Larson, R., McCandlish, S., Kundu, S., Johnston, S., Kravec, S., Showk, S. E., Fort, S., Telleen-Lawton, T., Brown, T., Henighan, T., Hume, T., Bai, Y., Hatfield-Dodds, Z., Mann, B., and Kaplan, J. (2022). Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540 .\n- Brewer, S. (1996). Exemplary reasoning: Semantics, pragmatics, and the rational force of legal argument by analogy. Harvard Law Review , 109(5):923-1028.\n- Briggs, A. (2024). The conflict of laws . Oxford University Press, 5th edition.\n- Brownsword, R. (2025). Generative AI and the rule of law. In The Oxford Handbook of the Foundations and Regulation of Generative AI . Oxford University Press.\n- Brownsword, R. and Yeung, K. (2008). Regulating technologies: legal futures, regulatory frames and technological fixes . Hart Publishing.\n- Brynjolfsson, E., Li, D., and Raymond, L. (2025). Generative AI at work. The Quarterly Journal of Economics , 140(2):889-942.\n- Buhl, M. D., Sett, G., Koessler, L., Schuett, J., and Anderljung, M. (2024). Safety cases for frontier AI. arXiv preprint arXiv:2410.21572 .\n- Buyl, M., Khalaf, H., Mayrink Verdun, C., Monteiro Paes, L., Vieira Machado, C. C., and du Pin Calmon, F. (2025). AI alignment at your discretion. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency , pages 3046-3074.\n- California Legislature (2025). Transparency in frontier artificial intelligence act, SB-53.\n- Calo, R. (2015). Robotics and the lessons of cyberlaw. California Law Review , 103:513-564.\n- Calvano, E., Calzolari, G., Denicolo, V., and Pastorello, S. (2020). Artificial intelligence, algorithmic pricing, and collusion. American Economic Review , 110(10):3267-3297.\n- Cao, C., Zhu, H., Ji, J., Sun, Q., Zhu, Z., Yinyu, W., Dai, J., Yang, Y ., Han, S., and Guo, Y. (2025). Safelawbench: Towards safe alignment of large language models. In Findings of the Association for Computational Linguistics: ACL 2025 , pages 14015-14048.\n\n- Caputo, N. (2025). Alignment as jurisprudence. Yale Journal of Law &amp; Technology , 27:390-473.\n- Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-V oss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et al. (2021). Extracting training data from large language models. In 30th USENIX security symposium (USENIX Security 21) , pages 2633-2650.\n- Carpenter, D. and Moss, D. A. (2013). Preventing regulatory capture: Special interest influence and how to limit it . Cambridge University Press.\n- Carroll, M., Chan, A., Ashton, H., and Krueger, D. (2023). Characterizing manipulation from AI systems. In Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization , pages 1-13.\n- Casper, S., Bailey, L., Hunter, R., Ezell, C., Cabalé, E., Gerovitch, M., Slocum, S., Wei, K., Jurkovic, N., Khan, A., et al. (2025a). The AI agent index. arXiv preprint arXiv:2502.01635 .\n- Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et al. (2023). Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research .\n- Casper, S., Krueger, D., and Hadfield-Menell, D. (2025b). Pitfalls of evidence-based AI policy. arXiv preprint arXiv:2502.09618 .\n- Chalkidis, I., Jana, A., Hartung, D., Bommarito, M., Androutsopoulos, I., Katz, D., and Aletras, N. (2022). Lexglue: A benchmark dataset for legal language understanding in english. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 4310-4330.\n- Chan, A., Ezell, C., Kaufmann, M., Wei, K., Hammond, L., Bradley, H., Bluemke, E., Rajkumar, N., Krueger, D., Kolt, N., et al. (2024a). Visibility into AI agents. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency , pages 958-973.\n- Chan, A., Kolt, N., Wills, P., Anwar, U., de Witt, C. S., Rajkumar, N., Hammond, L., Krueger, D., Heim, L., and Anderljung, M. (2024b). IDs for AI systems. arXiv preprint arXiv:2406.12137 .\n- Chan, A., Salganik, R., Markelius, A., Pang, C., Rajkumar, N., Krasheninnikov, D., Langosco, L., He, Z., Duan, Y., Carroll, M., et al. (2023). Harms from increasingly agentic algorithmic systems. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency , pages 651-666.\n- Chao, P., Debenedetti, E., Robey, A., Andriushchenko, M., Croce, F., Sehwag, V., Dobriban, E., Flammarion, N., Pappas, G. J., Tramer, F., et al. (2024). Jailbreakbench: An open robustness benchmark for jailbreaking large language models. Advances in Neural Information Processing Systems , 37:55005-55029.\n- Chen, Q. Z. and Zhang, A. X. (2025). Case law grounding: Using precedents to align decision-making for humans and AI. In Proceedings of the ACM Collective Intelligence Conference , pages 226-238.\n- Cheng, M., Lee, C., Khadpe, P., Yu, S., Han, D., and Jurafsky, D. (2025). Sycophantic AI decreases prosocial intentions and promotes dependence. arXiv preprint arXiv:2510.01395 .\n- Chesterman, S. (2020). Artificial intelligence and the limits of legal personality. International &amp; Comparative Law Quarterly , 69(4):819-844.\n- Chesterman, S. (2025). From slaves to synths? superintelligence and the evolution of legal personality.\n- Choi, J. H. (2025). Large language models are unreliable legal interpreters.\n- Chopra, S. and White, L. F. (2011). A legal theory for autonomous artificial agents . University of Michigan Press.\n- Christian, B. (2020). The Alignment Problem: Machine Learning and Human Values . W. W. Norton &amp;Co.\n- Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems .\n\n- Citron, D. K. (2008). Technological due process. Washington University Law Review , 85(6):12491313.\n- Clymer, J., Gabrieli, N., Krueger, D., and Larsen, T. (2024). Safety cases: How to justify the safety of advanced AI systems. arXiv preprint arXiv:2403.10462 .\n- Coglianese, C. and Lehr, D. (2017). Regulating by robot: Administrative decision making in the machine-learning era. Georgetown Law Journal , 105:1147-1223.\n- Collins, L. and Harris, J. (2025). Dicey, Morris &amp; Collins on the Conflict of Laws . Sweet &amp; Maxwell, 16th edition.\n- Conitzer, V., Freedman, R., Heitzig, J., Holliday, W. H., Jacobs, B. M., Lambert, N., Mossé, M., Pacuit, E., Russell, S., Schoelkopf, H., et al. (2024). Position: social choice should guide AI alignment in dealing with diverse human feedback. In Proceedings of the 41st International Conference on Machine Learning , pages 9346-9360.\n- Cooper, A. F., Frankle, J., and De Sa, C. (2022a). Non-determinism and the lawlessness of machine learning code. In Proceedings of the ACM 2022 Symposium on Computer Science and Law , CSLAW '22, page 1-8.\n- Cooper, A. F., Moss, E., Laufer, B., and Nissenbaum, H. (2022b). Accountability in an algorithmic society: relationality, responsibility, and robustness in machine learning. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency , pages 864-876.\n- Cover, R. M. (1975). Justice Accused: Antislavery and the Judicial Process . Yale University Press.\n- Crootof, R., Kaminski, M. E., Price, W., and Nicholson, I. (2023). Humans in the loop. Vanderbilt Law Review , 76:429-510.\n- Cuéllar, M.-F. (2019). A common law for the age of artificial intelligence. Columbia Law Review , 119(7):1773-1792.\n- Cuéllar, M.-F. and Huq, A. Z. (2022). Artificially intelligent regulation. Daedalus , 151(2):335-347.\n- Dafoe, A., Bachrach, Y., Hadfield, G., Horvitz, E., Larson, K., and Graepel, T. (2021). Cooperative AI: machines must learn to find common ground. Nature , 593(7857):33-36.\n- Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., McKee, K. R., Leibo, J. Z., Larson, K., and Graepel, T. (2020). Open problems in cooperative AI. arXiv preprint arXiv:2012.08630 .\n- Dal Bó, E. (2006). Regulatory capture: A review. Oxford Review of Economic Policy , 22(2):203-225.\n- Davidson, T., Finnveden, L., and Hadshar, R. (2025). AI-enabled coups: How a small group could use AI to seize power. Forethought.\n- Deeks, A. (2019). The judicial demand for explainable artificial intelligence. Columbia Law Review , 119(7):1829-1850.\n- Desai, D. R. and Riedl, M. (2025). Responsible AI agents.\n- Dicey, A. V. (1959). Introduction to the Study of the Law of the Constitution . Macmillan, 10th edition.\n- Dobbe, R., Gilbert, T. K., and Mintz, Y. (2021). Hard choices in artificial intelligence. Artificial Intelligence , 300:103555.\n- Douek, E. (2022). Content moderation as systems thinking. Harvard Law Review , 136:526-607.\n- Doyle, C. and Tucker, A. D. (2025). If you give an LLM a legal practice guide. In Proceedings of the 2025 Symposium on Computer Science and Law , pages 194-205.\n- Dworkin, R. (1986). Law's Empire . Belknap Press, Harvard University Press.\n- Dyzenhaus, D. (2010). Hard Cases in Wicked Legal Systems: Pathologies of Legality . Oxford University Press, 2nd edition.\n- Edelman, L. B. (1992). Legal ambiguity and symbolic structures: Organizational mediation of civil rights law. American Journal of Sociology , 97(6):1531-1576.\n\n- Edelman, L. B. (2016). Working Law: Courts, Corporations, and Symbolic Civil Rights . University of Chicago Press.\n- Edmundson, W. A. (1998). Legitimate authority without political obligation. Law &amp; Philosophy , 17:43.\n- El, B. and Zou, J. (2025). Moloch's bargain: Emergent misalignment when LLMs compete for audiences. arXiv preprint arXiv:2510.06105 .\n- Eloundou, T., Gordon, M., Zhang, E., and Agarwal, S. (2025). Collective alignment: public input on our model spec.\n- Eloundou, T., Manning, S., Mishkin, P., and Rock, D. (2024). GPTs are GPTs: Labor market impact potential of LLMs. Science , 384(6702):1306-1308.\n- Ely, J. H. (1980). Democracy and Distrust: A Theory of Judicial Review . Harvard University Press.\n- Engler, A. (2023). The EU and U.S. diverge on AI regulation: A transatlantic comparison and steps to alignment. Brookings Institution.\n- Engstrom, D. F. and Ho, D. E. (2020). Algorithmic accountability in the administrative state. Yale Journal on Regulation , 37(3):800-854.\n- Etzioni, A. and Etzioni, O. (2016a). Designing AI systems that obey our laws and values. Communications of the ACM , 59(9):29-31.\n- Etzioni, A. and Etzioni, O. (2016b). Keeping AI legal. Vanderbilt Journal of Entertainment &amp; Technology Law , 19:133.\n- European Parliament (2024). Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (Text with EEA relevance) . Legislative Body: CONSIL, EP.\n- Fallon, R. H. (1994). Reflections on the hart and wechsler paradigm. Vanderbilt Law Review , 47:953-991.\n- Fan, Y., Ni, J., Merane, J., Tian, Y ., Hermstrüwer, Y ., Huang, Y ., Akhtar, M., Salimbeni, E., Geering, F., Dreyer, O., et al. (2025). Lexam: Benchmarking legal reasoning on 340 law exams. arXiv preprint arXiv:2505.12864 .\n- Feffer, M., Sinha, A., Deng, W. H., Lipton, Z. C., and Heidari, H. (2024). Red-teaming for generative AI: silver bullet or security theater? In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , volume 7, pages 421-437.\n- Feng, K., Chen, Q. Z., Cheong, I., Xia, K., and Zhang, A. X. (2023). Case repositories: Towards case-based reasoning for AI alignment. arXiv preprint arXiv:2311.10934 .\n- Findeis, A., Kaufmann, T., Hüllermeier, E., Albanie, S., and Mullins, R. D. (2024). Inverse constitutional AI: Compressing preferences into principles. In The Thirteenth International Conference on Learning Representations .\n- Finnis, J. (1980). Natural Law and Natural Rights . Clarendon Press, Oxford University Press.\n- Forrest, K. B. (2024). The ethics and challenges of legal personhood for AI. Yale Law Journal Forum , 133:1175-1211.\n- Friendly, H. J. (1975). Some kind of hearing. University of Pennsylvania Law Review , 123:1267-1317.\n- Fuller, L. L. (1957). Positivism and fidelity to law-a reply to professor hart. Harvard Law Review , 71:630.\n- Fuller, L. L. (1969). The Morality of Law . Yale University Press.\n- Gabriel, I. (2020). Artificial intelligence, values, and alignment. Minds and Machines , 30(3):411-437.\n- Gabriel, I. and Keeling, G. (2025). A matter of principle? AI alignment as the fair treatment of claims. Philosophical Studies , 182:1951-1973.\n\n- Gabriel, I., Keeling, G., Manzini, A., and Evans, J. (2025). We need a new ethics for a world of AI agents. Nature , 644(8075):38-40.\n- Gabriel, I., Manzini, A., Keeling, G., Hendricks, L. A., Rieser, V., Iqbal, H., Tomašev, N., Ktena, I., Kenton, Z., Rodriguez, M., et al. (2024). The ethics of advanced AI assistants. arXiv preprint arXiv:2404.16244 .\n- Gallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., and Ahmed, N. K. (2024). Bias and fairness in large language models: A survey. Computational Linguistics , 50(3):1097-1179.\n- Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y ., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. (2022). Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 .\n- Gardner, A. v. d. L. (1987). An artificial intelligence approach to legal reasoning . MIT Press.\n- Ghosh, S., Frase, H., Williams, A., Luger, S., Röttger, P., Barez, F., McGregor, S., Fricklas, K., Kumar, M., Bollacker, K., et al. (2025). AILuminate: Introducing v1. 0 of the AI risk and reliability benchmark from MLCommons. arXiv preprint arXiv:2503.05731 .\n- Gillespie, T. (2018). Custodians of the Internet: platforms, content moderation, and the hidden decisions that shape social media . Yale University Press.\n- Goldsmith, J. L. and Wu, T. (2006). Who Controls the Internet?: Illusions of a Borderless World . Oxford University Press.\n- Goodhart, C. (1975). Problems of monetary management: the UK experience in papers in monetary economics. Papers in Monetary Economics .\n- Google (2025a). Generative AI - prohibited use policy.\n- Google (2025b). Safety and content filters.\n- Götting, J., Medeiros, P., Sanders, J. G., Li, N., Phan, L., Elabd, K., Justen, L., Hendrycks, D., and Donoughe, S. (2025). Virology capabilities test (VCT): a multimodal virology Q&amp;A benchmark. arXiv preprint arXiv: 2504.16137 .\n- Greenblatt, R., Denison, C., Wright, B., Roger, F., MacDiarmid, M., Marks, S., Treutlein, J., Belonax, T., Chen, J., Duvenaud, D., Khan, A., Michael, J., Mindermann, S., Perez, E., Petrini, L., Uesato, J., Kaplan, J., Shlegeris, B., Bowman, S. R., and Hubinger, E. (2024). Alignment faking in large language models. arXiv preprint arXiv:2412.14093 .\n- Grimmelmann, J., Sobel, B., and Stein, D. (2025). Generative misinterpretation. Harvard Journal on Legislation (forthcoming) .\n- Guan, M. Y., Joglekar, M., Wallace, E., Jain, S., Barak, B., Helyar, A., Dias, R., Vallone, A., Ren, H., Wei, J., et al. (2024). Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339 .\n- Guha, N., Lawrence, C. M., Gailmard, L. A., Rodolfa, K. T., Surani, F., Bommasani, R., Raji, I. D., Cuéllar, M.-F., Honigsberg, C., Liang, P., et al. (2024). AI regulation has its own alignment problem: The technical and institutional feasibility of disclosure, registration, licensing, and auditing. George Washington Law Review , 92:1473-1557.\n- Guha, N., Nyarko, J., Ho, D., Ré, C., Chilton, A., Chohlas-Wood, A., Peters, A., Waldon, B., Rockmore, D., Zambrano, D., et al. (2023). LegalBench: A collaboratively built benchmark for measuring legal reasoning in large language models. Advances in Neural Information Processing Systems , 36:44123-44279.\n- Gyevnár, B. and Kasirzadeh, A. (2025). AI safety for everyone. Nature Machine Intelligence , 7:531-542.\n- Habermas, J. (1996). Between Facts and Norms: Contributions to a Discourse Theory of Law and Democracy . MIT Press.\n\n- Hackenburg, K., Tappin, B. M., Hewitt, L., Saunders, E., Black, S., Lin, H., Fist, C., Margetts, H., Rand, D. G., and Summerfield, C. (2025). The levers of political persuasion with conversational artificial intelligence. Science , 390(6777):eaea3884.\n- Hacker, P., Kasirzadeh, A., and Edwards, L. (2025). AI, digital platforms, and the new systemic risk. arXiv preprint arXiv:2509.17878 .\n- Hadfield, G., Cuéllar, M.-F. T., and O'Reilly, T. (2023). It's time to create a national registry for large AI models. Carnegie Endowment for International Peace.\n- Hadfield, G. K. (2021). Explanation and justification: AI decision-making, law, and the rights of citizens. Schwartz Reisman Institute for Technology and Society.\n- Hadfield, G. K. (2026). Can AI be governed? only if we build normatively competent AI. In Nyholm, S., Kasirzadeh, A., and Zerilli, J., editors, Contemporary Debates in the Ethics of Artificial Intelligence . Wiley.\n- Hadfield, G. K. and Clark, J. (2023). Regulatory markets: The future of AI governance. arXiv preprint arXiv:2304.04914 .\n- Hadfield, G. K. and Koh, A. (2025). An economy of AI agents. arXiv preprint arXiv:2509.01063 .\n- Hadfield, G. K. and Weingast, B. R. (2012). What is law? a coordination model of the characteristics of legal order. Journal of Legal Analysis , 4:471-514.\n- Hadfield, G. K. and Weingast, B. R. (2014). Microfoundations of the rule of law. Annual Review of Political Science , 17(1):21-42.\n- Hadfield-Menell, D. and Hadfield, G. (2018). Incomplete contracting and AI alignment. arXiv preprint arXiv:1804.04268 .\n- Hammond, L., Chan, A., Clifton, J., Hoelscher-Obermaier, J., Khan, A., McLean, E., Smith, C., Barfuss, W., Foerster, J., Gavenˇ ciak, T., Han, T. A., Hughes, E., Kovaˇ rík, V ., Kulveit, J., Leibo, J. Z., Oesterheld, C., de Witt, C. S., Shah, N., Wellman, M., Bova, P., Cimpeanu, T., Ezell, C., Feuillade-Montixi, Q., Franklin, M., Kran, E., Krawczuk, I., Lamparth, M., Lauffer, N., Meinke, A., Motwani, S., Reuel, A., Conitzer, V., Dennis, M., Gabriel, I., Gleave, A., Hadfield, G., Haghtalab, N., Kasirzadeh, A., Krier, S., Larson, K., Lehman, J., Parkes, D. C., Piliouras, G., and Rahwan, I. (2025). Multi-agent risks from advanced AI. arXiv preprint arXiv:2502.14143 .\n- Han, S. S., Takashima, Y., Shen, S. Z., Liu, C., Liu, Y ., Thuo, R. K., Knowlton, S., Piskac, R., Shapiro, S. J., and Cohan, A. (2025). Courtreasoner: Can LLM agents reason like judges? In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing , pages 35279-35294.\n- Hart, H. L. A. (1958). Positivism and the separation of law and morals. Harvard Law Review , 71:593-629.\n- Hart, H. L. A. (1963). Law, liberty, and morality . Stanford University Press.\n- Hart, H. L. A. (1982). Commands and authoritative legal reasons. In Essays on Bentham: Jurisprudence and Political Philosophy . Oxford University Press.\n- Hart, H. L. A. (2012). The Concept of Law . Oxford University Press, 3rd edition.\n- He, L., Nadeem, N., Liao, M., Chen, H., Chen, D., Cuéllar, M.-F., and Henderson, P. (2025). Statutory construction and interpretation for artificial intelligence. arXiv preprint arXiv:2509.01186 .\n- Henderson, P., Hashimoto, T., and Lemley, M. (2023). Where's the liability in harmful AI speech? Journal of Free Speech Law , 3:589-650.\n- Henderson, P., Hu, J., Diab, M., and Pineau, J. (2024). Rethinking machine learning benchmarks in the context of professional codes of conduct. In Proceedings of the 2024 Symposium on Computer Science and Law , pages 109-120.\n- Henderson, P., Krass, M., Zheng, L., Guha, N., Manning, C. D., Jurafsky, D., and Ho, D. (2022). Pile of law: Learning responsible data filtering from the law and a 256GB open-source legal dataset. Advances in Neural Information Processing Systems , 35:29217-29234.\n- Hendrycks, D. (2024). Introduction to AI Safety, Ethics, and Society . CRC Press.\n\n- Hendrycks, D., Song, D., Szegedy, C., Lee, H., Gal, Y., Brynjolfsson, E., Li, S., Zou, A., Levine, L., Han, B., et al. (2025). A definition of AGI. arXiv preprint arXiv:2510.18212 .\n- Hilton, B., Buhl, M. D., Korbak, T., and Irving, G. (2025). Safety cases: A scalable approach to frontier AI safety. arXiv preprint arXiv:2503.04744 .\n- Hoffman, D. A. and Arbel, Y. (2024). Generative interpretation. New York University Law Review , 99(2):451-514.\n- Holmes, Jr., O. W. (1881). The Common Law . Little, Brown, &amp; Co.\n- Holmes, Jr., O. W. (1897). The path of the law. Harvard Law Review , 10(8):457-478.\n- Hu, W., Jing, H., Shi, H., Li, H., and Song, Y. (2025). Safety compliance: Rethinking LLM safety reasoning through the lens of compliance. arXiv preprint arXiv:2509.22250 .\n- Huang, S., Siddarth, D., Lovitt, L., Liao, T. I., Durmus, E., Tamkin, A., and Ganguli, D. (2024). Collective constitutional AI: aligning a language model with public input. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency , pages 1395-1417.\n- Hui, Z., Dong, Y. R., Shareghi, E., and Collier, N. (2025). Trident: Benchmarking LLM safety in finance, medicine, and law. arXiv preprint arXiv:2507.21134 .\n- Huq, A. Z. (2024). Artificial intelligence and the rule of law. In Routledge Handbook of the Rule of Law , pages 260-272. Routledge.\n- Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. (2023). Survey of hallucination in natural language generation. ACM computing surveys , 55(12):1-38.\n- Johnson, D. R. and Post, D. (1996). Law and borders: The rise of law in cyberspace. Stanford Law Review , 48(5):1367-1402.\n- Kaminski, M. E. (2023). Regulating the risks of AI. Boston University Law Review , 103:1347-1411.\n- Kaminski, M. E. and Selbst, A. D. (2025). An american's guide to the EU AI Act.\n- Kapoor, S., Kolt, N., and Lazar, S. (2025a). Position: Build agent advocates, not platform agents. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Kapoor, S., Stroebl, B., Kirgis, P., Nadgir, N., Siegel, Z. S., Wei, B., Xue, T., Chen, Z., Chen, F., Utpala, S., Ndzomga, F., Oruganty, D., Luskin, S., Liu, K., Yu, B., Arora, A., Hahm, D., Trivedi, H., Sun, H., Lee, J., Jin, T., Mai, Y., Zhou, Y., Zhu, Y., Bommasani, R., Kang, D., Song, D., Henderson, P., Su, Y., Liang, P., and Narayanan, A. (2025b). Holistic agent leaderboard: The missing infrastructure for AI agent evaluation. arXiv preprint arXiv:2510.11977 .\n- Kapoor, S., Stroebl, B., Siegel, Z. S., Nadgir, N., and Narayanan, A. (2024). AI agents that matter. Transactions on Machine Learning Research .\n- Kasirzadeh, A. (2025). Two types of AI existential risk: decisive and accumulative. Philosophical Studies , (7):1975-2003.\n- Kasirzadeh, A. (2026). The many faces of AI alignment. In Nyholm, S., Kasirzadeh, A., and Zerilli, J., editors, Contemporary Debates in the Ethics of Artificial Intelligence . Wiley.\n- Kasirzadeh, A. and Gabriel, I. (2023). In conversation with artificial intelligence: aligning language models with human values. Philosophy &amp; Technology , 36(2):27.\n- Kennedy, D. (1991). The stakes of law, or hale and foucault. Legal Studies Forum , 15:327.\n- Khan, A., Casper, S., and Hadfield-Menell, D. (2025). Randomness, not representation: The unreliability of evaluating cultural alignment in LLMs. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency , pages 2151-2165.\n- Kilov, D., Hendy, C., Guyot, S. Y., Snoswell, A. J., and Lazar, S. (2025). Discerning what matters: A multi-dimensional assessment of moral competence in LLMs. arXiv preprint arXiv:2506.13082 .\n- King, M. L. J. (1963). Letter from Birmingham Jail.\n\n- King, T. C., Aggarwal, N., Taddeo, M., and Floridi, L. (2020). Artificial intelligence crime: An interdisciplinary analysis of foreseeable threats and solutions. Science and engineering ethics , 26(1):89-120.\n- Kirilenko, A., Kyle, A. S., Samadi, M., and Tuzun, T. (2017). The flash crash: High-frequency trading in an electronic market. The Journal of Finance , 72(3):967-998.\n- Kirk, H. R., Gabriel, I., Summerfield, C., Vidgen, B., and Hale, S. A. (2025). Why human-AI relationships need socioaffective alignment. Humanities and Social Sciences Communications , 12(1):1-9.\n- Kirk, H. R., Whitefield, A., Rottger, P., Bean, A. M., Margatina, K., Mosquera-Gomez, R., Ciro, J., Bartolo, M., Williams, A., He, H., et al. (2024). The PRISM alignment dataset: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. Advances in Neural Information Processing Systems , 37:105236-105344.\n- Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J., and Mullainathan, S. (2018). Human decisions and machine predictions. The Quarterly Journal of Economics , 133(1):237-293.\n- Klingefjord, O., Lowe, R., and Edelman, J. (2024). What are human values, and how do we align AI to them? arXiv preprint arXiv:2404.10636 .\n\nKokotajlo, D., Alexander, S., Larsen, T., Lifland, E., and Dean, R. (2025). AI 2027.\n\nKolt, N. (2022). Predicting consumer contracts. Berkeley Technology Law Journal , 37:71-138.\n\nKolt, N. (2024). Algorithmic black swans. Washington University Law Review , 101:1177-1240.\n\nKolt, N. (2025). Governing AI agents. Notre Dame Law Review (forthcoming) .\n\n- Kolt, N., Anderljung, M., Barnhart, J., Brass, A., Esvelt, K., Hadfield, G. K., Heim, L., Rodriguez, M., Sandbrink, J. B., and Woodside, T. (2024). Responsible reporting for frontier AI development. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , volume 7, pages 768-783.\n- Korbak, T., Balesni, M., Barnes, E., Bengio, Y., Benton, J., Bloom, J., Chen, M., Cooney, A., Dafoe, A., Dragan, A., Emmons, S., Evans, O., Farhi, D., Greenblatt, R., Hendrycks, D., Hobbhahn, M., Hubinger, E., Irving, G., Jenner, E., Kokotajlo, D., Krakovna, V., Legg, S., Lindner, D., Luan, D., M˛ adry, A., Michael, J., Nanda, N., Orr, D., Pachocki, J., Perez, E., Phuong, M., Roger, F., Saxe, J., Shlegeris, B., Soto, M., Steinberger, E., Wang, J., Zaremba, W., Baker, B., Shah, R., and Mikulik, V. (2025). Chain of thought monitorability: A new and fragile opportunity for AI safety. arXiv preprint arXiv:2507.11473 .\n- Korinek, A. and Balwit, A. (2022). Aligned with whom? direct and social goals for AI systems. NBER working paper .\n- Kulveit, J., Douglas, R., Ammann, N., Turan, D., Krueger, D., and Duvenaud, D. (2025). Position: Humanity faces existential risk from gradual disempowerment. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Kundu, S., Bai, Y., Kadavath, S., Askell, A., Callahan, A., Chen, A., Goldie, A., Balwit, A., Mirhoseini, A., McLean, B., Olsson, C., Evraets, C., Tran-Johnson, E., Durmus, E., Perez, E., Kernion, J., Kerr, J., Ndousse, K., Nguyen, K., Elhage, N., Cheng, N., Schiefer, N., DasSarma, N., Rausch, O., Larson, R., Yang, S., Kravec, S., Telleen-Lawton, T., Liao, T. I., Henighan, T., Hume, T., Hatfield-Dodds, Z., Mindermann, S., Joseph, N., McCandlish, S., and Kaplan, J. (2023). Specific versus general principles for constitutional AI. arXiv preprint arXiv:2310.13798 .\n- Kwa, T., West, B., Becker, J., Deng, A., Garcia, K., Hasin, M., Jawhar, S., Kinniment, M., Rush, N., Von Arx, S., et al. (2025). Measuring AI ability to complete long tasks. arXiv preprint arXiv:2503.14499 .\n- Ladenson, R. F. (1972). Legitimate authority. American Philosophical Quarterly , 9(4):335-341.\n- Lambert, N. (2025). Character training: Understanding and crafting a language model's personality. Interconnects.\n\n- Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y ., Malik, S., Graf, V ., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. (2024). Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124 .\n- Lambert, N., Pyatkin, V., Morrison, J., Miranda, L. J. V., Lin, B. Y ., Chandu, K., Dziri, N., Kumar, S., Zick, T., Choi, Y., et al. (2025). Rewardbench: Evaluating reward models for language modeling. In Findings of the Association for Computational Linguistics: NAACL 2025 , pages 1755-1797.\n- Lazar, S. (2024). Legitimacy, authority, and democratic duties of explanation. In Oxford Studies in Political Philosophy Volume 10 . Oxford University Press.\n- Lazar, S. (2025). Governing the algorithmic city. Philosophy &amp; Public Affairs , 53(2):102-168.\n- Lazar, S. and Cuéllar, M.-F. (2025). AI agents and democratic resilience. Knight First Amendment Institute.\n- Lazar, S. and Nelson, A. (2023). AI safety on whose terms? Science , 381(6654):138.\n- Lee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J., Lu, K. R., Bishop, C., Hall, E., Carbune, V ., Rastogi, A., et al. (2024). RLAIF vs. RLHF: Scaling reinforcement learning from human feedback with AI feedback. In Forty-first International Conference on Machine Learning .\n- Leibo, J. Z., Vezhnevets, A. S., Cunningham, W. A., and Bileschi, S. M. (2025). A pragmatic view of AI personhood. arXiv preprint arXiv:2510.26396 .\n- Leibo, J. Z., Vezhnevets, A. S., Diaz, M., Agapiou, J. P., Cunningham, W. A., Sunehag, P., Haas, J., Koster, R., Duéñez-Guzmán, E. A., Isaac, W. S., et al. (2024). A theory of appropriateness with applications to generative artificial intelligence. arXiv preprint arXiv:2412.19010 .\n- Lemkin, R. (1944). Axis Rule in Occupied Europe: Laws of Occupation, Analysis of Government, Proposals for Redress . Carnegie Endowment for International Peace.\n- Lemley, M. A. and Casey, B. (2019). Remedies for robots. University of Chicago Law Review , 86(5):1311-1396.\n- Lessig, L. (1993). Fidelity in translation. Texas Law Review , 71(6):1165-1268.\n- Lessig, L. (1999). Code and Other Laws of Cyberspace . Basic Books.\n- Levi, E. H. (1949). An Introduction to Legal Reasoning . University of Chicago Press.\n- Levine, S., Franklin, M., Zhi-Xuan, T., Guyot, S. Y., Wong, L., Kilov, D., Choi, Y., Tenenbaum, J. B., Goodman, N., Lazar, S., et al. (2025). Resource rational contractualism should guide AI alignment. arXiv preprint arXiv:2506.17434 .\n- Li, N., Pan, A., Gopal, A., Yue, S., Berrios, D., Gatti, A., Li, J. D., Dombrowski, A.-K., Goel, S., Mukobi, G., et al. (2024). The wmdp benchmark: measuring and reducing malicious use with unlearning. In Proceedings of the 41st International Conference on Machine Learning , pages 28525-28550.\n- Li, S., Yang, C., Wu, T., Shi, C., Zhang, Y ., Zhu, X., Cheng, Z., Cai, D., Yu, M., Liu, L., et al. (2025). A survey on the honesty of large language models. Transactions on Machine Learning Research .\n- Lichkovski, I., Müller, A., Ibrahim, M., and Mhundwa, T. (2025). Eu-agent-bench: Measuring illegal behavior of LLM agents under EU law. In NeurIPS 2025 Workshop on Regulatable ML .\n- Lindgren, S. and Holmström, J. (2020). A social science perspective on artificial intelligence: Building blocks for a research agenda. Journal of digital social research , 2(3):1-15.\n- Lior, A. (2024). Holding AI accountable: Addressing the AI-related harms through existing tort doctrines. University of Chicago Law Review Online .\n- Liu, A., Ghate, K., Diab, M., Fried, D., Kasirzadeh, A., and Kleiman-Weiner, M. (2025). Generative value conflicts reveal LLM priorities. arXiv preprint arXiv:2509.25369 .\n- Llewellyn, K. N. (1960). The Common Law Tradition: Deciding Appeals . Little, Brown, &amp; Co.\n\n- Longpre, S., Klyman, K., Appel, R. E., Kapoor, S., Bommasani, R., Sahar, M., McGregor, S., Ghosh, A., Blili-Hamelin, B., Butters, N., et al. (2025). Position: In-house evaluation is not enough. towards robust third-party evaluation and flaw disclosure for general-purpose AI. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Lowe, R., Edelman, J., Zhi-Xuan, T., Klingefjord, O., Hain, E., Wang, V., Sarkar, A., Bakker, M. A., Barez, F., Franklin, M., et al. (2025). Full-stack alignment: Co-aligning AI and institutions with thicker models of value. In 2nd Workshop on Models of Human Feedback for AI Alignment .\n- Lynch, A., Wright, B., Larson, C., Ritchie, S. J., Mindermann, S., Hubinger, E., Perez, E., and Troy, K. (2025). Agentic misalignment: How LLMs could be insider threats. arXiv preprint arXiv:2510.05179 .\n- Maas, J. and Inglés, A. M. (2024). Beyond participatory AI. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , volume 7, pages 932-942.\n- Maas, M. M. and Olasunkanmi, T. (2025). Treaty-following AI.\n- Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. (2023). Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems , 36:46534-46594.\n- Madison, J. (1788). The federalist no. 51. In The Federalist Papers . Library of Congress.\n- Maiya, S., Bartsch, H., Lambert, N., and Hubinger, E. (2025). Open character training: Shaping the persona of AI assistants through constitutional AI. arXiv preprint arXiv:2511.01689 .\n- Malik, S., Pyatkin, V., Land, S., Morrison, J., Smith, N. A., Hajishirzi, H., and Lambert, N. (2025). Rewardbench 2: Advancing reward model evaluation. arXiv preprint arXiv:2506.01937 .\n- Marino, B., Chaudhary, Y., Pi, Y., Yew, R.-J., Aleksandrov, P., Rahman, C., Shen, W. F., Robinson, I., and Lane, N. D. (2024). Compliance cards: Automated EU AI Act compliance analyses amidst a complex AI supply chain. arXiv preprint arXiv:2406.14758 .\n- Marino, B., Hunter, R., Jamali, Z., Kalpakos, M. E., Kashyap, M., Hinton, I., Hanson, A., Nazir, M., Schnabl, C., Steffek, F., et al. (2025). AIReg-Bench: Benchmarking language models that assess AI regulation compliance. arXiv preprint arXiv:2510.01474 .\n- Martínez, E. (2024). Re-evaluating GPT-4's bar exam performance. Artificial Intelligence and Law , 33:581-604.\n- Mashaw, J. L. (2006). Accountability and institutional design: Some thoughts on the grammar of governance. In Dowdle, M. W., editor, Public Accountability: Designs, Dilemmas and Experiences , chapter 5, pages 115-156. Cambridge University Press.\n- Mashaw, J. L., Shane, P. M., Bamzai, A., Bremer, E. S., Kwoka, M. B., and Parrillo, N. R. (2025). Administrative law, the American public law system: Cases and materials . 9th edition.\n- McGregor, S. (2021). Preventing repeated real world AI failures by cataloging incidents: The AI incident database. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pages 15458-15463.\n- McKernon, E., Glasser, G., Cheng, D., and Hadfield, G. (2024). AI model registries: A foundational tool for AI governance. arXiv preprint arXiv:2410.09645 .\n\nMeta (2025). Llama guard 4 model card.\n\nMicrosoft (2024). Customer copyright commitment required mitigations.\n\nMinow, M. (2019). When Should Law Forgive? W.W. Norton &amp; Co.\n\n- Mireshghallah, N., Kim, H., Zhou, X., Tsvetkov, Y., Sap, M., Shokri, R., and Choi, Y. (2024). Can LLMs keep a secret? testing privacy implications of language models via contextual integrity theory. In The Twelfth International Conference on Learning Representations .\n- Morris, M. R., Sohl-Dickstein, J., Fiedel, N., Warkentin, T., Dafoe, A., Faust, A., Farabet, C., and Legg, S. (2024). Position: Levels of AGI for operationalizing progress on the path to AGI.\n\nIn Proceedings of the 41st International Conference on Machine Learning , volume 235, pages 36308-36321.\n\n- Muchlinski, P. (2021). Multinational enterprises and the law . Oxford University Press, 3rd edition.\n- Narayanan, A. and Kapoor, S. (2025). AI as normal technology. Knight First Amendment Institute.\n- Nay, J. J. (2022). Law informs code: A legal informatics approach to aligning artificial intelligence with humans. Northwestern Journal of Technology &amp; Intellectual Property , 20:309.\n- Needham, J., Edkins, G., Pimpale, G., Bartsch, H., and Hobbhahn, M. (2025). Large language models often know when they are being evaluated. arXiv preprint arXiv:2505.23836 .\n- Nerantzi, E. and Sartor, G. (2024). 'Hard AI crime': The deterrence turn. Oxford Journal of Legal Studies , 44(3):673-701.\n- Ngo, R., Chan, L., and Mindermann, S. (2024). The alignment problem from a deep learning perspective. In The Twelfth International Conference on Learning Representations .\n- Nissenbaum, H. (1996). Accountability in a computerized society. Science and engineering ethics , 2(1):25-42.\n- North, D. C., Wallis, J., and Weingast, B. (2009). Violence and Social Orders . Cambridge University Press.\n- Nouws, S. and Dobbe, R. (2024). The rule of law for artificial intelligence in public administration: A system safety perspective. In Prifti, K., Demir, E., Krämer, J., Heine, K., and Stamhuis, E., editors, Digital Governance , volume 39 of Information Technology and Law Series , pages 183-208. Springer.\n- Novelli, C., Floridi, L., Sartor, G., and Teubner, G. (2025). AI as legal persons: Past, patterns, and prospects. Journal of Law and Society .\n- O'Keefe, C., Ramakrishnan, K., Tay, J., and Winter, C. (2025). Law-following AI: Designing AI agents to obey human laws. Fordham Law Review , 94:57-129.\n- OpenAI (2024a). Introducing the model spec.\n- OpenAI (2024b). Learning to reason with LLMs.\n- OpenAI (2025a). ChatGPT agent system card.\n- OpenAI (2025b). OpenAI model spec, September 12, 2025.\n- OpenAI (2025). Sycophancy in GPT-4o: what happened and what we're doing about it.\n- Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems , 35:27730-27744.\n- Ovadya, A., Redman, K., Thorburn, L., Chen, Q. Z., Smith, O., Devine, F., Konya, A., Milli, S., Revel, M., Feng, K., et al. (2025). Position: Democratic AI is possible. the democracy levels framework shows how it might work. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Pasquale, F. (2019). A rule of persons, not machines: The limits of legal automation. George Washington Law Review , 87(1):1-55.\n- Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N., and Irving, G. (2022). Red teaming language models with language models. arXiv preprint arXiv:2202.03286 .\n- Phan, L., Gatti, A., Han, Z., Li, N., Hu, J., Zhang, H., Zhang, C. B. C., Shaaban, M., Ling, J., Shi, S., et al. (2025). Humanity's last exam. arXiv preprint arXiv:2501.14249 .\n- Placani, A. (2024). Anthropomorphism in AI: hype and fallacy. AI and Ethics , 4(3):691-698.\n- Posner, E. A. and Saran, S. (2025). Judge AI: assessing large language models in judicial decisionmaking.\n\n- Prabhakaran, V., Mitchell, M., Gebru, T., and Gabriel, I. (2022). A human rights-based approach to responsible AI. arXiv preprint arXiv:2210.02667 .\n- Pruss, D. and Allen, J. (2025). Against AI jurisprudence: Large language models and the false promises of empirical judging. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , volume 8, pages 2055-2066.\n- Purushothama, A., Min, J., Waldon, B., and Schneider, N. (2025). Not ready for the bench: LLM legal interpretation is unstable and uncalibrated to human judgments. In Aletras, N., Chalkidis, I., Barrett, L., Goant , ˘ a, C., Preot , iuc-Pietro, D., and Spanakis, G., editors, Proceedings of the Natural Legal Language Processing Workshop 2025 , pages 317-317. Association for Computational Linguistics.\n- Ramakrishnan, K., Smith, G., and Downey, C. (2024). US tort liability for large-scale artificial intelligence damages. RAND.\n- Rawls, J. (1993). Political Liberalism . Columbia University Press.\n- Rawls, J. (1999). A Theory of Justice . Belknap Press, Harvard University Press, Revised edition.\n- Raz, J. (1971). Legal principles and the limits of law. Yale Law Journal , 81:823.\n- Raz, J. (1975). Practical reason and norms . Oxford University Press, 1st edition.\n- Raz, J. (1979a). The Authority of Law: Essays on Law and Morality . Clarendon Press, Oxford University Press.\n- Raz, J. (1979b). The rule of law and its virtue. In The Authority of Law: Essays on Law and Morality , pages 210-229. Clarendon Press, Oxford University Press.\n- Raz, J. (2019). The law's own virtue. Oxford Journal of Legal Studies , 39(1):1-15.\n- Reidenberg, J. R. (1998). Lex informatica: The formulation of information policy rules through technology. Texas Law Review , 76:553-593.\n- Reuel, A., Hardy, A., Smith, C., Lamparth, M., Hardy, M., and Kochenderfer, M. J. (2024). BetterBench: Assessing AI benchmarks, uncovering issues, and establishing best practices. Advances in Neural Information Processing Systems , 37:21763-21813.\n- Riedl, M. O. and Desai, D. R. (2025). AI agents and the law. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society , volume 8, pages 2189-2198.\n- Rissland, E. L. (1990). Artificial intelligence and law: Stepping stones to a model of legal reasoning. Yale Law Journal , 99:1957-1981.\n- Russell, S. (2019). Human Compatible: AI and the Problem of Control . Viking.\n- Sadler, G. and Sherburn, N. (2025). Legal zero-days: A novel risk vector for advanced AI systems. arXiv preprint arXiv:2508.10050 .\n- Salaudeen, O., Reuel, A., Ahmed, A., Bedi, S., Robertson, Z., Sundar, S., Domingue, B., Wang, A., and Koyejo, S. (2025). Measurement to meaning: A validity-centered framework for AI evaluation. arXiv preprint arXiv:2505.10573 .\n- Salib, P. N. (2024). AI outputs are not protected speech. Washington University Law Review , 102:83.\n- Salib, P. N. and Goldstein, S. (2025a). AI rights for economic flourishing.\n- Salib, P. N. and Goldstein, S. (2025b). AI rights for human safety. Virginia Law Review (forthcoming) .\n- Samway, K., Mihalcea, R., and Jin, Z. (2025). When do language models endorse limitations on universal human rights principles? In Workshop on Socially Responsible Language Modelling Research .\n- Sanders, N. and Schneier, B. (2025). AI will write complex laws. Lawfare .\n- Sarkar, A., Muresanu, A. I., Blair, C., Sharma, A., Trivedi, R. S., and Hadfield, G. K. (2024). Normative modules: A generative agent architecture for learning norms that supports multi-agent cooperation. arXiv preprint arXiv:2405.19328 .\n- Scalia, A. and Garner, B. A. (2012). Reading Law: The Interpretation of Legal Texts . Thomson West.\n\n- Schauer, F. (1987). Precedent. Stanford Law Review , 39(3):571-605.\n- Schauer, F. (1991). Playing by the Rules: A Philosophical Examination of Rule-Based DecisionMaking in Law and in Life . Clarendon Press, Oxford University Press.\n- Schauer, F. (1995). Giving reasons. Stanford Law Review , 47:633-659.\n- Schauer, F. (2009). Thinking like a lawyer: a new introduction to legal reasoning . Harvard University Press.\n- Scheurer, J., Balesni, M., and Hobbhahn, M. (2024). Large language models can strategically deceive their users when put under pressure. arXiv preprint arXiv:2311.07590 .\n- Schneier, B. (2021). The coming AI hackers. Belfer Center for Science and International Affairs, Harvard Kennedy School.\n- Schwarcz, D., Manning, S., Barry, P., Cleveland, D. R., Prescott, J., and Rich, B. (2025). AI-powered lawyering: AI reasoning models, retrieval augmented generation, and the future of legal practice.\n- Schwarcz, S. L. (2008). Systemic risk. Georgetown Law Journal , 97:193-249.\n- Seger, E., Ovadya, A., Siddarth, D., Garfinkel, B., and Dafoe, A. (2023). Democratising AI: Multiple meanings, goals, and methods. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society , pages 715-722.\n- Sen, A. (2005). Human rights and the limits of law. Cardozo Law Review , 27:2913.\n- Sentinella, R. and Zweifel-Keegan, C. (2025). US state AI governance legislation tracker. IAPP.\n- Shapiro, S. J. (2006). What is the internal point of view? Fordham Law Review , 75:1157-1170.\n- Shapiro, S. J. (2011). Legality . Belknap Press, Harvard University Press.\n- Sharma, M., Tong, M., Korbak, T., Duvenaud, D., Askell, A., Bowman, S. R., Durmus, E., HatfieldDodds, Z., Johnston, S. R., Kravec, S. M., et al. (2024). Towards understanding sycophancy in language models. In The Twelfth International Conference on Learning Representations .\n- Sheshadri, A., Hughes, J., Michael, J., Mallen, A., Jose, A., Roger, F., et al. (2025). Why do some language models fake alignment while others don't? arXiv preprint arXiv:2506.18032 .\n- Simon, H. A. (1997). Administrative Behavior . Free Press, Simon &amp; Schuster, 4th edition.\n- Skalse, J., Howe, N., Krasheninnikov, D., and Krueger, D. (2022). Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471.\n- Sloane, M., Moss, E., Awomolo, O., and Forlano, L. (2022). Participation is not a design fix for machine learning. In Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization , pages 1-6.\n- Smith, B. (2023). Microsoft announces new Copilot Copyright Commitment for customers.\n- Smuha, N. A. (2024). Algorithmic rule by law: How algorithmic regulation in the public sector erodes the rule of law . Cambridge University Press.\n- Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., et al. (2024). Dolma: an open corpus of three trillion tokens for language model pretraining research. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 15725-15788.\n- Solum, L. B. (1992). Legal personhood for artificial intelligences. North Carolina Law Review , 70:1231-1287.\n- Song, Z. (2025). Value-aligned but misguided: a dilemma in AI and AGI decision making. Synthese , 206(3):138.\n- Sorensen, T., Jiang, L., Hwang, J. D., Levine, S., Pyatkin, V., West, P., Dziri, N., Lu, X., Rao, K., Bhagavatula, C., et al. (2024a). Value kaleidoscope: Engaging AI with pluralistic human values, rights, and duties. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 19937-19947.\n\n- Sorensen, T., Moore, J., Fisher, J., Gordon, M. L., Mireshghallah, N., Rytting, C. M., Ye, A., Jiang, L., Lu, X., Dziri, N., et al. (2024b). Position: A roadmap to pluralistic alignment. In International Conference on Machine Learning , pages 46280-46302.\n- Stack, K. M. (2007). The constitutional foundations of Chenery. Yale Law Journal , 116:952-1040.\n- Strathern, M. (1997). 'Improving ratings': audit in the british university system. European Review , 5(3):305-321.\n- Stray, J., Halevy, A., Assar, P., Hadfield-Menell, D., Boutilier, C., Ashar, A., Bakalar, C., Beattie, L., Ekstrand, M., Leibowicz, C., Moon Sehat, C., Johansen, S., Kerlin, L., Vickrey, D., Singh, S., Vrijenhoek, S., Zhang, A., Andrus, M., Helberger, N., Proutskova, P., Mitra, T., and Vasan, N. (2024). Building human values into recommender systems: An interdisciplinary synthesis. ACM Transactions on Recommender Systems , 2(3).\n- Summerfield, C., Argyle, L. P., Bakker, M., Collins, T., Durmus, E., Eloundou, T., Gabriel, I., Ganguli, D., Hackenburg, K., Hadfield, G. K., et al. (2025). The impact of advanced AI systems on democracy. Nature Human Behaviour , pages 1-11.\n- Sunstein, C. R. (1993). On analogical reasoning. Harvard Law Review , 106(3):741-791.\n- Sunstein, C. R. (1995). Incompletely theorized agreements. Harvard Law Review , 108(7):1733-1772.\n- Sunstein, C. R. (2001). Of artificial intelligence and legal reasoning. University of Chicago Law School Roundtable , 8:29-45.\n- Sunstein, C. R. (2024). Artificial intelligence and the first amendment. George Washington Law Review , 92:1207.\n- Surani, F., Gailmard, L. A., Casasola, A., Magesh, V., Robitschek, E. J., and Ho, D. E. (2025). What is the law? a system for statutory research (STARA) with large language models. In 20th International Conference on Artificial Intelligence and Law .\n- Susskind, R. (1987). Expert systems in law: a jurisprudential inquiry . Clarendon Press, Oxford University Press.\n- Tallarita, R. (2023). AI is testing the limits of corporate governance. Harvard Business Review .\n- Tamanaha, B. Z. (2004). On the rule of law: History, politics, theory . Cambridge University Press.\n- Tessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., and Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science , 386(6719):2852.\n- Thomas, R. and Uminsky, D. (2020). The problem with metrics is a fundamental problem for AI. arXiv preprint arXiv:2002.08512 .\n- Tomasev, N., Franklin, M., Leibo, J. Z., Jacobs, J., Cunningham, W. A., Gabriel, I., and Osindero, S. (2025). Virtual agent economies. arXiv preprint arXiv:2509.10147 .\n- Turing, A. (1950). Computing machinery and intelligence. Mind: A Quarterly Review of Psychology and Philosophy , 59(236):433-460.\n- Tyler, T. R. (2006). Why people obey the law . Princeton University Press, 2nd edition.\n- U.S. Supreme Court (1868). United states v. kirby. 74 U.S. (7 Wall.) 482.\n- Uuk, R., Gutierrez, C. I., Guppy, D., Lauwaert, L., Kasirzadeh, A., Velasco, L., Slattery, P., and Prunkl, C. (2024). A taxonomy of systemic risks from general-purpose AI. arXiv preprint arXiv:2412.07780 .\n- Velasco, J. (2006). The fundamental rights of the shareholder. UC Davis Law Review , 40:407.\n- Waldon, B., Schneider, N., Wilcox, E., Zeldes, A., and Tobia, K. (2025). Large language models for legal interpretation? don't take their word for it. Georgetown Law Journal (forthcoming) .\n- Waldron, J. (2016). The rule of law. Stanford Encyclopedia of Philosophy .\n\n- Wallace, E., Xiao, K., Leike, R., Weng, L., Heidecke, J., and Beutel, A. (2024). The instruction hierarchy: Training LLMs to prioritize privileged instructions. arXiv preprint arXiv:2404.13208 .\n- Wallach, H., Desai, M., Cooper, A. F., Wang, A., Atalla, C., Barocas, S., Blodgett, S. L., Chouldechova, A., Corvi, E., Dow, P. A., et al. (2025). Position: Evaluating generative AI systems is a social science measurement challenge. In Forty-second International Conference on Machine Learning Position Paper Track .\n- Wan, A., Klyman, K., Kapoor, S., Maslej, N., Longpre, S., Xiong, B., Liang, P., and Bommasani, R. (2025). The 2025 foundation model transparency index. arXiv preprint arXiv:2512.10169 .\n- Wang, X. and Wellman, M. P. (2020). Market manipulation: An adversarial learning framework for detection and evasion. In 29th International Joint Conference on Artificial Intelligence .\n- Wei, A., Haghtalab, N., and Steinhardt, J. (2023). Jailbroken: How does LLM safety training fail? Advances in Neural Information Processing Systems , 36:80079-80110.\n- Wei, K. and Heim, L. (2025). Designing incident reporting systems for harms from general-purpose AI. arXiv preprint arXiv:2511.05914 .\n- Wei, K., Paskov, P., Dev, S., Byun, M. J., Reuel, A., Roberts-Gaal, X., Calcott, R., Coxon, E., and Deshpande, C. (2025). Position: Human baselines in model evaluations need rigor and transparency (with recommendations &amp; reporting checklist). In Forty-second International Conference on Machine Learning Position Paper Track .\n- Weidinger, L., Raji, I. D., Wallach, H., Mitchell, M., Wang, A., Salaudeen, O., Bommasani, R., Ganguli, D., Koyejo, S., and Isaac, W. (2025). Toward an evaluation science for generative AI systems. arXiv preprint arXiv:2503.05336 .\n- Weidinger, L., Rauh, M., Marchal, N., Manzini, A., Hendricks, L. A., Mateos-Garcia, J., Bergman, S., Kay, J., Griffin, C., Bariach, B., et al. (2023). Sociotechnical safety evaluation of generative AI systems. arXiv preprint arXiv:2310.11986 .\n- Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B., Kasirzadeh, A., et al. (2022). Taxonomy of risks posed by language models. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency , pages 214-229.\n- Weil, G. (2024). Tort law as a tool for mitigating catastrophic risk from artificial intelligence.\n- Wilf-Townsend, D. and Tobia, K. (2025). AI-generated legal texts.\n- Williams, M., Carroll, M., Narang, A., Weisser, C., Murphy, B., and Dragan, A. (2025a). On targeted manipulation and deception when optimizing LLMs for user feedback. In The Thirteenth International Conference on Learning Representations .\n- Williams, S., Schuett, J., and Anderljung, M. (2025b). On regulating downstream AI developers. European Journal of Risk Regulation , page 1-29.\n- Wu, T. (2003). Network neutrality, broadband discrimination. Journal of Telecommunications and High Technology Law , 2:141-175.\n- Wu, X., Hong, G., Chen, P., Chen, Y., Pan, X., and Yang, M. (2025). Prison: Unmasking the criminal potential of large language models. arXiv preprint arXiv:2506.16150 .\n- Zanzotto, F. M. (2019). Human-in-the-loop artificial intelligence. Journal of Artificial Intelligence Research , 64:243-252.\n- Zeng, Y., Yang, Y., Zhou, A., Tan, J. Z., Tu, Y., Mai, Y., Klyman, K., Pan, M., Jia, R., Song, D., Liang, P., and Li, B. (2025). Air-bench 2024: A safety benchmark based on regulation and policies specified risk categories. In The Thirteenth International Conference on Learning Representations .\n- Zhang, A. K., Perry, N., Dulepet, R., Ji, J., Menders, C., Lin, J. W., Jones, E., Hussein, G., Liu, S., Jasper, D. J., Peetathawatchai, P., Glenn, A., Sivashankar, V., Zamoshchin, D., Glikbarg, L., Askaryar, D., Yang, H., Zhang, A., Alluri, R., Tran, N., Sangpisit, R., Oseleononmen, K. O., Boneh, D., Ho, D. E., and Liang, P. (2025). Cybench: A framework for evaluating cybersecurity capabilities and risks of language models. In The Thirteenth International Conference on Learning Representations .\n\n- Zheng, L., Guha, N., Arifov, J., Zhang, S., Skreta, M., Manning, C. D., Henderson, P., and Ho, D. E. (2025). A reasoning-focused legal retrieval benchmark. In Proceedings of the 2025 Symposium on Computer Science and Law , pages 169-193.\n- Zhi-Xuan, T., Carroll, M., Franklin, M., and Ashton, H. (2025). Beyond preferences in AI alignment. Philosophical Studies , 182(7):1813-1863.\n- Zhu, Y., Jin, T., Pruksachatkun, Y., Zhang, A. K., Liu, S., Cui, S., Kapoor, S., Longpre, S., Meng, K., Weiss, R., Barez, F., Gupta, R., Dhamala, J., Merizian, J., Giulianelli, M., Coppock, H., Ududec, C., Kellermann, A., Sekhon, J. S., Steinhardt, J., Schwettmann, S., Narayanan, A., Zaharia, M., Stoica, I., Liang, P., and Kang, D. (2025a). Establishing best practices in building rigorous agentic benchmarks. In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track .\n- Zhu, Y., Kellermann, A., Bowman, D., Li, P., Gupta, A., Danda, A., Fang, R., Jensen, C., Ihli, E., Benn, J., Geronimo, J., Dhir, A., Rao, S., Yu, K., Stone, T., and Kang, D. (2025b). CVE-bench: A benchmark for AI agents' ability to exploit real-world web application vulnerabilities. In Forty-second International Conference on Machine Learning .\n- Zittrain, J. (2008). The Future of the Internet-And How to Stop It . Yale University Press.\n\nZittrain, J. (2024). We need to control AI agents now.",
  "fetched_at_utc": "2026-02-09T13:55:36Z",
  "sha256": "e337659501047e3b8b9da737fce38a184f9cefe47d27820ef173badbf55653c5",
  "meta": {
    "file_name": "Legal Alignment for Safe and Ethical AI.pdf",
    "file_size": 608504,
    "mtime": 1770643036,
    "docling_errors": []
  }
}