{
  "doc_id": "pdf-pdfs-european-union-artificial-intelligence-act-bird-bird-778dc0538cbf",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\European Union Artificial Intelligence Act - Bird & Bird.pdf",
  "title": "European Union Artificial Intelligence Act - Bird & Bird",
  "text": "## European Union Artificial Intelligence Act: a guide\n\n7 April 2025\n\n## Contents\n\n<!-- image -->\n\n- OVERVIEW, KEY CONCEPTS &amp; TIMING OF IMPLEMENTATION 1\n\nOverview Key concepts\n\nTimeline\n\n- MATERIAL AND TERRITORIAL SCOPE 2\n\n<!-- image -->\n\nMaterial scope Territorial scope Exclusions\n\nRelationship with other regulatory frameworks\n\n- PROHIBITED AI PRACTICES 3\n\n<!-- image -->\n\nProhibited AI practices To whom do the prohibitions apply? Enforcement and Fines\n\n- HIGH-RISK AI SYSTEMS 4\n\nClassification of an AI system as a high-risk AI system\n\nObligations for providers of high-risk AI systems\n\nHarmonised standards and conformity assessment procedure for providers of high-risk AI systems\n\nObligations for deployers of high-risk AI systems Obligations for other parties in connection with high-risk AI systems\n\n- GENERAL-PURPOSE AI MODELS 5\n\nBackground and relevance of general-purpose AI models of personal data\n\nTerminology and general-purpose AI value chain Obligations for providers of general-purpose AI models\n\nGeneral-purpose AI models with systemic risk\n\n<!-- image -->\n\nLegal 500 for Artificial Intelligence\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- TRANSPARENCY OBLIGATIONS 6\n\nGeneral transparency obligations\n\nTransparency obligations for high-risk AI systems Timing and format\n\nTransparency obligations at the national level and codes of practice\n\nRelationship with other regulatory frameworks\n\n- REGULATORY SANDBOXES 7\n\nAI regulatory sandboxes Real-world testing of AI systems\n\n- ENFORCEMENT &amp; GOVERNANCE 8\n\n<!-- image -->\n\nOverview Post-marking obligations Market surveillance authorities Procedures for enforcement Authorities protecting fundamental rights General-purpose AI models Penalties Remedies for third parties Governance\n\n- AI ACT: WHAT'S NEXT 9\n\nAI Act application deadlines Delegated Acts Implementing Acts Commission Guidelines Codes of conduct and practice Standards Liability\n\n- OUR GLOBAL CONTRIBUTORS 10\n\nDistinguished for our client satisfaction\n\n<!-- image -->\n\n## Overview, key concepts &amp; timing of implementation\n\n## Overview\n\nThe European Union (EU) stands as a pioneer in the regulation of artificial intelligence (AI), setting a global benchmark with its proactive approach to ensuring ethical and responsible AI development. Indeed, it seems we may witness a new Brussels effect, reminiscent of the influence wielded by the GDPR. The EU's comprehensive and precautionary framework prioritises transparency, accountability, and human rights.\n\nThe AI Act applies beyond the borders of the EU - many of its provisions apply regardless of whether the providers are established or located within the EU or in a third country. The AI Act applies to any provider or entity responsible for deploying an AI system if ' the output produced by the system is intended to be used ' in the EU. Foreign suppliers must appoint an authorised representative in the Union to ensure compliance with the Act's provisions. However, the AI Act does not apply to public authorities of third countries or to international organisations under police and judicial cooperation agreements with the Union, nor to AI systems placed on the market for military defence or national security purposes. This broad scope aims to ensure comprehensive regulation of AI systems and their uses.\n\n## What you can expect from this guide\n\n- This chapter provides an overview of the whole AI Act, its key concepts and the dates from when its provisions will apply.\n- Chapter 2 looks at the territorial and material scope of the AI Act.\n- Chapters 3, 4, 5 and 6 address the requirements the AI Act imposes on different types of AI - prohibited practices; high risk systems; general purpose AI; and AI where greater transparency is needed.\n- Chapter 7 explains the AI Act's arrangements for testing AI in regulatory sandboxes. Chapter 8 looks at governance and enforcement.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n4\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n- Chapter 9 summarises the numerous further measures that have to follow the adoption of the AI Act.\n- Last, Chapter 10 includes all the contributors to this guide.\n\n## A risk-focused approach\n\nThe EU approach to AI regulation is characterised by its risk-based framework. This regulation adopts a technology-neutral perspective, categorising AI systems based on their risk level, ranging from minimal to high risk. This system ensures that higher-risk AI applications, particularly those that can significantly impact fundamental rights, are either prohibited or subjected to stricter requirements and oversight.\n\nThe EU places a strong emphasis on promoting the development and use of responsible AI. The AI Act mandates strict measures for data security and user privacy, ensuring that AI systems are designed and deployed with these considerations at the forefront. This includes rigorous requirements for how data is handled and protected, ensuring that users' personal information remains secure.\n\nAdditionally, the AI Act requires comprehensive risk assessments for AI systems. These assessments help identify and mitigate potential risks associated with AI technologies, fostering transparency and accountability among AI providers. By making these evaluations mandatory, the EU ensures that AI developers thoroughly understand and address the implications of their technologies.\n\nThis proactive approach aims to build public trust in AI technologies by protecting users' rights and well-being. By prioritising data security, privacy, and risk management, the EU seeks to reassure the public that AI can be used safely and ethically. This focus on responsible development helps to promote broader acceptance and integration\n\nof AI technologies, ultimately benefiting society as a whole. The AI Act has been developed not only to create laws for AI systems, but also to establish an ethical framework for their use, to ensure that organisations consider the impact of their AI systems on people, other businesses, the environment and many other aspects of our lives.\n\n## Ethics at the heart of the AI Act\n\nThe AI Act explicitly builds on the Ethical Guidelines on Trustworthy AI, which were published by the European Commission in 2019. While these guidelines remain non-binding, many of their principles have been directly incorporated into the AI Act. The best example of this approach is that in many of its provisions, the AI Act refers directly to the fundamental rights enshrined in the Charter of Fundamental Rights of the European Union. For example, high-risk AI systems are those that have a significant harmful impact on the health, safety and fundamental rights of persons in the Union.\n\nThe proper application of the AI Act will in many cases require an analysis of the risks to fundamental rights, which includes both legal and ethical issues. It can therefore be said that ethics has been embedded into the AI Act.\n\n## Governance\n\nThe European Union adopts a decentralised supervision model, promoting collaboration with various national authorities. The AI Act establishes the European Artificial Intelligence Office (the AI Office) as an independent entity, serving as the central authority on AI expertise across the EU, and playing a crucial role in implementing of the legal framework. This office will encourage the development of trustworthy AI and support international collaboration. The European Artificial Intelligence Board will be composed of one representative per Member State and the European Data Protection Supervisor shall participate as observer.\n\nThe AI Office aims to promote and facilitate the creation, review, and adaptation of codes of good practice, considering international approaches. To ensure these codes reflect the current state of the art and incorporate diverse perspectives, the AI Office will collaborate with relevant national authorities and may consult with civil society organisations, stakeholders, and experts, including scientific experts.\n\n<!-- image -->\n\n## Key concepts\n\nAI systems (see also Chapter 2)\n\nMost of the AI Act applies to 'AI systems' , which the Act defines as ' a machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments '.\n\nIt is worth noting that the AI Act does not define 'artificial intelligence' , but only the term 'artificial intelligence system' . The definition of an AI system is intentionally consistent with the OECD definition of an AI system. The definition does not mention any specific technology or currently known approaches to artificial intelligence systems. With the rapidly evolving nature of AI, this prevents the AI Act from becoming obsolete due to technological developments.\n\nA key element of this definition is the AI system's ability to 'infer' . This should allow for a clear distinction between AI systems and traditional software. If a computer program operates according to rules defined in advance by the programmers, it is not an AI system; if a system is built using techniques that allow the program to create rules of its own based on input data or data sets provided to the program, then it is an AI system. The definition of an AI system is discussed further in guidelines published by the Commission on 6 February 2025.\n\n## Obligations across the supply chain\n\n(see also Chapter 2)\n\nThe AI Act applies to all participants in the supply chain, starting with the 'provider' and also encompassing the 'importer' , 'distributor' and 'deployer' of the system. Most responsibilities lie with the provider, and next with the deployer.\n\nAn importer, distributor or deployer may become a provider of the high-risk AI system if they have put their name or trademark on the system. They may become a provider of a high-risk system (see page 5) if they make substantial modifications to, or modify the intended purpose of the AI system, which renders the system high-risk.\n\n## Risk approach to classification of AI systems\n\nThe AI Act defines risk as ' the combination of the probability of harm occurring and the severity of that harm.'\n\nThe risk-based classification of AI systems is a fundamental aspect of the AI Act, focusing on the potential harm to health, safety, and fundamental human rights that an AI system may cause. This approach categorises AI systems into four distinct risk levels:\n\n1. Unacceptable risk: AI systems that pose such significant risks are unacceptable and therefore prohibited.\n2. High risk: High-risk AI systems are subject to stringent regulatory requirements.\n3. Limited risk: AI systems in this category pose a limited risk, but have specific transparency obligations.\n4. Minimal or no risk: AI systems that pose minimal or no risk have no regulatory restrictions under the AI Act.\n\n## Unacceptable risk: prohibited practices\n\n(see also Chapter 3)\n\nThe AI Act contains a list of prohibited AI practices, which should be understood as a prohibition on placing on the market, putting into service, or using an AI system that employs any of these practices. The list prohibits:\n\n- using subliminal techniques or purposefully manipulative or deceptive techniques to materially distort behaviour, leading to significant harm;\n- exploiting vulnerabilities of an individual or group due to their specific characteristics, leading to significant harm;\n- social scoring systems i.e. evaluating or classifying of an individual or group based on their social behaviour or personal characteristics, leading to detrimental or unfavourable treatment;\n- evaluating a person's likelihood of committing a criminal offence, based solely on profiling or personal characteristics; except when used to support human assessment based\n\n<!-- image -->\n\non objective and verifiable facts linked to a criminal activity;\n\n- facial recognition databases based on untargeted scraping from the internet or CCTV;\n- inferring emotions in workplaces or educational institutions, except for medical or safety reasons;\n- biometric categorisation systems that categorise a person based on their sensitive data, except for labelling or filtering lawfully acquired biometric datasets such as images in the area of law enforcement;\n- real-time remote biometric identification systems in publicly available spaces for law enforcement purposes, except in narrowly defined circumstances.\n\nIn some cases, the AI Act contains exceptions that allow these 'prohibited' practices to be used in certain situations. A good example is real-time biometric identification, where the Regulation allows its use in exceptional circumstances. The application of these exceptions requires notifications or prior authorisations. The Commission published guidelines on prohibited AI practices on 4 February 2025.\n\n## High-risk AI systems (see also Chapter 4)\n\nThe extensive regulation of high-risk AI systems constitutes a major part of the AI Act. AI systems are identified as high-risk AI systems if they have a significant harmful impact on the health, safety and fundamental rights of persons in the Union. There are two categories of high-risk AI systems which are regulated differently:\n\n- AI systems intended to be used as a product or a safety component of a product which is covered by EU harmonisation legislation, such as civil aviation, vehicle security, marine equipment, radio equipment, toys, lifts, pressure equipment, medical devices, personal protective equipment (listed in Annex I to the AI Act).\n- AI systems listed in Annex III, such as AI used in education, employment, credit scoring, law enforcement, migration, remote biometric identification systems, and AI systems used as a safety component in critical infrastructure. This list can be amended by the Commission.\n\nThe first category of high-risk systems is covered by both the harmonisation legislation and the AI Act.\n\nProviders have an option of integrating the requirements of the AI Act into the procedures required under the respective Union harmonisation legislation listed in Section A of Annex I. In addition, only selected provisions of the AI Act apply to highrisk AI systems in relation to products covered by Union harmonisation legislation listed in Section B of Annex I (such as aviation equipment).\n\nPractical assistance in  the classification of highrisk AI systems will be provided no later than 2 February 2026 by the Commission, to include a comprehensive list of practical examples of use cases of high-risk and non-high-risk AI systems.\n\n## Exceptions to the qualification of high-risk AI system\n\nIf a high-risk AI system listed in Annex III does not pose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing the outcome of decision making, it will not be treated as a high-risk AI system.\n\nSuch situations may only arise in four cases where the AI system is intended to:\n\n- perform a narrow procedural task;\n- improve the result of a previously completed human activity;\n- detect decision-making patterns or deviations from prior decision-making patterns, and is not meant to replace or influence the previously completed human assessment without proper human review; or\n- perform a preparatory task to an assessment relevant for the purposes of the use cases listed in Annex III.\n\nIf, however, the AI system performs profiling of natural persons, it is always considered a highrisk AI system and cannot fall into one of the above exceptions.\n\nThis exemption is likely to play an important role in practice, as it allows avoiding the obligations and costs associated with placing a high-risk AI system on the market. One of the options is, for example, to carve out those parts of an AI system that can take advantage of this exemption to limit the scope of the high-risk AI system.\n\nHowever, even if a provider relies on the exemption, its assessment of the system must be documented, and the system must still be\n\n<!-- image -->\n\nregistered in the EU database for high-risk systems before it is placed on the market or put into service.\n\n## Extensive obligations for high-risk AI systems\n\nThe requirements that must be met by providers of high-risk AI systems are strict. These requirements include, in particular, the need to document every stage of the development of the AI system, to meet obligations regarding the use of high-quality data for training, to produce system documentation that provides users with full information about the nature and purpose of the system, or to ensure the accuracy, robustness and cybersecurity of the systems. High-risk AI systems will also have to be registered in an EU database, which will be publicly available.\n\n## Obligations across the supply chain of AI systems\n\nThe AI Act imposes obligations on all participants in the supply chain of a high-risk system throughout its life cycle. The responsibilities are not only those of the 'provider', but also those of the 'importer', 'distributor' and 'deployer' of the system, although most of the responsibilities lie with the provider and the deployer.\n\nThe primary duty of the importer and distributor is to verify that the high-risk AI system being imported or distributed meets the requirements of the AI Act. Moreover, an importer, distributor or deployer may become a provider of the high-risk AI system if they have put their name or trademark on the system, made substantial modifications or they have modified the intended purpose of the AI system, which renders the system high-risk.\n\n## General-purpose AI models (see also Chapter 5)\n\nThe distinction between AI models and AI systems is crucial for the application of the AI Act. AI models are essential components of AI systems, but they do not constitute AI systems on their own. AI models require the addition of other components, such as a user interface, to become AI systems. The AI Act mostly regulates AI systems, not models. However, it does contain rules on general-purpose AI models.\n\nThe AI Act provides rules for all general-purpose AI models and additional rules for generalpurpose AI models that pose systemic risks. They apply in the following situations:\n\n- where the provider of a general-purpose AI model integrates its own model into its own AI system that is made available on the market or put into service;\n- where the provider of a general-purpose AI model only offers its own model to providers of AI systems.\n\nThe distinction may be particularly important in cases where a general-purpose AI model of one provider is used in a general-purpose AI system of a second provider, which in turn is integrated into another AI system with a more specific purpose, built by a third provider.\n\n## Transparency obligations (see also Chapter 6)\n\nThe AI Act includes transparency obligations for four types of AI systems:\n\n- AI systems designed to interact directly with natural persons;\n- AI systems, including general-purpose AI systems, that generate synthetic audio, image, video or text content;\n- emotion recognition or biometric categorisation systems;\n- AI systems that generate or manipulate images, audio or video that are deepfakes.\n\nIn all these cases, the user must be informed about the use of the AI system. There are also more detailed obligations, for example to mark the output in a machine-readable way so that it can be identified as artificially generated or manipulated.\n\n## Complex supervision and enforcement structure (see also Chapter 8)\n\nThe AI Act provides for a complex, multi-level structure for overseeing implementation. It includes both national and EU level entities. At each level there will be several types of bodies, such as notifying authorities and notified bodies, conformity assessment bodies, the European AI Board, the AI Office, national competent authorities and market surveillance authorities.\n\nThese authorities will not only control compliance, but also support the market by,\n\n<!-- image -->\n\namong other things, developing codes of conduct, organising AI regulatory sandboxes and providing support for SMEs and start-ups.\n\n## Role of technical standards, codes of practice and guidelines (see also Chapters 7, 8 and 9)\n\nThe AI Act requires providers of high-risk AI systems to affix a European Conformity (CE) marking. The CE marking will show compliance with the requirements of the AI Act. For the mark to be issued, providers will have to apply harmonised technical standards. In addition, high-risk AI systems or general-purpose AI models which are in conformity with harmonised standards shall be presumed to be in conformity with the requirements of the AI Act to the extent that those standards cover those requirements or obligations. Consequently, the rather general provisions of the AI Act will be complemented by technical standards that will provide the concrete forms of compliance with the AI Act. Thus, we can expect that the CE marking and technical standards will play very important role in practical application of the AI Act.\n\nCodes of practice should also form an important role. If they are not prepared by market participants, the Commission may provide the common rules within implementing acts. The Commission can also, by way of an implementing act, approve a code of practice and give it a general validity within the Union. In addition, the Commission has the obligation to develop several guidelines on the practical implementation of the Regulation.\n\nThe AI Act can therefore be seen as just a framework for more detailed obligation that will result from many further documents and legal acts.\n\n## Enforcement (see also Chapter 8)\n\nThe AI Act stipulates significant penalties for non-compliance, which vary depending on the nature of the violation and the size of the entity involved. Actions that may incur high penalties include:\n\n- non-compliance with the rules on prohibited AI practices outlined in article 5. Offenders in such cases may face administrative fines of up to €35,000,000 or up to 7% of annual worldwide turnover, whichever is higher, for undertakings.\n\n- violations related to data, data governance, and transparency: AI systems found in breach of these provisions could be fined up to €20 million or 4% of annual global turnover.\n- failure to comply with any of the provisions set out in article 99 (e.g. relating to high-risk AI systems), will be subject to administrative fines of up to €15 million or, if the offender is a company, up to 3% of its global turnover in the preceding financial year, whichever is higher.\n\nThese penalties underscore the importance of complying with the AI Act's regulations. It is essential for companies to fully grasp these penalties and ensure that their AI systems meet the Act's requirements.\n\n## Timeline\n\nThe AI Act becomes applicable on a staggered basis. There are also transitional arrangements for AI systems that had been placed on the market or put into service before certain dates. The AI Act applies to all operators of high-risk AI systems that have been placed on the market or put into service before 2 August 2026, unless those systems are subsequently subject to significant change in design (in which case, the provisions would apply in full with respect to the redesigned system). The relevant dates of application are set out below.\n\n| 12 July 2024     | The AI Act was published in the Official Journal of the EU, triggering the dates for specific provisions in the Regulation becoming applicable.                                                                                                      |\n|------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2 February 2025  | Prohibited practices ban applies (Chapter II). AI literacy rules apply (article 4).                                                                                                                                                                  |\n| 2 May 2025       | Codes of practice for general-purpose AI must be ready (article 56 (9)).                                                                                                                                                                             |\n| 2 August 2025    | National authorities designated (Chapter III Section 4). Obligations for General-purpose AI (GPAI) (Chapter V). Governance (at EU and national level) (Chapter VII). Confidentiality and penalties (other than in relation to gen-AI) (Chapter XII). |\n| 2 August 2026    | Start of application of all other provisions of the EU AI Act (unless a later date applies below).                                                                                                                                                   |\n| 2 August 2027    | High-risk categories listed in Annex I. General-purpose AI models placed on the market before 2 August 2025 (article 111).                                                                                                                           |\n| 2 August 2030    | High-risk AI systems (other than those listed below), which have been placed on the market or put into service before 2 August 2026 and which are intended to be used by public authorities (article 111).                                           |\n| 31 December 2030 | Components of large-scale IT systems listed in Annex X, which have been placed on the market or put into service before 2 August 2027 (article 111).                                                                                                 |\n\n<!-- image -->\n\n## Material and territorial scope\n\n<!-- image -->\n\n- The AI Act covers AI systems, general-purpose AI models and prohibited AI practices.\n- Obligations can be imposed on six categories of economic actors: providers, importers, distributors, product manufacturers, authorised representatives and deployers.\n- Economic operators involved with high-risk AI systems have significant obligations. Providers and deployers of certain categories of AI systems are also subject to transparency obligations.\n- Providers of general-purpose AI models are subject to obligations.\n- The AI Act applies when an AI system or general-purpose AI model is placed on the EU market, put into service in the EU, imported into or distributed in the EU. It also applies where an AI system is used by a deployer who has their place of establishment or is in the EU.\n- Providers and deployers of AI systems who fall within scope of the AI Act are subject to AI literacy requirements from 2 February 2025.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nDetermine whether you, your suppliers or your customers will be an operator falling within the material and territorial scope of the AI Act.\n\n<!-- image -->\n\n<!-- image -->\n\n- If you or your supply chain fall within the scope of the AI Act, check whether any AI systems or AI models fall within one or more of the regulated categories.\n- If you are a provider or deployer of AI systems within the scope of the AI Act, ensure you have taken steps to comply with the Act's AI literacy requirements.\n\n## Material scope\n\nThe AI Act primarily provides harmonised rules for the placing on the market, the putting into service, and the use of AI systems. It imposes an extensive set of obligations on 'high-risk' AI systems and transparency obligations on certain AI systems. It also prohibits certain AI practices and regulates the supply of general-purpose AI models in the EU.\n\nThe AI Act also sets out rules for market monitoring, market surveillance, governance and enforcement, which includes administrative fines, as well as measures to support innovation, with a particular focus on small and medium enterprises, such as through the operation of AI sandboxes. It also establishes two new bodies: (i) the European Artificial Intelligence Board which is tasked with advising and assisting the European Commission and EU Member States to facilitate the consistent and effective application of the AI Act; and (ii) the AI Office, which has been established within the European Commission and is tasked with implementing the AI Act, fostering the development and use of trustworthy AI and promoting international cooperation.\n\n## Regulated persons: Operators\n\nThe AI Act imposes obligations on six categories of entities: providers, deployers, importers, distributors, product manufacturers and authorised representatives - the term 'operator' is used to describe all of them. There will always be a provider for an AI system or a generalpurpose AI model. Whether there will also be\n\nThe regulated operators under the AI Act are:\n\n| Operator                                                    |                         | Role                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n|-------------------------------------------------------------|-------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Relevant for both AI systems and general- purpose AI models | Provider (article 3(3)) | Develops an AI system or a general-purpose AI model or has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge. Although the definition of 'placing on the market' refers to the EU market, a person can still be deemed a provider regulated by the AI Act even if they do not place an AI system on the EU market, where the output of the AI system is used in the EU. See 'Territorial Scope' further below. A provider can be a natural or legal person, public authority, agency or other body. EU institutions, bodies, offices and agencies may also act as a provider of an AI system. |\n\n<!-- image -->\n\nother operators will depend on the way in which the AI system or general-purpose AI model is being supplied and deployed.  Most operators are defined with reference to three key terms adapted from the EU product legislation referenced in Annex I of the AI Act: 'making available' , 'placing on the market' and 'putting into service' .\n\n'making available' is the supply of an AI system or a general-purpose AI model for distribution or use on the EU market in the course of a commercial activity, whether in return for payment or free of charge;\n\n'placing on the market' is the first making available of an AI system or a generalpurpose AI model on the EU market; and\n\n'putting into service' is the supply of an AI system for first use directly to the deployer or for own use in the EU for its intended purposes.\n\nThe term 'use' is not defined in the AI Act. In essence, 'use' would be perceived by reference to the key characteristic of an AI system which is to infer, from inputs it receives, how to generate outputs. These three terms are discussed in section 2.3 of the Commission's Guidelines on prohibited AI practices, which provides illustrative examples of each activity in the context of the restrictions on prohibited practices.\n\n|                              |                                          | It is also possible to become a provider where an AI system has already been placed on the market or put into service in the EU by another provider, by taking one of the steps set out in article 25(1) (a)-(c). See further below, under 'High-risk AI systems' .                                                                                                                                                                                                                                                                                                                                                                                              |\n|------------------------------|------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                              | Authorised representative (article 3(5)) | An EU-established natural or legal person appointed by a provider established outside the EU to act as their authorised representative. The role includes ensuring that the documentation required by the AI Act is available to the competent authorities and co-operating with those authorities. See article 22 (for high-risk AI systems) and article 54 (for general- purpose AI models).                                                                                                                                                                                                                                                                   |\n| Relevant for AI systems only | Deployer (article 3(4))                  | Uses an AI system under its authority (excluding use in the course of personal, non-professional activity). A deployer can be a natural or legal person, public authority, agency or other body. EU institutions, bodies, offices and agencies may also act as a deployer of an AI system.                                                                                                                                                                                                                                                                                                                                                                       |\n|                              | Importer (article 3(6))                  | Natural or legal person located or established in the EU that places an AI system bearing the name or trademark of a person not established in the EU on the EU market.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n|                              | Distributor (article 3(7))               | Natural or legal person in the supply chain, other than the provider or the importer, that makes an AI system available on the EU market.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n|                              | Product manufacturer (article 25(3))     | In certain circumstances, a product manufacturer will be considered the 'provider' of a high-risk AI system where: this is a safety component of a product covered by the AI Act (by virtue of being subject to the EU product safety legislation referenced in Section A of Annex I), and the manufacturer places the AI system on the EU market or puts it into service in the EU together with that product and under its own name or trademark. The term 'product manufacturer' is not defined in the AI Act - but Recital 87 clarifies that this is the 'manufacturer' defined under the EU product safety legislation referenced in Annex I to the AI Act. |\n\n## Indirect obligations under the AI Act\n\nThe AI Act imposes indirect obligations on component suppliers to providers of high-risk AI systems. Those supplying AI systems, tools, services, components, or processes that are used or integrated in a high-risk AI system are required to enter into a written agreement with the provider of the high-risk AI system and to enable the latter to comply with its obligations under the AI Act (article 25(4)). This obligation does not apply to third parties who make such tools, services, processes or components (other than general-purpose AI models) accessible to the public under a free and open-source licence.\n\n<!-- image -->\n\n## Rights granted by the AI Act\n\nUnlike the GDPR, which provides a comprehensive set of rights to individuals, the rights under the AI Act are limited. The AI Act only confers a right to explanation of individual decision-making on affected persons located in the EU (article 86). Affected persons are those who are subject to a decision which has a legal or similarly significant effect on them and which is based on the output of one of the high-risk AI systems identified in Annex III. The wording used here is similar to that used under the automated decision-making provisions of the GDPR (article 22 GDPR); the scope of the two provisions however is not identical.\n\n## Regulated subject matter: AI systems\n\nAn AI system is defined broadly in article 3(1) as: ' a machine-based system that is designed to operate with varying levels of autonomy, and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments' .\n\nThis definition is intended to align with the definition used by the OECD AI Principles. A key characteristic of AI systems is their capability to infer, i.e. to obtain outputs and to derive models or algorithms, or both, from inputs or data. Instead, traditional software, which executes operations based solely on rules defined by natural persons is not, on its own, considered an AI system.\n\nIn February 2025, the Commission published guidelines for this definition. These guidelines provide further explanations for each aspect of the definition, with a clear emphasis on the 'ability to infer.' In a positive sense, the guidelines outline various machine learning approaches that enable this ability. At the same time, they list systems - particularly those primarily based on mathematical or statistical methods - that do not possess this ability and should therefore not fall within the scope of the AI Act. A noteworthy negative example is 'logistic regression,' which is widely used in the financial sector.\n\nAn AI system can be used on a standalone basis or as a component of a product, irrespective of whether the AI system is physically integrated into the product or serves the product's functionality without being integrated into it.\n\nUnder the AI Act, AI systems fall into the following categories:\n\n- high-risk AI systems;\n- AI systems with transparency risks; and\n- all other AI systems.\n\nAn AI system can also form part of a prohibited AI practice. This can be because of certain features of that AI system or because of the way the AI system would be used.\n\n<!-- image -->\n\n## High-risk AI systems\n\nSection III of the AI Act regulates high-risk AI systems. These are AI systems that pose a significant risk of harm to the health, safety and fundamental rights of persons in the EU. An AI system may be classified as high-risk in two ways:\n\n- Article 6(1): The AI system is used as a safety component in a product that is regulated by certain EU product safety legislation (the Union harmonisation legislation listed in Annex I of the AI Act) and is subject to the conformity assessment procedure with a third-party conformity assessment body under such legislation, or constitutes on its own such a product (e.g. an AI system which is used for medical diagnostic purposes will itself be a regulated medical device); or\n- Article 6(2): The AI system falls within one of the eight categories set out in Annex III of the AI Act - unless the provider can demonstrate and document that such AI system does not pose a significant risk of harm.\n\nMost of the obligations regarding high-risk AI systems fall on providers (which includes product manufacturers as we describe further above), whilst a more limited set of obligations is imposed on deployers, on importers and distributors, and where relevant, authorised representatives.\n\nSee Chapter 4 of this guide for more details.\n\n## AI systems with transparency risks\n\nThe AI Act imposes certain transparency obligations on:\n\n- providers of AI systems intended to interact directly with natural persons (article 50(1));\n- providers of AI systems generating synthetic audio, image, video or text content (article 50(2));\n- deployers of an emotion recognition system or a biometric categorisation system (article 50(3)); and\n- deployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake (article 50(4)).\n\nSee Chapter 6 of this guide for more details.\n\n## All other AI systems\n\nAll other types of AI systems, which do not fall under the above categories and are not used for prohibited AI practices are not subject to direct legal obligations under the AI Act. Voluntary codes of conduct may be drawn up in future covering this broader category of AI systems and those deploying them (article 95). Providers and deployers may choose to adhere to these codes of conduct.\n\nAside from rules relating to specific categories of AI systems, those qualifying as the provider or deployer of any AI system under the AI Act are required to take AI literacy measures to ensure that their staff and other persons dealing with the operation and use of AI systems on their behalf, have a sufficient level of knowledge, skills and understanding regarding the deployment of AI systems, their opportunities and risks (article 4). This obligation aims to foster the development, operation and use of AI in a trustworthy manner in the EU - however, it is worth noting that this provision refers to voluntary codes of conduct and that administrative fines are not foreseen for failure to comply with the AI literacy obligation.\n\n## Regulated subject matter: Prohibited AI practices\n\nThe AI Act prohibits the placing on the market, putting into service and use of AI systems that have certain prohibited features and/or are intended to be used for certain prohibited purposes, e.g. AI systems that create or expand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage. These practices are deemed to be particularly harmful and abusive and contradict EU values and fundamental rights. The prohibited AI practices are listed in article 5 of the AI Act. This list does not affect the prohibitions of AI practices that infringe other EU law (such as data protection, non-discrimination, consumer protection and competition law).\n\nSee Chapter 3 of this guide for more detail.\n\n<!-- image -->\n\n## Regulated subject matter: general-purpose AI models\n\nA general-purpose AI model is defined in article 3(63) as: ' an AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market '\n\nThe AI Act does not provide a definition of an ' AI model '; recital 97 notes that although AI models are essential components of AI systems, they do not constitute AI systems on their own and require further components, such as a user interface, to become AI systems. The characteristics of general-purpose AI models are discussed further in recitals 98 and 99.\n\nThe AI Act regulates general-purpose AI models and imposes additional obligations for generalpurpose AI models with systemic risks. The rules apply to providers of general-purpose AI models, once these models are placed on the market: this can be done in various ways, such as through libraries, APIs, as a direct download or as a physical copy.\n\nRecital 97 suggests that the rules on generalpurpose AI models can also apply when these models are integrated into or form part of an AI system. When the provider of a generalpurpose AI model integrates its own model into its own AI system that is made available in the market or put into service, then recital 97 suggests that model will be viewed as being placed on the market and the general-purpose AI model provisions will apply, in addition to those regarding AI systems.\n\n<!-- image -->\n\nMaterial Scope article 1 recitals 1-3, 6-8\n\nThose who integrate third party generalpurpose AI models into their own AI systems are considered 'downstream providers' and are granted certain rights under the AI Act. However, the AI Act appears to envisage that a provider who fine-tunes a third party general-purpose AI model and integrates that fine-tuned model into their own AI system (or otherwise places a finetuned general-purpose AI model on the market or puts it into service) will be considered the provider of this with respect to that fine-tuning only (see recital109).\n\nSee Chapter 5 of this guide for more detail.\n\n## Territorial scope\n\n## AI System provisions\n\nThe AI Act is intended to have a broad jurisdictional scope for its AI system provisions: these are engaged when an AI system, either on its own or as part of a product covered by the EU product safety legislation in Annex I, is:\n\n- placed on the EU market, put into service in the EU, imported into or distributed in the EU; or\n- used by a deployer who has their place of establishment or is located in the EU.\n\nThe first point applies applies irrespective of where the provider of the AI system is established. The concept of ' establishment ' is not defined in the AI Act. It is expected that this would be interpretated broadly, similar to the use of this term under other EU legislation, such as the GDPR.\n\nIn addition to those cases, the AI system\n\n<!-- image -->\n\nprovisions also apply if the output produced by an AI system outside the EU is used in the EU. In that case, the non-EU established/ located providers and deployers will also be caught by the scope of the AI Act. Recital 22 clarifies that in those instances the AI Act will apply even though the relevant AI systems are not placed on the market, put into service or used in the EU.\n\n## Prohibited AI Practices\n\nThe AI Act's provisions relating to prohibited AI practices apply to the placing on the EU market, putting into service in the EU and use of the relevant AI practices set out in Article 5. As we saw above, the definitions of 'placing on the market' and 'putting into service' refer to the EU market. The AI Act itself does not specify what a prohibited 'use' would entail. The Commission's Guidelines on prohibited AI practices suggest that use 'should be understood in a broad manner to cover the use or deployment of the system at any moment of its lifecycle after having been placed on the market or put into service' and further that use 'may also cover the integration of the AI system in the services and processes of the person(s) making use of the AI system, including as part of more complex systems, processes or infrastructure.'\n\n## General-purpose AI Models\n\nThe AI Act's general-purpose AI model provisions will be engaged where a provider of a generalpurpose AI model places it on the market in the EU or puts it into service in the EU - irrespective of where the provider is located or established.\n\n<!-- image -->\n\narticle 2\n\n## Exclusions\n\nCertain activities are entirely outside the AI Act's scope. The AI Act does not apply to:\n\n- areas outside the scope of EU law (e.g. activities concerning national security). This is the case irrespective of the type of entity entrusted under national legislation with carrying out the exempted activities. Given the very broad competences of the EU, as set out in the TFEU, this provision will have very limited scope of application in practice;\n- AI systems placed on the market, put into service, or used with or without modification - or where their output is used in the EU, exclusively for military, defence or national security purposes, regardless of the type of entity carrying out those activities. An AI system placed on the market or put into service for an excluded purpose (military, defence or national security) and one or more non-excluded purposes (e.g. civilian purposes or law enforcement) is subject to the AI Act and providers of those systems should ensure compliance with the AI Act;\n- public authorities in a third country or international organisations that use AI systems in the framework of international cooperation or agreements for law enforcement and judicial cooperation with the EU or EU member states, provided that such a third country or international organisation provides adequate safeguards for the protection of fundamental rights and freedoms of individuals. The national authorities and EU institutions, bodies, offices and agencies making use of those outputs remain subject to EU law;\n- AI systems and models, including their output, specifically developed and put into service for the sole purpose of scientific research and development;\n- research, testing or development of AI systems or models prior to their being placed on the market or put into service, excluding though testing in real world conditions;\n- deployers who are individuals and use the AI system in the course of a purely personal, non-professional activity. This is similar to the GDPR's 'household exemption' - whilst providers of those AI systems continue to be subject to the AI Act; and\n- AI systems released under free and opensource licences, unless they are placed on the market or put into service as high-risk AI systems, as a prohibited AI system or as a system that is covered by the Act's transparency obligations.\n\n<!-- image -->\n\n## Relationship with other regulatory frameworks\n\n- As a Regulation, the AI Act is directly applicable in EU Member States without the need for implementing legislation. EU Member States are prevented from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by the AI Act. This is only provided for in limited circumstances: for example, EU member states may introduce more restrictive laws on the use of remote biometric identification systems - some of which constitute prohibited AI practices (article 5(5)) and the use of post-remote biometric identification systems, which constitute high-risk AI systems (article 26(10)).\n- The AI Act's provisions on high-risk AI systems are built around the New Legislative Framework for EU products. This is a legislative package that sets out rules for the placing of products on the EU market, enhances market surveillance rules and rules for conformity assessments and CE marking. It also establishes a common legal framework for industrial products in the form of a toolbox of measures for use in future legislation. The AI Act specifies how these tools set out in the New Legislative Framework should apply in the context of AI systems.\n- In parallel, the AI Act complements Union harmonisation legislation - this is the set of EU product safety legislation on the basis of which certain AI systems are to be classified as high-risk.\n- The obligations of the AI Act apply in addition to and without prejudice to the obligations under GDPR, the e-Privacy Directive and the Law Enforcement Directive.\n\n## Prohibited AI Practices\n\n<!-- image -->\n\n- Article 5 lists eight prohibited practices which are deemed to pose an unacceptable level of risk.\n- Prohibitions come into effect on 2 February 2025.\n- The prohibited practices are:\n- -Subliminal, manipulative, or deceptive techniques\n- -Techniques exploiting vulnerable groups in each case which materially distorts behaviour and risks significant harm\n- -Social scoring in certain use cases\n- -Predicting criminality based on profiling\n- -Scraping the web or CCTV for facial recognition databases\n- -Inferences of emotions at workplaces or schools\n- -Biometric categorisation to infer race, political opinion, trade union membership, religious or political beliefs, sex life or sexual orientation\n- -Real-time remote biometric identification in public spaces for law enforcement purposes.\n- Many of the prohibitions have exceptions case by case analysis is needed.\n- The list is not final: it will be re-assessed annually.\n- Non-compliance sanctioned by fines up to €35 million or 7% of total worldwide annual turnover for the proceeding financial year (whichever is higher).\n- The prohibitions are operator-agnostic and apply irrespective of the role of the actor (i.e. whether provider, deployer, distributor or importer).\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n4\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nCheck the AI systems you use to see if they fall under the prohibited category.\n\n<!-- image -->\n\n<!-- image -->\n\n- Check for updates to this list annually as the list of prohibited practices may change over time.\n\nConsider whether any exceptions apply. The prohibited practices are not absolute; many have exceptions.\n\n## Prohibited AI practices\n\nThe AI Act relies on a risk-based approach, so different requirements apply in accordance with the level of risk. This chapter concentrates on prohibited practices i.e. those which conflict with the values of the European Union and are a clear threat to fundamental rights such as freedom, equality and privacy. The prohibitions are an attempt by law makers to respond to transparency and ethics concerns and to guarantee the protection of human rights.\n\nThe prohibited practices are listed exhaustively in article 5 (and are further explained in recitals 28 - 45 of the Act and by guidelines issued by the Commission on 4 February 2025) and provide a clear framework for what AI can and cannot do within the EU. The prohibitions in Article 5 apply from 2 February 2025 and are therefore the first provisions to come into force, highlighting their importance.The list of prohibited practices in article 5 is exhaustive, but not final. The Commission will assess the need for amendment of the list of prohibited practices annually (article 112) and can submit findings to the European Parliament and Council. So, there may be variations to the list of prohibited practices in due course.\n\nThere are currently eight prohibited practices, which focus on practices that materially distort peoples' behaviour, or raise concerns in democratic societies. Special attention has been given to biometric identification systems. However, there are detailed exceptions to many of the prohibitions and each practice should be considered on a case-by-case basis.\n\n## Article 5(1)(a) Subliminal, manipulative or deceptive techniques\n\nThe first prohibition concerns AI systems deploying subliminal, manipulative or deceptive techniques in cases where:\n\n- the techniques either aim to, or actually have, the effect of materially distorting the behaviour of an individual or a group;\n- by appreciably impairing the ability of individuals to make informed decisions; and\n- causing them to take decisions they would not otherwise have taken, and that either cause or are reasonably likely to cause them significant harm.\n\n<!-- image -->\n\nThe techniques expressly mentioned in recital 29 involve: deployment of subliminal components such as audio, image, video stimuli that persons cannot perceive, or other manipulative or deceptive techniques that subvert or impair a person's autonomy, decision-making, or free choice, in ways so that people are not consciously aware of those techniques or, where they are aware of them, can still be deceived or are not able to control or resist them. The reference in recital 29 to machine-brain interfaces having the capability to materially distort human behaviour in a significantly harmful manner may also be the Act's attempt to regulate tools that employ neural data which is currently under discussion in other jurisdictions such as Colorado, California, and Chile.\n\nFor an AI system to be prohibited, there needs to be a causal link between the deceptive techniques and the significant harm caused. The threshold of 'significant' harm was added in the legislative process and makes clear that not all dark patterns would fall under this provision.\n\nThe provision is open for interpretation and, in particular, the word 'deceptive' will lead to further discussions. According to the Commission's guidelines, deceptive techniques could cover presenting false or misleading information with the objective or effect of misleading individuals, if the other requirements of the first prohibition are met.\n\n## Article 5(1)(b) Exploitation of vulnerabilities\n\nThe second category of prohibited AI practices aims to protect vulnerable people. There are three groups: vulnerability due to age, disability, or due to specific social or economic situations.\n\nAn AI system is only prohibited if it has the objective or the effect of materially distorting the behaviour of an individual and does so in a manner that causes or is likely to cause someone significant harm.\n\nAn exploitation from a socio-economic perspective does not exist, according to the Commission guidelines on prohibited practices, if the situation may be experienced by any person irrespective of their socio-economic situation (e.g. grievances or loneliness). In such case, however, an exploitation may be covered under Article 5(1)(a) AI Act.\n\nAI systems that inadvertently impact sociodisadvantaged groups due to biased training data do not automatically exploit vulnerabilities, as there is no intentional targeting. However, under the Commission guidelines on prohibited practices, if AI providers or deployers are aware that their systems unlawfully discriminate against socio-economically disadvantaged persons and foresee significant harm without taking corrective action, they may still be considered to exploit these vulnerabilities.\n\nAn exploitation of a person's economic situation could exist in cases where an AI system is used to find persons in poverty to exploit their weaknesses economically. Organisations using AI systems for marketing and sales should make sure they test their systems against this requirement.\n\nThe concept of significant harm is common to both subliminal techniques and exploitation of vulnerable groups. In the legislative process, requirements that the harm needed to be physical or psychological were dropped. It seems that a broad approach is intended to be taken to the concept of harm, although recital 29 still gives the examples of important adverse impacts on physical and psychological health, alongside financial interests. The recital also notes that harms can be accumulated over time.\n\nThis prohibition is not intended to affect lawful medical treatment (e.g. psychological treatment of a mental disease carried out with consent). Recital 29 also implicitly recognises that advertising and some other commercial practices inherently depend on nudging - and states that the intent is not to prohibit common, legitimate and lawful commercial practices, particularly in the field of advertising. Consent can play a crucial role in these scenarios. In persuasive interactions, individuals are aware of the influence attempt and can make choices freely and autonomously.\n\n## Article 5(1)(c) Social scoring\n\nThe third prohibition concerns so-called social scoring, i.e. classifying individuals or groups over a period based on their social behaviour, or known, inferred, or predicted personal characteristics. Social scoring is prohibited in two cases:\n\n- if it leads to unfavourable treatment in social contexts that are unrelated to the context in which the data was originally generated; and\n- if this leads to unfavourable treatment of individuals or groups that is unjustified or disproportionate to their social behaviour or its gravity.\n\n<!-- image -->\n\nSocial scoring is used by several governments around the world. The government in the Netherlands stepped down in 2021 due to a flawed risk-scoring algorithm, which lead to unjustified accusation of fraud for welfare benefits based on personal characteristics and behaviour. The algorithm in that case targeted minorities and people based on their economic situation. Whilst governments might be the first example that comes to mind when thinking about social scoring, the provision is wider and encompasses all social scoring systems in public or private contexts. Many algorithms inherently depend on behavioural scores. However, the AI Act only prohibits those scoring systems resulting in unfavourable treatment in unrelated social contexts. This key restriction targets the consequences of social scoring, preventing unjust outcomes, or discrimination of individuals or groups.\n\nThe social scoring prohibition under the AI Act therefore depends on the context the data has been obtained from and the context the data is being used. As the Commission guidelines on prohibited practices illustrate, lawful activities, like credit and risk scoring in financial services, are permitted if they improve service quality or prevent fraud. Conversely, an insurance company using spending and other financial data from a bank to set life insurance premiums is provided as an example of unlawful social scoring.\n\n## Article 5(1)(d) Profiling for criminal risk assessment\n\nThe fourth prohibition is placing on the market, putting into service, or using AI systems that assess or predict the likelihood of a person committing criminal offences based solely on profiling or on assessing the personality traits and characteristics of a person. There is an exception for AI systems used to support human assessment of involvement of a person in a criminal activity, which is based on objective and verifiable facts directly linked to a criminal activity - i.e. detection tools which are factual and supplement, but do not supplant, human decision making. This prohibition aims to avoid the scenario whereby people are treated as guilty for crimes they have not (yet) committed - as illustrated in the film Minority Report . It is tied to human dignity as laid down in article 1 of the Charter of Fundamental Rights.\n\nThe Commission guidelines on prohibited practices emphasise that the prohibition can extend to private entities if they act with public authority or assist law enforcement. For instance, a private company analysing data for law enforcement might face prohibition if specific criteria are met.\n\nThe Commission guidelines also suggest that retrospective human assessments of AI system evaluations can fall outside the scope under certain conditions. This is informed by CJEU case law, which underscores the importance of human review to ensure that AI-driven decisions are based on objective criteria and are nondiscriminatory, thus extending beyond the initial exemption in the AI Act.\n\n## Article 5(1)(e) Facial recognition databases\n\nThe fifth prohibited practice is the placing on the market, putting into service for the specific purpose, or use of AI systems to create or expand facial recognition databases through untargeted scraping of facial images from the internet or CCTV footage. Recital 43 considers this practice to add to the feeling of mass surveillance and that it can lead to gross violations of fundamental rights, including the right to privacy. This may be a response to the investigations by supervisory authorities into Clearview AI.\n\nThe Commission guidelines on prohibited practices regarding facial recognition databases clarify several key points. Such databases can be temporary, centralised, or decentralised, and they fall under Article 5(1)(e), if they can be used for facial recognition, regardless of their primary purpose. Targeted scraping, such as collecting images of specific individuals or using reverse image searches, is allowed, but combining it with untargeted scraping is prohibited. The prohibition does not cover untargeted scraping of other biometric data, like voice samples, or databases not used for recognition, such as those for AI model training without identifying individuals.\n\n## Article 5(1)(f) Inference of emotions in working life and education\n\nThe sixth prohibited practice is the placing on the market, putting into service for this specific purpose, or use of AI systems to infer emotions in workplace or schools, except for safety or medical reasons such as systems intended for therapeutical use. The guidelines clarify that the definition of both the school and workplace\n\n<!-- image -->\n\nshould be interpreted widely and in the case of workplace use they should also cover the selection and hiring phases of recruitment. The exception for the safety or medical reasons on the other hand is to be interpreted narrowly. For example, systems measuring burnout or depression in the workplace would not be exempted.\n\nRecital 18 distinguishes between emotions or intentions such as happiness, sadness, anger etc. It explains that the notion does not include physical states, such as pain or fatigue (so, systems used in detecting the state of fatigue of professional pilots or drivers for the purpose of preventing accidents would not be affected). It also does not include detection of readily apparent expressions such as a frown or a smile, or gestures such as the movement of hands, arms or head, or characteristics of a person's voice, such as a raised voice or whispering. However, the guidelines still do not clarify the meaning of 'intention' which are also covered by the definition of emotion recognition systems.\n\nThe AI Act has a defined term of 'emotion recognition system' , which means an ' AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of biometric data '.\n\nCuriously, article 5(1)(f) does not use this term, and refers to any use of AI systems to infer emotions (i.e. without the requirement that this should be derived from biometric data). However, the Commission's guidelines clarified that Article 5(1)(f) should be read as referring to the emotion recognition systems as the defined term under the Act. They further clarified that nonbiometric emotion recognition systems (e.g. text-based) are not prohibited provided they are not used in conjunction with biometric data such as keystroke analysis. The Act references the inaccuracy of biometric emotion recognition systems and their intrusive nature in settings where there is an imbalance of power (such as workplace and schools) as the reason for the prohibition in such settings. However, the AI Act does not explain why it considers non-biometric emotion recognition systems as less intrusive or more accurate than biometric systems.\n\n## Article 5(1)(g) Biometric categorisation\n\nThe seventh prohibition is on the use of biometric categorisation systems that categorise individuals based on their biometric data to deduce or infer certain (not all) special category data under the GDPR, namely: race, political opinions, trade union membership, religious or political beliefs, sex life or sexual orientation.\n\nSpecial category data under the GDPR that are not covered in the prohibition are inferences of ethnic origin, health, and genetic data. However, inferring such types of data would likely fall under the high-risk category under Annex III. Additionally, the prohibition does not cover labelling or filtering of lawfully acquired biometric datasets or categorising of biometric data by law enforcement (e.g. sorting of images according to hair colour or eye colour by law enforcement to search for suspects). However as recital 54 suggests that AI systems intended to be used for biometric categorisation according to sensitive attributes or special category data under the GDPR, in so far as they are not prohibited the AI Act, should be classified as high-risk and the guidelines also state that most AI systems that fall under an exception from a prohibition listed in Article 5 AI Act will qualify as high-risk this would suggest that the exempted labelling and filtering systems would fall under the high-risk category.\n\nRecital 16 clarifies that biometric categorisation systems do not include purely ancillary features which are linked to another commercial service, where the feature cannot, for objective technical reasons, be used without the main service, and where this is not a circumvention mechanism to evade AI Act rules (e.g. retail try before you buy filters, or social media filters).\n\nThe guidelines also clarify that the scope of biometric categorisation excludes categorisation according to clothes or accessories, such as scarfs or crosses, or social media activity.\n\n## Article 5(1)(h) Real-time remote biometric identification in public spaces\n\nThe eighth and last prohibition is the use of real-time remote biometric identification systems ('RBI') in publicly accessible spaces for law enforcement purposes. RBI systems are AI systems for the purpose of identifying natural persons, without their involvement, typically at a\n\n<!-- image -->\n\ndistance, by comparing biometric data with that contained in a reference database. Real-time systems include those where there is a short delay in the comparison. The AI Act does not define how much time amounts to 'significant delay' . However, the guidelines suggest that this would likely be the case for when the person is likely to have left the place where the biometric data was taken and not allow for a quick reaction from the law enforcement.\n\nBiometric systems used for verification (i.e. confirming that someone is who they claim to be, to access a service, a device, or to have security access to premises) are distinguished from RBI and so not covered by this prohibition (recital 15). The guidelines clarify that the distinction between the identification and verification comes from the active involvement of the individual in the process which may have minor impact on fundamental rights of natural persons. For active involvement, however, it is not sufficient that persons are informed about the presence of cameras, but they need to step actively and consciously in front of a camera that is installed in a way fostering active participation.\n\nThe AI Act allows (but does not require) member states to permit use of RBI for law enforcement purposes in limited situations where the use of RBI is strictly necessary for:\n\n- targeted searches for specific victims of abduction, human trafficking, or sexual exploitation as well as searching for missing persons;\n- the prevention of a specific, substantial, and imminent threat to the life or physical safety, or a genuine and present or foreseeable threat of terrorist attack; or\n- the localisation or identification of a person suspected of having committed a criminal offence, conducting a criminal investigation, prosecution or executing a criminal penalty for serious offences - being those referred to in Annex II and punishable in the Member State concerned by a prison sentence for a maximum period of at least four years.\n\nThe exemptions only permit RBI used to confirm the identity of the specifically targeted individual. In addition, use of RBI should consider the nature of the situation, in particular the seriousness, probability, and scale of the harm that would be caused if the system were not used, against the consequences of use on the rights and freedoms of the persons concerned.\n\nFurther, protections include the need to complete a fundamental rights assessment, registration of the system in an EU database in line with article 49, and prior authorisation of each use case by judicial or administrative authority (subject to urgency measures). In addition, each use of RBI in publicly accessible spaces must be notified to the relevant market surveillance authority and the national data protection authority. The national authorities must then report to the European Commission which, in turn, prepares an annual state of the nation report on usage of RBI in accordance with these provisions.\n\n## To whom do the prohibitions apply?\n\nAs set out in Chapter 2, the AI Act distinguishes between different actors involved in AI systems, attributing specific responsibilities based on their role in relation to the AI model or system. This method ensures that those who have the most influence over the development and implementation of AI technologies adhere to the highest standards.\n\nHowever, the rules on prohibited practices are operator-agnostic. In other words, they apply universally, independent of the specific role of the actor (i.e. whether they are involved in the provision, development, deployment, distribution, or use of AI systems engaging in prohibited practices).\n\nThis wide-ranging application highlights the Act's dedication to stopping practices that could infringe on fundamental rights or present intolerable risks, emphasising a comprehensive approach to regulation that covers all types of interaction with harmful AI technologies.\n\n<!-- image -->\n\n## Enforcement and fines\n\nWhen a practice is prohibited, the AI system in question may not be used in the EU. In the case of an infringement, competent authorities may issue a fine of up to 7% of the total worldwide annual turnover of the offender for the preceding financial year or 35 million EUR, whichever is higher.\n\nNational market surveillance authorities will be responsible for ensuring compliance with the AI Act's provisions regarding prohibited AI systems. They will report to the European Commission annually about use of prohibited practices that occurred during the year and about the measures they have taken.\n\n<!-- image -->\n\n| Subliminal, manipulative or deceptive techniques           | article 5(1)(a)   | recitals 28 & 29   |\n|------------------------------------------------------------|-------------------|--------------------|\n| Exploitation of vulnerabilities                            | article 5(1)(b)   | recitals 28 & 29   |\n| Social scoring                                             | article 5(1)(c)   | recital 31         |\n| Profiling for criminal risk assessment                     | article 5(1)(d)   | recital 42         |\n| Facial recognition database                                | article 5(1)(e)   | recital 43         |\n| Inference of emotions in working life and education        | article 5(1)(f)   | recitals 44 - 45   |\n| Biometric categorisation                                   | article 5(1)(g)   | recital 30         |\n| Real-time remote biometric identification in public spaces | article 5(1)(h)   | recitals 32 - 41   |\n\n## Other useful resources\n\n- Commission guidelines on prohibited artificial intelligence practices established by Regulation (EU 2024/1689 (AI Act)\n- ETHICS GUIDELINES FOR TRUSTWORTHY AI: High-Level Expert Group on Artificial Intelligence (2019)\n- EDPB Guidelines on Processing Personal Data Through Video Devices\n- EDPB Guidelines on Use of Facial Recognition Technology In The Area of Law Enforcement\n- EDPB Guidelines on Automated Decision Making and Profiling\n- EDPB-EDPS Joint Opinion On The Proposal For The Artificial Intelligence Act\n- EDPB guidelines on Deceptive Design Patterns in Social Media\n- Guidelines on dark patterns from the Finnish Market Authority\n\n<!-- image -->\n\n## High-risk AI systems\n\n<!-- image -->\n\n- AI systems fall within the scope of 'high-risk' if they are intended to be used as:\n- -products, or safety components of products, which must undergo third-party conformity assessment pursuant to the legislation covered by Annex I; or\n- -for one of the purposes described in Annex III.\n- Providers, deployers, importers, distributors and suppliers to providers of high-risk AI systems have obligations under the AI Act. Market parties can have multiple roles in parallel and need to comply with multiple sets of obligations simultaneously.\n- Providers of high-risk AI systems have the heaviest compliance burden and need to carry out a conformity assessment before the system can be placed on the market or put into service.\n- It's possible to become the provider of a high-risk AI system (e.g. by placing your own name/trademark on the system, making a substantial modification, or using the system for different purposes than intended by the original provider).\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nDetermine whether the AI system falls within the scope of high-risk as meant in article 6, in conjunction with Annexes I and III.\n\n<!-- image -->\n\nDetermine your role in the value chain (provider, deployer, importer, distributor, or third-party supplier) and review the corresponding obligations.\n\n## Classification of an AI system as a high-risk AI system\n\nThe main part of the AI Act regulates high-risk AI systems. These are AI systems that can have a significant harmful impact on the health, safety and fundamental rights of persons in the EU. There are two main categories of highrisk AI systems:\n\n- a. systems which are intended to be used as safety components of products or systems, or which are themselves products or systems, falling within the scope of Union harmonisation legislation listed in Annex I, if required to undergo a third-party conformity assessment pursuant to this legislation; and\n- b. systems whose intended purpose falls within the scope of the use cases set out in Annex III of the AI Act.\n\n## Category A: Annex I systems\n\nRegarding the first category (a), the product safety legislation listed in Annex I covers the following categories:\n\n- machinery\n- toys\n- recreational craft and personal watercraft\n- lifts/elevators\n- equipment and protective systems for potentially explosive atmospheres\n- radio equipment\n- pressure equipment\n- cableway installations\n- personal protective equipment\n- appliances burning gaseous fuels, medical devices\n- in vitro diagnostic medical devices\n- civil aviation\n- 2/3-wheel vehicles\n- agricultural and forestry vehicles\n- marine equipment\n- rail systems\n- motor vehicles and their trailers\n- unmanned aircraft\n\n<!-- image -->\n\nNote that the legislation in Annex I covers the categories above, but can also cover related products. For example, the Machinery Regulation covers lifting accessories and removable mechanical transmission devices as well as machinery. It's also the core regulation for robotics, another steadily growing area of AI adoption for which the AI Act and its high-risk requirements will become highly relevant.\n\nSafety components fulfil a safety function for a product, where their failure or malfunction would endanger the health and safety of persons or property. You should make an assessment pursuant to the applicable product safety regulation in Annex I to see whether the AI system would have to undergo thirdparty conformity assessment pursuant to that legislation. For example, in the Medical Device Regulation, medical devices in class IIa and higher are subject to the third-party conformity procedure. If an AI-system qualifies as a safety component of such a medical device, or if it constitutes such a medical device itself, it is a high-risk AI system pursuant to the AI Act.\n\nSome of the legislation covered in Annex I also uses terms such as 'high-risk' and 'medium-risk' . However, these categories are independent from the classification as high risk under the AI Act. For example, under applicable product safety legislation a product can be classed as 'mediumrisk' , but if the product has to  to undergo thirdparty conformity assessment, then an AI system that is a safety component of that product, or that itself constitutes such a product, will be high-risk under the AI Act.\n\n## Category B: Annex III systems\n\nThe stand-alone list of high-risk systems currently contains:\n\n- Biometrics: remote biometric identification of individuals, biometric categorisation of individuals and/or emotion recognition of individuals.\n\n- Management and operation of critical infrastructure: to directly protect physical integrity or health and safety of individuals and property in relation to the management and operation of critical digital infrastructure (e.g., internet exchange points, DNS services, TLD registries, cloud computing services, data centres, content delivery networks, trust service providers, electronic communication networks or services), or in the supply of water, gas, heating, or electricity.\n- Education and vocational training : decisionmaking in education and vocational training (e.g. selection, evaluation, assessment and monitoring of students or individuals applying to be students).\n- Recruitment and HR : decision-making in recruitment and HR (e.g. selection, evaluation, assessment, promotion, termination, task allocation and monitoring of employees and/ or other workers and/or applicants).\n- Essential services: evaluating the (continued) eligibility of individuals for public assistance benefits (e.g. healthcare services, social security allowances, disability benefits); evaluating creditworthiness of individuals or establishing their credit score (with the exception of the detection of financial fraud); risk assessment and pricing in relation to individuals in the case of life and health insurance; and evaluating and classifying emergency calls or making decisions in relation to dispatching or prioritisation of the dispatching of emergency first response services (e.g. police, firefighters, medical aid); and emergency healthcare patient triage.\n- Crime analytics: assessment by/on behalf of/ in support of law enforcement authorities: (i) of the risk of individuals of becoming a victim or (re-)offender; (ii) of personality traits and characteristics; (iii) of past criminal behaviour of individuals or groups; or (iv) consisting of profiling of persons, in the course of the detection, investigation or prosecution of criminal offences.\n- Evidence gathering and evaluation: evaluation of reliability of evidence during the investigation or prosecution of criminal offences, or in the course of applications for asylum, visa or residence permits, or with regard to associated complaints; use of polygraphs or similar tools by/on behalf of/ in support of law enforcement authorities or authorities conducting migration, asylum and/ or border control.\n- Immigrant identification, migration risk and migration application assessment: detecting, recognising or identifying individuals (with the exception of verification of travel documents) in the context of migration, asylum or border control management; assessment of risk (e.g. security risk, risk of irregular migration or health risk) posed by individuals who intend to enter or have entered the territory of an EU country and examination of applications for asylum, visa or residence permits and for associated complaints.\n- Administration of justice: assisting judicial authorities or alternative dispute resolution institutions in researching and interpreting facts and the law and in applying the law to facts.\n- Democratic processes: influencing the outcome of an election or referendum or voting behaviour of individuals.\n\n<!-- image -->\n\nNote that Annex III may be amended by the Commission (article 7).\n\nThe intended purpose is defined in article 3(12) as: ' the use for which an AI system is intended by the provider, including the specific context and conditions of use, as specified in the information supplied by the provider in the instructions for use, promotional or sales materials and statements, as well as in the technical documentation.'\n\n## Exceptions: not sufficiently high-risk\n\nArticle 6(3) provides that AI systems whose intended purpose falls within the scope of Annex III, so that they would (absent this provision be high-risk) shall nonetheless not be considered as high-risk if they do not pose a significant risk of harm to the health, safety or fundamental rights of natural persons. The article mentions four criteria. The exemption can be relied upon if one or more of these criteria are fulfilled (article 6(3) and recital 53):\n\n- the AI system is intended to perform a narrow procedural task;\n- -Example: a system which transforms unstructured data into structured data or a system which detects duplicates of documents\n\n- the AI system is intended to improve the result of a previously completed human activity;\n- -Example: a system which improves the professional tone or academic style of language used in already drafted documents\n- the AI system is intended to detect decisionmaking patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment, without proper human review; or\n- -Example: a system which  checks  flags inconsistencies or anomalies in the grades applied by a teacher, when compared with an existing grading pattern for that teacher\n- the AI system is intended to perform a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III\n- -Example: a system for translating documents.\n\nThe exception does not apply if the AI system involves profiling of natural persons within the meaning of article 4(4) of Regulation (EU) 2016/679 (GDPR) or article 3 (4) of Directive (EU) 2016/680 (Data Protection Enforcement Directive) or article 3, (5) of Regulation (EU) 2018/1725 (Data Protection for EU institutions) (recital 53).\n\nCompanies deciding to make use of this exemption should note that they carry the burden of proof as to whether the system is high-risk. The assessment under  article 6(3) must be documented before the system is placed on the market or put into service and the system must be registered (articles 49(2) and 6(4)). Providers of such systems must provide this documentation to national competent authorities on request.\n\nThe Commission will provide guidelines specifying the practical implementation of article 6, including a comprehensive list of practical examples of high-risk and non-high-risk use cases of AI systems (article 6(5)). It may also adopt delegated acts adding to or modifying the criteria for article 6(3). The guidelines are expected to be published within six months after entry into force of the AI Act.\n\n## Obligations for providers of high-risk AI systems\n\nThe AI Act provides a detailed list of obligations for providers and deployers of high-risk AI systems as follows in Chapter III, Sections 2, 3 and 4:\n\n## Obligations for providers on high-risk AI systems\n\n| Requirements of Section 2                | Ensure compliance with requirements of Section 2 (see below).                                                                                                                              |\n|------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Name of provider and contact information | Indicate on the system (or, if not possible, on its packaging or accompanying documentation) the name of the provider or its brand and its contact information.                            |\n| Quality management system                | Have a quality management system complying with article 17. (Article 17 provides a detailed list of aspects of the system to be documented through policies, procedures and instructions). |\n\nKeep the documentation referred to in article 18. The documentation will include:\n\n- technical documentation (article 11)\n- documentation concerning the quality management system (article 17)\n- documentation concerning changes approved by notified bodies, where applicable\n- decisions and other documents issued by notified bodies, where applicable\n- the EU declaration of conformity (article 47).\n\n<!-- image -->\n\n## Documentation\n\n## Logs\n\n## Conformity Assessment\n\n## Declaration of conformity\n\n## CE marking\n\n## Registration obligation\n\n## Corrective actions / provision of information\n\n## Demonstration of conformity\n\n## Accessibility requirements\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nIf the system is under their control, keep logs automatically generated by the system (article 19).\n\nSuch logs must be kept for a period appropriate to the intended purpose of the high-risk AI system. The period should be at least six months (unless any personal data protection provisions state otherwise).\n\nEnsure that the system undergoes the relevant conformity assessment procedure in article 43, prior to being placed on the market or put into service (see below).\n\nDraw up an EU declaration of conformity (article 47). See below.\n\nAffix the CE marking to the high-risk AI system (or, if not possible, on its packaging or accompanying documentation).\n\nThe CE marking will confirm the conformity of the high-risk AI system with the AI Act as per article 48.\n\nSee below.\n\nComply with EU Database registration obligations (article 49(I)). See below.\n\nIn cases where the system is not in conformity with the AI Act, take the necessary corrective actions, or withdraw, disable, or recall it.\n\nWhere the system presents a risk to safety, or the fundamental rights of persons, inform the competent market surveillance authorities and, where applicable, the notified body that issued a certificate for that system (article 79).\n\nUpon a reasoned request of a national competent authority, demonstrate the conformity of the system with the requirements set out in Section 2 (see above), providing all necessary information and documentation.\n\nThe duties relating to cooperation with competent authorities are set out in more detail in article 21.\n\nAny information shared with a national competent authority shall be treated as confidential.\n\nEnsure the high-risk AI system complies with accessibility requirements in accordance with:\n\n- Directive (EU) 2016/2102 (on the accessibility of the websites and mobile applications of public sector bodies); and\n- Directive (EU) 2019/882 (on the accessibility requirements for products and services).\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Harmonised standards and conformity assessment procedure for providers of high-risk AI systems\n\n## Harmonised standards\n\nHarmonised standards will be published in the Official Journal of the European Union. If the AI system complies with these standards, there will be a presumption of conformity with the requirements for high-risk AI systems in Chapter III, Section 2 (article 40(1).\n\nHarmonised standards are highly relevant in practice. Under traditional product safety laws, 'manufacturers' usually follow them to demonstrate compliance with product safety law requirements. This will be similar under the AI Act.\n\nThe European Commission issued a (draft) standardisation request in accordance with article 40(2) to standardisation bodies CEN/CELENEC, requesting these bodies to draft harmonised standards covering the requirements of Chapter III, Section 2 by 30 April 2025.\n\n## Conformity assessment procedure\n\nThe conformity assessment procedure for high-risk AI systems under article 43 requires providers to demonstrate compliance with the requirements for high-risk AI systems in Section 2 of Chapter III (overview below).\n\n## Annex III high-risk AI systems\n\nHere, the AI Act outlines two primary procedures for conformity assessment. Most providers of high-risk AI systems in Annex III (i.e. those referred to in points 2 to 8 of Annex III),  must follow the internal control procedure specified in Annex VI, without involving a notified body. Providers of high-risk AI systems listed in point 1 of Annex III (biometrics), who have applied harmonised standards or common specifications, as referenced in articles 40 and 41 must also follow the internal control procedure sufficient. However, for providers of high-risk biometric systems who have not done this theinvolvement of a notified body is required.\n\n## Annex I high-risk AI systems\n\nIf a high-risk AI system falls under Union harmonisation legislation listed in Section A of Annex I, the conformity assessment procedures from those legal acts apply. The high-risk AI system requirements of Section 2 in Chapter III are integrated into this assessment, and specific provisions of Annex VII also apply. Notified bodies under these legal acts must comply with certain requirements of the AI Act, to ensure consistent oversight.\n\n## New conformity assessments for substantial modifications\n\nSubstantial modifications to high-risk AI systems necessitate a new conformity assessment. However, changes that form part of the system's predetermined learning process do not count as substantial modifications.\n\n## Requirements for high-risk AI systems\n\n## Focus on Articles 8-15; requirements for high-risk AI systems\n\nCompliance with the requirements (Article 8)\n\nRisk management (Article 9)\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n4\n\nArticle 8 emphasises that high-risk AI systems must meet technical and organisational requirements (articles 9-15) throughout their life cycle, considering the intended use and the status of the technology. It's crucial to prioritise requirements impacting humans and if suitable trade-offs are not found, the AI system should not be deployed.\n\nArticle 9 requires providers to establish a risk management system. This is an ongoing process to identify, analyse, and mitigate foreseeable risks, including designing risk reduction measures, implementing controls, and providing user information and training. The measures taken must be documented and high-risk AI systems tested at appropriate stages to ensure consistent performance.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n| Data governance (Article 10)                                    | Robust data governance is a critical component of the technical and organisational requirements for high-risk AI systems. High-quality, representative, and to the best extent possible error-free and complete training, validation, and testing datasets are required to ensure proper functioning and safety of the system. Providers must also take measures to mitigate biases in datasets that could lead to prohibited discrimination, including by processing special categories of personal data under specific conditions. Certified third-party services can be employed for data integrity verification and to demonstrate compliance with the AI Act's data governance requirements.   |\n|-----------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Technical documentation and record keeping (Articles 11 and 12) | Articles 11 and 12 necessitate detailed technical documentation and record-keeping logs throughout the system's lifecycle. Providers must prepare this before deployment and regularly update it. It should cover all aspects of the system, including its characteristics, algorithms, data, training, testing, validation, and risk management. High-risk AI systems should also automatically record usage logs to provide traceability and identify potential risks or needed modifications.                                                                                                                                                                                                    |\n| Transparency and provision of information (Article 13)          | Article 13 mandates clear, comprehensive instructions for deployers of high-risk AI systems. These instructions should enable deployers to understand and use the system's outputs correctly. The system's decision- making must be understandable, and details on its identity, characteristics, limitations, purpose, accuracy, risks, capabilities, oversight, maintenance, and expected lifespan must be provided. All documentation should be tailored to the needs and knowledge level of the intended deployers.                                                                                                                                                                             |\n| Human oversight (Article 14)                                    | Human oversight measures must prevent or minimise risks to health, safety, and rights. These measures must be proportionate to the system's risks and level of autonomy. Human operators should also be able to override the system if necessary. Oversight can be achieved through: • Built-in system constraints and responsiveness to human operators. • Provider-identified measures for deployers to help them make informed, autonomous decisions. • Oversight approaches can include human-in-the-loop, human-on-the- loop, or human-in-command, depending on the application's risks.                                                                                                       |\n| Accuracy, robustness and cybersecurity (Article 15)             | Article 15 mandates that high-risk AI systems must achieve suitable accuracy, robustness, and cybersecurity levels. Accuracy measures include minimising prediction errors, robustness measures ensure systems can handle errors and inconsistencies. Lastly, cybersecurity measures shall protect against unauthorised system alterations in which case compliance can be demonstrated through the EU Cyber Resilience Act for relevant AI systems subject to the EU Cyber Resilience Act.                                                                                                                                                                                                         |\n\n<!-- image -->\n\n## Obligations for deployers of high-risk AI systems\n\nThe AI Act provides for obligations for deployers of high-risk AI systems (article 26):\n\n## Technical and organisational measures\n\nDeployers must take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems.\n\n## Human oversight\n\nDeployers must assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support.\n\n## Input data\n\nWhere the deployer exercises control over input data, that deployer must ensure that the input data is relevant and sufficiently representative. In other words, this principle states the deployer's responsibility as to the quality of the input data.\n\n## Monitoring high-risk AI system\n\nDeployers must monitor the operation of the highrisk AI system based on the instructions for use.\n\nDeployers must inform providers in accordance with article 72 relating to post-marketing activities. If the deployer identifies a  risk  per article 79(1) it will immediately inform the provider, and then the importer or distributor and the relevant market surveillance authorities and suspend the use of that system. If a serious incident is identified, deployers must also immediately inform the provider, and then the importer or distributor and the relevant market surveillance authorities of that incident.\n\n## Logs\n\nDeployers of high-risk AI systems must keep logs automatically generated by that high-risk AI system where these  logs are under their control, for a period appropriate to the intended purpose of the high-risk AI system. This period is at least six months, unless provided otherwise in applicable Union or national law, in particular on the protection of personal data.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Information to the workers' representatives\n\nDeployers who are employers must inform workers' representatives and the affected workers that they will be subject to the use of the high-risk AI system.\n\n## Public authority deployers\n\nDeployers of high-risk AI systems who are public authorities, or Union institutions, bodies, offices or agencies must comply with the EU Database registration obligations under article 49.\n\n## Data protection impact assessment\n\nIf deployers of high-risk AI systems are required to perform a data protection impact assessment under article 35 of Regulation (EU) 2016/679 (GDPR) or article 27 of Directive (EU) 2016/680 (Data Protection Enforcement Directive), they must make use of the information provided by the provider under article 13 of the AI Act.\n\n## Investigation for criminal offences - highrisk AI system for post-remote biometric identification\n\nWithout prejudice to Directive (EU) 2016/680 (Data Protection Enforcement Directive), in the framework of an investigation for the targeted search of a person suspected or convicted of having committed a criminal offence, someone who wishes to  deploy a high-risk AI system for post-remote biometric identification must request an authorisation for this use, ex-ante, or without undue delay and no later than 48 hours, from a judicial authority or an administrative authority.\n\n## Fundamental rights impact assessment for high-risk AI systems\n\nPrior to deploying a high-risk AI system referred to in article 6(2) (i.e. high-risk AI systems detailed in Annex III of AI Act), deployers that are:\n\n- I. bodies governed by public law, or\n- II. private entities providing public services, and in each case are\n- III. deployers of high-risk AI systems intended to be used\n\n- a. to evaluate the creditworthiness of natural persons or establish their credit score (apart from AI systems used for the purpose of detecting financial fraud), and\n- b. for risk assessment and pricing in relation to natural persons in the case of life and health insurance must perform an assessment of the impact of the use of the system on fundamental rights (FRIA) . There is an exception for high-risk AI systems relating to critical infrastructure.\n\nThe assessment consists of:\n\n- a description of the deployer's processes in which the high-risk AI system will be used in line with its intended purpose;\n- a description of the time period within which, and the frequency with which, each high-risk AI system is intended to be used;\n- the categories of natural persons and groups likely to be affected by its use in the specific context;\n- the specific risks of harm likely to have an impact on the categories of natural persons or groups of persons identified pursuant to point above, considering the information given by the provider pursuant to article 13;\n- a description of the implementation of human oversight measures, according to the instructions for use; and\n- the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal governance and complaint mechanisms.\n\n<!-- image -->\n\n## Obligations for other parties in connection with high-risk AI systems\n\nMost obligations regarding high-risk systems in the AI Act are directed at providers and deployers. However, there are also a limited set of obligations for other parties: namely, importers and distributors of high-risk AI systems, and suppliers of any systems, tools, services, components or processes which are used or integrated in high-risk AI systems. Examples of services by suppliers include model (re)training, testing and evaluation and integration into software (recital 88). The obligations do not apply to suppliers that offer the relevant product or service under a free and open-source licence (article 25(4)). Additionally, it is possible for parties other than the original provider of an AI system to be assigned the role of provider of a high-risk AI system by the AI Act.\n\n## Obligations for importers, distributors and suppliers\n\nArticles 23, 24 and 25 set out the obligations for importers, distributors and suppliers:\n\n## Importers (article 23)\n\nVerification: before placing the system on the market, verifying that the provider has genuinely:\n\n- carried out the conformity assessment procedure;\n- drawn up the technical documentation;\n- affixed the CE marking and has attached the EU declaration of conformity; and\n- appointed an authorised representative.\n\nRisk flagging: inform the provider, the authorised representative and the market surveillance authority when the system presents a risk 1 to health, safety or fundamental rights of persons.\n\nCare: ensure that storage or transport conditions do not jeopardise compliance with the requirements in Section 2.\n\n## Distributors (article 24)\n\nVerification: before making the system available on the market, verifying that:\n\n- it bears the CE marking;\n- it is accompanied by a copy of the EU declaration of conformity and instructions for use; and\n- the provider and the importer, as applicable, have complied with their respective obligations.\n\nRisk flagging : not make the system available when the distributor considers or has reason to consider that the system is not in conformity with the requirements set out in Section 2, until the system has been brought into conformity, and where the system presents a risk to health, safety or fundamental rights of persons, immediately inform the provider or the importer of the system and the competent authorities, giving details, in particular, of the non-compliance and of any corrective actions taken.\n\nCare: ensure that storage or transport conditions do not jeopardise compliance with the requirements in Section 2.\n\n1.  Risk here means: 'having the potential to affect adversely health and safety of persons in general, health and safety (...) to a degree which goes beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the product concerned, including the duration of use and, where applicable, its putting into service, installation and maintenance requirements' (article 79(1) AI Act in conjunction with Article 3(19) of Regulation (EU) 2019/1020 (Market surveillance regulation).\n\n<!-- image -->\n\n## Suppliers (article 25)\n\nProvide assistance: by written agreement, specifying the necessary information, capabilities, technical access and other assistance based on the generally acknowledged state of the art, in order to enable the provider of the high-risk AI system to fully comply with their obligation.\n\nThe AI Office/Commission may also develop and recommend voluntary model contractual terms between providers of high-risk AI systems and their third-party suppliers (article 25(4)) and recital 90).\n\n## Importers (article 23)\n\nCooperation with authorities: upon a reasoned request, provide competent authorities with all necessary information/ documentation, including technical documentation, to demonstrate conformity of the system and cooperate with these authorities in any action they take in relation to the system.\n\nRecord keeping: keep, for a period of ten years after the system has been placed on the market/put into service, a copy of: the certificate issued by the notified body (in the event of third-party conformity assessment), the instructions for use and the EU declaration of conformity.\n\nContact details: indicate name, registered trade name or registered trademark and the address at which the importer can be contacted on the system and its packaging or accompanying documentation.\n\n## Distributors (article 24)\n\nCooperation with authorities: upon a reasoned request, provide competent authorities with all necessary information/documentation regarding their obligations in the rows above to demonstrate the conformity of that system, and cooperate with these authorities in any action they take in relation to the system.\n\nCorrective actions: take the corrective actions necessary to bring the system into conformity, where the distributor considers or has reason to consider the system not to be in conformity with the requirements set out in Section 2, or withdraw or recall the system, or ensure that the provider, the importer or any relevant operator, as appropriate, takes those corrective actions.\n\n<!-- image -->\n\n## Suppliers (article 25)\n\n## Becoming a provider of someone else's (high-risk) AI system\n\nArticle 25(1) provides that a person will be considered the provider of a high-risk AI system, even if that person was not originally the provider of the AI system, when that person:\n\n- places their name or trademark on a highrisk AI system which is already placed on the market or put into service;\n- makes a substantial modification 2  to an existing high-risk AI system in such a way that it remains high-risk; and/or\n- modifies the intended purpose of an AI system of an AI system which is not currently high-risk so that it becomes high-risk.\n\nIf any of these three situations occur, the original provider will no longer be considered the provider of the (new or newly used) AI-system. One situation which often occurs in practice that could lead to such switching of provider roles is the deployment of a general-purpose AI system by a deployer in a way that falls within the high-risk category as set out in article 6 (and Annexes I and III). As such, if a person deploys  a general-purpose AI system in a high-risk way, that deployer assumes the responsibilities of a provider.\n\nThe new provider will assume all the obligations of a provider of a high-risk AI system. The original provider is obliged to closely cooperate with the new provider and make available the necessary information and provide reasonably expected technical access and other assistance to the new provider to bring the system into conformity with the AI Act (article 25(2)). If, however, that original provider had 'clearly specified' that the AI system was not to be changed into a high-risk AI system (article 25(2)) or 'expressly excluded the change of the AI system into a high-risk AI system' (recital 86), for example by prohibiting deployment for high-risk purposes in the applicable contract(s), then that original provider is not obligated to do\n\n2.  A 'substantial modification' is defined in article 3(23) as 'a change to an AI system after its placing on the market or putting into service which is not foreseen or planned in the initial conformity assessment carried out by the provider and as a result of which the compliance of the AI system with the requirements set out in Chapter III, Section 2 is affected or results in a modification to the intended purpose for which the AI system has been assessed' . The Commission will provide further guidelines on the practical implementation of the provisions related to substantial modification (Article 96(1)(c)). Recital 84 also provides that provisions established in certain Union harmonisation legislation based on the New Legislative Framework, such as the Medical Device Regulation, should continue to apply. For example, article 16(2) of the Medical Device Regulation provides that certain changes should not be modifications of a device that could affect its compliance with the applicable requirements, and these provisions should continue to apply to high-risk AI systems which are medical devices within the meaning of the Medical Device Regulation.\n\n<!-- image -->\n\nthis. If high-risk deployment is not prohibited, then the co-operation obligation applies, but is without prejudice to the need to observe and protect intellectual property rights, confidential business information and trade secrets (article 25(5)). As such, the original provider does not have to help to the extent that it compromises their own intellectual property rights or trade secrets (recital 88).\n\nThe Commission will provide guidelines on the application of the requirements and obligations referred to in this article 25 (article 96(1)(a)).\n\n<!-- image -->\n\n|                                                            | Where can I find this?     | Where can I find this?                      |\n|------------------------------------------------------------|----------------------------|---------------------------------------------|\n| Scope of high-risk systems                                 | Scope of high-risk systems | article 6, Annexes I and III recitals 46-63 |\n| Requirements for providers of high-risk AI systems         | articles 8-22, 47-49       | 43, recitals 64-83, 123-128, 147, 131       |\n| Requirements for deployers of high-risk AI systems article | 26,                        | 27 recitals 91-96                           |\n| Requirements for importers of high-risk AI systems         | article 23                 | recitals 83                                 |\n| Requirements for distributors of high-risk AI systems      | article 24                 | recitals 83                                 |\n| Requirement for third-party suppliers to high-risk systems | article 25                 | recitals 83-90                              |\n| Standards                                                  | article 40,                | 41 recital 121                              |\n| Conformity assessment procedure                            | article 28                 | recital 149                                 |\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## General-purpose AI models\n\n<!-- image -->\n\n- General-purpose AI models are versatile AI components demonstrating immense generality in the tasks they can handle, particularly encompassing current generative AI models.\n- Fine-tuning and modification of generalpurpose AI models may result in new generalpurpose AI models.\n- Providers of general-purpose AI models are tasked with a number of transparency obligations both towards the AI Office and competent authorities as well as towards AI systems providers intending to integrate their AI systems with general-purpose AI models.\n- General purpose AI models that pose systemic risks, i.e., the most versatile and powerful models to date, are under heightened evaluation, transparency, security, risk assessment and incident management obligations. The classification procedure for general-purpose AI models with systemic risk should be a key area of focus for generalpurpose AI models providers.\n- The development and publication of codes of practice will help general-purpose AI models providers identify specific technical and organisational measures to implement in order to comply with their obligations.\n- Provisions regarding general-purpose AI models will apply from 2 August 2025.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n4\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFamiliarise yourself with the concepts of general-purpose AI models, generalpurpose AI systems, AI systems, and high-risk AI systems - and their relation to each other. This understanding is crucial for assessing which systems your company uses or markets and for making informed legal evaluations.\n\n- For providers of general-purpose AI models: undertake a thorough governance review and make necessary adjustments to ensure compliance - the obligations for providers of general-purpose AI models are among the strictest in the AI Act.\n- For providers of general-purpose AI models: conduct a comprehensive legal IP assessment - regulations for generalpurpose AI models are heavily intertwined with IP laws, particularly regarding the copyright policy and the various training data obligations.\n- For providers of general-purpose AI models: continuously and closely monitor the thresholds for 'systemic risk,' as these may be adjusted over time via delegated acts.\n- For providers of general-purpose AI models: keep an eye out for the development and publication of codes of practice, which will include specific and technical details on how to comply with the obligations for general-purpose model providers in practice. Sign up to our Connected newsletter and keep up with the latest developments here!\n\n<!-- image -->\n\n## Background and relevance of general-purpose AI models\n\nOne of the most prominent debates in the legislative process of the AI Act revolved around the regulation of general-purpose AI. The first draft of the AI Act (the Commission's proposal of April 2021) was based on the understanding that each AI system is created for a specific purpose, and that this purpose can be associated with a specific risk potential. This classification did not have in mind foundation models which are trained on broad data such that it can be applied across a wide range of use cases. These AI models did not fit into the risk-based scheme of the first draft of the AI Act. The categorisation had to be expanded to include a new category that took into account the specific capabilities and dangers of such models. In the summer of 2023, the 'foundation model' (later renamed general-purpose AI) was added to the then-current draft of the AI Act.\n\nThe AI Act's chapter on the regulation of generalpurpose AI models holds significant importance for two main reasons:\n\n- firstly, it addresses generative AI, a subset of AI that is currently opening up the most intriguing new opportunities in the business environment and encompasses the majority of corporate use cases; and\n- secondly, the requirements for generalpurpose AI under the AI Act, alongside those for high-risk AI systems, are the most demanding in the AI Act, necessitating the utmost diligence in corporate implementation.\n\nThis significance is only somewhat diminished by the fact that all requirements are directed solely at providers, not deployers.\n\n<!-- image -->\n\n## Terminology and general-purpose AI value chain\n\n## General-purpose AI models and generalpurpose AI systems\n\nArticle 3(63) outlines the characteristics of a general-purpose AI model, emphasising its versatility and competence across various tasks.\n\nRecital 98 highlights two key indicators:\n\n1. having at least a billion parameters; and\n2. being trained with a large amount of data using self-supervision.\n\nThese models are distinguished by their ability to integrate into and function within diverse downstream systems or applications. Typically, general-purpose AI models undergo extensive training with large datasets, often utilising methods like self-supervision at scale. Recital 99 further specifies that large generative AI Models, such as LLMs or Diffusion Models, are typical examples of general-purpose AI models.\n\nRecital 97 clarifies that while general-purpose AI models are crucial components of AI systems, they are not AI systems themselves. Additional elements, such as user interfaces, are needed to transform general-purpose AI models into fully operational AI systems. A general-purpose AI system is an AI system built upon a generalpurpose AI model, maintaining its versatility across various tasks (article 3(66) and recital 100). To clarify with an example, a system that solely performs translations would likely not qualify as a general-purpose AI system.\n\n## General-purpose AI systems and high-risk AI systems\n\nRecital 85 emphasises that general-purpose AI systems, due to their versatility, may function as high-risk AI systems or as components within them. Providers of general-purpose AI systems must collaborate closely with providers of highrisk AI systems to ensure compliance with the AI Act and to distribute responsibilities fairly along the AI value chain (see Chapter 4 for more on high-risk systems).\n\n## Modification and fine-tuning of generalpurpose AI models\n\nModifying or fine-tuning a general-purpose AI model, where a new specialised training data set is fed into the model to achieve better performance for specific tasks, does not transform it into a general-purpose AI system; it remains an abstract model without an interface. Instead, such actions create a modified or fine-tuned general-purpose AI model. Recital 97 and recital 109 specify that a provider who modifies or fine-tunes a general-purpose AI model has limited obligations related only to the changes made, including providing technical documentation or a summary of the training data used.\n\n## Obligations for providers of general-purpose AI models\n\nA provider of a general-purpose AI model that places such a model on the market, or integrates it with its own AI system and places it on the market or puts it into service, is obliged to:\n\n- a. prepare and maintain up-to-date technical documentation containing i.a. a description of the model and information on its development process (including training, testing and validation) for the purpose of making it available to the AI Office and competent authorities (article 53(1)(a)) -a list of the minimum information required is provided in Annex XI;\n- b. prepare, maintain up-to-date and make certain information and documentation available to downstream AI systems providers (i.e. those who wish to integrate their AI systems with the general-purpose AI model) so that they can understand the model's characteristics and comply with their own obligations (article 53(1)(b)) - a list of the minimum information required is provided in Annex XII; providers are allowed to balance the information they share against their need to protect confidential business information and trade secrets;\n- c. establish a policy to comply with the EU regulations on copyright and related rights (article 53(1)(c)), taking into account, i.a., the right to opt-out of text and data mining as provided for in article 4(3) of Directive (EU) 2019/790 -on copyright and related rights\n4. in the Digital Single Market (the AI Act does not specify other matters that have to be addressed in the policy);\n- d. prepare and publicly share a comprehensive summary on the data used for training the model (article 53(1)(d)) - the AI Office is tasked with providing a template for this purpose; as the recital 107 explains the summary should allow interested parties to exercise their rights by, for example, listing main data collections, databases or data archives used;\n- e. cooperate with the relevant authorities when they exercise the powers granted to them under AI Act (article 53(3)); and\n- f. if the provider is established outside the EU: appoint an authorised representative in the EU (article 54(1)).\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nIf a provider releases a general-purpose AI model under a free and open-source licence and makes relevant information publicly available, it is not obliged to fulfil the requirements listed in a-b and f above - unless the general-purpose AI model is qualified as presenting a systemic risk (article 53(2) and article 54(6)).\n\n## General-purpose AI models with systemic risk\n\n## Qualification criteria\n\nThe AI Act introduces specific heightened obligations for general-purpose AI models presenting 'systemic risks' , e.g. reasonably foreseeable negative effects relating to major accidents, disruption of critical sectors, serious consequences to public health and safety, public and economic security, democratic processes, the dissemination of false or discriminatory content, etc. (recital 110).\n\nAccording to article 51(1) of the AI Act, a general-purpose AI model is classified as a general-purpose AI model with systemic risk if it meets one of these two conditions: (a) it has ' high impact capabilities ' evaluated on the basis of technical tools and methodologies, or (b) is designated by the Commission as having capabilities or impact equivalent to those set out in point (a) having regard to the criteria set out in Annex XIII of the AI Act. These criteria notably\n\ninclude the number of parameters of the model, the quality or size of the data set, the amount of computation used for training, the model's impact on the European market, the number of registered users the EU.\n\nIn addition, a model is presumed to have ' high impact capabilities ' if it is trained with more than 10^25 floating point operations, i.e., massive computing powers (article 51(2)). At the time of this Guide, only a handful of Large Language Models seem to meet this threshold.\n\nArticle 52 of the AI Act sets out the classification procedure. Most notably, providers of generalpurpose AI models which meet the systemic risk classification conditions must notify the Commission without delay, and at the latest within two weeks after that requirement is met or it becomes known that it will be met. Providers may present arguments to demonstrate that their models do not pose systemic risks despite meeting the requirements. Should such arguments be rejected by the Commission, the concerned models will be considered as presenting\n\n<!-- image -->\n\nsystemic risks. Upon 'reasoned request' of a provider, the Commission may decide to reassess the classification (article 52(5)).\n\nA list of general-purpose AI models with systemic risk will be published and updated by the Commission (article 52(6).\n\n## Obligations for providers of general-purpose AI models with systemic risk\n\nIn addition to the general requirements applicable to all general-purpose AI models providers, the AI Act imposes additional heightened obligations on providers of generalpurpose AI models with systemic risk (articles 53(1) and 55(1)). These obligations apply prior to the models' placing on the market and throughout their entire lifecycle, and relate to:\n\n- models evaluation;\n- assessment and mitigation of systemic risks;\n- incident management and reporting;\n- increased level of cybersecurity protection; and\n- extended technical documentation.\n\n## Transparency\n\n<!-- image -->\n\nThe AI Act classifies AI systems by risk level, with increased transparency demands for high-risk categories. Transparency is required for high-risk AI systems before they are placed on the market or put into service. See Chapter 4 of this guide for more details regarding the transparency requirements for high-risk AI systems.\n\nAdditionally, the AI Act mandates transparency requirements under article 50 for specific types of products, requiring that adequate information be provided to individuals, by either providers or deployers.\n\n- Disclaimers: providers of AI systems intended to interact directly with individuals' need to design and develop them, so that the individuals will be informed about the fact that they are interacting with an AI system.\n- Marking requirement: providers of AI systems must mark AI-generated content (audio, images, videos, text) in a way that distinguishes it from humangenerated content.\n- Deepfake marking: AI-generated content (images, audio, video) that resembles real entities and could mislead people into believing it is authentic must be labelled.\n- Emotion recognition system/ biometric categorisation system: deployers of AIsystems should make individuals aware of the operation of these systems.\n\nThe AI Act's transparency obligations collate with the other regulatory framework in the EU. In particular, there is some overlap between the transparency requirements of the GDPR and the AI Act, although the latter is more technical in nature.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## For providers\n\n<!-- image -->\n\nImplement Marking: ensure AI-generated content is marked in a machine-readable format.\n\n<!-- image -->\n\nImplement Disclaimers: ensure proper disclaimers are added to AI systems intended to interact directly with individuals.\n\n## For deployers\n\n<!-- image -->\n\nDeepfakes: label as 'Deepfake' in a clear and distinguishable manner to disclose their artificial creation or manipulation.\n\n<!-- image -->\n\nEmotion recognition system/ biometric categorisation system: make individuals aware that such a system is operating.\n\n## General transparency obligations\n\nThe AI Act acknowledges the importance of transparency in the use of AI systems. Individuals should be enabled to understand the AI system's design and use, and there should be accountability for decisions made by companies and public authorities. Transparency is also essential for creating public trust in AI systems and ensuring their responsible deployment.\n\nTransparency also enhances the broader concept of 'AI literacy', developing awareness about the opportunities and risks of AI and the possible harm it can cause. Such awareness should especially be developed amongst:\n\n- individuals concerned, giving them a better understanding of their rights in the context of AI, and\n- deployers, allowing them to deploy AI systems in an informed way.\n\nProviders, and in certain circumstances deployers as well, have their own transparency requirements. The AI Act classifies AI systems by risk level, with increasing transparency demands for higher risk categories.\n\nThe transparency requirements for specific types of products are described below.\n\n## Provider Obligations:\n\n## Chatbots (article 50(1) AI Act)\n\nArticle 50(1) of the AI Act mandates that providers of AI systems need to ensure that such systems intended to interact directly with individuals are designed and developed such that the individuals concerned are informed that they are interacting with an AI system.\n\n- Target audience: when implementing that obligation, the provider should identify not only the intended but also the broader potential target audience to whom the disclaimer may be displayed. The characteristics of individuals belonging to vulnerable groups due to their age or disability should be taken into account, to the extent the AI system is also intended to interact with those groups. The intended or potential target audience has a significant impact on accessibility considerations.\n- Form: in practice, providers can design a disclaimer in different forms (e.g. as an avatar, icon or interface), as long as it provides clear information that the individual is interacting with an AI system.\n\n<!-- image -->\n\n## Exemptions\n\n- Obvious cases: if, considering the circumstances and the context of the AI system use, it is obvious for an individual who is reasonably well-informed, observant and circumspect that they are interacting with an AI system, then the system is exempt from this transparency requirement.\n- Legal use: AI systems that are permitted by law for use in detecting, preventing, investigating, or prosecuting criminal activities, that are subject to appropriate safeguards for the rights and freedoms of third parties, are also exempt from these transparency requirements, unless those systems are available for the public to report a criminal offence.\n\n## Marking of AI-Generated Content (article 50(2) AI Act)\n\nArticle 50(2) of the AI Act mandates that providers of AI systems, including generalpurpose AI systems, must appropriately mark synthetic content such as audio, images, videos, or text. Recital 133 explains the rationale: with AI technology advancing, AI-generated synthetic content is becoming increasingly indistinguishable from human-generated content, posing the risk of misinformation, manipulation, fraud, impersonation, and consumer deception.\n\n## The Marking Obligation\n\n- Marking : only providers of AI systems are required to mark AI-generated content. This requirement does not extend to deployers or other users of the content.\n- Format : the output must be marked in a machine-readable format to indicate that it is artificially generated or manipulated.\n\n- Technical standards : the markings should be effective, interoperable, robust, and reliable. Providers need to consider the type of content, implementation costs, and current technical standards.\n\n## Marking methods:\n\n- -Watermarks: visible watermarks can be easily implemented - but also removed with basic editing tools, whereas invisible watermarks require specialised software for detection and removal.\n- -Metadata: this provides information about the file's creation and origin but can be easily altered or removed with file editing tools.\n- -Algorithmic fingerprints: AI models leave unique traces or anomalies in the content they generate. For instance, AI-generated images might have minor distortions in textures or patterns, and AI-created audio files could display unnatural pauses or tonal shifts.\n- -Cryptographic signatures: digital signatures embedded using cryptographic methods, such as a cryptographic hash that verifies content authenticity. Even minor changes in the data result in a different hash, ensuring easy verification of alterations.\n\nNumerous tools and initiatives exist to manage and detect AI-generated content. Certain platforms use deepfake detection software that analyses algorithmic patterns and embedded metadata, while others rely on metadata and cryptographic hashes to authenticate the source of the content. For example, platforms might use voice analysis tools to detect synthetic audio, or employ blockchain technology to track the origin of and modifications to digital art.\n\n## Exemptions\n\n- Editorial assistance: AI systems that mainly provide support for routine editing tasks or do not significantly change the original input data are exempt from the marking obligation.\n- Legal use: AI systems that are authorised for use in detecting, preventing, investigating, or prosecuting criminal activities are also exempt from the marking requirement.\n\n<!-- image -->\n\n## Deployer obligations:\n\n## Emotion recognition/ biometric categorisation systems (article 50(3) AI Act)\n\nArticle 50(3) of the AI Act sets forth specific transparency requirements for deployers of:\n\n- Emotion recognition systems: AI systems used for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data, e.g., nonverbal signs such as facial expression.\n\nor\n\n- Biometric categorisation systems: AI systems used for the purpose of assigning natural persons to specific categories on the basis of their biometric data. Such specific categories can relate, e.g., to aspects such as sex, age, hair colour, eye colour, tattoos, personal traits, ethnic origin, personal preferences and interests.\n\nSee Chapter 4 of this guide for more details on when the use of emotion recognition systems or biometric categorisation systems is prohibited.\n\nWhen these systems are allowed, deployers must inform the natural persons exposed to them about the use of the system. In particular, individuals should be notified when they are exposed to AI systems that, by processing their biometric data, can identify or infer their emotions or intentions or assign them to specific categories.\n\n## Exemptions\n\n- Legal use: AI systems that are permitted for use in detecting, preventing or investigating criminal activities that are subject to appropriate safeguards for the rights and freedoms of third parties and in accordance with the Union law, are exempt from these requirements.\n- Biometric categorisation systems of ancillary use: AI systems whose use is ancillary to another commercial service and strictly necessary for objective technical reasons are exempt from these requirements.\n\nAt present, there are no definitive guidelines on the scope of information that should be provided. Deployers, when using these systems, process personal data in accordance with GDPR and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable, apart from the requirements on\n\nthe legal basis of the processing. This means that these regulations also constitute separate transparency obligations for deployers acting as controllers. In such cases, individuals should nevertheless be informed about the processing of their data as required under Article 13 and 14 GDPR. In relation to any automated processing, controllers are expected to additionally explain the logic behind their decision-making. In the case of an AI system, this might be provided as part of an explainability statement - a document providing a non-technical explanation of i.a. why the organisation uses AI, how AI was developed, and how it operates and is used.\n\n## Deepfakes (article 50(4) AI Act)\n\nArticle 50(4) of the AI Act sets forth specific labelling requirements for content known as 'Deepfakes' . These obligations are crucial for ensuring transparency when AI systems are used to generate or manipulate content.\n\n## Definition of Deepfakes (article 3(60) AI Act)\n\nDeployers using AI to create content that:\n\n- generates or manipulates images, audio, or video;\n- significantly resembles real people, objects, places, entities, or events; and\n- could mislead a person into believing the content is authentic or truthful.\n\nExamples of Deepfakes:\n\n- Deepfake video calls mimicking company executives to trick employees into transferring large sums of money.\n- AI-generated audio of politicians misleading voters about election dates via robocalls.\n- Deepfake video ads impersonating political figures to manipulate public opinion on social media.\n- Fake Zoom interviews using deepfake technology to impersonate high-profile individuals.\n- Digital avatars delivering fabricated news reports to deceive viewers.\n\n<!-- image -->\n\n## Labelling requirements\n\nThe AI Act mandates that any content generated or manipulated by AI systems must be clearly and distinguishably labelled to disclose its artificial creation or manipulation. This requirement aims to ensure transparency and prevent the public from being misled by such content. At present, there are no definitive guidelines on how content should be labelled. This issue is likely to be addressed in future Codes of Conduct.\n\nTechniques such as watermarks, metadata identifications, fingerprints or other methods should be employed to indicate the content's artificial nature (see recital 133). It is crucial that these labels are easily, instantly and constantly visible to the audience. For instance, in the case of videos, pre-roll labels or persistent watermarks may be used to meet these requirements effectively.\n\n## Exemptions\n\nThere are certain reliefs and exceptions to the labelling requirements under article 50(4) AI Act:\n\n- The transparency requirements are more relaxed for artistic, creative, satirical, fictional, or similar works. Examples of such works include AI-generated movies or parodies, digital art exhibits, and AI-generated music videos. In these instances, the obligation is to disclose the AI involvement in a manner that does not disrupt the viewer's experience. This can be achieved through subtle watermarks, brief audio disclaimers, or notes in the description texts on digital platforms.\n- The obligation to label AI-generated content does not apply if the AI system's use is legally authorised for the purposes of detecting, preventing, investigating or prosecuting criminal offences.\n- The labelling obligation may not apply if the AI-generated content has undergone human review or editorial control, with a natural or legal person holding editorial responsibility for the publication. This means that, if a human has reviewed and approved the AI-generated content, ensuring its accuracy and integrity, the stringent labelling requirements may be relaxed. This exception recognises the role of human oversight in maintaining the quality and reliability of AI-generated content.\n\n## Transparency obligations for high-risk AI systems\n\nArticle 50(6) explains that the transparency obligations outlined here operate alongside other regulatory requirements. They neither replace nor reduce the obligations specified in Chapter III or other transparency requirements under EU or national legislation.\n\nSee Chapter 4 of this guide for more details.\n\n## Timing and format\n\nAll the information required to meet the transparency obligations under article 50 must be provided to the individuals concerned:\n\n- in a clear and distinguishable manner;\n- by no later than the time of their first interaction or exposure to the AI system; and\n- in conformity with the applicable accessibility requirements.\n\nThe accessibility requirement means that the information should be accessible to diverse audiences, including individuals with disabilities. In practice, this may imply that, depending on the circumstances, disclaimers or other marking methods will have to be displayed not only in written form but also in aural and (audio) visual form.\n\nAnother aspect to be taken into account is that the individual should be provided with an amount of information that is clear and adequate but not overwhelming.\n\n## Transparency obligations at the national level and codes of practice\n\nThe transparency obligations outlined in article 50(1)-(4) AI Act are designed to coexist with other regulatory requirements, according to article 50(6) AI Act. They neither replace nor\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\ndiminish the requirements set forth in Chapter III or other transparency mandates under Union or national law.\n\nThe AI Office is responsible for promoting and facilitating the development of codes of practice to support the effective implementation of the transparency obligations under article 50(1)-(4) AI Act at the EU level, under article 50(7) AI Act. These codes are intended to clarify the methods for detecting and labelling AI-generated content, to enhance cooperation throughout the value chain, and to ensure that the public can clearly distinguish between content created by humans, and content generated by AI (recital 135).\n\n## Relationship with other regulatory frameworks\n\n- The AI Act's marking obligations under article 50(2)-(4) support the Digital Services Act's (DSA) requirements for very large online platforms (VLOP) and search engines (VLOS) to identify and mitigate the risks associated with the dissemination of deepfakes (article 33 et seq. DSA). If the AI provider is separate from the VLOP or VLOS, these markings enable the platforms to recognise AI-generated content more efficiently. Conversely, if a VLOP or VLOS is also the AI provider, their DSA obligations are further detailed and enhanced by the AI Act.\n- The transparency regulations for deepfakes will correlate with the European guidelines on misleading advertising (see Unfair Commercial Practices Directive) as well as national criminal provisions on deepfakes.\n- The AI Act's transparency obligations also support and supplement the transparency requirements under Regulation (EU) 2016/679. However, the GDPR transparency requirements apply if personal data is processed when using AI technologies at all different stages of the AI lifecycle (e.g. when developing, testing or deploying AI technologies), and apply to controllers. Developers and providers of AI tools will not always be acting in such a role. In such case they may still be obliged to provide specific information to controllers to enable the latter to meet their obligations.\n\n## AI regulatory sandboxes\n\n<!-- image -->\n\n- The AI Act enables the establishment of 'AI regulatory sandboxes' to provide a controlled environment in which to test innovative AI systems for a limited period before they are placed on the market.\n- This regime is intended to encourage AI providers (or potential providers) to experiment with new and innovative products under supervision by regulators. There are specific incentives aimed at encouraging participation by SMEs and start-ups.\n- Each Member State must establish at least one AI regulatory sandbox by 2 August 2026, although this can be done in co-operation with other Member States.\n- The Commission is expected to adopt implementing acts to set out detailed arrangements for the establishment, operation and supervision of AI regulatory sandboxes.\n- The AI Act also provides for 'real-world' testing of AI systems, both inside and outside of regulatory sandboxes, subject to certain conditions to protect participants.\n- The regimes relating to AI regulatory sandboxes and real-world testing are intended to be harmonise across the Union. However, there is the potential for divergent approaches at a national level, leading to a possibility of 'forum shopping' by providers.\n- Participation in AI regulatory sandboxes and real-world testing is voluntary. AI providers should familiarise themselves with the relevant provisions of the AI Act if they intend to participate in a sandbox or real-world tests and should look out for further announcements and guidance on these topics, including detailed arrangements for AI regulatory sandboxes to be specified by the Commission in due course.\n- You should think about the countries in which you would like to test your AI services/products. Although the AI Act intends to establish a harmonised regime, there may be national differences which make some Member States more appropriate for you than others.\n- Once you decide to participate in an AI regulatory sandbox, you will need to prepare a sandbox plan and follow the guidelines and supervision provided by the relevant national competent authority. If you decide to conduct real-world tests, you will also need to prepare a testing plan and seek approval from the relevant market surveillance authority.\n- When you successfully complete an AI regulatory sandbox process, you should obtain an exit report from the relevant national competent authority. This may be useful to accelerate the conformity assessment process for your AI product/service.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## AI regulatory sandboxes\n\nThe AI Act enables the creation of 'regulatory sandboxes' to provide a controlled environment in which to test innovative AI systems for a limited period before they are placed on the market or otherwise put into service. The objectives of the AI regulatory sandbox regime include:\n\n- fostering AI innovation while ensuring innovative AI systems comply with the AI Act;\n- enhancing legal certainty for innovators;\n- enhancing national competent authority understanding of the opportunities, risks and the impacts of AI use;\n- supporting cooperation and the sharing of best practices; and\n- accelerating access to markets, including by removing barriers for SMEs and start-ups.\n\n## What is a regulatory sandbox under the AI Act?\n\nThe AI Act defines an 'AI regulatory sandbox' as:\n\n' a controlled framework set up by a competent authority which offers providers or prospective providers of AI systems the possibility to develop, train, validate and test, where appropriate in real-world conditions, an innovative AI system, pursuant to a sandbox plan for a limited time under regulatory supervision. '\n\nAI regulatory sandboxes can be established in physical, digital or hybrid form and may accommodate physical as well as digital products.\n\n## Obligation on Member States to establish AI regulatory sandboxes\n\nThe obligation to establish AI regulatory sandboxes rests with the Member States and their national competent authorities (see Chapter 8 for more on these). Each Member State must establish at least one AI regulatory sandbox by 2 August 2026. However, Member\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nStates can choose to either (i) establish one or more AI regulatory sandboxes at national level; (ii) jointly establish a sandbox with the national competent authorities of one or more other Member States or (iii) participate in an existing sandbox.\n\nNational competent authorities establishing AI regulatory sandboxes should cooperate with other relevant national competent authorities where appropriate and may also involve other actors within the AI ecosystem. The EU Data Protection Supervisor may also establish an AI regulatory sandbox for European Union institutions, bodies, offices and agencies.\n\nA list of planned and existing sandboxes will be made publicly available by the AI Office. The Commission also intends to develop a single interface containing relevant information relating to AI regulatory sandboxes to allow stakeholders to:\n\n- interact with AI regulatory sandboxes;\n- raise enquiries with national competent authorities; and\n- seek non-binding guidance on the conformity of innovative AI products, services or business models.\n\n## Who can participate in AI regulatory sandboxes?\n\nThe sandbox regime is aimed at providers (or prospective providers) of AI systems, although applications can be submitted in partnership with deployers and other relevant third parties.\n\nThere are specific provisions which are designed to encourage participation by SMEs and startups, including:\n\n- access to sandboxes should generally be free of charge for SMEs and start-ups;\n- priority access for SMEs and start-ups with a registered office or branch in the EU; and\n- SMEs and start-ups should have access to guidance on the implementation of the AI Act and other value-added services.\n\n## Liability\n\nProviders and prospective providers participating in an AI regulatory sandbox (including SMEs and start-ups) will remain liable for any harm inflicted on third parties as a result of the experimentation taking place in the sandbox. However, administrative fines will not be imposed on prospective providers if:\n\n- they observe the relevant sandbox plan and the terms and conditions for their participation; and\n- follow (in good faith) any guidance given by the national competent authority.\n\n## Implementation of the sandbox regime\n\nIn order to avoid fragmentation across the EU, the Commission intends to adopt implementing acts specifying the detailed arrangements for the establishment, operation and supervision of AI regulatory sandboxes, including common principles on:\n\n- eligibility and selection criteria for participation;\n- procedures for the application, participation, monitoring, exiting from and termination of sandboxes; and\n- the terms and conditions applicable to participants.\n\nThese implementing acts are intended to ensure that AI regulatory sandboxes:\n\n- are open to any provider who meets fair and transparent eligibility criteria;\n- allow broad and equal access and keep up with demand for participation;\n- facilitate the development of tools and infrastructure for testing and explaining dimensions of AI systems relevant for regulatory learning, such as accuracy, robustness and cybersecurity, as well as measures to mitigate risks to fundamental rights and society at large;\n- facilitate the involvement of relevant actors within the AI ecosystem (e.g. notified bodies and standardisation organisations, testing and experimentation facilities, research and experimentation labs and European Digital Innovation Hubs), and also that participation in an AI regulatory sandbox is uniformly\n\n<!-- image -->\n\nrecognised (and carries the same legal effects) across the EU.\n\n## National competent authority obligations\n\nNational competent authorities must:\n\n- allocate sufficient resources to ensure their sandbox regime complies with the requirements of the AI Act;\n- provide guidance to sandbox participants on how to fulfil the requirements of the AI Act;\n- provide participants with an exit report detailing the activities carried out in the sandbox, results and learning outcomes, which can later be used to demonstrate compliance with the AI Act through the conformity assessment process or relevant market surveillance activities; and\n- provide annual reports to the AI Office and the Board (see Chapter 8 for more on these), identifying best practices, incidents and lessons learnt.\n\nNational competent authorities will retain supervisory powers in relation to sandbox activities, including the ability to suspend or terminate activities carried out within a sandbox where it is necessary to address significant risks to fundamental rights or health and safety.\n\n## Processing of personal data within sandboxes\n\nPersonal data which has been lawfully collected for other purposes can be used in an AI regulatory sandbox subject to compliance with various conditions set out in the AI Act (all of which must be met for the relevant processing activities to be permitted). Some of the key conditions include:\n\n- the relevant AI system being deployed in the sandbox must be aimed at safeguarding substantial public interest (e.g. public health, energy sustainability, safety of critical infrastructure);\n- use of the personal data must be necessary and could not be substituted with anonymised or synthetic data;\n- the personal data must be handled in a separate and protected environment and must be subject to appropriate technical and organisational measures; and\n\n- a detailed description of the process and rationale behind the training, testing and validation of the AI system is retained, together with the testing results.\n\n## Real-world testing of AI systems\n\nThe AI Act also enables the testing of AI systems in 'real-world conditions', subject to certain conditions.\n\nThe AI Act defines 'testing in real-world conditions' as follows:\n\n'the temporary testing of an AI system for its intended purpose in real-world conditions outside a laboratory or otherwise simulated environment, with a view to gathering reliable and robust data and to assessing and verifying the conformity of the AI system with the requirements of [the AI Act] '.\n\nSuch real-world testing will not qualify as placing the relevant AI system on the market or putting it into service, provided that the relevant requirements of the AI Act are complied with. (See Chapter 2 for more on these concepts).\n\nThe AI Act primarily focusses on real-world testing of high-risk AI systems outside of AI regulatory sandboxes. However, the AI Act also contemplates the possibility of AI systems (whether high-risk or not) being subject to realworld testing within the framework of an AI regulatory sandbox, under the supervision of a national competent authority.\n\nIn both scenarios, the real-world testing must comply with various conditions set out in the AI Act (all of which must be met for the testing to be permitted, although there is greater\n\n<!-- image -->\n\nflexibility where the testing is conducted within a sandbox). Some of the key conditions include:\n\n- the proposed real-word tests have been approved by the relevant market surveillance authority and registered in the EU database for high-risk AI systems;\n- the provider conducting the testing is established in the EU (or has appointed a legal representative established in the EU);\n- testing is limited to a maximum of 6 months (which can be extended for an additional 6 months, although this requirement can be derogated from in relation to real-world testing within a sandbox environment);\n- participants in the real-world testing are properly protected - they must give informed consent, outcomes must be reversible (or capable of being disregarded) and they must be able to withdraw at any time; and\n- market surveillance authorities can conduct unannounced inspections on the conduct of real-world tests.\n\nProviders and prospective providers will be liable for any damage caused in the course of their real-world testing.\n\n## Is there a risk of 'forum shopping' in relation to participation in sandboxes and real-world testing?\n\nAlthough the AI Act aims to harmonise the regimes relating to AI regulatory sandboxes and real-world testing across the EU, industry representatives and stakeholders will no doubt closely monitor their development and may elect participate in sandboxes and/or real-world testing in jurisdictions which are perceived to have the most industry-friendly approach (including in how liability relating to participation in sandboxes or real-world testing is determined).\n\n## Enforcement and governance\n\n<!-- image -->\n\n- The AI Act puts in place a post-market monitoring, reporting and information sharing process.\n- Most obligations are on providers of high-risk AI systems who have to have post-market monitoring systems and procedures to report serious incidents.\n- The serious incident reporting obligations can also sometimes apply to deployers.\n- The timelines for reporting can be immediate.\n- Reports need to be made to market surveillance authorities in Member States where the incident occurred; reporting to multiple authorities may therefore be needed.\n- There is a multi-pronged approach to enforcement:\n- -The European Data Protection Supervisor handles EU institutions etc.\n- -The European Commission handles providers of general-purpose AI models.\n- -Competent authorities in each Member State are otherwise responsible.\n- Sanctions are tiered, by reference to the seriousness of the provision that has been infringed.\n- Affected persons have a right to explanation of individual decision-making.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Providers of high-risk AI systems should:\n\n- Watch for the European Commission template post-market monitoring plan, to be adopted by 2 February 2026.\n- Prepare and implement a post-market monitoring plan.\n- If already subject to existing postmarket monitoring obligations, or a regulated financial services provider, consider if you can integrate your AI Act obligations into these systems.\n\n## Providers of high-risk systems should:\n\n- Consider if they are already subject to other equivalent obligations; if so, check if you have double reporting obligations or not.\n- Ensure quality management systems include serious incident reporting procedures.\n- Ensure these procedures establish the nature of the serious incident (death, serious harm to health, violation of fundamental rights etc) and if they are widespread.\n- Identify to whom you would have to report.\n\n## Deployers of high-risk systems should:\n\n- Develop stand-by procedures so they can report if needed.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Providers and developers should:\n\n- Check for the European Commission guidance due by 2 August 2025.\n- Keep this under review as it will be re-assessed.\n\nOperators of non-high-risk AI systems should:\n\n- Ensure they comply with all existing product safety legislation.\n- Providers of general-purpose AI models should:\n- Look out for, and consider responding to the consultation on, the European Commission implementing act relating to the arrangements for enforcement by the European Commission.\n\nAll organisations in the AI value chain\n\n- should:\n- Look out for, and consider responding to consultations on, rules relating to enforcement adopted at Member State level.\n- Note the requirement to cooperate with market surveillance authorities where there is sufficient reason to consider that an AI system presents a risk.\n- Note that disclosure of training, validation and testing data sets and source code might have to be disclosed.\n\nDeployers of high-risk systems should:\n\n- Ensure they are able to provide clear and meaningful explanations as to the AI's decision-making procedure.\n\n## Overview\n\nThe AI Act outlines a governance framework that provides for the implementation and supervision of both the ex ante requirements for AI systems and ex post surveillance and enforcement.  The former is described in preceding chapters.  The latter is the subject of this chapter, together with a description of the governance structure.\n\nThe enforcement regime addresses two types of risk: risks to product safety, and risks to fundamental rights.  In relation to the former, the AI Act builds upon existing product safety legislation and is mostly enforced by national market surveillance authorities.  Where risks to fundamental rights are identified, the market surveillance authorities shall inform and fully cooperate with the relevant national public authorities or bodies protecting fundamental rights.\n\nConsistent with the risk-based approach in the AI Act, a multi-layered enforcement structure with different regimes applying to AI systems with varying risks is provided.  For high-risk AI systems, the AI Act mandates, firstly, postmarket monitoring obligations and, secondly, a requirement to report serious incidents.  The serious incident reporting obligations can also sometimes apply to deployers, who should therefore also be aware of them.\n\nThe marketing surveillance authorities can require operators to take all appropriate measures to ensure that AI systems do not present a risk and, where necessary, can demand the withdrawal of a product or AI system from the market.  Very significant fines for noncompliance with the terms of the AI Act can also be levied.\n\nFor general-purpose AI models, the European Commission has exclusive powers to supervise and enforce the obligations in the AI Act.\n\nThe governance structure in the AI Act provides for the setting up of new institutional bodies at both the EU level (the AI Office, the European AI Board, the Advisory Forum and the Scientific Panel) and national level (notifying authorities and market surveillance authorities) and the roles and competencies of each of them are outlined.  The coordination between these bodies will be key to the effective implementation and enforcement of the AI Act.\n\n<!-- image -->\n\nTopics addressed in this chapter are as follows:\n\n- Post-marketing obligations\n- Market surveillance authorities\n- Procedures for enforcement\n- Authorities protecting fundamental rights\n- General-purpose AI models\n- Penalties\n- Remedies for third parties\n- Governance\n\n## Post-marketing obligations\n\n## Post-market monitoring system for high-risk AI systems\n\nSince AI systems have the ability to adapt and continue to learn after they are placed on the market, it is important to monitor their performance once they are put on the market. Recital 155 explains that the aim of the postmarket monitoring system is to ensure that providers of high-risk AI systems can consider experience from use of the system, so as to ensure ongoing compliance and improvement of the system.\n\nProviders of high-risk AI systems must include a post-market monitoring plan as part of the technical documentation that they draw up before they put the system on the market (articles 72(3) and 11(1)). This plan must be in line with the European Commission template, to be adopted by 2 February 2026. The postmarketing obligations will ensure that any need to immediately apply any necessary corrective or preventative actions are identified (article 3(25)).\n\nArticle 72 provides that the post-market monitoring system (and the documentation of the system) must be proportionate to the nature of the AI technology and the risks of the systems. This system must actively and systematically collect, document, and analyse relevant data throughout the AI system's lifetime, so as to allow the provider to evaluate continuous compliance. The data could be provided by deployers, or\n\nby others (although sensitive operational data from law enforcement authority deployers is excluded).  Where relevant, the system should also include analysis of interactions with other AI systems, including devices and software.\n\nProviders of certain types of high-risk AI systems, who already have post-market monitoring systems in place, can integrate their obligations under the AI Act into those existing systems, provided this achieves an equivalent level of protection. This is the case for high-risk AI systems covered by Union harmonisation legislation listed in Section A of Annex I (i.e. including certain machinery, toys and medical devices). It's also the case for financial institutions who are subject to requirements under Union financial services law regarding their internal governance, arrangements or processes, where these institutions place on the market high-risk AI systems listed in Annex III point 5 (in particular, evaluation of creditworthiness or for risk assessment and pricing in relation to life and health insurance) (article 72(4)).\n\n## Reporting of information on serious incidents for high-risk AI systems\n\nProviders of high-risk AI systems must report 'serious incidents' and the provider's quality management system must contain procedures relating to this (article 17(1)(i)). Ordinarily, deployers of high-risk AI systems must report serious incidents to the provider. However, if the deployer cannot reach the provider, then the serious incident reporting obligations of article 73 apply directly to the deployer (article 26(5)). Accordingly, deployers should also be aware of these provisions.  The European Commission is to issue guidance for providers on incident reporting by 2 August 2025 and must keep this under regular review.\n\nSerious incidents are defined at article 3(49) and mean an incident or malfunctioning of an AI system that directly or indirectly causes:\n\n- death, or serious harm to a person's health;\n- serious and irreversible disruption to management or operation of critical infrastructure;\n- violation of Union laws protecting fundamental rights; or\n- serious harm to property or the environment.\n\nSerious incidents must be reported within set timelines, as set out below. If necessary, the provider or deployer may submit an initial report, which can be completed later (article 73(5)).\n\n| Situation                                                                                                                                            | Period                                                                                                                                                                                                     |\n|------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Widespread infringement Or Serious incident involving critical infrastructure                                                                        | Immediately                                                                                                                                                                                                |\n|                                                                                                                                                      | ≤ than 2 days after awareness of the incident                                                                                                                                                              |\n| Death of a person                                                                                                                                    | ≤ 10 days after awareness of the serious incident; or Immediately after establishing or suspecting a causal relationship between the serious incident and the AI system if earlier                         |\n| Other situations (i.e. serious harm to health, fundamental rights violations, serious harm to property or environment - unless these are widespread) | ≤ 15 days after awareness of the serious incident; or Immediately after the provider has established a causal link, or the reasonable likelihood of a link, between the AI system and the serious incident |\n\n<!-- image -->\n\nAfter reporting, the provider must promptly conduct necessary investigations, including a risk assessment and corrective actions.  The provider must not do anything that would alter the AI system in a way that may affect any subsequent evaluation of the cause of the incident before it has informed the competent authorities.\n\nReports of serious incidents have to be made to the market surveillance authorities of the Member States where the incident occurred (article 73(1)).  It follows that if a serious incident affects multiple Member States or affects multiple sectors so that there are multiple market surveillance authorities within a Member State, then multiple reports will need to be made.\n\nThe market surveillance authority must take appropriate measures (which can include withdrawal or recall of the product) within seven days of receiving the notification and must also immediately notify the European Commission of any serious incident, whether or not they have taken action (article 73(8/11)).\n\n## Non-high-risk AI systems\n\nAI systems relating to products that are not high-risk nevertheless must be safe when placed on the market or put into service.  Regulation (EU) 2023/988 on general product safety and Regulation (EU) 2019/1020 on market surveillance and compliance of products apply to all AI systems governed by the AI Act, but these two Regulations provide the safety net for nonhigh-risk products (recital 166 and article 74(1)).\n\nRegulation (EU) 2019/1020 requires all operators to inform the relevant market surveillance authority when they have reason to believe that a product presents a risk under article 3(19) (see definition below).  To the list of risks in article 3(19), the AI Act has added risks to fundamental rights of persons (article 79(1)).\n\n<!-- image -->\n\n' Product presenting a risk ' means a product having the potential to affect adversely health and safety of persons in general, health and safety in the workplace, protection of consumers, the environment, public security and other public interests, protected by the applicable Union harmonisation legislation, to a degree which goes beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the product concerned, including the duration of use and, where applicable, its putting into service, installation and maintenance requirements.\n\n## Market surveillance authorities\n\nMember States play a key role as the enforcement of the AI Act will often require a local presence. Member States must each designate at least one market surveillance authority and one, if there is more than one, of these authorities must be set as a single point of contact vis-à-vis the public and other counterparts at Member State and Union level.  The Member State shall notify the European Commission of the single point of contact and the European Commission will make a list of them available to the public (recital 153 and article 70(1/2)). The Member States have until 2 August 2025 to comply with these provisions (article 113(b)).\n\n## Which entities are to be designated market surveillance authorities?\n\nMember States have some flexibility in designating market surveillance authorities; they can either establish a new body dedicated to enforcing the AI Act or integrate the requirements of the AI Act into the framework of an existing body already responsible for market surveillance under the Union harmonisation laws listed in Section A of Annex I or the existing bodies regulating financial or credit institutions regulated by Union law (article 74(3/6/7)). However, for high-risk systems in the area of biometrics, law enforcement, migration, asylum and border control management and the administration of justice, Member States must designate either the national Data Protection Authority established by Regulation\n\n(EU) 2016/679 or the supervisory authority designated under Directive (EU) 2016/680 (article 74(8)).\n\nWhere AI systems relate to products already covered by the Union harmonisation legislation listed in Section A of Annex I and where such legal acts already provide for procedures ensuring an equivalent level of protection and having the same objective as the AI Act, the sectoral procedures shall apply instead of the national level enforcement procedures set out in articles 79 to 83 (see below under the heading 'Procedures for enforcement').\n\nIn this instance, dual reporting of serious incidents is not required and providers report under those other laws (article 73(9) and 73(10)). These exceptions specifically apply to:\n\n- Annex III-type high-risk AI systems, where the provider is subject to Union law that establishes reporting obligations equivalent to those set out in the AI Act. Such equivalence may - for example - exist for critical infrastructure, which is covered by cybersecurity regulations that contain standalone incident reporting obligations that might be considered equivalent to those under the AI Act. However, it may not always be clear whether reporting obligations under other Union laws are considered equivalent to the reporting obligations under the AI Act; and\n- high-risk AI systems that are safety components of devices, or are themselves devices, covered by Regulations (EU) 2017/745 on medical devices and (EU) 2017/746 on in vitro diagnostic medical devices. These both contain reporting obligations, according to which serious incidents must be reported to the competent authorities if they entail (a) the death of a patient, user or other person, (b) the temporary or permanent serious deterioration of a patient's, user's or other person's state of health, or (c) a serious public health threat.\n\nHowever, in both instances, if the infringement relates to a violation of fundamental rights, it must still be notified under the AI Act and the relevant market surveillance authority must inform the national fundamental rights authority/ authorities.\n\nFor AI systems used by Union institutions, agencies, offices, and bodies (with the exception of the Court of Justice of the European Union acting in its judicial capacity), the European Data Protection Supervisor will be the market surveillance authority (article 74(9)).\n\n<!-- image -->\n\n## Powers of the market surveillance authorities\n\nThe market surveillance authorities have all the broad enforcement powers set out in Regulation (EU) 2019/1020 in addition to further powers granted by the AI Act. For example, they have the power to:\n\n- make operators disclose relevant documents, data and information on compliance. The AI Act adds that providers of high-risk AI systems may be compelled to disclose:\n- -training, validation and testing data sets used for the development of high-risk AI systems, including, where appropriate and subject to security safeguards, through application programming interfaces (API) or other relevant technical means and tools enabling remote access (article 74(12)); and\n- -where the testing or auditing procedures and verifications based on the data and documentation provided by the provider have been exhausted or proved insufficient, the source code if it is necessary to assess the conformity of a high-risk AI system with the requirements set out in chapter III, Section 2 (article 74(13));\n- make unannounced on-site inspections and make test purchases (article 74(5));\n- conduct investigations (engaging with the European Commission where high-risk AI systems are found to present a serious risk across two or more Member States) (article 74(11));\n- require operators to take appropriate actions to bring instances of non-compliance to an end, both formal non-compliance (article 83) and to eliminate a risk (articles 79-82);\n- take appropriate measures where an operator fails to take corrective action or where the non-compliance persists, including withdrawal or recall (articles 73(8), 79-83); and\n- impose penalties (articles 99-101).\n\nThe market surveillance authorities shall also ensure that testing in real world conditions is in accordance with the AI Act (see Chapter 7). They have the power to require the provider or deployer to modify the testing or suspend or terminate it (article 76(3)).\n\n## Handling of confidential information\n\nAny information or documentation obtained by market surveillance authorities shall be treated in accordance with the confidentiality obligations set out in article 78.  The provisions in article 78 also apply to the European Commission, the authorities protecting fundamental rights and natural and legal persons involved in the application of the AI Act.  Such persons shall carry out their tasks in a manner which not only protects confidential information and trade secrets, but also protects intellectual property rights and the rights in source code, public and national security interests and classified information.\n\nThese provisions shall apply from 2 August 2025.\n\n## Procedures for enforcement\n\nAs already noted, the following procedures do not apply where there exists already harmonising legislation providing an equivalent level of protection and having the same objective as the AI Act.\n\n## AI systems presenting a risk (articles 79 and 81)\n\nWhere a market surveillance authority has sufficient reason to consider an AI system presents a risk (see definition above), it must carry out an evaluation as to whether the AI system is compliant with the AI Act.\n\nIf it does not comply, the market surveillance authority shall without undue delay notify the relevant operator and require them to take all appropriate corrective actions to bring the AI system into compliance or to withdraw the AI system from the market, or to recall it.  The market surveillance authority shall state how long the operator has to comply, but it will be no longer than 15 working days.\n\nIf operator does not take adequate corrective action by the end of the specified period, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system being made available on its national market or put into service, to withdraw the product or the standalone AI system from that market or to recall it.  The market surveillance authority must inform the operator of the grounds on which its decision is based.\n\n<!-- image -->\n\nWhere the non-compliance is not restricted to its national territory, the market surveillance authority shall inform the European Commission and the other Member States without undue delay of the results of the evaluation and of the actions which it has required the operator to take and the provisional measures which it has taken if the operator has not complied.\n\nThe provisional measures shall be deemed justified if no objection has been raised by either a market surveillance authority of a Member State or by the European Commission within three months (reduced to 30 days in the event of non-compliance with the prohibitions referred to in article 5). However, if objections are raised, the European Commission shall consult with the market surveillance authority and the operator or operators and, within six months (or 60 days for an article 5 issue), decide whether the provisional measure is justified.  If it is, all Member States shall ensure that they take appropriate restrictive measures in respect of the AI system concerned, such as requiring withdrawal from their market.  If it is not, the provisional measure will be withdrawn.\n\nThese provisions are without prejudice to the procedural rights of the operator set out in article 18 of Regulation (EU) 2019/1020, including the right to be heard.\n\n## AI systems classified by the provider as nonhigh-risk (article 80)\n\nIf the market surveillance authorities have sufficient reason to consider an AI system classified by the provider as non-high-risk under article 6(3) is indeed high-risk, it must carry out an evaluation.\n\nThe procedure to be followed is very much as described above, but article 80 specifically refers to the ability to fine the relevant provider.\n\nIn exercising their power to monitor the application of article 80, market surveillance authorities may take into account the information stored in the EU database of high-risk AI systems (see below under the heading 'Governance at Union Level: Role of the European Commission').\n\n## Compliant AI systems which present a risk (article 82)\n\nIf a market surveillance authority finds that a high-risk AI system complies with the AI Act, but it nevertheless presents a risk to the health or safety of persons, to fundamental rights, or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that it no longer does so.\n\n## Formal non-compliance (article 83)\n\nWhere a market surveillance authority finds that, for example, a CE marking has not been affixed where it should, no authorised representative has been appointed or technical documentation is not available, it shall require the relevant provider to correct the matter within a prescribed period.\n\nIf the non-compliance persists, then the market surveillance authority shall take appropriate and proportionate measures to restrict or prohibit the high-risk AI system being made available on the market or to ensure that it is recalled or withdrawn from the market without delay.\n\n## Authorities protecting fundamental rights\n\nIn addition to identifying market surveillance authorities, by 2 November 2024, each Member State must identify the public authorities or bodies supervising and enforcing the obligations under Union law protecting fundamental rights, including the right to non-discrimination, in relation to the use of high-risk AI systems referred to in Annex III and shall notify them to the European Commission.\n\nWhere market surveillance authorities identify risks to fundamental rights they must notify the relevant national public authority supervising their protection.\n\nThese bodies have the power to request and access any documentation created or maintained under the AI Act when access to that documentation is necessary for effectively fulfilling their mandates. The relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request and, where the documentation proves insufficient may request\n\n<!-- image -->\n\nthe market surveillance authority to organise testing of the high-risk AI system through technical means (article 77).\n\n## General-purpose AI models\n\nThe European Commission is the sole authority responsible for supervising and enforcing obligations on providers of general-purpose AI models. The rationale behind this is to benefit from centralised expertise and synergies at Union level (article 88).  In practice, however, the AI Office (see below under the heading 'Governance') will carry out all necessary actions to monitor the effective implementation of the AI Act with regard to general-purpose AI models, provided that the organisational powers of the European Commission and the division of competences between Member States and the Union are not affected.\n\nThe AI Office can investigate possible breaches of the rules by providers of general-purpose AI models on its own initiative, following the results of its monitoring activities, or at a request from market surveillance authorities.\n\nIt has the powers of a market surveillance authority for AI systems which are based on a general-purpose AI model, where the model and system are developed by the same provider.\n\nMarket surveillance authorities must cooperate with the AI Office to carry out compliance evaluations if a market surveillance authority considers that a general-purpose AI system (that can be used by deployers for at least one highrisk purpose) is non-compliant with the AI Act.\n\nMarket surveillance authorities can request the AI Office to provide information related to generalpurpose AI models, where the market surveillance authority is unable to access that information (and as a result is unable to conclude its investigation into a high-risk system) (article 75).\n\n## Penalties\n\nAny person, which fails to comply with the AI Act - whether a natural or legal person, a public authority or an EU or national institution - can be sanctioned for non-compliance. The provisions on penalties under the AI Act exceed even those provided for in the GDPR (which are up to EUR\n\n20,000,000 or 4% of annual worldwide turnover). The maximum fine was revised throughout the legislative process but was ultimately set at EUR 35,000,000 or 7% of annual worldwide turnover.\n\nFines can be imposed by national authorities, the European Data Protection Supervisor, or the European Commission.  The European Data\n\nProtection Supervisor can impose fines on Union institutions, agencies and bodies. The European Commission can impose fines on providers of general-purpose AI models.  National authorities can impose fines on other operators.\n\nThe AI Act has a tiered approach to penalties, as shown below.\n\n| Grounds of infringement                                                                                         | EU bodies                     | All other persons                                                                                                                                                                                                                                                  |\n|-----------------------------------------------------------------------------------------------------------------|-------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                                                                                                 | Penalties imposed by EDPS     | Penalties imposed by national authorities (unless GPAI models, in which case imposed by the European Commission).                                                                                                                                                  |\n|                                                                                                                 |                               | For sanctioned persons which are undertakings, the penalties are capped at the higher of the %-based amount or the figure below. If the undertaking is an SME, they are capped at the lower amount. For other sanctioned persons, the specified figure is the cap. |\n| Supplying incorrect, incomplete or misleading information to notified bodies or national competent authorities. | ≤ €750,000 (article 100(3))   | ≤ 1% total worldwide annual turnover in preceding year; or ≤ €7,500,000 (article 99(5))                                                                                                                                                                            |\n| Obligations relating to high-risk AI systems. Obligations relating to providers of general -purpose AI models.  | ≤ €750,000 (article 100(3))   | ≤ 3% of total worldwide annual turnover in preceding year; or ≤ €15,000,000 (article 99(4) for high-risk AI systems; article 101(1) for general-purpose AI models)                                                                                                 |\n| Obligations relating to prohibited practices.                                                                   | ≤ €1,500,000 (article 100(2)) | ≤ 7% of total worldwide annual turnover in preceding year; or ≤ €35,000,000 (article 99(3))                                                                                                                                                                        |\n\nCuriously, there appear to be no penalties for failure to comply with the AI literacy obligations at article 4.\n\n## Penalties and fines imposed by national authorities\n\nIt is the responsibility of Member States to provide for effective, proportionate, and dissuasive sanctions.  These measures may include both monetary and non-monetary\n\n<!-- image -->\n\nmeasures or warnings.  They must be notified to the European Commission by the date of entry into application (article 99(1/2)).\n\nPenalties are to be imposed on a case-by-case basis.  The competent national authority should consider all relevant circumstances of the specific situation, with due regard to the nature, gravity, and duration of the infringement and its consequences, as well as the size of the provider (article 99(7)).\n\nEnforcement at Member State level must be subject to appropriate procedural safeguards, including effective judicial remedies.\n\n## Fines on Union institutions, bodies, offices and agencies\n\nThe European Data Protection Supervisor has the power to impose fines on Union institutions, agencies and bodies.  Before adopting a decision on a fine, the EDPS should communicate its preliminary findings to the Union institution and give it an opportunity to be heard. The fine is not to affect the effective operation of the institution and the funds collected by the imposition of fines are to be contributed to the general budget of the EU.\n\n## Fines on providers of general-purpose AI models\n\nThe European Commission may impose fines on providers of general-purpose AI models for infringements (article 101).  Unlike the other provisions on penalties and fines in chapter XII, which apply from 2 August 2025, article 101 does not apply until 2 August 2026.\n\nThe European Commission will publish an implementing act with details on arrangements and procedural safeguards for proceedings.\n\nWhen imposing a fixed amount or periodic penalty payment, the European Commission should take due account of the nature, gravity and duration of the infringement, and the principles of proportionality and appropriateness. Before adopting a decision on a fine, the European Commission should communicate its preliminary findings to the provider of the general-purpose AI model and give it an opportunity to be heard. The imposition of a fine must be subject to appropriate procedural safeguards, including judicial review before the Court of Justice of the European Union.  The CJEU may cancel, reduce or increase the amount of a fine imposed.\n\n<!-- image -->\n\n## Remedies for third parties\n\n## Complaint to a market surveillance authority (article 85)\n\nUnion and Member State law already provide some effective remedies for natural and legal persons whose rights and freedoms are adversely affected by the use of AI systems. Notwithstanding, the AI Act introduces a new complaints mechanism.  It mandates that any natural or legal person may submit a complaint to the competent market surveillance authority if it has grounds for believing there has been an infringement of the AI Act.\n\nCompare: Under the GDPR, a data subject has the right to lodge a complaint with a supervisory authority about an alleged infringement if the data subject believes that the processing of personal data relating to him or her violates rights under the GDPR.\n\nIn contrast, a complaint lodged under the AI Act may concern not only an infringement of the rights of the complainant, but also compliance with the AI Act as a whole.  In addition, under the GDPR a remedy can be filed only by the data subjects; under the AI Act, a complaint can also be filed by a legal person.\n\n## Right to explanation of individual decisionmaking (article 86)\n\nUnder the AI Act, any affected person is entitled to receive 'clear and meaningful' explanations from the deployer concerning decisions made by high-risk AI systems (except for critical infrastructure systems).  These explanations must clarify the decision-making procedure used and the main elements of the decision made by the AI system (article 86).\n\nThe right can be invoked if:\n\n- a deployer's decision is mainly based on the output of high-risk AI systems; and\n- that decision has legal effects or similarly significant effects on an affected person that adversely affect his or her health, safety or fundamental rights.\n\nCompare: The right to an explanation under the AI Act aligns with a controller's obligation under the GDPR concerning automated decisionmaking processes (article 22 GDPR).  Under the GDPR, the controller must provide the data\n\nsubject with meaningful information on the logic and significance of the consequences of such processing.\n\nArticle 86 of the AI Act complements the data subject's right to an explanation under the GDPR; it is more specific to AI as it requires the deployer to explain the role of the AI system in the decision.  In addition, the AI Act grants this right to all affected persons who can also be legal persons.  National data protection authorities under the GDPR are still the competent authorities to enforce the controller's obligation to provide information when it comes to automated decision-making involving personal data processing, regardless of what authority is competent to enforce article 86 of the AI Act.\n\n## Protection for whistleblowers (article 87)\n\nDirective (EU) 2019/1937 on the protection of persons who report breaches of Union law applies to the reporting of infringements of the AI Act.\n\n## Downstream providers' complaint (article 89)\n\nThe AI Act enables complaints by downstream providers (deployers of general-purpose AI systems) about possible violations of the rules set out in the Act.\n\nComplaints can be made to the AI Office and must be well-substantiated. They should include at least:\n\n- details of the provider of the general-purpose AI model that is the subject of the complaint, and its point of contact;\n- a description of the relevant facts, together with the provisions that have been breached;\n- the reasons why the complainant believes there has been an infringement; and\n- any other information that the requesting downstream provider deems relevant, including, where appropriate, information gathered at its own initiative.\n\nThe possibility for downstream providers to make such complaints enables the AI Office to effectively oversee the enforcement of the AI Act.\n\n<!-- image -->\n\n## Governance\n\nThe governance structure has been established to coordinate and support the application of the AI Act. Its aim is to build capabilities at both Union and national levels, integrate stakeholders, and ensure trustworthy and constructive cooperation.\n\n## Governance at Union Level: role of the European Commission\n\nThe European Commission is tasked by the AI Act with many responsibilities including developing and implementing delegated acts, developing and publishing guidelines, setting standards and best practice and making binding decisions to implement the AI Act effectively.  In practice, these tasks will be carried out by the AI Office (part of the administrative structure of the Directorate-General for Communication Networks, Content and Technology) in its role of supporting the European Commission.\n\nOne of the tasks that the European Commission, in collaboration with the Member States, must perform is set out in chapter VIII of the AI Act. The European Commission must set up and maintain an EU database for high-risk AI systems referred to in article 6(2) and AI systems that are not considered as high-risk pursuant to article 6(3).  The database will contain:\n\n- the data listed in Sections A and B of Annex VIII entered into the EU database by the provider or the authorised representative; and\n- the data listed in Section C of Annex VIII entered into the EU database by the deployer who is, or who acts on behalf of, a public authority, agency or body.\n\nThe data will be available to the public (with the exception of data relating to AI systems in the areas of law enforcement, migration, asylum and boarder control management).\n\n## The supranational bodies set up by the AI Act\n\n| Role of the AI Office                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | Actions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| The AI Office was established by the European Commission by its decision of 24 January 2024 (C/2024/1459). The AI Office's function is to oversee the advancements in AI models, including as regards general-purpose AI models, the interaction with the scientific community, and to play a key role in investigations and testing, enforcement and to have a global vocation (recital 5 of the decision). The AI Office may involve independent experts to carry out evaluations on its behalf. The AI Office must establish systems and procedures to manage and prevent potential conflicts of interest and must develop Union expertise and capabilities in the field of AI. The AI Office has a role in the surveillance and control of general-purpose AI systems (article 75). | Monitoring and enforcement: Monitor compliance and implementation of obligations for providers of general-purpose AI models. Investigation: Investigate infringements by requesting documentation and information, conducting evaluations and requesting measures from providers of general-purpose AI models. Risk management: Request appropriate measures, including risk mitigation, in cases of identified systemic risks, as well as restricting market availability, withdrawing or recalling the model. Coordination and support: Support national authorities in creating AI regulatory sandboxes and facilitate cooperation and information- sharing and encourage and facilitate the creation of codes of conduct. Coordinate joint investigations by market surveillance authorities and the European Commission. Advice: Issue recommendations and written opinions to the European Commission and the Board regarding codes of conduct, codes of practice and guidelines. |\n\n## Role of the European Artificial Intelligence Actions\n\n## Board (The Board)\n\nThe Board comprises representatives from each Member State and is tasked with advising and assisting the European Commission and the Member States on the consistent and effective application of the AI Act. Additionally, the Board issues guidelines and recommendations (articles 65 and 66).\n\nRepresentatives are appointed for a term of three years, renewable once. They may be individuals from public entities with expertise in AI and the authority to facilitate national-level coordination. The Board is chaired by one of its representatives.\n\n<!-- image -->\n\nCoordination and cooperation: Among national competent authorities and Union institutions, bodies, offices and agencies, as well as relevant Union expert groups and networks.\n\nExpertise sharing: Collect and share technical and regulatory expertise, best practices and guidance documents.\n\nAdvice and recommendations: Provide advice on the implementation of the AI Act, in particular as regards the enforcement of rules on general-purpose AI models, issue recommendations and written opinions (at the request of the European Commission or on its own initiative).\n\n## Role of the European Artificial Intelligence Actions\n\n## Board (The Board)\n\nThe Board must establish two dedicated standing subgroups:\n\n- The standing subgroup for notifying authorities provides a platform for cooperation and exchange on issues related to notified bodies\n- The standing subgroup for market surveillance acts as the administrative cooperation group (ADCO) for the AI Act.\n\nThe Board may establish other standing or temporary subgroups as appropriate for the purpose of examining specific issues.\n\nThe European Data Protection Supervisor and the AI Office attend the Board's meetings as observers. Other national and Union authorities, bodies, or experts or representatives of the advisory forum may be invited on a case-by-case basis.\n\n## Role of the Advisory Forum\n\nThe Advisory Forum has been created to ensure the involvement of stakeholders in the implementation and application of the AI Act (article 67).\n\nMembers are appointed by the European Commission and represent a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society, and academia with recognised expertise in the field of AI.\n\nMembers are appointed for a term of two years, which may be extended up to four years. They elect two co-chairs from among the members for a term of two years, renewable once.\n\nThe Fundamental Rights Agency (FRA), the European Union Agency for Cybersecurity (ENISA), the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC), and the European Telecommunications Standards Institute (ETSI) shall be permanent members of the Advisory Forum.\n\nThe Advisory Forum may establish standing or temporary sub-groups as appropriate for examining specific questions.\n\nThe Advisory Forum meets at least twice a year and may invite experts and other stakeholders to its meetings.\n\n<!-- image -->\n\nHarmonisation: Standardise administrative practices and facilitate the development of common criteria and a shared understanding.\n\nPublic awareness on AI: Work towards AI literacy, public awareness and understanding of the benefits, risks, safeguards and rights and obligations in relation to the use of AI systems.\n\nInternational Cooperation: Advise the European Commission in relation to international matters on AI and cooperate with competent authorities of third countries and with international organisations.\n\n## Actions\n\nAdvice and technical expertise: Provide advice to the Board and the European Commission. Prepare opinions, recommendations, and written contributions upon request.\n\nConsultancy group: The European Commission has to consult the Forum when preparing a standardisation request or drafting common specifications as referred to in article 41.\n\nAnnual report: Prepare and publish an annual report on its activities.\n\n| Role of the scientific panel of independent expert                                                                                                                                                                                       | Actions                                                                                                                                                                                           |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| The scientific panel is created to integrate the scientific community in supporting the European Commission's enforcement activities (article 68).                                                                                       | Support the AI Office in the implementation and enforcement as regards general-purpose AI models and system:                                                                                      |\n| Experts are selected by the European Commission based on their current scientific or technical expertise in AI.                                                                                                                          | • Develop tools and methodologies for evaluating capabilities.                                                                                                                                    |\n| The number of experts is determined by the European Commission, in consultation with the Board, based on the required expertise needs, ensuring fair gender and geographical representation.                                             | • Advise on the classification including systemic risk. • Contribute to the development of tools and templates.                                                                                   |\n| To provide the scientific panel with the necessary information for performing its tasks, a mechanism should be established allowing the panel to request the European Commission to obtain documentation or information from a provider. | • Support market surveillance authorities: At their request including with regard to cross- border market surveillance activities. • Assist in the Union safeguard procedure pursuant article 81. |\n| An implementing act will define how the scientific panel and its members can issue alerts and request assistance from the AI Office.                                                                                                     | Support Member States with their enforcement activities upon demand: • Member States may be required to pay fees for the advice and support provided by the scientific panel.                     |\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Governance at national level: national competent authorities\n\nMember States play a crucial role in the application and enforcement of the AI Act. To ensure effective application, harmonisation, and coordination within the Union and among\n\n## Role of the notifying authority(ies)\n\nThis authority is responsible for establishing and applying the framework for conformity assessment bodies (article 28).\n\nThe authority must have an adequate number of competent personnel with the necessary expertise in fields such as information technology, AI, and law, including the supervision of fundamental rights.\n\nNotifying authorities must avoid any conflict of interest with conformity assessment bodies, ensuring the objectivity and impartiality of their activities. In particular, the decision to notify  a conformity assessment body must not be made by the  person  who assessed  the conformity assessment body.\n\n<!-- image -->\n\nMember States, each Member State must designate at least one notifying authority and one market surveillance authority. Together, they constitute the national competent authorities.  For AI systems used by Union institutions, agencies, offices, and bodies, the European Data Protection Supervisor will be the supervisory authority.\n\n## Actions\n\nSetting up and carrying out procedures: Establish and execute necessary procedures for the assessment, designation, notification, and monitoring of conformity assessment bodies. Develop these procedures in cooperation with the notifying authorities of other Member States.\n\nAdvice and guidance: Provide guidance and advice on the implementation of the AI Act, considering the input from the Board and the European Commission, and consulting national competent authorities under other Union laws, if applicable.\n\nActivity and service restrictions:\n\n- Must not offer or provide any activities performed by conformity assessment bodies.\n- Must not offer consultancy services on a commercial or competitive basis.\n\n## Role of the market surveillance authority(ies)\n\nResponsible for carrying out the activities and taking the measures pursuant to Regulation (EU) 2019/1020 (market surveillance and compliance of products) on market surveillance and compliance of products.\n\nOne of the market surveillance authorities will be designated by each Member State as the single point of contact for the public and other counterparts at both Member State and Union levels.\n\nThe European Data Protection Supervisor will act as the market surveillance authority for Union institutions, agencies, and bodies under the AI Act.\n\nMarket surveillance authorities for highrisk AI systems in biometrics, used for law enforcement, migration, asylum, border control, justice, and democratic processes, should have strong investigative and corrective powers. This includes access to all personal data and necessary information for their task.\n\nMember States must facilitate coordination between market surveillance authorities and other relevant national authorities.\n\n<!-- image -->\n\n## Where can I find this?\n\nGovernance: Chapter VII\n\nEU Database: Chapter VIII\n\nEnforcement: Chapters IX and XII\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n4\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Actions\n\nMany of task and responsibilities of the market surveillance authorities are described above, but in addition they have the following tasks and responsibilities assigned to them:\n\n- Authorisation for high-risk AI systems: Member States can temporarily authorise specific high-risk AI systems to be placed on the market or put into service in their territory for exceptional reasons of public security, health, environmental protection, or key infrastructure, pending conformity assessments (article 46).\n- Annual reporting: to the European Commission and national competition authorities on surveillance activities and prohibited practices including: (i) any information identified that is of potential interest for the application of competition law; (ii) use of any prohibited practices; and (iii) measures taken in relation to those practices.\n- Advice and guidance: Provide guidance and advice on the implementation of the AI Act, considering the input from the Board and the European Commission, and consulting national competent authorities under other Union laws, if applicable.\n\nrecitals 148-154, 163,179\n\nrecital 131\n\nrecitals 162-164 and 168-172\n\n## AI Act: What's Next\n\n<!-- image -->\n\n- The AI Act entered into force on 1 August 2024.\n- Most provisions are set to apply from 2 August 2026, and others are being phased in over a period of six to 36 months from the date of entry into force.\n- The European Commission will develop delegated and implementing acts, guidelines, codes of conduct and standards. These initiatives are aimed at providing practical guidance, ethical principles and technical specifications related to the AI Act, with the goal of ensuring the effective implementation of the legislation.\n- The Commission also sent, in July 2024, an updated version of its proposed AI Liability Directive to both the European Parliament and the Council for consideration.\n- Bird &amp; Bird's AI experts are equipped to monitor the forthcoming initiatives expected under AI Act and help you navigate the different processes and requirements.\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nAll actors dealing with AI systems should actively monitor the development of the legislative and non-legislative initiatives outlined in this chapter.\n\n## AI Act: What's Next\n\nThis chapter provides an overview of the application deadlines of the AI Act and the forthcoming initiatives expected under the Regulation. The EU institutions regard the AI Act as a new form of 'living regulation' that will be supplemented on an ongoing basis via secondary legislation and other initiatives, in an effort to keep pace with technological advances. Over the coming months, the AI Act envisions the adoption of a range of delegated and implementing acts, guidance documents, codes of conduct, codes of practice and standardisation requests. These initiatives are designed to provide practical guidance, ethical principles and technical specifications regarding the Regulation, with the aim of ensuring effective implementation.\n\nThe requirements laid down in such documents will greatly shape the effective implementation of the AI Act and the ability of actors to comply with its obligations.\n\nAll actors dealing with AI systems would therefore be advised to actively monitor the work of the Commission in developing the legislative and nonlegislative initiatives mentioned in this chapter.\n\nBird &amp; Bird's Regulatory and Public Affairs team is equipped to monitor the forthcoming initiatives expected under AI Act and help you navigate the different processes and requirements.\n\n## AI Act application deadlines\n\nFollowing its publication in the EU Official Journal 1 on 12 July 2024, the AI Act entered into force on 1 August 2024.\n\nThe relevant dates of application are set out below.\n\n| 12 July 2024    | The AI Act was published in the Official Journal of the EU, triggering the dates for specific provisions in the Regulation becoming applicable.                                                                                                      |\n|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2 February 2025 | Prohibited practices ban applies (Chapter II). AI literacy rules apply (article 4).                                                                                                                                                                  |\n| 2 May 2025      | Codes of practice for general-purpose AI must be ready (article 56 (9)).                                                                                                                                                                             |\n| 2 August 2025   | National authorities designated (Chapter III Section 4). Obligations for General-Purpose AI (GPAI) (Chapter V). Governance (at EU and national level) (Chapter VII). Confidentiality and penalties (other than in relation to gen-AI) (Chapter XII). |\n| 2 August 2026   | Start of application of all other provisions of the EU AI Act (unless a later date applies below).                                                                                                                                                   |\n\n<!-- image -->\n\n| 2 August 2027    | High-risk categories listed in Annex I). General purpose AI models placed on the market before 2 August 2025 (article 111).                                                                                |\n|------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 2 August 2030    | High-risk AI systems (other than those listed below), which have been placed on the market or put into service before 2 August 2026 and which are intended to be used by public authorities (article 111). |\n| 31 December 2030 | Components of large-scale IT systems listed in Annex X, which have been placed on the market or put into service before 2 August 2027 (article 111).                                                       |\n\nBetween 1 August 2024 and 2 August 2027, the European Commission is expected to adopt various documents to implement the Regulation. These comprise delegated and implementing acts, guidance documents, codes of conduct, codes of practice and standardisation requests. Apart from a few exceptions, there are no specific set deadlines for the publication of these initiatives by the Commission. Nonetheless, it is assumed that the Commission will aim to adopt such documents ahead of the application deadlines of the respective provisions.\n\n## Delegated acts\n\nSeveral provisions will be the subject of delegated acts, to be adopted by the Commission to specify obligations and operational implementation. Article 97 grants the power to adopt delegated acts to the Commission for a five-year period that started on 1 August 2024. The Commission must report on this delegation nine months before the end of the period. This period is automatically extended for another five years unless the European Parliament or the Council opposes it three months before the end of each period.\n\nAs mentioned above, there are no specific set deadlines for the adoption of such delegated acts. However, it must be presumed that their adoption will precede the application deadlines for the related provisions in the AI Act (see article 113).\n\nPursuant to article 97(4), before adopting a delegated act, the Commission will have to carry out public consultations during its preparatory work and will also consult with the relevant Expert Groups (composed of Member States experts).\n\nOnce adopted, the Commission must notify the European Parliament and the Council simultaneously. A delegated act only enters into force if neither the European Parliament nor the Council objects within three months of notification, extendable by another three months if needed. The European Parliament or the Council can revoke this power at any time, but this will not affect the validity of existing delegated acts. In accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making 2 , the Commission will have to ensure that the European Parliament and the Council receive all documents at the same time as Member States' experts, and the Parliament and Council's experts should systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts.\n\nThe AI Act foresees the adoption of the following delegated acts where the Commission considers this to be necessary:\n\n- Article 6(6/7) : amend article 6(3) by adding new conditions to those laid down in paragraph 3, by modifying or by deleting them if there is concrete and reliable evidence of the existence of AI systems that should not fall under Annex III or that should not fall under the conditions of article 6(3);\n\n2.  Inter-institutional  Agreement between the European Parliament, the Council of the European Union and the European Commission on Better Law-Making, OJ L 123, 12.5.2016.\n\n<!-- image -->\n\n- Article 7(1/3) : amend Annex III, by adding, modifying or removing use-cases of high-risk AI systems;\n- Article 11(3) : amend Annex IV, where necessary, to ensure that, in light of technical progress, the technical documentation provides all the information necessary to assess the compliance of the system;\n- Article 43(5) : amend Annexes VI and VII by updating them in light of technical progress;\n- Article 45(6) : amend article 43(1/2) in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to third-party conformity assessments;\n- Article 47(5) : amend Annex V by updating the content of the EU declaration of conformity set out in that Annex, in order to introduce elements that become necessary in light of technical progress;\n- Article 51(3) : amend the thresholds for systemic general-purpose AI models listed in article 51(1/2) as well as to supplement benchmarks and indicators in light of evolving technological developments, such as algorithmic improvements or increased hardware efficiency, when necessary, for these thresholds to reflect the state of the art;\n- Article 52(4) : amend Annex XIII by specifying and updating the criteria for systemic general-purpose AI models;\n- Article 53(5) : detail measurement and calculation methodologies with a view to allowing for comparable and verifiable documentation to facilitate compliance with Annex XI; and\n- Article 53(6) : amend Annexes XI and XII in light of evolving technological development.\n\n## Implementing acts\n\nArticle 98(2) of the AI Act confers on the European Commission the power to adopt implementing acts in accordance with Regulation 182/2011 3 . Implementing acts aim to create uniform conditions for the implementation of a specific legislative act, if and when this is necessary. With respect to the drafting of the implementing acts, the Commission will be assisted by a 'Comitology' Committee comprising Member State experts.\n\nAs is the case for delegated acts, the timeline for adoption of the expected implementing acts is not specified in the text, except for the foreseen implementing act referred to in article 72(3), which is due by 2 February 2026. Therefore, it should be presumed that the relevant implementing acts will be adopted ahead of the application deadlines for the related provisions in the AI Act (see above and article 113).\n\nThe AI Act foresees the adoption of the following implementing acts, where the Commission deems it necessary to:\n\n- Article 37(2) : suspend, restrict or withdraw the designation of notified bodies when the Member State fails to take the necessary corrective measures;\n- Article 41(1/4/6) : establish, in consultation with the 'Advisory Forum' referred to in article 67, common specifications for the requirements for high-risk AI systems or for the obligations for general-purpose AI models set out in Chapter V, Sections 2 and 3. When a reference to a harmonised standard is published in the Official Journal of the European Union, which covers the same requirements set out in Section 2 of this Chapter III, the Commission shall repeal the implementing act referred to in article 41(1). Where a Member State considers that a common specification does not entirely meet the requirements set out in Section 2 of this Chapter III, the Commission shall assess that information and, if appropriate, amend the implementing act referred to in article 41(1);\n- Article 50(7) : approve codes of practice drawn up to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content, in accordance with the procedure laid down in article 56(6). If the code of practice is not adequate, the Commission may adopt an implementing act to lay down a set of common rules for the implementation of the transparency\n\n3.  Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 February 2011 laying down the rules and general principles concerning mechanisms for control by Member States of the Commission's exercise of implementing powers, OJ L 55, 28.2.2011.\n\n<!-- image -->\n\nobligations for providers and deployers of certain AI systems of article 50;\n\n- Article 56(6) : approve a code of practice for general-purpose AI models and give it a general validity within the Union. If, by 2 August 2025, a code of practice cannot be finalised, or if the AI Office deems it is not adequate, the Commission may provide, by means of implementing acts, common rules for the implementation of the obligations provided for in articles 53 and 55, including the issues set out in article 56(2);\n- Article 58(1) : specify the detailed arrangements for the establishment, development, implementation, operation and supervision of the AI regulatory sandboxes;\n- Article 60(1) : specify the detailed elements of the real-world testing plan for providers of high-risk AI systems;\n- Article 68(1) : make provisions on the establishment of a scientific panel of independent experts (the 'scientific panel' ) intended to support the enforcement activities of the AI Act;\n- Article 72(3) : publish, by 2 February 2026, an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan for providers of high-risk AI systems and the list of elements to be included in the plan;\n- Article 92(6) : set out the detailed arrangements and the conditions for the AI Office of general-purpose AI models evaluations, including the detailed arrangements for involving independent experts, and the procedure for the selection thereof; and\n- Article 101(6) : lay down detailed arrangements and procedural safeguards for proceedings in view of the possible fines on providers of general-purpose AI models.\n\n## Commission Guidelines\n\n'Commission Guidelines' are explanatory documents produced by the Commission services to provide practical and informal guidance about how particular provisions of the AI Act should be applied.\n\n<!-- image -->\n\nThe AI Act foresees the adoption of the following Commission Guidelines:\n\n- Article 6(5) : after consulting the European Artificial Intelligence Board, and no later than 2 February 2026, specifying the practical implementation of article 6, including a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk;\n- Article 63(1) : on the elements of the quality management system which may be complied with in a simplified manner considering the needs of microenterprises, without affecting the level of protection or the need for compliance with the requirements in respect of high-risk AI systems (no set deadline for these guidelines);\n- Article 73(7) : to facilitate compliance with the reporting obligations of serious incident. The guidance has to be adopted by 2 August 2025, and will have to be assessed regularly by the Commission;\n- Article 96 : on the practical implementation of this Regulation. There is no set deadline for the development of these guidelines. However, the related provisions apply from 2 August 2026. In particular, the Commission is to develop guidelines on:\n- -the application of the requirements and obligations referred to in articles 8 to 15 and in article 25;\n- -the prohibited practices referred to in article 5;\n- -the practical implementation of the provisions related to substantial modification;\n- -the practical implementation of transparency obligations laid down in article 50;\n- -detailed information on the relationship of the AI Act with the EU harmonisation legislation listed in Annex I, as well as with other relevant EU laws, including as regards consistency in their enforcement; and\n- -the application of the definition of an AI system as set out in article 3, point (1).\n\n## Codes of conduct and practice\n\n## Codes of conduct\n\nCodes of conduct are documents of a voluntary nature that establish ethical guidelines and principles for the development and use of AI in certain conditions. They are also intended to foster the development of AI policies within organisations for the voluntary application of specific AI Act obligations.\n\nThe AI Act calls for the adoption of the following codes of conduct:\n\n- Recital 20 and article 4 : voluntary codes of conduct to advance AI literacy among persons dealing with the development, operation and use of AI.\n- -While there is no set deadline for the development of voluntary codes of practice to advance AI literacy, the related provisions on AI literacy in Article 4 will apply from 2 February 2025.\n- Recital 165 and article 95 : codes of conduct intended to foster the voluntary application to AI systems of some or all the mandatory requirements applicable to high-risk AI systems. These are adapted in light of the intended purpose of the systems and the lower risk involved, and take into account the available technical solutions and industry best practices such as model and data cards:\n- -to ensure that the voluntary codes of conduct are effective, they should be based on clear objectives and key performance indicators to measure the achievement of those objectives;\n- -they should also be developed in an inclusive way, as appropriate, with the involvement of relevant stakeholders such as business and civil society organisations, academia, research organisations, trade unions and consumer protection organisations; and\n- -while there is no set deadline for the development of voluntary codes of practice intended to foster the application to AI systems of some or all the mandatory requirements applicable to high-risk AI systems, the related provisions included in Article 95 will apply from 2 February 2026. By 2 August 2028 and every three years thereafter, the Commission is due to\n\n<!-- image -->\n\nevaluate the impact and effectiveness of such voluntary codes of conduct.\n\n## Codes of practice\n\nCodes of practice represent a central tool for proper compliance with specific obligations under the AI Act. In particular, one code of practice will detail the AI Act rules for providers of general-purpose AI models and generalpurpose AI models with systemic risks. Another code of practice will focus on the detection and labelling of artificially generated or manipulated content. Organisations should be able to rely on codes of practice to demonstrate compliance with the relevant obligations, which is known as a 'presumption of conformity' .\n\nSpecifically, the AI Act calls on the European Commission's AI Office to facilitate the drawing up of the following codes of practice together with all interested stakeholders:\n\n- Article 50(7): codes of practice at EU level to facilitate the effective implementation of the obligations in article 50(2/4), regarding the detection and labelling of artificially generated or manipulated content. The Commission may adopt implementing acts to approve those codes of practice. While there is no set deadline for the development of voluntary codes of practice to facilitate the effective implementation of the obligations in article 50(2/4), the related provisions included in Article 50 will apply from 2 February 2026.\n- Article 56(1/3) : by 2 May 2025, codes of practice for general-purpose AI models. These will duly take into account international approaches as well as a diverse set of perspectives, by collaborating with relevant national competent authorities and, where appropriate, by consulting with civil society organisations and other relevant stakeholders and experts. These include the 'Scientific Panel' of independent experts established under the AI Act.\n\nBy 2 August 2028 and every three years thereafter, the Commission will have to evaluate the impact and effectiveness of voluntary codes of practice.\n\nOn 30 July 2024, the European AI Office opened a call for expressions of interest to participate in the drawing-up of the first general-purpose AI Code of Practice. Interested parties could express\n\ntheir interest in participating by 25 August 2024. According to the Commission, this Code will be prepared by means of an iterative drafting process by April 2025, nine months from the AI Act's entry into force on 1 August 2024. The Code of Practice will facilitate the proper application of the rules of the AI Act for general-purpose AI models.\n\nThe Commission may decide to approve the Code of Practice and give it a general validity within the European Union by means of an implementing act, pursuant to article 56(6).  If the Code of Practice is not deemed adequate, the Commission will provide common rules for the implementation of the relevant obligations.\n\nIn addition, on 30 July 2024, the AI Office launched a consultation on trustworthy generalpurpose AI models under the AI Act, specifically regarding the template for the summary of the content used for the training of the generalpurpose AI models and the accompanying guidance. The deadline for responses was 10 September 2024.\n\n## Standards\n\n## Initial standardisation work\n\nThe process of drafting European standards in support of the AI Act started well before the adoption of the AI Act, with the Commission's proposal on harmonised rules on artificial intelligence adopted as the Commission Implementing Decision C(2023)3215 on 22 May 2023.\n\nThis Implementing Decision requested the European Committee for Standardisation (CEN) and the European Committee for Electrotechnical Standardisation (CENELEC) to draft the following new European standards or European standardisation deliverables on AI by 30 April 2025:\n\n- European standard(s) and/or European standardisation deliverable(s) on risk management systems for AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on governance and quality of datasets used to build AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on record keeping through logging capabilities by AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on transparency and information provisions for users of AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on human oversight of AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on accuracy specifications for AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on robustness specifications for AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on cybersecurity specifications for AI systems;\n- European standard(s) and/or European standardisation deliverable(s) on quality management systems for providers of AI systems, including post-market monitoring processes; and\n- European standard(s) and/or European standardisation deliverable(s) on conformity assessment for AI systems.\n\n<!-- image -->\n\nThis standardisation request to CEN and CENELEC was made pursuant to action 63 of the European Commission 2022 'Annual Union Work Programme for European standardisation' with the aim of ensuring that AI systems are safe and trustworthy.\n\nFor the drafting of these standards, CEN and CENELEC have set up a specific joint technical committee named 'CEN-CENELEC JTC 21 Artificial Intelligence' . CEN and CENELEC are also collaborating on the drafting with the European Telecommunications Standards Institute (ETSI) , an independent, not-for-profit, standardisation organisation in the field of information and communication.\n\n## AI Act standardisation request\n\nArticle 40(2) of the AI Act calls on the European Commission to present, without undue delay after the entry into force of the Regulation , standardisation requests for harmonised EU AI standards covering:\n\n- all requirements set out in Section 2 of Chapter III of the AI Act; and\n\n- as applicable, standardisation requests covering obligations set out in Chapter V, Sections 2 and 3 of the AI Act.\n\nThese requests revise the requests included in Commission Implementing Decision C(2023)3215 . This was also anticipated in the Commission's Standardisation Work Programme for 2024 published in February 2024. Indeed, Action 15 of the Work Programme calls for a 'revision of the standardisation request in support of Union policy on artificial intelligence' , thereby calling for the revision of the Commission Decision in view of the final AI Act text.\n\nAccording to article 40(2) of the AI Act, the standardisation requests should also ask for deliverables on reporting and documentation processes to improve AI systems' resource performance. Such requests could include reducing the consumption of energy and of other resources by high-risk AI systems during their lifecycle and the energy-efficient development of general-purpose AI models. The Commission should draft the requests after consulting with the European Artificial Intelligence Board and relevant stakeholders, including the Advisory Forum of stakeholders established under the AI Act.\n\nIn addition, when issuing standardisation requests to the relevant European standardisation organisations, the Commission should specify that standards have to be clear and consistent. This prerequisite includes standards developed in the various sectors for products covered by the existing EU harmonisation legislation listed in Annex I. They are aimed at ensuring that high-risk AI systems or general-purpose AI models placed on the market or put into service in the EU meet the relevant requirements or obligations laid down in the AI Act.\n\nBy 2 August 2028 and every four years thereafter, the Commission will have to submit a report to review the progress made regarding the development of standardisation deliverables on the energy-efficient development of general-purpose AI models. In this context, the Commission will also be required to assess the need for further measures or actions, including binding measures or actions. The report will have to be submitted to the European Parliament and to the Council and made public.\n\n<!-- image -->\n\n## Liability\n\n## Commission amends proposal to align with AI Act\n\nFinally, it is worth noting that at the end of July 2024, the European Commission sent an updated version of its proposal adapting non-contractual civil liability rules to artificial intelligence (AI Liability Directive or AILD) to both the European Parliament and the Council. This proposal, which was first tabled by the Commission in September 2022, aims to address the risks generated by specific uses of AI through a set of rules focusing on respect of fundamental rights and safety. The current changes are designed to align the AI Liability Directive proposal with the completed AI Act.\n\nIt is notable that the new proposal amends article 4 regarding the increased potential responsibility of companies deploying AI systems. These deployers would now be presumed liable for damage caused if they 'did not monitor the operation of the AI system or, where appropriate, suspend [its] use' or did not use 'sufficiently representative' input data.\n\nThe European Parliament's lead draftsperson ( 'rapporteur' ) for this file, the German Christiandemocratic MEP Axel Voss, had previously requested the European Parliamentary Research Service to conduct an 'alternative impact assessment' to evaluate whether the AILD is still necessary in view of adoption of the AI Act. While the future of the proposed AI Liability Directive remains uncertain, it may proceed in a reduced form.\n\n## AI Guide Contributors\n\nAs a market-leading law firm for technology, ranked Tier 1 for AI (first ranking of its kind within the European legal directory community) and TMT by Legal 500 in 12 jurisdictions and Band 1 for global multi-jurisdictional TMT by Chambers, we distinguish ourselves through our deep understanding of the technical intricacies involved in AI technology development and deployment. This expertise enables us to effectively collaborate with developers and commercial teams, speaking their language and asking the right questions from the outset. Our international AI group comprises over 120 experts , covering virtually every intersection where this transformative technology meets law and regulation. From handling groundbreaking IP litigation and guiding clients through complex regulatory changes to implementing effective governance frameworks and innovating commercial and contractual arrangements.\n\nIf you have any questions about the content, please get in touch with any of the contributors below or your usual Bird &amp; Bird contact. You can also find out more about the latest AI developments in our AI Hub\n\n## Belgium\n\nBenoit Van Asbroeck Of Counsel +3222826067 benoit.van.asbroeck@twobirds.com\n\n<!-- image -->\n\n## Finland\n\nTobias Bräutigam Partner +358962266758 tobias.brautigam@twobirds.com\n\n<!-- image -->\n\n## Germany\n\n<!-- image -->\n\nDr. Miriam Ballhausen Partner +4940460636000 miriam.ballhausen@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n3\n\nFrancine Cunningham Regulatory and Public Affairs Director +3222826056 francine.cunningham@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## France\n\n<!-- image -->\n\nAnne-Sophie Lampe Partner +33142686333 anne-sophie.lampe@twobirds.com\n\n<!-- image -->\n\nDr. Nils Lölfing Counsel +4921120056000 nils.loelfing@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nPaolo Sasdelli Regulatory and Public Affairs Advisor +3222826076 paolo.sasdelli@twobirds.com\n\nCen Zhang Senior Associate +33142686000 cen.zhang@twobirds.com\n\n<!-- image -->\n\nOliver Belitz Counsel +4969742226000 oliver.belitz@twobirds.com\n\n<!-- image -->\n\n## Germany\n\n<!-- image -->\n\nDr. Simon Hembt Senior Associate +4969742226000 simon.hembt@twobirds.com\n\nAleksandra Mizerska Lawyer +48225837900 aleksandra.mizerska@twobirds.com\n\n<!-- image -->\n\nDr. Maria Jurek Senior Associate +48225837839 maria.jurek@twobirds.com\n\n<!-- image -->\n\n## Spain\n\n<!-- image -->\n\nJoaquín Muñoz Partner +34917906007 joaquin.munoz@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n3\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Italy\n\n<!-- image -->\n\nGian Marco Rinaldi Counsel +390230356071 gianmarco.rinaldi@twobirds.com\n\nAndrzej Stelmachowski Associate +48225837977 andrzej.stelmachowski @twobirds.com\n\n<!-- image -->\n\nMarta Kwiatkowska-Cylke Counsel +48225837964 marta.kwiatkowska-cylke@ twobirds.com\n\n<!-- image -->\n\n## The Netherlands\n\n<!-- image -->\n\nFeyo Sickinghe Of Counsel +31703538904 feyo.sickinghe@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n## Poland\n\n<!-- image -->\n\nAleksandra Cywinska Senior Associate +48225837875 aleksandra.cywinska@twobirds.com\n\nIzabela Kowalczuk-Pakula Partner +48225837932 izabela.kowalczuk-pakula@ twobirds.com\n\n<!-- image -->\n\nPawel Lipski Partner +48225837991 pawel.lipski@twobirds.com\n\n<!-- image -->\n\n## United Kingdom\n\nAlex Jameson Senior Associate +442078507139 alex.jameson@twobirds.com\n\n<!-- image -->\n\n## United Kingdom\n\n<!-- image -->\n\nIan Edwards Partner +442079056377 ian.edwards@twobirds.com\n\n<!-- image -->\n\nLiz McAuliffe Associate +442074156787 liz.mcauliffe@twobirds.com\n\n<!-- image -->\n\nToby Bond Partner +442074156718 toby.bond@twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nKaterina Tassi Senior Associate +442074156066 katerina.tassi@twobirds.com\n\n<!-- image -->\n\nNora Santalu Associate +442079826513 nora.santalu@twobirds.com\n\n<!-- image -->\n\nWill Bryson Senior Associate +442074156746 will.bryson@twobirds.com\n\nKatharine Stephens Partner +442074156104 katharine.stephens@ twobirds.com\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nRuth Boardman Partner +442074156018 ruth.boardman@twobirds.com\n\n<!-- image -->\n\n## twobirds.com\n\nThe information given in this document concerning technical legal or professional subject matter is for guidance only and does not constitute legal or professional advice.  Always consult a suitably qualified lawyer on any specific legal problem or matter. Bird &amp; Bird assumes no responsibility for such information contained in this document and disclaims all liability in respect of such information.\n\nThis document is confidential.  Bird &amp; Bird is, unless otherwise stated, the owner of copyright of this document and its contents. No part of this document may be published, distributed, extracted, re-utilised, or reproduced in any material form.\n\nBird &amp; Bird is an international legal practice comprising Bird &amp; Bird LLP and its affiliated and associated businesses.\n\nBird &amp; Bird LLP is a limited liability partnership, registered in England and Wales with registered number OC340318 and is authorised and regulated by the Solicitors Regulation Authority (SRA) with SRA ID497264. Its registered office and principal place of business is at 12 New Fetter Lane, London EC4A 1JP. A list of members of Bird &amp; Bird LLP and of any non-members who are designated as partners, and of their respective professional qualifications, is open to inspection at that address.\n\n9\n\n10",
  "fetched_at_utc": "2026-02-09T13:45:39Z",
  "sha256": "778dc0538cbfa4cceab9b0e55c717e4c52dc5e046f973acda44dc46137e05dd8",
  "meta": {
    "file_name": "European Union Artificial Intelligence Act - Bird & Bird.pdf",
    "file_size": 1313072,
    "mtime": 1767775345,
    "docling_errors": []
  }
}