{
  "doc_id": "pdf-pdfs-general-purpose-ai-model-compliance-guide-part-2-oliver-patel-8a89bcb2e20d",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel.pdf",
  "title": "General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel",
  "text": "<!-- image -->\n\n<!-- image -->\n\nI'm Oliver Patel, author and creator of Enterprise AI Governance .\n\nThis free newsletter delivers practical, actionable, and timely insights for AI governance professionals.\n\nMy goal is simple: to empower you to understand, implement, and master AI governance.\n\nIf you haven't already, sign up below and share it with your colleagues. Thank you!\n\nFollow me on LinkedIn for more frequent updates.\n\nWelcome to Part 2 of the General-Purpose AI (GPAI) Model Compliance Guide. This 3part series is posted exclusively on Enterprise AI Governance. To all subscribers and new readers, thanks for supporting the newsletter!\n\nThis week's edition provides a comprehensive yet accessible overview of the EU AI Act's provisions for GPAI Models with Systemic Risk .\n\nYou will learn:\n\n- ✅ What is a GPAI model with systemic risk?\n- ✅ What are the notification and exception procedures for providers of GPAI models\n\nwith systemic risk?\n\n<!-- image -->\n\n✅ What are the compliance obligations for providers of GPAI models with systemic risk?\n\n- ✅ What exactly is a 'systemic risk'?\n- ✅ Deep dive on the GPAI Code of Practice: Safety and Security Chapter\n\n<!-- image -->\n\n<!-- image -->\n\nIf you haven't read Part 1, you should check it out here and read it first. It provides a detailed breakdown of the Obligations for Providers of GPAI Models , including what the core obligations for all GPAI models are, how and when these obligations will be enforced by the AI Office, as well as specific considerations for open-source models and legacy GPAI models (released before 2 August 2025). All this is essential background information that is necessary to fully understand the compliance implications for GPAI models with systemic risk.\n\nPart 3, coming next week, will cover the knotty issue of 'Downstream Actors': Modification, Deployment, and Use of GPAI Models . This will address the important question of who exactly is a provider of a GPAI model-it could be you!\n\nNote: this series assumes familiarity with the EU AI Act, its core concepts, and the topic of general-purpose AI and foundation models more broadly. Also, I was not involved in the multi-stakeholder process of drafting and developing the EU's GPAI Code of Practice. Finally, none of this should be taken as legal advice. Always consult a legal professional.\n\nThe following official sources have been used to create this guide:\n\n- EU AI Act full text\n- European Commission Guidelines for Providers of GPAI Models\n- GPAI Code of Practice\n- Transparency Chapter\n- Model Documentation Form\n- Copyright Chapter\n- Safety and Security Chapter\n- Template for publishing GPAI Model Training Data Summary\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.\n\n## What is a GPAI model with systemic risk?\n\nThere are two broad categories of GPAI models that the AI Act regulates. These are:\n\n1.  GPAI models\n2.  GPAI models with systemic risk\n\nAs explained in Part 1 of this series, GPAI models 'are trained with a large amount of data using self-supervision at scale [… ] display significant generality and are capable of performing a wide range of distinct tasks'. The European Commission's recent guidelines also clarify that if an AI model's training compute exceeds 10^23 floatingpoint operations, and it can generate language (either text or audio), or generate image or video based on text inputs, then it should be considered a GPAI model.\n\nAn AI model is classified as a GPAI model with systemic risk if it meets the above definition and criteria and also has ' high impact capabilities '. The AI Act defines this as capabilities that ' match or exceed the capabilities recorded in the most advanced GPAI models '.\n\nIf the cumulative amount of computation used to train a GPAI model exceeds 10^25 floating-point operations, then it is, by default, presumed to have 'high impact capabilities' and thus classified as a GPAI model with systemic risk.\n\nBy relying on this compute threshold, the EU's position is that there is a direct correlation between how much computational resource is used to train an AI model and both the general-purpose capabilities of the model and the level of risk that it poses.\n\nThis means that the larger an AI model is (e.g., in terms of number of parameters) and the more data that is used to train it (e.g., in terms of different examples), the more likely it is to be classified as a GPAI model with systemic risk.\n\nWhilst the European Commission acknowledges that ' training compute is an imperfect proxy for generality and capabilities ', it argues that it is ' the most suitable approach at present' .\n\nHowever, the European Commission is empowered to amend this compute threshold, or introduce an entirely new indicator, via a delegated act. This means it can do so independently, without reopening and amending the AI Act itself. However, the European Parliament and the Council (i.e., the member states) have a right to object to any such changes.\n\nInterestingly, it is possible for a GPAI model to be classified as a GPAI model with systemic risk even if it does not meet the 10^25 floating-point operations compute threshold. This would require the European Commission to determine that it nonetheless has high impact capabilities, despite the lower cumulative amount of compute used to train it.\n\nIn making such a decision-that would prove controversial due to the impact on the impacted provider-the European Commission would consider factors like the size of the model, input and output modalities, benchmark and evaluation results, model\n\nautonomy level, and the number of end-users. Ultimately, it would have to prove that its capabilities match or exceed those of the most advanced GPAI models, despite the fact that less compute was used to train it.\n\n## What are the notification and exception procedures for providers of GPAI models with systemic risk?\n\nThe key point for enterprises is that they must carefully forecast, measure, track, and record their estimates of the amount of computational resource used to develop, train, modify, and fine-tune GPAI models, in order to determine what compliance obligations they may have to adhere to.\n\nWhen estimating and measuring compute levels, the European Commission's guidance is that providers should ' as a general rule, account for all compute that contributed or will contribute to the model's capabilities '. This even includes the compute expended to generate synthetic data for training, even if not all the synthetic data was eventually used to train the GPAI model.\n\nOnce an organisation knows that a GPAI model it has developed (or is in the process of developing) meets the threshold for training compute (which means it is classified\n\nas a GPAI model with systemic risk), it must notify the European Commission of this as soon as possible, and within two weeks at the latest. In some cases, this notification will be required before the overall training process is completed (e.g., if the threshold is exceeded mid-training run). This notification should include both the precise computation amount as well as a detailed explanation of how this has been estimated.\n\nIn its guidelines, the European Commission recommends that ' providers should estimate the cumulative amount of training compute that they will use ' before the training process begins. If their pre-training estimate surpasses the systemic risk threshold, they should inform the Commission of this.\n\nZooming out, this notification procedure enables the European Commission's AI Office to fulfill its role as the regulator overseeing and enforcing the AI Act's provisions on GPAI models. It will also promote transparency, as the European Commission will publish a list of all GPAI models with systemic risk that are in scope of the AI Act.\n\nFinally, it is possible for a provider of a GPAI model that is by default classified as a GPAI model with systemic risk to secure an exception. To do this, the provider must demonstrate that its GPAI model does not have 'high impact capabilities' and therefore does not pose systemic risks and should not be classified as such, despite surpassing the cumulative compute for training threshold of 10^25 floating-point operations.\n\nProviders can do this by pointing to evidence like benchmark and evaluation results, especially if these demonstrate a capability gap between their model and the most advanced AI models. It is important to note that providers cannot get out of the GPAI model with systemic risk classification merely by implementing robust controls and safeguards which mitigate the systemic risk. To secure an exception, they must convince the European Commission, with cold, hard evidence, that the model genuinely does not have high impact capabilities.\n\nThe AI Act describes this as an 'exceptional' scenario, requiring European Commission approval. In such instances, the burden of proof will be on the provider.\n\nGiven the extensive additional compliance obligations for providers of GPAI models with systemic risk (as compared to GPAI models), the question of which GPAI models are and are not classified as posing systemic risk is significant.\n\nPrecisely what these additional compliance obligations are is explained below.\n\n## What are the compliance obligations for providers of GPAI models with systemic risk?\n\nPart 1 of this series provides a detailed breakdown of the compliance obligations for providers of GPAI models. In summary, the four core obligations for GPAI models are:\n\n1.  Develop, maintain, and keep up-to-date comprehensive technical documentation.\n2.  Produce and make publicly available a detailed summary of the content and data used to train the GPAI model.\n3.  Implement a policy to comply with EU copyright and intellectual property law.\n4.  Cooperate with the European Commission and regulatory authorities and appoint an EU-based authorised representative (if based outside of the EU).\n\nProviders of open-source GPAI models are exempt from obligations 1 and 4. This means they still need to publish a training data summary and implement a copyright compliance policy.\n\nProviders of GPAI models with systemic risk must comply with all of the above obligations. Also, providers of open-source GPAI models with systemic risk are not exempt from any of the above obligations. This means that sufficiently advanced and capable open-source AI models are treated the same, from an AI Act compliance perspective, as proprietary models.\n\nIn other words, providers of any GPAI model with systemic risk, that is placed on the market or made available in the EU, irrespective of whether it is open-source, must\n\ncomply with all the above obligations (for providers of GPAI models), as well as the additional obligations for providers of GPAI models with systemic risk.\n\nThere are four core additional obligations that only apply to providers of GPAI models with systemic risk. These are:\n\n1.  Perform model evaluation using state of the art tools and protocols. This includes conducting adversarial testing to enable the identification and mitigation of ' systemic risks '.\n2.  Assess and mitigate potential systemic risks that may stem from the development, deployment, or use of the GPAI model with systemic risk.\n3.  Track, document, and report information about serious incidents and any corrective measures to address them.\n4.  Ensure an 'adequate level of cybersecurity protection' for both the GPAI model with systemic risk and the physical infrastructure of the model.\n\nThese obligations reflect the fact that EU lawmakers deem it both appropriate and necessary for the most advanced and capable foundation models to be subject to rigorous governance-including stringent safety and security testing and evaluation procedures, the implementation of technical guardrails and safeguards to mitigate risk, continuous monitoring and oversight, and documented accountability and risk ownership-due to the widespread use of these models and the distinct possibility\n\nthat this use could lead to significant negative impact.\n\nHowever, the text of the AI Act itself does not provide much detail about how to approach and implement the above four obligations. That is why there is a GPAI Code of Practice. The Code provides a detailed, standardised, and step-by-step compliance framework for GPAI model providers.\n\nThe GPAI Code of Practice: Safety and Security Chapter , which is most relevant for these obligations, is analysed below. But first, we explore the definition of 'systemic risk', which is at the heart of these obligations.\n\n## What exactly is a 'systemic risk'?\n\nThe overarching purpose of the above obligations is for providers to uncover and mitigate the systemic risks which their most capable and advanced GPAI models pose. This includes reducing the likelihood of these risks materialising and reducing their impact if they do materialise.\n\nThis raises an important question for GPAI model providers: what exactly is a 'systemic risk'?\n\nThe GPAI Code of Practic e builds on the AI Act by providing additional detail regarding precisely how providers should define, identify, and evaluate the systemic risks that their GPAI models could pose.\n\nIt classifies the following four risks as systemic risks. This means that if any providers that are also signatories identify any of these risks, they must be classified and treated as systemic risks:\n\n- Chemical, biological, radiological, and nuclear (CBRN) , e.g., a GPAI model that makes it easier or otherwise enables the design, development, and use of CBRNrelated weapons or materials.\n- Loss of control , e.g., a GPAI model that autonomously self-replicates and creates new, more advanced AI models, without human awareness or control.\n- Cyber offence , e.g., a GPAI model that can be used to significantly lower barriers to entry for scaling cyber attacks.\n- Harmful manipulation , e.g., a GPAI model that targets large populations of people and uses deceptive techniques to promote harmful or destructive behaviour.\n\nRecital 110 of the AI Act complements this, by providing an illustrative list of examples of systemic risks:\n\n- Major accidents.\n- Disruptions of critical sectors.\n- Serious consequences to public health and safety.\n- Negative effects of democratic processes.\n- Negative effects on public or economic security.\n- The dissemination of illegal, false, or discriminatory content.\n\nMore broadly the GPAI Code of Practice clarifies the essential characteristics of a systemic risk, to further enable their identification. This clarification is based on the formal definition of systemic risk provided in the AI Act. The three essential characteristics of a systemic risk are:\n\n1.  The risk is directly related to the GPAI model's high-impact capabilities.\n2.  The risk has a significant impact on the EU due to its reach or due to the actual or potential negative impact on public health, safety, public security, fundamental rights, or society as a whole.\n3.  The impact can spread widely, at scale, through connected AI systems and the AI and industry ecosystem more broadly.\n\nThe EU's view is that as model capabilities and model reach increase, so do the potential systemic risks. Recital 110 of the AI Act also highlights that such systemic\n\nrisks can arise due to various factors and causes, including (but not limited to):\n\n- Model misuse\n- Model reliability\n- Model fairness\n- Model security\n- Model autonomy level\n- Tool access\n- Model modalities\n- Release and distribution mechanisms\n- Potential to remove model guardrails\n\nThe detailed information provided in the AI Act and the Code of Practice is sufficient to enable providers of GPAI models with systemic risk to fulfil their obligation of identifying, evaluating, and mitigating the specific systemic risks their GPAI models may pose.\n\nDeep dive on the GPAI Code of Practice: Safety and Security Chapter\n\nThe final section of this article summarises and analyses the key elements of the GPAI Code of Practice: Safety and Security Chapter.\n\nThis Chapter focuses exclusively on the specific obligations for providers of GPAI models with systemic risk. It provides a comprehensive and standardised set of commitments and measures that signatory organisations will implement, in order to adhere to the four compliance obligations for GPAI models with systemic risk (detailed above).\n\nFor a dedicated explainer on the GPAI Code of Practice, check out this previous post on Enterprise AI Governance. The most important things to know about the GPAI Code of Practice are that i) it was approved by the EU on 1 August 2025, ii) it is a voluntary resource which helps providers comply with the full suite of obligations for GPAI models, and iii) it consists of three chapters: 1) Transparency, 2) Copyright, and 3) Safety and Security.\n\nThe European Commission strongly encourages providers to sign the Code of Practice and even indicated that it will be more trusting of signatory organisations. However, the Code itself is not law.\n\nThe Safety and Security Chapter, which is by far the most detailed of the three chapters, consists of ten commitments. All commitments directly relate to GPAI model with systemic risk risk identification, management, mitigation, treatment, monitoring,\n\nownership, and accountability.\n\nBelow is a detailed breakdown of each of the ten commitments and a summary of the most important measures that signatory organisations have committed to implementing.",
  "fetched_at_utc": "2026-02-09T13:52:27Z",
  "sha256": "8a89bcb2e20d4fac684e9bd81b11c216d4bacec3cc31def65e740339999263f1",
  "meta": {
    "file_name": "General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel.pdf",
    "file_size": 1723762,
    "mtime": 1767775345,
    "docling_errors": []
  }
}