{
  "doc_id": "pdf-pdfs-netherlands-ai-act-guide-3015a2b513d5",
  "source_type": "local_pdf",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Netherlands AI Act Guide.pdf",
  "title": "Netherlands AI Act Guide",
  "text": "AI Act Guide \n\n# Version 1.1 – September 2025 AI Act Guide  | 2\n\nCover image: Wes Cockx & Google DeepMind / BetterImages of AI / AI large language models / CC-BY 4.0 AI Act Guide  | Guide to reading and disclaimer  3\n\nGuide to reading this document and disclaimer \n\nYou develop AI systems or are considering using them in your organisation. In that case, you may \n\nwell come into contact with the AI Act. This guide has been prepared as a tool to provide you with \n\neasy insight into the key aspects of the AI Act.  No rights may be derived from the content of this \n\nguide.  The legal text of the AI Act always takes precedence. \n\nPlease submit any feedback on this guide  to  ai-verordening@minezk.nl . Your feedback will be \n\nused to improve future editions of the guide. \n\nIf you are reading a paper version of the guide , visit  ondernemersplein.nl  for the latest version. \n\nThe website also provides references to the latest guidelines from the European Commission. At the \n\ntime of publication, guidelines have been issued regarding the definition of AI, prohibited AI and \n\nGeneral Purpose AI models. \n\n# The AI Act \n\nThe AI Act is an extensive legal document governing Artificial Intelligence (AI) for the entire European Union \n\n(EU). The AI Act contains rules for the responsible development and use of AI by businesses, government \n\nand other organisations. The aim of the regulation is to protect the safety, health and fundamental rights \n\nof natural persons. Application of the regulation means that organisations can be certain that the AI they \n\nuse is responsible and that they can enjoy the benefits and opportunities offered by AI. \n\nThe regulation will be introduced in phases and the majority will apply as from mid-2026 onwards. \n\nA number of AI systems have already been prohibited since February 2025. Given this situation, it is \n\nimportant that you make the necessary preparations. To help you in that process, this guide lists the \n\nmost important provisions from the AI Act. However, no rights may be derived from the information \n\ncontained in this document. Its sole purpose is to provide support. Click  here 1 for the complete text of \n\nthe regulation. \n\n## What does the AI Act mean for your organisation? \n\nDepending on the type of AI system and the use to which the organisation puts that system, requirements \n\nwill be imposed on its development and use. Whether requirements are imposed will among others depend \n\non the risk the AI system represents to safety, health and fundamental rights. Different requirements will \n\nbe imposed on organisations that develops an AI-system or has it developed than on organisations that \n\nmake use of AI. To find out what the AI Act means for your organisation, it is important to work through \n\nthe four steps listed below. These steps are explained further in this guide: \n\nStep 1 (Risk):  Is our (AI) system covered by one of the risk categories? \n\nStep 2 (AI):  Is our system ‘AI’ classified according to the AI Act? \n\nStep 3 (Role):  Are we the provider or deployer of the AI system? \n\nStep 4 (Obligations):  What obligations must we comply with? \n\nNote:  Many other guides and step-by-step plans start at step 2 (is our system ‘AI’) rather than step 1 \n\n(risk categories). After all, if an AI system does not qualify as ‘AI’ there are no requirements subject to the \n\nAI Act. However, even for systems which are not categorised as AI according to the AI Act, it is important \n\nto have a clear idea of the risks of the purposes for which they are used. That is why in this AI Act Guide, \n\nwe have chosen to start with the risk categories.  \n\n> 1https://eur-lex.europa.eu/legal-content/NL/TXT/?uri=CELEX:32024R1689\n\nAI Act Guide  | Step 1  4\n\n# Step 1. (Risk) Is our (AI) system covered by one \n\n# of the risk categories? \n\nAll AI systems are subject to the AI Act, but depending on the risk, different requirements are imposed \n\non different categories of system. The risk is determined by the intended application or the product for \n\nwhich the AI system is being developed, sold and used: \n\n• Prohibited AI practices:  these AI systems may not be placed on the market, put into service for used \n\nfor certain practices. 2\n\n• High-risk AI systems:  these AI systems must satisfy a number of requirements to mitigate the risks \n\nbefore they may be placed on the market or used. 3\n\nOther requirements will also apply to AI models and AI systems capable of performing certain tasks: \n\n• General purpose AI models and systems : these models and systems will be subject to specific \n\ninformation requirements. In certain cases, other requirements must be complied with in order to \n\nmitigate risks. 4\n\n• Generative AI and chatbots : these applications will be subject to specific transparency requirements \n\ndepending on whether the system is or is not a high-risk system. 5\n\nThe same AI system can sometimes be covered by multiple categories. A chatbot, for example, can be \n\ndeployed for a high-risk application and/or based on a general purpose AI model. AI systems not covered \n\nby any of the categories described above are not required to comply with the requirements from the \n\nAI  Act. Nevertheless, you must remember that AI systems and the development of AI systems may also \n\nbe required to comply with requirements from other regulations such as the General Data Protection \n\nRegulation (GDPR). \n\nTo determine whether you are required to comply with requirements from the AI Act, it is important to \n\nfirst identify the category that covers your AI system. Below we discuss the different risk categories in \n\nmore detail. \n\n## 1.1.  Prohibited AI systems \n\nCertain AI practices bring about an unacceptable risk for people and society. These practices have \n\ntherefore been prohibited since February 2025. This means that these systems may not be placed on \n\nthe market, put into service or used for these practices. These prohibitions apply both to providers and \n\ndeployers since 2 February 2025 (further explanation is provided under  Step 3. Are we the provider or \n\ndeployer of the AI system? on page 12 ).  \n\n> 2\n\nChapter II, Article 5 AI Act.  \n\n> 3\n\nChapter III, Article 6 through to 49 AI Act.  \n\n> 4\n\nChapter V, Article 51 through to 56 AI Act.  \n\n> 5\n\nChapter IV, Article 50 AI Act. Specific transparency requirements will also apply for emotion recognition and biometric \n\ncategorisation systems. As these systems are also high-risk AI systems, they will also have to adhere to the requirement for \n\nthese systems, as described in step 4.2. AI Act Guide  | Step 1  5\n\nProhibited AI systems 6\n\n1.  Systems intended to  manipulate human behaviour  with a view to restricting the free choice of \n\nindividuals and which can result in significant harm to those persons. \n\n2.  Systems which  exploit the vulnerabilities  of persons due to their age, disability or a specific social \n\nor economic situation and which are likely to cause significant harm to those persons. \n\n3.  Systems that draw up a point system of rewards and punishments based on social behaviour \n\nor personality traits, known as  social scoring , which could lead to detrimental or unfavourable \n\ntreatment. \n\n4.  Prohibition on systems for making  risk assessments to predict the risk of a person committing \n\na criminal offence , based solely on profiling or personality or other traits. \n\n5.  Systems which create or expand  facial recognition databases  through the untargeted  scraping \n\nof facial images from the internet or CCTV footage. \n\n6.  Systems for  emotion recognition  in the workplace and in education institutions, except where \n\nintended for medical or safety reasons. \n\n7.  Systems used for categorising  individual persons using biometric categorisation systems  in \n\ncertain sensitive categories such as race and sexual orientation. \n\n8.  The use of real-time remote biometric identification systems in publicly accessible spaces \n\nfor the purposes of law enforcement.  There are a number of exceptions in cases in which use \n\nis strictly necessary, for example when searching for specific victims of obduction, trafficking in \n\nhuman beings or missing persons. These applications are subject to additional guarantees. \n\n## 1.2.  High-risk AI systems \n\nHigh-risk AI systems may result in risks to health, safety or fundamental rights of natural persons, such \n\nas the right to privacy and the right not to be discriminated. At the same time, these systems can also \n\nhave positive impact on natural persons and organisations, if they are reliable and the risks are mitigated. \n\nAgainst that background, from August 2026 onwards, high-risk AI systems must comply with a variety \n\nof requirements before being placed on the market, used or put into service. This means that during the \n\ndevelopment of the system,  providers  must ensure that the system satisfies these requirements before \n\nit is first placed on the market or used. A professional party that uses the AI system subject to its personal \n\nresponsibility is considered a  deployer  (explained in more detail under  Step 3. Are we the provider or \n\ndeployer of the AI system? on page 12 ). \n\nDeployers are also subject to obligations with the aim of mitigating risks resulting from the specific use of \n\nthe system. There are two types of high-risk AI systems: \n\n• High-risk products : AI systems that are directly or indirectly also subject to a selection of  existing \n\nproduct regulations  (see below). For example an AI system as a safety component of a lift or an AI \n\nsystem that in and of itself is a medical device. \n\n• High-risk applications : AI systems developed and deployed for specific applications in ‘high-risk \n\napplication areas’. These are eight application areas for AI that range from AI for law enforcement to AI \n\nin education. Within those eight areas, around 30 different specific applications have been identified \n\nthat result in high risks, such as AI systems that support the deployment of emergency first response \n\nservices. \n\nThe product groups and application areas in which AI systems are categorised as high-risk appear in the \n\nfigures below. \n\nThe obligations for this category also apply to high-risk products as from 2 August 2027 and to high-risk \n\napplication areas as from 2 August 2026. The obligations are described under  4.2. High-risk AI on page 13 . \n\n> 6Article 5 AI Act, see also Commission Guidelines on prohibited artificial intelligence practices.\n\nAI Act Guide  | Step 1  6\n\nHigh-risk AI as (safety element of) existing products \n\nThese are products already regulated within the EU. A product is considered as representing a risk if \n\nin accordance with existing product regulations, third-party approval is required before the product \n\ncan be placed on the market (conformity assessment). If AI is a safety-related component of the risk \n\nproduct or if the risk product itself is an AI system, it is considered as high-risk AI. This applies to \n\nproducts covered by the following product legislation: 7\n\n• Machines  (Directive 2006/42/EC) \n\n• Toys  (Directive 2009/48/EC) \n\n• Recreational craft  (Directive 2013/53/EU) \n\n• Lifts  (Directive 2014/33/EU) \n\n• Equipment and protective systems intended for use in potentially explosive atmospheres \n\n(Directive 2014/34/EU) \n\n• Radio equipment  (Directive 2014/53/EU) \n\n• Pressure equipment  (Directive 2014/68/EU) \n\n• Cableway installations  (Regulation (EU) 2016/424) \n\n• Personal protective equipment  (Regulation (EU) 2016/425) \n\n• Appliances burning gaseous fuels  (Regulation (EU) 2016/425) \n\n• Medical devices  (Regulation (EU) 2017/745) \n\n• In-vitro diagnostic medical devices  (Regulation (EU) 2017/746) \n\nIn addition, the AI Act contains a further list of products also considered as high-risk AI, but which \n\nare not subject to any direct requirements under the AI Act. Nevertheless, at a later moment, the \n\nrequirements from the AI Act will be used to clarify the specific product legislation applicable to \n\nthese products. It is not yet known when this will take place, and it will differ from product to \n\nproduct. The products in question are subject to the following product legislation: 8\n\n• Civil aviation security  (Regulation (EC) 300/2008 and Regulation (EU) 2018/1139) \n\n• Two or three-wheeled vehicles and quadricycles  (Regulation (EU) 168/2013) \n\n• Agricultural and forestry vehicles  (Regulation (EU) 167/2013) \n\n• Marine equipment  (Directive 2014/90/EU) \n\n• Interoperability of the railway system in the EU  (Directive (EU) 2016/797) \n\n• Motor vehicles and trailers  (Regulation (EU) 2018/858 and Regulation (EU) 2019/2144) \n\nHigh-risk application areas \n\nAn AI system is within the scope of one of the high-risk application areas if the provider intended the \n\nuse of the AI system in one of these areas. In the documentation of the AI system, the provider must \n\nexplicitly state the purpose, including the instructions for use, advertising materials and any other \n\ntechnical documentation.  Note:  Even if the provider did not intend the AI system as being high-risk \n\nwhen it was placed on the market, it may still be that in practice, a deployer does use the system for \n\none of the high-risk application areas. In that case, the deployer is seen as the provider, and as such \n\nbecomes responsible for the requirements imposed on high-risk AI systems. See also  chapter 4.2 .\n\nThere are eight high-risk application areas. This does not mean that all AI systems covered by \n\nthe often abstractly described application areas are necessarily high-risk. A number of specific \n\napplications are listed for each area. 9\n\nTip:  First check whether your AI system is covered by one of the eight application areas and then \n\ndetermine whether your AI system is one of the AI systems described in that category. Only in that \n\ncase are you dealing with a high-risk AI system that must comply with the requirements.    \n\n> 7Article 6(1) and Annex I, Section A AI Act.\n> 8Article 2(2) and Annex I, Section B AI Act.\n> 9Annex III AI Act.\n\nAI Act Guide  | Step 1  7\n\n1.  Biometrics \n\n• Remote biometric identification systems, unless the system is only used for verification. \n\n• Systems used for biometric categorisation according to sensitive or protected attributes. \n\n• Systems for emotion recognition. \n\n2.  Critical infrastructure \n\n• Systems intended to be used as safety components for the management and operation of critical \n\ndigital infrastructure, road traffic or in the supply of water, gas, heating or electricity. \n\n3.  Education and vocational training \n\n• Systems for admission to or allocation of (vocational) education. \n\n• Systems for evaluating learning outcomes. \n\n• Systems for assessing the level of (vocational) education. \n\n• Systems for monitoring students during tests. \n\n4.  Employment, workers’ management and access to self employment. \n\n• Systems for the recruitment or selection of candidates. \n\n• Systems to be used to make decisions affecting terms of work-related relationships, the allocation \n\nof tasks or the monitoring and evaluation of workers. \n\n5.  Essential private services and public services and benefits \n\n• Systems for evaluating the eligibility to essential public assistance benefits and services. \n\n• Systems for evaluating the creditworthiness or credit score of natural persons unless used for the \n\npurposes of detecting financial fraud. \n\n• Systems for risk assessment and pricing in relation to life and health insurance. \n\n• Systems for evaluating emergency calls and prioritising the dispatch of emergency first response \n\nservices and emergency health care patient triage systems. \n\n6.  Law enforcement \n\n• Systems for law enforcement to assess the risk of a natural person becoming the victim of \n\ncriminal offences. \n\n• Systems for law enforcement to be deployed as polygraphs or similar tools. \n\n• Systems for law enforcement for evaluating the reliability of evidence. \n\n• Systems for law enforcement for assessing or predicting the risk of a natural personal offending or \n\nto assess past criminal behaviour of natural persons or groups. \n\n• Systems for law enforcement for the profiling of natural persons in the course of detection, \n\ninvestigation or prosecution of criminal offences. \n\n7.  Migration, asylum and border control \n\n• Systems for public authorities to be used as polygraphs or similar tools. \n\n• Systems for public authorities for assessing a security risk, the risk of irregular migration or a \n\nhealth risk upon entry into a country. \n\n• Systems for public authorities to assist in the examination of applications for asylum, visa or \n\nresidence permit, including associated complaints. \n\n• Systems for public authorities for detecting, recognising or identifying natural persons, with the \n\nexception of the verification of travel documents. \n\n8.  Administration of justice and democratic processes \n\n• Systems to be used by a judicial authority to assist in applying the law and resolving disputes and \n\nresearching and interpreting facts and applying the law to a concrete set of facts. \n\n• Systems for influencing the outcome of an election or referendum or the voting behaviour \n\nof natural persons, with the exception of tools used to support political campaigns from an \n\nadministrative or logistic point of view. AI Act Guide  | Step 1  8\n\nExceptions to high-risk application areas \n\nThere are a number of specific exceptions in which AI systems are covered by one of the application \n\nareas but which are not seen as high-risk AI. This applies where there is no significant risk to health, \n\nsafety or fundamental human rights. This for example applies if an AI system has  no significant \n\nimpact on the outcome of a decision,  for example because the system is intended for :10 \n\n• Performing a narrow procedural task; \n\n• Improving the result of a previously completed human activity; \n\n• Detecting decision-making patterns or deviations from prior decision-making patterns and not \n\nmeant to replace or influence the previously completed human assessment; \n\n• Performing a preparatory task to an assessment relevant to one of the high-risk application areas. \n\nIt should also be noted that an AI system used for profiling natural persons cannot make use of \n\nthis exception. If you have determined that your (non-profiling) AI system is subject to one of the \n\nexceptions, you must record this fact and register the AI system in the EU database for high-risk AI \n\nsystems. 11  At a later moment, the European Commission will draw up a list of examples to clarify \n\nwhat is and what is not covered by the exceptions. \n\n## 1.3.  General purpose AI models and AI systems \n\nAn AI model is an essential component of an AI system, but is not an AI system in and of itself. This \n\nrequires more elements, for example a user interface. 12 \n\nA general purpose AI model  (General Purpose AI) can successfully perform a wide range of different \n\ntasks and as such can be integrated in a variety of AI systems. These models are often trained on large \n\nvolumes of data using self-supervision techniques. 13 \n\nThe broad deployability of these models via specific AI systems means that they are used for a wide \n\nrange of applications. These can include high-risk applications. Due to the potential large impact of these \n\nmodels, from August 2025 onwards, they must comply with various requirements. \n\nIf an AI system is based on a general purpose AI model which itself can actually serve multiple purposes, \n\nthen it is a  general purpose AI system. 14 \n\nThe obligations applicable to this category apply from 2 August 2025 and are described under \n\n4.3. General  purpose AI models and systems on page 18 .     \n\n> 10 Article 6(3) AI Act.\n> 11 Article 6(4) AI Act.\n> 12 Consideration 97 AI Act.\n> 13 Article 3(63) AI Act.\n> 14 Article 3(66) AI Act.\n\nAI Act Guide  | Step 1  9\n\n## 1.4.  Generative AI and Chatbots \n\nCertain AI systems are subject to transparency obligations. 15  These are systems with which natural \n\npersons often interact directly. It must therefore be clear to these natural persons that they are \n\ninteracting with AI or that the content has been manipulated or generated. \n\n• Systems used for generating audio, images, video or text ( generative AI ); \n\n• Systems made for interaction ( chatbots ). \n\nThe obligations applicable to this category apply from 2 August 2026 and are described under \n\n4.4. Generative AI  and Chatbots on page 19 .\n\n## 1.5.  Other AI \n\nSee  4.5. Other AI on page 20  for more information on AI systems not covered by one of the risk \n\ncategories described above.  \n\n> 15 Article 50 AI Act.\n\nAI Act Guide  | Step 2  10 \n\n# Step 2. Is our system ‘AI’ classified according to \n\n# the AI Act? \n\nThe AI Act imposes regulations on AI systems. There are different ideas about what AI is and what is not \n\nAI. The AI Act offers the following definition, which is intended to demarcate the nature of AI as a product \n\non the market: \n\n“An AI system means a  machine-based system  that is designed to operate  with varying levels of autonomy  and \n\nthat may exhibit  adaptiveness  after deployment and that, for  explicit or implicit objectives, infers,  from the  input  it \n\nreceives, how to generate  outputs such as predictions, content, recommendations, or decisions  that can influence \n\nphysical or virtual environments .” 16 \n\nThese different elements can be present both in the development phase and in the use phase. What is \n\nthe meaning of the terms used in this definition? 17 \n\n• Autonomy : This element is satisfied if autonomy is present in a system to a certain extent, even if very \n\nlimited. Systems without any form of autonomy are systems that only operate if human actions or \n\ninterventions are required for all actions by that system. \n\n• Adaptiveness:  It is stated that a system ‘may’ exhibit adaptiveness after deployment. Although \n\nadaptiveness is therefore not identified as a decisive element, its presence is an indication that the \n\nsystem is an AI system. \n\n• It infers how to generate output on the basis of input (capacity to infer):  This relates not only to \n\ngenerating output during the ‘use phase’ but also the capacity of an AI system to infer models and/or \n\nalgorithms from data during the development phase. \n\nWhat does this cover? 18 \n\n• Systems that make use of  machine learning  in which the system learns how certain objectives can be \n\nachieved on the basis of data. Examples of machine learning are (un)supervised learning, self-super -\n\nvised learning, reinforcement learning and deep learning. \n\n• Systems that make use of  knowledge and logic-based approaches  which make learning, reasoning or \n\nmodelling possible on the basis of determined knowledge or a symbolic representation of a task to be \n\nsolved. Examples of these approaches are knowledge representation, inductive (logic) programming, \n\nknowledge bases, inference and deductive engines, (symbolic) reasoning, expert systems and search \n\nand optimisation methods. \n\nWhat is  not  covered? \n\n• Systems based on rules laid down exclusively by natural persons to conduct automatic actions. Some \n\nof these systems can to a certain extent derive how to generate output from input received, but are still \n\nbeyond the definition because they are only able to analyse patterns to a limited extent or are unable to \n\nautonomously adapt their output. Examples can be systems for improved mathematical optimisation, \n\nstandard data processing, systems based on classic heuristics and simple prediction systems. \n\n• Systems designed to be used with full human intervention. These systems do not operate with a \n\nminimum level of autonomy.  \n\n> 16\n\nArticle 3(1) AI Act.  \n\n> 17\n\nCommission Guidelines on the definition of an artificial intelligence system.  \n\n> 18\n\nConsideration 12 AI Act. AI Act Guide  | Step 2  11 \n\nThe European Commission has issued a guideline to further clarify the definition of AI. You can find the \n\nlatest version on this guideline at  ondernemersplein.nl .\n\nIf your system is not considered AI under the AI Act but is covered by one of the risk categories, it is \n\nimportant to hold a discussion within your organisation about the extent to which the system still \n\nrepresents risks and to mitigate these risks by complying with (specific) requirements from the AI Act. \n\nSystems beyond the scope of the AI Act may nevertheless still have to comply with requirements from \n\nother legislation and regulations. AI Act Guide  | Step 3  12 \n\n# Step 3. Are we the provider or deployer of the \n\n# AI system? \n\nOnce you have determined the risk category that covers you AI system and whether your AI system is in \n\nfact subject to the AI Act, you must then determine whether you are the provider or deployer. \n\n• Provider : a person or organisation that develops or commissions the development of an AI system or \n\nmodel and places it on the market or puts the AI system into service. 19 \n\n• Deployer : a person or organisation using an AI system under its personal authority. This does not \n\ninclude non-professional use. 20 \n\nThe description of the requirements in step 4 describes for each risk category which obligations apply \n\nto providers and deployers. Each is required to comply with other obligations. The strictest obligations \n\napply to providers. \n\nNote:  As deployer, in certain cases you can also become provider of a high-risk AI system such that you \n\nare required to comply with the high-risk obligations for providers. 21  This is further explained in steps \n\n4.2. High-risk  AI on page 13  and  4.3. General purpose AI models and systems on page 18 .\n\nNote : the AI Act also includes other roles such as authorised representative, importer and distributor. 22 \n\nThe obligations governing these actors are not discussed in this guide.  \n\n> 19\n\nArticle 3(3) AI Act.  \n\n> 20\n\nArticle 3(4) AI Act.  \n\n> 21\n\nArticle 25 AI Act.  \n\n> 22\n\nArticle 3(5), (6) and (7), Articles 22, 23 and 24. AI Act Guide  | Step 4  13 \n\n# Step 4. What obligations must we comply with? \n\n## 4.1.  Prohibited AI practices \n\nThese AI practices result in unacceptable risk and have therefore been prohibited since 2 February  2025. \n\nThis means that AI systems cannot be placed on the market or used for these practices. These prohibitions \n\napply to both  providers  and  deployers. 23 \n\nThere are sharply demarcated exceptions to the prohibition on the use of real-time remote biometric \n\nidentification systems in publicly accessible spaces for the purposes of law enforcement, and the use of \n\nthose systems must be provided with a basis in national legislation. There are also additional guarantees \n\nrelating to the deployment of these systems. \n\n## 4.2.  High-risk AI \n\nThe majority of requirements from the AI Act will apply to high-risk AI systems.  Providers  must comply \n\nwith various obligations such as: 24 \n\n• System for risk management; \n\n• Data and data governance; \n\n• Technical documentation; \n\n• Record-keeping (logs); \n\n• Transparency and information; \n\n• Human oversight; \n\n• Accuracy, robustness and cybersecurity; \n\n• Quality management system; \n\n• Monitoring. \n\nIf you as provider comply or believe you comply with all these obligations, you will be required to \n\nconduct a  conformity assessment . In certain cases you can conduct this assessment yourself, while \n\nin other cases it must be conducted by a third party on your behalf. 25  A future version of this guide will \n\nexplain in more detail when you are required to conduct which procedure. \n\nDeployers  must also comply with various obligations. Additional obligations apply to government \n\norganisations using AI systems. 26 \n\nEach obligation is explained in the figure below. These obligations will be further elaborated over the \n\ncoming years in European standards. Participation will be organised via the standardisation institutes of \n\nthe European Member States. In the Netherlands this is the  NEN 27 .\n\nNote:  In two cases, as deployer of a high-risk AI system, you yourself can become the provider of \n\nthat system: 28 \n\n• When you as deployer place your own name or trademark on the high-risk system; \n\n• When you as deployer make a substantial modification to the high-risk AI system that was not \n\nintended by the provider as a consequence of which the system no longer complies with the require -\n\nments or as a consequence of which the purpose of the system intended by the provider changes. This \n\nlatter situation arises if you make use of an AI system that was not intended for high-risk applications, \n\nfor any such applications.  \n\n> 23\n\nArticle 5 AI Act.  \n\n> 24\n\nArticle 16 and Section 2 of Chapter III (Articles 8-15) AI Act.  \n\n> 25\n\nArticle 43 AI Act.  \n\n> 26\n\nArticles 26 and 27 AI Act.  \n\n> 27\n\nhttps://www.nen.nl/ict/digitale-ethiek-en-veiligheid/ai . \n\n> 28\n\nArticle 25 AI Act. AI Act Guide  | Step 4  14 \n\n## Requirements for high-risk AI systems \n\n1.  System for risk management 29 \n\nVarious steps must be taken for this system: \n\n• Identification and analysis of foreseeable risks the system can pose to health, safety or \n\nfundamental rights. \n\n• Taking suitable risk management measures to ensure that the risks that remain following \n\nimplementation of these measures are acceptable. \n\nThe following points must be taken into account: \n\n• The risks must be identified and dealt with before the AI system is placed on the market or used \n\nand subsequently continuously during the use of the AI system. \n\n• Foreseeable abuse of the system must be taken into account. \n\n• The context of the use, including the knowledge and experience of the deployer of such AI \n\nsystems or the fact that children and vulnerable groups may experience consequences of the \n\nAI system, must be taken into account. It may for example be necessary to offer training to the \n\npeople working with the AI system. \n\n• The risk management measures must be tested to check that they are actually effective. \n\nThis must be carried out on the basis of benchmarks appropriate to the purpose for which the \n\nAI system is deployed. \n\n• If a risk management system must also be established pursuant to existing product legislation, \n\nthis may be combined to form a single risk management system. \n\n2.  Data and data governance 30 \n\nDifferent requirements are imposed on the datasets used for training, validating and testing \n\nhigh-risk AI systems. \n\n• Data management appropriate to the purpose of the AI system, including: \n\n• The registration of the processes, including processes for data gathering and data processing; \n\n• The recording of assumptions about the datasets; \n\n• An assessment of the availability, quantity and suitability of the datasets including possible \n\nbiases that could have consequences for natural persons; \n\n• Measures for tracing, preventing and mitigating biases; \n\n• Tackling shortcomings in the datasets that may prevent compliance with the AI Act (for example \n\nmitigating risks according to the risk management system). \n\n• For the purpose for which they are used, datasets must be sufficiently representative and as far \n\nas possible error-free. The context in which the AI system is to be used must also be taken into \n\naccount; for example the geographical or social context. \n\n• Subject to a number of strict conditions, special categories of personal data (a term from the \n\nGeneral Data Protection Regulation) may be processed as a means of tackling bias.   \n\n> 29 Article 9 AI Act.\n> 30 Article 10 AI Act.\n\nAI Act Guide  | Step 4  15 \n\n3.  Technical documentation 31 \n\nThe technical documentation must demonstrate that the high-risk AI system complies with the \n\nrequirements laid down in the AI Act. The technical documentation must among others include: \n\n• A general description of the AI system including the intended purpose of the system, the name of \n\nthe provider and instructions for use; \n\n• A detailed description of the elements of the AI system and of the development process for that \n\nsystem, including the steps in development, the design choices, the expected output from the \n\nsystem, the risk management system and the datasets used; \n\n• Detailed information about the monitoring, operation and control of the AI system, including the \n\ndegree of accuracy at individual and general level, risks, the system for evaluation during use and \n\nmeasures for monitoring and human oversight; \n\n• An overview of the applicable standards; \n\n• The EU conformity declaration (the CE mark). \n\nSME enterprises can record their technical documentation in a simplified manner. At a future \n\nmoment, the European Commission will issue a relevant form. \n\n4.  Record-keeping (logs) 32 \n\nAutomatic logs must be retained during the lifecycle of the AI system so that risks can be traced in a \n\ntimely manner and the operation of the system can be monitored. \n\nThese logs must be stored for at least six months. At least the following events must be recorded: \n\n• The duration of each use of the AI system; \n\n• The input data and the control of that data by the AI system (and the reference database); \n\n• The identification of the natural persons involved in the verification of the results. \n\n5.  Transparency and information 33 \n\nThe provider of the AI system knows how the system operates and how it should be used. For that \n\nreason, the provider must ensure that the AI system is sufficiently transparent so that deployers \n\nunderstand how they can correctly make use of the output from the system. \n\nWith this in mind,  instructions for use  must be drawn up, that include at least the following points: \n\n• Contact details; \n\n• The purpose, characteristics, capacities and limitations of the performance of the AI system; \n\n• The measures for human oversight. \n\n6.  Human oversight 34 \n\nHigh-risk AI systems must be designed by the provider in such a way that during use they can be \n\neffectively overseen by natural persons in order to mitigate the risks for natural persons. Human \n\noversight shall be context dependent – the greater the risks, the stricter the oversight must be. \n\nThe measures for oversight may be technical in nature (for example a clear human-machine \n\ninterface), or measures that must be undertaken by the deployer (for example a compulsory course \n\nfor their personnel).     \n\n> 31 Article 11 AI Act.\n> 32 Article 12 AI Act.\n> 33 Article 13 AI Act.\n> 34 Article 14 AI Act.\n\nAI Act Guide  | Step 4  16 \n\nThe eventual objective of these measures is to ensure that the natural persons who use the \n\nAI system are capable of the following: \n\n• They understand the capacities of the system and can monitor its functioning; \n\n• They are aware of automation bias; \n\n• They can correctly interpret and if necessary ignore or replace the output; \n\n• They can halt the system. \n\n7.  Accuracy, robustness and cybersecurity 35 \n\nHigh-risk AI systems must offer an appropriate level of accuracy, robustness and cybersecurity. \n\nTo achieve this, benchmarks and measuring methods are developed by the European Commission. \n\nAt least the following measures must be mentioned: \n\n• Technical and organisational measures to prevent errors that occur in interaction between the AI \n\nsystem and natural persons; \n\n• Solutions for robustness such as backups or security measures in the event of defects; \n\n• Removing or mitigating negative influencing of the system by limiting feedback loops; \n\n• Cybersecurity that prevents unauthorised third-party access by tracing, responding to and dealing \n\nwith attacks. These are attacks aimed at data poisoning, model poisoning, adapting input or \n\nobtaining confidential data. \n\n8.  Quality management system 36 \n\nThe quality management system must ensure that the requirements from the AI Act are complied \n\nwith. How extensive the quality management system must be will depend on the size of the \n\norganisation. For example by documenting the following: \n\n• A strategy for compliance; \n\n• Techniques, procedures and measures for the design, development and quality assurance of the \n\nAI system; \n\n• Whether standardisation is used; \n\n• Systems and procedures for data management, risk management, monitoring, incident reporting \n\nand documentation. \n\n9.  Monitoring 37 \n\nAs soon as an AI system has been placed on the market or is in use, providers must monitor the \n\nsystem on the basis of use data, thereby determining whether the system continues to comply with \n\nthe requirements from the AI Act. For this purpose, providers must draw up a monitoring plan. \n\nIf the provider of a high-risk AI system discovers that the system no longer functions in compliance \n\nwith the AI Act, corrective measures must be taken immediately to correct the situation. This may \n\neven include recalling the system if necessary. The provider must also work alongside the deployer \n\nand duly inform the surveillance authorities. \n\nSerious incidents involving the AI system must be reported to the surveillance authorities. 38     \n\n> 35 Article 15 AI Act.\n> 36 Article 17 AI Act.\n> 37 Article 72 AI Act.\n> 38 Article 73 AI Act.\n\nAI Act Guide  | Step 4  17 \n\nOther requirements \n\n• The registration of the high-risk AI system in the EU database. 39 \n\n• The contact details of the provider must be registered with the AI system. 40 \n\n• The technical documentation, documentation concerning the quality management system and \n\ndocumentation concerning the conformity assessment must be kept for 10 years. 41 \n\nObligations for deployers of high-risk AI systems \n\nNot only providers but also the deployers of high-risk AI systems are subject to obligations. After \n\nall they are the parties who control how the AI system is used in practice and as such have a major \n\nimpact on the risks that can occur. \n\nDeployers must: 42 \n\n• Take appropriate technical and organisational measures to ensure that the high-risk AI system is \n\nused in accordance with the instructions for use; \n\n• Assign human oversight to natural persons who have the necessary competence, training and \n\nauthority; \n\n• Ensure that the input data is relevant and sufficiently representative, wherever possible; \n\n• Monitor the operation of the AI system on the basis of the instructions for use; \n\n• If the deployer has reason to believe that the system no longer complies with the requirements \n\nfrom the AI Act, the deployer must duly inform the provider and cease use of the system; \n\n• Inform the provider and surveillance authorities of possible risks and serious incidents that have \n\noccurred; \n\n• Keep the logbook under their control for at least six months; \n\n• Inform worker representation if the AI system is to be deployed on the shop floor; \n\n• Duly inform people if decisions are taken about natural persons using the high-risk AI system; \n\n• If use is made of AI for emotion recognition or biometric categorisation, the natural persons in \n\nrespect of whom the system is used must be duly informed. \n\nSpecific obligations for government organisations as deployers  In addition to the obligations \n\ndescribed above, government organisations must comply with a number of additional obligations: \n\n• Register use of a high-risk system in the EU database; 43 \n\n• Assess the potential consequences for fundamental rights if the high-risk AI system is used \n\nwith a view to the specific context within which use takes place (a  fundamental rights impact \n\nassessment ). They will for example consider the duration of use, the processes within which \n\nthe system is used and the potential impact of use on the fundamental rights of natural persons \n\nand groups. Following identification of the risks, deployers must take measures for human \n\noversight and deal with possible risks. A report must also be submitted to the market surveillance \n\nauthorities unless an appeal can be made to an exception based on public safety or protection of \n\nhuman health. 44 \n\nNote:  This obligation also applies to private entities providing public services, the use of AI systems \n\nfor assessing the creditworthiness of natural persons and AI systems for risk assessments for life and \n\nhealth insurance.       \n\n> 39 Article 49 AI Act.\n> 40 Article 16(b) AI Act.\n> 41 Article 18 AI Act.\n> 42 Article 26 AI Act.\n> 43 Article 49(3) AI Act.\n> 44 Article 27 AI Act.\n\nAI Act Guide  | Step 4  18 \n\n## 4.3.  General purpose AI models and systems \n\nObligations for providers of general purpose AI models 45 \n\nGeneral purpose AI models can be integrated in all kinds of different AI systems. It is essential \n\nthat the providers of these AI systems know what the AI model is and is not capable of. Specific \n\nrequirements are also imposed on the training of these models because training often makes use of \n\nlarge datasets. The providers of these models must: \n\n• Draw up technical documentation of the model including the training and testing process and the \n\nresults and evaluation; \n\n• Draw up and keep up to date information and documentation for providers of AI systems \n\nwho intend to integrate the model in their AI system. The information must provide a good \n\nunderstanding of the capacities and limitations of the AI model and must enable the provider of \n\nthe AI system to comply with the obligations from the AI Act. \n\n• Draw up a policy to ensure that they train the model without infringing the copyrights of natural \n\npersons and organisations; \n\n• Draw up and publish a sufficiently detailed summary about the content used for training the AI model. \n\nProviders of open source models are not required to comply with the first two obligations (technical \n\ndocumentation and drawing up information for downstream providers). \n\nObligations for providers of general purpose AI models with systemic risks 46 \n\nIn certain cases, general purpose AI models can generate systemic risks. This applies if the model \n\nhas high impact capacity, for example due to the scope of the model or due to (potential) negative \n\nimpact on public health, safety, fundamental rights or society as a whole. This is at least assumed \n\nif at least 10 25  floating point operations (FLOPs) are used to train the model. On the basis of specific \n\ncriteria, the European Commission can also determine that the model has a similar major impact in \n\nsome other way. These models must: \n\n• Comply with the obligations for general purpose AI models; \n\n• Implement model evaluations to map out the systemic risks; \n\n• Mitigate systemic risks; \n\n• Record information about serious incidents and report those incidents to the AI Office; \n\n• Ensure appropriate cybersecurity. \n\nNote : these obligations only apply to the largest AI models. \n\nProviders of these models with systemic risks cannot appeal to an exception for open source.   \n\n> 45 Article 53 AI Act.\n> 46 Article 55 AI Act.\n\nAI Act Guide  | Step 4  19 \n\nWhat rights do you have if you integrate a general purpose AI model in your (high-risk) AI system? \n\nAs indicated above, you must at least receive information and documentation to enable you to determine \n\nfor yourself how you can make use of the model in your AI system for the chosen purpose. If you include \n\nthe model in a high-risk AI system, as a provider you must then still comply with the obligations from the \n\nAI Act. \n\nHow should you deal with general purpose AI systems?  As indicated in  1.3. General purpose AI models \n\nand AI systems on page 8 , there are also AI systems that can serve multiple purposes. Take for example \n\nthe widely known AI chatbots.  Note:  If you deploy these systems for high-risk purposes, according to the \n\nAI Act you yourself become a provider of a high-risk AI system. 47  It is then up to you to comply with the \n\napplicable obligations. In this situation it is very difficult to comply with the obligations for a high-risk AI \n\nsystem, which means that you may run the risk of receiving a penalty. \n\n## 4.4.  Generative AI and Chatbots \n\nTo ensure that natural persons know whether they are talking to an AI system or are seeing content \n\ngenerated by AI, transparency obligations are imposed on generative AI and chatbots. \n\nRules for providers of chatbots 48 \n\nProviders of systems designed for direct interaction with natural persons must ensure that these \n\nnatural persons are informed that they are interacting with an AI system. \n\nRules for providers of generative AI 49 \n\nProviders of systems that generate audio, image, video or text content must ensure that the output \n\nis marked in a machine readable format so that the output can be detected as artificially generated \n\nor manipulated. \n\nRules for deployers of generative AI 50 \n\nDeployers of systems that generate audio, image or video content must ensure that it is clear that \n\nthe content is artificially generated or manipulated. This can for example be achieved by applying a \n\nwatermark. For creative, satirical, fictional or analogue work, this may be carried out in a way that \n\ndoes not ruin the work. \n\nA special regime applies to artificially generated text. Only in cases where texts are used with the \n\npurpose of ‘informing the public on matters of public interest’, the fact must be disclosed that the \n\ntext has been artificially generated or manipulated. If there is editorial review and responsibility, this \n\nneed not be carried out. \n\nRules for deployers of emotion recognition systems or systems for biometric categorisation 51 \n\nThe deployers of these AI systems must inform the natural persons exposed to these systems about \n\nhow the system works.      \n\n> 47 Article 25(1)(c) AI Act.\n> 48 Article 50(1) AI Act.\n> 49 Article 50(2) AI Act.\n> 50 Article 50(4) AI Act.\n> 51 Article 50(3) AI Act.\n\nAI Act Guide  | Step 4  20 \n\n## 4.5.  Other AI \n\nAI systems beyond the categories referred to above are not required to comply with requirements \n\naccording to the AI Act. \n\nBut note : if you as  deployer  make use of ‘other category’ AI systems for a high-risk application as \n\nreferred to in the AI Act (see  1.2. High-risk AI systems on page 5 ), it automatically becomes a high-risk AI \n\nsystem and you must comply with the requirements from the AI Act as a system  provider .52  \n\n> 52 Article 25(1)(c) AI Act.\n\nColofon \n\nDit is een uitgave van; \n\nMinisterie van Economische Zaken \n\nPostbus 20901  | 2500 EX Den Haag \n\nwww.rijksoverheid.nl",
  "fetched_at_utc": "2026-02-08T18:51:37Z",
  "sha256": "3015a2b513d55c3a2595ac227ceb79dfccae8b256d0a076b69168d4b000aad77",
  "meta": {
    "file_name": "Netherlands AI Act Guide.pdf",
    "file_size": 293902,
    "relative_path": "pdfs\\Netherlands AI Act Guide.pdf",
    "jina_status": 20000,
    "jina_code": 200,
    "usage": {
      "tokens": 9985
    }
  }
}