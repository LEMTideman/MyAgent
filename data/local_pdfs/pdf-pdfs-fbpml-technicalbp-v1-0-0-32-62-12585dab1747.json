{
  "doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-32-62-12585dab1747",
  "source_type": "local_pdf",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-32-62.pdf",
  "title": "FBPML_TechnicalBP_V1.0.0-32-62",
  "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n32 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) identify and mitigate risk of disproportionately unfavorable Outcomes for protected (Sub)populations; and \n\n(b) minimise the unequal distribution of Product and Model errors to prevent reinforcing and/or deriving social \n\ninequalities and/or ills, and (c) promote compliance with existing anti-discrimination laws and statutes. \n\nWhat do we mean when we refer to Fairness? \n\nFairness is a complex socio-technical challenge for which there is no single generic definition. Broadly speaking -\n\nFairness is about identifying bias in a machine learning Model or Product and mitigating discrimination with \n\nrespect to sensitive, and (usually) legally protected attributes such as ethnicity, gender, age, religion, disability, \n\nor sexual orientation. \n\nAlgorithmic discrimination can take many forms and may occur unintentionally. Machine learning Products might \n\nunfairly allocate opportunities, resources, or information, and they might fail to provide the same quality of \n\nservice to some people as they do to others. \n\nThe conversation about fairness distinguishes between group fairness and individual fairness measures. Group \n\nfairness ensures some form of statistical parity (e.g. equal calibration, equal false positive/negative rate) across \n\nprotected groups. Individual fairness requires that individuals who are similar with respect to the predictive task \n\nbe assigned similar outcomes regardless of the sensitive attribute. \n\nWhy is Fairness relevant? \n\nMachine learning Products are increasingly used to inform high-stakes decisions that impact people’s lives.It is \n\ntherefore important that ML-driven decisions do not reflect discriminatory behavior toward certain populations. \n\nIt is the responsibility of data science practitioners and business leaders to design machine learning Products \n\nthat minimizes bias and promotes inclusive representation. \n\nSome business leaders express concerns about a potential increase in the risk of reputational damage and legal \n\nallegations in case of discriminatory ‘black box’ Models. AI fairness can substantially reduce these concerns. \n\nAnother reason for taking AI fairness seriously is the development of regulatory frameworks for AI. For example, \n\nthe European Commission published a white paper on AI in 2020, which was followed in 2021 by a regulatory \n\nframework proposal for AI in the European Union. \n\nHow to apply Fairness? \n\nFairness should be considered throughout the product lifecycle. Given that AI systems are usually designed \n\nto evolve with experience, fairness should be closely monitored during deployment as well as during product \n\ndevelopment. The Technical Best Practices Guidelines provide detailed guidance into implementing fairness in \n\nyour AI products. \n\n# Section 11. Fairness & Non-\n\n# Discrimination Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n33 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) identify and mitigate risk of disproportionately unfavorable Outcomes for protected (Sub)populations; and \n\n(b) minimise the unequal distribution of Product and Model errors to prevent reinforcing and/or deriving social \n\ninequalities and/or ills, and (c) promote compliance with existing anti-discrimination laws and statutes. \n\n11.1 Product Definitions \n\nControl:  Aim: \n\n11.1.1.  (Sub)populations \n\nDefinition \n\nDefine (Sub)populations that are subject \n\nto Fairness concern, with input from \n\nDomain and/or legal experts when \n\nrelevant. \n\nTo (a) ensure that vulnerable \n\nand affected populations are \n\nappropriately identified in all \n\nsubsequent Fairness testing and \n\nModel build; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.1.2.  (Sub)population \n\nData \n\nGather data on (Sub)population \n\nmembership. If a proxy approach is used, \n\nensure the performance of the proxy is \n\nadequate in this context. \n\nTo (a) facilitate Fairness testing \n\npre- and post-Model deployment; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.3.  (Sub)population \n\nOutcome \n\nPerceptions \n\nDocument and assess whether scored \n\n(Sub)populations would view Model \n\nOutcomes as favorable or not, using \n\ninput from subject matter experts and \n\nstakeholders in affected (Sub)populations. \n\nDocument and assess any divergent views \n\namongst (Sub)populations. \n\nTo (a) ensure uniformity in (Sub) \n\npopulation outcome perception, \n\nif applicable; (b) highlight \n\nOutcome effects for different \n\n(Sub)populations; and (c) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.1.4.  Erroneous \n\nOutcome \n\nConsequence \n\nEstimation \n\nDivergence \n\nDocument and assess the results \n\nof erroneous (false positive & false \n\nnegative) outcome consequences, both \n\nreal and perceived, specifically in terms \n\nof divergence between relevant (Sub) \n\npopulations. If material divergence \n\npresent, take measures to harmonise \n\nOutcome perceptions and/or mitigate \n\nerroneous Outcome consequences in \n\nModel design, exploration, development, \n\nand production. \n\nTo (a) ensure uniformity in \n\nerroneous Outcomes for (Sub) \n\npopulations; (b) highlight outcome \n\neffects for different (Sub) \n\npopulations; and (c) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.1.5.  Positive Outcome \n\nSpread \n\nDocument and assess the degree to \n\nwhich Model positive outcomes can be \n\ndistributed to non-scored (Sub)population, \n\nwhen contextually appropriate. If present, \n\ntake measures to promote Model Outcome \n\ndistribution in Model design, exploration, \n\ndevelopment, and production. \n\nTo (a) ensure the non-prejudicial \n\nspread of positive Model Outcomes; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n34 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\n11.1.6.  Enduring Bias \n\nEstimation \n\nDocument and assess whether exclusions \n\nfrom Product usage might perpetuate \n\npre-existing societal inequalities between \n\n(Sub)populations. If present, take \n\nmeasures to mitigate societal inequalities \n\nperpetuation in Model design, exploration, \n\ndevelopment, and production. \n\nTo (a) ensure the non-prejudicial \n\nspread of Model Outcomes; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.7.  Appropriate \n\nFairness Metrics \n\nConsult Domain experts to inform \n\nwhich Fairness metrics are contextually \n\nmost appropriate for the Model when \n\nconducting Fairness testing. \n\nTo (a) ensure that fairness testing \n\nand subsequent Model changes (i) \n\nresult in outcome changes which \n\nare relevant for (Sub)populations; \n\nand/or (ii) are consistent with \n\nregulatory guidance and context-\n\nspecific best practices; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.8.  Model Implications  Document and assess the downside risks \n\nof Model misclassification/inaccuracy \n\nfor modeled populations. Use the relative \n\nseverity of these risks to inform the \n\nchoice of Fairness metrics. \n\nTo (a) ensure that improving in the \n\nchosen Fairness metrics achieves \n\nthe greatest Fairness in Model \n\ndecisioning after deployed; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.9.  Fairness Testing \n\nApproach \n\nDocument and assess the Fairness testing \n\nmethodologies that will be applied to \n\nModel and/or candidate Models, along with \n\nany applicable thresholds for statistical/ \n\npractical significance, acceptable \n\nperformance loss tolerance, amongst \n\nother metrics. \n\nTo (a) prevent Fairness testing \n\nmethodology and associated \n\nthresholds change during \n\nModel review; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\nObjective \n\nTo identify and control for Fairness and Non-Discrimination risks based on the available datasets. \n\n11.2 Exploraition \n\nControl:  Aim: \n\n11.2.1.  (Sub)population Data \n\nAccess \n\nKeep separate Model development \n\ndata and (Sub)population membership \n\ndata (if applicable Regulations allow \n\nthe possession and processing of such \n\nin the first place), especially if the use \n\nof (Sub)population data in the Model is \n\nprohibited or would introduce fairness \n\nconcerns. \n\nTo (a) guarantee that (Sub) \n\npopulation membership data does \n\nnot inadvertently leak into a Model \n\nduring development. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n35 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\n11.2.2.  Univariate \n\nAssessments \n\nDocument and perform univariate \n\nassessments of relationship between \n\n(Sub)populations and Model input \n\nFeatures, including appropriate \n\ncorrelation statistics. \n\nTo (a) identify input Feature trends \n\nassociated with (Sub)populations; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.2.3.  Prohibited Data \n\nSources \n\nDevelop and maintain an index of data \n\nsources or features that should not be \n\nmade available or utilized because of \n\nthe risks of harming (Sub)populations, \n\nspecifically Protected Classes. \n\nTo (a) prohibit the actioning of data \n\nsources that will disproportionately \n\nprejudice (Sub)populations; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.2.4.  Data \n\nRepresentativeness \n\nEnsure the membership rates of (Sub) \n\npopulations in Model development data \n\nalign with expectations and that data is \n\nrepresentative of Domain populations. \n\nTo (a) guarantee that Model \n\nperformance and Fairness testing \n\nduring model development will \n\nprovide a consistent picture \n\nof Model performance after \n\ndeployment; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.2.5.  (Sub)population \n\nProxies and \n\nRelationships \n\nDocument and assess the relationship \n\nbetween potential input Features and \n\n(membership of) (Sub)populations \n\nof interest based on, amongst other \n\nthings, (i) reviews with diverse Domain \n\nexperts, (ii) explicit encoding of (Sub) \n\npopulation membership, (iii) correlation \n\nanalyses, (iv) visualization methods. If \n\nrelationships exist, the concerned input \n\nFeatures should be excluded from Model \n\ndatasets, unless a convincing case can \n\nbe made that an (adapted version of) the \n\ninput Feature will not adversely affect \n\nany (Sub)populations, and document this. \n\nTo (a) prevent Model decisions \n\nbased directly or indirectly on \n\nprotected attributes or protected \n\nclass membership; (b) reduce \n\nthe risk of Model bias against \n\nrelevant (Sub)populations; (c) \n\nunderstand any differences in \n\ndata distributions across (Sub) \n\npopulations before development \n\nbegins; and (c) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\nAssociated Controls  Review the following controls with particular attention in the context of bias \n\nand fairness with respect to protected (Sub)populations: \n\nSection 12.2.2. - Missing and Bad Data Assessment. \n\nSection 13.2.4. - Selection Function; which is concerned with accurate \n\nrepresentation of (Sub)populations. \n\nSection 13.3.1. - 13.3.4.; which are concerned with the choice and definition of \n\nthe Target Feature. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n36 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n11.3.1.  Explainability (xAI) \n\n(Sub)population \n\nOutcomes \n\nKeep separate Model development data \n\nand (Sub)population membership data \n\n(if applicable Regulations allow the \n\npossession and processing of such in the \n\nfirst place), especially if the use of (Sub) \n\npopulation data in the Model is prohibited \n\nor would introduce fairness concerns. \n\nTo (a) guarantee that (Sub) \n\npopulation membership data does \n\nnot inadvertently leak into a Model \n\nduring development. \n\n11.3.2.  Model Architecture \n\nand Interpretability \n\nChoose Model architecture that maximizes \n\ninterpretability and identification \n\nof causes of unfairness. Consider \n\ndifferent methodologies within the \n\nsame Model architecture (ex. monotonic \n\nXGBoost, explainable neural networks). \n\nEvaluate whether Product Aims can be \n\naccomplished with a more interpretable \n\nModel. \n\nTo (a) provide information that can \n\nguide Model-builders; (b) ensure \n\nthat Model decisions are made in \n\nline with expectations; (c) allow \n\nProduct Subjects and/or End Users \n\nto understand why they received \n\ncorresponding Outcomes; (d) help \n\ninform the causes of Fairness \n\nissues if issues are detected; \n\nand (e) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.3.  Fairness Testing of \n\nOutcomes \n\nFocus fairness testing initially on \n\noutcomes that are immediately \n\nexperienced by (Sub)populations. For \n\nexample, if a model uses a series of \n\nsub-Models to generate a score and \n\na threshold is applied to that score to \n\ndetermine an Outcome, focus on Fairness \n\nissues related to that Outcome. If issues \n\nare identified, then diagnose the issue \n\nby moving “up-the-chain” and testing the \n\nModel score and sub-Models. \n\nTo (a) ensure that the testing \n\nperformed best reflects what will \n\nhappen when Models are deployed \n\nin the real world; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.4.  Disparate Impact \n\nTesting \n\nIf applicable, test Model(s) for disparate \n\nimpact. Evaluate whether Model(s) predict \n\na Positive Outcome at the same rate \n\nacross (Sub)populations. \n\nTo (a) ensure that (Sub)population \n\nmembers are receiving the Positive \n\nOutcome as often as their peers; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.5.  Equalized \n\nOpportunity \n\nTesting \n\nIf applicable, test Model(s) for equalized \n\nopportunity. Evaluate whether Model(s) \n\npredict a Positive Outcome for (Sub) \n\npopulation members that are actually in \n\nthe positive class at the same rates as \n\nacross (Sub)populations. \n\nTo (a) ensure that (Sub)population \n\nmembers who should receive the \n\nPositive Outcome are receiving the \n\nPositive Outcome as often as their \n\npeers; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\nObjective \n\nTo minimise the unequal distribution of Product and Model errors for (Sub)populations during Model development \n\nin the most appropriate manner. \n\n11.3. Development Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n37 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\n11.3.6.  Equalized Odds \n\nTesting \n\nIf applicable, test Model(s) for equalized \n\nodds. Evaluate whether Model(s) predict \n\na Positive & Negative Outcome for (Sub) \n\npopulation members that are actually in \n\nthe positive & negative class respectively \n\nat the same rates across (Sub)populations. \n\nTo (a) ensure that (i) protected \n\n(Sub)populations who should \n\nreceive the Positive Outcome are \n\nreceiving the Positive Outcome as \n\noften as other (Sub)populations, \n\nand (ii) protected (Sub)populations \n\nwho should not receive the Positive \n\nOutcome are not receiving the \n\nPositive Outcome as often as other \n\n(Sub)populations; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.7.  Conditional \n\nStatistical Parity \n\nTesting \n\nIf applicable, test Model(s) for conditional \n\nstatistical parity. Evaluate whether \n\nModel(s) predict a Positive Outcome at \n\nthe same rate across (Sub)populations \n\ngiven some predefined set of “legitimate \n\nexplanatory factors”. \n\nTo (a) ensure that (Sub)populations \n\nmembers are receiving the Positive \n\nOutcome just as often as (Sub) \n\npopulations with similar underlying \n\ncharacteristics; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.8.  Calibration Testing \n\nAcross (Sub) \n\npopulations \n\nIf applicable, test Model(s) for calibration. \n\nEvaluate whether (Sub)populations \n\nmembers with the same predicted \n\nOutcome have an equal probability of \n\nactually being in the positive class. \n\nTo (a) ensure that Subpopulations \n\neach have the same likelihood of \n\ndeserving the Positive Outcome \n\nfor a given Model prediction; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.9.  Differential Validity \n\nTesting \n\nIf applicable, test Model(s) for differential \n\nvalidity. Evaluate whether Model \n\nperformance varies meaningfully by (Sub) \n\npopulation, with a special focus on any \n\ngroups that are underrepresented in \n\nmodelling data. \n\nTo (a) ensure that the Model’s \n\npredictive abilities aren’t isolated in \n\nor concentrated to (Sub)population \n\nmembers; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.10.  Feature Selection \n\nFairness Review \n\nEvaluate the impact of removing or \n\nmodifying potentially problematic input \n\nFeatures on Fairness metrics and Model \n\nquality. \n\nTo (a) assess whether more fair \n\nalternative Models can be made \n\nthat fulfill Model objectives; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.11.  Modeling \n\nMethodology \n\nFairness Review \n\nEvaluate the impact of changing Modelling \n\nmethodology choices (f.e. algorithm, \n\nsegmentation, hyperparameters, etc.) on \n\nFairness metrics and Model quality. \n\nTo (a) assess whether more fair \n\nalternative Models can be made \n\nthat fulfill the Model objectives; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n38 FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo maintain operationalised Fairness at the level established during Model Development. \n\n11.4 Production \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n11.4.1.  Domain Population \n\nStability \n\nContinually assess the stability of the \n\nDomain population being scored, both in \n\nterms of its composition relative to the \n\nModel development population, and the \n\nquality of the Model by class. \n\nTo (a) ensure the continued \n\naccuracy of Fairness tests \n\nand metrics; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.4.2.  Fairness Testing \n\nSchedule \n\nDefine a policy for timing of re-\n\nassessment of Model fairness that \n\nincludes re-testing at regular intervals \n\nand/or established trigger events (e.g. \n\nany modifications to Model inputs or \n\nstructure, changes to the composition of \n\nthe modeled population, impactful policy \n\nchanges). \n\nTo (a) detect issues with Model \n\nFairness that may not have existed \n\nduring pre-deployment of the \n\nModel; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n11.4.3.  Input Data \n\nTransparency \n\nEnsure that Product Subjects have the \n\nability to observe attributes relied on \n\nin the modeling decision and correct \n\ninaccuracy. Collect data around this \n\nprocess and use it to identify issues in the \n\ndata sourcing/aggregation pipeline. \n\nTo (a) ensure that the Model is \n\nmaking decisions on accurate \n\ndata; (b) learn whether there are \n\nproblems with Model’s data assets; \n\nand (c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.4.4.  Feature Attribution  Ensure that Product Subjects can \n\nunderstand why the Model made the \n\ndecision it did, or how the Model output \n\ncontributed to the decision. Ideally, an \n\nunderstanding would include which \n\nfeatures were most important in the \n\ndecision and give some guidance as \n\nto how the subject could improve in \n\nthe eyes of the Model. (See Section 13 -\n\nRepresentativeness & Specification for \n\nfurther information.) \n\nTo (a) ensure that Product Subjects \n\n(i) have some level of trust/ \n\nunderstanding in the Model that \n\naffect them and (ii) feel that they \n\nhave agency over the process \n\nand that Model Outcomes are not \n\narbitrary. \n\n11.4.5.  Product Subject \n\nAppeal Process \n\nIncorporate a “right of appeal” procedure \n\ninto the Model’s deployment, where \n\nProduct Subjects can request a human \n\nreview of the modeling decision. Collect \n\ndata around this process and use it to \n\ninform Model design choices. \n\nTo (a) ensure that Product Subjects \n\nare, at a minimum, made aware of \n\nthe results of Model decisions; and \n\n(b) allow inaccurate predictions to \n\nbe corrected. \n\n11.4.6.  Feature attribution \n\nMonitoring \n\nAs part of regularly scheduled review, or \n\nmore frequently, monitor any changes in \n\nfeature attribution or other explainable \n\nmetric by sub-population. (See Section \n\n15 - Monitoring & Maintenance for further \n\ninformation. \n\nTo (a) detect reasons for changes \n\nin Model performance, as well \n\nas any changes earlier in the \n\ndata pipeline; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n39 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure Data Quality and prevent unintentional effects, changes and/or deviations in Product and Model \n\noutputs associated with poor Product data. \n\nWhat is Data Quality? \n\nLike many other concepts in Machine learning and data science, Data quality is something without a single and \n\nwidely accepted definition. Nonetheless, we think of -\n\nData Quality as data which is fit for use for its intended purpose and satisfies business, system and technical \n\nrequirements. \n\nIn technical terms, data quality can be a measure of its completeness, accuracy, consistency, reliability and \n\nwhether it is up-to-date. \n\nData integrity is sometimes used interchangeably with data quality. However, data integrity is a broader concept \n\nthan data quality and can encompass data quality, data governance and data protection. \n\nWhy is Data quality important? \n\nIt is not difficult for all stakeholders involved in a Project to agree that good data quality is of prime importance. \n\nBad data quality means a business or an organization may not have a good grasp on whether they are successful \n\nin meeting prior set objectives or not. Bad data quality results in poor analytical solutions, wrong insights and \n\nconclusions. This translates into inadequate response to market opportunities, an inability to timely react to \n\ncustomers’ requests, increased costs, and last, but not least, potential shortcomings in meeting compliance \n\nrequirements. In short, poor data results in poor products and poor decisions. This is undesirable. \n\nThe How of Data quality \n\nData quality is something that needs to be addressed throughout the product lifecycle, not only in the early \n\nstages of it, and not in any stage in isolation. \n\n# Section 12. Data Quality Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n40 FBPML Technical Best Practices v1.0.0. \n\n12. Data Quality - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo determine if the quality of the data shall be sufficient, or can be made sufficient, to achieve the Product \n\nDefinitions. \n\n12.1 Exploration \n\nControl:  Aim: \n\n12.1.1.  Data Definitions  Document and ensure all subtleties \n\nof definitions of all data dimensions \n\nare clear, inclusive of but not limited \n\nto gathering methods, allowed values, \n\ncollection frequency, etc. If not, acquire \n\nsuch knowledge, or discard the dimension. \n\nTo (a) assess and prevent \n\nunjustified assumptions about the \n\nmeaning of a data dimension or its \n\nvalues; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n12.1.2.  Data Modeling  Document and ensure all relationships \n\nbetween (the fields of) different datasets \n\nare clear, in the light of their Data \n\nDefinitions. (See Section 12.1.1 - Data \n\nDefinitions for further information.) If \n\nthis “Data Model” is not clear or available, \n\ncreate it, or discard the datasets. \n\nTo (a) prevent the creation and/or \n\ncombination of invalid datasets; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n12.1.3.  Missing and Bad \n\nData Assessment \n\nDocument and assess (a) the occurrence \n\nrates and (b) co-variances of missing \n\nvalues and nonsensical values throughout \n\nthe Model data. If either is significant, \n\ninvestigate causes and consider \n\ndiscarding affected data dimension(s) \n\nor commit dedicated research and \n\ndevelopment to mitigating measures for \n\naffected data dimension(s). (See Section \n\n12.3.1. - Live Data Quality for further \n\ninformation.) \n\nTo assess (a) the risk of low quality \n\ndata introducing bias to Model \n\ndata and/or Outcomes; and (b) \n\nwhether Model dataset(s) quality is \n\nsufficient for Product Definitions; \n\nand (c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n12.1.4.  Data Veracity \n\nUncertainty & \n\nPrecision \n\nDocument and assess the veracity and \n\nprecision of data. If compromised, \n\nuncertain and/or unknown, document and \n\nassess (i) the causes and sources hereof \n\nand (ii) statistical accuracy .Incorporate \n\nappropriate statistical handling \n\nprocedures, such as calibration, and \n\nappropriate control mechanisms in Model, \n\nor discard the data dimension. \n\nTo assess (a) the risk of low quality \n\ndata introducing bias to Model data \n\nand/or outcomes; (b) a priori the \n\nplausibly achievable performance; \n\n(c) whether the Model dataset(s) \n\nquality is sufficient for Product \n\nDefinitions; and (d) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n41 FBPML Technical Best Practices v1.0.0. \n\n12. Data Quality - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo determine if Model performance is affected or biased due to data quality issues. \n\n12.2 Development \n\nControl:  Aim: \n\n12.2.1.  Missing and Bad \n\nData Handling \n\nDocument and assess how missing and \n\nnonsensical data (a) are handled in the \n\nModel, through datapoint exclusion or \n\ndata imputation; (b) affect the Selection \n\nFunction through datapoint removal; (c) \n\naffect Model performance and Fairness for \n\nsubpopulations through data imputation. \n\nIf (Sub)populations are unequally affected, \n\ntake additional measures to increase \n\ndata quality and/or improve Model \n\nresilience. Consult Domain experts during \n\nassessment and mitigation. \n\nTo (a) prevent introducing bias to \n\nModel Outcomes due to low quality \n\ndata; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n12.2.2.  Error - Quality \n\nCorrelation \n\nDocument and assess whether low-quality \n\ndatapoints (those with low-confidence, \n\nuncertain, nonsensical, missing and/or \n\nimputed attributes) correlate with high \n\n(rates of) error, and how this affects \n\n(Sub)populations. If so, take additional \n\nmeasures to increase data quality and/or \n\nimprove Model performance for specific \n\n(Sub)populations. \n\nTo (a) prevent introducing bias \n\nto Model Outcomes due to low \n\nquality data; (b) whether the Model \n\ndataset(s) quality is sufficient \n\nfor Product Definition(s); and \n\n(c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nObjective \n\nTo ensure the quality of incoming data to the Product during operations. \n\n12.3 Production \n\nControl:  Aim: \n\n12.3.1.  Live Data Quality  Document and assess whether live \n\nincoming data with low quality (low-\n\nconfidence, uncertain, nonsensical, \n\nmissing and/or imputed attributes) can be \n\nhandled appropriately by the Model on the \n\nper-Data Subject level. If not, implement \n\nadditional measures, and/or re-assess \n\nvalidity of Product Definition(s) in view \n\nof non-applicability to low quality live \n\nsubsets. \n\nTo (a) assess and control that all \n\nProduct Subjects can be supported \n\nappropriately by the live Product; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n42 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) ensure that Product data and Model(s) are representative of, and accurately specified for, Product Domain \n\nas far as is reasonably practical; and (b) guard against unintentional Product and Model behaviour and Outcomes \n\nas far as is reasonably practical. \n\nWhat is Representativeness and Specification? \n\nRepresentativeness is a concept that is often used in statistics and machine learning with regards to the data \n\nwe use to train a Model. A representative data sample is a set from a larger statistical population that adequately \n\nreplicates the larger group according to a characteristic or quality under study. Put less metaphorically -\n\nRepresentativeness means the ability of the Model and its data to adequately replicate and represent that \n\ncharacteristics of its operational environment. \n\nIt should not be confused with representation learning (also known as feature learning) in machine learning. \n\nThe latter refers to a set of techniques for automatically detecting feature patterns and in fact replaces manual \n\nfeature engineering. \n\nSpecification is a less known term. In our context -\n\nSpecification refers to ensuring the appropriate degrees of freedom in the Model. \n\nFor example, we have selected the appropriate cost function for the problem at hand, the target variable is \n\nappropriate and not a proxy for what we are really interested in measuring, etc. It is like representativeness \n\nbut for the Model, and not the data. Unlike the performance robustness section, many of the controls here will \n\nbe difficult to precisely measure quantitatively. However, we should still try to consider as many scenarios as \n\npossible and minimize all risks stemming from not addressing them rigorously. \n\nWhy is Representativeness and Specification important? \n\nIf the data is not representative with relation to the goal of the Product, it will not serve us well. It will result \n\nin poor performing Models when deployed, and it will inherently contain bias (not in the fairness and non-\n\ndiscrimination sense but in relation to sampling). This can lead to misleading conclusions and unrealistic \n\nassumptions and expectations. Correct specifications on the other hand relates to selecting appropriate and \n\nrigorous features, selection function, and target, etc. This ensures that the Model we develop is rigorous, robust \n\nand has a properly specified number of parameters. \n\nThe How of Representativeness and Specification \n\nRepresentativeness and Specification is something that needs to be addressed throughout the product lifecycle, \n\nnot only in the early stages of it, and not in any stage in isolation. \n\n# Section 13. Representativeness \n\n# & Specification Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n43 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) ensure the pragmatic formulation and accurate specification of Product Definition(s); (b) minimise Model \n\nsimplifications, assumptions and ambiguities; and (c) ensure adequate vigil of the non-reducible ones throughout \n\nthe Product Lifecycle. \n\n13.1 Product Definitions \n\nControl:  Aim: \n\n13.1.1.  R&S Product \n\nDefinition(s) \n\nAssessment \n\nDocument and assess whether recorded \n\nProduct Definition(s) are complete, \n\nunambiguous and representative of \n\nintended Product Outcomes. If they are \n\nnot, refine them as much as is reasonably \n\npractical. \n\nTo (a) enable reliable execution of \n\nall further research, development \n\nand assessments; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.2.  Product \n\nAssumptions \n\nDocument and assess Product \n\nassumptions, the likelihood of their \n\nappropriateness, their continued validity, \n\nand inherent risks. \n\nTo (a) detect, mitigate and review \n\nProduct assumptions and their \n\ninherent risks; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.3.  Product \n\nSimplifications \n\nDocument and assess Product \n\nsimplifications, the likelihood of their \n\nappropriateness, and their inherent risks. \n\nTo (a) detect, mitigate and review \n\nProduct simplifications and their \n\ninherent risks; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.4.  Product Limits  Document and assess the limitations \n\nof the Product’s application and the \n\napplicability of Product Definitions. \n\nTo (a) detect and review Model \n\nlimitations in light of (i) Model \n\nassumptions and (ii) Model \n\nsimplifications; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.5.  R&S Problem \n\nDefinition Review \n\nR&S Product Definition(s) ought to be \n\nreviewed continually, specifically when \n\nsignificant Model changes occur. \n\nTo ensure that R&S Product \n\nDefinition(s) are kept up-to-\n\ndate to ensure their continued \n\neffectiveness, suitability, and \n\naccuracy; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n44 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n13.2.1.  Data Subjectivity  Document and assess whether the \n\nModel dataset(s) contain subjective \n\ncomponents. If subjective components \n\nare present, take measures to handle or \n\navoid subjectivity risks in Product and/ \n\nor Model design as much as is reasonably \n\npractical. \n\nTo (a) assess and control for the \n\naccuracy of the specification \n\nof Model inputs, manipulations, \n\nOutcomes, and interpretations \n\nto ensure the unambiguous \n\napplicability of Model(s) in Product \n\nDomain(s); and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.2.2.  Heterogeneous \n\nVariable \n\nSimplification \n\nDocument and assess whether Model \n\ndatasets contain, or Model components \n\nproduce, simplified input Features that \n\nrepresent inherently heterogeneous \n\nconcepts in Product Domains. If \n\nsimplified, take measures to reflect the \n\nheterogeneity of Product Domains as \n\nmuch as is reasonably practical. \n\nTo (a) detect, review and \n\ncontrol for the simplification of \n\nheterogeneous input Variables; \n\n(b) prevent generalization and \n\nspurious correlation; and (c) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. \n\n13.2.3.  Hidden Variables  Document and assess whether \n\nModel datasets are missing, or Model \n\ncomponents hide relevant attributes of \n\nProduct Subjects or systemic Variables \n\nwith respect to Product Domains. If \n\nhidden, obtain additional data and/ \n\nor account for the hidden Variables in \n\nmodelling as much as is reasonably \n\npractical. \n\nTo (a) assess and control for hidden \n\ncorrelations and causal relations in \n\nModel datasets and Variables and/ \n\nor risks of relations being spurious, \n\nambiguous and/or confounding; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n13.2.4.  Selection Function  Document and assess the propensity \n\nof subpopulations and subpopulation \n\nmembers to be (accurately) recorded in \n\nModel datasets, with particular care for \n\n(i) unrecorded individuals, (ii) Protected \n\nClasses, and (iii) survivorship effects. \n\nIncorporate the Selection Function \n\nin Model development and evaluation \n\nin particular during Fairness & Non-\n\nDiscrimination, Performance Robustness \n\ncontrols. \n\nTo (a) assess and control for the \n\naccuracy of Model and Model \n\ndatasets in representing (Sub) \n\npopulations; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\nObjective \n\nTo (a) ensure that Model dataset(s) correspond to the Product Definition in sufficient detail, completeness and \n\nwithout material unambiguity; and (b) to identify associated risks in order to ensure an adequate vigil throughout \n\nthe Product Lifecycle. \n\n13.2 Exploration Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n45 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\n13.2.5.  Feature \n\nConstraints \n\nEvaluate whether any constraints should \n\nbe applied to input Features, such as \n\nmonotonicity or constraints on input \n\nFeature interactions in consultation with \n\nDomain experts. If determined, utilise \n\nidentified constraints. \n\nTo (a) ensure that (i) Model \n\nOutcomes are maximally \n\ninterpretable and (ii) Model \n\nbehavior for individual Model \n\nSubjects is consistent with Domain \n\nexperience; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\nControl:  Aim: \n\n13.3.1.  Target \n\nSubjectivity \n\nDocument and assess whether the Target \n\nFeature(s) objectively represent Product \n\nDomain(s). If subjective, consider refining \n\nProduct Definition(s), choosing a different \n\nTarget Feature, or taking measures \n\nto promote the objectivity of Product \n\nOutcomes. \n\nTo (a) ensure that Product Outcomes \n\nare representative of subpopulations \n\nand applications, and are not \n\nmisinterpreted; (b) ensure that Models \n\nare optimized only and precisely \n\naccording to Product Definitions; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n13.3.2.  Target Proxies  Document and assess whether the \n\nTarget Feature(s) are proxies for the true \n\nTarget(s) of Interest in Product Domain(s). \n\nIf Target Features are proxies, take \n\nmeasures to ensure and review non-\n\ndivergence of Product Outcomes with \n\nregard to Product Definitions. \n\nTo (a) ensure that Product Outcomes \n\nare representative of subpopulations \n\nand applications, and are not \n\nmisinterpreted; (b) ensure that Models \n\nare optimized only and precisely \n\naccording to Product Definitions; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n13.3.3.  Target Proxy \n\nvs. True Target \n\nof Interest \n\nContrasting \n\nIf the Target Feature is a proxy (i) \n\ndocument and assess whether the \n\ntrue Target(s) of Interest correlate with \n\nprotected attributes and classes, including \n\nthrough hidden systemic Variables as \n\nmuch as is reasonably practical; and (ii) \n\ndocument and assess whether the true \n\nTarget(s) of Interest and the proxy Target \n\nFeature(s) correlate differently with the \n\nModel datasets. If true, take measures to \n\nmitigate this as much as is reasonably \n\npractical. \n\nTo (a) ensure that the Model design \n\nis oriented to the true Target(s) of \n\nInterest; and (b) highlight associated \n\nrisks that might occur in the lack \n\nthereof in the Product Lifecycle. \n\nObjective \n\nTo (a) ensure that Model design is sufficiently specified to represent Product Domain(s) and the Product \n\nDefinition(s) as much as is reasonably practical; and (b) minimise the risks of (i) adverse effects from the Model’s \n\noptimisation leading to unintended loopholes and local optima, and (ii) mis-balancing competing optimisation \n\nrequirements in Model design and development. \n\n13.3 Development Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n46 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\n13.3.4.  Heterogeneous \n\nTarget Variable \n\nSimplification \n\nDocument and assess whether the \n\nTarget Feature is a simplification of, or \n\ncontains a subset of, true Target(s) of \n\nInterest. If true, consider refining Product \n\nDefinitions, recovering the heterogeneity, \n\nor failing that, take measures to mitigate \n\nand review this as much as is reasonably \n\npractical. \n\nIdem Section 11.3.1-2; and to (a) detect \n\nand control for risks of generalization \n\nand spurious correlation creation. \n\n13.3.5.  Cost Function \n\nSpecification & \n\nOptimisation \n\nDocument and assess the risk propensity \n\nfor - (i) optimizing for subset of \n\nobjectives to the detriment of other \n\nProduct objectives, (ii) optimizing for \n\nOutcomes that are unintended and/or \n\nnot aligned with any Product objectives, \n\n(iii) feedback loops (when containing \n\nnested optimization loops), and (iv) \n\nModel confinement in adverse or less-\n\nthan-optimal parameter or solution \n\nspace - through Model cost function \n\nand optimisation procedures during the \n\nProduct Lifecycle. If risks occur, take \n\nmeasures to mitigate them as much as is \n\nreasonably practical. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n13.3.6.  Importance \n\nWeighting \n\nDocument and assess whether Model \n\ndata points are weighted by design or as \n\ncollateral effect. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle \n\n13.3.7.  Asymmetric \n\nError Weights \n\nDocument and assess whether Model \n\nerrors, and error rates, are weighted \n\nasymmetrically in the Model. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle \n\n13.3.8.  Feature \n\nWeighting \n\nDocument and assess whether Model \n\nfeatures are weighted by design or as \n\ncollateral effect. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n47 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n13.4.1.  Asymmetric \n\nError Costs \n\nDocument and assess whether Product \n\nDomain(s) costs produced by different \n\nModel errors types are accounted for in \n\nProduct implementation and application \n\nin software and processes. If not, take \n\nmeasures to ensure that they are. \n\nTo (a) ensure that Product \n\nDomain(s) and Product Subjects \n\nconsequences are accurately \n\nconsidered when implementing \n\nProduct outcomes; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n13.4.2.  Output \n\nInterpretation(s) \n\nDocument and assess whether Product \n\nOutcomes can be clearly, completely \n\nand unambiguously interpreted by the \n\nnon-technical parties and whether these \n\ninterpretations remain representative \n\nof Product Definition(s) and Model inner \n\nworkings. If not, take measures to ensure \n\nthat they are as much as is reasonably \n\npractical. \n\nTo (a) prevent (i) misinterpretation \n\nof Product Outcomes, (ii) the \n\napplication of Products in contexts \n\nand/or to Subjects for which \n\ntheir appropriateness and/or \n\nquality is unconfirmed, unknown, \n\nand/or unsatisfactory, (iii) the \n\nintentional and/or unintentional \n\nmisuse of Product components \n\nand Outcomes; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nObjective \n\nTo ensure that the Implementation of the Product and Model(s) align with and represent Product Definition(s) and \n\nProduct Domain(s). \n\n13.4 Production \n\n13.3.9.  Output \n\nInterpretation(s) \n\nDocument and assess whether the \n\ninterpretation of the Model Outcomes are \n\nclearly, completely and unambiguously \n\ndefined. If not, take measures to promote \n\nOutcome interpretation(s) clarity and \n\ncompleteness as much as is reasonably \n\npractical. \n\nTo (a) guard against the \n\nmisinterpretation and/or \n\nmisapplication of Model Outcomes; \n\nand (b) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n13.3.10.  Time-dependent \n\nData Modeling \n\nDocument and assess whether all time-\n\ndependent aspects of data generation \n\n(including but not limited to gathering, \n\ncalibration, cleaning, and annotation), data \n\nmodeling and data usage are specified \n\nand incorporated in Model design and \n\nProduct Definition(s). \n\nTo (a) prevent data leakage and other \n\nforms of “time traveling” information \n\nleading to inaccurate representations \n\nof the data and/or Data Subjects; and \n\n(b) highlight associated risks that \n\nmight occur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n48 FBPML Technical Best Practices v1.0.0. \n\n# Section 14. Performance \n\n# Robustness \n\nObjective: \n\nTo warrant Model Outcomes and prevent unintentional Model behaviour a priori under operational conditions as \n\nfar as is reasonably practical. \n\nWhat is Performance Robustness? \n\nModel robustness is the property of an algorithm that, when tested on a training sample and on a similar testing \n\nsample, the performance is similar. In other words, a robust model is one for which the testing error is similar to \n\nthe training error. Performance robustness takes into account prospective scenarios where (one of more) inputs \n\nor assumptions are (drastically) changed due to unforeseen circumstances and, in light of these, the ability of the \n\nModel to to still consistently generate accurate output. Put more holistically -\n\nPerformance Robustness means the ability of the Model to generate consistent, accurate results across different \n\nsampling tests and in light of changes in operational circumstances. \n\nWhy do we need Performance Robustness? \n\nA Model that is not robust will hopefully not end up being used and deployed. Good performance in training, \n\nbut significantly worse performance when tested on real data, is one of the reasons many proof-of-concepts \n\ndo not end up being utilized. A Model which is not robust will inevitably deteriorate over time. Its predictions \n\nand recommendations will deviate from the ground truth and the end users will lose trust in the Model and may \n\nstop utilizing it altogether. This is the optimistic case. More worrisome is when users of the Model continue to \n\nuse a poor performing Model and are unaware of its poor accuracy or precision, but still take it into account \n\nwhen making (important) judgement calls. In scenarios where there is no human-in-the-loop, detecting poor \n\nperformance robustness can be even more difficult and time costly. This will result in more unknown harm, which \n\nis naturally hard to detect and determine. So, it is clear that it is in everyone’s interest to ensure the Model’s \n\nperformance robustness. \n\nHow to ensure Performance Robustness? \n\nThough performance robustness needs to be of a certain level to even consider deploying the Model, it is \n\nsomething that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not \n\nin any stage in isolation. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n49 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo prevent performance loss due to Product Definition changes. \n\nObjective \n\nTo prevent performance loss due to (a) data and/or data definition instability; (b) volatile data elements; and/or (c) \n\nprospective increases in scale. \n\n14.1 Product Definitions \n\n14.2 Exploration \n\nControl:  Aim: \n\n14.1.1.  Product \n\nDefinition(s) \n\nStability \n\nDocument and assess the stability of historic \n\nand prospective Product Definition(s) and \n\nProduct Aim(s). If unstable, take measures \n\nto redefine or, failing that, to correct for or \n\nmitigate as much as is reasonably practical. \n\nTo (a) ensure that Product \n\nDefinition(s) and Models remain \n\nstable and up-to-date in light of \n\nProduct Domain Stability; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n14.1.2.  Product Domain \n\nStability \n\nDocument and assess the stability of \n\nhistoric and prospective Product Domain(s). \n\nIf unstable, revise Product Definition(s) \n\naccordingly to ensure Product consistency \n\nand stability. \n\nTo (a) ensure that Product \n\nDefinition(s) and Models remain \n\nstable and up-to-date in light of \n\nProduct Domain Stability; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nControl:  Aim: \n\n14.2.1.  Data Drift \n\nAssessment \n\nDocument and assess historic and \n\nprospective changes in data distribution, \n\ninclusive of missing and nonsensical data. If \n\ndata drift is apparent and/or expected in the \n\nfuture, implement mitigating measures as \n\nmuch as is reasonably practical. \n\nTo (a) assess and promote the \n\nstability of data distributions (data \n\ndrift); (b) determine the need for \n\ndata distributions monitoring, \n\nrisk-based mitigation strategies \n\nand responses, drift resistance \n\nand adaptation simulations and \n\noptimization, and data distribution \n\ncalibration; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.2.2.  Data Definition \n\nTemporal \n\nStability \n\nDocument and assess - both technically \n\nand conceptually - historic and prospective \n\nchanges of each data dimension definition. \n\nIf unstable, consider refining Product \n\nDefinitions and/or limiting usage of unstable \n\ndata dimensions. \n\nTo (a) assess and control for the \n\nneed for Model design adaptation \n\nbased on data definition stability; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n50 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\n14.2.3.  Outlier \n\nOccurrence \n\nRates \n\nDocument and assess outliers, their causes, \n\nand occurrence rates as a function of their \n\nlocation in data space. If numerous and \n\npersistent, include mitigating measures in \n\nModel design accordingly. \n\nTo (a) identify outliers and \n\nassess the need for Model design \n\nadaptation; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.2.4.  Selection \n\nFunction \n\nTemporal \n\nStability \n\nDocument and assess the historic and \n\nprospective behaviour of Selection \n\nFunction(s) of Model data. (See Section \n\n13.2.4. - Selection Function for more \n\ninformation.) If unstable, take measures \n\nto account for past and future changes, \n\nand/or promote the consistency and \n\nrepresentativeness of Model datasets and \n\ndata gathering as much as is reasonably \n\npractical. \n\nTo (a) assess and control for \n\nhard-to-measure changes to the \n\nrelation between Model datasets \n\nand Product Domain(s); (b) identify \n\nthe risk of hard-to-diagnose Model \n\nperformance degradation and \n\nbias throughout Product Lifecycle \n\n(to be controlled by 14.3.6); and \n\n(c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n14.2.5.  Data Generating \n\nProcess \n\nTemporal \n\nStability \n\nDocument and assess the historic and \n\nprospective behaviour of data generating \n\nprocesses, and their influence on the \n\nSelection Function. If unstable, take \n\nmeasures to account for past and future \n\nchanges and/or promote the stability and \n\nconsistency of data generation processes as \n\nmuch as is reasonably practical. \n\nTo (a) assess and control for \n\nhard-to-measure changes to the \n\nrelation between Model datasets \n\nand Product Domain(s); (b) identify \n\nthe risk of hard-to-diagnose Model \n\nperformance degradation and \n\nbias throughout Product Lifecycle \n\n(to be controlled by 14.3.6); and \n\n(c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle \n\nObjective \n\nTo characterize, determine and control for Model performance variation, risks and robustness under live \n\nconditions a priori and throughout the Product Lifecycle. \n\n14.3 Development \n\nControl:  Aim: \n\n14.3.1.  Target Feature \n\nDefinition \n\nStability \n\nDocument and assess - both technically \n\nand conceptually - the historic and \n\nprospective stability of the Target Feature \n\ndefinition. If unstable, consider refining \n\nProduct Definitions and/or choosing a \n\ndifferent Target Feature. \n\nTo (a) assess the need for Model design \n\nand Product Definition adaptation \n\nbased on Target Feature definition \n\nstability; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n51 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\n14.3.2.  Blind \n\nPerformance \n\nValidation \n\nDocument and validate that Model \n\nPerformance can always be reproduced \n\non never-before-seen hold-out data-\n\nsubsets and prove that these hold-out \n\ndata-subsets are never used to guide \n\nModel and Product design choices by \n\ncomparing Model performance on the \n\nhold-out dataset. If performance cannot \n\nbe reproduced on never-before-seen \n\nhold-out data-subset, take measures to \n\nimprove robustness and Model fitting as \n\nmuch as is reasonably practical. \n\nTo (a) ensure Model performance \n\nrobustness against insufficient \n\ngeneralization capabilities on live data \n\n(such as overfitting); and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.3.3.  Error \n\nDistributions \n\nDocument and assess error and/or \n\nresidual distributions along as many \n\ndimensions and/or subsets as is \n\npractically feasible. If distributions are \n\ntoo broad and/or too unequal between \n\nsubsets, improve Model(s). \n\nTo (a) assess and control for \n\nperformance influence of data \n\npoints and/or groups; (b) assess \n\nand control for the distribution of \n\nerrors to influence - (i) performance \n\nrobustness as a function of data drift, \n\n(ii) the systematic performance of \n\nminority data-subsets, and (iii) the \n\nrisks of unacceptable errors and/or \n\ncatastrophic failure; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.3.4.  Output Edge \n\nCases \n\nDocument and assess the causes, \n\noccurrence probabilities, overall \n\nperformance impact of Edge Cases \n\noutput by Model(s), inclusive of on Model \n\ntraining and design. If their influence \n\nis significant, improve model design. If \n\noccurrence is high, increase Model, code \n\nand data quality control. \n\nTo (a) assess and control for the \n\nimpact of Output Edge Cases on Model \n\ndesign, bugs and performance; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n14.3.5.  Performance \n\nRoot Cause \n\nAnalysis \n\nDocument and assess Model performance \n\nRoot Cause Analysis as well as its \n\ntesting method. If Root Cause Analysis \n\nis ineffective, simplify Model and/or \n\nincrease diagnostics like logging and \n\ntracking. \n\nTo (a) assess and control for Model \n\nperformance changes and assist \n\nin Model design, development, and \n\ndebugging; (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n14.3.6.  Model Drift \n\n& Model \n\nRobustness \n\nSimulations \n\nDocument and perform simulations of \n\nModel training and retraining cycles, \n\nusing historic and synthetic data. \n\nDocument and assess the effects of \n\ntemporal changes to, amongst other \n\nthings, the Selection Function, Data \n\nGenerating Process and Data Drift \n\non the drift in performance and error \n\ndistributions of said simulations. If Model \n\ndrift is apparent, document and perform \n\nfurther simulations for Model drift \n\nresponse optimization, and/or consider \n\nrefining Product Definitions. \n\nTo (a) assess and control for Model \n\npropensity for Model drift; (b) \n\ndetermine the robustness of Model \n\nperformance as a function of data \n\nchanges; (c) determine appropriate \n\nProduct response to drift; and (d) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n52 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\n14.3.7.  Catastrophic \n\nFailures \n\nDocument and assess the prevalence of \n\npredictions with High Confidence Values, \n\nbut large Evaluation Errors. If apparent, \n\nimprove Model to avoid these, and/or \n\nimplement processes to mitigate these as \n\nmuch as is reasonably practical. \n\nTo (a) assess the propensity of the \n\nModel for catastrophic failures; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n14.3.8.  Performance \n\nUncertainty \n\nand Sensitivity \n\nAnalysis \n\nDocument and assess the probability \n\ndistribution of the model performance \n\nusing cross-validation, statistical \n\nand simulation techniques under - (a) \n\nthe assumption that the distribution \n\nof training and validation data is \n\nrepresentative of the distribution of \n\nlive data; and (b) multiple realistic \n\nvariations to the Model data due to both \n\nstatistical and contextual causes. If Model \n\nperformance variation is high, improve \n\nModel and/or take measures to mitigate \n\nperformance variation impact. \n\nTo (a) assess and control for the \n\nrange of expected values of Model \n\nperformance under both constant \n\nand changing conditions; (b) assess \n\nand control for whether trained model \n\nperformance is consistent with these \n\nranges; (c) identify main sources of \n\nuncertainty and variation for further \n\ncontrol; and (d) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n14.3.9.  Outlier Handling  Document and assess the effect of \n\nvarious outlier handling procedures on \n\n(a) Performance Robustness and (b) \n\nRepresentativeness & Specification. \n\nEnsure that only procedures are \n\nimplemented that positively affect both. \n\nTo (a) ensure that outlier removal \n\nis not used to heedlessly improve \n\ntest-time performance only and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\nObjective \n\nTo ensure the future satisfaction of Product Definition(s) through the technical and functional implementation of \n\nthe Product Model(s) and systems. \n\n14.4 Production \n\nControl:  Aim: \n\n14.4.1.  Real World \n\nRobustness \n\nDocument and assess potential future \n\nchange in the applied effects of the Product, \n\nsuch as through diminishing returns and/ \n\nor psychological effects. If significant \n\nchange or decrease is expected, consider \n\nrefining Product Definitions and/or develop \n\nprocedures for mitigation. \n\nTo (a) assess and control for the \n\nvariation in applied effects of the \n\nProduct on Product Definition(s) \n\nand performance; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.4.2.  Performance \n\nStress Testing \n\nPerform and document experiments \n\ndesigned to attempt to induce failures in the \n\nProduct and/or Model, for example, but not \n\nlimited to, by supplying large quantities of or \n\nunusual data to the training or inferencing \n\nphases. \n\nTo (a) identify and control for \n\nrisks associated with operational \n\nscenario’s outside of regimes \n\nencountered during Model \n\ndevelopment. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n53 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure that Products and Models remain within acceptable operational bounds. \n\nWhat is Monitoring and maintenance? \n\nMachine learning -\n\nMonitoring refers to the processes of tracking and analysing the performance of a model over time and once it is \n\ndeployed in production \n\nIt provides early warning signals for performance issues. Maintenance is closely related to monitoring but is a \n\nmore actionable concept. \n\nMaintenance relates to the activities we need to perform upon detecting or suspecting possible deterioration in \n\nthe performance of the model. \n\nThough it’s a process closely related to Models in production, note that maintenance and monitoring steps need \n\nto be designed and addressed in early stages of the Product Lifecycle too. \n\nWhy is Monitoring and maintenance important? \n\nMonitoring and maintenance is not only important but it is a ‘must have’ for any Product that is deployed in a \n\nproduction environment. Over time, the ‘live’ data will differ in small or significant ways from the historical data \n\nused to train the Model. Trends and preferences will change too. The way certain data sources are measured and \n\ncoded will also change over time: new data sources are added, while others become unavailable. Therefore, we \n\nneed to continuously, real-time monitor the Models that are deployed. A Model that is not maintained or updated \n\nover time eventually deteriorates, makes errors and could lead to a loss of trust and varying degrees of harm (if \n\nthe domain in question is a high-stakes decision domain). \n\nThe How of Monitoring and maintenance \n\nModel monitoring and maintenance though most commonly discussed in the deployment phase, is something \n\nthat needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage \n\nin isolation. \n\n# Section 15. Monitoring & \n\n# Maintenance Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n54 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) track Model performance in production; and (b) ensure desired Model performance. \n\nObjective \n\nTo (a) define robust Product and/or Model monitoring requirements, inclusive of concerns related to Features and \n\nskews of the data; and (b) ensure the continued monitoring of Products and/or Models throughout their lifecycles. \n\n15.1 Product Definitions \n\n15.2 Exploration \n\nControl:  Aim: \n\n15.1.1.  Monitoring \n\nObjectives \n\nBased on Product Definition(s), document \n\nand assess Product and Model monitoring \n\nobjectives, inclusive of which Product \n\nand/or Model elements need close \n\nmonitoring attention, such as Model \n\ndata and code. Document and assess \n\nthe associated risks of failing to achieve \n\nModel and/or Product Monitoring \n\nObjectives. \n\nTo (a) define Product and Model \n\nmonitoring objectives; and (b) \n\nhighlight associated risks for failed \n\nmonitoring. \n\n15.1.2.  Monitoring Risks  Document and assess the associated risks \n\nof failing to achieve Monitoring Objectives. \n\nTo (a) define Product and Model \n\nmonitoring risks. \n\nControl:  Aim: \n\n15.2.1.  Data Source \n\nMismatch: \n\nTraining & \n\nProduction Data \n\nDefine and deploy methods to detect \n\nthe degree to which data sources \n\nand Features, in Model training and \n\nproduction data, match one another. If \n\nmismatch is detected, take measures to \n\nensure that data sources and Features \n\nare adequately matched in both Model \n\ntraining and production data. \n\nTo (a) reduce nonsensical predictions \n\nof the Model due to (i) missing data, (ii) \n\nlack of data incorporated, or (iii) data \n\nmeasurement scaling, encoding and/or \n\nmeaning; (b) to reduce the discrepancy \n\nbetween training and production data; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n15.2.2.  Data \n\nDefinitions and \n\nMeasurements: \n\nTraining & \n\nProduction Data \n\nDefine and deploy methods by which to \n\ndetect the degree to which data sources \n\nin Model training and production have \n\nthe same definitions and measurement \n\nscales . \n\nTo (a) reduce nonsensical predictions \n\nof the Model due to (i) missing data, (ii) \n\nlack of data incorporated, or (iii) data \n\nmeasurement scaling, encoding and/or \n\nmeaning; (b) to reduce the discrepancy \n\nbetween training and production data; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n15.2.3.  Data \n\nDependencies \n\nand Upstream \n\nChanges \n\nDerive and implement change \n\nassessments for changes in data due \n\nto - (i) one or multiple internal or external \n\nsources (partial) updates, (ii) substantial \n\nsource change, and/or (iii) changes in \n\ndata production and/or delivery. \n\nTo (a) reduce nonsensical predictions \n\nof the Model due to (i) missing data, (ii) \n\nlack of data incorporated, or (iii) data \n\nmeasurement scaling, encoding and/or \n\nmeaning; (b) to reduce the discrepancy \n\nbetween training and production data; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n55 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\n15.2.4.  Data Drift \n\nDetection \n\nDefine and deploy monitoring metrics \n\nand thresholds for detecting sudden \n\nand/or gradual, short term and/or long \n\nterm changes in data distributions, \n\ngiving priority to those that can detect \n\npast observed changes. (See Section \n\n12.2.1- Missing and Bad Data Handling \n\nfor further information). Document and \n\nassess distribution families, statistical \n\nmoments, similarity measures, trends \n\nand seasonalities. \n\nTo (a) prevent predictions from \n\ndiverging from training data and/ \n\nor Product Definitions by assessing \n\nwhether production data is \n\nrepresentative of older data; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n15.2.5.  Product and/or \n\nProduct Domain \n\nChanges: Trends \n\nand Preferences \n\nDefine and deploy (a) monitoring \n\nmethods for detecting changes in \n\nProduct Domain(s) and/or Product \n\nDefinition(s); and (b) timeframes and/ \n\nor contextual triggers for reassessment \n\nof Product Domain(s) and Product \n\nDefinition(s) continued stability. \n\nTo (a) ensure Models capture accurate, \n\nrelevant, and current trends and \n\npreferences in Product Domain(s); (b) \n\nreduce Model ‘blind spots’ and better \n\ncapture malicious events/attempts; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\nObjective \n\nTo (a) create metrics for (i) Model performance and (ii) Model performance deterioration; and (b) ensure the \n\ncontinued monitoring of Products and/or Models throughout their lifecycles. \n\n15.3 Development \n\nControl:  Aim: \n\n15.3.1.  Model \n\nPerformance \n\nDeterioration \n\nThresholds \n\nDocument, assess, and set thresholds \n\nfor Model performance deterioration in \n\nconsultation with Stakeholders. \n\nTo (a) ensure clear guidelines \n\nand indices of Model failure and \n\nperformance deterioration; (b) \n\nreduce the risk of unacknowledged \n\nModel failure and performance \n\ndeterioration; (c) reduce the \n\nlikelihood of Model decay, ensure \n\nrobustness and good performance \n\nin terms of selected metrics \n\nand scenarios; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n15.3.2.  Product \n\nContextual \n\nIndicators: \n\nModel \n\nPerformance \n\nDeterioration \n\nDocument, assess, and set Product and \n\nProduct Domain specific indicators of Model \n\nperformance deterioration, inclusive of \n\ntechnical and non-technical indicators. \n\nTo (a) ensure clear guidelines \n\nand indices of Model failure and \n\nperformance deterioration; (b) \n\nreduce the risk of unacknowledged \n\nModel failure and performance \n\ndeterioration; (c) reduce the \n\nlikelihood of Model decay, ensure \n\nrobustness and good performance \n\nin terms of selected metrics \n\nand scenarios; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n56 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\n15.3.3.  Reactive Model \n\nMaintenance \n\nIndicators \n\nDocument, assess, and set thresholds for \n\nModel failure and reactive maintenance \n\nTo (a) ensure clear guidelines \n\nand indices of Model failure and \n\nperformance deterioration; (b) \n\nreduce the risk of unacknowledged \n\nModel failure and performance \n\ndeterioration; (c) reduce the \n\nlikelihood of Model decay, ensure \n\nrobustness and good performance \n\nin terms of selected metrics \n\nand scenarios; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n15.3.4.  Awareness of \n\nfeedback loops \n\nDefine and deploy as far as is reasonably \n\npractical (a) methods to detect whether \n\nfeedback loops are occuring, and/or (b) \n\ntechnical and non-technical warning \n\nindicators for increased risk of the same. \n\nAs per Section 17 - Security: to \n\nprevent (in)direct adverse social \n\nand environmental effects as a \n\nconsequence of self-reinforcing \n\ninteractions with the Model(s); \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nObjective \n\nTo (a) identify operational maintenance metrics; and (b) ensure timely update, re-train and re-deployment of \n\nModel(s). \n\n15.4 Production \n\nControl:  Aim: \n\n15.4.1.  Operational \n\nPerformance \n\nThresholds \n\nDefine and set metrics and tolerance \n\nintervals for operational performance of \n\nModels and Products, such as, amongst \n\nother things, latencies, memory size, CPU \n\nand GPU usage. \n\nTo (a) prevent unavailable and \n\nunreliable service; (b) enable quick \n\ndetection of bugs in the code; (c) \n\nensure smooth integration of the \n\nModel with the rest of the systems; \n\nand (d) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n15.4.2.  Continuous \n\nDelivery of \n\nMetrics: Model \n\nPerformance \n\nContinuously report on and record \n\nmetrics about Model performance, \n\npredictions, errors, Features, and \n\nassociated performance metrics to \n\nrelevant Stakeholders (as decided upon \n\nin Section 13.2 _- Representativeness & \n\nSpecification: Exploration and Section \n\n13.3 - Representativeness & Specification: \n\nDevelopment). \n\nTo (a) enable rapid identification of \n\nModel decay, and/or red flags and \n\nbugs in Model and/or data pipelines; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n57 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\n15.4.3.  Model Decay & \n\nData Updates \n\nOperationalise procedures to mitigate Data \n\nDrift and/or Model decay (as described in \n\nSection 14.2 _- Performance Robustness: \n\nExploration and Section 14.3 - Performance \n\nRobustness: Development). \n\nTo (a) ensure timely implementation \n\nof any changes required in data \n\nand/or Modelling pipelines; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n15.4.4.  Model Re-\n\ntraining \n\nOperationalise procedures on how Model \n\nre-training ought to be conducted as well \n\nas approached, inclusive of, amongst other \n\nthings, -\n\n(1) when will (i) a new Model be deployed, \n\nand/or (ii) a Model with the same \n\nhyperparameters but trained on new data; \n\nand/or \n\n(2) when operationalizing re-trained \n\nModels ought they be run in parallel with \n\nolder Models and/or do to gracefully \n\ndecommission older Models. \n\nTo (a) ensure timely implementation \n\nof any changes required in data \n\nand/or Modelling pipelines; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n15.4.5.  Create \n\nContingency \n\nPlans \n\nDevelop and put in place contingency plans \n\nin case of technical failures and out-of-\n\nbounds behaviour based on (a) bounds and \n\nthreshold set in other controls; and (b) risk \n\nassessment of failure modes. \n\nTo (a) prevent adverse effects \n\nfrom failures and unexpected \n\nbehaviour by providing clear \n\ninstructions on roll-back, mitigation \n\nand remediation; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n58 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure Model functions and outputs are explainable and justifiable as far as is practically reasonable in \n\norder to (a) foster explainability for Stakeholders, (b) promote Model trust, (c) facilitate Model debugging and \n\nunderstanding, and (d) promote compliance with existing laws and statutes. \n\nWhat do we mean when we refer to Explainability? \n\nThere is not one agreed-upon definition of explainability but the working definition we have adopted is that \n\nExplainability refers to making the behavior and decisions of a complex machine learning model understandable \n\nto humans. \n\nClosely related to the concept of explainability is interpretability. Interpretability refers to the degree to which a \n\nhuman can inherently understand the cause of a Model’s decision. In other words, interpretability relates to using \n\nModels that are transparent and can be inherently understood by humans; while explainability concerns making \n\ncomplex, non-transparent models understandable to humans. Many researchers and practitioners use the terms \n\ninterchangeably. \n\nTransparency is another closely related concept to explainability and interpretability. It is the broadest of the \n\nthree. Transparency refers to the openness of the workings and/or processes and/or features of data, Models \n\nand the overall project (the Product). Transparency can be both comprehensible or incomprehensible depending \n\non its content. Transparency does not necessarily mean comprehension: this is important and why it differs \n\nfrom explainability and interpretability. Again, transparency just refers to the openness of the workings and/or \n\nprocesses and/or features of data, Models and the overall project - whether technical or not. \n\nWhy is Explainability relevant? \n\nWhen we talk about machine learning used for high-stakes decisions, there is a strong agreement that it is \n\nextremely important for the public and for machine learning practitioners to understand the inner workings and \n\ndecision-making of Models. This is because through such understandings, we can ensure that machine learning \n\nis done fairly or, rather, that it does not generate unfair or harmful consequences. To put it more simply, we can \n\nensure human oversight and correction over machine learning operations. Explainability also is very important \n\nfor promoting trust and social acceptance of machine learning. People do not often trust and accept things they \n\ndo not understand. Through explainability, we can help people understand machine learning and, in turn, trust it. \n\nHow to apply Explainability? \n\nIn order to generate thorough and thoughtful explainability, it must be considered continuously throughout all \n\nstages of the product lifecycle. This means that explainability must be addressed at the (a) Product Definition(s), \n\n(b) Exploration, (c) Development and (d) Production stages of machine learning operations. \n\n# Section 16. Explainability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n59 FBPML Technical Best Practices v1.0.0. \n\n16. Explainability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) ensure the transparency of Product Definitions; (b) foster multi-stakeholder buy-in through explanations; \n\nand (c) reduce ethical risks in Product Definition(s) decision-making and Model Runs. \n\n16.1 Product Definitions \n\nControl:  Aim: \n\n16.1.1.  Explainability \n\nAims \n\nHaving consideration for (a) Product \n\nDefinition(s), (b) the explanations and/or \n\ntransparency sought, (c) the Model adopted, \n\nand (d) datasets used, document and assess \n\nthe explainability aims of the Model. \n\nTo (a) clearly document the \n\nexplainability and transparency \n\naims of the Model; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.1.2.  Explainability \n\nStakeholder \n\nDocument and assess the internal and \n\nexternal Stakeholders affected by the Model. \n\nTo identify the Model explainability \n\nStakeholders; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.1.3.  Explainability \n\nRisks \n\nAssessment \n\nDocument and assess the individual risks \n\nof failing to provide model explainability, \n\ninclusive of a legal liability and Explainability \n\nStakeholders mistrust. \n\nTo identify the risks of failing \n\nto provide Model explainability; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n16.1.4.  Legal \n\nRequirements \n\nfor \n\nInterpretability \n\nDocument and assess any specific \n\nlegal requirements for Explainability in \n\nconsultation with legal experts. \n\nTo (a) ensure that minimum \n\nstandards for explainability are \n\nmet and legal risk is addressed; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n16.1.5.  Explainability \n\nRequirements \n\nDocument and assess the explainability \n\nand transparency requirements and \n\nlevels in light of (a) Explainability Aims, \n\n(b) Explainability Stakeholders, and (c) \n\nExplainability Risks, taking care that the \n\nelicitation of said requirements involves \n\nappropriate guidance, education and \n\nunderstanding of Stakeholders. \n\nTo (a) clearly document the \n\nexplainability requirements of the \n\nModel; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n60 FBPML Technical Best Practices v1.0.0. \n\n16. Explainability - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n16.2.1.  Stakeholder \n\nAppraisal \n\nDocument and conduct (a) ad-hoc \n\ninterviews, (b) structured surveys and/ \n\nor (c) workshops with Explainability \n\nStakeholders about their Model and \n\nProduct concerns and literacy. \n\nTo (a) generate Explainability \n\nStakeholders analytics in order to map \n\nModel explainability requirements and \n\ndemands; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n16.2.2.  Stakeholder \n\nAppraisal \n\nAnalysis \n\nDocument, analyse and map the \n\noutcomes of the Stakeholder Appraisal \n\nagainst the Explainability Aims and \n\nExplainability Risks. \n\nTo (a) map and analyse Model \n\nexplainability requirements and \n\ndemands in light of the needs of \n\nExplainability Stakeholders; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n16.2.3.  Explainability \n\nMatrix \n\nDocument, assess, and derive a matrix \n\nevaluating and ranking the metrics and/ \n\nor criteria of explanations needed for \n\nbased on the (a) Stakeholder Appraisal \n\nAnalyse, (b) Explainability Aims, (c) \n\nExplainability Risks, and (d) Explainability \n\nRequirements, inclusive of explanations \n\naccuracy, fidelity, consistency, stability, \n\ncomprehensibility, certainty, and \n\nrepresentativeness. \n\nTo (a) derive a clear matrix from \n\nwhich to assess Model explainability \n\nrequirements; and (b) highlight \n\nassociated risks that might occur in the \n\nProduct Lifecycle. \n\n16.2.4.  Explainability \n\nFeature \n\nSelection \n\nDocument and analyse the degree of \n\nFeature explainability needed in light of \n\nthe Explainability Matrix. \n\nTo (a) identify the requisite degree of \n\nFeature explainability needed; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle, such \n\nas later stage Model retraining due to \n\nfeature ambiguity. \n\n16.2.5.  Explainability \n\nModelling \n\nMapping & \n\nAnalysis \n\nDocument and analyse the technical \n\nneeds of Model explainability in light \n\nof the Explainability Matrix, inclusive \n\nof considerations of global vs. local \n\nexplainability and/or pre-modelling \n\nexplainability, modelling explainability, \n\nand post-hoc modelling explainability \n\nTo (a) identify the technical needs \n\nof the Explainability Matrix; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n16.2.6.  Explanation \n\nFrequency \n\n& Delivery \n\nAssessment \n\nDocument and assess the frequency, \n\nmost suitable and practically reasonable \n\nmethods of communicating Model \n\nexplainability in light of the Explainability \n\nMatrix and Stakeholder Appraisal \n\nAnalysis. \n\nTo (a) identify the most appropriate \n\nmethod of communicating Model \n\nexplainability in order to promote \n\nexplainability comprehension; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\nObjective \n\nTo identify and document Model explainability and transparency requirements, inclusive of Stakeholder needs. \n\n16.2 Exploration Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n61 FBPML Technical Best Practices v1.0.0. \n\n16. Explainability - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n16.3.1.  Explainability \n\nFeature \n\nSelection \n\nAssessment \n\nConduct a Feature analysis of the \n\nExplainability Feature Selection in order to \n\nremove correlated and dependent Features. \n\nTo (a) interrogate the assumption \n\nof zero Feature dependency in \n\nexplainability modelling; (b) prevent \n\nmisleading Model explainability \n\nand transparency; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.3.2.  Global \n\nExplainability \n\nModel Run \n\nDocument and run as many types of global \n\nexplainability Models as is reasonably \n\npractical, such as Feature importances, \n\nFeature interactions, global surrogate \n\nModels, perturbation-based techniques \n\nor gradient-based techniques. When \n\nthere is doubt about the stability of the \n\ntechniques being used, test their quality \n\nthrough alternative parameterizations or by \n\ncomparing across techniques. \n\nTo (a) generate global explainability \n\nof the model; (b) help promote \n\nmodel debugging; (c) ensure \n\nexplainability fidelity and stability \n\nthrough numerous explainability \n\nmodel runs; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.3.3.  Local \n\nExplainability \n\nModel Run \n\nDocument and run as many types of local \n\nexplainability Models as is reasonably \n\npractical, such as perturbation-based \n\ntechniques or gradient-based techniques \n\nor, for more specific examples, Local \n\nInterpretable Model-Agnostic Explanations \n\n(LIME), SHAP values, Anchor explanations \n\namongst others. When there is doubt \n\nabout the stability of the techniques being \n\nused, test their quality through alternative \n\nparameterizations or by comparing across \n\ntechniques. \n\nTo (a) generate global explainability \n\nof the model; (b) help promote \n\nmodel debugging; (c) ensure \n\nexplainability fidelity and stability \n\nthrough numerous explainability \n\nmodel runs; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.3.4.  Visual \n\nExplanations \n\nAssessment \n\nDevelop visual aids to present and represent \n\nModel explainability and transparency \n\ninsights, such as Tabular Graphics, Partial \n\nDependency Plots, Individual Conditional \n\nExpectations, and/or Accumulated Local \n\nEffects plot. \n\nTo promote explainability \n\ncomprehension. \n\n16.3.5.  Example-based \n\nand Contrastive \n\nExplanations \n\nAssessment \n\nDevelop example-based and contrastive \n\nexplanations to present and represent \n\nModel explainability insights, such as the \n\nunderlying distribution of the data or select \n\nparticular instances. \n\nTo promote explainability \n\ncomprehension, such as of \n\ncomplex data distributions and/ \n\nor datasets for Explainability \n\naudiences. \n\nObjective \n\nTo ensure that Model design represents the explainability requirements and demands of transparency aims as \n\nmuch as is reasonably practical. \n\n16.3 Development Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n62 FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo monitor and track the performance of the explanations and trigger when any of the explainability approaches \n\nneed to be re-trained. \n\n16.4 Production \n\nControl:  Aim: \n\n16.4.1.  Explainability \n\nModel \n\nThresholds \n\nSet clear performance thresholds and \n\nlimitations for explainability Model(s). \n\nTo (a) define parameters for \n\nthe continued suitability and \n\nperformance of explainability \n\nModel(s); and (b) highlight \n\nassociated risks. \n\n16.4.2.  Explainability \n\nModel Review & \n\nMonitoring \n\nPeriodically, or when significant Model \n\nchanges occur, review implemented \n\nexplainability Model(s) in light of \n\nExplainability Model Thresholds. \n\nTo (a) ensure the continued \n\nsuitability and performance of \n\nexplainability Model(s) and their \n\nexplanations; and (b) highlight \n\nassociated risks. \n\n16.4.3.  Explanation \n\nTracking & \n\nMonitoring \n\nDocument and conduct (a) ad-hoc \n\ninterviews, (b) structured surveys, and/or (c) \n\nworkshops with Explainability Stakeholders \n\non explanations provided and adjust \n\noutcomes in Section 14 - Performance \n\nRobustness accordingly. \n\nTo ensure the continued \n\neffectiveness and suitability of \n\nprovided Model explanations.",
  "fetched_at_utc": "2026-02-08T18:50:45Z",
  "sha256": "12585dab1747c66e4581210aea88c74799e2f45c82c2f1f703f702da76123171",
  "meta": {
    "file_name": "FBPML_TechnicalBP_V1.0.0-32-62.pdf",
    "file_size": 945946,
    "relative_path": "pdfs\\FBPML_TechnicalBP_V1.0.0-32-62.pdf",
    "jina_status": 20000,
    "jina_code": 200,
    "usage": {
      "tokens": 20487
    }
  }
}