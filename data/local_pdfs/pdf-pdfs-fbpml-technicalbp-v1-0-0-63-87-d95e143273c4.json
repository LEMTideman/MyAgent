{
  "doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-63-87-d95e143273c4",
  "source_type": "local_pdf",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-63-87.pdf",
  "title": "FBPML_TechnicalBP_V1.0.0-63-87",
  "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n63 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) prevent adversarial actions against, and encourage graceful failures for, Products and/or Models; (b) \n\navert malicious extraction of Models, data and/or intellectual property; (c) prevent Model based physical and/or \n\nirreparable harms; and (d) prevent erosion of trust in Outputs or methods. \n\nWhat do we mean when we refer to Security? \n\nSecurity is broadly defined as the state of being free from danger or threat. Building on this definition, within the \n\ncontext of machine learning -\n\nSecurity refers to the state of ensuring that machine learning Products and/or Models are free from adversarial \n\ndanger, threat or attacks. \n\nAdversarial danger, threat or attacks are understood as the malicious intent to negatively impact machine \n\nlearning Products’ and/or Models’ functionality and/or metrics without organisation consent, whether threatened \n\nor actualised. If an organisation does consent to any such activity, this is - rather - a form of penetration testing \n\nand/or security analysis, as opposed to an adversarial danger, threat or attack. \n\nWhy is Security relevant? \n\nMachine learning Product and/or Model security is imperative to ensure operational robust performance. Without \n\nthe ability to secure the Product’s and/or Model’s integrity from adversarial danger, threat or attack, malicious \n\nthird parties can use an organisation’s Products and Models to either unlawfully enrich themselves or, more \n\nseriously, cause operational environment harms, including death and/or destruction. These are intolerable risks \n\nas they undermine organisation, societal and machine learning trust and confidence. \n\nHow to apply Security? \n\nIn order to generate thorough and thoughtful security, it must be considered continuously throughout all \n\nstages of the product lifecycle. This means that security must be addressed at the (a) Product Definition(s), (b) \n\nExploration & Development, (c) Production and (d) Confidence & Trust stages of machine learning operations. \n\n# Section 17. Security Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n64 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo identify and control for Adversarial risks and motives based on Product Definition, characterized by adversary \n\ngoals. \n\n17.1 Product Definitions \n\nControl:  Aim: \n\n17.1.1.  Exfiltration \n\nAttacks \n\nDocument and assess whether the data \n\nemployed and gathered by the Product, and \n\nthe intellectual property generated possess \n\nvalue for potential adversarial actors. \n\nTo (a) identify the risks associated \n\nwith (i) Product Subject physical, \n\nfinancial, social and psychological \n\nwellbeing, and (ii) Organization \n\nfinancial wellbeing; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n17.1.2.  Evasion Attacks  Document and assess whether Product \n\nSubjects gain advantage from evading \n\nand/or manipulating the Product Outputs. \n\nDocument and assess whether adversarial \n\nactors stand to gain advantage in \n\nmanipulating Product Subject by evading \n\nand/or manipulating Product Output. \n\nTo (a) identify the risks associated \n\nwith Product Output manipulation \n\nin regard to malicious and \n\nnefarious motives; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n17.1.3.  Targeted \n\nSabotage \n\nDocument and assess whether adversarial \n\nactors can cause harm to specific targeted \n\nProduct Subjects by manipulating Product \n\nOutputs. \n\nDocument and assess whether \n\nadversarial actors can cause \n\nharm to specific targeted Product \n\nSubjects by manipulating Product \n\nOutputs. \n\n17.1.4.  Performance \n\nDegradation \n\nAttack \n\nDocument and assess whether a malicious \n\nperformance degradation for a specific (Sub) \n\npopulation can cause harm to that (Sub) \n\npopulation. Document and assess whether \n\ngeneral performance degradation can cause \n\nharm to society, Product Subjects, the \n\nOrganization, the Domain and/or the field of \n\nMachine Learning. \n\nTo (a) identify the risks in \n\nregard to (i) Product Subjects’ \n\nphysical, financial, social and \n\npsychological wellbeing, (ii) the \n\nOrganization’s financial and \n\nreputational wellbeing, (iii) society-\n\nwide environmental, social and \n\neconomic wellbeing, and (iv) the \n\nDomains’ reputational wellbeing; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n65 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\n17.2.1.  Data Poisoning \n\nAssessment \n\nDocument and assess the ease and extent with \n\nwhich adversarial actors may influence training \n\ndata through manipulating and/or introducing \n\n- (i) raw data; (ii) annotation processes; (iii) \n\nnew data points; (iv) data gathering systems \n\n(like sensors); (v) metadata; and/or (vi) multiple \n\ncomponents thereof simultaneously. If this \n\nconstitutes an elevated risk, document, assess \n\nand implement measurements that can be taken \n\nto detect and/or prevent the above manipulation \n\nof training data. \n\nTo (a) prevent adversarial \n\nactors from seeding \n\nsusceptibility to Evasion \n\nAttacks, Targeted Sabotage \n\nand Performance Degradation \n\nAttacks by way of (i) \n\nintroducing hard to detect \n\ntriggers, (ii) increasing \n\nnoise, and/or (iii) occluding \n\nor otherwise degrading \n\ninformation content; and (b) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. \n\n17.2.2.  Public Datasets  Employ public datasets whose characteristics \n\nand Error Rates are well known as a benchmark \n\nand/or make the Product evaluation results \n\npublic. \n\nTo (a) increase the probability \n\nof detection adversarial \n\nattacks, such as Data \n\nPoisoning, by enabling \n\ncomparison with and by public \n\nresources; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.3.  Data Exfiltration \n\nSusceptibility \n\nDocument and assess the susceptibility of the \n\nModel to data Exfiltration Attacks through - (i) \n\nthe leakage of (parts of) input data through \n\nModel Output; (ii) Model memorization of training \n\ndata that may be exposed through Model output; \n\n(iii) the inclusion by design of (some) training \n\ndata in stored Model artifacts; and/or (iv) \n\nrepeated querying of the Model. \n\nTo (a) warrant and control the \n\nrisk of Model data theft; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.2.4.  Model \n\nExfiltration \n\nSusceptibility \n\nDocument and assess the susceptibility of \n\nModels to Exfiltration Attacks with the aim of \n\nobtaining a copy, or approximation of, the Model \n\nor other Organization intellectual property, \n\nthrough repeated querying of the Model and \n\nanalysing the obtained results and confidence \n\nscores. \n\nTo (a) warrant and control the \n\nrisk of Model and intellectual \n\nproperty theft; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\nObjective \n\nTo identify and control for Adversarial Risks based on and originating in Model properties and/or Model data \n\nproperties. \n\n17.2. Exploration & Development \n\nControl:  Aim: Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n66 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\n17.2.5.  Exfiltration \n\nDefence \n\nTo reduce susceptibility of Exfiltration Attacks, \n\n(a) make Exfiltration Attacks computationally \n\nexpensive; (b) remove as much as possible \n\ninformation from Model Output; (c) add noise \n\nto Model Outputs through techniques such \n\nas differential privacy; (d) limit querying \n\npossibilities in volume and/or scope; and/or (e) \n\nchange Model architecture. \n\nTo (a) warrant and control the \n\nrisk of Exfiltration Attacks; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.2.6.  Adversarial \n\nInput \n\nSusceptibility \n\nDocument and assess the susceptibility \n\nof Models to be effectively influenced by \n\nmanipulated (inferencing) input. Reduce \n\nthis susceptibility by (a) increasing the \n\nrepresentational robustness (f.e. through \n\nmore complete embeddings or latent space \n\nrepresentation); and/or (b) applying robust \n\ntransformations (possibly cryptographic) and \n\ncleaning. \n\nTo (a) warrant the control of the \n\nrisk of Evasion and Sabotage \n\nAttacks, including Adversarial \n\nExamples; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.7.  Filtering \n\nSusceptibility \n\nIf sufficient potential motive has been \n\ndetermined for adversarial attack, document \n\nand assess the specific susceptibility of the \n\npre-processing filtering procedures of Models \n\nbeing evaded by tailored inputs, based on the \n\ninformation available to an adversarial attacker \n\nabout these procedures; in addition to the \n\ngeneral Susceptibility Assessment. Increase the \n\nrobustness of this filtering as far as practically \n\nfeasible. \n\nTo (a) warrant the control of the \n\nrisk of Evasion and Sabotage \n\nAttacks, including Adversarial \n\nExamples; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.8.  Training \n\nSusceptibility \n\nIf sufficient potential motives have been \n\ndetermined for adversarial attack, document \n\nand assess the specific susceptibility of Model \n\ntraining to attack through the manipulation of (a) \n\nthe partitioning of train, validation and test sets, \n\nand/or (b) Models’ hyperparameters; in addition \n\nto the general Susceptibility Assessment. \n\nImplement more strict access control on \n\nproduction-grade training and hyperparameter \n\noptimization procedures. \n\nTo (a) warrant the control of \n\nthe risk of Evasion, Sabotage \n\nand Performance Degradation \n\nAttacks; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.9.  Adversarial \n\nExample \n\nSusceptibility \n\nIf sufficient potential motives have been \n\ndetermined for adversarial attack, document and \n\nassess the specific susceptibility of Models to \n\nAdversarial Examples by considering - (a) sparse \n\nor empty regions of the input space, and/or (b) \n\nModel architectures; in addition to the general \n\nSusceptibility Assessment. Document and \n\nimplement specific protective measures, such \n\nas but not limited to adversarial training. \n\nTo (a) warrant the control of \n\nthe risk of Evasion Attacks, \n\nspecifically Adversarial \n\nExamples; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n67 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\n17.2.10.  Adversarial \n\nDefence \n\nIf sufficient potential motive and susceptibility \n\nto adversarial attacks have been determined, \n\nimplement as far as reasonably practical \n\n- (a) data testing methods for detection of \n\noutside influence on input and Output Data; \n\n(b) reproducibility; (c) increase redundancy \n\nby incorporating multimodal input; and/or (d) \n\nperiodic resets or cleaning of Models and data. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.11.  General \n\nSusceptibility -\n\nInformation \n\nDocument, assess and control the general \n\nsusceptibility to attack due to information \n\nobtainable by attackers, by considering (a) \n\nsensitivity to input noise and/or noise as \n\na protective measure; (b) the amount of \n\ninformation an adversarial actor may obtain \n\nfrom over-extensive logging; and/or (c) whether \n\nproviding confidence scores as Output is \n\nbeneficial to adversarial actors. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.12.  General \n\nSusceptibility -\n\nExploitability \n\nDocument, assess and control the general \n\nModel susceptibility to attack due to exploitable \n\nproperties of Models, considering (a) overfit \n\nor highly sensitivity Models and Model \n\nhyperparameters are easier to attack; (b) an \n\nover-reliance on gradient methods that make \n\nModels more predictable and inspectable; (c) \n\nModels may be pushed past their applicability \n\nboundaries if input is not validated; and (d) non-\n\nrandom random number generators might be \n\nreplaced by cryptographically secure random \n\nnumber generators. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.13.  General \n\nSusceptibility -\n\nDetection \n\nDocument, assess and control the capability to \n\ndetect attacks through the ability to understand \n\nwhen Model behaviour is anomalous by (a) \n\ndecreasing Model opaqueness, and/or (b) \n\nincreasing Model robustness. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.14.  Open Source \n\nand Transfer \n\nLearning \n\nVulnerability \n\nDocument the correspondence between \n\npotential attack motives and attack \n\nsusceptibility posed by using, re-using or \n\nemploying for transfer learning open source \n\nModels, Model weights, and/or Model parameters \n\nthrough - (a) maliciously inserted behaviour and/ \n\nor code (“trojans”), (b) the ability of an adversarial \n\nactor to investigate and attack open source \n\nModels unhindered; and (c) improper (re-)use. \n\nConsider using non-open source Models or \n\nmaking significant changes aimed at reducing \n\nsusceptibility. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n68 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo identify and control for Adversarial Risks based on and/or originating in Models production. \n\n17.3 Production \n\nControl:  Aim: \n\n17.3.1.  IT Security  Traditional IT security practices are referred \n\nto. Areas of particular importance to ML-\n\nbased systems include - (a) backdoor access \n\nto the Product, in particular the components \n\nvulnerable to attack risk as identified in \n\nother controls; (b) remote host servers \n\nvulnerability; (c) hardened and isolated \n\nsystems; (d) malicious insiders (e)man-in-\n\nthe-middle attacks; and/or (f) denial-of-\n\nservice. \n\nTraditional IT security practices \n\nare referred to. Areas of particular \n\nimportance to ML-based systems \n\ninclude - (a) backdoor access \n\nto the Product, in particular \n\nthe components vulnerable to \n\nattack risk as identified in other \n\ncontrols; (b) remote host servers \n\nvulnerability; (c) hardened and \n\nisolated systems; (d) malicious \n\ninsiders (e)man-in-the-middle \n\nattacks; and/or (f) denial-of-\n\nservice. \n\n17.3.2.  Periodic Review \n\nand Validation \n\nIf motive and risk for Adversarial Attack is \n\nhigh, perform more stringent and frequent \n\nreview and validation activities. \n\nTo (a) warrant and control the risk \n\nof Adversarial Attacks in general \n\nby increasing detection probability \n\nand fixing vulnerabilities quickly; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.3.3.  input and Output \n\nVulnerability \n\nDocument and assess the vulnerability of \n\nthe Product and related systems to direct \n\nmanipulation of inputs and Outputs. \n\nDirect Output manipulation if possible is the \n\nmost straightforward, simplest, cheapest \n\nand hardest to detect attack \n\nTo (a) create redundancy with input \n\nand inferencing hyperparameter \n\nsusceptibility; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n17.3.4.  Defense \n\nStrength \n\nAssessment \n\nDocument and assess the strength and \n\nweaknesses of all layers of defense against \n\nattacks and identify the weakest links. \n\nTo (a) build defense in depth; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n69 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n17.4.1.  Trust Erosion  Document and assess the potential impact \n\non trust from adversarial and defacement \n\nattacks, and establish a strategy to mitigate \n\ntrust erosion in case of successful attacks. \n\nTo (a) prevent erosion of trust in \n\nProduct Outputs, the Product, the \n\nOrganization, and/or Domains from \n\npreventing beneficial Products \n\nand technologies to be employed; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.4.2.  Confidence  Document and assess the degree of over-\n\nand under-confidence in the Product output \n\nby Product Team, Stakeholder(s) and End \n\nUsers. Encourage an appropriate level of \n\nconfidence through education and self-\n\nreflection. \n\nNote: Underconfidence will lead to users \n\nover-ruling the Product in unexpected ways. \n\nOverconfidence leads to lower scrutiny and \n\ntherefore lowers the chance of detection \n\nand prevention of attacks. \n\nTo (a) balance the risk of \n\ncompromising Product effects \n\nagainst reduced vigilance; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.4.3.  Warning Fatigue  Document and assess the frequency of \n\nwarnings and alerts provided to Product \n\noperators, maintainers, and Product \n\nSubjects, and refine the thresholds and \n\nprocesses such that no over-exposure to \n\nalerts can lead to systematic ignoring of \n\nalerts. \n\nTo (a) prevent an overexposure \n\nto alerts that can lead to ignoring \n\nserious defects and incidents, \n\ncausing harm; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nObjective \n\nTo identify and control for Adversarial Risks based on and/or originating in Product trust and confidence. \n\n17.4 Confidence & Trust Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n70 \n\nFBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) prevent Model-based physical and/or irreparable harms; and (b) identify and mitigate risks due to Product \n\nfailures, including Model failures, IT failures, and process failures. \n\nWhat do we mean when we refer to Safety? \n\nWhen referring to safety in the context of machine learning we mean-\n\nSafety means the protection of the operational environment - and its subjects - from negative physical and/or \n\nother harms that might result from machine learning Products and/or Models, either directly or indirectly. \n\nPut slightly differently, when we discuss safety we are not talking about the safety of machine learning Products \n\nand Models (called, rather, security), but, instead, the operational environment within which machine learning \n\nProducts and Models operate. Specifically, the harms and risks that machine learning Products and Models might \n\ncause for these environments and their subjects. For example, an autonomous vehicle crashing and causing \n\ninjury, death, or destruction. \n\nWhy is Safety important? \n\nMachine learning Product and Model safety is imperative to ensure the integrity of the operational environment. \n\nWithout such safety, machine learning Products and Models can cause grave operational environment harms, \n\nsuch as physical injury or, at worst, death. These are intolerable risks as they undermine organisation, societal \n\nand machine learning trust and confidence. Moreover, they cause irreparable real damage in the real world. \n\nHow to apply Safety? \n\nIn order to generate thorough and thoughtful safety, it must be considered continuously throughout all stages of \n\nthe product lifecycle. This means that safety must be addressed at the (a) Product Definition(s), (b) Exploration, \n\n(c) Development and (d) Production stages of machine learning operations. \n\n# Section 18. Safety Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n71 \n\n18. Safety - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo establish the appropriate safety-oriented attitudes based on first principles and Product Definitions. \n\nObjective \n\nTo start the process of identifying,specifying and controlling (potential) risks and failures modes of the Model(s) \n\nand Product based on research and exploration, and sustain this process throughout the Product Lifecycle. \n\n18.1 Product Definitions \n\n18.2 Exploration \n\nControl:  Aim: \n\n18.1.1.  Physical and \n\nIrreparable \n\nHarm Risk \n\nDocument and assess whether any \n\nlikely failure modes can cause physical \n\nand/or irreparable harm, based on the \n\nProduct Definitions. If such is the case, \n\nwarrant increased oversight and attention \n\nthroughout the Product Lifecycle to risks \n\nand controls in general and from this section \n\nin particular. \n\nTo warrant the necessary amount of \n\ncontrol and resources throughout \n\nthe Product Lifecycle with regard \n\nto preventing and mitigating \n\nsignificant threats to individuals’ \n\nphysical, financial, social, and \n\npsychological well being. \n\n18.1.2.  Domain-first \n\nHumble Culture \n\nDocument and establish tenents for Product \n\nTeam culture to promote risk awareness \n\nand prevent blind spots, inclusive of (a) put \n\nDomain expertise central; (b) never assume \n\nonly positive effects; (c) admit uncertainty \n\nwhen assessing impacts. \n\nTo promote risk awareness and \n\nprevent blindspots in analysing \n\nfailure modes and other safety \n\nrelated controls and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nControl:  Aim: \n\n18.2.1.  Forecast Failure \n\nModes \n\n(a) Document and assess continuously \n\nthroughout the Product Lifecycle all \n\npotential failure modes that can be \n\nidentified through - (i) researching past \n\nfailures; and/or (ii) interrogating all \n\ncomponents or product and context with \n\nan open mind; (b) rank identified failure \n\nmodes according to likelihood and severity; \n\n(c) prepare for and mitigate these risks as \n\nfar as is reasonably practical in order of risk \n\nthroughout the Product Lifecycle. \n\nTo (a) reduce harmful \n\nconsequences of failures through \n\nanticipation and preparation; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n18.2.2.  Prediction \n\nLimits \n\nDocument and assess with a diversity of \n\nStakeholders the real limitations on the \n\nProduct and Model Outcomes that ought be \n\nstrictly enforced in order to prevent physical \n\nand/or irreparable harm and/or other Failure \n\nModes. \n\nTo prevent Model and Product \n\nOutcomes from violating clear, \n\nfixed and safe operating bounds. \n\n18.2.3.  Surprise Diary  Document continuously throughout the \n\nProduct Lifecycle all surprising findings and \n\noccurrences. \n\nTo discover and subsequently \n\ncontrol for previously unknown or \n\nunanticipated failures modes. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n72 \n\n18. Safety - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo control Safety risks and failure modes based on testing and controlling Model and Product technical \n\ncomponents. \n\nObjective \n\nTo control for Safety risks and failure modes and prevent physical and/or irreparable harm by performing \n\nassessments and implementing measures at the systemic and organisational level. \n\n18.3 Development \n\n18.4 Production \n\nControl:  Aim: \n\n18.3.1.  General \n\nIT Testing \n\nPractices \n\nAdhere to all traditional IT/Software Testing \n\nbest practices. \n\nTo warrant and control the risk of \n\nfailures due to code, software and \n\nother IT mistakes in general. \n\n18.3.2.  Testing by \n\nDomain Experts \n\nDocument and perform testing of the \n\nModel(s) and Product by Domain experts. \n\nTo warrant that Product and \n\nProduct Safety are tested against \n\nthe most relevant requirements and \n\nprevent blind spots caused by lack \n\nof multidisciplinarity. \n\n18.3.3.  Algorithm \n\nBenchmarking \n\nDocument and perform benchmark testing \n\nof Models and Model code against well-\n\nknown, trusted and/or simpler Models/code. \n\nTo warrant the correct \n\nimplementation of Models and \n\ncode, and safeguard reproducibility \n\nin general. \n\nControl:  Aim: \n\n18.4.1.  System Failure \n\nPropagation \n\nDocument and assess how failures in \n\nModels and Product components propagate \n\nto other components and other systems, \n\nand what damage they may cause there. \n\nIncorporate such information in Failure \n\nMode risk assessments and implementation \n\nof Graceful Failures and Kill Switches. \n\nTo (a) prevent blind spots and \n\ncascading failures and (b) provide \n\nessential input for creating \n\nmitigation measures with a \n\nminimum of uncontrolled side-\n\neffects. \n\n18.4.2.  Graceful Failure  Document and assess whether (i) Model \n\nerrors, (ii) Model failures, (iii) Product \n\nfailures, (iv) IT failures, and/or (v) process \n\nand implementation failures - whether \n\ncaused by attack or not - can result in \n\nphysical or irreparable harm to humans, \n\nsociety and/or the environment. If present, \n\nmitigate these risks by implementing \n\ntechnological and/or process measures that \n\nmake these failures graceful. \n\nTo identify risks and mitigating \n\nmeasures throughout the Product \n\nLifecycle with regard to significant \n\nthreats to individuals’ physical, \n\nfinancial, social and psychological \n\nwellbeing. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n73 \n\n18. Safety - FBPML Technical Best Practices v1.0.0. \n\n18.4.3.  Kill Switch  Document and implement Kill Switches \n\naccording to the findings of all previous \n\ncontrols, taking into account (a) instructions \n\nand procedures for engaging the Kill Switch; \n\n(b) who is/are responsible for engaging the \n\nKill Switch; (c) what impacts the engagement \n\nof the Kill Switch has on users, other parts of \n\nthe Product and other systems. \n\nDocument and implement Kill \n\nSwitches according to the findings \n\nof all previous controls, taking \n\ninto account (a) instructions and \n\nprocedures for engaging the Kill \n\nSwitch; (b) who is/are responsible \n\nfor engaging the Kill Switch; (c) \n\nwhat impacts the engagement of \n\nthe Kill Switch has on users, other \n\nparts of the Product and other \n\nsystems. \n\n18.4.4.  Safety Stress \n\nTesting \n\nDocument and perform scenario-based \n\nstress testing of Product in Domain, Society \n\nand Environmental contexts, for realistic but \n\nrare high-impact scenarios, recording the \n\nProduct’s reaction to and influence on the \n\nDomain, Society and Environment. \n\nTo control and prepare for worst-\n\ncase scenarios in the context \n\nof rapid and/or large changes \n\nin Domain, in Society or in \n\nEnvironment. \n\n18.4.5.  Product Incident \n\nResponse \n\nDocument and prepare Product-specific \n\nimplementation of the Organisation Incident \n\nResponse Plan insofar as that does not cover \n\nthe Product’s specific risks, if appropriate. \n\nTo (a) control for and contain \n\nProduct Incidents; (b) minimize \n\nharms stemming from Product \n\nIncidents; (c) repair harms caused \n\nby Product Incidents; and (d) \n\nincorporate lessons learned. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n74 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure (a) building desirable solutions; (b) human control over Products and Models; and (c) that individuals \n\naffected by Product and Model outputs can obtain redress. \n\nWhat is Human-centric Design? \n\nHuman-centric (or also called human-centered) Design is a creative approach to problem-solving, by involving \n\nthe human perspective in all steps of problem solving. \n\nIn the context of machine learning and the Model framework, Human-centric Design makes you stay focused on \n\nthe user when designing with ML, therefore building desirable solutions for your target users. Moreover, it also \n\nensures that the right stakeholders are involved throughout the whole design and development process and helps \n\nto properly identify the right opportunity areas. Lastly, human-centric design encompasses the extent to which \n\nhumans have control over the Model and its output as well as the degree to which humans can obtain redress, if \n\nthey are affected by the Model. \n\nWhy Human-centric Design? \n\nIncorporating Human-centric Design in the Product is vital. It ensures the Model is not built in isolation but \n\nis integrated with other problems and, most of all, that it helps solve the right questions. Having the right \n\nStakeholders (beyond the technical teams) involved in the whole Model lifecycle translates to higher trust levels \n\nin the Model, increases the rate of adoption, as well as results in more human-friendly and creative solutions. Not \n\nhaving the human-centric part of the Model will inevitably result in an inferior Model - and one which very likely \n\nend up on a ‘shelf’ and, therefore, not be applied in practice. \n\nHow to ensure Human-centric Design? \n\nHuman-centric Design is something that needs to be addressed throughout the product lifecycle, not only in the \n\nearly stages of it, and not in any stage in isolation. \n\n# Section 19. Human-Centred Design Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n75 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo discover and gain insight so that the Product and Model(s) will solve the right problems, designed for human \n\nneeds and values, before building it. \n\nObjective \n\nTo (a) cluster, (b) find insights and (c) define the right opportunity area, ensuring to focus on the right questions to \n\nsolve in preparation for the development and production phase. \n\n19.1 Product Definitions \n\n19.2 Exploration \n\nControl:  Aim: \n\n19.1.1.  Human Centered \n\nMachine \n\nLearning \n\nIncorporate the human (non-technical) \n\nperspective in your (technical) process of \n\nexploration, development and production \n\nby applying user research, design thinking, \n\nprototyping and rapid feedback, and human \n\nfactors when defining a usable product or \n\nmodel. \n\nTo (a) ensure that Product(s) and \n\nModel(s) are not only feasible and \n\nviable, but also align with a human \n\nneeds; and (b) highlight associated \n\nrisks failing such. \n\n19.1.2.  UX (or user) \n\nresearch \n\nFocus on understanding user behaviours, \n\nneeds, and motivations through observation \n\ntechniques, task analysis, user interviews, \n\nand other research methodologies. \n\nTo prevent (a) a focus on technology \n\nfrom overshadowing a focus on \n\nproblem solving; and (b) cognitive \n\nbiases from adverse influence \n\nProduct and Model design. \n\n19.1.3.  Design for \n\nHuman values \n\nInclude activities for (a) the identification \n\nof societal values, (b) deciding on a moral \n\ndeliberation approach (e.g. through \n\nalgorithms, user control or regulation), and \n\n(c) methods to link values to formal system \n\nrequirements (e.g. value sensitive design \n\n(VSD) mapping). \n\nTo reflect societal concerns about \n\nthe ethics of AI, and ensure that AI \n\nsystems are developed responsibly, \n\nincorporating social, ethical values \n\nand ensuring that systems will \n\nuphold human values. The moral \n\nquality of a technology depends on \n\nits consequences. \n\nControl:  Aim: \n\n19.2.1.  Design Thinking  Ensure an iterative development process by \n\n(a) empathize: research your users’ needs, \n\n(b) define: state your users’ most important \n\nneeds and problems to solve, (c) ideate: \n\nchallenge assumptions and create ideas, (d) \n\nprototype: start to create solutions and (e) \n\ntest: gather user feedback early and often. \n\nTo let data scientists organise and \n\nstrategise their next steps in the \n\nexploratory phase. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n76 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\n19.2.2.  Ethical \n\nassessment \n\nDiscuss with your team to what extend \n\n(a) the AI product actively or passively \n\ndiscriminates against groups of people \n\nin a harmful way; (b) everyone involved in \n\nthe development and use of the AI product \n\nunderstands, accepts and is able to \n\nexercise their rights and responsibilities; \n\n(c) the intended users of an AI product \n\ncan meaningfully understand the purpose \n\nof the product, how it works, and (where \n\napplicable) how specific decisions were \n\nmade. \n\nDiscuss with your team to what \n\nextend (a) the AI product actively \n\nor passively discriminates against \n\ngroups of people in a harmful \n\nway; (b) everyone involved in the \n\ndevelopment and use of the AI \n\nproduct understands, accepts \n\nand is able to exercise their \n\nrights and responsibilities; (c) the \n\nintended users of an AI product \n\ncan meaningfully understand the \n\npurpose of the product, how it \n\nworks, and (where applicable) how \n\nspecific decisions were made. \n\n19.2.3.  Estimating the \n\nvalue vs effort \n\nof possible \n\nopportunity \n\nareas \n\nExplore the details of what mental Models \n\nand expectations people might bring when \n\ninteracting with an ML system as well as \n\nwhat data would be needed for that system. \n\nE.g. an Impact Matrix. \n\nTo reveal the automatic \n\nassumptions people will bring to an \n\nML-powered product, to be used \n\nas prompts for a product team \n\ndiscussion or as stimuli in user \n\nresearch. (See also Section 4.11 -\n\nUser Experience Mapping.) \n\nObjective \n\nTo (a) ensure rapid iteration and targeted feedback from relevant Stakeholders, allowing a larger range of \n\npossible solutions to be considered in the selection process. (b) Increase the creativity and options considered, \n\nwhile avoiding avoiding personal biases and/or pigeon-holing a solution. \n\n19.3 Development \n\nControl:  Aim: \n\n19.3.1.  Prototyping  1: Focus on quick and minimum viable \n\nprototypes that offer enough tangibility \n\nto find out whether they solve the initial \n\nproblem or answer the initial question. \n\nDocument how test participants react and \n\nwhat assumptions they make when they \n\n“use” your mockup. \n\n2: Design a so-called ‘Wizard of Oz’ test; have \n\nparticipants interact with what they believe \n\nto be an autonomous system, but which is \n\nactually being controlled by a human (usually \n\na team member) \n\nTo gain early feedback (without \n\nhaving to actually have build an \n\nML product) needed to (a) adjust \n\nor pivot your Products(s) and/or \n\nModel(s) thus ensuring business \n\nviability; and/or (b) assess the cost \n\nand benefits of potential features \n\nwith more validity than using \n\ndummy examples or conceptual \n\ndescriptions. \n\n19.3.2.  Cost weighing of \n\nfalse positives \n\nand false \n\nnegatives \n\nWhile all errors are equal to an ML system, \n\nnot all errors are equal to all people. Discuss \n\nwith your team how mistakes of your ML \n\nmodel might affect the user’s experience of \n\nthe product. \n\nto avoid sensitive decisions being \n\ntaken (a) autonomously; or (b) \n\nwithout human consideration. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n77 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\n19.3.3.  Visual \n\nStorytelling \n\nFocus on explanatory analysis over \n\nexploratory analysis, taking the mental \n\nmodels of your target audience in account. \n\nTo avoid uninformed decisions \n\nabout your product or model by \n\nnon-technical stakeholders, when \n\npresenting complex analysis, \n\nmodels, and findings. \n\n19.3.4.  Preventative \n\nProcess Design \n\nDocument and assess whether high-risk \n\nand/or high-impact Model (sub)problems \n\nor dilemmas that are present in the Product \n\n(as determined from following the Best \n\nPractices Framework) can be mitigated or \n\navoided by applying non-Model process \n\nand implementation solutions. If non-Model \n\nsolutions are not applied, document the \n\nreasons for this, document the sustained \n\npresence of these risks and implement \n\nappropriate incident response measures. \n\nTo (a) prevent high-risk and/or \n\nhigh-impact Model (sub)problems \n\nor dilemmas through non-Model \n\nprocess and implementation \n\nsolutions; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nObjective \n\nTo ensure (a) delivering a user-friendly product, (b) increasing the adoption rate of your product or model, \n\nfocussing on (dis-)trust as main fundamental risk of ML models with (non-technical) end users \n\n19.4 Production \n\nControl:  Aim: \n\n19.4.1.  Trust; increased \n\nby design \n\nAllow for users to develop systems \n\nheuristics (ease of use) via design patterns \n\nwhile at the same time facilitate a detailed \n\nunderstanding to those who value the \n\n‘intelligent’ technology used. (See Section \n\n19.4.2 -Design for Human Error; Section \n\n19.4.3 - Algorithmic transparency; and \n\nSection 19.4.4 - Progressive disclosure for \n\nfurther information.) \n\nTo avoid (a) that the user does not \n\ntrust the outcome, and will act \n\ncounter to the design, causing at \n\nbest inefficiencies and at worst \n\nserious harms, or (b) that -trusting \n\nan application will do what we think \n\nit will do- an user can confirm their \n\ntrust is justified. \n\n19.4.2.  Design for \n\nHuman Error \n\n(a) Understand the causes of error and \n\ndesign to minimise those causes; (b) \n\nDo sensibility checks. Does the action \n\npass the “common sense” test (e.g. is the \n\nnumber is correct? - 10.000g or 10.000kg) \n\n(c) Make it possible to reverse actions - to \n\n“undo” them - or make it harder to do what \n\ncannot be reversed (eg. add constraints \n\nto block errors - either change the color \n\nto red or mention “Do you want to delete \n\nthis file? Are you sure?”). (d) make it easier \n\nfor people to discover the errors that do \n\noccur, and make them easier to correct \n\nTo (a) increase trust between the \n\nend user and the model; (b) minimize \n\nthe opportunities for errors while \n\nalso mitigating the consequences. \n\nIncrease the trust users have with \n\nyour product by design for deliberate \n\nmis-use of your model (making your \n\nmodel or product “idiot-proof”) so \n\nusers are (a) able to insert data to \n\ncompare the model outcome with \n\ntheir own expected outcome which \n\nwill increase their trust, or (b) users \n\nable to test the limitations of your \n\nproduct or model -via fake or highly \n\nunlikely data- without breaking your \n\nproduct or model. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n78 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\n19.4.3.  Algorithmic \n\ntransparency \n\nAssess the appropriate system heuristics \n\n(eg. ease of use), document all factors that \n\ninfluence the algorithmic decisions, and \n\nuse them as a design tool to make them \n\nvisible, or transparent, to users who use or \n\nare affected by the ML systems. \n\nTo (a) increase trust between the end \n\nuser and the model; (b) increase end-\n\nuser control; (c) improve acceptance \n\nrate of tool; (d) promote user \n\nlearning with complex data; and (e) \n\nenable oversight by developers. \n\n19.4.4.  Progressive \n\ndisclosure \n\nAt the point where the end-user interacts \n\nwith the Product outcomes, show them \n\nonly the initial features and/or information \n\nnecessary at that point in the interaction \n\n(thus initially hiding more advanced \n\ninterface controls). Show the secondary \n\nfeatures and/or information only when the \n\nuser requests it (show less, provide more-\n\nprinciple). \n\nTo greatly reduce unwanted \n\ncomplexity for the end-user and thus \n\npreventing (a) end-user non-adoption \n\nor misunderstanding and (b) ensuring \n\nan increased feeling of trust by the \n\nusers. \n\n19.4.5.  Human in the \n\nloop (HITL) \n\nEmbed human interaction with machine \n\nlearning systems to be able to label \n\nor correct inaccuracies in machine \n\npredictions. \n\nTo avoid the risk of the Product \n\napplying a materially detrimental or \n\ncatastrophic Product Outcome to \n\na Product Subject without human \n\nintervention. \n\n19.4.6.  Remediation  Document, assess and implement in \n\nthe Model(s), Product and Organization \n\nprocesses, requirements for enabling \n\nProduct Subjects to challenge and obtain \n\nredress for Product Outcomes applied to \n\nthem. \n\nTo ensure detrimental Product \n\nOutcomes are easily reverted when \n\nappropriate. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n79 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo prevent (in)direct adverse social and environmental effects as a consequence of interactions amongst \n\nProducts, Models, the Organisation, and the Public. \n\nWhat is Systemic Stability? \n\nModel stability is a relatively popular notion. It is usually centered at putting a bound at the Model’s generalization \n\nerror. \n\nSystemic Stability refers to the robustness of the Model (or lack thereof) stemming from the interaction between \n\nthe Model, Organization, environment and the broader public (society at large). \n\nThere are numerous potential risks that can emerge in this interaction. Many of them can impact the stability of \n\nthe Model - beyond the context of traditional performance robustness or deterioration of the Model over time. \n\nAnother way to think of it is as the extent to which the Model and/or its building blocks are susceptible to chain \n\neffects and self-reinforcing interactions between the Model, Organization, environment and society. \n\nWhy Systemic stability? \n\nSystemic stability forces one to think beyond the traditional definitions of Model stability and its potential \n\ncauses and consequences. Systemic stability ensures that we consider the effect on the Model and society \n\ndue to the interaction between the Model, the Organization, the environment and society. This means thinking \n\nabout susceptibility to feedback loops, self-fulfilling prophecies and how either of them may impact the data \n\nor the Model and its output. It, therefore, reduces risks related to deteriorated performance and minimises the \n\npropagation of undesirable biases. \n\nHow to ensure Systemic stability? \n\nIn order to ensure systemic stability, it must be considered continuously throughout all stages of the product \n\nlifecycle. This means that systemic stability must be addressed at the (a) Product Definition(s), (b) Exploration, (c) \n\nDevelopment and (d) Production stages of machine learning operations. \n\n# Section 20. System Stability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n80 \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct Definition(s). \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct exploration. \n\n20.1 Product Definitions \n\n20.2 Exploration \n\nControl:  Aim: \n\n20.1.1.  Product \n\nAssumption \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in invalidating \n\nProduct Assumptions. If so, attempt to \n\nredefine Product Assumptions to warrant \n\ntheir longevity. \n\nTo (a) prevent unpredictable social \n\nand/or environmental behaviour \n\nthrough Product Outcomes; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. \n\nControl:  Aim: \n\n20.2.1.  Selection \n\nFunction \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in invalidating \n\nProduct Assumptions. If so, attempt to \n\nredefine Product Assumptions to warrant \n\ntheir longevity. \n\nTo (a) prevent unpredictable social \n\nand/or environmental behaviour \n\nthrough Product Outcomes; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.2.2.  Data Definition \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes \n\nto the Selection Function, and whether \n\nthis is a self-reinforcing interaction. If \n\ntrue, attempt to mitigate or stabilize \n\nassociated effects through refining \n\nProduct Definition(s) and/or improving \n\nModel design and/or Product and process \n\nimplementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n81 \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. \n\n20.2.3.  Data Generating \n\nProcess \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes to \n\nthe Product data definitions, and whether \n\nthis is a self-reinforcing interaction. If \n\ntrue, attempt to mitigate or stabilize \n\nassociated effects through refining \n\nProduct Definition(s) and/or improving \n\nModel design and/or Product and process \n\nimplementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.2.4.  Data \n\nDistributions \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes to \n\nthe data generating process, and whether \n\nthis is a self-reinforcing interaction. If \n\ntrue, attempt to mitigate or stabilize \n\nassociated effects through refining \n\nProduct Definition(s) and/or improving \n\nModel design and/or Product and process \n\nimplementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.2.5.  Hidden Variable \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in the creation \n\nof new hidden Variables in the system. If \n\ntrue, record the new Variable during data \n\ngathering, or prevent the creation of the \n\nnew Variable through improved Product \n\nDefinition(s) and implementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n82 \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct development. \n\n20.3 Development \n\nControl:  Aim: \n\n20.3.1.  Target Feature \n\nDefinition \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes to the \n\nTarget Feature definition. If true, attempt to \n\nmitigate associated effects through refining \n\nProduct Output and/or Model design and/or \n\ndevelopment. \n\nTo (a) determine and prevent risk of \n\nunpredictable behaviour once the \n\nProduct outcomes are applied; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.3.2.  Optimization \n\nFeedback Loop \n\nSusceptibility \n\nDocument and assess whether the cost \n\nfunction and/or optimization algorithm \n\nexhibits a feedback loop behaviour that \n\nincludes the gathering of data that has been \n\ninfluenced by previous Model iterations, and \n\nwhether this behaviour is self-reinforcing \n\nor self-limiting. If true, attempt to mitigate \n\nassociated effects through refining \n\nProduct Output and/or Model design and/or \n\ndevelopment. \n\nIdem Section 20.2.1- Selection \n\nFunction Susceptibility \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct application. \n\n20.4 Production \n\nControl:  Aim: \n\n20.4.1.  Self-fulfilling \n\nProphecies \n\nDocument and assess whether applying \n\nProduct Outputs will result in change to \n\nProduct inputs, dependencies and/or \n\nDomain(s) (other than those mentioned in \n\ncontrols elsewhere) and whether this is a \n\nself-reinforcing interaction. If true, attempt \n\nto mitigate associated effects through \n\nrefining Product Output and/or Model design \n\nand/or development. \n\nIdem Section 20.2.1- Selection \n\nFunction Susceptibility \n\n20.4.2.  Hidden Variable \n\nDependencies \n\nDocument and assess whether the effect \n\nof applying Product Outputs depends \n\non Hidden Variables. If true, control for \n\nHidden Variables, for example through \n\nmarginalization and/or by deriving indicators \n\nfor Hidden Variables influence. \n\nTo (a) prevent diverging effects on \n\nseemingly similar individuals or \n\ndatapoints; (b) prevent or detect \n\nhigh-risk interactions; and (c) \n\nhighlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n83 \n\n20.4.3.  Society \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs results in potentially \n\nharmful societal or environmental changes, \n\nand research the possible knock-on effects \n\nas far as reasonably practical. \n\nTo (a) identify and prevent both \n\ndirect and indirect adverse effects \n\non society and the environment; \n\n(b) determine if there is a risk of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.4.4.  Domain \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs results in changes to \n\napplication Domain(s), and research \n\nthe possible knock-on effects as far as \n\nreasonably practical. \n\nTo (a) identify and prevent both \n\ndirect and indirect adverse \n\neffects on Product Domain(s); \n\n(b) determine if there is a risk of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.4.5.  Other \n\nOrganisation \n\nProducts \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs result in changes to inputs, \n\ndependencies and/or context for other \n\nOrganisation Products. If true, attempt to \n\nmitigate associated effects through refining \n\nProduct Output and/or Model design and/or \n\ndevelopment. \n\nTo (a) identify and prevent both \n\ndirect and indirect adverse \n\neffects on the Organisation or \n\nother Organisation Products; (b) \n\ndetermine if there is a risk of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n84 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure the clear and complete Traceability of Products, Models and their assets (inclusive of, amongst other \n\nthings, data, code, artifacts, output, and documentation) for as long as is reasonably practical. \n\nWhat do we mean when we refer to Product Traceability? \n\nProduct Traceability refers to the ability to identify, track and trace elements of the Product as it is designed, \n\ndeveloped, and implemented. \n\nAlternatively put, Product Traceability, is the ability to trace and track all Product elements and decisions \n\nthroughout the Product Lifecyle. It is the identification, indexing, storage, and management of each unique \n\nProduct element. \n\nWhy is Product Traceability relevant? \n\nThrough Product Traceability, each element of the Product can be easily identified and, thereafter, re-examined, \n\nand amended. This allows for greater Product accountability and transparency as, through this process, each \n\nProduct element and its developers can be identified. \n\nHow to apply Product Traceability? \n\nIn order to generate thorough and thoughtful Product Traceability, it must be considered continuously throughout \n\nall stages of the Product Lifecycle. This means that Product Traceability must be addressed at the (a) Product \n\nDefinition(s), (b) Exploration, (c) Development and (d) Production stages of Machine Learning Operations. \n\n# Section 21. Product Traceability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n85 \n\n21. Product Traceability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo document and maintain an overview of the requirements necessary to complete the Product and the \n\ninterdependencies in the Product design phase. \n\nObjective \n\nTo document the impact analysis of each requirement. \n\n21.1 Product Definitions \n\n21.2 Exploration \n\nControl:  Aim: \n\n21.3.1.  Document \n\nStorage \n\nDefine a single fixed storage solution for all \n\nreports, documents, and other traceability \n\nfiles. \n\nTo (a) prevent the usage and \n\ndissemination of outdated and/ \n\nor incorrect files; (b) prevent the \n\nhaphazards storage of Product \n\nreports, documents and/or files; \n\nand (c) highlight associated risks in \n\nthe Product Lifecycle. \n\n21.3.2.  Version Control \n\nof Documents \n\nEnsure that document changes are tracked \n\nwhen changes are made. Subsequent \n\nversions ought to list version number, \n\nauthor, date of change, and short \n\ndescription of the changes made. \n\nTo (a) track changes to any and all \n\ndocuments; (b) ensure everyone \n\nis using the same and latest \n\ndocument version; and (c) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.3.  Architectural \n\nRequirements \n\nDocument \n\nDocument which information technology \n\nresources are necessary for each element \n\nof the Product to provide a necessary \n\noverview of system requirements and \n\ncost distribution. Document the reasons \n\neach resource was chosen along with \n\njustifications. \n\nTo (a) provide clear documentation \n\nof which system resources are \n\nused, where they are used, why \n\nthey are used, and costs; and (b) \n\nhighlight associated risks in the \n\nProduct Lifecycle. \n\nControl:  Aim: \n\n21.2.1.  Document \n\nImpact Analysis \n\nof Requirements \n\nDocument and complete an impact analysis \n\non the resources and design of the Product \n\nthat can result in technical debt. \n\nTo (a) avoid Product failures due \n\nto unresolved technical debt by \n\ndocumenting potential sources of \n\nfriction and the solutions; and (b) \n\nhighlight associated risks in the \n\nProduct Lifecycle. \n\n21.2.2.  Resource \n\nTraceability \n\nMatrix \n\nProvide and keep up to date a clear view of \n\nthe relationships and interdependencies \n\nbetween resources in a documented matrix. \n\nTo (a) document and show resource \n\ncoverage for each use case; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n86 \n\n21. Product Traceability - FBPML Technical Best Practices v1.0.0. \n\n21.2.3.  Design \n\nTraceability \n\nMatrix \n\nProvide and keep up to date a clear view of \n\nthe relationships and interdependencies \n\nbetween designs and interactions thereof in \n\na documented matrix. \n\nTo (a) document design and \n\nexecution status; (b) clearly trace \n\ncurrent work and what can be \n\npursued next; and (c) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.2.4.  Results \n\nReproducibility \n\nLogs \n\nThroughout the entire Product Lifecycle, \n\nwhenever a Product component - inclusive \n\nof Models, experiments, analyses, \n\ntransformation, and evaluations - are run, all \n\nparameters, hyperparameters and results \n\nought to be logged and/or tracked, including \n\nunique identifier(s) for runs, artifacts, code \n\nand environments. \n\nTo (a) enable Absolute \n\nReproducibility; (b) validate Models \n\nand Outcomes through enablement \n\nof analysis of logs, run comparisons \n\nand reproducibility. \n\nObjective \n\nTo document and maintain the status of each product and the testing results. Ensure 100% test coverage. \n\nPrevent inconsistencies between project elements and prevent feature creep. \n\n21.3 Development \n\nControl:  Aim: \n\n21.3.1.  Backlog  Ensure that an effective backlog is \n\nmaintained to track work items and serve as \n\na historical representation and timeline of \n\ncompleted features and velocity. \n\nTo (a) ensure a comprehensive \n\nbreakdown of Features and \n\ntasks necessary to achieve full \n\nproduct functionality; (b) provide \n\nhighly readable coarse-grained \n\nversioning; and (c) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.2.  Documentation \n\nfor Technical \n\nContributors \n\nMaintain technical documentation that \n\nenables all current and future contributors \n\nto efficaciously and safely develop and \n\nmaintain the Product, including such \n\ninformation as description of each file, the \n\nworkflow, author, environments, accrued \n\ntechnical debt. \n\nTo (a) maintain Product technical \n\nintegrity by ensuring safe \n\ncontribution and maintenance \n\npractices; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.3.  Version Control \n\nof Code \n\nMaintain uninterrupted version control \n\nsystems and practices of all code used by, in \n\nand during the Product and its Lifecycle. \n\nTo (a) maintain Product technical \n\nintegrity by ensuring safe \n\ncontribution and maintenance \n\npractices; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n87 \n\n21.3.4.  Docstrings and \n\nCode Comments \n\nDocument in each function the author of \n\ncode, purpose of code, input, Output, and \n\nimprovements to be made. Document the \n\nsource of inputs and potentially a short \n\nbusiness description of data used. \n\nTo (a) ensure Model clarity as to \n\ntechnical progress; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.5.  Project Status \n\nReports \n\nEnsure that all status reports and similar \n\ncommunications to Management and \n\nStakeholders are stored and maintained, \n\ninclusive of team updates, reports to the \n\nProduct Manager, and Stakeholder reports \n\nby request. \n\nTo (a) maintain a formal written \n\nrecord of decisions, progress and \n\ncontext evolution; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21. Product Traceability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo document the observed impact of updates to the product. Document product runs and their input for \n\nreproducibility. \n\n21.4 Production \n\nControl:  Aim: \n\n21.4.1.  Version control \n\nthrough CI/CD \n\nMaintain distinct production versions \n\nto easily revert or roll back to a working \n\nprevious Product, if production issues arise. \n\nProperly set up CI/CD enables easy redeploy \n\nof any artifact and version. \n\nTo (a) provide functional Product \n\nto users at all times; (b) seamlessly \n\nredeploy Product versions if \n\nneeded; and (c) highlight associated \n\nrisks in the Product Lifecycle. \n\n21.4.2.  Data Lineage \n\nManifest \n\nUtilise a data lake for production data, \n\nintermediate results, and end results. Each \n\nstep should be documented in a manifest \n\nthat is passed from one step of the process \n\nto the next and always accompanies stored \n\ndata and results. \n\nTo (a) create a structured way for \n\ntracing where data has been, what \n\nwas done to it, and results; and (b) \n\nhighlight associated risks in the \n\nProduct Lifecycle.",
  "fetched_at_utc": "2026-02-08T19:08:01Z",
  "sha256": "d95e143273c46729361d8e462f63055d85030182d8f6e0d3a1fc96c16703e2b4",
  "meta": {
    "file_name": "FBPML_TechnicalBP_V1.0.0-63-87.pdf",
    "file_size": 1007448,
    "relative_path": "pdfs\\FBPML_TechnicalBP_V1.0.0-63-87.pdf",
    "jina_status": 20000,
    "jina_code": 200,
    "usage": {
      "tokens": 14691
    }
  }
}