{
  "doc_id": "pdf-pdfs-ai-risk-management-framework-playbook-nist-af493a99a01e",
  "source_type": "local_pdf",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Risk Management Framework Playbook - NIST.pdf",
  "title": "AI Risk Management Framework Playbook - NIST",
  "text": "AI RMF AI RMF \n\n# PLAYBOOK PLAYBOOK Table of Contents \n\nGOVERN ................................ ................................ ................................ ................................ ................................ ........... 4\n\nGOVERN 1.1 ................................ ................................ ................................ ................................ ................................ ..................... 4\n\nGOVERN 1.2 ................................ ................................ ................................ ................................ ................................ ..................... 5\n\nGOVERN 1.3 ................................ ................................ ................................ ................................ ................................ ..................... 7\n\nGOVERN 1.4 ................................ ................................ ................................ ................................ ................................ ..................... 9\n\nGOVERN 1.5 ................................ ................................ ................................ ................................ ................................ .................. 11 \n\nGOVERN 1.6 ................................ ................................ ................................ ................................ ................................ .................. 12 \n\nGOVERN 1.7 ................................ ................................ ................................ ................................ ................................ .................. 13 \n\nGOVERN 2.1 ................................ ................................ ................................ ................................ ................................ .................. 15 \n\nGOVERN 2.2 ................................ ................................ ................................ ................................ ................................ .................. 17 \n\nGOVERN 2.3 ................................ ................................ ................................ ................................ ................................ .................. 18 \n\nGOVERN 3.1 ................................ ................................ ................................ ................................ ................................ .................. 19 \n\nGOVERN 3.2 ................................ ................................ ................................ ................................ ................................ .................. 21 \n\nGOVERN 4.1 ................................ ................................ ................................ ................................ ................................ .................. 23 \n\nGOVERN 4.2 ................................ ................................ ................................ ................................ ................................ .................. 24 \n\nGOVERN 4.3 ................................ ................................ ................................ ................................ ................................ .................. 27 \n\nGOVERN 5.1 ................................ ................................ ................................ ................................ ................................ .................. 28 \n\nGOVERN 5.2 ................................ ................................ ................................ ................................ ................................ .................. 30 \n\nGOVERN 6.1 ................................ ................................ ................................ ................................ ................................ .................. 32 \n\nGOVERN 6.2 ................................ ................................ ................................ ................................ ................................ .................. 33 \n\nMANAGE ................................ ................................ ................................ ................................ ................................ ........ 35 \n\nMANAGE 1.1 ................................ ................................ ................................ ................................ ................................ ................. 35 \n\nMANAGE 1.2 ................................ ................................ ................................ ................................ ................................ ................. 36 \n\nMANAGE 1.3 ................................ ................................ ................................ ................................ ................................ ................. 37 \n\nMANAGE 1.4 ................................ ................................ ................................ ................................ ................................ ................. 39 \n\nMANAGE 2.1 ................................ ................................ ................................ ................................ ................................ ................. 40 \n\nMANAGE 2.2 ................................ ................................ ................................ ................................ ................................ ................. 42 \n\nMANAGE 2.3 ................................ ................................ ................................ ................................ ................................ ................. 48 \n\nMANAGE 2.4 ................................ ................................ ................................ ................................ ................................ ................. 49 \n\nMANAGE 3.1 ................................ ................................ ................................ ................................ ................................ ................. 51 \n\nMANAGE 3.2 ................................ ................................ ................................ ................................ ................................ ................. 52 \n\nMANAGE 4.1 ................................ ................................ ................................ ................................ ................................ ................. 53 \n\nMANAGE 4.2 ................................ ................................ ................................ ................................ ................................ ................. 54 \n\nMANAGE 4.3 ................................ ................................ ................................ ................................ ................................ ................. 56 \n\nMAP ................................ ................................ ................................ ................................ ................................ ................ 58 \n\nMAP 1.1 ................................ ................................ ................................ ................................ ................................ ........................... 58 \n\nMAP 1.2 ................................ ................................ ................................ ................................ ................................ ........................... 62 \n\nMAP 1.3 ................................ ................................ ................................ ................................ ................................ ........................... 63 \n\nMAP 1.4 ................................ ................................ ................................ ................................ ................................ ........................... 65 \n\nMAP 1.5 ................................ ................................ ................................ ................................ ................................ ........................... 66 \n\nMAP 1.6 ................................ ................................ ................................ ................................ ................................ ........................... 68 \n\nMAP 2.1 ................................ ................................ ................................ ................................ ................................ ........................... 70 MAP 2.2 ................................ ................................ ................................ ................................ ................................ ........................... 71 \n\nMAP 2.3 ................................ ................................ ................................ ................................ ................................ ........................... 74 \n\nMAP 3.1 ................................ ................................ ................................ ................................ ................................ ........................... 77 \n\nMAP 3.2 ................................ ................................ ................................ ................................ ................................ ........................... 79 \n\nMAP 3.3 ................................ ................................ ................................ ................................ ................................ ........................... 80 \n\nMAP 3.4 ................................ ................................ ................................ ................................ ................................ ........................... 82 \n\nMAP 3.5 ................................ ................................ ................................ ................................ ................................ ........................... 84 \n\nMAP 4.1 ................................ ................................ ................................ ................................ ................................ ........................... 86 \n\nMAP 4.2 ................................ ................................ ................................ ................................ ................................ ........................... 88 \n\nMAP 5.1 ................................ ................................ ................................ ................................ ................................ ........................... 89 \n\nMAP 5.2 ................................ ................................ ................................ ................................ ................................ ........................... 90 \n\nMEASURE ................................ ................................ ................................ ................................ ................................ ...... 93 \n\nMEASURE 1.1 ................................ ................................ ................................ ................................ ................................ ............... 93 \n\nMEASURE 1.2 ................................ ................................ ................................ ................................ ................................ ............... 95 \n\nMEASURE 1.3 ................................ ................................ ................................ ................................ ................................ ............... 96 \n\nMEASURE 2.1 ................................ ................................ ................................ ................................ ................................ ............... 98 \n\nMEASURE 2.2 ................................ ................................ ................................ ................................ ................................ ............... 99 \n\nMEASURE 2.3 ................................ ................................ ................................ ................................ ................................ ............ 102 \n\nMEASURE 2.4 ................................ ................................ ................................ ................................ ................................ ............ 104 \n\nMEASURE 2.5 ................................ ................................ ................................ ................................ ................................ ............ 106 \n\nMEASURE 2.6 ................................ ................................ ................................ ................................ ................................ ............ 108 \n\nMEASURE 2.7 ................................ ................................ ................................ ................................ ................................ ............ 110 \n\nMEASURE 2.8 ................................ ................................ ................................ ................................ ................................ ............ 112 \n\nMEASURE 2.9 ................................ ................................ ................................ ................................ ................................ ............ 115 \n\nMEASURE 2.10 ................................ ................................ ................................ ................................ ................................ ......... 118 \n\nMEASURE 2.11 ................................ ................................ ................................ ................................ ................................ ......... 121 \n\nMEASURE 2.12 ................................ ................................ ................................ ................................ ................................ ......... 126 \n\nMEASURE 2.13 ................................ ................................ ................................ ................................ ................................ ......... 128 \n\nMEASURE 3.1 ................................ ................................ ................................ ................................ ................................ ............ 129 \n\nMEASURE 3.2 ................................ ................................ ................................ ................................ ................................ ............ 131 \n\nMEASURE 3.3 ................................ ................................ ................................ ................................ ................................ ............ 132 \n\nMEASURE 4.1 ................................ ................................ ................................ ................................ ................................ ............ 134 \n\nMEASURE 4.2 ................................ ................................ ................................ ................................ ................................ ............ 137 \n\nMEASURE 4.3 ................................ ................................ ................................ ................................ ................................ ............ 140 The Playbook provides suggested actions for achieving the outcomes laid out in \n\n# the AI Risk Management Framework (AI RMF) Core (Tables 1 – 4 in AI RMF \n\n# 1.0). Suggestions are aligned to each sub-category within the four AI RMF \n\n# functions (Govern, Map, Measure, Manage). \n\n# The Playbook is neither a checklist nor set of steps to be followed in its entirety. \n\n# Playbook suggestions are voluntary. Organizations may utilize this information \n\n# by borrowing as many – or as few – suggestions as apply to their industry use \n\n# case or interests. \n\n# About the Playbook \n\n# Govern  Map  Measure  Manage \n\n# FORWARD GOVERN 4 of 142 \n\n# Govern \n\nPolicies, processes, procedures and practices across the organization related to the \n\nmapping, measuring and managing of AI risks are in place, transparent, and implemented \n\neffectively. \n\n# GOVERN 1.1 \n\nLegal and regulatory requirements involving AI are understood, managed, and documented. \n\nAbout \n\nAI systems may be subject to specific applicable legal and regulatory requirements. Some \n\nlegal requirements can mandate (e.g., nondiscrimination, data privacy and security \n\ncontrols) documentation, disclosure, and increased AI system transparency. These \n\nrequirements are complex and may not be applicable or differ across applications and \n\ncontexts. \n\nFor example, AI system testing processes for bias measurement, such as disparate impact, \n\nare not applied uniformly within the legal context. Disparate impact is broadly defined as a \n\nfacially neutral policy or practice that disproportionately harms a group based on a \n\nprotected trait. Notably, some modeling algorithms or debiasing techniques that rely on \n\ndemographic information, could also come into tension with legal prohibitions on disparate \n\ntreatment (i.e., intentional discrimination). \n\nAdditionally, some intended users of AI systems may not have consistent or reliable access \n\nto fundamental internet technologies (a phenomenon widely described as the “digital \n\ndivide”) or may experience difficulties interacting with AI systems due to disabilities or \n\nimpairments. Such factors may mean different communities experience bias or other \n\nnegative impacts when trying to access AI systems. Failure to address such design issues \n\nmay pose legal risks, for example in employment related activities affecting persons with \n\ndisabilities. \n\nSuggested Actions \n\n• Maintain awareness of the applicable legal and regulatory considerations and \n\nrequirements specific to industry, sector, and business purpose, as well as the \n\napplication context of the deployed AI system. \n\n• Align risk management efforts with applicable legal standards. \n\n• Maintain policies for training (and re -training) organizational staff about necessary \n\nlegal or regulatory considerations that may impact AI -related design, development and \n\ndeployment activities. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent has the entity defined and documented the regulatory environment —\n\nincluding minimum requirements in laws and regulations? 5 of 142 \n\n• Has the system been reviewed for its compliance to applicable laws, regulations, \n\nstandards, and guidance? \n\n• To what extent has the entity defined and documented the regulatory environment —\n\nincluding applicable requirements in laws and regulations? \n\n• Has the system been reviewed for its compliance to relevant applicable laws, \n\nregulations, standards, and guidance? \n\nAI Transparency Resources \n\nGAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nAndrew Smith, \"Using Artificial Intelligence and Algorithms,\" FTC Business Blog (2020). \n\nRebecca Kelly Slaughter, \"Algorithms and Economic Justice,\" ISP Digital Future Whitepaper \n\n& YJoLT Special Publication (2021). \n\nPatrick Hall, Benjamin Cox, Steven Dickerson, Arjun Ravi Kannan, Raghu Kulkarni, and \n\nNicholas Schmidt, \"A United States fair lending perspective on machine learning,\" Frontiers \n\nin Artificial Intelligence 4 (2021). \n\nAI Hiring Tools and the Law, Partnership on Employment & Accessible Technology (PEAT, \n\npeatworks.org). \n\n# GOVERN 1.2 \n\nThe characteristics of trustworthy AI are integrated into organizational policies, processes, \n\nand procedures. \n\nAbout \n\nPolicies, processes, and procedures are central components of effective AI risk management \n\nand fundamental to individual and organizational accountability. All stakeholders benefit \n\nfrom policies, processes, and procedures which require preventing harm by design and \n\ndefault. \n\nOrganizational policies and procedures will vary based on available resources and risk \n\nprofiles, but can help systematize AI actor roles and responsibilities throughout the AI \n\nlifecycle. Without such policies, risk management can be subjective across the organization, \n\nand exacerbate rather than minimize risks over time. Policies, or summaries thereof, are \n\nunderstandable to relevant AI actors. Policies reflect an understanding of the underlying \n\nmetrics, measurements, and tests that are necessary to support policy and AI system design, \n\ndevelopment, deployment and use. \n\nLack of clear information about responsibilities and chains of command will limit the \n\neffectiveness of risk management. \n\nSuggested Actions \n\nOrganizational AI risk management policies should be designed to: 6 of 142 \n\n• Define key terms and concepts related to AI systems and the scope of their purposes \n\nand intended uses. \n\n• Connect AI governance to existing organizational governance and risk controls. \n\n• Align to broader data governance policies and practices, particularly the use of sensitive \n\nor otherwise risky data. \n\n• Detail standards for experimental design, data quality, and model training. \n\n• Outline and document risk mapping and measurement processes and standards. \n\n• Detail model testing and validation processes. \n\n• Detail review processes for legal and risk functions. \n\n• Establish the frequency of and detail for monitoring, auditing and review processes. \n\n• Outline change management requirements. \n\n• Outline processes for internal and external stakeholder engagement. \n\n• Establish whistleblower policies to facilitate reporting of serious AI system concerns. \n\n• Detail and test incident response plans. \n\n• Verify that formal AI risk management policies align to existing legal standards, and \n\nindustry best practices and norms. \n\n• Establish AI risk management policies that broadly align to AI system trustworthy \n\ncharacteristics. \n\n• Verify that formal AI risk management policies include currently deployed and third -\n\nparty AI systems. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent do these policies foster public trust and confidence in the use of the AI \n\nsystem? \n\n• What policies has the entity developed to ensure the use of the AI system is consistent \n\nwith its stated values and principles? \n\n• What policies and documentation has the entity developed to encourage the use of its AI \n\nsystem as intended? \n\n• To what extent are the model outputs consistent with the entity’s values and principles \n\nto foster public trust and equity? \n\nAI Transparency Resources \n\nGAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). \n\nGAO, “Artificial Intelligence: An Accountability Framework for Federal Agencies and Other \n\nEntities,” GAO@100 (GAO -21 -519SP), June 2021. \n\nNIST, \"U.S. Leadership in AI: A Plan for Federal Engagement in Developing Technical \n\nStandards and Related Tools\". 7 of 142 \n\nLipton, Zachary and McAuley, Julian and Chouldechova, Alexandra, Does mitigating ML’s \n\nimpact disparity require treatment disparity? Advances in Neural Information Processing \n\nSystems, 2018. \n\nJessica Newman (2023) “A Taxonomy of Trustworthiness for Artificial Intelligence: \n\nConnecting Properties of Trustworthiness with Risk Management and the AI Lifecycle,” UC \n\nBerkeley Center for Long -Term Cybersecurity. \n\nEmily Hadley (2022). Prioritizing Policies for Furthering Responsible Artificial Intelligence \n\nin the United States. 2022 IEEE International Conference on Big Data (Big Data), 5029 -5038. \n\nSAS Institute, “The SAS® Data Governance Framework: A Blueprint for Success”. \n\nISO, “Information technology — Reference Model of Data Management, “ ISO/IEC TR \n\n10032:200. \n\n“Play 5: Create a formal policy,” Partnership on Employment & Accessible Technology \n\n(PEAT, peatworks.org). \n\n\"National Institute of Standards and Technology. (2018). Framework for improving critical \n\ninfrastructure cybersecurity. \n\nKaitlin R. Boeckl and Naomi B. Lefkovitz. \"NIST Privacy Framework: A Tool for Improving \n\nPrivacy Through Enterprise Risk Management, Version 1.0.\" National Institute of Standards \n\nand Technology (NIST), January 16, 2020. \n\n“plainlanguage.gov – Home,” The U.S. Government. \n\n# GOVERN 1.3 \n\nProcesses and procedures are in place to determine the needed level of risk management \n\nactivities based on the organization's risk tolerance. \n\nAbout \n\nRisk management resources are finite in any organization. Adequate AI governance policies \n\ndelineate the mapping, measurement, and prioritization of risks to allocate resources \n\ntoward the most material issues for an AI system to ensure effective risk management. \n\nPolicies may specify systematic processes for assigning mapped and measured risks to \n\nstandardized risk scales. \n\nAI risk tolerances range from negligible to critical – from, respectively, almost no risk to \n\nrisks that can result in irredeemable human, reputational, financial, or environmental \n\nlosses. Risk tolerance rating policies consider different sources of risk, (e.g., financial, \n\noperational, safety and wellbeing, business, reputational, or model risks). A typical risk \n\nmeasurement approach entails the multiplication, or qualitative combination, of measured \n\nor estimated impact and likelihood of impacts into a risk score (risk ≈ impact x likelihood). \n\nThis score is then placed on a risk scale. Scales for risk may be qualitative, such as red -\n\namber -green (RAG), or may entail simulations or econometric approaches. Impact 8 of 142 \n\nassessments are a common tool for understanding the severity of mapped risks. In the most \n\nfulsome AI risk management approaches, all models are assigned to a risk level. \n\nSuggested Actions \n\n• Establish policies to define mechanisms for measuring or understanding an AI system’s \n\npotential impacts, e.g., via regular impact assessments at key stages in the AI lifecycle, \n\nconnected to system impacts and frequency of system updates. \n\n• Establish policies to define mechanisms for measuring or understanding the likelihood \n\nof an AI system’s impacts and their magnitude at key stages in the AI lifecycle. \n\n• Establish policies that define assessment scales for measuring potential AI system \n\nimpact. Scales may be qualitative, such as red -amber -green (RAG), or may entail \n\nsimulations or econometric approaches. \n\n• Establish policies for assigning an overall risk measurement approach for an AI system, \n\nor its important components, e.g., via multiplication or combination of a mapped risk’s \n\nimpact and likelihood (risk ≈ impact x likelihood). \n\n• Establish policies to assign systems to uniform risk scales that are valid across the \n\norganization’s AI portfolio (e.g. documentation templates), and acknowledge risk \n\ntolerance and risk levels may change over the lifecycle of an AI system. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• How do system performance metrics inform risk tolerance decisions? \n\n• What policies has the entity developed to ensure the use of the AI system is consistent \n\nwith organizational risk tolerance? \n\n• How do the entity’s data security and privacy assessments inform risk tolerance \n\ndecisions? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nThe Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, \n\n2019). \n\nBrenda Boultwood, How to Develop an Enterprise Risk -Rating Approach (Aug. 26, 2021). \n\nGlobal Association of Risk Professionals (garp.org). Accessed Jan. 4, 2023. \n\nGAO -17 -63: Enterprise Risk Management: Selected Agencies’ Experiences Illustrate Good \n\nPractices in Managing Risk. 9 of 142 \n\n# GOVERN 1.4 \n\nThe risk management process and its outcomes are established through transparent \n\npolicies, procedures, and other controls based on organizational risk priorities. \n\nAbout \n\nClear policies and procedures relating to documentation and transparency facilitate and \n\nenhance efforts to communicate roles and responsibilities for the Map, Measure and \n\nManage functions across the AI lifecycle. Standardized documentation can help \n\norganizations systematically integrate AI risk management processes and enhance \n\naccountability efforts. For example, by adding their contact information to a work product \n\ndocument, AI actors can improve communication, increase ownership of work products, \n\nand potentially enhance consideration of product quality. Documentation may generate \n\ndownstream benefits related to improved system replicability and robustness. Proper \n\ndocumentation storage and access procedures allow for quick retrieval of critical \n\ninformation during a negative incident. Explainable machine learning efforts (models and \n\nexplanatory methods) may bolster technical documentation practices by introducing \n\nadditional information for review and interpretation by AI Actors. \n\nSuggested Actions \n\n• Establish and regularly review documentation policies that, among others, address \n\ninformation related to: \n\n• AI actors contact informations \n\n• Business justification \n\n• Scope and usages \n\n• Expected and potential risks and impacts \n\n• Assumptions and limitations \n\n• Description and characterization of training data \n\n• Algorithmic methodology \n\n• Evaluated alternative approaches \n\n• Description of output data \n\n• Testing and validation results (including explanatory visualizations and \n\ninformation) \n\n• Down - and up -stream dependencies \n\n• Plans for deployment, monitoring, and change management \n\n• Stakeholder engagement plans \n\n• Verify documentation policies for AI systems are standardized across the organization \n\nand remain current. \n\n• Establish policies for a model documentation inventory system and regularly review its \n\ncompleteness, usability, and efficacy. \n\n• Establish mechanisms to regularly review the efficacy of risk management processes. \n\n• Identify AI actors responsible for evaluating efficacy of risk management processes and \n\napproaches, and for course -correction based on results. 10 of 142 \n\n• Establish policies and processes regarding public disclosure of the use of AI and risk \n\nmanagement material such as impact assessments, audits, model documentation and \n\nvalidation and testing results. \n\n• Document and review the use and efficacy of different types of transparency tools and \n\nfollow industry standards at the time a model is in use. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\n• What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\n• How will the appropriate performance metrics, such as accuracy, of the AI be monitored \n\nafter the AI is deployed? How much distributional shift or model drift from baseline \n\nperformance is acceptable? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011). \n\nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). \n\nMargaret Mitchell et al., “Model Cards for Model Reporting.” Proceedings of 2019 FATML \n\nConference. \n\nTimnit Gebru et al., “Datasheets for Datasets,” Communications of the ACM 64, No. 12, 2021. \n\nEmily M. Bender, Batya Friedman, Angelina McMillan -Major (2022). A Guide for Writing \n\nData Statements for Natural Language Processing. University of Washington. Accessed July \n\n14, 2022. \n\nM. Arnold, R. K. E. Bellamy, M. Hind, et al. FactSheets: Increasing trust in AI services through \n\nsupplier’s declarations of conformity. IBM Journal of Research and Development 63, 4/5 \n\n(July -September 2019), 6:1 -6:13. \n\nNavdeep Gill, Abhishek Mathur, Marcos V. Conde (2022). A Brief Overview of AI Governance \n\nfor Responsible Machine Learning Systems. ArXiv, abs/2211.13130. \n\nJohn Richards, David Piorkowski, Michael Hind, et al. A Human -Centered Methodology for \n\nCreating AI FactSheets. Bulletin of the IEEE Computer Society Technical Committee on Data \n\nEngineering. 11 of 142 \n\nChristoph Molnar, Interpretable Machine Learning, lulu.com. \n\nDavid A. Broniatowski. 2021. Psychological Foundations of Explainability and \n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology \n\n(NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD. \n\nOECD (2022), “OECD Framework for the Classification of AI systems”, OECD Digital \n\nEconomy Papers, No. 323, OECD Publishing, Paris. \n\n# GOVERN 1.5 \n\nOngoing monitoring and periodic review of the risk management process and its outcomes \n\nare planned, organizational roles and responsibilities are clearly defined, including \n\ndetermining the frequency of periodic review. \n\nAbout \n\nAI systems are dynamic and may perform in unexpected ways once deployed or after \n\ndeployment. Continuous monitoring is a risk management process for tracking unexpected \n\nissues and performance changes, in real -time or at a specific frequency, across the AI \n\nsystem lifecycle. \n\nIncident response and “appeal and override” are commonly used processes in information \n\ntechnology management. These processes enable real -time flagging of potential incidents, \n\nand human adjudication of system outcomes. \n\nEstablishing and maintaining incident response plans can reduce the likelihood of additive \n\nimpacts during an AI incident. Smaller organizations which may not have fulsome \n\ngovernance programs, can utilize incident response plans for addressing system failures, \n\nabuse or misuse. \n\nSuggested Actions \n\n• Establish policies to allocate appropriate resources and capacity for assessing impacts \n\nof AI systems on individuals, communities and society. \n\n• Establish policies and procedures for monitoring and addressing AI system \n\nperformance and trustworthiness, including bias and security problems, across the \n\nlifecycle of the system. \n\n• Establish policies for AI system incident response, or confirm that existing incident \n\nresponse policies apply to AI systems. \n\n• Establish policies to define organizational functions and personnel responsible for AI \n\nsystem monitoring and incident response activities. \n\n• Establish mechanisms to enable the sharing of feedback from impacted individuals or \n\ncommunities about negative impacts from AI systems. \n\n• Establish mechanisms to provide recourse for impacted individuals or communities to \n\ncontest problematic AI system outcomes. \n\n• Establish opt -out mechanisms. 12 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent does the system/entity consistently measure progress towards stated \n\ngoals and objectives? \n\n• Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\n• Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\nReferences \n\nNational Institute of Standards and Technology. (2018). Framework for improving critical \n\ninfrastructure cybersecurity. \n\nNational Institute of Standards and Technology. (2012). Computer Security Incident \n\nHandling Guide. NIST Special Publication 800 -61 Revision 2. \n\n# GOVERN 1.6 \n\nMechanisms are in place to inventory AI systems and are resourced according to \n\norganizational risk priorities. \n\nAbout \n\nAn AI system inventory is an organized database of artifacts relating to an AI system or \n\nmodel. It may include system documentation, incident response plans, data dictionaries, \n\nlinks to implementation software or source code, names and contact information for \n\nrelevant AI actors, or other information that may be helpful for model or system \n\nmaintenance and incident response purposes. AI system inventories also enable a holistic \n\nview of organizational AI assets. A serviceable AI system inventory may allow for the quick \n\nresolution of: \n\n• specific queries for single models, such as “when was this model last refreshed?” \n\n• high -level queries across all models, such as, “how many models are currently deployed \n\nwithin our organization?” or “how many users are impacted by our models?” \n\nAI system inventories are a common element of traditional model risk management \n\napproaches and can provide technical, business and risk management benefits. Typically \n\ninventories capture all organizational models or systems, as partial inventories may not \n\nprovide the value of a full inventory. 13 of 142 \n\nSuggested Actions \n\n• Establish policies that define the creation and maintenance of AI system inventories. \n\n• Establish policies that define a specific individual or team that is responsible for \n\nmaintaining the inventory. \n\n• Establish policies that define which models or systems are inventoried, with preference \n\nto inventorying all models or systems, or minimally, to high risk models or systems, or \n\nsystems deployed in high -stakes settings. \n\n• Establish policies that define model or system attributes to be inventoried, e.g, \n\ndocumentation, links to source code, incident response plans, data dictionaries, AI actor \n\ncontact information. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Who is responsible for documenting and maintaining the AI system inventory details? \n\n• What processes exist for data generation, acquisition/collection, ingestion, \n\nstaging/storage, transformations, security, maintenance, and dissemination? \n\n• Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nReferences \n\n“A risk -based integrity level schema”, in IEEE 1012, IEEE Standard for System, Software, \n\nand Hardware Verification and Validation. See Annex B. \n\nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). \n\nSee “Model Inventory,” pg. 26. \n\nVertaAI, “ModelDB: An open -source system for Machine Learning model versioning, \n\nmetadata, and experiment management.” Accessed Jan. 5, 2023. \n\n# GOVERN 1.7 \n\nProcesses and procedures are in place for decommissioning and phasing out of AI systems \n\nsafely and in a manner that does not increase risks or decrease the organization’s \n\ntrustworthiness. \n\nAbout \n\nIrregular or indiscriminate termination or deletion of models or AI systems may be \n\ninappropriate and increase organizational risk. For example, AI systems may be subject to \n\nregulatory requirements or implicated in future security or legal investigations. To maintain \n\ntrust, organizations may consider establishing policies and processes for the systematic and \n\ndeliberate decommissioning of AI systems. Typically, such policies consider user and 14 of 142 \n\ncommunity concerns, risks in dependent and linked systems, and security, legal or \n\nregulatory concerns. Decommissioned models or systems may be stored in a model \n\ninventory along with active models, for an established length of time. \n\nSuggested Actions \n\n• Establish policies for decommissioning AI systems. Such policies typically address: \n\n• User and community concerns, and reputational risks. \n\n• Business continuity and financial risks. \n\n• Up and downstream system dependencies. \n\n• Regulatory requirements (e.g., data retention). \n\n• Potential future legal, regulatory, security or forensic investigations. \n\n• Migration to the replacement system, if appropriate. \n\n• Establish policies that delineate where and for how long decommissioned systems, \n\nmodels and related artifacts are stored. \n\n• Establish practices to track accountability and consider how decommission and other \n\nadaptations or changes in system deployment contribute to downstream impacts for \n\nindividuals, groups and communities. \n\n• Establish policies that address ancillary data or artifacts that must be preserved for \n\nfulsome understanding or execution of the decommissioned AI system, e.g., predictions, \n\nexplanations, intermediate input feature representations, usernames and passwords, \n\netc. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What processes exist for data generation, acquisition/collection, ingestion, \n\nstaging/storage, transformations, security, maintenance, and dissemination? \n\n• To what extent do these policies foster public trust and confidence in the use of the AI \n\nsystem? \n\n• If anyone believes that the AI no longer meets this ethical framework, who will be \n\nresponsible for receiving the concern and as appropriate investigating and remediating \n\nthe issue? Do they have authority to modify, limit, or stop the use of the AI? \n\n• If it relates to people, were there any ethical review applications/reviews/approvals? \n\n(e.g. Institutional Review Board applications) \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\n• Datasheets for Datasets. 15 of 142 \n\nReferences \n\nMichelle De Mooy, Joseph Jerome and Vijay Kasschau, “Should It Stay or Should It Go? The \n\nLegal, Policy and Technical Landscape Around Data Deletion,” Center for Democracy and \n\nTechnology, 2017. \n\nBurcu Baykurt, \"Algorithmic accountability in US cities: Transparency, impact, and political \n\neconomy.\" Big Data & Society 9, no. 2 (2022): 20539517221115426. \n\nUpol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. “The Algorithmic Imprint.” \n\nProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency \n\n(2022). \n\n“Information System Decommissioning Guide,” Bureau of Land Management, 2011. \n\n# GOVERN 2.1 \n\nRoles and responsibilities and lines of communication related to mapping, measuring, and \n\nmanaging AI risks are documented and are clear to individuals and teams throughout the \n\norganization. \n\nAbout \n\nThe development of a risk -aware organizational culture starts with defining \n\nresponsibilities. For example, under some risk management structures, professionals \n\ncarrying out test and evaluation tasks are independent from AI system developers and \n\nreport through risk management functions or directly to executives. This kind of structure \n\nmay help counter implicit biases such as groupthink or sunk cost fallacy and bolster risk \n\nmanagement functions, so efforts are not easily bypassed or ignored. \n\nInstilling a culture where AI system design and implementation decisions can be questioned \n\nand course - corrected by empowered AI actors can enhance organizations’ abilities to \n\nanticipate and effectively manage risks before they become ingrained. \n\nSuggested Actions \n\n• Establish policies that define the AI risk management roles and responsibilities for \n\npositions directly and indirectly related to AI systems, including, but not limited to \n\n• Boards of directors or advisory committees \n\n• Senior management \n\n• AI audit functions \n\n• Product management \n\n• Project management \n\n• AI design \n\n• AI development \n\n• Human -AI interaction \n\n• AI testing and evaluation \n\n• AI acquisition and procurement 16 of 142 \n\n• Impact assessment functions \n\n• Oversight functions \n\n• Establish policies that promote regular communication among AI actors participating in \n\nAI risk management efforts. \n\n• Establish policies that separate management of AI system development functions from \n\nAI system testing functions, to enable independent course -correction of AI systems. \n\n• Establish policies to identify, increase the transparency of, and prevent conflicts of \n\ninterest in AI risk management efforts. \n\n• Establish policies to counteract confirmation bias and market incentives that may \n\nhinder AI risk management efforts. \n\n• Establish policies that incentivize AI actors to collaborate with existing legal, oversight, \n\ncompliance, or enterprise risk functions in their AI risk management activities. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\n• Who is ultimately responsible for the decisions of the AI and is this person aware of the \n\nintended uses and limitations of the analytic? \n\n• Are the responsibilities of the personnel involved in the various AI governance \n\nprocesses clearly defined? \n\n• What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\n• Did your organization implement accountability -based practices in data management \n\nand protection (e.g. the PDPA and OECD Privacy Principles)? \n\nAI Transparency Resources \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nAndrew Smith, “Using Artificial Intelligence and Algorithms,” FTC Business Blog (Apr. 8, \n\n2020). \n\nOff. Superintendent Fin. Inst. Canada, Enterprise -Wide Model Risk Management for Deposit -\n\nTaking Institutions, E -23 (Sept. 2017). \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011). \n\nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). 17 of 142 \n\nISO, “Information Technology — Artificial Intelligence — Guidelines for AI applications,” \n\nISO/IEC CD 5339. See Section 6, “Stakeholders’ perspectives and AI application framework.” \n\n# GOVERN 2.2 \n\nThe organization’s personnel and partners receive AI risk management training to enable \n\nthem to perform their duties and responsibilities consistent with related policies, \n\nprocedures, and agreements. \n\nAbout \n\nTo enhance AI risk management adoption and effectiveness, organizations are encouraged \n\nto identify and integrate appropriate training curricula into enterprise learning \n\nrequirements. Through regular training, AI actors can maintain awareness of: \n\n• AI risk management goals and their role in achieving them. \n\n• Organizational policies, applicable laws and regulations, and industry best practices and \n\nnorms. \n\nSee [MAP 3.4]() and [3.5]() for additional relevant information. \n\nSuggested Actions \n\n• Establish policies for personnel addressing ongoing education about: \n\n• Applicable laws and regulations for AI systems. \n\n• Potential negative impacts that may arise from AI systems. \n\n• Organizational AI policies. \n\n• Trustworthy AI characteristics. \n\n• Ensure that trainings are suitable across AI actor sub -groups - for AI actors carrying out \n\ntechnical tasks (e.g., developers, operators, etc.) as compared to AI actors in oversight \n\nroles (e.g., legal, compliance, audit, etc.). \n\n• Ensure that trainings comprehensively address technical and socio -technical aspects of \n\nAI risk management. \n\n• Verify that organizational AI policies include mechanisms for internal AI personnel to \n\nacknowledge and commit to their roles and responsibilities. \n\n• Verify that organizational policies address change management and include \n\nmechanisms to communicate and acknowledge substantial AI system changes. \n\n• Define paths along internal and external chains of accountability to escalate risk \n\nconcerns. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Are the relevant staff dealing with AI systems properly trained to interpret AI model \n\noutput and decisions as well as to detect and manage bias in data? 18 of 142 \n\n• How does the entity determine the necessary skills and experience needed to design, \n\ndevelop, deploy, assess, and monitor the AI system? \n\n• How does the entity assess whether personnel have the necessary skills, training, \n\nresources, and domain knowledge to fulfill their assigned responsibilities? \n\n• What efforts has the entity undertaken to recruit, develop, and retain a workforce with \n\nbackgrounds, experience, and perspectives that reflect the community impacted by the \n\nAI system? \n\nAI Transparency Resources \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). \n\n“Developing Staff Trainings for Equitable AI,” Partnership on Employment & Accessible \n\nTechnology (PEAT, peatworks.org). \n\n# GOVERN 2.3 \n\nExecutive leadership of the organization takes responsibility for decisions about risks \n\nassociated with AI system development and deployment. \n\nAbout \n\nSenior leadership and members of the C -Suite in organizations that maintain an AI portfolio, \n\nshould maintain awareness of AI risks, affirm the organizational appetite for such risks, and \n\nbe responsible for managing those risks.. \n\nAccountability ensures that a specific team and individual is responsible for AI risk \n\nmanagement efforts. Some organizations grant authority and resources (human and \n\nbudgetary) to a designated officer who ensures adequate performance of the institution’s AI \n\nportfolio (e.g. predictive modeling, machine learning). \n\nSuggested Actions \n\n• Organizational management can: \n\n• Declare risk tolerances for developing or using AI systems. \n\n• Support AI risk management efforts, and play an active role in such efforts. \n\n• Integrate a risk and harm prevention mindset throughout the AI lifecycle as part of \n\norganizational culture \n\n• Support competent risk management executives. \n\n• Delegate the power, resources, and authorization to perform risk management to \n\neach appropriate level throughout the management chain. 19 of 142 \n\n• Organizations can establish board committees for AI risk management and oversight \n\nfunctions and integrate those functions within the organization’s broader enterprise \n\nrisk management approaches. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Did your organization’s board and/or senior management sponsor, support and \n\nparticipate in your organization’s AI governance? \n\n• What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\n• Do AI solutions provide sufficient information to assist the personnel to make an \n\ninformed decision and take actions accordingly? \n\n• To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\nAI Transparency Resources \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011) \n\nOff. Superintendent Fin. Inst. Canada, Enterprise -Wide Model Risk Management for Deposit -\n\nTaking Institutions, E -23 (Sept. 2017). \n\n# GOVERN 3.1 \n\nDecision -makings related to mapping, measuring, and managing AI risks throughout the \n\nlifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, \n\nexperience, expertise, and backgrounds). \n\nAbout \n\nA diverse team that includes AI actors with diversity of experience, disciplines, and \n\nbackgrounds to enhance organizational capacity and capability for anticipating risks is \n\nbetter equipped to carry out risk management. Consultation with external personnel may \n\nbe necessary when internal teams lack a diverse range of lived experiences or disciplinary \n\nexpertise. \n\nTo extend the benefits of diversity, equity, and inclusion to both the users and AI actors, it is \n\nrecommended that teams are composed of a diverse group of individuals who reflect a \n\nrange of backgrounds, perspectives and expertise. \n\nWithout commitment from senior leadership, beneficial aspects of team diversity and \n\ninclusion can be overridden by unstated organizational incentives that inadvertently \n\nconflict with the broader values of a diverse workforce. 20 of 142 \n\nSuggested Actions \n\nOrganizational management can: \n\n• Define policies and hiring practices at the outset that promote interdisciplinary roles, \n\ncompetencies, skills, and capacity for AI efforts. \n\n• Define policies and hiring practices that lead to demographic and domain expertise \n\ndiversity; empower staff with necessary resources and support, and facilitate the \n\ncontribution of staff feedback and concerns without fear of reprisal. \n\n• Establish policies that facilitate inclusivity and the integration of new insights into \n\nexisting practice. \n\n• Seek external expertise to supplement organizational diversity, equity, inclusion, and \n\naccessibility where internal expertise is lacking. \n\n• Establish policies that incentivize AI actors to collaborate with existing \n\nnondiscrimination, accessibility and accommodation, and human resource functions, \n\nemployee resource group (ERGs), and diversity, equity, inclusion, and accessibility \n\n(DEIA) initiatives. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Are the relevant staff dealing with AI systems properly trained to interpret AI model \n\noutput and decisions as well as to detect and manage bias in data? \n\n• Entities include diverse perspectives from technical and non -technical communities \n\nthroughout the AI life cycle to anticipate and mitigate unintended consequences \n\nincluding potential bias and discrimination. \n\n• Stakeholder involvement: Include diverse perspectives from a community of \n\nstakeholders throughout the AI life cycle to mitigate risks. \n\n• Strategies to incorporate diverse perspectives include establishing collaborative \n\nprocesses and multidisciplinary teams that involve subject matter experts in data \n\nscience, software development, civil liberties, privacy and security, legal counsel, and \n\nrisk management. \n\n• To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\nAI Transparency Resources \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• Datasheets for Datasets. \n\nReferences \n\nDylan Walsh, “How can human -centered AI fight bias in machines and people?” MIT Sloan \n\nMgmt. Rev., 2021. \n\nMichael Li, “To Build Less -Biased AI, Hire a More Diverse Team,” Harvard Bus. Rev., 2020. 21 of 142 \n\nBo Cowgill et al., “Biased Programmers? Or Biased Data? A Field Experiment in \n\nOperationalizing AI Ethics,” 2020. \n\nNaomi Ellemers, Floortje Rink, “Diversity in work groups,” Current opinion in psychology, \n\nvol. 11, pp. 49 –53, 2016. \n\nKatrin Talke, Søren Salomo, Alexander Kock, “Top management team diversity and strategic \n\ninnovation orientation: The relationship and consequences for innovativeness and \n\nperformance,” Journal of Product Innovation Management, vol. 28, pp. 819 –832, 2011. \n\nSarah Myers West, Meredith Whittaker, and Kate Crawford,, “Discriminating Systems: \n\nGender, Race, and Power in AI,” AI Now Institute, Tech. Rep., 2019. \n\nSina Fazelpour, Maria De -Arteaga, Diversity in sociotechnical machine learning systems. Big \n\nData & Society. January 2022. doi:10.1177/20539517221082027 \n\nMary L. Cummings and Songpo Li, 2021a. Sources of subjectivity in machine learning \n\nmodels. ACM Journal of Data and Information Quality, 13(2), 1 –9\n\n“Staffing for Equitable AI: Roles & Responsibilities,” Partnership on Employment & \n\nAccessible Technology (PEAT, peatworks.org). Accessed Jan. 6, 2023. \n\n# GOVERN 3.2 \n\nPolicies and procedures are in place to define and differentiate roles and responsibilities for \n\nhuman -AI configurations and oversight of AI systems. \n\nAbout \n\nIdentifying and managing AI risks and impacts are enhanced when a broad set of \n\nperspectives and actors across the AI lifecycle, including technical, legal, compliance, social \n\nscience, and human factors expertise is engaged. AI actors include those who operate, use, \n\nor interact with AI systems for downstream tasks, or monitor AI system performance. \n\nEffective risk management efforts include: \n\n• clear definitions and differentiation of the various human roles and responsibilities for \n\nAI system oversight and governance \n\n• recognizing and clarifying differences between AI system overseers and those using or \n\ninteracting with AI systems. \n\nSuggested Actions \n\n• Establish policies and procedures that define and differentiate the various human roles \n\nand responsibilities when using, interacting with, or monitoring AI systems. \n\n• Establish procedures for capturing and tracking risk information related to human -AI \n\nconfigurations and associated outcomes. \n\n• Establish policies for the development of proficiency standards for AI actors carrying \n\nout system operation tasks and system oversight tasks. 22 of 142 \n\n• Establish specified risk management training protocols for AI actors carrying out \n\nsystem operation tasks and system oversight tasks. \n\n• Establish policies and procedures regarding AI actor roles, and responsibilities for \n\nhuman oversight of deployed systems. \n\n• Establish policies and procedures defining human -AI configurations (configurations \n\nwhere AI systems are explicitly designated and treated as team members in primarily \n\nhuman teams) in relation to organizational risk tolerances, and associated \n\ndocumentation. \n\n• Establish policies to enhance the explanation, interpretation, and overall transparency \n\nof AI systems. \n\n• Establish policies for managing risks regarding known difficulties in human -AI \n\nconfigurations, human -AI teaming, and AI system user experience and user interactions \n\n(UI/UX). \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\n• To what extent has the entity documented the appropriate level of human involvement \n\nin AI -augmented decision -making? \n\n• How will the accountable human(s) address changes in accuracy and precision due to \n\neither an adversary’s attempts to disrupt the AI or unrelated changes in \n\noperational/business environment, which may impact the accuracy of the AI? \n\n• To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\n• How does the entity assess whether personnel have the necessary skills, training, \n\nresources, and domain knowledge to fulfill their assigned responsibilities? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\nReferences \n\nMadeleine Clare Elish, \"Moral Crumple Zones: Cautionary tales in human -robot interaction,\" \n\nEngaging Science, Technology, and Society, Vol. 5, 2019. \n\n“Human -AI Teaming: State -Of -The -Art and Research Needs,” National Academies of \n\nSciences, Engineering, and Medicine, 2022. \n\nBen Green, \"The Flaws Of Policies Requiring Human Oversight Of Government Algorithms,\" \n\nComputer Law & Security Review 45 (2022). 23 of 142 \n\nDavid A. Broniatowski. 2021. Psychological Foundations of Explainability and \n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology \n\n(NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD. \n\nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). \n\n# GOVERN 4.1 \n\nOrganizational policies, and practices are in place to foster a critical thinking and safety -first \n\nmindset in the design, development, deployment, and uses of AI systems to minimize \n\nnegative impacts. \n\nAbout \n\nA risk culture and accompanying practices can help organizations effectively triage the most \n\ncritical risks. Organizations in some industries implement three (or more) “lines of \n\ndefense,” where separate teams are held accountable for different aspects of the system \n\nlifecycle, such as development, risk management, and auditing. While a traditional three -\n\nlines approach may be impractical for smaller organizations, leadership can commit to \n\ncultivating a strong risk culture through other means. For example, “effective challenge,” is a \n\nculture - based practice that encourages critical thinking and questioning of important \n\ndesign and implementation decisions by experts with the authority and stature to make \n\nsuch changes. \n\nRed -teaming is another risk measurement and management approach. This practice \n\nconsists of adversarial testing of AI systems under stress conditions to seek out failure \n\nmodes or vulnerabilities in the system. Red -teams are composed of external experts or \n\npersonnel who are independent from internal AI actors. \n\nSuggested Actions \n\n• Establish policies that require inclusion of oversight functions (legal, compliance, risk \n\nmanagement) from the outset of the system design process. \n\n• Establish policies that promote effective challenge of AI system design, implementation, \n\nand deployment decisions, via mechanisms such as the three lines of defense, model \n\naudits, or red -teaming – to minimize workplace risks such as groupthink. \n\n• Establish policies that incentivize safety -first mindset and general critical thinking and \n\nreview at an organizational and procedural level. \n\n• Establish whistleblower protections for insiders who report on perceived serious \n\nproblems with AI systems. \n\n• Establish policies to integrate a harm and risk prevention mindset throughout the AI \n\nlifecycle. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent has the entity documented the AI system’s development, testing \n\nmethodology, metrics, and performance outcomes? 24 of 142 \n\n• Are organizational information sharing practices widely followed and transparent, such \n\nthat related past failed designs can be avoided? \n\n• Are training manuals and other resources for carrying out incident response \n\ndocumented and available? \n\n• Are processes for operator reporting of incidents and near -misses documented and \n\navailable? \n\n• How might revealing mismatches between claimed and actual system performance help \n\nusers understand limitations and anticipate risks and impacts?” \n\nAI Transparency Resources \n\n• Datasheets for Datasets. \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011) \n\nPatrick Hall, Navdeep Gill, and Benjamin Cox, “Responsible Machine Learning,” O’Reilly \n\nMedia, 2020. \n\nOff. Superintendent Fin. Inst. Canada, Enterprise -Wide Model Risk Management for Deposit -\n\nTaking Institutions, E -23 (Sept. 2017). \n\nGAO, “Artificial Intelligence: An Accountability Framework for Federal Agencies and Other \n\nEntities,” GAO@100 (GAO -21 -519SP), June 2021. \n\nDonald Sull, Stefano Turconi, and Charles Sull, “When It Comes to Culture, Does Your \n\nCompany Walk the Talk?” MIT Sloan Mgmt. Rev., 2020. \n\nKathy Baxter, AI Ethics Maturity Model, Salesforce. \n\nUpol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, and Hal Daumé. 2024. Seamful XAI: \n\nOperationalizing Seamful Design in Explainable AI. Proc. ACM Hum. -Comput. Interact. 8, \n\nCSCW1, Article 119. https://doi.org/10.1145/3637396 \n\n# GOVERN 4.2 \n\nOrganizational teams document the risks and potential impacts of the AI technology they \n\ndesign, develop, deploy, evaluate and use, and communicate about the impacts more \n\nbroadly. \n\nAbout \n\nImpact assessments are one approach for driving responsible technology development \n\npractices. And, within a specific use case, these assessments can provide a high -level \n\nstructure for organizations to frame risks of a given algorithm or deployment. Impact 25 of 142 \n\nassessments can also serve as a mechanism for organizations to articulate risks and \n\ngenerate documentation for managing and oversight activities when harms do arise. \n\nImpact assessments may: \n\n• be applied at the beginning of a process but also iteratively and regularly since goals \n\nand outcomes can evolve over time. \n\n• include perspectives from AI actors, including operators, users, and potentially \n\nimpacted communities (including historically marginalized communities, those with \n\ndisabilities, and individuals impacted by the digital divide), \n\n• assist in “go/no -go” decisions for an AI system. \n\n• consider conflicts of interest, or undue influence, related to the organizational team \n\nbeing assessed. \n\nSee the MAP function playbook guidance for more information relating to impact \n\nassessments. \n\nSuggested Actions \n\n• Establish impact assessment policies and processes for AI systems used by the \n\norganization. \n\n• Align organizational impact assessment activities with relevant regulatory or legal \n\nrequirements. \n\n• Verify that impact assessment activities are appropriate to evaluate the potential \n\nnegative impact of a system and how quickly a system changes, and that assessments \n\nare applied on a regular basis. \n\n• Utilize impact assessments to inform broader evaluations of AI system risk. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• How has the entity identified and mitigated potential impacts of bias in the data, \n\nincluding inequitable or discriminatory outcomes? \n\n• How has the entity documented the AI system’s data provenance, including sources, \n\norigins, transformations, augmentations, labels, dependencies, constraints, and \n\nmetadata? \n\n• To what extent has the entity clearly defined technical specifications and requirements \n\nfor the AI system? \n\n• To what extent has the entity documented and communicated the AI system’s \n\ndevelopment, testing methodology, metrics, and performance outcomes? \n\n• Have you documented and explained that machine errors may differ from human \n\nerrors? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Datasheets for Datasets. 26 of 142 \n\nReferences \n\nDillon Reisman, Jason Schultz, Kate Crawford, Meredith Whittaker, “Algorithmic Impact \n\nAssessments: A Practical Framework For Public Agency Accountability,” AI Now Institute, \n\n2018. \n\nH.R. 2231, 116th Cong. (2019). \n\nBSA The Software Alliance (2021) Confronting Bias: BSA’s Framework to Build Trust in AI. \n\nAnthony M. Barrett, Dan Hendrycks, Jessica Newman and Brandie Nonnecke. Actionable \n\nGuidance for High -Consequence AI Risk Management: Towards Standards Addressing AI \n\nCatastrophic Risks. ArXiv abs/2206.08966 (2022) https://arxiv.org/abs/2206.08966 \n\nDavid Wright, “Making Privacy Impact Assessments More Effective.\" The Information \n\nSociety 29, 2013. \n\nKonstantinia Charitoudi and Andrew Blyth. A Socio -Technical Approach to Cyber Risk \n\nManagement and Impact Assessment. Journal of Information Security 4, 1 (2013), 33 -41. \n\nEmanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, & Jacob Metcalf. \n\n2021. “Assembling Accountability: Algorithmic Impact Assessment for the Public Interest”. \n\nMicrosoft. Responsible AI Impact Assessment Template. 2022. \n\nMicrosoft. Responsible AI Impact Assessment Guide. 2022. \n\nMicrosoft. Foundations of assessing harm. 2022. \n\nMauritz Kop, “AI Impact Assessment & Code of Conduct,” Futurium, May 2019. \n\nDillon Reisman, Jason Schultz, Kate Crawford, and Meredith Whittaker, “Algorithmic Impact \n\nAssessments: A Practical Framework For Public Agency Accountability,” AI Now, Apr. 2018. \n\nAndrew D. Selbst, “An Institutional View Of Algorithmic Impact Assessments,” Harvard \n\nJournal of Law & Technology, vol. 35, no. 1, 2021 \n\nAda Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. \n\nAccessed July 14, 2022. \n\nKathy Baxter, AI Ethics Maturity Model, Salesforce \n\nRavit Dotan, Borhane Blili -Hamelin, Ravi Madhavan, Jeanna Matthews, Joshua Scarpino, & \n\nCarol Anderson. (2024). A Flexible Maturity Model for AI Governance Based on the NIST AI \n\nRisk Management Framework [Technical Report]. IEEE. https://ieeeusa.org/product/a -\n\nflexible -maturity -model -for -ai -governance 27 of 142 \n\n# GOVERN 4.3 \n\nOrganizational practices are in place to enable AI testing, identification of incidents, and \n\ninformation sharing. \n\nAbout \n\nIdentifying AI system limitations, detecting and tracking negative impacts and incidents, \n\nand sharing information about these issues with appropriate AI actors will improve risk \n\nmanagement. Issues such as concept drift, AI bias and discrimination, shortcut learning or \n\nunderspecification are difficult to identify using current standard AI testing processes. \n\nOrganizations can institute in -house use and testing policies and procedures to identify and \n\nmanage such issues. Efforts can take the form of pre -alpha or pre -beta testing, or deploying \n\ninternally developed systems or products within the organization. Testing may entail \n\nlimited and controlled in -house, or publicly available, AI system testbeds, and accessibility \n\nof AI system interfaces and outputs. \n\nWithout policies and procedures that enable consistent testing practices, risk management \n\nefforts may be bypassed or ignored, exacerbating risks or leading to inconsistent risk \n\nmanagement activities. \n\nInformation sharing about impacts or incidents detected during testing or deployment can: \n\n• draw attention to AI system risks, failures, abuses or misuses, \n\n• allow organizations to benefit from insights based on a wide range of AI applications \n\nand implementations, and \n\n• allow organizations to be more proactive in avoiding known failure modes. \n\nOrganizations may consider sharing incident information with the AI Incident Database, the \n\nAIAAIC, users, impacted communities, or with traditional cyber vulnerability databases, \n\nsuch as the MITRE CVE list. \n\nSuggested Actions \n\n• Establish policies and procedures to facilitate and equip AI system testing. \n\n• Establish organizational commitment to identifying AI system limitations and sharing of \n\ninsights about limitations within appropriate AI actor groups. \n\n• Establish policies for reporting and documenting incident response. \n\n• Establish policies and processes regarding public disclosure of incidents and \n\ninformation sharing. \n\n• Establish guidelines for incident handling related to AI system risks and performance. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? Consulting the community or end users at the earliest 28 of 142 \n\nstages of development to ensure there is transparency on the technology used and how \n\nit is deployed. \n\n• Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\n• To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nAI Transparency Resources \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\nReferences \n\nSean McGregor, “Preventing Repeated Real World AI Failures by Cataloging Incidents: The \n\nAI Incident Database,” arXiv:2011.08512 [cs], Nov. 2020, arXiv:2011.08512. \n\nChristopher Johnson, Mark Badger, David Waltermire, Julie Snyder, and Clem Skorupka, \n\n“Guide to cyber threat information sharing,” National Institute of Standards and Technology, \n\nNIST Special Publication 800 -150, Nov 2016. \n\nMengyi Wei, Zhixuan Zhou (2022). AI Ethics Issues in Real World: Evidence from AI Incident \n\nDatabase. ArXiv, abs/2206.07635. \n\nBSA The Software Alliance (2021) Confronting Bias: BSA’s Framework to Build Trust in AI. \n\n“Using Combined Expertise to Evaluate Web Accessibility,” W3C Web Accessibility Initiative. \n\n# GOVERN 5.1 \n\nOrganizational policies and practices are in place to collect, consider, prioritize, and \n\nintegrate feedback from those external to the team that developed or deployed the AI \n\nsystem regarding the potential individual and societal impacts related to AI risks. \n\nAbout \n\nBeyond internal and laboratory -based system testing, organizational policies and practices \n\nmay consider AI system fitness -for -purpose related to the intended context of use. \n\nParticipatory stakeholder engagement is one type of qualitative activity to help AI actors \n\nanswer questions such as whether to pursue a project or how to design with impact in \n\nmind. This type of feedback, with domain expert input, can also assist AI actors to identify \n\nemergent scenarios and risks in certain AI applications. The consideration of when and how \n\nto convene a group and the kinds of individuals, groups, or community organizations to \n\ninclude is an iterative process connected to the system's purpose and its level of risk. Other \n\nfactors relate to how to collaboratively and respectfully capture stakeholder feedback and \n\ninsight that is useful, without being a solely perfunctory exercise. 29 of 142 \n\nThese activities are best carried out by personnel with expertise in participatory practices, \n\nqualitative methods, and translation of contextual feedback for technical audiences. \n\nParticipatory engagement is not a one -time exercise and is best carried out from the very \n\nbeginning of AI system commissioning through the end of the lifecycle. Organizations can \n\nconsider how to incorporate engagement when beginning a project and as part of their \n\nmonitoring of systems. Engagement is often utilized as a consultative practice, but this \n\nperspective may inadvertently lead to “participation washing.” Organizational transparency \n\nabout the purpose and goal of the engagement can help mitigate that possibility. \n\nOrganizations may also consider targeted consultation with subject matter experts as a \n\ncomplement to participatory findings. Experts may assist internal staff in identifying and \n\nconceptualizing potential negative impacts that were previously not considered. \n\nSuggested Actions \n\n• Establish AI risk management policies that explicitly address mechanisms for collecting, \n\nevaluating, and incorporating stakeholder and user feedback that could include: \n\n• Recourse mechanisms for faulty AI system outputs. \n\n• Bug bounties. \n\n• Human -centered design. \n\n• User -interaction and experience research. \n\n• Participatory stakeholder engagement with individuals and communities that may \n\nexperience negative impacts. \n\n• Verify that stakeholder feedback is considered and addressed, including environmental \n\nconcerns, and across the entire population of intended users, including historically \n\nexcluded populations, people with disabilities, older people, and those with limited \n\naccess to the internet and other basic technologies. \n\n• Clarify the organization’s principles as they apply to AI systems – considering those \n\nwhich have been proposed publicly – to inform external stakeholders of the \n\norganization’s values. Consider publishing or adopting AI principles. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\n• To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\n• How easily accessible and current is the information available to external stakeholders? \n\n• What was done to mitigate or reduce the potential for harm? \n\n• Stakeholder involvement: Include diverse perspectives from a community of \n\nstakeholders throughout the AI life cycle to mitigate risks. 30 of 142 \n\nAI Transparency Resources \n\n• Datasheets for Datasets. \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\n• Stakeholders in Explainable AI, Sep. 2018. \n\nReferences \n\nISO, “Ergonomics of human -system interaction — Part 210: Human -centered design for \n\ninteractive systems,” ISO 9241 -210:2019 (2nd ed.), July 2019. \n\nRumman Chowdhury and Jutta Williams, \"Introducing Twitter’s first algorithmic bias \n\nbounty challenge,\" \n\nLeonard Haas and Sebastian Gießler, “In the realm of paper tigers – exploring the failings of \n\nAI ethics guidelines,” AlgorithmWatch, 2020. \n\nJosh Kenway, Camille Francois, Dr. Sasha Costanza -Chock, Inioluwa Deborah Raji, & Dr. Joy \n\nBuolamwini. 2022. Bug Bounties for Algorithmic Harms? Algorithmic Justice League. \n\nAccessed July 14, 2022. \n\nMicrosoft Community Jury , Azure Application Architecture Guide. \n\n“Definition of independent verification and validation (IV&V)”, in IEEE 1012, IEEE Standard \n\nfor System, Software, and Hardware Verification and Validation. Annex C, \n\n# GOVERN 5.2 \n\nMechanisms are established to enable AI actors to regularly incorporate adjudicated \n\nfeedback from relevant AI actors into system design and implementation. \n\nAbout \n\nOrganizational policies and procedures that equip AI actors with the processes, knowledge, \n\nand expertise needed to inform collaborative decisions about system deployment improve \n\nrisk management. These decisions are closely tied to AI systems and organizational risk \n\ntolerance. \n\nRisk tolerance, established by organizational leadership, reflects the level and type of risk \n\nthe organization will accept while conducting its mission and carrying out its strategy. \n\nWhen risks arise, resources are allocated based on the assessed risk of a given AI system. \n\nOrganizations typically apply a risk tolerance approach where higher risk systems receive \n\nlarger allocations of risk management resources and lower risk systems receive less \n\nresources. \n\nSuggested Actions \n\n• Explicitly acknowledge that AI systems, and the use of AI, present inherent costs and \n\nrisks along with potential benefits. 31 of 142 \n\n• Define reasonable risk tolerances for AI systems informed by laws, regulation, best \n\npractices, or industry standards. \n\n• Establish policies that ensure all relevant AI actors are provided with meaningful \n\nopportunities to provide feedback on system design and implementation. \n\n• Establish policies that define how to assign AI systems to established risk tolerance \n\nlevels by combining system impact assessments with the likelihood that an impact \n\noccurs. Such assessment often entails some combination of: \n\n• Econometric evaluations of impacts and impact likelihoods to assess AI system risk. \n\n• Red -amber -green (RAG) scales for impact severity and likelihood to assess AI \n\nsystem risk. \n\n• Establishment of policies for allocating risk management resources along \n\nestablished risk tolerance levels, with higher -risk systems receiving more risk \n\nmanagement resources and oversight. \n\n• Establishment of policies for approval, conditional approval, and disapproval of the \n\ndesign, implementation, and deployment of AI systems. \n\n• Establish policies facilitating the early decommissioning of AI systems that surpass an \n\norganization’s ability to reasonably mitigate risks. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Who is ultimately responsible for the decisions of the AI and is this person aware of the \n\nintended uses and limitations of the analytic? \n\n• Who will be responsible for maintaining, re -verifying, monitoring, and updating this AI \n\nonce deployed? \n\n• Who is accountable for the ethical considerations during all stages of the AI lifecycle? \n\n• To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\n• Does the AI solution provide sufficient information to assist the personnel to make an \n\ninformed decision and take actions accordingly? \n\nAI Transparency Resources \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\n• Stakeholders in Explainable AI, Sep. 2018. \n\n• AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011) \n\nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). 32 of 142 \n\nThe Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, \n\n2019). Retrieved on July 12, 2022. \n\n# GOVERN 6.1 \n\nPolicies and procedures are in place that address AI risks associated with third -party \n\nentities, including risks of infringement of a third party’s intellectual property or other \n\nrights. \n\nAbout \n\nRisk measurement and management can be complicated by how customers use or integrate \n\nthird -party data or systems into AI products or services, particularly without sufficient \n\ninternal governance structures and technical safeguards. \n\nOrganizations usually engage multiple third parties for external expertise, data, software \n\npackages (both open source and commercial), and software and hardware platforms across \n\nthe AI lifecycle. This engagement has beneficial uses and can increase complexities of risk \n\nmanagement efforts. \n\nOrganizational approaches to managing third -party (positive and negative) risk may be \n\ntailored to the resources, risk profile, and use case for each system. Organizations can apply \n\ngovernance approaches to third -party AI systems and data as they would for internal \n\nresources — including open source software, publicly available data, and commercially \n\navailable models. \n\nSuggested Actions \n\n• Collaboratively establish policies that address third -party AI systems and data. \n\n• Establish policies related to: \n\n• Transparency into third -party system functions, including knowledge about training \n\ndata, training and inference algorithms, and assumptions and limitations. \n\n• Thorough testing of third -party AI systems. (See MEASURE for more detail) \n\n• Requirements for clear and complete instructions for third -party system usage. \n\n• Evaluate policies for third -party technology. \n\n• Establish policies that address supply chain, full product lifecycle and associated \n\nprocesses, including legal, ethical, and other issues concerning procurement and use of \n\nthird -party software or hardware systems and data. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Did you establish mechanisms that facilitate the AI system’s auditability (e.g. \n\ntraceability of the development process, the sourcing of training data and the logging of \n\nthe AI system’s processes, outcomes, positive and negative impact)? 33 of 142 \n\n• If a third party created the AI, how will you ensure a level of explainability or \n\ninterpretability? \n\n• Did you ensure that the AI system can be audited by independent third parties? \n\n• Did you establish a process for third parties (e.g. suppliers, end users, subjects, \n\ndistributors/vendors or workers) to report potential vulnerabilities, risks or biases in \n\nthe AI system? \n\n• To what extent does the plan specifically address risks associated with acquisition, \n\nprocurement of packaged software from vendors, cybersecurity controls, computational \n\ninfrastructure, data, data science, deployment mechanics, and system failure? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\n• AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\n• Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI -\n\n2019. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011) \n\n“Proposed Interagency Guidance on Third -Party Relationships: Risk Management,” 2021. \n\nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). \n\n# GOVERN 6.2 \n\nContingency processes are in place to handle failures or incidents in third -party data or AI \n\nsystems deemed to be high -risk. \n\nAbout \n\nTo mitigate the potential harms of third -party system failures, organizations may \n\nimplement policies and procedures that include redundancies for covering third -party \n\nfunctions. \n\nSuggested Actions \n\n• Establish policies for handling third -party system failures to include consideration of \n\nredundancy mechanisms for vital third -party AI systems. \n\n• Verify that incident response plans address third -party AI systems. 34 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent does the plan specifically address risks associated with acquisition, \n\nprocurement of packaged software from vendors, cybersecurity controls, computational \n\ninfrastructure, data, data science, deployment mechanics, and system failure? \n\n• Did you establish a process for third parties (e.g. suppliers, end users, subjects, \n\ndistributors/vendors or workers) to report potential vulnerabilities, risks or biases in \n\nthe AI system? \n\n• If your organization obtained datasets from a third party, did your organization assess \n\nand manage the risks of using such datasets? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\n• AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011) \n\n“Proposed Interagency Guidance on Third -Party Relationships: Risk Management,” 2021. \n\nOff. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). MANAGE 35 of 142 \n\n# Manage \n\nAI risks based on assessments and other analytical output from the Map and Measure \n\nfunctions are prioritized, responded to, and managed. \n\n# MANAGE 1.1 \n\nA determination is made as to whether the AI system achieves its intended purpose and \n\nstated objectives and whether its development or deployment should proceed. \n\nAbout \n\nAI systems may not necessarily be the right solution for a given business task or problem. A \n\nstandard risk management practice is to formally weigh an AI system’s negative risks \n\nagainst its benefits, and to determine if the AI system is an appropriate solution. Tradeoffs \n\namong trustworthiness characteristics —such as deciding to deploy a system based on \n\nsystem performance vs system transparency –may require regular assessment throughout \n\nthe AI lifecycle. \n\nSuggested Actions \n\n• Consider trustworthiness characteristics when evaluating AI systems’ negative risks \n\nand benefits. \n\n• Utilize TEVV outputs from map and measure functions when considering risk treatment. \n\n• Regularly track and monitor negative risks and benefits throughout the AI system \n\nlifecycle including in post -deployment monitoring. \n\n• Regularly assess and document system performance relative to trustworthiness \n\ncharacteristics and tradeoffs between negative risks and opportunities. \n\n• Evaluate tradeoffs in connection with real -world use cases and impacts and as \n\nenumerated in Map function outcomes. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• How do the technical specifications and requirements align with the AI system’s goals \n\nand objectives? \n\n• To what extent are the metrics consistent with system goals, objectives, and constraints, \n\nincluding ethical and compliance considerations? \n\n• What goals and objectives does the entity expect to achieve by designing, developing, \n\nand/or deploying the AI system? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF Companion to the Model AI Governance Framework – Implementation and Self -\n\nAssessment Guide for Organizations 36 of 142 \n\nReferences \n\nArvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. \n\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, \n\n2021). \n\nFraser, Henry L and Bello y Villarino, Jose -Miguel, Where Residual Risks Reside: A \n\nComparative Approach to Art 9(4) of the European Union's Proposed AI Regulation \n\n(September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), \n\nMicrosoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. \n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* \n\n'20). Association for Computing Machinery, New York, NY, USA, 695. \n\n# MANAGE 1.2 \n\nTreatment of documented AI risks is prioritized based on impact, likelihood, or available \n\nresources or methods. \n\nAbout \n\nRisk refers to the composite measure of an event’s probability of occurring and the \n\nmagnitude (or degree) of the consequences of the corresponding events. The impacts, or \n\nconsequences, of AI systems can be positive, negative, or both and can result in \n\nopportunities or risks. \n\nOrganizational risk tolerances are often informed by several internal and external factors, \n\nincluding existing industry practices, organizational values, and legal or regulatory \n\nrequirements. Since risk management resources are often limited, organizations usually \n\nassign them based on risk tolerance. AI risks that are deemed more serious receive more \n\noversight attention and risk management resources. \n\nSuggested Actions \n\n• Assign risk management resources relative to established risk tolerance. AI systems \n\nwith lower risk tolerances receive greater oversight, mitigation and management \n\nresources. \n\n• Document AI risk tolerance determination practices and resource decisions. \n\n• Regularly review risk tolerances and re -calibrate, as needed, in accordance with \n\ninformation from AI system monitoring and assessment . 37 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\n• What assessments has the entity conducted on data security and privacy impacts \n\nassociated with the AI system? \n\n• Does your organization have an existing governance structure that can be leveraged to \n\noversee the organization’s use of AI? \n\nAI Transparency Resources \n\n• WEF Companion to the Model AI Governance Framework – Implementation and Self -\n\nAssessment Guide for Organizations \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nReferences \n\nArvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. \n\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, \n\n2021). \n\nFraser, Henry L and Bello y Villarino, Jose -Miguel, Where Residual Risks Reside: A \n\nComparative Approach to Art 9(4) of the European Union's Proposed AI Regulation \n\n(September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), \n\nMicrosoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. \n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* \n\n'20). Association for Computing Machinery, New York, NY, USA, 695. \n\n# MANAGE 1.3 \n\nResponses to the AI risks deemed high priority as identified by the Map function, are \n\ndeveloped, planned, and documented. Risk response options can include mitigating, \n\ntransferring, avoiding, or accepting. 38 of 142 \n\nAbout \n\nOutcomes from GOVERN -1, MAP -5 and MEASURE -2, can be used to address and document \n\nidentified risks based on established risk tolerances. Organizations can follow existing \n\nregulations and guidelines for risk criteria, tolerances and responses established by \n\norganizational, domain, discipline, sector, or professional requirements. In lieu of such \n\nguidance, organizations can develop risk response plans based on strategies such as \n\naccepted model risk management, enterprise risk management, and information sharing \n\nand disclosure practices. \n\nSuggested Actions \n\n• Observe regulatory and established organizational, sector, discipline, or professional \n\nstandards and requirements for applying risk tolerances within the organization. \n\n• Document procedures for acting on AI system risks related to trustworthiness \n\ncharacteristics. \n\n• Prioritize risks involving physical safety, legal liabilities, regulatory compliance, and \n\nnegative impacts on individuals, groups, or society. \n\n• Identify risk response plans and resources and organizational teams for carrying out \n\nresponse functions. \n\n• Store risk management and system documentation in an organized, secure repository \n\nthat is accessible by relevant AI Actors and appropriate personnel. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Has the system been reviewed to ensure the AI system complies with relevant laws, \n\nregulations, standards, and guidance? \n\n• To what extent has the entity defined and documented the regulatory environment —\n\nincluding minimum requirements in laws and regulations? \n\n• Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Datasheets for Datasets. \n\nReferences \n\nArvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). 39 of 142 \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. \n\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, \n\n2021). \n\nFraser, Henry L and Bello y Villarino, Jose -Miguel, Where Residual Risks Reside: A \n\nComparative Approach to Art 9(4) of the European Union's Proposed AI Regulation \n\n(September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), \n\nMicrosoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. \n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* \n\n'20). Association for Computing Machinery, New York, NY, USA, 695. \n\n# MANAGE 1.4 \n\nNegative residual risks (defined as the sum of all unmitigated risks) to both downstream \n\nacquirers of AI systems and end users are documented. \n\nAbout \n\nOrganizations may choose to accept or transfer some of the documented risks from MAP \n\nand MANAGE 1.3 and 2.1. Such risks, known as residual risk, may affect downstream AI \n\nactors such as those engaged in system procurement or use. Transparent monitoring and \n\nmanaging residual risks enables cost benefit analysis and the examination of potential \n\nvalues of AI systems versus its potential negative impacts. \n\nSuggested Actions \n\n• Document residual risks within risk response plans, denoting risks that have been \n\naccepted, transferred, or subject to minimal mitigation. \n\n• Establish procedures for disclosing residual risks to relevant downstream AI actors . \n\n• Inform relevant downstream AI actors of requirements for safe operation, known \n\nlimitations, and suggested warning labels as identified in MAP 3.4. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\n• Who will be responsible for maintaining, re -verifying, monitoring, and updating this AI \n\nonce deployed? \n\n• How will updates/revisions be documented and communicated? How often and by \n\nwhom? \n\n• How easily accessible and current is the information available to external stakeholders? 40 of 142 \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• Datasheets for Datasets. \n\nReferences \n\nArvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. \n\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, \n\n2021). \n\nFraser, Henry L and Bello y Villarino, Jose -Miguel, Where Residual Risks Reside: A \n\nComparative Approach to Art 9(4) of the European Union's Proposed AI Regulation \n\n(September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), \n\nMicrosoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. \n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* \n\n'20). Association for Computing Machinery, New York, NY, USA, 695. \n\n# MANAGE 2.1 \n\nResources required to manage AI risks are taken into account, along with viable non -AI \n\nalternative systems, approaches, or methods – to reduce the magnitude or likelihood of \n\npotential impacts. \n\nAbout \n\nOrganizational risk response may entail identifying and analyzing alternative approaches, \n\nmethods, processes or systems, and balancing tradeoffs between trustworthiness \n\ncharacteristics and how they relate to organizational principles and societal values. Analysis \n\nof these tradeoffs is informed by consulting with interdisciplinary organizational teams, \n\nindependent domain experts, and engaging with individuals or community groups. These \n\nprocesses require sufficient resource allocation. \n\nSuggested Actions \n\n• Plan and implement risk management practices in accordance with established \n\norganizational risk tolerances. \n\n• Verify risk management teams are resourced to carry out functions, including 41 of 142 \n\n• Establishing processes for considering methods that are not automated; semi -\n\nautomated; or other procedural alternatives for AI functions. \n\n• Enhance AI system transparency mechanisms for AI teams. \n\n• Enable exploration of AI system limitations by AI teams. \n\n• Identify, assess, and catalog past failed designs and negative impacts or outcomes to \n\navoid known failure modes. \n\n• Identify resource allocation approaches for managing risks in systems: \n\n• deemed high -risk, \n\n• that self -update (adaptive, online, reinforcement self -supervised learning or \n\nsimilar), \n\n• trained without access to ground truth (unsupervised, semi -supervised, learning or \n\nsimilar), \n\n• with high uncertainty or where risk management is insufficient. \n\n• Regularly seek and integrate external expertise and perspectives to supplement \n\norganizational diversity (e.g. demographic, disciplinary), equity, inclusion, and \n\naccessibility where internal capacity is lacking. \n\n• Enable and encourage regular, open communication and feedback among AI actors and \n\ninternal or external stakeholders related to system design or deployment decisions. \n\n• Prepare and document plans for continuous monitoring and feedback mechanisms. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Are mechanisms in place to evaluate whether internal teams are empowered and \n\nresourced to effectively carry out risk management functions? \n\n• How will user and other forms of stakeholder engagement be integrated into risk \n\nmanagement processes? \n\nAI Transparency Resources \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• Datasheets for Datasets. \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nReferences \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nDavid Wright. 2013. Making Privacy Impact Assessments More Effective. The Information \n\nSociety, 29 (Oct 2013), 307 -315. 42 of 142 \n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model \n\nReporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency \n\n(FAT* '19). Association for Computing Machinery, New York, NY, USA, 220 –229. \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. \n\narXiv:1803.09010. \n\n# MANAGE 2.2 \n\nMechanisms are in place and applied to sustain the value of deployed AI systems. \n\nAbout \n\nSystem performance and trustworthiness may evolve and shift over time, once an AI system \n\nis deployed and put into operation. This phenomenon, generally known as drift, can degrade \n\nthe value of the AI system to the organization and increase the likelihood of negative \n\nimpacts. Regular monitoring of AI systems’ performance and trustworthiness enhances \n\norganizations’ ability to detect and respond to drift, and thus sustain an AI system’s value \n\nonce deployed. Processes and mechanisms for regular monitoring address system \n\nfunctionality and behavior - as well as impacts and alignment with the values and norms \n\nwithin the specific context of use. For example, considerations regarding impacts on \n\npersonal or public safety or privacy may include limiting high speeds when operating \n\nautonomous vehicles or restricting illicit content recommendations for minors. \n\nRegular monitoring activities can enable organizations to systematically and proactively \n\nidentify emergent risks and respond according to established protocols and metrics. \n\nOptions for organizational responses include 1) avoiding the risk, 2)accepting the risk, 3) \n\nmitigating the risk, or 4) transferring the risk. Each of these actions require planning and \n\nresources. Organizations are encouraged to establish risk management protocols with \n\nconsideration of the trustworthiness characteristics, the deployment context, and real \n\nworld impacts. \n\nSuggested Actions \n\n• Establish risk controls considering trustworthiness characteristics, including: \n\n• Data management, quality, and privacy (e.g. minimization, rectification or \n\ndeletion requests) controls as part of organizational data governance policies. \n\n• Machine learning and end -point security countermeasures (e.g., robust models, \n\ndifferential privacy, authentication, throttling). \n\n• Business rules that augment, limit or restrict AI system outputs within certain \n\ncontexts \n\n• Utilizing domain expertise related to deployment context for continuous \n\nimprovement and TEVV across the AI lifecycle. \n\n• Development and regular tracking of human -AI teaming configurations. 43 of 142 \n\n• Model assessment and test, evaluation, validation and verification (TEVV) \n\nprotocols. \n\n• Use of standardized documentation and transparency mechanisms. \n\n• Software quality assurance practices across AI lifecycle. \n\n• Mechanisms to explore system limitations and avoid past failed designs or \n\ndeployments. \n\n• Establish mechanisms to capture feedback from system end users and potentially \n\nimpacted groups while system is in deployment. \n\n• stablish mechanisms to capture feedback from system end users and potentially \n\nimpacted groups about how changes in system deployment (e.g., introducing new \n\ntechnology, decommissioning algorithms and models, adapting system, model or \n\nalgorithm) may create negative impacts that are not visible along the AI lifecycle. \n\n• Review insurance policies, warranties, or contracts for legal or oversight requirements \n\nfor risk transfer procedures. \n\n• Document risk tolerance decisions and risk acceptance procedures. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\n• Could the AI system expose people to harm or negative impacts? What was done to \n\nmitigate or reduce the potential for harm? \n\n• How will the accountable human(s) address changes in accuracy and precision due to \n\neither an adversary’s attempts to disrupt the AI or unrelated changes in the operational \n\nor business environment? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nSafety, Validity and Reliability Risk Management Approaches and Resources \n\nAI Incident Database. 2022. AI Incident Database. \n\nAIAAIC Repository. 2022. AI, algorithmic and automation incidents collected, dissected, \n\nexamined, and divulged. \n\nAlexander D'Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification \n\nPresents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. 44 of 142 \n\nAndrew L. Beam, Arjun K. Manrai, Marzyeh Ghassemi. 2020. Challenges to the \n\nReproducibility of Machine Learning Models in Health Care. Jama 323, 4 (January 6, 2020), \n\n305 -306. \n\nAnthony M. Barrett, Dan Hendrycks, Jessica Newman et al. 2022. Actionable Guidance for \n\nHigh -Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic \n\nRisks. arXiv:2206.08966. \n\nDebugging Machine Learning Models, In Proceedings of ICLR 2019 Workshop, May 6, 2019, \n\nNew Orleans, Louisiana. \n\nJessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, \n\nExploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. \n\nJoelle Pineau, Philippe Vincent -Lamarre, Koustuv Sinha, et al. 2020. Improving \n\nReproducibility in Machine Learning Research (A Report from the NeurIPS 2019 \n\nReproducibility Program) arXiv:2003.12206. \n\nKirstie Whitaker. 2017. Showing your working: a how to guide to reproducible research. \n\n(August 2017). \n\n[LINK](https://github.com/WhitakerLab/ReproducibleResearch/blob/master/PRESENTA \n\nTIONS/Whitaker_ICON_August2017.pdf), \n\nNetflix. Chaos Monkey. \n\nPeter Henderson, Riashat Islam, Philip Bachman, et al. 2018. Deep reinforcement learning \n\nthat matters. Proceedings of the AAAI Conference on Artificial Intelligence. 32, 1 (Apr. \n\n2018). \n\nSuchi Saria, Adarsh Subbaswamy. 2019. Tutorial: Safe and Reliable Machine Learning. \n\narXiv:1904.07204. \n\nKang, Daniel, Deepti Raghavan, Peter Bailis, and Matei Zaharia. \"Model assertions for \n\nmonitoring and improving ML models.\" Proceedings of Machine Learning and Systems 2 \n\n(2020): 481 -496. \n\nManaging Risk Bias \n\nNational Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et \n\nal. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing \n\nBias in Artificial Intelligence. \n\nBias Testing and Remediation Approaches \n\nAlekh Agarwal, Alina Beygelzimer, Miroslav Dudík, et al. 2018. A Reductions Approach to \n\nFair Classification. arXiv:1803.02453. \n\nBrian Hu Zhang, Blake Lemoine, Margaret Mitchell. 2018. Mitigating Unwanted Biases with \n\nAdversarial Learning. arXiv:1801.07593. 45 of 142 \n\nDrago Plečko, Nicolas Bennett, Nicolai Meinshausen. 2021. Fairadapt: Causal Reasoning for \n\nFair Data Pre -processing. arXiv:2110.10200. \n\nFaisal Kamiran, Toon Calders. 2012. Data Preprocessing Techniques for Classification \n\nwithout Discrimination. Knowledge and Information Systems 33 (2012), 1 –33. \n\nFaisal Kamiran; Asim Karim; Xiangliang Zhang. 2012. Decision Theory for Discrimination -\n\nAware Classification. In Proceedings of the 2012 IEEE 12th International Conference on \n\nData Mining, December 10 -13, 2012, Brussels, Belgium. IEEE, 924 -929. \n\nFlavio P. Calmon, Dennis Wei, Karthikeyan Natesan Ramamurthy, et al. 2017. Optimized \n\nData Pre -Processing for Discrimination Prevention. arXiv:1704.03354. \n\nGeoff Pleiss, Manish Raghavan, Felix Wu, et al. 2017. On Fairness and Calibration. \n\narXiv:1709.02012. \n\nL. Elisa Celis, Lingxiao Huang, Vijay Keswani, et al. 2020. Classification with Fairness \n\nConstraints: A Meta -Algorithm with Provable Guarantees. arXiv:1806.06055. \n\nMichael Feldman, Sorelle Friedler, John Moeller, et al. 2014. Certifying and Removing \n\nDisparate Impact. arXiv:1412.3756. \n\nMichael Kearns, Seth Neel, Aaron Roth, et al. 2017. Preventing Fairness Gerrymandering: \n\nAuditing and Learning for Subgroup Fairness. arXiv:1711.05144. \n\nMichael Kearns, Seth Neel, Aaron Roth, et al. 2018. An Empirical Study of Rich Subgroup \n\nFairness for Machine Learning. arXiv:1808.08166. \n\nMoritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of Opportunity in Supervised \n\nLearning. In Proceedings of the 30th Conference on Neural Information Processing Systems \n\n(NIPS 2016), 2016, Barcelona, Spain. \n\nRich Zemel, Yu Wu, Kevin Swersky, et al. 2013. Learning Fair Representations. In \n\nProceedings of the 30th International Conference on Machine Learning 2013, PMLR 28, 3, \n\n325 -333. \n\nToshihiro Kamishima, Shotaro Akaho, Hideki Asoh & Jun Sakuma. 2012. Fairness -Aware \n\nClassifier with Prejudice Remover Regularizer. In Peter A. Flach, Tijl De Bie, Nello Cristianini \n\n(eds) Machine Learning and Knowledge Discovery in Databases. European Conference \n\nECML PKDD 2012, Proceedings Part II, September 24 -28, 2012, Bristol, UK. Lecture Notes in \n\nComputer Science 7524. Springer, Berlin, Heidelberg. \n\nSecurity and Resilience Resources \n\nFTC Start With Security Guidelines. 2015. \n\nGary McGraw et al. 2022. BIML Interactive Machine Learning Risk Framework. Berryville \n\nInstitute for Machine Learning. 46 of 142 \n\nIlia Shumailov, Yiren Zhao, Daniel Bates, et al. 2021. Sponge Examples: Energy -Latency \n\nAttacks on Neural Networks. arXiv:2006.03463. \n\nMarco Barreno, Blaine Nelson, Anthony D. Joseph, et al. 2010. The Security of Machine \n\nLearning. Machine Learning 81 (2010), 121 -148. \n\nMatt Fredrikson, Somesh Jha, Thomas Ristenpart. 2015. Model Inversion Attacks that \n\nExploit Confidence Information and Basic Countermeasures. In Proceedings of the 22nd \n\nACM SIGSAC Conference on Computer and Communications Security (CCS '15), October \n\n2015. Association for Computing Machinery, New York, NY, USA, 1322 –1333. \n\nNational Institute for Standards and Technology (NIST). 2022. Cybersecurity Framework. \n\nNicolas Papernot. 2018. A Marauder's Map of Security and Privacy in Machine Learning. \n\narXiv:1811.01134. \n\nReza Shokri, Marco Stronati, Congzheng Song, et al. 2017. Membership Inference Attacks \n\nagainst Machine Learning Models. arXiv:1610.05820. \n\nAdversarial Threat Matrix (MITRE). 2021. \n\nInterpretability and Explainability Approaches \n\nChaofan Chen, Oscar Li, Chaofan Tao, et al. 2019. This Looks Like That: Deep Learning for \n\nInterpretable Image Recognition. arXiv:1806.10574. \n\nCynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes \n\ndecisions and use interpretable models instead. arXiv:1811.10154. \n\nDaniel W. Apley, Jingyu Zhu. 2019. Visualizing the Effects of Predictor Variables in Black Box \n\nSupervised Learning Models. arXiv:1612.08468. \n\nDavid A. Broniatowski. 2021. Psychological Foundations of Explainability and \n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology \n\n(NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD. \n\nForough Poursabzi -Sangdeh, Daniel G. Goldstein, Jake M. Hofman, et al. 2021. Manipulating \n\nand Measuring Model Interpretability. arXiv:1802.07810. \n\nHongyu Yang, Cynthia Rudin, Margo Seltzer. 2017. Scalable Bayesian Rule Lists. \n\narXiv:1602.08610. \n\nP. Jonathon Phillips, Carina A. Hahn, Peter C. Fontana, et al. 2021. Four Principles of \n\nExplainable Artificial Intelligence. National Institute of Standards and Technology (NIST) IR \n\n8312. National Institute of Standards and Technology, Gaithersburg, MD. \n\nScott Lundberg, Su -In Lee. 2017. A Unified Approach to Interpreting Model Predictions. \n\narXiv:1705.07874. 47 of 142 \n\nSusanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in \n\ndeployment of clinical decision -aids. npj Digital Medicine 4, Article 31 (2021). \n\nYin Lou, Rich Caruana, Johannes Gehrke, et al. 2013. Accurate intelligible models with \n\npairwise interactions. In Proceedings of the 19th ACM SIGKDD international conference on \n\nKnowledge discovery and data mining (KDD '13), August 2013. Association for Computing \n\nMachinery, New York, NY, USA, 623 –631. \n\nPost -Decommission \n\nUpol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. “The Algorithmic Imprint.” \n\nProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency \n\n(2022). \n\nPrivacy Resources \n\nNational Institute for Standards and Technology (NIST). 2022. Privacy Framework. \n\nData Governance \n\nMarijn Janssen, Paul Brous, Elsa Estevez, Luis S. Barbosa, Tomasz Janowski, Data \n\ngovernance: Organizing data for trustworthy Artificial Intelligence, Government \n\nInformation Quarterly, Volume 37, Issue 3, 2020, 101493, ISSN 0740 -624X. \n\nSoftware Resources \n\n• PiML  (explainable models, performance assessment) \n\n• Interpret  (explainable models) \n\n• Iml  (explainable models) \n\n• Drifter  library (performance assessment) \n\n• Manifold  library (performance assessment) \n\n• SALib  library (performance assessment) \n\n• What -If Tool  (performance assessment) \n\n• MLextend  (performance assessment) \n\n- AI Fairness 360: \n\n• Python  (bias testing and mitigation) \n\n• R (bias testing and mitigation) \n\n• Adversarial -robustness -toolbox  (ML security) \n\n• Robustness  (ML security) \n\n• tensorflow/privacy  (ML security) \n\n• NIST De -identification Tools  (Privacy and ML security) \n\n• Dvc  (MLops, deployment) \n\n• Gigantum  (MLops, deployment) \n\n• Mlflow  (MLops, deployment) \n\n• Mlmd  (MLops, deployment) \n\n• Modeldb  (MLops, deployment) 48 of 142 \n\n# MANAGE 2.3 \n\nProcedures are followed to respond to and recover from a previously unknown risk when it \n\nis identified. \n\nAbout \n\nAI systems – like any technology – can demonstrate non -functionality or failure or \n\nunexpected and unusual behavior. They also can be subject to attacks, incidents, or other \n\nmisuse or abuse – which their sources are not always known apriori. Organizations can \n\nestablish, document, communicate and maintain treatment procedures to recognize and \n\ncounter, mitigate and manage risks that were not previously identified. \n\nSuggested Actions \n\n• Protocols, resources, and metrics are in place for continual monitoring of AI systems’ \n\nperformance, trustworthiness, and alignment with contextual norms and values \n\n• Establish and regularly review treatment and response plans for incidents, negative \n\nimpacts, or outcomes. \n\n• Establish and maintain procedures to regularly monitor system components for drift, \n\ndecontextualization, or other AI system behavior factors, \n\n• Establish and maintain procedures for capturing feedback about negative impacts. \n\n• Verify contingency processes to handle any negative impacts associated with mission -\n\ncritical AI systems, and to deactivate systems. \n\n• Enable preventive and post -hoc exploration of AI system limitations by relevant AI actor \n\ngroups. \n\n• Decommission systems that exceed risk tolerances. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Who will be responsible for maintaining, re -verifying, monitoring, and updating this AI \n\nonce deployed? \n\n• Are the responsibilities of the personnel involved in the various AI governance \n\nprocesses clearly defined? (Including responsibilities to decommission the AI system.) \n\n• What processes exist for data generation, acquisition/collection, ingestion, \n\nstaging/storage, transformations, security, maintenance, and dissemination? \n\n• How will the appropriate performance metrics, such as accuracy, of the AI be monitored \n\nafter the AI is deployed? \n\nAI Transparency Resources \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF - Companion to the Model AI Governance Framework – Implementation and Self -\n\nAssessment Guide for Organizations. \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. 49 of 142 \n\nReferences \n\nAI Incident Database. 2022. AI Incident Database. \n\nAIAAIC Repository. 2022. AI, algorithmic and automation incidents collected, dissected, \n\nexamined, and divulged. \n\nAndrew Burt and Patrick Hall. 2018. What to Do When AI Fails. O’Reilly Media, Inc. (May 18, \n\n2020). Retrieved October 17, 2022. \n\nNational Institute for Standards and Technology (NIST). 2022. Cybersecurity Framework. \n\nSANS Institute. 2022. Security Consensus Operational Readiness Evaluation (SCORE) \n\nSecurity Checklist [or Advanced Persistent Threat (APT) Handling Checklist]. \n\nSuchi Saria, Adarsh Subbaswamy. 2019. Tutorial: Safe and Reliable Machine Learning. \n\narXiv:1904.07204. \n\n# MANAGE 2.4 \n\nMechanisms are in place and applied, responsibilities are assigned and understood to \n\nsupersede, disengage, or deactivate AI systems that demonstrate performance or outcomes \n\ninconsistent with intended use. \n\nAbout \n\nPerformance inconsistent with intended use does not always increase risk or lead to \n\nnegative impacts. Rigorous TEVV practices are useful for protecting against negative \n\nimpacts regardless of intended use. When negative impacts do arise, superseding \n\n(bypassing), disengaging, or deactivating/decommissioning a model, AI system \n\ncomponent(s), or the entire AI system may be necessary, such as when: \n\n• a system reaches the end of its lifetime \n\n• detected or identified risks exceed tolerance thresholds \n\n• adequate system mitigation actions are beyond the organization’s capacity \n\n• feasible system mitigation actions do not meet regulatory, legal, norms or standards. \n\n• impending risk is detected during continual monitoring, for which feasible mitigation \n\ncannot be identified or implemented in a timely fashion. \n\nSafely removing AI systems from operation, either temporarily or permanently, under these \n\nscenarios requires standard protocols that minimize operational disruption and \n\ndownstream negative impacts. Protocols can involve redundant or backup systems that are \n\ndeveloped in alignment with established system governance policies (see GOVERN 1.7), \n\nregulatory compliance, legal frameworks, business requirements and norms and l standards \n\nwithin the application context of use. Decision thresholds and metrics for action s to bypass \n\nor deactivate system components are part of continual monitoring procedures. Incidents \n\nthat result in a bypass/deactivate decision require documentation and review to \n\nunderstand root causes, impacts, and potential opportunities for mitigation and \n\nredeployment. Organizations are encouraged to develop risk and change management 50 of 142 \n\nprotocols that consider and anticipate upstream and downstream consequences of both \n\ntemporary and/or permanent decommissioning, and provide contingency options. \n\nSuggested Actions \n\n• Regularly review established procedures for AI system bypass actions, including plans \n\nfor redundant or backup systems to ensure continuity of operational and/or business \n\nfunctionality. \n\n• Regularly review Identify system incident thresholds for activating bypass or \n\ndeactivation responses. \n\n• Apply change management processes to understand the upstream and downstream \n\nconsequences of bypassing or deactivating an AI system or AI system components. \n\n• Apply protocols, resources and metrics for decisions to supersede, bypass or deactivate \n\nAI systems or AI system components. \n\n• Preserve materials for forensic, regulatory, and legal review. \n\n• Conduct internal root cause analysis and process reviews of bypass or deactivation \n\nevents. \n\n• Decommission and preserve system components that cannot be updated to meet \n\ncriteria for redeployment. \n\n• Establish criteria for redeploying updated system components, in consideration of \n\ntrustworthy characteristics \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\n• Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\n• What testing, if any, has the entity conducted on the AI system to identify errors and \n\nlimitations (i.e. adversarial or stress testing)? \n\n• To what extent does the entity have established procedures for retiring the AI system, if \n\nit is no longer needed? \n\n• How did the entity use assessments and/or evaluations to determine if the system can \n\nbe scaled up, continue, or be decommissioned? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nReferences \n\nDecommissioning Template. Application Lifecycle And Supporting Docs. Cloud and \n\nInfrastructure Community of Practice. 51 of 142 \n\nDevelop a Decommission Plan. M3 Playbook. Office of Shared Services and Solutions and \n\nPerformance Improvement. General Services Administration. \n\n# MANAGE 3.1 \n\nAI risks and benefits from third -party resources are regularly monitored, and risk controls \n\nare applied and documented. \n\nAbout \n\nAI systems may depend on external resources and associated processes, including third -\n\nparty data, software or hardware systems. Third parties’ supplying organizations with \n\ncomponents and services, including tools, software, and expertise for AI system design, \n\ndevelopment, deployment or use can improve efficiency and scalability. It can also increase \n\ncomplexity and opacity, and, in -turn, risk. Documenting third -party technologies, personnel, \n\nand resources that were employed can help manage risks. Focusing first and foremost on \n\nrisks involving physical safety, legal liabilities, regulatory compliance, and negative impacts \n\non individuals, groups, or society is recommended. \n\nSuggested Actions \n\n• Have legal requirements been addressed? \n\n• Apply organizational risk tolerance to third -party AI systems. \n\n• Apply and document organizational risk management plans and practices to third -party \n\nAI technology, personnel, or other resources. \n\n• Identify and maintain documentation for third -party AI systems and components. \n\n• Establish testing, evaluation, validation and verification processes for third -party AI \n\nsystems which address the needs for transparency without exposing proprietary \n\nalgorithms . \n\n• Establish processes to identify beneficial use and risk indicators in third -party systems \n\nor components, such as inconsistent software release schedule, sparse documentation, \n\nand incomplete software change management (e.g., lack of forward or backward \n\ncompatibility). \n\n• Organizations can establish processes for third parties to report known and potential \n\nvulnerabilities, risks or biases in supplied resources. \n\n• Verify contingency processes for handling negative impacts associated with mission -\n\ncritical third -party AI systems. \n\n• Monitor third -party AI systems for potential negative impacts and risks associated with \n\ntrustworthiness characteristics. \n\n• Decommission third -party systems that exceed risk tolerances. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• If a third party created the AI system or some of its components, how will you ensure a \n\nlevel of explainability or interpretability? Is there documentation? 52 of 142 \n\n• If your organization obtained datasets from a third party, did your organization assess \n\nand manage the risks of using such datasets? \n\n• Did you establish a process for third parties (e.g. suppliers, end users, subjects, \n\ndistributors/vendors or workers) to report potential vulnerabilities, risks or biases in \n\nthe AI system? \n\n• Have legal requirements been addressed? \n\nAI Transparency Resources \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF - Companion to the Model AI Governance Framework – Implementation and Self -\n\nAssessment Guide for Organizations. \n\n• Datasheets for Datasets. \n\nReferences \n\nOffice of the Comptroller of the Currency. 2021. Proposed Interagency Guidance on Third -\n\nParty Relationships: Risk Management. July 12, 2021. \n\n# MANAGE 3.2 \n\nPre -trained models which are used for development are monitored as part of AI system \n\nregular monitoring and maintenance. \n\nAbout \n\nA common approach in AI development is transfer learning, whereby an existing pre -\n\ntrained model is adapted for use in a different, but related application. AI actors in \n\ndevelopment tasks often use pre -trained models from third -party entities for tasks such as \n\nimage classification, language prediction, and entity recognition, because the resources to \n\nbuild such models may not be readily available to most organizations. Pre -trained models \n\nare typically trained to address various classification or prediction problems, using \n\nexceedingly large datasets and computationally intensive resources. The use of pre -trained \n\nmodels can make it difficult to anticipate negative system outcomes or impacts. Lack of \n\ndocumentation or transparency tools increases the difficulty and general complexity when \n\ndeploying pre -trained models and hinders root cause analyses. \n\nSuggested Actions \n\n• Identify pre -trained models within AI system inventory for risk tracking. \n\n• Establish processes to independently and continually monitor performance and \n\ntrustworthiness of pre -trained models, and as part of third -party risk tracking. \n\n• Monitor performance and trustworthiness of AI system components connected to pre -\n\ntrained models, and as part of third -party risk tracking. \n\n• Identify, document and remediate risks arising from AI system components and pre -\n\ntrained models per organizational risk management procedures, and as part of third -\n\nparty risk tracking. \n\n• Decommission AI system components and pre -trained models which exceed risk \n\ntolerances, and as part of third -party risk tracking. 53 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• How has the entity documented the AI system’s data provenance, including sources, \n\norigins, transformations, augmentations, labels, dependencies, constraints, and \n\nmetadata? \n\n• Does this dataset collection/processing procedure achieve the motivation for creating \n\nthe dataset stated in the first section of this datasheet? \n\n• How does the entity ensure that the data collected are adequate, relevant, and not \n\nexcessive in relation to the intended purpose? \n\n• If the dataset becomes obsolete how will this be communicated? \n\nAI Transparency Resources \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF - Companion to the Model AI Governance Framework – Implementation and Self -\n\nAssessment Guide for Organizations. \n\n• Datasheets for Datasets. \n\nReferences \n\nLarysa Visengeriyeva et al. “Awesome MLOps,“ GitHub. Accessed January 9, 2023. \n\n# MANAGE 4.1 \n\nPost -deployment AI system monitoring plans are implemented, including mechanisms for \n\ncapturing and evaluating input from users and other relevant AI actors, appeal and \n\noverride, decommissioning, incident response, recovery, and change management. \n\nAbout \n\nAI system performance and trustworthiness can change due to a variety of factors. Regular \n\nAI system monitoring can help deployers identify performance degradations, adversarial \n\nattacks, unexpected and unusual behavior, near -misses, and impacts. Including pre - and \n\npost -deployment external feedback about AI system performance can enhance \n\norganizational awareness about positive and negative impacts, and reduce the time to \n\nrespond to risks and harms. \n\nSuggested Actions \n\n• Establish and maintain procedures to monitor AI system performance for risks and \n\nnegative and positive impacts associated with trustworthiness characteristics. \n\n• Perform post -deployment TEVV tasks to evaluate AI system validity and reliability, bias \n\nand fairness, privacy, and security and resilience. \n\n• Evaluate AI system trustworthiness in conditions similar to deployment context of use, \n\nand prior to deployment. \n\n• Establish and implement red -teaming exercises at a prescribed cadence, and evaluate \n\ntheir efficacy. 54 of 142 \n\n• Establish procedures for tracking dataset modifications such as data deletion or \n\nrectification requests. \n\n• Establish mechanisms for regular communication and feedback between relevant AI \n\nactors and internal or external stakeholders to capture information about system \n\nperformance, trustworthiness and impact. \n\n• Share information about errors, near -misses, and attack patterns with incident \n\ndatabases, other organizations with similar systems, and system users and \n\nstakeholders. \n\n• Respond to and document detected or reported negative impacts or issues in AI system \n\nperformance and trustworthiness. \n\n• Decommission systems that exceed establish risk tolerances. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent has the entity documented the post -deployment AI system’s testing \n\nmethodology, metrics, and performance outcomes? \n\n• How easily accessible and current is the information available to external stakeholders? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities, \n\n• Datasheets for Datasets. \n\nReferences \n\nNavdeep Gill, Patrick Hall, Kim Montgomery, and Nicholas Schmidt. \"A Responsible Machine \n\nLearning Workflow with Focus on Interpretable Models, Post -hoc Explanation, and \n\nDiscrimination Testing.\" Information 11, no. 3 (2020): 137. \n\n# MANAGE 4.2 \n\nMeasurable activities for continual improvements are integrated into AI system updates \n\nand include regular engagement with interested parties, including relevant AI actors. \n\nAbout \n\nRegular monitoring processes enable system updates to enhance performance and \n\nfunctionality in accordance with regulatory and legal frameworks, and organizational and \n\ncontextual values and norms. These processes also facilitate analyses of root causes, system \n\ndegradation, drift, near -misses, and failures, and incident response and documentation. \n\nAI actors across the lifecycle have many opportunities to capture and incorporate external \n\nfeedback about system performance, limitations, and impacts, and implement continuous \n\nimprovements. Improvements may not always be to model pipeline or system processes, \n\nand may instead be based on metrics beyond accuracy or other quality performance \n\nmeasures. In these cases, improvements may entail adaptations to business or \n\norganizational procedures or practices. Organizations are encouraged to develop 55 of 142 \n\nimprovements that will maintain traceability and transparency for developers, end users, \n\nauditors, and relevant AI actors. \n\nSuggested Actions \n\n• Integrate trustworthiness characteristics into protocols and metrics used for continual \n\nimprovement. \n\n• Establish processes for evaluating and integrating feedback into AI system \n\nimprovements. \n\n• Assess and evaluate alignment of proposed improvements with relevant regulatory and \n\nlegal frameworks \n\n• Assess and evaluate alignment of proposed improvements connected to the values and \n\nnorms within the context of use. \n\n• Document the basis for decisions made relative to tradeoffs between trustworthy \n\ncharacteristics, system risks, and system opportunities \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• How will user and other forms of stakeholder engagement be integrated into the model \n\ndevelopment process and regular performance review once deployed? \n\n• To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\n• To what extent has the entity defined and documented the regulatory environment —\n\nincluding minimum requirements in laws and regulations? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities, \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nYen, Po -Yin, et al. \"Development and Evaluation of Socio -Technical Metrics to Inform HIT \n\nAdaptation.\" \n\nCarayon, Pascale, and Megan E. Salwei. \"Moving toward a sociotechnical systems approach \n\nto continuous health information technology design: the path forward for improving \n\nelectronic health record usability and reducing clinician burnout.\" Journal of the American \n\nMedical Informatics Association 28.5 (2021): 1026 -1028. \n\nMishra, Deepa, et al. \"Organizational capabilities that enable big data and predictive \n\nanalytics diffusion and organizational performance: A resource -based perspective.\" \n\nManagement Decision (2018). 56 of 142 \n\n# MANAGE 4.3 \n\nIncidents and errors are communicated to relevant AI actors including affected \n\ncommunities. Processes for tracking, responding to, and recovering from incidents and \n\nerrors are followed and documented. \n\nAbout \n\nRegularly documenting an accurate and transparent account of identified and reported \n\nerrors can enhance AI risk management activities., Examples include: \n\n• how errors were identified, \n\n• incidents related to the error, \n\n• whether the error has been repaired, and \n\n• how repairs can be distributed to all impacted stakeholders and users. \n\nSuggested Actions \n\n• Establish procedures to regularly share information about errors, incidents and \n\nnegative impacts with relevant stakeholders, operators, practitioners and users, and \n\nimpacted parties. \n\n• Maintain a database of reported errors, near -misses, incidents and negative impacts \n\nincluding date reported, number of reports, assessment of impact and severity, and \n\nresponses. \n\n• Maintain a database of system changes, reason for change, and details of how the change \n\nwas made, tested and deployed. \n\n• Maintain version history information and metadata to enable continuous improvement \n\nprocesses. \n\n• Verify that relevant AI actors responsible for identifying complex or emergent risks are \n\nproperly resourced and empowered. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What corrective actions has the entity taken to enhance the quality, accuracy, reliability, \n\nand representativeness of the data? \n\n• To what extent does the entity communicate its AI strategic goals and objectives to the \n\ncommunity of stakeholders? How easily accessible and current is the information \n\navailable to external stakeholders? \n\n• What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities, 57 of 142 \n\nReferences \n\nWei, M., & Zhou, Z. (2022). AI Ethics Issues in Real World: Evidence from AI Incident \n\nDatabase. ArXiv, abs/2206.07635. \n\nMcGregor, Sean. \"Preventing repeated real world AI failures by cataloging incidents: The AI \n\nincident database.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. \n\nNo. 17. 2021. \n\nMacrae, Carl. \"Learning from the failure of autonomous and intelligent systems: Accidents, \n\nsafety, and sociotechnical sources of risk.\" Risk analysis 42.9 (2022): 1999 -2025. MAP 58 of 142 \n\n# Map \n\nContext is established and understood. \n\n# MAP 1.1 \n\nIntended purpose, potentially beneficial uses, context -specific laws, norms and \n\nexpectations, and prospective settings in which the AI system will be deployed are \n\nunderstood and documented. Considerations include: specific set or types of users along \n\nwith their expectations; potential positive and negative impacts of system uses to \n\nindividuals, communities, organizations, society, and the planet; assumptions and related \n\nlimitations about AI system purposes; uses and risks across the development or product AI \n\nlifecycle; TEVV and system metrics. \n\nAbout \n\nHighly accurate and optimized systems can cause harm. Relatedly, organizations should \n\nexpect broadly deployed AI tools to be reused, repurposed, and potentially misused \n\nregardless of intentions. \n\nAI actors can work collaboratively, and with external parties such as community groups, to \n\nhelp delineate the bounds of acceptable deployment, consider preferable alternatives, and \n\nidentify principles and strategies to manage likely risks. Context mapping is the first step in \n\nthis effort, and may include examination of the following: \n\n• intended purpose and impact of system use. \n\n• concept of operations. \n\n• intended, prospective, and actual deployment setting. \n\n• requirements for system deployment and operation. \n\n• end user and operator expectations. \n\n• specific set or types of end users. \n\n• potential negative impacts to individuals, groups, communities, organizations, and \n\nsociety – or context -specific impacts such as legal requirements or impacts to the \n\nenvironment. \n\n• unanticipated, downstream, or other unknown contextual factors. \n\n• how AI system changes connect to impacts. \n\nThese types of processes can assist AI actors in understanding how limitations, constraints, \n\nand other realities associated with the deployment and use of AI technology can create \n\nimpacts once they are deployed or operate in the real world. When coupled with the \n\nenhanced organizational culture resulting from the established policies and procedures in \n\nthe Govern function, the Map function can provide opportunities to foster and instill new \n\nperspectives, activities, and skills for approaching risks and impacts. \n\nContext mapping also includes discussion and consideration of non -AI or non -technology \n\nalternatives especially as related to whether the given context is narrow enough to manage 59 of 142 \n\nAI and its potential negative impacts. Non -AI alternatives may include capturing and \n\nevaluating information using semi -autonomous or mostly -manual methods. \n\nSuggested Actions \n\n• Maintain awareness of industry, technical, and applicable legal standards. \n\n• Examine trustworthiness of AI system design and consider, non -AI solutions \n\n• Consider intended AI system design tasks along with unanticipated purposes in \n\ncollaboration with human factors and socio -technical domain experts. \n\n• Define and document the task, purpose, minimum functionality, and benefits of the AI \n\nsystem to inform considerations about whether the utility of the project or its lack of. \n\n• Identify whether there are non -AI or non -technology alternatives that will lead to more \n\ntrustworthy outcomes. \n\n• Examine how changes in system performance affect downstream events such as \n\ndecision -making (e.g: changes in an AI model objective function create what types of \n\nimpacts in how many candidates do/do not get a job interview). \n\n• Determine actions to map and track post -decommissioning stages of AI deployment and \n\npotential negative or positive impacts to individuals, groups and communities. \n\n• Determine the end user and organizational requirements, including business and \n\ntechnical requirements. \n\n• Determine and delineate the expected and acceptable AI system context of use, \n\nincluding: \n\n• social norms \n\n• Impacted individuals, groups, and communities \n\n• potential positive and negative impacts to individuals, groups, communities, \n\norganizations, and society \n\n• operational environment \n\n• Perform context analysis related to time frame, safety concerns, geographic area, \n\nphysical environment, ecosystems, social environment, and cultural norms within the \n\nintended setting (or conditions that closely approximate the intended setting. \n\n• Gain and maintain awareness about evaluating scientific claims related to AI system \n\nperformance and benefits before launching into system design. \n\n• Identify human -AI interaction and/or roles, such as whether the application will \n\nsupport or replace human decision making. \n\n• Plan for risks related to human -AI configurations, and document requirements, roles, \n\nand responsibilities for human oversight of deployed systems. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent is the output of each component appropriate for the operational \n\ncontext? 60 of 142 \n\n• Which AI actors are responsible for the decisions of the AI and is this person aware of \n\nthe intended uses and limitations of the analytic? \n\n• Which AI actors are responsible for maintaining, re -verifying, monitoring, and updating \n\nthis AI once deployed? \n\n• Who is the person(s) accountable for the ethical considerations across the AI lifecycle? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities, \n\n• “Stakeholders in Explainable AI,” Sep. 2018. \n\n• \"Microsoft Responsible AI Standard, v2\". \n\nReferences \n\nSocio -technical systems \n\nAndrew D. Selbst, danah boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in \n\nSociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and \n\nTransparency (FAccT'19). Association for Computing Machinery, New York, NY, USA, 59 –68. \n\nProblem formulation \n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial \n\nintelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004 -3702. \n\nSamir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of \n\nthe Conference on Fairness, Accountability, and Transparency (FAccT'19). Association for \n\nComputing Machinery, New York, NY, USA, 39 –48. \n\nContext mapping \n\nEmilio Gómez -González and Emilia Gómez. 2020. Artificial intelligence in medicine and \n\nhealthcare. Joint Research Centre (European Commission). \n\nSarah Spiekermann and Till Winkler. 2020. Value -based Engineering for Ethics by Design. \n\narXiv:2004.13676. \n\nSocial Impact Lab. 2017. Framework for Context Analysis of Technologies in Social Change \n\nProjects (Draft v2.0). \n\nSolon Barocas, Asia J. Biega, Margarita Boyarskaya, et al. 2021. Responsible computing \n\nduring COVID -19 and beyond. Commun. ACM 64, 7 (July 2021), 30 –32. \n\nIdentification of harms \n\nHarini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm \n\nthroughout the Machine Learning Life Cycle. arXiv:1901.10002. \n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of \n\nImagination in AI Infused System Development and Deployment. arXiv:2011.13416. \n\nMicrosoft. Foundations of assessing harm. 2022. 61 of 142 \n\nUnderstanding and documenting limitations in ML \n\nAlexander D'Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification \n\nPresents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. \n\nArvind Narayanan. \"How to Recognize AI Snake Oil.\" Arthur Miller Lecture on Science and \n\nEthics (2019). \n\nJessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, \n\nExploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. \n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model \n\nReporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency \n\n(FAT* '19). Association for Computing Machinery, New York, NY, USA, 220 –229. \n\nMatthew Arnold, Rachel K. E. Bellamy, Michael Hind, et al. 2019. FactSheets: Increasing \n\nTrust in AI Services through Supplier's Declarations of Conformity. arXiv:1808.07261. \n\nMatthew J. Salganik, Ian Lundberg, Alexander T. Kindel, Caitlin E. Ahearn, Khaled Al -\n\nGhoneim, Abdullah Almaatouq, Drew M. Altschul et al. \"Measuring the Predictability of Life \n\nOutcomes with a Scientific Mass Collaboration.\" Proceedings of the National Academy of \n\nSciences 117, No. 15 (2020): 8398 -8403. \n\nMichael A. Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co -\n\nDesigning Checklists to Understand Organizational Challenges and Opportunities around \n\nFairness in AI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing \n\nSystems (CHI ‘20). Association for Computing Machinery, New York, NY, USA, 1 –14. \n\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. \n\narXiv:1803.09010. \n\nBender, E. M., Friedman, B. & McMillan -Major, A., (2022). A Guide for Writing Data \n\nStatements for Natural Language Processing. University of Washington. Accessed July 14, \n\n2022. \n\nMeta AI. System Cards, a new resource for understanding how AI systems work, 2021. \n\nWhen not to deploy \n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. \n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* \n\n'20). Association for Computing Machinery, New York, NY, USA, 695. \n\nPost -decommission \n\nUpol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. “The Algorithmic Imprint.” \n\nProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency \n\n(2022). 62 of 142 \n\nStatistical balance \n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting \n\nracial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 \n\nOct. 2019), 447 -453. \n\nAssessment of science in AI \n\nArvind Narayanan. How to recognize AI snake oil. \n\nEmily M. Bender. 2022. On NYT Magazine on AI: Resist the Urge to be Impressed. (April 17, \n\n2022). \n\n# MAP 1.2 \n\nInter -disciplinary AI actors, competencies, skills and capacities for establishing context \n\nreflect demographic diversity and broad domain and user experience expertise, and their \n\nparticipation is documented. Opportunities for interdisciplinary collaboration are \n\nprioritized. \n\nAbout \n\nSuccessfully mapping context requires a team of AI actors with a diversity of experience, \n\nexpertise, abilities and backgrounds, and with the resources and independence to engage in \n\ncritical inquiry. \n\nHaving a diverse team contributes to more broad and open sharing of ideas and \n\nassumptions about the purpose and function of the technology being designed and \n\ndeveloped – making these implicit aspects more explicit. The benefit of a diverse staff in \n\nmanaging AI risks is not the beliefs or presumed beliefs of individual workers, but the \n\nbehavior that results from a collective perspective. An environment which fosters critical \n\ninquiry creates opportunities to surface problems and identify existing and emergent risks. \n\nSuggested Actions \n\n• Establish interdisciplinary teams to reflect a wide range of skills, competencies, and \n\ncapabilities for AI efforts. Verify that team membership includes demographic diversity, \n\nbroad domain expertise, and lived experiences. Document team composition. \n\n• Create and empower interdisciplinary expert teams to capture, learn, and engage the \n\ninterdependencies of deployed AI systems and related terminologies and concepts from \n\ndisciplines outside of AI practice such as law, sociology, psychology, anthropology, \n\npublic policy, systems design, and engineering. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent do the teams responsible for developing and maintaining the AI system \n\nreflect diverse opinions, backgrounds, experiences, and perspectives? 63 of 142 \n\n• Did the entity document the demographics of those involved in the design and \n\ndevelopment of the AI system to capture and communicate potential biases inherent to \n\nthe development process, according to forum participants? \n\n• What specific perspectives did stakeholders share, and how were they integrated across \n\nthe design, development, deployment, assessment, and monitoring of the AI system? \n\n• To what extent has the entity addressed stakeholder perspectives on the potential \n\nnegative impacts of the AI system on end users and impacted populations? \n\n• What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\n• Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? Consulting the community or end users at the earliest \n\nstages of development to ensure there is transparency on the technology used and how \n\nit is deployed. \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\n• AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\nReferences \n\nSina Fazelpour and Maria De -Arteaga. 2022. Diversity in sociotechnical machine learning \n\nsystems. Big Data & Society 9, 1 (Jan. 2022). \n\nMicrosoft Community Jury , Azure Application Architecture Guide. \n\nFernando Delgado, Stephen Yang, Michael Madaio, Qian Yang. (2021). Stakeholder \n\nParticipation in AI: Beyond \"Add Diverse Stakeholders and Stir\". \n\nKush Varshney, Tina Park, Inioluwa Deborah Raji, Gaurush Hiranandani, Narasimhan \n\nHarikrishna, Oluwasanmi Koyejo, Brianna Richardson, and Min Kyung Lee. Participatory \n\nspecification of trustworthy machine learning, 2021. \n\nDonald Martin, Vinodkumar Prabhakaran, Jill A. Kuhlberg, Andrew Smart and William S. \n\nIsaac. “Participatory Problem Formulation for Fairer Machine Learning Through \n\nCommunity Based System Dynamics”, ArXiv abs/2005.07572 (2020). \n\n# MAP 1.3 \n\nThe organization’s mission and relevant goals for the AI technology are understood and \n\ndocumented. 64 of 142 \n\nAbout \n\nDefining and documenting the specific business purpose of an AI system in a broader \n\ncontext of societal values helps teams to evaluate risks and increases the clarity of “go/no -\n\ngo” decisions about whether to deploy. \n\nTrustworthy AI technologies may present a demonstrable business benefit beyond implicit \n\nor explicit costs, provide added value, and don't lead to wasted resources. Organizations can \n\nfeel confident in performing risk avoidance if the implicit or explicit risks outweigh the \n\nadvantages of AI systems, and not implementing an AI solution whose risks surpass \n\npotential benefits. \n\nFor example, making AI systems more equitable can result in better managed risk, and can \n\nhelp enhance consideration of the business value of making inclusively designed, accessible \n\nand more equitable AI systems. \n\nSuggested Actions \n\n• Build transparent practices into AI system development processes. \n\n• Review the documented system purpose from a socio -technical perspective and in \n\nconsideration of societal values. \n\n• Determine possible misalignment between societal values and stated organizational \n\nprinciples and code of ethics. \n\n• Flag latent incentives that may contribute to negative impacts. \n\n• Evaluate AI system purpose in consideration of potential risks, societal values, and \n\nstated organizational principles. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• How does the AI system help the entity meet its goals and objectives? \n\n• How do the technical specifications and requirements align with the AI system’s goals \n\nand objectives? \n\n• To what extent is the output appropriate for the operational context? \n\nAI Transparency Resources \n\n• Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI –\n\n2019, [LINK](https://altai.insight -centre.org/), \n\n• Including Insights from the Comptroller General’s Forum on the Oversight of Artificial \n\nIntelligence An Accountability Framework for Federal Agencies and Other Entities, \n\n2021, \n\nReferences \n\nM.S. Ackerman (2000). The Intellectual Challenge of CSCW: The Gap Between Social \n\nRequirements and Technical Feasibility. Human –Computer Interaction, 15, 179 - 203. 65 of 142 \n\nMcKane Andrus, Sarah Dean, Thomas Gilbert, Nathan Lambert, Tom Zick (2021). AI \n\nDevelopment for the Public Interest: From Abstraction Traps to Sociotechnical Risks. \n\nAbeba Birhane, Pratyusha Kalluri, Dallas Card, et al. 2022. The Values Encoded in Machine \n\nLearning Research. arXiv:2106.15590. \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nIason Gabriel, Artificial Intelligence, Values, and Alignment. Minds & Machines 30, 411 –437 \n\n(2020). \n\nPEAT “Business Case for Equitable AI”. \n\n# MAP 1.4 \n\nThe business value or context of business use has been clearly defined or – in the case of \n\nassessing existing AI systems – re -evaluated. \n\nAbout \n\nSocio -technical AI risks emerge from the interplay between technical development \n\ndecisions and how a system is used, who operates it, and the social context into which it is \n\ndeployed. Addressing these risks is complex and requires a commitment to understanding \n\nhow contextual factors may interact with AI lifecycle actions. One such contextual factor is \n\nhow organizational mission and identified system purpose create incentives within AI \n\nsystem design, development, and deployment tasks that may result in positive and negative \n\nimpacts. By establishing comprehensive and explicit enumeration of AI systems’ context of \n\nof business use and expectations, organizations can identify and manage these types of \n\nrisks. \n\nSuggested Actions \n\n• Document business value or context of business use \n\n• Reconcile documented concerns about the system’s purpose within the business context \n\nof use compared to the organization’s stated values, mission statements, social \n\nresponsibility commitments, and AI principles. \n\n• Reconsider the design, implementation strategy, or deployment of AI systems with \n\npotential impacts that do not reflect institutional values. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What goals and objectives does the entity expect to achieve by designing, developing, \n\nand/or deploying the AI system? \n\n• To what extent are the system outputs consistent with the entity’s values and principles \n\nto foster public trust and equity? \n\n• To what extent are the metrics consistent with system goals, objectives, and constraints, \n\nincluding ethical and compliance considerations? 66 of 142 \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\nReferences \n\nAlgorithm Watch. AI Ethics Guidelines Global Inventory. \n\nEthical OS toolkit. \n\nEmanuel Moss and Jacob Metcalf. 2020. Ethics Owners: A New Model of Organizational \n\nResponsibility in Data -Driven Technology Companies. Data & Society Research Institute. \n\nFuture of Life Institute. Asilomar AI Principles. \n\nLeonard Haas, Sebastian Gießler, and Veronika Thiel. 2020. In the realm of paper tigers –\n\nexploring the failings of AI ethics guidelines. (April 28, 2020). \n\n# MAP 1.5 \n\nOrganizational risk tolerances are determined and documented. \n\nAbout \n\nRisk tolerance reflects the level and type of risk the organization is willing to accept while \n\nconducting its mission and carrying out its strategy. \n\nOrganizations can follow existing regulations and guidelines for risk criteria, tolerance and \n\nresponse established by organizational, domain, discipline, sector, or professional \n\nrequirements. Some sectors or industries may have established definitions of harm or may \n\nhave established documentation, reporting, and disclosure requirements. \n\nWithin sectors, risk management may depend on existing guidelines for specific \n\napplications and use case settings. Where established guidelines do not exist, organizations \n\nwill want to define reasonable risk tolerance in consideration of different sources of risk \n\n(e.g., financial, operational, safety and wellbeing, business, reputational, and model risks) \n\nand different levels of risk (e.g., from negligible to critical). \n\nRisk tolerances inform and support decisions about whether to continue with development \n\nor deployment - termed “go/no -go”. Go/no -go decisions related to AI system risks can take \n\nstakeholder feedback into account, but remain independent from stakeholders’ vested \n\nfinancial or reputational interests. \n\nIf mapping risk is prohibitively difficult, a \"no -go\" decision may be considered for the \n\nspecific system. \n\nSuggested Actions \n\n• Utilize existing regulations and guidelines for risk criteria, tolerance and response \n\nestablished by organizational, domain, discipline, sector, or professional requirements. 67 of 142 \n\n• Establish risk tolerance levels for AI systems and allocate the appropriate oversight \n\nresources to each level. \n\n• Establish risk criteria in consideration of different sources of risk, (e.g., financial, \n\noperational, safety and wellbeing, business, reputational, and model risks) and different \n\nlevels of risk (e.g., from negligible to critical). \n\n• Identify maximum allowable risk tolerance above which the system will not be \n\ndeployed, or will need to be prematurely decommissioned, within the contextual or \n\napplication setting. \n\n• Articulate and analyze tradeoffs across trustworthiness characteristics as relevant to \n\nproposed context of use. When tradeoffs arise, document them and plan for traceable \n\nactions (e.g.: impact mitigation, removal of system from development or use) to inform \n\nmanagement decisions. \n\n• Review uses of AI systems for “off -label” purposes, especially in settings that \n\norganizations have deemed as high -risk. Document decisions, risk -related trade -offs, \n\nand system limitations. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Which existing regulations and guidelines apply, and the entity has followed, in the \n\ndevelopment of system risk tolerances? \n\n• What criteria and assumptions has the entity utilized when developing system risk \n\ntolerances? \n\n• How has the entity identified maximum allowable risk tolerance? \n\n• What conditions and purposes are considered “off -label” for system use? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\nReferences \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nThe Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, \n\n2019). \n\nBrenda Boultwood, How to Develop an Enterprise Risk -Rating Approach (Aug. 26, 2021). \n\nGlobal Association of Risk Professionals (garp.org). Accessed Jan. 4, 2023. \n\nVirginia Eubanks, 1972 -, Automating Inequality: How High -tech Tools Profile, Police, and \n\nPunish the Poor. New York, NY, St. Martin's Press, 2018. 68 of 142 \n\nGAO -17 -63: Enterprise Risk Management: Selected Agencies’ Experiences Illustrate Good \n\nPractices in Managing Risk. \n\nNIST Risk Management Framework. \n\n# MAP 1.6 \n\nSystem requirements (e.g., “the system shall respect the privacy of its users”) are elicited \n\nfrom and understood by relevant AI actors. Design decisions take socio -technical \n\nimplications into account to address AI risks. \n\nAbout \n\nAI system development requirements may outpace documentation processes for traditional \n\nsoftware. When written requirements are unavailable or incomplete, AI actors may \n\ninadvertently overlook business and stakeholder needs, over -rely on implicit human biases \n\nsuch as confirmation bias and groupthink, and maintain exclusive focus on computational \n\nrequirements. \n\nEliciting system requirements, designing for end users, and considering societal impacts \n\nearly in the design phase is a priority that can enhance AI systems’ trustworthiness. \n\nSuggested Actions \n\n• Proactively incorporate trustworthy characteristics into system requirements. \n\n• Establish mechanisms for regular communication and feedback between relevant AI \n\nactors and internal or external stakeholders related to system design or deployment \n\ndecisions. \n\n• Develop and standardize practices to assess potential impacts at all stages of the AI \n\nlifecycle, and in collaboration with interdisciplinary experts, actors external to the team \n\nthat developed or deployed the AI system, and potentially impacted communities . \n\n• Include potentially impacted groups, communities and external entities (e.g. civil society \n\norganizations, research institutes, local community groups, and trade associations) in \n\nthe formulation of priorities, definitions and outcomes during impact assessment \n\nactivities. \n\n• Conduct qualitative interviews with end user(s) to regularly evaluate expectations and \n\ndesign plans related to Human -AI configurations and tasks. \n\n• Analyze dependencies between contextual factors and system requirements. List \n\npotential impacts that may arise from not fully considering the importance of \n\ntrustworthiness characteristics in any decision making. \n\n• Follow responsible design techniques in tasks such as software engineering, product \n\nmanagement, and participatory engagement. Some examples for eliciting and \n\ndocumenting stakeholder requirements include product requirement documents \n\n(PRDs), user stories, user interaction/user experience (UI/UX) research, systems \n\nengineering, ethnography and related field methods. 69 of 142 \n\n• Conduct user research to understand individuals, groups and communities that will be \n\nimpacted by the AI, their values & context, and the role of systemic and historical biases. \n\nIntegrate learnings into decisions about data selection and representation. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\n• To what extent is this information sufficient and appropriate to promote transparency? \n\nPromote transparency by enabling external stakeholders to access information on the \n\ndesign, operation, and limitations of the AI system. \n\n• To what extent has relevant information been disclosed regarding the use of AI systems, \n\nsuch as (a) what the system is for, (b) what it is not for, (c) how it was designed, and (d) \n\nwhat its limitations are? (Documentation and external communication can offer a way \n\nfor entities to provide transparency.) \n\n• How will the relevant AI actor(s) address changes in accuracy and precision due to \n\neither an adversary’s attempts to disrupt the AI system or unrelated changes in the \n\noperational/business environment, which may impact the accuracy of the AI system? \n\n• What metrics has the entity developed to measure performance of the AI system? \n\n• What justifications, if any, has the entity provided for the assumptions, boundaries, and \n\nlimitations of the AI system? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Stakeholders in Explainable AI, Sep. 2018. \n\n• High -Level Expert Group on Artificial Intelligence set up by the European Commission, \n\nEthics Guidelines for Trustworthy AI. \n\nReferences \n\nNational Academies of Sciences, Engineering, and Medicine 2022. Fostering Responsible \n\nComputing Research: Foundations and Practices. Washington, DC: The National Academies \n\nPress. \n\nAbeba Birhane, William S. Isaac, Vinodkumar Prabhakaran, Mark Diaz, Madeleine Clare \n\nElish, Iason Gabriel and Shakir Mohamed. “Power to the People? Opportunities and \n\nChallenges for Participatory AI.” Equity and Access in Algorithms, Mechanisms, and \n\nOptimization (2022). \n\nAmit K. Chopra, Fabiano Dalpiaz, F. Başak Aydemir, et al. 2014. Protos: Foundations for \n\nengineering innovative sociotechnical systems. In 2014 IEEE 22nd International \n\nRequirements Engineering Conference (RE) (2014), 53 -62. 70 of 142 \n\nAndrew D. Selbst, danah boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in \n\nSociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and \n\nTransparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 59 –68. \n\nGordon Baxter and Ian Sommerville. 2011. Socio -technical systems: From design methods \n\nto systems engineering. Interacting with Computers, 23, 1 (Jan. 2011), 4 –17. \n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial \n\nintelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004 -3702. \n\nYilin Huang, Giacomo Poderi, Sanja Šćepanović, et al. 2019. Embedding Internet -of -Things in \n\nLarge -Scale Socio -technical Systems: A Community -Oriented Design in Future Smart Grids. \n\nIn The Internet of Things for Smart Urban Ecosystems (2019), 125 -150. Springer, Cham. \n\nVictor Udoewa, (2022). An introduction to radical participatory design: decolonising \n\nparticipatory design processes. Design Science. 8. 10.1017/dsj.2022.24. \n\n# MAP 2.1 \n\nThe specific task, and methods used to implement the task, that the AI system will support \n\nis defined (e.g., classifiers, generative models, recommenders). \n\nAbout \n\nAI actors define the technical learning or decision -making task(s) an AI system is designed \n\nto accomplish, or the benefits that the system will provide. The clearer and narrower the \n\ntask definition, the easier it is to map its benefits and risks, leading to more fulsome risk \n\nmanagement. \n\nSuggested Actions \n\n• Define and document AI system’s existing and potential learning task(s) along with \n\nknown assumptions and limitations. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent has the entity clearly defined technical specifications and requirements \n\nfor the AI system? \n\n• To what extent has the entity documented the AI system’s development, testing \n\nmethodology, metrics, and performance outcomes? \n\n• How do the technical specifications and requirements align with the AI system’s goals \n\nand objectives? \n\n• Did your organization implement accountability -based practices in data management \n\nand protection (e.g. the PDPA and OECD Privacy Principles)? \n\n• How are outputs marked to clearly show that they came from an AI? \n\nAI Transparency Resources \n\n• Datasheets for Datasets. 71 of 142 \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\n• ATARC Model Transparency Assessment (WD) – 2020. \n\n• Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020. \n\nReferences \n\nLeong, Brenda (2020). The Spectrum of Artificial Intelligence - An Infographic Tool. Future \n\nof Privacy Forum. \n\nBrownlee, Jason (2020). A Tour of Machine Learning Algorithms. Machine Learning \n\nMastery. \n\n# MAP 2.2 \n\nInformation about the AI system’s knowledge limits and how system output may be utilized \n\nand overseen by humans is documented. Documentation provides sufficient information to \n\nassist relevant AI actors when making informed decisions and taking subsequent actions. \n\nAbout \n\nAn AI lifecycle consists of many interdependent activities involving a diverse set of actors \n\nthat often do not have full visibility or control over other parts of the lifecycle and its \n\nassociated contexts or risks. The interdependencies between these activities, and among the \n\nrelevant AI actors and organizations, can make it difficult to reliably anticipate potential \n\nimpacts of AI systems. For example, early decisions in identifying the purpose and objective \n\nof an AI system can alter its behavior and capabilities, and the dynamics of deployment \n\nsetting (such as end users or impacted individuals) can shape the positive or negative \n\nimpacts of AI system decisions. As a result, the best intentions within one dimension of the \n\nAI lifecycle can be undermined via interactions with decisions and conditions in other, later \n\nactivities. This complexity and varying levels of visibility can introduce uncertainty. And, \n\nonce deployed and in use, AI systems may sometimes perform poorly, manifest \n\nunanticipated negative impacts, or violate legal or ethical norms. These risks and incidents \n\ncan result from a variety of factors. For example, downstream decisions can be influenced \n\nby end user over -trust or under -trust, and other complexities related to AI -supported \n\ndecision -making. \n\nAnticipating, articulating, assessing and documenting AI systems’ knowledge limits and how \n\nsystem output may be utilized and overseen by humans can help mitigate the uncertainty \n\nassociated with the realities of AI system deployments. Rigorous design processes include \n\ndefining system knowledge limits, which are confirmed and refined based on TEVV \n\nprocesses. \n\nSuggested Actions \n\n• Document settings, environments and conditions that are outside the AI system’s \n\nintended use. 72 of 142 \n\n• Design for end user workflows and toolsets, concept of operations, and explainability \n\nand interpretability criteria in conjunction with end user(s) and associated qualitative \n\nfeedback. \n\n• Plan and test human -AI configurations under close to real -world conditions and \n\ndocument results. \n\n• Follow stakeholder feedback processes to determine whether a system achieved its \n\ndocumented purpose within a given use context, and whether end users can correctly \n\ncomprehend system outputs or results. \n\n• Document dependencies on upstream data and other AI systems, including if the \n\nspecified system is an upstream dependency for another AI system or other data. \n\n• Document connections the AI system or data will have to external networks (including \n\nthe internet), financial markets, and critical infrastructure that have potential for \n\nnegative externalities. Identify and document negative impacts as part of considering \n\nthe broader risk thresholds and subsequent go/no -go deployment as well as post -\n\ndeployment decommissioning decisions. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Does the AI system provide sufficient information to assist the personnel to make an \n\ninformed decision and take actions accordingly? \n\n• What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\n• Based on the assessment, did your organization implement the appropriate level of \n\nhuman involvement in AI -augmented decision -making? \n\nAI Transparency Resources \n\n• Datasheets for Datasets. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\n• ATARC Model Transparency Assessment (WD) – 2020. \n\n• Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020. \n\nReferences \n\nContext of use \n\nInternational Standards Organization (ISO). 2019. ISO 9241 -210:2019 Ergonomics of \n\nhuman -system interaction — Part 210: Human -centred design for interactive systems. \n\nNational Institute of Standards and Technology (NIST), Mary Theofanos, Yee -Yin Choong, et \n\nal. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: \n\nEnsuring Successful Systems for First Responders. 73 of 142 \n\nHuman -AI interaction \n\nCommittee on Human -System Integration Research Topics for the 711th Human \n\nPerformance Wing of the Air Force Research Laboratory and the National Academies of \n\nSciences, Engineering, and Medicine. 2022. Human -AI Teaming: State -of -the -Art and \n\nResearch Needs. Washington, D.C. National Academies Press. \n\nHuman Readiness Level Scale in the System Development Process, American National \n\nStandards Institute and Human Factors and Ergonomics Society, ANSI/HFES 400 -2021 \n\nMicrosoft Responsible AI Standard, v2. \n\nSaar Alon -Barkat, Madalina Busuioc, Human –AI Interactions in Public Sector Decision \n\nMaking: “Automation Bias” and “Selective Adherence” to Algorithmic Advice, Journal of \n\nPublic Administration Research and Theory, 2022;, muac007. \n\nZana Buçinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: \n\nCognitive Forcing Functions Can Reduce Overreliance on AI in AI -assisted Decision -making. \n\nProc. ACM Hum. -Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. \n\nMary L. Cummings. 2006 Automation and accountability in decision support system \n\ninterface design.The Journal of Technology Studies 32(1): 23 –31. \n\nEngstrom, D. F., Ho, D. E., Sharkey, C. M., & Cuéllar, M. F. (2020). Government by algorithm: \n\nArtificial intelligence in federal administrative agencies. NYU School of Law, Public Law \n\nResearch Paper, (20 -54). \n\nSusanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in \n\ndeployment of clinical decision -aids. npj Digital Medicine 4, Article 31 (2021). \n\nBen Green. 2021. The Flaws of Policies Requiring Human Oversight of Government \n\nAlgorithms. Computer Law & Security Review 45 (26 Apr. 2021). \n\nBen Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to \n\nA.I. Harm. (June 15, 2021). \n\nGrgić -Hlača, N., Engel, C., & Gummadi, K. P. (2019). Human decision making with machine \n\nassistance: An experiment on bailing and jailing. Proceedings of the ACM on Human -\n\nComputer Interaction, 3(CSCW), 1 -25. \n\nForough Poursabzi -Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating \n\nand Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on \n\nHuman Factors in Computing Systems (CHI '21). Association for Computing Machinery, New \n\nYork, NY, USA, Article 237, 1 –52. \n\nC. J. Smith (2019). Designing trustworthy AI: A human -machine teaming framework to \n\nguide development. arXiv preprint arXiv:1910.03515. 74 of 142 \n\nT. Warden, P. Carayon, EM et al. The National Academies Board on Human System \n\nIntegration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine \n\nTeaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. \n\n2019;63(1):631 -635. doi:10.1177/1071181319631100. \n\n# MAP 2.3 \n\nScientific integrity and TEVV considerations are identified and documented, including those \n\nrelated to experimental design, data collection and selection (e.g., availability, \n\nrepresentativeness, suitability), system trustworthiness, and construct validation. \n\nAbout \n\nStandard testing and evaluation protocols provide a basis to confirm assurance in a system \n\nthat it is operating as designed and claimed. AI systems’ complexities create challenges for \n\ntraditional testing and evaluation methodologies, which tend to be designed for static or \n\nisolated system performance. Opportunities for risk continue well beyond design and \n\ndeployment, into system operation and application of system -enabled decisions. Testing \n\nand evaluation methodologies and metrics therefore address a continuum of activities. \n\nTEVV is enhanced when key metrics for performance, safety, and reliability are interpreted \n\nin a socio -technical context and not confined to the boundaries of the AI system pipeline. \n\nOther challenges for managing AI risks relate to dependence on large scale datasets, which \n\ncan impact data quality and validity concerns. The difficulty of finding the “right” data may \n\nlead AI actors to select datasets based more on accessibility and availability than on \n\nsuitability for operationalizing the phenomenon that the AI system intends to support or \n\ninform. Such decisions could contribute to an environment where the data used in \n\nprocesses is not fully representative of the populations or phenomena that are being \n\nmodeled, introducing downstream risks. Practices such as dataset reuse may also lead to \n\ndisconnect from the social contexts and time periods of their creation. This contributes to \n\nissues of validity of the underlying dataset for providing proxies, measures, or predictors \n\nwithin the model. \n\nSuggested Actions \n\n• Identify and document experiment design and statistical techniques that are valid for \n\ntesting complex socio -technical systems like AI, which involve human factors, emergent \n\nproperties, and dynamic context(s) of use. \n\n• Develop and apply TEVV protocols for models, system and its subcomponents, \n\ndeployment, and operation. \n\n• Demonstrate and document that AI system performance and validation metrics are \n\ninterpretable and unambiguous for downstream decision making tasks, and take socio -\n\ntechnical factors such as context of use into consideration. \n\n• Identify and document assumptions, techniques, and metrics used for testing and \n\nevaluation throughout the AI lifecycle including experimental design techniques for data \n\ncollection, selection, and management practices in accordance with data governance \n\npolicies established in GOVERN. 75 of 142 \n\n• Identify testing modules that can be incorporated throughout the AI lifecycle, and verify \n\nthat processes enable corroboration by independent evaluators. \n\n• Establish mechanisms for regular communication and feedback among relevant AI \n\nactors and internal or external stakeholders related to the validity of design and \n\ndeployment assumptions. \n\n• Establish mechanisms for regular communication and feedback between relevant AI \n\nactors and internal or external stakeholders related to the development of TEVV \n\napproaches throughout the lifecycle to detect and assess potentially harmful impacts \n\n• Document assumptions made and techniques used in data selection, curation, \n\npreparation and analysis, including: \n\n• identification of constructs and proxy targets, \n\n• development of indices – especially those operationalizing concepts that are \n\ninherently unobservable (e.g. “hireability,” “criminality.” “lendability”). \n\n• Map adherence to policies that address data and construct validity, bias, privacy and \n\nsecurity for AI systems and verify documentation, oversight, and processes. \n\n• Identify and document transparent methods (e.g. causal discovery methods) for \n\ninferring causal relationships between constructs being modeled and dataset attributes \n\nor proxies. \n\n• Identify and document processes to understand and trace test and training data lineage \n\nand its metadata resources for mapping risks. \n\n• Document known limitations, risk mitigation efforts associated with, and methods used \n\nfor, training data collection, selection, labeling, cleaning, and analysis (e.g. treatment of \n\nmissing, spurious, or outlier data; biased estimators). \n\n• Establish and document practices to check for capabilities that are in excess of those \n\nthat are planned for, such as emergent properties, and to revisit prior risk management \n\nsteps in light of any new capabilities. \n\n• Establish processes to test and verify that design assumptions about the set of \n\ndeployment contexts continue to be accurate and sufficiently complete. \n\n• Work with domain experts and other external AI actors to: \n\n• Gain and maintain contextual awareness and knowledge about how human \n\nbehavior, organizational factors and dynamics, and society influence, and are \n\nrepresented in, datasets, processes, models, and system output. \n\n• Identify participatory approaches for responsible Human -AI configurations and \n\noversight tasks, taking into account sources of cognitive bias. \n\n• Identify techniques to manage and mitigate sources of bias (systemic, \n\ncomputational, human - cognitive) in computational models and systems, and the \n\nassumptions and decisions in their development.. \n\n• Investigate and document potential negative impacts due related to the full product \n\nlifecycle and associated processes that may conflict with organizational values and \n\nprinciples. 76 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Are there any known errors, sources of noise, or redundancies in the data? \n\n• Over what time -frame was the data collected? Does the collection time -frame match the \n\ncreation time -frame \n\n• What is the variable selection and evaluation process? \n\n• How was the data collected? Who was involved in the data collection process? If the \n\ndataset relates to people (e.g., their attributes) or was generated by people, were they \n\ninformed about the data collection? (e.g., datasets that collect writing, photos, \n\ninteractions, transactions, etc.) \n\n• As time passes and conditions change, is the training data still representative of the \n\noperational environment? \n\n• Why was the dataset created? (e.g., were there specific tasks in mind, or a specific gap \n\nthat needed to be filled?) \n\n• How does the entity ensure that the data collected are adequate, relevant, and not \n\nexcessive in relation to the intended purpose? \n\nAI Transparency Resources \n\n• Datasheets for Datasets. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• ATARC Model Transparency Assessment (WD) – 2020. \n\n• Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020. \n\nReferences \n\nChallenges with dataset selection \n\nAlexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social Data: \n\nBiases, Methodological Pitfalls, and Ethical Boundaries. Front. Big Data 2, 13 (11 July 2019). \n\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its \n\n(dis)contents: A survey of dataset development and use in machine learning research. \n\narXiv:2012.05345. \n\nCatherine D'Ignazio and Lauren F. Klein. 2020. Data Feminism. The MIT Press, Cambridge, \n\nMA. \n\nMiceli, M., & Posada, J. (2022). The Data -Production Dispositif. ArXiv, abs/2205.11963. \n\nBarbara Plank. 2016. What to do about non -standard (or non -canonical) language in NLP. \n\narXiv:1608.07836. 77 of 142 \n\nDataset and test, evaluation, validation and verification (TEVV) processes in AI system \n\ndevelopment \n\nNational Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et \n\nal. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing \n\nBias in Artificial Intelligence. \n\nInioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, et al. 2021. AI and the \n\nEverything in the Whole Wide World Benchmark. arXiv:2111.15366. \n\nStatistical balance \n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting \n\nracial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 \n\nOct. 2019), 447 -453. \n\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its \n\n(dis)contents: A survey of dataset development and use in machine learning research. \n\narXiv:2012.05345. \n\nSolon Barocas, Anhong Guo, Ece Kamar, et al. 2021. Designing Disaggregated Evaluations of \n\nAI Systems: Choices, Considerations, and Tradeoffs. Proceedings of the 2021 AAAI/ACM \n\nConference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, \n\nUSA, 368 –378. \n\nMeasurement and evaluation \n\nAbigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of \n\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). \n\nAssociation for Computing Machinery, New York, NY, USA, 375 –385. \n\nBen Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in \n\nMachine Learning Practice. arXiv:2205.05256. \n\nLaura Freeman, \"Test and evaluation for artificial intelligence.\" Insight 23.1 (2020): 27 -30. \n\nExisting frameworks \n\nNational Institute of Standards and Technology. (2018). Framework for improving critical \n\ninfrastructure cybersecurity. \n\nKaitlin R. Boeckl and Naomi B. Lefkovitz. \"NIST Privacy Framework: A Tool for Improving \n\nPrivacy Through Enterprise Risk Management, Version 1.0.\" National Institute of Standards \n\nand Technology (NIST), January 16, 2020. \n\n# MAP 3.1 \n\nPotential benefits of intended AI system functionality and performance are examined and \n\ndocumented. 78 of 142 \n\nAbout \n\nAI systems have enormous potential to improve quality of life, enhance economic prosperity \n\nand security costs. Organizations are encouraged to define and document system purpose \n\nand utility, and its potential positive impacts and benefits beyond current known \n\nperformance benchmarks. \n\nIt is encouraged that risk management and assessment of benefits and impacts include \n\nprocesses for regular and meaningful communication with potentially affected groups and \n\ncommunities. These stakeholders can provide valuable input related to systems’ benefits \n\nand possible limitations. Organizations may differ in the types and number of stakeholders \n\nwith which they engage. \n\nOther approaches such as human -centered design (HCD) and value -sensitive design (VSD) \n\ncan help AI teams to engage broadly with individuals and communities. This type of \n\nengagement can enable AI teams to learn about how a given technology may cause positive \n\nor negative impacts, that were not originally considered or intended. \n\nSuggested Actions \n\n• Utilize participatory approaches and engage with system end users to understand and \n\ndocument AI systems’ potential benefits, efficacy and interpretability of AI task output. \n\n• Maintain awareness and documentation of the individuals, groups, or communities who \n\nmake up the system’s internal and external stakeholders. \n\n• Verify that appropriate skills and practices are available in -house for carrying out \n\nparticipatory activities such as eliciting, capturing, and synthesizing user, operator and \n\nexternal feedback, and translating it for AI design and development functions. \n\n• Establish mechanisms for regular communication and feedback between relevant AI \n\nactors and internal or external stakeholders related to system design or deployment \n\ndecisions. \n\n• Consider performance to human baseline metrics or other standard benchmarks. \n\n• Incorporate feedback from end users, and potentially impacted individuals and \n\ncommunities about perceived system benefits . \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Have the benefits of the AI system been communicated to end users? \n\n• Have the appropriate training material and disclaimers about how to adequately use the \n\nAI system been provided to end users? \n\n• Has your organization implemented a risk management system to address risks \n\ninvolved in deploying the identified AI system (e.g. personnel risk or changes to \n\ncommercial objectives)? \n\nAI Transparency Resources \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. 79 of 142 \n\n• Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI –\n\n2019. [LINK](https://altai.insight -centre.org/), \n\nReferences \n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial \n\nintelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004 -3702. \n\nSamir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of \n\nthe Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for \n\nComputing Machinery, New York, NY, USA, 39 –48. \n\nVincent T. Covello. 2021. Stakeholder Engagement and Empowerment. In Communicating in \n\nRisk, Crisis, and High Stress Situations (Vincent T. Covello, ed.), 87 -109. \n\nYilin Huang, Giacomo Poderi, Sanja Šćepanović, et al. 2019. Embedding Internet -of -Things in \n\nLarge -Scale Socio -technical Systems: A Community -Oriented Design in Future Smart Grids. \n\nIn The Internet of Things for Smart Urban Ecosystems (2019), 125 -150. Springer, Cham. \n\nEloise Taysom and Nathan Crilly. 2017. Resilience in Sociotechnical Systems: The \n\nPerspectives of Multiple Stakeholders. She Ji: The Journal of Design, Economics, and \n\nInnovation, 3, 3 (2017), 165 -182, ISSN 2405 -8726. \n\n# MAP 3.2 \n\nPotential costs, including non -monetary costs, which result from expected or realized AI \n\nerrors or system functionality and trustworthiness - as connected to organizational risk \n\ntolerance - are examined and documented. \n\nAbout \n\nAnticipating negative impacts of AI systems is a difficult task. Negative impacts can be due \n\nto many factors, such as system non -functionality or use outside of its operational limits, \n\nand may range from minor annoyance to serious injury, financial losses, or regulatory \n\nenforcement actions. AI actors can work with a broad set of stakeholders to improve their \n\ncapacity for understanding systems’ potential impacts – and subsequently – systems’ risks. \n\nSuggested Actions \n\n• Perform context analysis to map potential negative impacts arising from not integrating \n\ntrustworthiness characteristics. When negative impacts are not direct or obvious, AI \n\nactors can engage with stakeholders external to the team that developed or deployed \n\nthe AI system, and potentially impacted communities, to examine and document: \n\n• Who could be harmed? \n\n• What could be harmed? \n\n• When could harm arise? \n\n• How could harm arise? 80 of 142 \n\n• Identify and implement procedures for regularly evaluating the qualitative and \n\nquantitative costs of internal and external AI system failures. Develop actions to \n\nprevent, detect, and/or correct potential risks and related impacts. Regularly evaluate \n\nfailure costs to inform go/no -go deployment decisions throughout the AI system \n\nlifecycle. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent does the system/entity consistently measure progress towards stated \n\ngoals and objectives? \n\n• To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\n• Have you documented and explained that machine errors may differ from human \n\nerrors? \n\nAI Transparency Resources \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI –\n\n2019. [LINK](https://altai.insight -centre.org/), \n\nReferences \n\nAbagayle Lee Blank. 2019. Computer vision machine learning and future -oriented ethics. \n\nHonors Project. Seattle Pacific University (SPU), Seattle, WA. \n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of \n\nImagination in AI Infused System Development and Deployment. arXiv:2011.13416. \n\nJeff Patton. 2014. User Story Mapping. O'Reilly, Sebastopol, CA. \n\nMargarita Boenig -Liptsin, Anissa Tanweer & Ari Edmundson (2022) Data Science Ethos \n\nLifecycle: Interplay of ethical thinking and data science practice, Journal of Statistics and \n\nData Science Education, DOI: 10.1080/26939169.2022.2089411 \n\nJ. Cohen, D. S. Katz, M. Barker, N. Chue Hong, R. Haines and C. Jay, \"The Four Pillars of \n\nResearch Software Engineering,\" in IEEE Software, vol. 38, no. 1, pp. 97 -105, Jan. -Feb. 2021, \n\ndoi: 10.1109/MS.2020.2973362. \n\nNational Academies of Sciences, Engineering, and Medicine 2022. Fostering Responsible \n\nComputing Research: Foundations and Practices. Washington, DC: The National Academies \n\nPress. \n\n# MAP 3.3 \n\nTargeted application scope is specified and documented based on the system’s capability, \n\nestablished context, and AI system categorization. 81 of 142 \n\nAbout \n\nSystems that function in a narrow scope tend to enable better mapping, measurement, and \n\nmanagement of risks in the learning or decision -making tasks and the system context. A \n\nnarrow application scope also helps ease TEVV functions and related resources within an \n\norganization. \n\nFor example, large language models or open -ended chatbot systems that interact with the \n\npublic on the internet have a large number of risks that may be difficult to map, measure, \n\nand manage due to the variability from both the decision -making task and the operational \n\ncontext. Instead, a task -specific chatbot utilizing templated responses that follow a defined \n\n“user journey” is a scope that can be more easily mapped, measured and managed. \n\nSuggested Actions \n\n• Consider narrowing contexts for system deployment, including factors related to: \n\n• How outcomes may directly or indirectly affect users, groups, communities and \n\nthe environment. \n\n• Length of time the system is deployed in between re -trainings. \n\n• Geographical regions in which the system operates. \n\n• Dynamics related to community standards or likelihood of system misuse or \n\nabuses (either purposeful or unanticipated). \n\n• How AI system features and capabilities can be utilized within other \n\napplications, or in place of other existing processes. \n\n• Engage AI actors from legal and procurement functions when specifying target \n\napplication scope. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent has the entity clearly defined technical specifications and requirements \n\nfor the AI system? \n\n• How do the technical specifications and requirements align with the AI system’s goals \n\nand objectives? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI –\n\n2019. [LINK](https://altai.insight -centre.org/), \n\nReferences \n\nMark J. Van der Laan and Sherri Rose (2018). Targeted Learning in Data Science. Cham: \n\nSpringer International Publishing, 2018. \n\nAlice Zheng. 2015. Evaluating Machine Learning Models (2015). O'Reilly. 82 of 142 \n\nBrenda Leong and Patrick Hall (2021). 5 things lawyers should know about artificial \n\nintelligence. ABA Journal. \n\nUK Centre for Data Ethics and Innovation, “The roadmap to an effective AI assurance \n\necosystem”. \n\n# MAP 3.4 \n\nProcesses for operator and practitioner proficiency with AI system performance and \n\ntrustworthiness – and relevant technical standards and certifications – are defined, \n\nassessed and documented. \n\nAbout \n\nHuman -AI configurations can span from fully autonomous to fully manual. AI systems can \n\nautonomously make decisions, defer decision -making to a human expert, or be used by a \n\nhuman decision -maker as an additional opinion. In some scenarios, professionals with \n\nexpertise in a specific domain work in conjunction with an AI system towards a specific end \n\ngoal —for example, a decision about another individual(s). Depending on the purpose of the \n\nsystem, the expert may interact with the AI system but is rarely part of the design or \n\ndevelopment of the system itself. These experts are not necessarily familiar with machine \n\nlearning, data science, computer science, or other fields traditionally associated with AI \n\ndesign or development and - depending on the application - will likely not require such \n\nfamiliarity. For example, for AI systems that are deployed in health care delivery the experts \n\nare the physicians and bring their expertise about medicine —not data science, data \n\nmodeling and engineering, or other computational factors. The challenge in these settings is \n\nnot educating the end user about AI system capabilities, but rather leveraging, and not \n\nreplacing, practitioner domain expertise. \n\nQuestions remain about how to configure humans and automation for managing AI risks. \n\nRisk management is enhanced when organizations that design, develop or deploy AI \n\nsystems for use by professional operators and practitioners: \n\n• are aware of these knowledge limitations and strive to identify risks in human -AI \n\ninteractions and configurations across all contexts, and the potential resulting impacts, \n\n• define and differentiate the various human roles and responsibilities when using or \n\ninteracting with AI systems, and \n\n• determine proficiency standards for AI system operation in proposed context of use, as \n\nenumerated in MAP -1 and established in GOVERN -3.2. \n\nSuggested Actions \n\n• Identify and declare AI system features and capabilities that may affect downstream AI \n\nactors’ decision -making in deployment and operational settings for example how \n\nsystem features and capabilities may activate known risks in various human -AI \n\nconfigurations, such as selective adherence. \n\n• Identify skills and proficiency requirements for operators, practitioners and other \n\ndomain experts that interact with AI systems,Develop AI system operational 83 of 142 \n\ndocumentation for AI actors in deployed and operational environments, including \n\ninformation about known risks, mitigation criteria, and trustworthy characteristics \n\nenumerated in Map -1. \n\n• Define and develop training materials for proposed end users, practitioners and \n\noperators about AI system use and known limitations. \n\n• Define and develop certification procedures for operating AI systems within defined \n\ncontexts of use, and information about what exceeds operational boundaries. \n\n• Include operators, practitioners and end users in AI system prototyping and testing \n\nactivities to help inform operational boundaries and acceptable performance. Conduct \n\ntesting activities under scenarios similar to deployment conditions. \n\n• Verify model output provided to AI system operators, practitioners and end users is \n\ninteractive, and specified to context and user requirements defined in MAP -1. \n\n• Verify AI system output is interpretable and unambiguous for downstream decision \n\nmaking tasks. \n\n• Design AI system explanation complexity to match the level of problem and context \n\ncomplexity. \n\n• Verify that design principles are in place for safe operation by AI actors in decision -\n\nmaking environments. \n\n• Develop approaches to track human -AI configurations, operator, and practitioner \n\noutcomes for integration into continual improvement. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What policies has the entity developed to ensure the use of the AI system is consistent \n\nwith its stated values and principles? \n\n• How will the accountable human(s) address changes in accuracy and precision due to \n\neither an adversary’s attempts to disrupt the AI or unrelated changes in \n\noperational/business environment, which may impact the accuracy of the AI? \n\n• How does the entity assess whether personnel have the necessary skills, training, \n\nresources, and domain knowledge to fulfill their assigned responsibilities? \n\n• Are the relevant staff dealing with AI systems properly trained to interpret AI model \n\noutput and decisions as well as to detect and manage bias in data? \n\n• What metrics has the entity developed to measure performance of various components? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• WEF Companion to the Model AI Governance Framework - 2020. \n\nReferences \n\nNational Academies of Sciences, Engineering, and Medicine. 2022. Human -AI Teaming: \n\nState -of -the -Art and Research Needs. Washington, DC: The National Academies Press. 84 of 142 \n\nHuman Readiness Level Scale in the System Development Process, American National \n\nStandards Institute and Human Factors and Ergonomics Society, ANSI/HFES 400 -2021. \n\nHuman -Machine Teaming Systems Engineering Guide. P McDermott, C Dominguez, N \n\nKasdaglis, M Ryan, I Trahan, A Nelson. MITRE Corporation, 2018. \n\nSaar Alon -Barkat, Madalina Busuioc, Human –AI Interactions in Public Sector Decision \n\nMaking: “Automation Bias” and “Selective Adherence” to Algorithmic Advice, Journal of \n\nPublic Administration Research and Theory, 2022;, muac007. \n\nBreana M. Carter -Browne, Susannah B. F. Paletz, Susan G. Campbell , Melissa J. Carraway, \n\nSarah H. Vahlkamp, Jana Schwartz , Polly O’Rourke, “There is No “AI” in Teams: A \n\nMultidisciplinary Framework for AIs to Work in Human Teams; Applied Research \n\nLaboratory for Intelligence and Security (ARLIS) Report, June 2021. \n\nR Crootof, ME Kaminski, and WN Price II. Humans in the Loop (March 25, 2022). Vanderbilt \n\nLaw Review, Forthcoming 2023, U of Colorado Law Legal Studies Research Paper No. 22 -10, \n\nU of Michigan Public Law Research Paper No. 22 -011. \n\nS Mo Jones -Jang, Yong Jin Park, How do people react to AI failure? Automation bias, \n\nalgorithmic aversion, and perceived controllability, Journal of Computer -Mediated \n\nCommunication, Volume 28, Issue 1, January 2023, zmac029. \n\nA Knack, R Carter and A Babuta, \"Human -Machine Teaming in Intelligence Analysis: \n\nRequirements for developing trust in machine learning systems,\" CETaS Research Reports \n\n(December 2022). \n\nSD Ramchurn, S Stein , NR Jennings. Trustworthy human -AI partnerships. iScience. \n\n2021;24(8):102891. Published 2021 Jul 24. doi:10.1016/j.isci.2021.102891. \n\nM. Veale, M. Van Kleek, and R. Binns, “Fairness and Accountability Design Needs for \n\nAlgorithmic Support in High -Stakes Public Sector Decision -Making,” in Proceedings of the \n\n2018 CHI Conference on Human Factors in Computing Systems - CHI ’18. Montreal QC, \n\nCanada: ACM Press, 2018, pp. 1 –14. \n\n# MAP 3.5 \n\nProcesses for human oversight are defined, assessed, and documented in accordance with \n\norganizational policies from GOVERN function. \n\nAbout \n\nAs AI systems have evolved in accuracy and precision, computational systems have moved \n\nfrom being used purely for decision support —or for explicit use by and under the \n\ncontrol of a human operator —to automated decision making with limited input from \n\nhumans. Computational decision support systems augment another, typically human, \n\nsystem in making decisions.These types of configurations increase the likelihood of outputs \n\nbeing produced with little human involvement. 85 of 142 \n\nDefining and differentiating various human roles and responsibilities for AI systems’ \n\ngovernance, and differentiating AI system overseers and those using or interacting with AI \n\nsystems can enhance AI risk management activities. \n\nIn critical systems, high -stakes settings, and systems deemed high -risk it is of vital \n\nimportance to evaluate risks and effectiveness of oversight procedures before an AI system \n\nis deployed. \n\nUltimately, AI system oversight is a shared responsibility, and attempts to properly \n\nauthorize or govern oversight practices will not be effective without organizational buy -in \n\nand accountability mechanisms, for example those suggested in the GOVERN function. \n\nSuggested Actions \n\n• Identify and document AI systems’ features and capabilities that require human \n\noversight, in relation to operational and societal contexts, trustworthy characteristics, \n\nand risks identified in MAP -1. \n\n• Establish practices for AI systems’ oversight in accordance with policies developed in \n\nGOVERN -1. \n\n• Define and develop training materials for relevant AI Actors about AI system \n\nperformance, context of use, known limitations and negative impacts, and suggested \n\nwarning labels. \n\n• Include relevant AI Actors in AI system prototyping and testing activities. Conduct \n\ntesting activities under scenarios similar to deployment conditions. \n\n• Evaluate AI system oversight practices for validity and reliability. When oversight \n\npractices undergo extensive updates or adaptations, retest, evaluate results, and course \n\ncorrect as necessary. \n\n• Verify that model documents contain interpretable descriptions of system mechanisms, \n\nenabling oversight personnel to make informed, risk -based decisions about system \n\nrisks. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\n• How does the entity assess whether personnel have the necessary skills, training, \n\nresources, and domain knowledge to fulfill their assigned responsibilities? \n\n• Are the relevant staff dealing with AI systems properly trained to interpret AI model \n\noutput and decisions as well as to detect and manage bias in data? \n\n• To what extent has the entity documented the AI system’s development, testing \n\nmethodology, metrics, and performance outcomes? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. 86 of 142 \n\nReferences \n\nBen Green, “The Flaws of Policies Requiring Human Oversight of Government Algorithms,” \n\nSSRN Journal, 2021. \n\nLuciano Cavalcante Siebert, Maria Luce Lupetti, Evgeni Aizenberg, Niek Beckers, Arkady \n\nZgonnikov, Herman Veluwenkamp, David Abbink, Elisa Giaccardi, Geert -Jan Houben, \n\nCatholijn Jonker, Jeroen van den Hoven, Deborah Forster, & Reginald Lagendijk (2021). \n\nMeaningful human control: actionable properties for AI system development. AI and Ethics. \n\nMary Cummings, (2014). Automation and Accountability in Decision Support System \n\nInterface Design. The Journal of Technology Studies. 32. 10.21061/jots.v32i1.a.4. \n\nMadeleine Elish, M. (2016). Moral Crumple Zones: Cautionary Tales in Human -Robot \n\nInteraction (WeRobot 2016). SSRN Electronic Journal. 10.2139/ssrn.2757236. \n\nR Crootof, ME Kaminski, and WN Price II. Humans in the Loop (March 25, 2022). Vanderbilt \n\nLaw Review, Forthcoming 2023, U of Colorado Law Legal Studies Research Paper No. 22 -10, \n\nU of Michigan Public Law Research Paper No. 22 -011. \n\n[LINK](https://ssrn.com/abstract=4066781), \n\nBogdana Rakova, Jingying Yang, Henriette Cramer, & Rumman Chowdhury (2020). Where \n\nResponsible AI meets Reality. Proceedings of the ACM on Human -Computer Interaction, 5, 1 \n\n- 23. \n\n# MAP 4.1 \n\nApproaches for mapping AI technology and legal risks of its components – including the use \n\nof third -party data or software – are in place, followed, and documented, as are risks of \n\ninfringement of a third -party’s intellectual property or other rights. \n\nAbout \n\nTechnologies and personnel from third -parties are another potential sources of risk to \n\nconsider during AI risk management activities. Such risks may be difficult to map since risk \n\npriorities or tolerances may not be the same as the deployer organization. \n\nFor example, the use of pre -trained models, which tend to rely on large uncurated dataset \n\nor often have undisclosed origins, has raised concerns about privacy, bias, and \n\nunanticipated effects along with possible introduction of increased levels of statistical \n\nuncertainty, difficulty with reproducibility, and issues with scientific validity. \n\nSuggested Actions \n\n• Review audit reports, testing results, product roadmaps, warranties, terms of service, \n\nend user license agreements, contracts, and other documentation related to third -party \n\nentities to assist in value assessment and risk management activities. \n\n• Review third -party software release schedules and software change management plans \n\n(hotfixes, patches, updates, forward - and backward - compatibility guarantees) for \n\nirregularities that may contribute to AI system risks. 87 of 142 \n\n• Inventory third -party material (hardware, open -source software, foundation models, \n\nopen source data, proprietary software, proprietary data, etc.) required for system \n\nimplementation and maintenance. \n\n• Review redundancies related to third -party technology and personnel to assess \n\npotential risks due to lack of adequate support. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Did you establish a process for third parties (e.g. suppliers, end users, subjects, \n\ndistributors/vendors or workers) to report potential vulnerabilities, risks or biases in \n\nthe AI system? \n\n• If your organization obtained datasets from a third party, did your organization assess \n\nand manage the risks of using such datasets? \n\n• How will the results be independently verified? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\nReferences \n\nLanguage models \n\nEmily M. Bender, Timnit Gebru, Angelina McMillan -Major, and Shmargaret Shmitchell. 2021. \n\nOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜 . In Proceedings \n\nof the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21). \n\nAssociation for Computing Machinery, New York, NY, USA, 610 –623. \n\nJulia Kreutzer, Isaac Caswell, Lisa Wang, et al. 2022. Quality at a Glance: An Audit of Web -\n\nCrawled Multilingual Datasets. Transactions of the Association for Computational \n\nLinguistics 10 (2022), 50 –72. \n\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, et al. 2022. Taxonomy of Risks posed by \n\nLanguage Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency \n\n(FAccT '22). Association for Computing Machinery, New York, NY, USA, 214 –229. \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. 2021. On the Opportunities and Risks \n\nof Foundation Models. arXiv:2108.07258. \n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani \n\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, 88 of 142 \n\nOriol Vinyals, Percy Liang, Jeff Dean, William Fedus. “Emergent Abilities of Large Language \n\nModels.” ArXiv abs/2206.07682 (2022). \n\n# MAP 4.2 \n\nInternal risk controls for components of the AI system including third -party AI technologies \n\nare identified and documented. \n\nAbout \n\nIn the course of their work, AI actors often utilize open -source, or otherwise freely \n\navailable, third -party technologies – some of which may have privacy, bias, and security \n\nrisks. Organizations may consider internal risk controls for these technology sources and \n\nbuild up practices for evaluating third -party material prior to deployment. \n\nSuggested Actions \n\n• Track third -parties preventing or hampering risk -mapping as indications of increased \n\nrisk. \n\n• Supply resources such as model documentation templates and software safelists to \n\nassist in third -party technology inventory and approval activities. \n\n• Review third -party material (including data and models) for risks related to bias, data \n\nprivacy, and security vulnerabilities. \n\n• Apply traditional technology risk controls – such as procurement, security, and data \n\nprivacy controls – to all acquired third -party technologies. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Can the AI system be audited by independent third parties? \n\n• To what extent do these policies foster public trust and confidence in the use of the AI \n\nsystem? \n\n• Are mechanisms established to facilitate the AI system’s auditability (e.g. traceability of \n\nthe development process, the sourcing of training data and the logging of the AI \n\nsystem’s processes, outcomes, positive and negative impact)? \n\nAI Transparency Resources \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\n• WEF Model AI Governance Framework Assessment 2020. \n\n• Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI -\n\n2019. [LINK](https://altai.insight -centre.org/), \n\nReferences \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. Retrieved on July 7, 2022. \n\nProposed Interagency Guidance on Third -Party Relationships: Risk Management, 2021. 89 of 142 \n\nKang, D., Raghavan, D., Bailis, P.D., & Zaharia, M.A. (2020). Model Assertions for Monitoring \n\nand Improving ML Models. ArXiv, abs/2003.01668. \n\n# MAP 5.1 \n\nLikelihood and magnitude of each identified impact (both potentially beneficial and \n\nharmful) based on expected use, past uses of AI systems in similar contexts, public incident \n\nreports, feedback from those external to the team that developed or deployed the AI system, \n\nor other data are identified and documented. \n\nAbout \n\nAI actors can evaluate, document and triage the likelihood of AI system impacts identified in \n\nMap 5.1 Likelihood estimates may then be assessed and judged for go/no -go decisions \n\nabout deploying an AI system. If an organization decides to proceed with deploying the \n\nsystem, the likelihood and magnitude estimates can be used to assign TEVV resources \n\nappropriate for the risk level. \n\nSuggested Actions \n\n• Establish assessment scales for measuring AI systems’ impact. Scales may be qualitative, \n\nsuch as red -amber -green (RAG), or may entail simulations or econometric approaches. \n\nDocument and apply scales uniformly across the organization’s AI portfolio. \n\n• Apply TEVV regularly at key stages in the AI lifecycle, connected to system impacts and \n\nfrequency of system updates. \n\n• Identify and document likelihood and magnitude of system benefits and negative \n\nimpacts in relation to trustworthiness characteristics. \n\n• Establish processes for red teaming to identify and connect system limitations to AI \n\nlifecycle stage(s) and potential downstream impacts \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Which population(s) does the AI system impact? \n\n• What assessments has the entity conducted on trustworthiness characteristics for \n\nexample data security and privacy impacts associated with the AI system? \n\n• Can the AI system be tested by independent third parties? \n\nAI Transparency Resources \n\n• Datasheets for Datasets. \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\n• Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI -\n\n2019. [LINK](https://altai.insight -centre.org/), 90 of 142 \n\nReferences \n\nEmilio Gómez -González and Emilia Gómez. 2020. Artificial intelligence in medicine and \n\nhealthcare. Joint Research Centre (European Commission). \n\nArtificial Intelligence Incident Database. 2022. \n\nAnthony M. Barrett, Dan Hendrycks, Jessica Newman and Brandie Nonnecke. “Actionable \n\nGuidance for High -Consequence AI Risk Management: Towards Standards Addressing AI \n\nCatastrophic Risks\". ArXiv abs/2206.08966 (2022) \n\nGanguli, D., et al. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling \n\nBehaviors, and Lessons Learned. arXiv. https://arxiv.org/abs/2209.07858 \n\nUpol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, and Hal Daumé. 2024. Seamful XAI: \n\nOperationalizing Seamful Design in Explainable AI. Proc. ACM Hum. -Comput. Interact. 8, \n\nCSCW1, Article 119. https://doi.org/10.1145/3637396 \n\n# MAP 5.2 \n\nPractices and personnel for supporting regular engagement with relevant AI actors and \n\nintegrating feedback about positive, negative, and unanticipated impacts are in place and \n\ndocumented. \n\nAbout \n\nAI systems are socio -technical in nature and can have positive, neutral, or negative \n\nimplications that extend beyond their stated purpose. Negative impacts can be wide -\n\nranging and affect individuals, groups, communities, organizations, and society, as well as \n\nthe environment and national security. \n\nOrganizations can create a baseline for system monitoring to increase opportunities for \n\ndetecting emergent risks. After an AI system is deployed, engaging different stakeholder \n\ngroups – who may be aware of, or experience, benefits or negative impacts that are \n\nunknown to AI actors involved in the design, development and deployment activities –\n\nallows organizations to understand and monitor system benefits and potential negative \n\nimpacts more readily. \n\nSuggested Actions \n\n• Establish and document stakeholder engagement processes at the earliest stages of \n\nsystem formulation to identify potential impacts from the AI system on individuals, \n\ngroups, communities, organizations, and society. \n\n• Employ methods such as value sensitive design (VSD) to identify misalignments \n\nbetween organizational and societal values, and system implementation and impact. \n\n• Identify approaches to engage, capture, and incorporate input from system end users \n\nand other key stakeholders to assist with continuous monitoring for potential impacts \n\nand emergent risks. 91 of 142 \n\n• Incorporate quantitative, qualitative, and mixed methods in the assessment and \n\ndocumentation of potential impacts to individuals, groups, communities, organizations, \n\nand society. \n\n• Identify a team (internal or external) that is independent of AI design and development \n\nfunctions to assess AI system benefits, positive and negative impacts and their \n\nlikelihood and magnitude. \n\n• Evaluate and document stakeholder feedback to assess potential impacts for actionable \n\ninsights regarding trustworthiness characteristics and changes in design approaches \n\nand principles. \n\n• Develop TEVV procedures that incorporate socio -technical elements and methods and \n\nplan to normalize across organizational culture. Regularly review and refine TEVV \n\nprocesses. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• If the AI system relates to people, does it unfairly advantage or disadvantage a \n\nparticular social group? In what ways? How was this managed? \n\n• If the AI system relates to other ethically protected groups, have appropriate obligations \n\nbeen met? (e.g., medical data might include information collected from animals) \n\n• If the AI system relates to people, could this dataset expose people to harm or legal \n\naction? (e.g., financial social or otherwise) What was done to mitigate or reduce the \n\npotential for harm? \n\nAI Transparency Resources \n\n• Datasheets for Datasets. \n\n• GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\n• AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\n• Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\n• Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI -\n\n2019. [LINK](https://altai.insight -centre.org/), \n\nReferences \n\nSusanne Vernim, Harald Bauer, Erwin Rauch, et al. 2022. A value sensitive design approach \n\nfor designing AI -based worker assistance systems in manufacturing. Procedia Comput. Sci. \n\n200, C (2022), 505 –516. \n\nHarini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm \n\nthroughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from \n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of \n\nImagination in AI Infused System Development and Deployment. arXiv:2011.13416. \n\nKonstantinia Charitoudi and Andrew Blyth. A Socio -Technical Approach to Cyber Risk \n\nManagement and Impact Assessment. Journal of Information Security 4, 1 (2013), 33 -41. 92 of 142 \n\nRaji, I.D., Smart, A., White, R.N., Mitchell, M., Gebru, T., Hutchinson, B., Smith -Loud, J., Theron, \n\nD., & Barnes, P. (2020). Closing the AI accountability gap: defining an end -to -end framework \n\nfor internal algorithmic auditing. Proceedings of the 2020 Conference on Fairness, \n\nAccountability, and Transparency. \n\nEmanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, & Jacob Metcalf. \n\n2021. Assemlbing Accountability: Algorithmic Impact Assessment for the Public Interest. \n\nData & Society. Accessed 7/14/2022 at \n\nShari Trewin (2018). AI Fairness for People with Disabilities: Point of View. ArXiv, \n\nabs/1811.10670. \n\nAda Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. \n\nAccessed July 14, 2022. \n\nMicrosoft Responsible AI Impact Assessment Template. 2022. Accessed July 14, 2022. \n\nMicrosoft Responsible AI Impact Assessment Guide. 2022. Accessed July 14, 2022. \n\nMicrosoft Responsible AI Standard, v2. \n\nMicrosoft Research AI Fairness Checklist. \n\nPEAT AI & Disability Inclusion Toolkit – Risks of Bias and Discrimination in AI Hiring Tools. MEASURE 93 of 142 \n\n# Measure \n\nAppropriate methods and metrics are identified and applied. \n\n# MEASURE 1.1 \n\nApproaches and metrics for measurement of AI risks enumerated during the Map function \n\nare selected for implementation starting with the most significant AI risks. The risks or \n\ntrustworthiness characteristics that will not – or cannot – be measured are properly \n\ndocumented. \n\nAbout \n\nThe development and utility of trustworthy AI systems depends on reliable measurements \n\nand evaluations of underlying technologies and their use. Compared with traditional \n\nsoftware systems, AI technologies bring new failure modes, inherent dependence on \n\ntraining data and methods which directly tie to data quality and representativeness. \n\nAdditionally, AI systems are inherently socio -technical in nature, meaning they are \n\ninfluenced by societal dynamics and human behavior. AI risks – and benefits – can emerge \n\nfrom the interplay of technical aspects combined with societal factors related to how a \n\nsystem is used, its interactions with other AI systems, who operates it, and the social \n\ncontext in which it is deployed. In other words, What should be measured depends on the \n\npurpose, audience, and needs of the evaluations. \n\nThese two factors influence selection of approaches and metrics for measurement of AI \n\nrisks enumerated during the Map function. The AI landscape is evolving and so are the \n\nmethods and metrics for AI measurement. The evolution of metrics is key to maintaining \n\nefficacy of the measures. \n\nSuggested Actions \n\n• Establish approaches for detecting, tracking and measuring known risks, errors, \n\nincidents or negative impacts. \n\n• Identify testing procedures and metrics to demonstrate whether or not the system is fit \n\nfor purpose and functioning as claimed. \n\n• Identify testing procedures and metrics to demonstrate AI system trustworthiness \n\n• Define acceptable limits for system performance (e.g. distribution of errors), and \n\ninclude course correction suggestions if/when the system performs beyond acceptable \n\nlimits. \n\n• Define metrics for, and regularly assess, AI actor competency for effective system \n\noperation, \n\n• Identify transparency metrics to assess whether stakeholders have access to necessary \n\ninformation about system design, development, deployment, use, and evaluation. \n\n• Utilize accountability metrics to determine whether AI designers, developers, and \n\ndeployers maintain clear and transparent lines of responsibility and are open to \n\ninquiries. \n\n• Document metric selection criteria and include considered but unused metrics. 94 of 142 \n\n• Monitor AI system external inputs including training data, models developed for other \n\ncontexts, system components reused from other contexts, and third -party tools and \n\nresources. \n\n• Report metrics to inform assessments of system generalizability and reliability. \n\n• Assess and document pre - vs post -deployment system performance. Include existing \n\nand emergent risks. \n\n• Document risks or trustworthiness characteristics identified in the Map function that \n\nwill not be measured, including justification for non - measurement. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• How will the appropriate performance metrics, such as accuracy, of the AI be monitored \n\nafter the AI is deployed? \n\n• What corrective actions has the entity taken to enhance the quality, accuracy, reliability, \n\nand representativeness of the data? \n\n• Are there recommended data splits or evaluation measures? (e.g., training, \n\ndevelopment, testing; accuracy/AUC) \n\n• Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? \n\n• What testing, if any, has the entity conducted on the AI system to identify errors and \n\nlimitations (i.e. manual vs automated, adversarial and stress testing)? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• Datasheets for Datasets. \n\nReferences \n\nSara R. Jordan. “Designing Artificial Intelligence Review Boards: Creating Risk Metrics for \n\nReview of AI.” 2019 IEEE International Symposium on Technology and Society (ISTAS), \n\n2019. \n\nIEEE. “IEEE -1012 -2016: IEEE Standard for System, Software, and Hardware Verification and \n\nValidation.” IEEE Standards Association. \n\nACM Technology Policy Council. “Statement on Principles for Responsible Algorithmic \n\nSystems.” Association for Computing Machinery (ACM), October 26, 2022. \n\nPerez, E., et al. (2022). Discovering Language Model Behaviors with Model -Written \n\nEvaluations. arXiv. https://arxiv.org/abs/2212.09251 \n\nGanguli, D., et al. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling \n\nBehaviors, and Lessons Learned. arXiv. https://arxiv.org/abs/2209.07858 95 of 142 \n\nDavid Piorkowski, Michael Hind, and John Richards. \"Quantitative AI Risk Assessments: \n\nOpportunities and Challenges.\" arXiv preprint, submitted January 11, 2023. \n\nDaniel Schiff, Aladdin Ayesh, Laura Musikanski, and John C. Havens. “IEEE 7010: A New \n\nStandard for Assessing the Well -Being Implications of Artificial Intelligence.” 2020 IEEE \n\nInternational Conference on Systems, Man, and Cybernetics (SMC), 2020. \n\n# MEASURE 1.2 \n\nAppropriateness of AI metrics and effectiveness of existing controls is regularly assessed \n\nand updated including reports of errors and impacts on affected communities. \n\nAbout \n\nDifferent AI tasks, such as neural networks or natural language processing, benefit from \n\ndifferent evaluation techniques. Use -case and particular settings in which the AI system is \n\nused also affects appropriateness of the evaluation techniques. Changes in the operational \n\nsettings, data drift, model drift are among factors that suggest regularly assessing and \n\nupdating appropriateness of AI metrics and their effectiveness can enhance reliability of AI \n\nsystem measurements. \n\nSuggested Actions \n\n• Assess external validity of all measurements (e.g., the degree to which measurements \n\ntaken in one context can generalize to other contexts). \n\n• Assess effectiveness of existing metrics and controls on a regular basis throughout the \n\nAI system lifecycle. \n\n• Document reports of errors, incidents and negative impacts and assess sufficiency and \n\nefficacy of existing metrics for repairs, and upgrades \n\n• Develop new metrics when existing metrics are insufficient or ineffective for \n\nimplementing repairs and upgrades. \n\n• Develop and utilize metrics to monitor, characterize and track external inputs, including \n\nany third -party tools. \n\n• Determine frequency and scope for sharing metrics and related information with \n\nstakeholders and impacted communities. \n\n• Utilize stakeholder feedback processes established in the Map function to capture, act \n\nupon and share feedback from end users and potentially impacted communities. \n\n• Collect and report software quality metrics such as rates of bug occurrence and severity, \n\ntime to response, and time to repair (See Manage 4.3). \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What metrics has the entity developed to measure performance of the AI system? \n\n• To what extent do the metrics provide accurate and useful measure of performance? \n\n• What corrective actions has the entity taken to enhance the quality, accuracy, reliability, \n\nand representativeness of the data? 96 of 142 \n\n• How will the accuracy or appropriate performance metrics be assessed? \n\n• What is the justification for the metrics selected? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nACM Technology Policy Council. “Statement on Principles for Responsible Algorithmic \n\nSystems.” Association for Computing Machinery (ACM), October 26, 2022. \n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical \n\nLearning: Data Mining, Inference, and Prediction. 2nd ed. Springer -Verlag, 2009. \n\nHarini Suresh and John Guttag. “A Framework for Understanding Sources of Harm \n\nThroughout the Machine Learning Life Cycle.” Equity and Access in Algorithms, \n\nMechanisms, and Optimization, October 2021. \n\nChristopher M. Bishop. Pattern Recognition and Machine Learning. New York: Springer, \n\n2006. \n\nSolon Barocas, Anhong Guo, Ece Kamar, Jacquelyn Krones, Meredith Ringel Morris, Jennifer \n\nWortman Vaughan, W. Duncan Wadsworth, and Hanna Wallach. “Designing Disaggregated \n\nEvaluations of AI Systems: Choices, Considerations, and Tradeoffs.” Proceedings of the 2021 \n\nAAAI/ACM Conference on AI, Ethics, and Society, July 2021, 368 –78. \n\n# MEASURE 1.3 \n\nInternal experts who did not serve as front -line developers for the system and/or \n\nindependent assessors are involved in regular assessments and updates. Domain experts, \n\nusers, AI actors external to the team that developed or deployed the AI system, and affected \n\ncommunities are consulted in support of assessments as necessary per organizational risk \n\ntolerance. \n\nAbout \n\nThe current AI systems are brittle, the failure modes are not well described, and the systems \n\nare dependent on the context in which they were developed and do not transfer well \n\noutside of the training environment. A reliance on local evaluations will be necessary along \n\nwith a continuous monitoring of these systems. Measurements that extend beyond classical \n\nmeasures (which average across test cases) or expand to focus on pockets of failures where \n\nthere are potentially significant costs can improve the reliability of risk management \n\nactivities. Feedback from affected communities about how AI systems are being used can \n\nmake AI evaluation purposeful. Involving internal experts who did not serve as front -line \n\ndevelopers for the system and/or independent assessors regular assessments of AI systems \n\nhelps a fulsome characterization of AI systems’ performance and trustworthiness . 97 of 142 \n\nSuggested Actions \n\n• Evaluate TEVV processes regarding incentives to identify risks and impacts. \n\n• Utilize separate testing teams established in the Govern function (2.1 and 4.1) to enable \n\nindependent decisions and course -correction for AI systems. Track processes and \n\nmeasure and document change in performance. \n\n• Plan and evaluate AI system prototypes with end user populations early and \n\ncontinuously in the AI lifecycle. Document test outcomes and course correct. \n\n• Assess independence and stature of TEVV and oversight AI actors, to ensure they have \n\nthe required levels of independence and resources to perform assurance, compliance, \n\nand feedback tasks effectively \n\n• Evaluate interdisciplinary and demographically diverse internal team established in \n\nMap 1.2 \n\n• Evaluate effectiveness of external stakeholder feedback mechanisms, specifically related \n\nto processes for eliciting, evaluating and integrating input from diverse groups. \n\n• Evaluate effectiveness of external stakeholder feedback mechanisms for enhancing AI \n\nactor visibility and decision making regarding AI system risks and trustworthy \n\ncharacteristics. \n\n• Identify and utilize participatory approaches for assessing impacts that may arise from \n\nchanges in system deployment (e.g., introducing new technology, decommissioning \n\nalgorithms and models, adapting system, model or algorithm) \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\n• How easily accessible and current is the information available to external stakeholders? \n\n• To what extent does the entity communicate its AI strategic goals and objectives to the \n\ncommunity of stakeholders? \n\n• To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\n• To what extent is this information sufficient and appropriate to promote transparency? \n\nDo external stakeholders have access to information on the design, operation, and \n\nlimitations of the AI system? \n\n• What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. 98 of 142 \n\nReferences \n\nBoard of Governors of the Federal Reserve System. “SR 11 -7: Guidance on Model Risk \n\nManagement.” April 4, 2011. \n\n“Definition of independent verification and validation (IV&V)”, in IEEE 1012, IEEE Standard \n\nfor System, Software, and Hardware Verification and Validation. Annex C, \n\nMona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. “Participation Is Not a \n\nDesign Fix for Machine Learning.” Equity and Access in Algorithms, Mechanisms, and \n\nOptimization, October 2022. \n\nRediet Abebe and Kira Goldner. “Mechanism Design for Social Good.” AI Matters 4, no. 3 \n\n(October 2018): 27 –34. \n\nUpol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. “The Algorithmic Imprint.” \n\nProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency \n\n(2022). \n\n# MEASURE 2.1 \n\nTest sets, metrics, and details about the tools used during test, evaluation, validation, and \n\nverification (TEVV) are documented. \n\nAbout \n\nDocumenting measurement approaches, test sets, metrics, processes and materials used, \n\nand associated details builds foundation upon which to build a valid, reliable measurement \n\nprocess. Documentation enables repeatability and consistency, and can enhance AI risk \n\nmanagement decisions. \n\nSuggested Actions \n\n• Leverage existing industry best practices for transparency and documentation of all \n\npossible aspects of measurements. Examples include: data sheet for data sets, model \n\ncards \n\n• Regularly assess the effectiveness of tools used to document measurement approaches, \n\ntest sets, metrics, processes and materials used \n\n• Update the tools as needed \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\n• To what extent has the entity documented the AI system’s development, testing \n\nmethodology, metrics, and performance outcomes? 99 of 142 \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. \n\nReferences \n\nEmily M. Bender and Batya Friedman. “Data Statements for Natural Language Processing: \n\nToward Mitigating System Bias and Enabling Better Science.” Transactions of the \n\nAssociation for Computational Linguistics 6 (2018): 587 –604. \n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben \n\nHutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. “Model Cards for \n\nModel Reporting.” FAT *19: Proceedings of the Conference on Fairness, Accountability, and \n\nTransparency, January 2019, 220 –29. \n\nIEEE Computer Society. “Software Engineering Body of Knowledge Version 3: IEEE \n\nComputer Society.” IEEE Computer Society. \n\nIEEE. “IEEE -1012 -2016: IEEE Standard for System, Software, and Hardware Verification and \n\nValidation.” IEEE Standards Association. \n\nBoard of Governors of the Federal Reserve System. “SR 11 -7: Guidance on Model Risk \n\nManagement.” April 4, 2011. \n\nAbigail Z. Jacobs and Hanna Wallach. “Measurement and Fairness.” FAccT '21: Proceedings \n\nof the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, \n\n375 –85. \n\nJeanna Matthews, Bruce Hedin, Marc Canellas. Trustworthy Evidence for Trustworthy \n\nTechnology: An Overview of Evidence for Assessing the Trustworthiness of Autonomous \n\nand Intelligent Systems. IEEE -USA, September 29 2022. \n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. “Hard Choices in Artificial \n\nIntelligence.” Artificial Intelligence 300 (November 2021). \n\n# MEASURE 2.2 \n\nEvaluations involving human subjects meet applicable requirements (including human \n\nsubject protection) and are representative of the relevant population. \n\nAbout \n\nMeasurement and evaluation of AI systems often involves testing with human subjects or \n\nusing data captured from human subjects. Protection of human subjects is required by law \n\nwhen carrying out federally funded research, and is a domain specific requirement for some \n\ndisciplines. Standard human subjects protection procedures include protecting the welfare 100 of 142 \n\nand interests of human subjects, designing evaluations to minimize risks to subjects, and \n\ncompletion of mandatory training regarding legal requirements and expectations. \n\nEvaluations of AI system performance that utilize human subjects or human subject data \n\nshould reflect the population within the context of use. AI system activities utilizing non -\n\nrepresentative data may lead to inaccurate assessments or negative and harmful outcomes. \n\nIt is often difficult – and sometimes impossible, to collect data or perform evaluation tasks \n\nthat reflect the full operational purview of an AI system. Methods for collecting, annotating, \n\nor using these data can also contribute to the challenge. To counteract these challenges, \n\norganizations can connect human subjects data collection, and dataset practices, to AI \n\nsystem contexts and purposes and do so in close collaboration with AI Actors from the \n\nrelevant domains. \n\nSuggested Actions \n\n• Follow human subjects research requirements as established by organizational and \n\ndisciplinary requirements, including informed consent and compensation, during \n\ndataset collection activities. \n\n• Analyze differences between intended and actual population of users or data subjects, \n\nincluding likelihood for errors, incidents or negative impacts. \n\n• Utilize disaggregated evaluation methods (e.g. by race, age, gender, ethnicity, ability, \n\nregion) to improve AI system performance when deployed in real world settings. \n\n• Establish thresholds and alert procedures for dataset representativeness within the \n\ncontext of use. \n\n• Construct datasets in close collaboration with experts with knowledge of the context of \n\nuse. \n\n• Follow intellectual property and privacy rights related to datasets and their use, \n\nincluding for the subjects represented in the data. \n\n• Evaluate data representativeness through \n\n• investigating known failure modes, \n\n• assessing data quality and diverse sourcing, \n\n• applying public benchmarks, \n\n• traditional bias testing, \n\n• chaos engineering, \n\n• stakeholder feedback \n\n• Use informed consent for individuals providing data used in system testing and \n\nevaluation. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? 101 of 142 \n\n• How has the entity identified and mitigated potential impacts of bias in the data, \n\nincluding inequitable or discriminatory outcomes? \n\n• To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\n• To what extent has the entity identified and mitigated potential bias —statistical, \n\ncontextual, and historical —in the data? \n\n• If it relates to people, were they told what the dataset would be used for and did they \n\nconsent? What community norms exist for data collected from human communications? \n\nIf consent was obtained, how? Were the people provided with any mechanism to revoke \n\ntheir consent in the future or for certain uses? \n\n• If human subjects were used in the development or testing of the AI system, what \n\nprotections were put in place to promote their safety and wellbeing?. \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. \n\n• Datasheets for Datasets. \n\nReferences \n\nUnited States Department of Health, Education, and Welfare's National Commission for the \n\nProtection of Human Subjects of Biomedical and Behavioral Research. The Belmont Report: \n\nEthical Principles and Guidelines for the Protection of Human Subjects of Research. Volume \n\nII. United States Department of Health and Human Services Office for Human Research \n\nProtections. April 18, 1979. \n\nOffice for Human Research Protections (OHRP). “45 CFR 46.” United States Department of \n\nHealth and Human Services Office for Human Research Protections, March 10, 2021. \n\nOffice for Human Research Protections (OHRP). “Human Subject Regulations Decision \n\nChart.” United States Department of Health and Human Services Office for Human Research \n\nProtections, June 30, 2020. \n\nJacob Metcalf and Kate Crawford. “Where Are Human Subjects in Big Data Research? The \n\nEmerging Ethics Divide.” Big Data and Society 3, no. 1 (2016). \n\nBoaz Shmueli, Jan Fell, Soumya Ray, and Lun -Wei Ku. \"Beyond Fair Pay: Ethical Implications \n\nof NLP Crowdsourcing.\" arXiv preprint, submitted April 20, 2021. \n\nDivyansh Kaushik, Zachary C. Lipton, and Alex John London. \"Resolving the Human Subjects \n\nStatus of Machine Learning's Crowdworkers.\" arXiv preprint, submitted June 8, 2022. 102 of 142 \n\nOffice for Human Research Protections (OHRP). “International Compilation of Human \n\nResearch Standards.” United States Department of Health and Human Services Office for \n\nHuman Research Protections, February 7, 2022. \n\nNational Institutes of Health. “Definition of Human Subjects Research.” NIH Central \n\nResource for Grants and Funding Information, January 13, 2020. \n\nJoy Buolamwini and Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in \n\nCommercial Gender Classification.” Proceedings of the 1st Conference on Fairness, \n\nAccountability and Transparency in PMLR 81 (2018): 77 –91. \n\nEun Seo Jo and Timnit Gebru. “Lessons from Archives: Strategies for Collecting Sociocultural \n\nData in Machine Learning.” FAT* '20: Proceedings of the 2020 Conference on Fairness, \n\nAccountability, and Transparency, January 2020, 306 –16. \n\nMarco Gerardi, Katarzyna Barud, Marie -Catherine Wagner, Nikolaus Forgo, Francesca \n\nFallucchi, Noemi Scarpato, Fiorella Guadagni, and Fabio Massimo Zanzotto. \"Active \n\nInformed Consent to Boost the Application of Machine Learning in Medicine.\" arXiv \n\npreprint, submitted September 27, 2022. \n\nShari Trewin. \"AI Fairness for People with Disabilities: Point of View.\" arXiv preprint, \n\nsubmitted November 26, 2018. \n\nAndrea Brennen, Ryan Ashley, Ricardo Calix, JJ Ben -Joseph, George Sieniawski, Mona Gogia, \n\nand BNH.AI. AI Assurance Audit of RoBERTa, an Open source, Pretrained Large Language \n\nModel. IQT Labs, December 2022. \n\n# MEASURE 2.3 \n\nAI system performance or assurance criteria are measured qualitatively or quantitatively \n\nand demonstrated for conditions similar to deployment setting(s). Measures are \n\ndocumented. \n\nAbout \n\nThe current risk and impact environment suggests AI system performance estimates are \n\ninsufficient and require a deeper understanding of deployment context of use. \n\nComputationally focused performance testing and evaluation schemes are restricted to test \n\ndata sets and in silico techniques. These approaches do not directly evaluate risks and \n\nimpacts in real world environments and can only predict what might create impact based \n\non an approximation of expected AI use. To properly manage risks, more direct information \n\nis necessary to understand how and under what conditions deployed AI creates impacts, \n\nwho is most likely to be impacted, and what that experience is like. \n\nSuggested Actions \n\n• Conduct regular and sustained engagement with potentially impacted communities \n\n• Maintain a demographically diverse and multidisciplinary and collaborative internal \n\nteam 103 of 142 \n\n• Regularly test and evaluate systems in non -optimized conditions, and in collaboration \n\nwith AI actors in user interaction and user experience (UI/UX) roles. \n\n• Evaluate feedback from stakeholder engagement activities, in collaboration with human \n\nfactors and socio -technical experts. \n\n• Collaborate with socio -technical, human factors, and UI/UX experts to identify notable \n\ncharacteristics in context of use that can be translated into system testing scenarios. \n\n• Measure AI systems prior to deployment in conditions similar to expected scenarios. \n\n• Measure and document performance criteria such as validity (false positive rate, false \n\nnegative rate, etc.) and efficiency (training times, prediction latency, etc.) related to \n\nground truth within the deployment context of use. \n\n• Measure assurance criteria such as AI actor competency and experience. \n\n• Document differences between measurement setting and the deployment \n\nenvironment(s). \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What experiments were initially run on this dataset? To what extent have experiments \n\non the AI system been documented? \n\n• To what extent does the system/entity consistently measure progress towards stated \n\ngoals and objectives? \n\n• How will the appropriate performance metrics, such as accuracy, of the AI be monitored \n\nafter the AI is deployed? How much distributional shift or model drift from baseline \n\nperformance is acceptable? \n\n• As time passes and conditions change, is the training data still representative of the \n\noperational environment? \n\n• What testing, if any, has the entity conducted on theAI system to identify errors and \n\nlimitations (i.e.adversarial or stress testing)? \n\nAI Transparency Resources \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. \n\n• Datasheets for Datasets. \n\nReferences \n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical \n\nLearning: Data Mining, Inference, and Prediction. 2nd ed. Springer -Verlag, 2009. \n\nJessica Zosa Forde, A. Feder Cooper, Kweku Kwegyir -Aggrey, Chris De Sa, and Michael \n\nLittman. \"Model Selection's Disparate Impact in Real -World Deep Learning Applications.\" \n\narXiv preprint, submitted September 7, 2021. 104 of 142 \n\nInioluwa Deborah Raji, I. Elizabeth Kumar, Aaron Horowitz, and Andrew Selbst. “The Fallacy \n\nof AI Functionality.” FAccT '22: 2022 ACM Conference on Fairness, Accountability, and \n\nTransparency, June 2022, 959 –72. \n\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex \n\nHanna. “Data and Its (Dis)Contents: A Survey of Dataset Development and Use in Machine \n\nLearning Research.” Patterns 2, no. 11 (2021): 100336. \n\nChristopher M. Bishop. Pattern Recognition and Machine Learning. New York: Springer, \n\n2006. \n\nMd Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. \"A Comprehensive Study \n\non Deep Learning Bug Characteristics.\" arXiv preprint, submitted June 3, 2019. \n\nSwaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva, Chris Bryan, and Chitta Baral. \n\n\"DQI: Measuring Data Quality in NLP.\" arXiv preprint, submitted May 2, 2020. \n\nDoug Wielenga. \"Paper 073 -2007: Identifying and Overcoming Common Data Mining \n\nMistakes.\" SAS Global Forum 2007: Data Mining and Predictive Modeling, SAS Institute, \n\n2007. \n\nSoftware Resources \n\n• Drifter  library (performance assessment) \n\n• Manifold  library (performance assessment) \n\n• MLextend  library (performance assessment) \n\n• PiML  library (explainable models, performance assessment) \n\n• SALib  library (performance assessment) \n\n• What -If Tool  (performance assessment) \n\n# MEASURE 2.4 \n\nThe functionality and behavior of the AI system and its components – as identified in the \n\nMAP function – are monitored when in production. \n\nAbout \n\nAI systems may encounter new issues and risks while in production as the environment \n\nevolves over time. This effect, often referred to as “drift”, means AI systems no longer meet \n\nthe assumptions and limitations of the original design. Regular monitoring allows AI Actors \n\nto monitor the functionality and behavior of the AI system and its components – as \n\nidentified in the MAP function - and enhance the speed and efficacy of necessary system \n\ninterventions. \n\nSuggested Actions \n\n• Monitor and document how metrics and performance indicators observed in production \n\ndiffer from the same metrics collected during pre -deployment testing. When differences \n\nare observed, consider error propagation and feedback loop risks. 105 of 142 \n\n• Utilize hypothesis testing or human domain expertise to measure monitored \n\ndistribution differences in new input or output data relative to test environments \n\n• Monitor for anomalies using approaches such as control limits, confidence intervals, \n\nintegrity constraints and ML algorithms. When anomalies are observed, consider error \n\npropagation and feedback loop risks. \n\n• Verify alerts are in place for when distributions in new input data or generated \n\npredictions observed in production differ from pre -deployment test outcomes, or when \n\nanomalies are detected. \n\n• Assess the accuracy and quality of generated outputs against new collected ground -\n\ntruth information as it becomes available. \n\n• Utilize human review to track processing of unexpected data and reliability of generated \n\noutputs; warn system users when outputs may be unreliable. Verify that human \n\noverseers responsible for these processes have clearly defined responsibilities and \n\ntraining for specified tasks. \n\n• Collect uses cases from the operational environment for system testing and monitoring \n\nactivities in accordance with organizational policies and regulatory or disciplinary \n\nrequirements (e.g. informed consent, institutional review board approval, human \n\nresearch protections), \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent is the output of each component appropriate for the operational \n\ncontext? \n\n• What justifications, if any, has the entity provided for the assumptions, boundaries, and \n\nlimitations of the AI system? \n\n• How will the appropriate performance metrics, such as accuracy, of the AI be monitored \n\nafter the AI is deployed? \n\n• As time passes and conditions change, is the training data still representative of the \n\noperational environment? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nLuca Piano, Fabio Garcea, Valentina Gatteschi, Fabrizio Lamberti, and Lia Morra. “Detecting \n\nDrift in Deep Learning: A Methodology Primer.” IT Professional 24, no. 5 (2022): 53 –60. \n\nLarysa Visengeriyeva, et al. “Awesome MLOps.“ GitHub. 106 of 142 \n\n# MEASURE 2.5 \n\nThe AI system to be deployed is demonstrated to be valid and reliable. Limitations of the \n\ngeneralizability beyond the conditions under which the technology was developed are \n\ndocumented. \n\nAbout \n\nAn AI system that is not validated or that fails validation may be inaccurate or unreliable or \n\nmay generalize poorly to data and settings beyond its training, creating and increasing AI \n\nrisks and reducing trustworthiness. AI Actors can improve system validity by creating \n\nprocesses for exploring and documenting system limitations. This includes broad \n\nconsideration of purposes and uses for which the system was not designed. \n\nValidation risks include the use of proxies or other indicators that are often constructed by \n\nAI development teams to operationalize phenomena that are either not directly observable \n\nor measurable (e.g, fairness, hireability, honesty, propensity to commit a crime). Teams can \n\nmitigate these risks by demonstrating that the indicator is measuring the concept it claims \n\nto measure (also known as construct validity). Without this and other types of validation, \n\nvarious negative properties or impacts may go undetected, including the presence of \n\nconfounding variables, potential spurious correlations, or error propagation and its \n\npotential impact on other interconnected systems. \n\nSuggested Actions \n\n• Define the operating conditions and socio -technical context under which the AI system \n\nwill be validated. \n\n• Define and document processes to establish the system’s operational conditions and \n\nlimits. \n\n• Establish or identify, and document approaches to measure forms of validity, including: \n\n• construct validity (the test is measuring the concept it claims to measure) \n\n• internal validity (relationship being tested is not influenced by other factors or \n\nvariables) \n\n• external validity (results are generalizable beyond the training condition) \n\n• the use of experimental design principles and statistical analyses and modeling. \n\n• Assess and document system variance. Standard approaches include confidence \n\nintervals, standard deviation, standard error, bootstrapping, or cross -validation. \n\n• Establish or identify, and document robustness measures. \n\n• Establish or identify, and document reliability measures. \n\n• Establish practices to specify and document the assumptions underlying measurement \n\nmodels to ensure proxies accurately reflect the concept being measured. \n\n• Utilize standard software testing approaches (e.g. unit, integration, functional and chaos \n\ntesting, computer -generated test cases, etc.) \n\n• Utilize standard statistical methods to test bias, inferential associations, correlation, and \n\ncovariance in adopted measurement models. 107 of 142 \n\n• Utilize standard statistical methods to test variance and reliability of system outcomes. \n\n• Monitor operating conditions for system performance outside of defined limits. \n\n• Identify TEVV approaches for exploring AI system limitations, including testing \n\nscenarios that differ from the operational environment. Consult experts with knowledge \n\nof specific context of use. \n\n• Define post -alert actions. Possible actions may include: \n\n• alerting other relevant AI actors before action, \n\n• requesting subsequent human review of action, \n\n• alerting downstream users and stakeholder that the system is operating outside it’s \n\ndefined validity limits, \n\n• tracking and mitigating possible error propagation \n\n• action logging \n\n• Log input data and relevant system configuration information whenever there is an \n\nattempt to use the system beyond its well -defined range of system validity. \n\n• Modify the system over time to extend its range of system validity to new operating \n\nconditions. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What testing, if any, has the entity conducted on theAI system to identify errors and \n\nlimitations (i.e.adversarial or stress testing)? \n\n• Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\n• How has the entity identified and mitigated potential impacts of bias in the data, \n\nincluding inequitable or discriminatory outcomes? \n\n• To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\n• What goals and objectives does the entity expect to achieve by designing, developing, \n\nand/or deploying the AI system? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nReferences \n\nAbigail Z. Jacobs and Hanna Wallach. “Measurement and Fairness.” FAccT '21: Proceedings \n\nof the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, \n\n375 –85. \n\nDebugging Machine Learning Models. Proceedings of ICLR 2019 Workshop, May 6, 2019, \n\nNew Orleans, Louisiana. 108 of 142 \n\nPatrick Hall. “Strategies for Model Debugging.” Towards Data Science, November 8, 2019. \n\nSuchi Saria and Adarsh Subbaswamy. \"Tutorial: Safe and Reliable Machine Learning.\" arXiv \n\npreprint, submitted April 15, 2019. \n\nGoogle Developers. “Overview of Debugging ML Models.” Google Developers Machine \n\nLearning Foundational Courses, n.d. \n\nR. Mohanani, I. Salman, B. Turhan, P. Rodríguez and P. Ralph, \"Cognitive Biases in Software \n\nEngineering: A Systematic Mapping Study,\" in IEEE Transactions on Software Engineering, \n\nvol. 46, no. 12, pp. 1318 -1339, Dec. 2020, \n\nSoftware Resources \n\n• Drifter  library (performance assessment) \n\n• Manifold  library (performance assessment) \n\n• MLextend  library (performance assessment) \n\n• PiML  library (explainable models, performance assessment) \n\n• SALib  library (performance assessment) \n\n• What -If Tool  (performance assessment) \n\n# MEASURE 2.6 \n\nAI system is evaluated regularly for safety risks – as identified in the MAP function. The AI \n\nsystem to be deployed is demonstrated to be safe, its residual negative risk does not exceed \n\nthe risk tolerance, and can fail safely, particularly if made to operate beyond its knowledge \n\nlimits. Safety metrics implicate system reliability and robustness, real -time monitoring, and \n\nresponse times for AI system failures. \n\nAbout \n\nMany AI systems are being introduced into settings such as transportation, manufacturing \n\nor security, where failures may give rise to various physical or environmental harms. AI \n\nsystems that may endanger human life, health, property or the environment are tested \n\nthoroughly prior to deployment, and are regularly evaluated to confirm the system is safe \n\nduring normal operations, and in settings beyond its proposed use and knowledge limits. \n\nMeasuring activities for safety often relate to exhaustive testing in development and \n\ndeployment contexts, understanding the limits of a system’s reliable, robust, and safe \n\nbehavior, and real -time monitoring of various aspects of system performance. These \n\nactivities are typically conducted along with other risk mapping, management, and \n\ngovernance tasks such as avoiding past failed designs, establishing and rehearsing incident \n\nresponse plans that enable quick responses to system problems, the instantiation of \n\nredundant functionality to cover failures, and transparent and accountable governance. \n\nSystem safety incidents or failures are frequently reported to be related to organizational \n\ndynamics and culture. Independent auditors may bring important independent perspectives \n\nfor reviewing evidence of AI system safety. 109 of 142 \n\nSuggested Actions \n\n• Thoroughly measure system performance in development and deployment contexts, \n\nand under stress conditions. \n\n• Employ test data assessments and simulations before proceeding to production \n\ntesting. Track multiple performance quality and error metrics. \n\n• Stress -test system performance under likely scenarios (e.g., concept drift, high load) \n\nand beyond known limitations, in consultation with domain experts. \n\n• Test the system under conditions similar to those related to past known incidents or \n\nnear -misses and measure system performance and safety characteristics \n\n• Apply chaos engineering approaches to test systems in extreme conditions and \n\ngauge unexpected responses. \n\n• Document the range of conditions under which the system has been tested and \n\ndemonstrated to fail safely. \n\n• Measure and monitor system performance in real -time to enable rapid response when \n\nAI system incidents are detected. \n\n• Collect pertinent safety statistics (e.g., out -of -range performance, incident response \n\ntimes, system down time, injuries, etc.) in anticipation of potential information sharing \n\nwith impacted communities or as required by AI system oversight personnel. \n\n• Align measurement to the goal of continuous improvement. Seek to increase the range \n\nof conditions under which the system is able to fail safely through system modifications \n\nin response to in -production testing and events. \n\n• Document, practice and measure incident response plans for AI system incidents, \n\nincluding measuring response and down times. \n\n• Compare documented safety testing and monitoring information with established risk \n\ntolerances on an on -going basis. \n\n• Consult MANAGE for detailed information related to managing safety risks. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• What testing, if any, has the entity conducted on the AI system to identify errors and \n\nlimitations (i.e.adversarial or stress testing)? \n\n• To what extent has the entity documented the AI system’s development, testing \n\nmethodology, metrics, and performance outcomes? \n\n• Did you establish mechanisms that facilitate the AI system’s auditability (e.g. \n\ntraceability of the development process, the sourcing of training data and the logging of \n\nthe AI system’s processes, outcomes, positive and negative impact)? \n\n• Did you ensure that the AI system can be audited by independent third parties? \n\n• Did you establish a process for third parties (e.g. suppliers, end -users, subjects, \n\ndistributors/vendors or workers) to report potential vulnerabilities, risks or biases in \n\nthe AI system? 110 of 142 \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nAI Incident Database. 2022. \n\nAIAAIC Repository. 2022. \n\nNetflix. Chaos Monkey. \n\nIBM. “IBM's Principles of Chaos Engineering.” IBM, n.d. \n\nSuchi Saria and Adarsh Subbaswamy. \"Tutorial: Safe and Reliable Machine Learning.\" arXiv \n\npreprint, submitted April 15, 2019. \n\nDaniel Kang, Deepti Raghavan, Peter Bailis, and Matei Zaharia. \"Model assertions for \n\nmonitoring and improving ML models.\" Proceedings of Machine Learning and Systems 2 \n\n(2020): 481 -496. \n\nLarysa Visengeriyeva, et al. “Awesome MLOps.“ GitHub. \n\nMcGregor, S., Paeth, K., & Lam, K.T. (2022). Indexing AI Risks with Incidents, Issues, and \n\nVariants. ArXiv, abs/2211.10384. \n\n# MEASURE 2.7 \n\nAI system security and resilience – as identified in the MAP function – are evaluated and \n\ndocumented. \n\nAbout \n\nAI systems, as well as the ecosystems in which they are deployed, may be said to be resilient \n\nif they can withstand unexpected adverse events or unexpected changes in their \n\nenvironment or use – or if they can maintain their functions and structure in the face of \n\ninternal \n\nand external change and degrade safely and gracefully when this is necessary. Common \n\nsecurity concerns relate to adversarial examples, data poisoning, and the exfiltration of \n\nmodels, training data, or other intellectual property through AI system endpoints. AI \n\nsystems that can maintain confidentiality, integrity, and availability through protection \n\nmechanisms that prevent unauthorized access and use may be said to be secure. \n\nSecurity and resilience are related but distinct characteristics. While resilience is the ability \n\nto return to normal function after an unexpected adverse event, security includes resilience \n\nbut also encompasses protocols to avoid, protect against, respond to, or recover 111 of 142 \n\nfrom attacks. Resilience relates to robustness and encompasses unexpected or adversarial \n\nuse (or abuse or misuse) of the model or data. \n\nSuggested Actions \n\n• Establish and track AI system security tests and metrics (e.g., red -teaming activities, \n\nfrequency and rate of anomalous events, system down -time, incident response times, \n\ntime -to -bypass, etc.). \n\n• Use red -team exercises to actively test the system under adversarial or stress \n\nconditions, measure system response, assess failure modes or determine if system can \n\nreturn to normal function after an unexpected adverse event. \n\n• Document red -team exercise results as part of continuous improvement efforts, \n\nincluding the range of security test conditions and results. \n\n• Use red -teaming exercises to evaluate potential mismatches between claimed and actual \n\nsystem performance. \n\n• Use countermeasures (e.g, authentication, throttling, differential privacy, robust ML \n\napproaches) to increase the range of security conditions under which the system is able \n\nto return to normal function. \n\n• Modify system security procedures and countermeasures to increase robustness and \n\nresilience to attacks in response to testing and events experienced in production. \n\n• Verify that information about errors and attack patterns is shared with incident \n\ndatabases, other organizations with similar systems, and system users and stakeholders \n\n(MANAGE -4.1). \n\n• Develop and maintain information sharing practices with AI actors from other \n\norganizations to learn from common attacks. \n\n• Verify that third party AI resources and personnel undergo security audits and \n\nscreenings. Risk indicators may include failure of third parties to provide relevant \n\nsecurity information. \n\n• Utilize watermarking technologies as a deterrent to data and model extraction attacks. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent does the plan specifically address risks associated with acquisition, \n\nprocurement of packaged software from vendors, cybersecurity controls, computational \n\ninfrastructure, data, data science, deployment mechanics, and system failure? \n\n• What assessments has the entity conducted on data security and privacy impacts \n\nassociated with the AI system? \n\n• What processes exist for data generation, acquisition/collection, security, maintenance, \n\nand dissemination? \n\n• What testing, if any, has the entity conducted on the AI system to identify errors and \n\nlimitations (i.e. adversarial or stress testing)? \n\n• If a third party created the AI, how will you ensure a level of explainability or \n\ninterpretability? 112 of 142 \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nMatthew P. Barrett. “Framework for Improving Critical Infrastructure Cybersecurity \n\nVersion 1.1.” National Institute of Standards and Technology (NIST), April 16, 2018. \n\nNicolas Papernot. \"A Marauder's Map of Security and Privacy in Machine Learning.\" arXiv \n\npreprint, submitted on November 3, 2018. \n\nGary McGraw, Harold Figueroa, Victor Shepardson, and Richie Bonett. “BIML Interactive \n\nMachine Learning Risk Framework.” Berryville Institute of Machine Learning (BIML), 2022. \n\nMitre Corporation. “Mitre/Advmlthreatmatrix: Adversarial Threat Landscape for AI \n\nSystems.” GitHub, 2023. \n\nNational Institute of Standards and Technology (NIST). “Cybersecurity Framework.” NIST, \n\n2023. \n\nUpol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, and Hal Daumé. 2024. Seamful XAI: \n\nOperationalizing Seamful Design in Explainable AI. Proc. ACM Hum. -Comput. Interact. 8, \n\nCSCW1, Article 119. https://doi.org/10.1145/3637396 \n\nSoftware Resources \n\n• adversarial -robustness -toolbox \n\n• counterfit \n\n• foolbox \n\n• ml_privacy_meter \n\n• robustness \n\n• tensorflow/privacy \n\n• projectGuardRail \n\n# MEASURE 2.8 \n\nRisks associated with transparency and accountability – as identified in the MAP function –\n\nare examined and documented. \n\nAbout \n\nTransparency enables meaningful visibility into entire AI pipelines, workflows, processes or \n\norganizations and decreases information asymmetry between AI developers and operators \n\nand other AI Actors and impacted communities. Transparency is a central element of \n\neffective AI risk management that enables insight into how an AI system is working, and the \n\nability to address risks if and when they emerge. The ability for system users, individuals, or \n\nimpacted communities to seek redress for incorrect or problema tic AI system outcomes is 113 of 142 \n\none control for transparency and accountability. Higher level recourse processes are \n\ntypically enabled by lower level implementation efforts directed at explainability and \n\ninterpretability functionality. See Measure 2.9. \n\nTransparency and accountability across organizations and processes is crucial to reducing \n\nAI risks. Accountable leadership – whether individuals or groups – and transparent roles, \n\nresponsibilities, and lines of communication foster and incentivize quality assurance and \n\nrisk management activities within organizations. \n\nLack of transparency complicates measurement of trustworthiness and whether AI systems \n\nor organizations are subject to effects of various individual and group biases and design \n\nblindspots and could lead to diminished user, organizational and community trust, and \n\ndecreased overall system value. Enstating accountable and transparent organizational \n\nstructures along with documenting system risks can enable system improvement and risk \n\nmanagement efforts, allowing AI actors along the lifecycle to identify errors, suggest \n\nimprovements, and figure out new ways to contextualize and generalize AI system features \n\nand outcomes. \n\nSuggested Actions \n\n• Instrument the system for measurement and tracking, e.g., by maintaining histories, \n\naudit logs and other information that can be used by AI actors to review and evaluate \n\npossible sources of error, bias, or vulnerability. \n\n• Calibrate controls for users in close collaboration with experts in user interaction and \n\nuser experience (UI/UX), human computer interaction (HCI), and/or human -AI teaming. \n\n• Test provided explanations for calibration with different audiences including operators, \n\nend users, decision makers and decision subjects (individuals for whom decisions are \n\nbeing made), and to enable recourse for consequential system decisions that affect end \n\nusers or subjects. \n\n• Measure and document human oversight of AI systems: \n\n• Document the degree of oversight that is provided by specified AI actors regarding \n\nAI system output. \n\n• Maintain statistics about downstream actions by end users and operators such as \n\nsystem overrides. \n\n• Maintain statistics about and document reported errors or complaints, time to \n\nrespond, and response types. \n\n• Maintain and report statistics about adjudication activities. \n\n• Track, document, and measure organizational accountability regarding AI systems via \n\npolicy exceptions and escalations, and document “go” and “no/go” decisions made by \n\naccountable parties. \n\n• Track and audit the effectiveness of organizational mechanisms related to AI risk \n\nmanagement, including: 114 of 142 \n\n• Lines of communication between AI actors, executive leadership, users and \n\nimpacted communities. \n\n• Roles and responsibilities for AI actors and executive leadership. \n\n• Organizational accountability roles, e.g., chief model risk officers, AI oversight \n\ncommittees, responsible or ethical AI directors, etc. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\n• What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\n• Who is accountable for the ethical considerations during all stages of the AI lifecycle? \n\n• Who will be responsible for maintaining, re -verifying, monitoring, and updating this AI \n\nonce deployed? \n\n• Are the responsibilities of the personnel involved in the various AI governance \n\nprocesses clearly defined? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nNational Academies of Sciences, Engineering, and Medicine. Human -AI Teaming: State -of -\n\nthe -Art and Research Needs. 2022. \n\nInioluwa Deborah Raji and Jingying Yang. \"ABOUT ML: Annotation and Benchmarking on \n\nUnderstanding and Transparency of Machine Learning Lifecycles.\" arXiv preprint, \n\nsubmitted January 8, 2020. \n\nAndrew Smith. \"Using Artificial Intelligence and Algorithms.\" Federal Trade Commission \n\nBusiness Blog, April 8, 2020. \n\nBoard of Governors of the Federal Reserve System. “SR 11 -7: Guidance on Model Risk \n\nManagement.” April 4, 2011. \n\nJoshua A. Kroll. “Outlining Traceability: A Principle for Operationalizing Accountability in \n\nComputing Systems.” FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, \n\nAccountability, and Transparency, March 1, 2021, 758 –71. \n\nJennifer Cobbe, Michelle Seng Lee, and Jatinder Singh. “Reviewable Automated Decision -\n\nMaking: A Framework for Accountable Algorithmic Systems.” FAccT '21: Proceedings of the \n\n2021 ACM Conference on Fairness, Accountability, and Transparency, March 1, 2021, 598 –\n\n609. 115 of 142 \n\n# MEASURE 2.9 \n\nThe AI model is explained, validated, and documented, and AI system output is interpreted \n\nwithin its context – as identified in the MAP function – and to inform responsible use and \n\ngovernance. \n\nAbout \n\nExplainability and interpretability assist those operating or overseeing an AI system, as well \n\nas users of an AI system, to gain deeper insights into the functionality and trustworthiness \n\nof the system, including its outputs. \n\nExplainable and interpretable AI systems offer information that help end users understand \n\nthe purposes and potential impact of an AI system. Risk from lack of explainability may be \n\nmanaged by describing how AI systems function, with descriptions tailored to individual \n\ndifferences such as the user’s role, knowledge, and skill level. Explainable systems can be \n\ndebugged and monitored more easily, and they lend themselves to more thorough \n\ndocumentation, audit, and governance. \n\nRisks to interpretability often can be addressed by communicating a description of why \n\nan AI system made a particular prediction or recommendation. \n\nTransparency, explainability, and interpretability are distinct characteristics that support \n\neach other. Transparency can answer the question of “what happened”. Explainability can \n\nanswer the question of “how” a decision was made in the system. Interpretability can \n\nanswer the question of “why” a decision was made by the system and its \n\nmeaning or context to the user. \n\nSuggested Actions \n\n• Verify systems are developed to produce explainable models, post -hoc explanations and \n\naudit logs. \n\n• When possible or available, utilize approaches that are inherently explainable, such as \n\ntraditional and penalized generalized linear models , decision trees, nearest -neighbor \n\nand prototype -based approaches, rule -based models, generalized additive models , \n\nexplainable boosting machines and neural additive models. \n\n• Test explanation methods and resulting explanations prior to deployment to gain \n\nfeedback from relevant AI actors, end users, and potentially impacted individuals or \n\ngroups about whether explanations are accurate, clear, and understandable. \n\n• Document AI model details including model type (e.g., convolutional neural network, \n\nreinforcement learning, decision tree, random forest, etc.) data features, training \n\nalgorithms, proposed uses, decision thresholds, training data, evaluation data, and \n\nethical considerations. \n\n• Establish, document, and report performance and error metrics across demographic \n\ngroups and other segments relevant to the deployment context. 116 of 142 \n\n• Explain systems using a variety of methods, e.g., visualizations, model extraction, \n\nfeature importance, and others. Since explanations may not accurately summarize \n\ncomplex systems, test explanations according to properties such as fidelity, consistency, \n\nrobustness, and interpretability. \n\n• Assess the characteristics of system explanations according to properties such as \n\nfidelity (local and global), ambiguity, interpretability, interactivity, consistency, and \n\nresilience to attack/manipulation. \n\n• Test the quality of system explanations with end -users and other groups. \n\n• Secure model development processes to avoid vulnerability to external manipulation \n\nsuch as gaming explanation processes. \n\n• Test for changes in models over time, including for models that adjust in response to \n\nproduction data. \n\n• Use transparency tools such as data statements and model cards to document \n\nexplanatory and validation information. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Given the purpose of the AI, what level of explainability or interpretability is required \n\nfor how the AI made its determination? \n\n• Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\n• How has the entity documented the AI system’s data provenance, including sources, \n\norigins, transformations, augmentations, labels, dependencies, constraints, and \n\nmetadata? \n\n• What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. \n\nReferences \n\nChaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin. \n\n\"This Looks Like That: Deep Learning for Interpretable Image Recognition.\" arXiv preprint, \n\nsubmitted December 28, 2019. \n\nCynthia Rudin. \"Stop Explaining Black Box Machine Learning Models for High Stakes \n\nDecisions and Use Interpretable Models Instead.\" arXiv preprint, submitted September 22, \n\n2019. 117 of 142 \n\nDavid A. Broniatowski. \"NISTIR 8367 Psychological Foundations of Explainability and \n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology \n\n(NIST), 2021. \n\nAlejandro Barredo Arrieta, Natalia Díaz -Rodríguez, Javier Del Ser, Adrien Bennetot, Siham \n\nTabik, Alberto Barbado, Salvador Garcia, et al. “Explainable Artificial Intelligence (XAI): \n\nConcepts, Taxonomies, Opportunities, and Challenges Toward Responsible AI.” Information \n\nFusion 58 (June 2020): 82 –115. \n\nZana Buçinca, Phoebe Lin, Krzysztof Z. Gajos, and Elena L. Glassman. “Proxy Tasks and \n\nSubjective Measures Can Be Misleading in Evaluating Explainable AI Systems.” IUI '20: \n\nProceedings of the 25th International Conference on Intelligent User Interfaces, March 17, \n\n2020, 454 –64. \n\nP. Jonathon Phillips, Carina A. Hahn, Peter C. Fontana, Amy N. Yates, Kristen Greene, David \n\nA. Broniatowski, and Mark A. Przybocki. \"NISTIR 8312 Four Principles of Explainable \n\nArtificial Intelligence.\" National Institute of Standards and Technology (NIST), September \n\n2021. \n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben \n\nHutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. “Model Cards for \n\nModel Reporting.” FAT *19: Proceedings of the Conference on Fairness, Accountability, and \n\nTransparency, January 2019, 220 –29. \n\nKe Yang, Julia Stoyanovich, Abolfazl Asudeh, Bill Howe, HV Jagadish, and Gerome Miklau. “A \n\nNutritional Label for Rankings.” SIGMOD '18: Proceedings of the 2018 International \n\nConference on Management of Data, May 27, 2018, 1773 –76. \n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"'Why Should I Trust You?': \n\nExplaining the Predictions of Any Classifier.\" arXiv preprint, submitted August 9, 2016. \n\nScott M. Lundberg and Su -In Lee. \"A unified approach to interpreting model predictions.\" \n\nNIPS'17: Proceedings of the 31st International Conference on Neural Information \n\nProcessing Systems, December 4, 2017, 4768 -4777. \n\nDylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. “Fooling \n\nLIME and SHAP: Adversarial Attacks on Post Hoc Explanation Methods.” AIES '20: \n\nProceedings of the AAAI/ACM Conference on AI, Ethics, and Society, February 7, 2020, 180 –\n\n86. \n\nDavid Alvarez -Melis and Tommi S. Jaakkola. \"Towards robust interpretability with self -\n\nexplaining neural networks.\" NIPS'18: Proceedings of the 32nd International Conference on \n\nNeural Information Processing Systems, December 3, 2018, 7786 -7795. \n\nFinRegLab, Laura Biattner, and Jann Spiess. \"Machine Learning Explainability & Fairness: \n\nInsights from Consumer Lending.\" FinRegLab, April 2022. 118 of 142 \n\nMiguel Ferreira, Muhammad Bilal Zafar, and Krishna P. Gummadi. \"The Case for Temporal \n\nTransparency: Detecting Policy Change Events in Black -Box Decision Making Systems.\" \n\narXiv preprint, submitted October 31, 2016. \n\nHimabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. \"Interpretable & \n\nExplorable Approximations of Black Box Models.\" arXiv preprint, July 4, 2017. \n\nSoftware Resources \n\n• SHAP \n\n• LIME \n\n• Interpret \n\n• PiML \n\n• Iml \n\n• Dalex \n\n# MEASURE 2.10 \n\nPrivacy risk of the AI system – as identified in the MAP function – is examined and \n\ndocumented. \n\nAbout \n\nPrivacy refers generally to the norms and practices that help to safeguard human autonomy, \n\nidentity, and dignity. These norms and practices typically address freedom from intrusion, \n\nlimiting observation, or individuals’ agency to consent to disclosure or control of facets of \n\ntheir identities (e.g., body, data, reputation). \n\nPrivacy values such as anonymity, confidentiality, and control generally should guide \n\nchoices for AI system design, development, and deployment. Privacy -related risks may \n\ninfluence security, bias, and transparency and come with tradeoffs with these other \n\ncharacteristics. Like safety and security, specific technical features of an AI system may \n\npromote or reduce privacy. AI systems can also present new risks to privacy by allowing \n\ninference to identify individuals or previously private information about individuals. \n\nPrivacy -enhancing technologies (“PETs”) for AI, as well as data minimizing methods such as \n\nde -identification and aggregation for certain model outputs, can support design for privacy -\n\nenhanced AI systems. Under certain conditions such as data sparsity, privacy enhancing \n\ntechniques can result in a loss in accuracy, impacting decisions about fairness and other \n\nvalues in certain domains. \n\nSuggested Actions \n\n• Specify privacy -related values, frameworks, and attributes that are applicable in the \n\ncontext of use through direct engagement with end users and potentially impacted \n\ngroups and communities. \n\n• Document collection, use, management, and disclosure of personally sensitive \n\ninformation in datasets, in accordance with privacy and data governance policies 119 of 142 \n\n• Quantify privacy -level data aspects such as the ability to identify individuals or groups \n\n(e.g. k -anonymity metrics, l -diversity, t -closeness). \n\n• Establish and document protocols (authorization, duration, type) and access controls \n\nfor training sets or production data containing personally sensitive information, in \n\naccordance with privacy and data governance policies. \n\n• Monitor internal queries to production data for detecting patterns that isolate personal \n\nrecords. \n\n• Monitor PSI disclosures and inference of sensitive or legally protected attributes \n\n• Assess the risk of manipulation from overly customized content. Evaluate \n\ninformation presented to representative users at various points along axes of \n\ndifference between individuals (e.g. individuals of different ages, genders, races, \n\npolitical affiliation, etc.). \n\n• Use privacy -enhancing techniques such as differential privacy, when publicly sharing \n\ndataset information. \n\n• Collaborate with privacy experts, AI end users and operators, and other domain experts \n\nto determine optimal differential privacy metrics within contexts of use. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Did your organization implement accountability -based practices in data management \n\nand protection (e.g. the PDPA and OECD Privacy Principles)? \n\n• What assessments has the entity conducted on data security and privacy impacts \n\nassociated with the AI system? \n\n• Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\n• Does the dataset contain information that might be considered sensitive or confidential? \n\n(e.g., personally identifying information) \n\n• If it relates to people, could this dataset expose people to harm or legal action? (e.g., \n\nfinancial, social or otherwise) What was done to mitigate or reduce the potential for \n\nharm? \n\nAI Transparency Resources \n\n• WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. ( \n\n• Datasheets for Datasets. \n\nReferences \n\nKaitlin R. Boeckl and Naomi B. Lefkovitz. \"NIST Privacy Framework: A Tool for Improving \n\nPrivacy Through Enterprise Risk Management, Version 1.0.\" National Institute of Standards \n\nand Technology (NIST), January 16, 2020. 120 of 142 \n\nLatanya Sweeney. “K -Anonymity: A Model for Protecting Privacy.” International Journal of \n\nUncertainty, Fuzziness and Knowledge -Based Systems 10, no. 5 (2002): 557 –70. \n\nAshwin Machanavajjhala, Johannes Gehrke, Daniel Kifer, and Muthuramakrishnan \n\nVenkitasubramaniam. “L -Diversity: Privacy beyond K -Anonymity.” 22nd International \n\nConference on Data Engineering (ICDE'06), 2006. \n\nNinghui Li, Tiancheng Li, and Suresh Venkatasubramanian. \"CERIAS Tech Report 2007 -78 t -\n\nCloseness: Privacy Beyond k -Anonymity and -Diversity.\" Center for Education and Research, \n\nInformation Assurance and Security, Purdue University, 2001. \n\nJ. Domingo -Ferrer and J. Soria -Comas. \"From t -closeness to differential privacy and vice \n\nversa in data anonymization.\" arXiv preprint, submitted December 21, 2015. \n\nJoseph Near, David Darais, and Kaitlin Boeckly. \"Differential Privacy for Privacy -Preserving \n\nData Analysis: An Introduction to our Blog Series.\" National Institute of Standards and \n\nTechnology (NIST), July 27, 2020. \n\nCynthia Dwork. “Differential Privacy.” Automata, Languages and Programming, 2006, 1 –12. \n\nZhanglong Ji, Zachary C. Lipton, and Charles Elkan. \"Differential Privacy and Machine \n\nLearning: a Survey and Review.\" arXiv preprint, submitted December 24,2014. \n\nMichael B. Hawes. \"Implementing Differential Privacy: Seven Lessons From the 2020 United \n\nStates Census.\" Harvard Data Science Review 2, no. 2 (2020). \n\nHarvard University Privacy Tools Project. “Differential Privacy.” Harvard University, n.d. \n\nJohn M. Abowd, Robert Ashmead, Ryan Cumings -Menon, Simson Garfinkel, Micah Heineck, \n\nChristine Heiss, Robert Johns, Daniel Kifer, Philip Leclerc, Ashwin Machanavajjhala, Brett \n\nMoran, William Matthew Spence Sexton and Pavel Zhuravlev. \"The 2020 Census Disclosure \n\nAvoidance System TopDown Algorithm.\" United States Census Bureau, April 7, 2022. \n\nNicolas Papernot and Abhradeep Guha Thakurta. \"How to deploy machine learning with \n\ndifferential privacy.\" National Institute of Standards and Technology (NIST), December 21, \n\n2021. \n\nClaire McKay Bowen. \"Utility Metrics for Differential Privacy: No One -Size -Fits -All.\" National \n\nInstitute of Standards and Technology (NIST), November 29, 2021. \n\nHelen Nissenbaum. \"Contextual Integrity Up and Down the Data Food Chain.\" Theoretical \n\nInquiries in Law 20, L. 221 (2019): 221 -256. \n\nSebastian Benthall, Seda Gürses, and Helen Nissenbaum. “Contextual Integrity through the \n\nLens of Computer Science.” Foundations and Trends in Privacy and Security 2, no. 1 \n\n(December 22, 2017): 1 –69. 121 of 142 \n\nJenifer Sunrise Winter and Elizabeth Davidson. “Big Data Governance of Personal Health \n\nInformation and Challenges to Contextual Integrity.” The Information Society: An \n\nInternational Journal 35, no. 1 (2019): 36 –51. \n\n# MEASURE 2.11 \n\nFairness and bias – as identified in the MAP function – is evaluated and results are \n\ndocumented. \n\nAbout \n\nFairness in AI includes concerns for equality and equity by addressing issues such as \n\nharmful bias and discrimination. Standards of fairness can be complex and difficult to define \n\nbecause perceptions of fairness differ among cultures and may shift depending on \n\napplication. Organizations’ risk management efforts will be enhanced by recognizing and \n\nconsidering these differences. Systems in which harmful biases are mitigated are not \n\nnecessarily fair. For example, systems in which predictions are somewhat balanced across \n\ndemographic groups may still be inaccessible to individuals with disabilities or affected by \n\nthe digital divide or may exacerbate existing disparities or systemic biases. \n\nBias is broader than demographic balance and data representativeness. NIST has identified \n\nthree major categories of AI bias to be considered and managed: systemic, computational \n\nand statistical, and human -cognitive. Each of these can occur in the absence of prejudice, \n\npartiality, or discriminatory intent. \n\n• Systemic bias can be present in AI datasets, the organizational norms, practices, and \n\nprocesses across the AI lifecycle, and the broader society that uses AI systems. \n\n• Computational and statistical biases can be present in AI datasets and algorithmic \n\nprocesses, and often stem from systematic errors due to non -representative samples. \n\n• Human -cognitive biases relate to how an individual or group perceives AI system \n\ninformation to make a decision or fill in missing information, or how humans think \n\nabout purposes and functions of an AI system. Human -cognitive biases are omnipresent \n\nin decision -making processes across the AI lifecycle and system use, including the \n\ndesign, implementation, operation, and maintenance of AI. \n\nBias exists in many forms and can become ingrained in the automated systems that help \n\nmake decisions about our lives. While bias is not always a negative phenomenon, AI systems \n\ncan potentially increase the speed and scale of biases and perpetuate and amplify harms to \n\nindividuals, groups, communities, organizations, and society. \n\nSuggested Actions \n\n• Conduct fairness assessments to manage computational and statistical forms of bias \n\nwhich include the following steps: \n\n• Identify types of harms, including allocational, representational, quality of service, \n\nstereotyping, or erasure \n\n• Identify across, within, and intersecting groups that might be harmed 122 of 142 \n\n• Quantify harms using both a general fairness metric, if appropriate (e.g. \n\ndemographic parity, equalized odds, equal opportunity, statistical hypothesis tests), \n\nand custom, context -specific metrics developed in collaboration with affected \n\ncommunities \n\n• Analyze quantified harms for contextually significant differences across groups, \n\nwithin groups, and among intersecting groups \n\n• Refine identification of within -group and intersectional group disparities. \n\n• Evaluate underlying data distributions and employ sensitivity analysis during \n\nthe analysis of quantified harms. \n\n• Evaluate quality metrics including false positive rates and false negative rates. \n\n• Consider biases affecting small groups, within -group or intersectional \n\ncommunities, or single individuals. \n\n• Understand and consider sources of bias in training and TEVV data: \n\n• Differences in distributions of outcomes across and within groups, including \n\nintersecting groups. \n\n• Completeness, representativeness and balance of data sources. \n\n• Identify input data features that may serve as proxies for demographic group \n\nmembership (i.e., credit score, ZIP code) or otherwise give rise to emergent bias \n\nwithin AI systems. \n\n• Forms of systemic bias in images, text (or word embeddings), audio or other \n\ncomplex or unstructured data. \n\n• Leverage impact assessments to identify and classify system impacts and harms to end \n\nusers, other individuals, and groups with input from potentially impacted communities. \n\n• Identify the classes of individuals, groups, or environmental ecosystems which might be \n\nimpacted through direct engagement with potentially impacted communities. \n\n• Evaluate systems in regards to disability inclusion, including consideration of disability \n\nstatus in bias testing, and discriminatory screen out processes that may arise from non -\n\ninclusive design or deployment decisions. \n\n• Develop objective functions in consideration of systemic biases, in -group/out -group \n\ndynamics. \n\n• Use context -specific fairness metrics to examine how system performance varies across \n\ngroups, within groups, and/or for intersecting groups. Metrics may include statistical \n\nparity, error -rate equality, statistical parity difference, equal opportunity difference, \n\naverage absolute odds difference, standardized mean difference, percentage point \n\ndifferences. \n\n• Customize fairness metrics to specific context of use to examine how system \n\nperformance and potential harms vary within contextual norms. \n\n• Define acceptable levels of difference in performance in accordance with established \n\norganizational governance policies, business requirements, regulatory compliance, legal \n\nframeworks, and ethical standards within the context of use 123 of 142 \n\n• Define the actions to be taken if disparity levels rise above acceptable levels. \n\n• Identify groups within the expected population that may require disaggregated analysis, \n\nin collaboration with impacted communities. \n\n• Leverage experts with knowledge in the specific context of use to investigate substantial \n\nmeasurement differences and identify root causes for those differences. \n\n• Monitor system outputs for performance or bias issues that exceed established \n\ntolerance levels. \n\n• Ensure periodic model updates; test and recalibrate with updated and more \n\nrepresentative data to stay within acceptable levels of difference. \n\n• Apply pre -processing data transformations to address factors related to demographic \n\nbalance and data representativeness. \n\n• Apply in -processing to balance model performance quality with bias considerations. \n\n• Apply post -processing mathematical/computational techniques to model results in \n\nclose collaboration with impact assessors, socio -technical experts, and other AI actors \n\nwith expertise in the context of use. \n\n• Apply model selection approaches with transparent and deliberate consideration of bias \n\nmanagement and other trustworthy characteristics. \n\n• Collect and share information about differences in outcomes for the identified groups. \n\n• Consider mediations to mitigate differences, especially those that can be traced to past \n\npatterns of unfair or biased human decision making. \n\n• Utilize human -centered design practices to generate deeper focus on societal impacts \n\nand counter human -cognitive biases within the AI lifecycle. \n\n• Evaluate practices along the lifecycle to identify potential sources of human -cognitive \n\nbias such as availability, observational, and confirmation bias, and to make implicit \n\ndecision making processes more explicit and open to investigation. \n\n• Work with human factors experts to evaluate biases in the presentation of system \n\noutput to end users, operators and practitioners. \n\n• Utilize processes to enhance contextual awareness, such as diverse internal staff and \n\nstakeholder engagement. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\n• If it relates to people, does it unfairly advantage or disadvantage a particular social \n\ngroup? In what ways? How was this mitigated? \n\n• Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\n• How has the entity identified and mitigated potential impacts of bias in the data, \n\nincluding inequitable or discriminatory outcomes? \n\n• To what extent has the entity identified and mitigated potential bias —statistical, \n\ncontextual, and historical —in the data? 124 of 142 \n\n• Were adversarial machine learning approaches considered or used for measuring bias \n\n(e.g.: prompt engineering, adversarial models) \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. \n\n• Datasheets for Datasets. \n\nReferences \n\nAli Hasan, Shea Brown, Jovana Davidovic, Benjamin Lange, and Mitt Regan. “Algorithmic \n\nBias and Risk Assessments: Lessons from Practice.” Digital Society 1 (2022). \n\nRichard N. Landers and Tara S. Behrend. “Auditing the AI Auditors: A Framework for \n\nEvaluating Fairness and Bias in High Stakes AI Predictive Models.” American Psychologist \n\n78, no. 1 (2023): 36 –49. \n\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. “A \n\nSurvey on Bias and Fairness in Machine Learning.” ACM Computing Surveys 54, no. 6 (July \n\n2021): 1 –35. \n\nMichele Loi and Christoph Heitz. “Is Calibration a Fairness Requirement?” FAccT '22: 2022 \n\nACM Conference on Fairness, Accountability, and Transparency, June 2022, 2026 –34. \n\nShea Brown, Ryan Carrier, Merve Hickok, and Adam Leon Smith. “Bias Mitigation in Data \n\nSets.” SocArXiv, July 8, 2021. \n\nReva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, Andrew Burt, and Patrick Hall. \n\n\"NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in \n\nArtificial Intelligence.\" National Institute of Standards and Technology (NIST), 2022. \n\nMicrosoft Research. “AI Fairness Checklist.” Microsoft, February 7, 2022. \n\nSamir Passi and Solon Barocas. “Problem Formulation and Fairness.” FAT* '19: Proceedings \n\nof the Conference on Fairness, Accountability, and Transparency, January 2019, 39 –48. \n\nJade S. Franklin, Karan Bhanot, Mohamed Ghalwash, Kristin P. Bennett, Jamie McCusker, and \n\nDeborah L. McGuinness. “An Ontology for Fairness Metrics.” AIES '22: Proceedings of the \n\n2022 AAAI/ACM Conference on AI, Ethics, and Society, July 2022, 265 –75. \n\nZhang, B., Lemoine, B., & Mitchell, M. (2018). Mitigating Unwanted Biases with Adversarial \n\nLearning. Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. \n\nhttps://arxiv.org/pdf/1801.07593.pdf 125 of 142 \n\nGanguli, D., et al. (2023). The Capacity for Moral Self -Correction in Large Language Models. \n\narXiv. https://arxiv.org/abs/2302.07459 \n\nArvind Narayanan. “Tl;DS - 21 Fairness Definition and Their Politics by Arvind Narayanan.” \n\nDora's world, July 19, 2019. \n\nBen Green. “Escaping the Impossibility of Fairness: From Formal to Substantive Algorithmic \n\nFairness.” Philosophy and Technology 35, no. 90 (October 8, 2022). \n\nAlexandra Chouldechova. “Fair Prediction with Disparate Impact: A Study of Bias in \n\nRecidivism Prediction Instruments.” Big Data 5, no. 2 (June 1, 2017): 153 –63. \n\nSina Fazelpour and Zachary C. Lipton. “Algorithmic Fairness from a Non -Ideal Perspective.” \n\nAIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, February 7, \n\n2020, 57 –63. \n\nHemank Lamba, Kit T. Rodolfa, and Rayid Ghani. “An Empirical Comparison of Bias \n\nReduction Methods on Real -World Problems in High -Stakes Policy Settings.” ACM SIGKDD \n\nExplorations Newsletter 23, no. 1 (May 29, 2021): 69 –85. \n\nISO. “ISO/IEC TR 24027:2021 Information technology — Artificial intelligence (AI) — Bias \n\nin AI systems and AI aided decision making.” ISO Standards, November 2021. \n\nShari Trewin. \"AI Fairness for People with Disabilities: Point of View.\" arXiv preprint, \n\nsubmitted November 26, 2018. \n\nMathWorks. “Explore Fairness Metrics for Credit Scoring Model.” MATLAB & Simulink, \n\n2023. \n\nAbigail Z. Jacobs and Hanna Wallach. “Measurement and Fairness.” FAccT '21: Proceedings \n\nof the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, \n\n375 –85. \n\nTolga Bolukbasi, Kai -Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. \n\n\"Quantifying and Reducing Stereotypes in Word Embeddings.\" arXiv preprint, submitted \n\nJune 20, 2016. \n\nAylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. “Semantics Derived Automatically \n\nfrom Language Corpora Contain Human -Like Biases.” Science 356, no. 6334 (April 14, \n\n2017): 183 –86. \n\nSina Fazelpour and Maria De -Arteaga. “Diversity in Sociotechnical Machine Learning \n\nSystems.” Big Data and Society 9, no. 1 (2022). \n\nFairlearn. “Fairness in Machine Learning.” Fairlearn 0.8.0 Documentation, n.d. \n\nSafiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. New \n\nYork, NY: New York University Press, 2018. 126 of 142 \n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. “Dissecting \n\nRacial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366, no. \n\n6464 (October 25, 2019): 447 –53. \n\nAlekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. \"A \n\nReductions Approach to Fair Classification.\" arXiv preprint, submitted July 16, 2018. \n\nMoritz Hardt, Eric Price, and Nathan Srebro. \"Equality of Opportunity in Supervised \n\nLearning.\" arXiv preprint, submitted October 7, 2016. \n\nAlekh Agarwal, Miroslav Dudik, Zhiwei Steven Wu. \"Fair Regression: Quantitative \n\nDefinitions and Reduction -Based Algorithms.\" Proceedings of the 36th International \n\nConference on Machine Learning, PMLR 97:120 -129, 2019. \n\nAndrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet \n\nVertesi. “Fairness and Abstraction in Sociotechnical Systems.” FAT* '19: Proceedings of the \n\nConference on Fairness, Accountability, and Transparency, January 29, 2019, 59 –68. \n\nMatthew Kay, Cynthia Matuszek, and Sean A. Munson. “Unequal Representation and Gender \n\nStereotypes in Image Search Results for Occupations.” CHI '15: Proceedings of the 33rd \n\nAnnual ACM Conference on Human Factors in Computing Systems, April 18, 2015, 3819 –28. \n\nSoftware Resources \n\n• aequitas \n\n- AI Fairness 360: \n\n• Python \n\n• R\n\n• algofairness \n\n• fairlearn \n\n• fairml \n\n• fairmodels \n\n• fairness \n\n• solas -ai -disparity \n\n• tensorflow/fairness -indicators \n\n• Themis \n\n# MEASURE 2.12 \n\nEnvironmental impact and sustainability of AI model training and management activities –\n\nas identified in the MAP function – are assessed and documented. \n\nAbout \n\nLarge -scale, high -performance computational resources used by AI systems for training and \n\noperation can contribute to environmental impacts. Direct negative impacts to the \n\nenvironment from these processes are related to energy consumption, water consumption, 127 of 142 \n\nand greenhouse gas (GHG) emissions. The OECD has identified metrics for each type of \n\nnegative direct impact. \n\nIndirect negative impacts to the environment reflect the complexity of interactions between \n\nhuman behavior, socio -economic systems, and the environment and can include induced \n\nconsumption and “rebound effects”, where efficiency gains are offset by accelerated \n\nresource consumption. \n\nOther AI related environmental impacts can arise from the production of computational \n\nequipment and networks (e.g. mining and extraction of raw materials), transporting \n\nhardware, and electronic waste recycling or disposal. \n\nSuggested Actions \n\n• Include environmental impact indicators in AI system design and development plans, \n\nincluding reducing consumption and improving efficiencies. \n\n• Identify and implement key indicators of AI system energy and water consumption and \n\nefficiency, and/or GHG emissions. \n\n• Establish measurable baselines for sustainable AI system operation in accordance with \n\norganizational policies, regulatory compliance, legal frameworks, and environmental \n\nprotection and sustainability norms. \n\n• Assess tradeoffs between AI system performance and sustainable operations in \n\naccordance with organizational principles and policies, regulatory compliance, legal \n\nframeworks, and environmental protection and sustainability norms. \n\n• Identify and establish acceptable resource consumption and efficiency, and GHG \n\nemissions levels, along with actions to be taken if indicators rise above acceptable \n\nlevels. \n\n• Estimate AI system emissions levels throughout the AI lifecycle via carbon calculators or \n\nsimilar process. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Are greenhouse gas emissions, and energy and water consumption and efficiency \n\ntracked within the organization? \n\n• Are deployed AI systems evaluated for potential upstream and downstream \n\nenvironmental impacts (e.g., increased consumption, increased emissions, etc.)? \n\n• Could deployed AI systems cause environmental incidents, e.g., air or water pollution \n\nincidents, toxic spills, fires or explosions? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• Datasheets for Datasets. 128 of 142 \n\nReferences \n\nOrganisation for Economic Co -operation and Development (OECD). \"Measuring the \n\nenvironmental impacts of artificial intelligence compute and applications: The AI footprint.” \n\nOECD Digital Economy Papers, No. 341, OECD Publishing, Paris. \n\nVictor Schmidt, Alexandra Luccioni, Alexandre Lacoste, and Thomas Dandres. “Machine \n\nLearning CO2 Impact Calculator.” ML CO2 Impact, n.d. \n\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. \"Quantifying \n\nthe Carbon Emissions of Machine Learning.\" arXiv preprint, submitted November 4, 2019. \n\nMatthew Hutson. “Measuring AI’s Carbon Footprint: New Tools Track and Reduce \n\nEmissions from Machine Learning.” IEEE Spectrum, November 22, 2022. \n\nAssociation for Computing Machinery (ACM). \"TechBriefs: Computing and Climate Change.\" \n\nACM Technology Policy Council, November 2021. \n\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. “Green AI.” Communications of \n\nthe ACM 63, no. 12 (December 2020): 54 –63. \n\n# MEASURE 2.13 \n\nEffectiveness of the employed TEVV metrics and processes in the MEASURE function are \n\nevaluated and documented. \n\nAbout \n\nThe development of metrics is a process often considered to be objective but, as a human \n\nand organization driven endeavor, can reflect implicit and systemic biases, and may \n\ninadvertently reflect factors unrelated to the target function. Measurement approaches can \n\nbe oversimplified, gamed, lack critical nuance, become used and relied upon in unexpected \n\nways, fail to account for differences in affected groups and contexts. \n\nRevisiting the metrics chosen in Measure 2.1 through 2.12 in a process of continual \n\nimprovement can help AI actors to evaluate and document metric effectiveness and make \n\nnecessary course corrections. \n\nSuggested Actions \n\n• Review selected system metrics and associated TEVV processes to determine if they are \n\nable to sustain system improvements, including the identification and removal of errors. \n\n• Regularly evaluate system metrics for utility, and consider descriptive approaches in \n\nplace of overly complex methods. \n\n• Review selected system metrics for acceptability within the end user and impacted \n\ncommunity of interest. \n\n• Assess effectiveness of metrics for identifying and measuring risks. 129 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent does the system/entity consistently measure progress towards stated \n\ngoals and objectives? \n\n• Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\n• What corrective actions has the entity taken to enhance the quality, accuracy, reliability, \n\nand representativeness of the data? \n\n• To what extent are the model outputs consistent with the entity’s values and principles \n\nto foster public trust and equity? \n\n• How will the accuracy or appropriate performance metrics be assessed? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nArvind Narayanan. \"The limits of the quantitative approach to discrimination.\" 2022 James \n\nBaldwin lecture, Princeton University, October 11, 2022. \n\nDevansh Saxena, Karla Badillo -Urquiola, Pamela J. Wisniewski, and Shion Guha. “A Human -\n\nCentered Review of Algorithms Used within the U.S. Child Welfare System.” CHI ‘20: \n\nProceedings of the 2020 CHI Conference on Human Factors in Computing Systems, April 23, \n\n2020, 1 –15. \n\nRachel Thomas and David Uminsky. “Reliance on Metrics Is a Fundamental Challenge for \n\nAI.” Patterns 3, no. 5 (May 13, 2022): 100476. \n\nMomin M. Malik. \"A Hierarchy of Limitations in Machine Learning.\" arXiv preprint, \n\nsubmitted February 29, 2020. \n\n# MEASURE 3.1 \n\nApproaches, personnel, and documentation are in place to regularly identify and track \n\nexisting, unanticipated, and emergent AI risks based on factors such as intended and actual \n\nperformance in deployed contexts. \n\nAbout \n\nFor trustworthy AI systems, regular system monitoring is carried out in accordance with \n\norganizational governance policies, AI actor roles and responsibilities, and within a culture \n\nof continual improvement. If and when emergent or complex risks arise, it may be \n\nnecessary to adapt internal risk management procedures, such as regular monitoring, to \n\nstay on course. Documentation, resources, and training are part of an overall strategy to 130 of 142 \n\nsupport AI actors as they investigate and respond to AI system errors, incidents or negative \n\nimpacts. \n\nSuggested Actions \n\n• Compare AI system risks with: \n\n• simpler or traditional models \n\n• human baseline performance \n\n• other manual performance benchmarks \n\n• Compare end user and community feedback about deployed AI systems to internal \n\nmeasures of system performance. \n\n• Assess effectiveness of metrics for identifying and measuring emergent risks. \n\n• Measure error response times and track response quality. \n\n• Elicit and track feedback from AI actors in user support roles about the type of metrics, \n\nexplanations and other system information required for fulsome resolution of system \n\nissues. Consider: \n\n• Instances where explanations are insufficient for investigating possible error \n\nsources or identifying responses. \n\n• System metrics, including system logs and explanations, for identifying and \n\ndiagnosing sources of system error. \n\n• Elicit and track feedback from AI actors in incident response and support roles about \n\nthe adequacy of staffing and resources to perform their duties in an effective and timely \n\nmanner. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\n• To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\n• What metrics has the entity developed to measure performance of the AI system, \n\nincluding error logging? \n\n• To what extent do the metrics provide accurate and useful measure of performance? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF Companion to the Model AI Governance Framework – Implementation and Self -\n\nAssessment Guide for Organizations 131 of 142 \n\nReferences \n\nISO. \"ISO 9241 -210:2019 Ergonomics of human -system interaction — Part 210: Human -\n\ncentred design for interactive systems.\" 2nd ed. ISO Standards, July 2019. \n\nLarysa Visengeriyeva, et al. “Awesome MLOps.“ GitHub. \n\n# MEASURE 3.2 \n\nRisk tracking approaches are considered for settings where AI risks are difficult to assess \n\nusing currently available measurement techniques or where metrics are not yet available. \n\nAbout \n\nRisks identified in the Map function may be complex, emerge over time, or difficult to \n\nmeasure. Systematic methods for risk tracking, including novel measurement approaches, \n\ncan be established as part of regular monitoring and improvement processes. \n\nSuggested Actions \n\n• Establish processes for tracking emergent risks that may not be measurable with \n\ncurrent approaches. Some processes may include: \n\n• Recourse mechanisms for faulty AI system outputs. \n\n• Bug bounties. \n\n• Human -centered design approaches. \n\n• User -interaction and experience research. \n\n• Participatory stakeholder engagement with affected or potentially impacted \n\nindividuals and communities. \n\n• Identify AI actors responsible for tracking emergent risks and inventory methods. \n\n• Determine and document the rate of occurrence and severity level for complex or \n\ndifficult -to -measure risks when: \n\n• Prioritizing new measurement approaches for deployment tasks. \n\n• Allocating AI system risk management resources. \n\n• Evaluating AI system improvements. \n\n• Making go/no -go decisions for subsequent system iterations. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Who is ultimately responsible for the decisions of the AI and is this person aware of the \n\nintended uses and limitations of the analytic? \n\n• Who will be responsible for maintaining, re -verifying, monitoring, and updating this AI \n\nonce deployed? \n\n• To what extent does the entity communicate its AI strategic goals and objectives to the \n\ncommunity of stakeholders? 132 of 142 \n\n• Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\n• If anyone believes that the AI no longer meets this ethical framework, who will be \n\nresponsible for receiving the concern and as appropriate investigating and remediating \n\nthe issue? Do they have authority to modify, limit, or stop the use of the AI? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nISO. \"ISO 9241 -210:2019 Ergonomics of human -system interaction — Part 210: Human -\n\ncentred design for interactive systems.\" 2nd ed. ISO Standards, July 2019. \n\nMark C. Paulk, Bill Curtis, Mary Beth Chrissis, and Charles V. Weber. “Capability Maturity \n\nModel, Version 1.1.” IEEE Software 10, no. 4 (1993): 18 –27. \n\nJeff Patton, Peter Economy, Martin Fowler, Alan Cooper, and Marty Cagan. User Story \n\nMapping: Discover the Whole Story, Build the Right Product. O'Reilly, 2014. \n\nRumman Chowdhury and Jutta Williams. \"Introducing Twitter’s first algorithmic bias \n\nbounty challenge.\" Twitter Engineering Blog, July 30, 2021. \n\nHackerOne. \"Twitter Algorithmic Bias.\" HackerOne, August 8, 2021. \n\nJosh Kenway, Camille François, Sasha Costanza -Chock, Inioluwa Deborah Raji, and Joy \n\nBuolamwini. \"Bug Bounties for Algorithmic Harms?\" Algorithmic Justice League, January \n\n2022. \n\nMicrosoft. “Community Jury.” Microsoft Learn's Azure Application Architecture Guide, 2023. \n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. \"Overcoming Failures of \n\nImagination in AI Infused System Development and Deployment.\" arXiv preprint, submitted \n\nDecember 10, 2020. \n\n# MEASURE 3.3 \n\nFeedback processes for end users and impacted communities to report problems and \n\nappeal system outcomes are established and integrated into AI system evaluation metrics. \n\nAbout \n\nAssessing impact is a two -way effort. Many AI system outcomes and impacts may not be \n\nvisible or recognizable to AI actors across the development and deployment dimensions of \n\nthe AI lifecycle, and may require direct feedback about system outcomes from the \n\nperspective of end users and impacted groups. 133 of 142 \n\nFeedback can be collected indirectly, via systems that are mechanized to collect errors and \n\nother feedback from end users and operators \n\nMetrics and insights developed in this sub -category feed into Manage 4.1 and 4.2. \n\nSuggested Actions \n\n• Measure efficacy of end user and operator error reporting processes. \n\n• Categorize and analyze type and rate of end user appeal requests and results. \n\n• Measure feedback activity participation rates and awareness of feedback activity \n\navailability. \n\n• Utilize feedback to analyze measurement approaches and determine subsequent \n\ncourses of action. \n\n• Evaluate measurement approaches to determine efficacy for enhancing organizational \n\nunderstanding of real world impacts. \n\n• Analyze end user and community feedback in close collaboration with domain experts. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\n• Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? \n\n• How easily accessible and current is the information available to external stakeholders? \n\n• What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• WEF Companion to the Model AI Governance Framework – Implementation and Self -\n\nAssessment Guide for Organizations \n\nReferences \n\nSasha Costanza -Chock. Design Justice: Community -Led Practices to Build the Worlds We \n\nNeed. Cambridge: The MIT Press, 2020. \n\nDavid G. Robinson. Voices in the Code: A Story About People, Their Values, and the \n\nAlgorithm They Made. New York: Russell Sage Foundation, 2022. \n\nFernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder \n\nParticipation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted \n\nNovember 1, 2021. 134 of 142 \n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. “Human -\n\nCentered Design of Artificial Intelligence.” In Handbook of Human Factors and Ergonomics, \n\nedited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085 –1106. John Wiley & \n\nSons, 2021. \n\nBen Shneiderman. Human -Centered AI. Oxford: Oxford University Press, 2022 \n\nBatya Friedman, David G. Hendry, and Alan Borning. “A Survey of Value Sensitive Design \n\nMethods.” Foundations and Trends in Human -Computer Interaction 11, no. 2 (November \n\n22, 2017): 63 –125. \n\nBatya Friedman, Peter H. Kahn, Jr., and Alan Borning. \"Value Sensitive Design: Theory and \n\nMethods.\" University of Washington Department of Computer Science & Engineering \n\nTechnical Report 02 -12 -01, December 2002. \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. \n\n“Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.” SSRN, \n\nJuly 8, 2021. \n\nAlexandra Reeve Givens, and Meredith Ringel Morris. “Centering Disability Perspectives in \n\nAlgorithmic Fairness, Accountability, & Transparency.” FAT* '20: Proceedings of the 2020 \n\nConference on Fairness, Accountability, and Transparency, January 27, 2020, 684 -84. \n\n# MEASURE 4.1 \n\nMeasurement approaches for identifying AI risks are connected to deployment context(s) \n\nand informed through consultation with domain experts and other end users. Approaches \n\nare documented. \n\nAbout \n\nAI Actors carrying out TEVV tasks may have difficulty evaluating impacts within the system \n\ncontext of use. AI system risks and impacts are often best described by end users and others \n\nwho may be affected by output and subsequent decisions. AI Actors can elicit feedback from \n\nimpacted individuals and communities via participatory engagement processes established \n\nin Govern 5.1 and 5.2, and carried out in Map 1.6, 5.1, and 5.2. \n\nActivities described in the Measure function enable AI actors to evaluate feedback from \n\nimpacted individuals and communities. To increase awareness of insights, feedback can be \n\nevaluated in close collaboration with AI actors responsible for impact assessment, human -\n\nfactors, and governance and oversight tasks, as well as with other socio -technical domain \n\nexperts and researchers. To gain broader expertise for interpreting evaluation outcomes, \n\norganizations may consider collaborating with advocacy groups and civil society \n\norganizations. \n\nInsights based on this type of analysis can inform TEVV -based decisions about metrics and \n\nrelated courses of action. 135 of 142 \n\nSuggested Actions \n\n• Support mechanisms for capturing feedback from system end users (including domain \n\nexperts, operators, and practitioners). Successful approaches are: \n\n• conducted in settings where end users are able to openly share their doubts and \n\ninsights about AI system output, and in connection to their specific context of use \n\n(including setting and task -specific lines of inquiry) \n\n• developed and implemented by human -factors and socio -technical domain experts \n\nand researchers \n\n• designed to ensure control of interviewer and end user subjectivity and biases \n\n• Identify and document approaches \n\n• for evaluating and integrating elicited feedback from system end users \n\n• in collaboration with human -factors and socio -technical domain experts, \n\n• to actively inform a process of continual improvement. \n\n• Evaluate feedback from end users alongside evaluated feedback from impacted \n\ncommunities (MEASURE 3.3). \n\n• Utilize end user feedback to investigate how selected metrics and measurement \n\napproaches interact with organizational and operational contexts. \n\n• Analyze and document system -internal measurement processes in comparison to \n\ncollected end user feedback. \n\n• Identify and implement approaches to measure effectiveness and satisfaction with end \n\nuser elicitation techniques, and document results. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? \n\n• How will user and peer engagement be integrated into the model development process \n\nand periodic performance review once deployed? \n\n• To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\n• To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\n• WEF Companion to the Model AI Governance Framework – Implementation and Self -\n\nAssessment Guide for Organizations 136 of 142 \n\nReferences \n\nBatya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with \n\nMoral Imagination. Cambridge, MA: The MIT Press, 2019. \n\nBatya Friedman, David G. Hendry, and Alan Borning. “A Survey of Value Sensitive Design \n\nMethods.” Foundations and Trends in Human -Computer Interaction 11, no. 2 (November \n\n22, 2017): 63 –125. \n\nSteven Umbrello, and Ibo van de Poel. “Mapping Value Sensitive Design onto AI for Social \n\nGood Principles.” AI and Ethics 1, no. 3 (February 1, 2021): 283 –96. \n\nKaren Boyd. “Designing Up with Value -Sensitive Design: Building a Field Guide for Ethical \n\nML Development.” FAccT '22: 2022 ACM Conference on Fairness, Accountability, and \n\nTransparency, June 20, 2022, 2069 –82. \n\nJanet Davis and Lisa P. Nathan. “Value Sensitive Design: Applications, Adaptations, and \n\nCritiques.” In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van \n\nden Hoven, Pieter E. Vermaas, and Ibo van de Poel, January 1, 2015, 11 –40. \n\nBen Shneiderman. Human -Centered AI. Oxford: Oxford University Press, 2022. \n\nShneiderman, Ben. “Human -Centered AI.” Issues in Science and Technology 37, no. 2 \n\n(2021): 56 –61. \n\nShneiderman, Ben. “Tutorial: Human -Centered AI: Reliable, Safe and Trustworthy.” IUI '21 \n\nCompanion: 26th International Conference on Intelligent User Interfaces - Companion, April \n\n14, 2021, 7 –8. \n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. “Human -\n\nCentered Design of Artificial Intelligence.” In Handbook of Human Factors and Ergonomics, \n\nedited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085 –1106. John Wiley & \n\nSons, 2021. \n\nCaitlin Thompson. “Who's Homeless Enough for Housing? In San Francisco, an Algorithm \n\nDecides.” Coda, September 21, 2021. \n\nJohn Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. “Algorithmic Decision -\n\nMaking and the Control Problem.” Minds and Machines 29, no. 4 (December 11, 2019): 555 –\n\n78. \n\nFry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton & \n\nCompany, 2018. \n\nSasha Costanza -Chock. Design Justice: Community -Led Practices to Build the Worlds We \n\nNeed. Cambridge: The MIT Press, 2020. \n\nDavid G. Robinson. Voices in the Code: A Story About People, Their Values, and the \n\nAlgorithm They Made. New York: Russell Sage Foundation, 2022. 137 of 142 \n\nDiane Hart, Gabi Diercks -O'Brien, and Adrian Powell. “Exploring Stakeholder Engagement in \n\nImpact Evaluation Planning in Educational Development Work.” Evaluation 15, no. 3 \n\n(2009): 285 –306. \n\nAsit Bhattacharyya and Lorne Cummings. “Measuring Corporate Environmental \n\nPerformance – Stakeholder Engagement Evaluation.” Business Strategy and the \n\nEnvironment 24, no. 5 (2013): 309 –25. \n\nHendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. “A Modified \n\nStakeholder Participation Assessment Framework for Design Thinking in Health \n\nInnovation.” Healthcare 6, no. 3 (September 2018): 191 –96. \n\nFernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder \n\nParticipation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted \n\nNovember 1, 2021. \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. \n\n“Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.” SSRN, \n\nJuly 8, 2021. \n\nAlexandra Reeve Givens, and Meredith Ringel Morris. “Centering Disability Perspectives in \n\nAlgorithmic Fairness, Accountability, & Transparency.” FAT* '20: Proceedings of the 2020 \n\nConference on Fairness, Accountability, and Transparency, January 27, 2020, 684 -84. \n\n# MEASURE 4.2 \n\nMeasurement results regarding AI system trustworthiness in deployment context(s) and \n\nacross AI lifecycle are informed by input from domain experts and other relevant AI actors \n\nto validate whether the system is performing consistently as intended. Results are \n\ndocumented. \n\nAbout \n\nFeedback captured from relevant AI Actors can be evaluated in combination with output \n\nfrom Measure 2.5 to 2.11 to determine if the AI system is performing within pre -defined \n\noperational limits for validity and reliability, safety, security and resilience, privacy, bias \n\nand fairness, explainability and interpretability, and transparency and accountability. This \n\nfeedback provides an additional layer of insight about AI system performance, including \n\npotential misuse or reuse outside of intended settings. \n\nInsights based on this type of analysis can inform TEVV -based decisions about metrics and \n\nrelated courses of action. \n\nSuggested Actions \n\n• Integrate feedback from end users, operators, and affected individuals and communities \n\nfrom Map function as inputs to assess AI system trustworthiness characteristics. Ensure \n\nboth positive and negative feedback is being assessed. 138 of 142 \n\n• Evaluate feedback in connection with AI system trustworthiness characteristics from \n\nMeasure 2.5 to 2.11. \n\n• Evaluate feedback regarding end user satisfaction with, and confidence in, AI system \n\nperformance including whether output is considered valid and reliable, and explainable \n\nand interpretable. \n\n• Identify mechanisms to confirm/support AI system output (e.g. recommendations), and \n\nend user perspectives about that output. \n\n• Measure frequency of AI systems’ override decisions, evaluate and document results, \n\nand feed insights back into continual improvement processes. \n\n• Consult AI actors in impact assessment, human factors and socio -technical tasks to \n\nassist with analysis and interpretation of results. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent does the system/entity consistently measure progress towards stated \n\ngoals and objectives? \n\n• What policies has the entity developed to ensure the use of the AI system is consistent \n\nwith its stated values and principles? \n\n• To what extent are the model outputs consistent with the entity’s values and principles \n\nto foster public trust and equity? \n\n• Given the purpose of the AI, what level of explainability or interpretability is required \n\nfor how the AI made its determination? \n\n• To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nBatya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with \n\nMoral Imagination. Cambridge, MA: The MIT Press, 2019. \n\nBatya Friedman, David G. Hendry, and Alan Borning. “A Survey of Value Sensitive Design \n\nMethods.” Foundations and Trends in Human -Computer Interaction 11, no. 2 (November \n\n22, 2017): 63 –125. \n\nSteven Umbrello, and Ibo van de Poel. “Mapping Value Sensitive Design onto AI for Social \n\nGood Principles.” AI and Ethics 1, no. 3 (February 1, 2021): 283 –96. \n\nKaren Boyd. “Designing Up with Value -Sensitive Design: Building a Field Guide for Ethical \n\nML Development.” FAccT '22: 2022 ACM Conference on Fairness, Accountability, and \n\nTransparency, June 20, 2022, 2069 –82. 139 of 142 \n\nJanet Davis and Lisa P. Nathan. “Value Sensitive Design: Applications, Adaptations, and \n\nCritiques.” In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van \n\nden Hoven, Pieter E. Vermaas, and Ibo van de Poel, January 1, 2015, 11 –40. \n\nBen Shneiderman. Human -Centered AI. Oxford: Oxford University Press, 2022. \n\nShneiderman, Ben. “Human -Centered AI.” Issues in Science and Technology 37, no. 2 \n\n(2021): 56 –61. \n\nShneiderman, Ben. “Tutorial: Human -Centered AI: Reliable, Safe and Trustworthy.” IUI '21 \n\nCompanion: 26th International Conference on Intelligent User Interfaces - Companion, April \n\n14, 2021, 7 –8. \n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. “Human -\n\nCentered Design of Artificial Intelligence.” In Handbook of Human Factors and Ergonomics, \n\nedited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085 –1106. John Wiley & \n\nSons, 2021. \n\nCaitlin Thompson. “Who's Homeless Enough for Housing? In San Francisco, an Algorithm \n\nDecides.” Coda, September 21, 2021. \n\nJohn Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. “Algorithmic Decision -\n\nMaking and the Control Problem.” Minds and Machines 29, no. 4 (December 11, 2019): 555 –\n\n78. \n\nFry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton & \n\nCompany, 2018. \n\nSasha Costanza -Chock. Design Justice: Community -Led Practices to Build the Worlds We \n\nNeed. Cambridge: The MIT Press, 2020. \n\nDavid G. Robinson. Voices in the Code: A Story About People, Their Values, and the \n\nAlgorithm They Made. New York: Russell Sage Foundation, 2022. \n\nDiane Hart, Gabi Diercks -O'Brien, and Adrian Powell. “Exploring Stakeholder Engagement in \n\nImpact Evaluation Planning in Educational Development Work.” Evaluation 15, no. 3 \n\n(2009): 285 –306. \n\nAsit Bhattacharyya and Lorne Cummings. “Measuring Corporate Environmental \n\nPerformance – Stakeholder Engagement Evaluation.” Business Strategy and the \n\nEnvironment 24, no. 5 (2013): 309 –25. \n\nHendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. “A Modified \n\nStakeholder Participation Assessment Framework for Design Thinking in Health \n\nInnovation.” Healthcare 6, no. 3 (September 2018): 191 –96. 140 of 142 \n\nFernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder \n\nParticipation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted \n\nNovember 1, 2021. \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. \n\n“Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.” SSRN, \n\nJuly 8, 2021. \n\nAlexandra Reeve Givens, and Meredith Ringel Morris. “Centering Disability Perspectives in \n\nAlgorithmic Fairness, Accountability, & Transparency.” FAT* '20: Proceedings of the 2020 \n\nConference on Fairness, Accountability, and Transparency, January 27, 2020, 684 -84. \n\n# MEASURE 4.3 \n\nMeasurable performance improvements or declines based on consultations with relevant AI \n\nactors including affected communities, and field data about context -relevant risks and \n\ntrustworthiness characteristics, are identified and documented. \n\nAbout \n\nTEVV activities conducted throughout the AI system lifecycle can provide baseline \n\nquantitative measures for trustworthy characteristics. When combined with results from \n\nMeasure 2.5 to 2.11 and Measure 4.1 and 4.2, TEVV actors can maintain a comprehensive \n\nview of system performance. These measures can be augmented through participatory \n\nengagement with potentially impacted communities or other forms of stakeholder \n\nelicitation about AI systems’ impacts. These sources of information can allow AI actors to \n\nexplore potential adjustments to system components, adapt operating conditions, or \n\ninstitute performance improvements. \n\nSuggested Actions \n\n• Develop baseline quantitative measures for trustworthy characteristics. \n\n• Delimit and characterize baseline operation values and states. \n\n• Utilize qualitative approaches to augment and complement quantitative baseline \n\nmeasures, in close coordination with impact assessment, human factors and socio -\n\ntechnical AI actors. \n\n• Monitor and assess measurements as part of continual improvement to identify \n\npotential system adjustments or modifications \n\n• Perform and document sensitivity analysis to characterize actual and expected variance \n\nin performance after applying system or procedural updates. \n\n• Document decisions related to the sensitivity analysis and record expected influence on \n\nsystem performance and identified risks. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\n• To what extent are the model outputs consistent with the entity’s values and principles \n\nto foster public trust and equity? 141 of 142 \n\n• How were sensitive variables (e.g., demographic and socioeconomic categories) that \n\nmay be subject to regulatory compliance specifically selected or not selected for \n\nmodeling purposes? \n\n• Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\n• How will the accountable human(s) address changes in accuracy and precision due to \n\neither an adversary’s attempts to disrupt the AI or unrelated changes in the \n\noperational/business environment? \n\n• How will user and peer engagement be integrated into the model development process \n\nand periodic performance review once deployed? \n\nAI Transparency Resources \n\n• GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\n• Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nBatya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with \n\nMoral Imagination. Cambridge, MA: The MIT Press, 2019. \n\nBatya Friedman, David G. Hendry, and Alan Borning. “A Survey of Value Sensitive Design \n\nMethods.” Foundations and Trends in Human -Computer Interaction 11, no. 2 (November \n\n22, 2017): 63 –125. \n\nSteven Umbrello, and Ibo van de Poel. “Mapping Value Sensitive Design onto AI for Social \n\nGood Principles.” AI and Ethics 1, no. 3 (February 1, 2021): 283 –96. \n\nKaren Boyd. “Designing Up with Value -Sensitive Design: Building a Field Guide for Ethical \n\nML Development.” FAccT '22: 2022 ACM Conference on Fairness, Accountability, and \n\nTransparency, June 20, 2022, 2069 –82. \n\nJanet Davis and Lisa P. Nathan. “Value Sensitive Design: Applications, Adaptations, and \n\nCritiques.” In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van \n\nden Hoven, Pieter E. Vermaas, and Ibo van de Poel, January 1, 2015, 11 –40. \n\nBen Shneiderman. Human -Centered AI. Oxford: Oxford University Press, 2022. \n\nShneiderman, Ben. “Human -Centered AI.” Issues in Science and Technology 37, no. 2 \n\n(2021): 56 –61. \n\nShneiderman, Ben. “Tutorial: Human -Centered AI: Reliable, Safe and Trustworthy.” IUI '21 \n\nCompanion: 26th International Conference on Intelligent User Interfaces - Companion, April \n\n14, 2021, 7 –8. 142 of 142 \n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. “Human -\n\nCentered Design of Artificial Intelligence.” In Handbook of Human Factors and Ergonomics, \n\nedited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085 –1106. John Wiley & \n\nSons, 2021. \n\nCaitlin Thompson. “Who's Homeless Enough for Housing? In San Francisco, an Algorithm \n\nDecides.” Coda, September 21, 2021. \n\nJohn Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. “Algorithmic Decision -\n\nMaking and the Control Problem.” Minds and Machines 29, no. 4 (December 11, 2019): 555 –\n\n78. \n\nFry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton & \n\nCompany, 2018. \n\nSasha Costanza -Chock. Design Justice: Community -Led Practices to Build the Worlds We \n\nNeed. Cambridge: The MIT Press, 2020. \n\nDavid G. Robinson. Voices in the Code: A Story About People, Their Values, and the \n\nAlgorithm They Made. New York: Russell Sage Foundation, 2022. \n\nDiane Hart, Gabi Diercks -O'Brien, and Adrian Powell. “Exploring Stakeholder Engagement in \n\nImpact Evaluation Planning in Educational Development Work.” Evaluation 15, no. 3 \n\n(2009): 285 –306. \n\nAsit Bhattacharyya and Lorne Cummings. “Measuring Corporate Environmental \n\nPerformance – Stakeholder Engagement Evaluation.” Business Strategy and the \n\nEnvironment 24, no. 5 (2013): 309 –25. \n\nHendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. “A Modified \n\nStakeholder Participation Assessment Framework for Design Thinking in Health \n\nInnovation.” Healthcare 6, no. 3 (September 2018): 191 –96. \n\nFernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder \n\nParticipation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted \n\nNovember 1, 2021. \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. \n\n“Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.” SSRN, \n\nJuly 8, 2021. \n\nAlexandra Reeve Givens, and Meredith Ringel Morris. “Centering Disability Perspectives in \n\nAlgorithmic Fairness, Accountability, & Transparency.” FAT* '20: Proceedings of the 2020 \n\nConference on Fairness, Accountability, and Transparency, January 27, 2020, 684 -84.",
  "fetched_at_utc": "2026-02-08T19:07:09Z",
  "sha256": "af493a99a01e0e8c8e4484af7ac9975e95839574a51a1bc09803af5932d6a01b",
  "meta": {
    "file_name": "AI Risk Management Framework Playbook - NIST.pdf",
    "file_size": 2882270,
    "relative_path": "pdfs\\AI Risk Management Framework Playbook - NIST.pdf",
    "jina_status": 20000,
    "jina_code": 200,
    "usage": {
      "tokens": 72303
    }
  }
}