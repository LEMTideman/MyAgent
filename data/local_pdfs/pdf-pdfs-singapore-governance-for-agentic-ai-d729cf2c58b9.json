{
  "doc_id": "pdf-pdfs-singapore-governance-for-agentic-ai-d729cf2c58b9",
  "source_type": "local_pdf",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Singapore - Governance for Agentic AI.pdf",
  "title": "Singapore - Governance for Agentic AI",
  "text": "MODEL AI GOVERNANCE \n\n# FRAMEWORK FOR AGENTIC AI \n\nVersion 1.0 | Published 22 January 2026 \n\nPublished on 19 January 2026 by: Contents \n\nExecutive Summary ................................ ................................ ................................ ........... 1\n\n1 Introduction to Agentic AI ................................ ................................ ........................... 3\n\n1.1 What is Agentic AI? ................................ ................................ ............................. 3\n\n1.1.1 Core components of an agent ................................ ................................ ................... 3\n\n1.1.2 Multi -agent setups ................................ ................................ ................................ .... 4\n\n1.1.3 How agent design affects the limits and capabilities of each agent ............................. 4\n\n1.2 Risks of Agentic AI ................................ ................................ ............................... 6\n\n1.2.1 Sources of risk ................................ ................................ ................................ .......... 6\n\n1.2.2 Types of risk ................................ ................................ ................................ ............. 7\n\n2 Model AI Governance Framework for Agentic AI ................................ ........................... 8\n\n2.1 Assess and bound the risks upfront ................................ ................................ ...... 9\n\n2.1.1 Determine suitable use cases for agent deployment ................................ .................. 9\n\n2.1.2 Bound risks through design by defining agents limits and permissions ...................... 11 \n\n2.2 Make humans meaningfully accountable ................................ ............................ 13 \n\n2.2.1 Clear allocation of responsibilities within and outside the organisation .................... 13 \n\n2.2.2 Design for meaningful human oversight ................................ ................................ ... 16 \n\n2.3 Implement technical controls and processes ................................ ...................... 18 \n\n2.3.1 During design and development, use technical controls ................................ ........... 18 \n\n2.3.2 Before deploying, test agents ................................ ................................ .................. 19 \n\n2.3.3 When deploying, continuously monitor and test ................................ ....................... 20 \n\n2.4 Enable end -user responsibility ................................ ................................ ........... 22 \n\n2.4.1 Different users, different needs ................................ ................................ ............... 22 \n\n2.4.2 Users who interact with agents ................................ ................................ ................ 23 \n\n2.4.3 Users who integrate agents into their work processes ................................ .............. 23 \n\nAnnex A: Further resources ................................ ................................ ............................... 25 \n\nAnnex B: Call for feedback and case studies ................................ ................................ ...... 27 1\n\n# Executive Summary \n\nAgentic AI is the next evolution of AI , holding transformative potential for users and businesses. \n\nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \n\nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \n\nevolving, agents are already transforming the workplace through coding assistants, customer \n\nservice agents, and automating enterprise productivity workflows. \n\nThese greater capabilities also bring forth new risks . Agents’ access to sensitive data and ability \n\nto make changes to their environment, such as updating a customer database or making a payment, \n\nare double -edged swords. As we move towards deploying multiple agents with complex interactions, \n\noutcomes also become more unpredictable. \n\nHumans must remain accountable and properly manage these risks. While existing governance \n\nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \n\nneed to be translated in practice for agents. Meaningful human control and oversight need to be \n\nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \n\nhuman oversight over all agent workflows becomes impractical at scale .\n\nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \n\noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \n\nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \n\ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in -house or \n\nusing third -party agentic solutions. Building on our previous model governance frameworks, we have \n\noutlined key considerations for or ganisations in four areas when it comes to agents: \n\n1.  Assess and bound the risks upfront \n\nOrganisations should adapt their internal structures and processes to account for new risks \n\nfrom agents. Key to this is first understanding the risks posed by the agent’s actions, which \n\ndepend on factors such as the scope of actions the agent can take, the reversibility of those \n\nactions, and the agent’s level of autonomy. \n\nTo manage these risks early, organisations could limit the scope of impact of their agents by \n\ndesigning appropriate boundaries at the planning stage, such as limiting the agent’s access \n\nto tools and external systems. They could also ensure that the agent’s actions are traceable \n\nand controllable through establishing robust identity management and access controls for \n\nagents. \n\n2.  Make humans meaningfully accountable \n\nOnce the “green light” is given for agentic AI deployment, an organisation should take steps \n\nto ensure human accountability. However, the autonomy of agents may complicate \n\ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \n\nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \n\ntherefore important to clearly define the responsibilities of different stakeholders, both 2\n\nwithin the organisation and with external vendors, while emphasising adaptive governance, \n\nso that the organisation is set up to quickly understand new developments and update its \n\napproach as the technology evolves. \n\nSpecifically, “human -in -the -loop” has to be adapted to address automation bias, which has \n\nbecome a bigger concern with increasingly capable agents. This includes defining significant \n\ncheckpoints in the agentic workflow that require human approval, such as high -stakes or \n\nirreversible actions, and regularly auditing human oversight to check that it remains effective \n\nover time. \n\n3.  Implement technical controls and processes \n\nOrganisations should ensure the safe and reliable operationalisation of AI agents by \n\nimplementing technical measures across the agent lifecycle. During development, \n\norganisations should incorporate technical controls for new agentic components such as \n\nplanning, tools and still -maturing protocols , to address increased risks from these new \n\nattack surfaces. \n\nBefore deployment, organisations should test agents for baseline safety and reliability, \n\nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \n\nuse. New testing approaches will be needed to evaluate agents. \n\nDuring and after deployment , as agents interact dynamically with their environment and not \n\nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \n\ncontinuous monitoring after deployment. \n\n4.  Enable end -user responsibility \n\nTrustworthy deployment of agents does not rely solely on developers, but also on end -users \n\nusing them responsibly. To enable responsible use, as a baseline, users should be informed \n\nof the agent’s range of actions, access to data, and the user’s own responsibilities. \n\nOrganisations should consider laye ring on training to equip employees with the knowledge \n\nrequired to manage human -agent interactions and exercise effective oversight, while \n\nmaintaining their tradecraft and foundational skills. \n\nThis is a living document. We have worked with government agencies and leading companies to \n\ncollate current best practices, but this is a fast -developing space, and best practices will evolve. This \n\nframework will need to be continuously updated to keep pace with new developments. W e invite \n\nfeedback to refine the framework, and case studies demonstrating how the framework can be \n\napplied for responsible agentic deployment. 3\n\n# 1 Introduction to Agentic AI \n\n# 1.1 What is Agentic AI ?\n\nAgentic AI systems are systems that can plan across multiple steps to achieve specified \n\nobjectives, using AI agents .1 There is no consensus on what defines an agent, but there are certain \n\ncommon features – agents usually possess some degree of independent planning and action taking \n\n(e.g. searching the web or creating files) over multiple steps to achieve a user -defined goal. 2\n\nIn this framework, we focus on agents built on language models, which are increasingly being \n\nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \n\nas its brain to make decisions and complete tasks. However, it is w orth noting that software agents \n\nare not a new concept and other types of agents exist, such as those which use deterministic rules, \n\nor other neural networks, to make decisions. 3\n\n# 1.1.1 Core components of an agent \n\nCore components of a simple agent 4\n\nAs agents are built on top of language models, it is helpful to start with the core components of \n\na simple LLM -based app. \n\n1.  Model : an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \n\nthe “brain” of the agent. It processes instructions, interprets user inputs, and generates \n\ncontextually appropriate responses.  \n\n> 1\n\nAdapted from C yber Security Agency of Singapore (CSA) , Draft Addendum on Securing Agentic AI . \n\n> 2\n\nSee  International AI Safety Report . \n\n> 3\n\nSee World Economic Forum (WEF) , AI Agents in Action: Foundations for Evaluation and Governance . \n\n> 4\n\nAdapted from GovTech Singapor e,  Agentic Risk & Capability Framework , CSA Singapore , Draft \n\nAddendum on Securing Agentic AI  and Anthropic , Building Effective Agents ). \n\nGuide decisions \n\nand actions \n\nRetrieve and store \n\ninfo for long -term \n\nInteract with \n\nexternal systems \n\nModel \n\nPlanning & \n\nReasoning \n\nInstructions  Tools Memory \n\n1\n\nv\n\n2\n\nv\n\n3\n\nv\n\n5\n\nv\n\n4\n\nv4\n\n2.  Instructions : Natural language commands that define an agent's role, capabilities, and \n\nbehavioural constraints e.g. a system prompt for an LLM. \n\n3.  Memory : Information that is stored and accessible to the LLM, either in short or long -term \n\nstorage. Sometimes added to allow the model to obtain information from previous user \n\ninteractions or external knowledge sources. \n\nAn agent uses the model, instructions and memory in similar ways as an LLM -based app. In \n\naddition, it has other components that enable it to complete more complex tasks: \n\n4.  Planning and reasoning : The model is usually trained to reason and plan, meaning that it \n\ncan output a series of steps needed for a task. \n\n5.  Tools: Tools enable the agent to take actions and interact with other systems, such as writing \n\nto files and databases, controlling devices, or performing transactions. The model calls tools \n\nto complete a task. \n\n6.  Protocols: This is a standardised way for agents to communicate with tools and other agents. \n\nFor example, the Model Context Protocol (MCP) has been developed for agents to \n\ncommunicate with tools, 5 whereas the Agent2Agent Protocol (A2A) defines a standard for \n\nagents to communicate with each other. 6\n\n# 1.1.2 Multi -agent setups \n\nIn an agentic system, it is common for multiple agents to be set up to work together. This can \n\nsometimes improve performance, by allowing each agent to specialise in a certain function or task \n\nand work in parallel.  7\n\nThree common design patterns for multi -agent systems are :8\n\n• Sequential : Agents work one after another in a linear workflow. Each agent’s output \n\nbecomes the next agent’s input. \n\n• Supervisor : One supervising agent coordinates specialised agents under it. \n\n• Swarm : Agents work at the same time, handing off to another agent when needed \n\n# 1.1.3 How agent design affects the limits and capabilities of each agent \n\nWhile each agent may have the same core components, the design of each component can \n\nsignificantly affect what the agent can do . It is generally helpful to distinguish between two \n\nconcepts when considering what an agent can do: 9\n\n• Action -space (or authority, capabilities): Range of actions the agent is permitted to take, \n\ndetermined by the tools it is allowed to use, transactions it can execute, etc.              \n\n> 5See Anthropic, Model Context Protocol .\n> 6See Google, Agent2Agent Protocol .\n> 7See LangChain ,Benchmarking Multi -Agent Architectures .\n> 8Adapted from AWS ,Multi -Agent Collaboration Patterns with Strands Agents and Amazon Nova .\n> 9See WEF, AI Agents in Action: Foundations for Evaluation and Governance .\n\n5\n\n• Autonomy (or decision -making): Degree to which an agent can decide when and how to act \n\ntowards a goal, such as by defining the steps to be taken in a workflow. This can be \n\ndetermined by its instructions and level of human involvement. \n\n## Action -space \n\nAn agent’s action -space mainly depends on the tools it has access to, which can affect: \n\n• Systems it can access :\n\no Sandboxes only: Sandboxed tools (e.g. for code execution, data analysis) that cannot \n\naffect any other system \n\no Internal systems: Tools internal to the organisation, such as being able to search and \n\nupdate the organisation’s databases \n\no External systems: Tools that enable the agent to access external services, such as \n\nretrieving and updating data through third -party pre -defined APIs. \n\n• Actions it can take in relation to the system it can access :\n\no Read vs write: An agent may only be able to read and retrieve information from a \n\nsystem, rather than write to and modify data within the system. \n\nAn emerging modality of agentic AI is a computer use agent, whose primary tool is access to a \n\ncomputer and browser. This means that it can take any action that a human can take with a computer \n\nand browser without having to rely on specifically defined tool s and APIs. This significantly increases \n\nwhat the agent can access and do. \n\n## Autonomy \n\nAn agent’s autonomy mainly depends on its instructions component and the level of human \n\ninvolvement in the agentic system. \n\nIn terms of instructions, an agent can be given differing level of instructions: \n\n• Detailed instructions and SOP: An agent instructed to follow a detailed SOP to complete a \n\ntask would be limited in the decisions it can make at each stage. \n\n• Using its own judgment: An agent instructed to use its own judgment to complete a task \n\nwould have more freedom to define its plan and workflow. \n\nAnother relevant factor is the level of human involvement. When interacting with an agent, a human \n\ncan be involved to different levels: 10 \n\n• Agent proposes, human operates: The human directs and approves every step taken by an \n\nagent. \n\n• Agent and human collaborate: The human and agent work together. The agent requires \n\nhuman approval at significant steps, such as before writing to a database or making a \n\npayment. However, the human can intervene anytime by taking over the agent’s work or \n\npausing the agent and requesti ng a change.   \n\n> 10 See Knight First Amendment Institute at Columbia University, Levels of Autonomy for AI Agents .\n\n6\n\n• Agent operates, human approves: The agent requires human approval only at critical steps \n\nor failures, such as deleting a database or making a payment above a predefined amount. \n\n• Agent operat es, human observes: The agent does not require human approval as it \n\ncompletes its task, though its actions may be audited after the fact. \n\n# 1.2 Risks of Agentic AI \n\n# 1.2.1 Sources of risk \n\nThe new components of an agent constitute new sources of risks .11  The risks themselves are \n\nfamiliar – fundamentally, agents are software systems built on LLMs. They inherit traditional \n\nsoftware vulnerabilities (such as SQL injection) and LLM -specific risks (such as hallucination, bias, \n\ndata leakage and adversarial prompt injections). 12 \n\nHowever, the risks can manifest differently through the different components. For example: \n\n• Planning and reasoning: An agent can hallucinate and make a wrong plan to complete a \n\ntask. \n\n• Tools: An agent can hallucinate by calling non -existent tools or calling tools with the wrong \n\ninput, or call ing tools in a biased manner. As tools connect the agent to external systems, \n\nprompt or code injections can also manipulate the agent to exfiltrate or otherwise \n\nmanipulate the data it has access to. \n\n• Protocols: Finally, as new protocols emerge to handle agent communication, they can also \n\nbe poorly deployed or compromised e.g. an untrusted MCP server deployed with code to \n\nexfiltrate the user’s data. \n\nAs components within an agent or multiple agents interact, risks can also arise at the system \n\nlevel. 13  For example: \n\n• Cascading effect: A mistake by one agent can quickly escalate as its outputs are passed \n\nonto other agents. For example, in supply chain management, a hallucinated inventory figure \n\nfrom one agent could potentially cause downstream agents to reorder excessive or \n\ninsufficient stock. \n\n• Unpredictable outcomes: Agents working together can also compete or coordinate in \n\nunintended ways. For example, in manufacturing, different agents may be involved in \n\nmanaging machines and inventory. While coordinating to meet production goals, the agents \n\nmight interact unpredict ably due to complex optimi sation algorithms and over or under -\n\nprioritise one resource or machine, leading to unexpected bottlenecks.        \n\n> 11 BCG highlighted examples of new risks from agents e.g. agents that optimize their own goals locally\n> may create instability across the system, flawed behaviour by one agent may spread to other agents\n> (see What Happens When AI Stops Asking Permission? )\n> 12 Adapted from CSA ,Draft Addendum on Securing Agentic AI .\n> 13 See WEF, AI Agents in Action: Foundations for Evaluation and Governance , which highlighted a new\n> class of failure modes, linked to potentially misaligned interactions in multi -agent systems e.g.\n> orchestration drift, semantic misalignment, interconnectedness and cascading effects.\n\n7\n\n# 1.2.2 Types of risk \n\nBecause agents take actions in the real world, when they malfunction , it can lead to harmful \n\nreal -world impact . Organisations should be aware of these negative outcomes :\n\n• Erroneous actions : Incorrect actions such as an agent fixing appointments on the wrong \n\ndate or producing flawed code. The exact harmful outcome depends on the action in \n\nquestion, e.g. flawed code can lead to exploited security vulnerabilities, and wrong medical \n\nappointment s may affect a patient’s health outcomes. \n\n• Unauthorised actions : Actions taken by the agent outside its permitted scope or authority, \n\nsuch as taking an action without escalating it for human approval based on a company policy \n\nor standard operating procedure. \n\n• Biased or unfair actions : Actions that lead to unfair outcomes, especially when dealing with \n\ngroups of different profiles and demographics, such as biased vendor selection in \n\nprocurement, disbursements of grants, and/or hiring decisions. \n\n• Data breaches : Actions that lead to the exposure or manipulation of sensitive data. Such \n\ndata may be personally identifiable information or confidential information e.g. customer \n\ndetails, trade secrets, and/or internal communications. This can be due to a security breach, \n\nwhere attackers exploit agents to reveal private information, or an agent disclosing sensitive \n\ndata due to a failure to recognise it as sensitive. \n\n• Disruption to connected systems : As agents interact with other systems, they can cause \n\ndisruption to connected systems when they are compromised or malfunction e.g. deleting a\n\nproduction codebase, or overwhelming external system s with requests. \n\nErroneous actions  Unauthorised \n\nactions \n\nBiased or unfair \n\nactions \n\nData breaches \n\nDisruption to \n\nconnected \n\nsystems 8\n\n# 2 Model AI Governance Framework for Agentic AI \n\nFour dimensions of the MGF for Agentic AI \n\nThe MGF for Agentic AI builds on the responsible AI practices for organisations set out in MGF \n\n(2020) 14  by highlighting emerging best practices to address new concerns from agentic AI. This is so \n\nthat organisations can develop and use agentic AI with the requisite knowledge and judgment .\n\nThe framework begins with helping organisations to assess and bound the risks upfront . It \n\nhighlights new risks that should be considered during risk assessment, and design considerations at \n\nthe planning stage to limit the potential scope of impact of the agents, as well as ensure that agents \n\nare traceable and controllable. \n\nWhile agents may act autonomously, human responsibility continues to apply. Once the “green light” \n\nis given to deploy agentic AI, an organisation should take immediate steps to mak e humans \n\nmeaningfully accountabl e. This includes clearly defining responsibility across multiple actors \n\nwithin and outside the organisation involved in the agent lifecycle; and taking measures to ensure \n\nthat human -in -the -loop remains effective over time notwithstanding automation bias. \n\nTo ensure safe and reliable operationalisation of agents, an organisation should adopt technical \n\ncontrols and processes across the AI lifecycle. During development, guardrails for new \n\ncomponents in AI agents such as planning and tools should be implemented. Before deployment, \n\nagents should be tested for baseline safety and reliability. After deployment, agents should be \n\ncon tinuously monitored as they interact dynamically with their environment. \n\nFinally, trustworthy deployment of agents does not rest solely on developers, but also on end -users. \n\nOrganisations are responsible for enabling end -user responsibility by equipping them with \n\nessential information to use agents appropriately and exercise effective oversight, while maintaining \n\ntheir tradecraft and foundational skills.    \n\n> 14 See Model AI Governance Framework (2 nd Ed) .\n\n1. Assess and bound \n\nthe risks upfront \n\n2. Make humans \n\nmeaningfully accountable \n\n3. Implement technical \n\ncontrols and processes \n\n4. Enable end -user \n\nresponsibility 9\n\n# 2.1 Assess and bound the risks upfront \n\nAgents bring new risks, especially in their access to sensitive data and ability to change their \n\nenvironment through action -taking. Their adaptive, autonomous and multi -step nature also \n\nincreases the potential for unexpected actions, emergent risks and cas cading impacts. \n\nOrganisations should consider these new dimensions as part of risk assessment, and limit the scope \n\nof impact of their agents by designing appropriate boundaries at an early stage. \n\nWhen planning for the use of agentic AI, organisations should consider: \n\n• Determining suitable use cases for agent deployment by considering agent -specific \n\nfactors that can affect the likelihood and impact of the risk. \n\n• Design choices to bound the risks upfront by applying limits on agent’s access to tools and \n\nsystems and defining a robust identity and permissions framework. \n\n# 2.1.1 Determine suitable use cases for agent deployment \n\nRisk identification and assessment is the first step when considering if an agentic use case is \n\nsuitable for development or deployment . Risk is a function of likelihood (probability of the risk \n\nmanifesting) and impact (severity of impact if the risk manifests). \n\nThe following non -exhaustive factors affect the level of risk of an agentic use case: \n\nFactors affecting impact \n\nFactor  Description  Illustration \n\nDomain and use \n\ncase in which \n\nagent is being \n\ndeployed \n\nLevel of tolerance of error in the \n\ndomain and use case in which the \n\nagent is being deployed to \n\nAgent executing financial \n\ntransactions which require a high \n\ndegree of accuracy, vs agent that \n\nsummarises internal meetings \n\nAgent’s access to \n\nsensitive data \n\nWhether the agent can access \n\nsensitive data, such as personal \n\ninformation or confidential data \n\nAgent that requires access to \n\npersonal customer data gives rise to \n\nthe risk of leaking such data, vs \n\nagent who only has access to \n\npublicly available information \n\nAgent’s access to \n\nexternal systems \n\nWhether the agent can access \n\nexternal systems \n\nAgent that sends data to third -party \n\nAPIs can leak data to these third \n\nparties, or disrupt these systems by \n\nmaking too many requests, vs agent \n\nthat only has access to sandboxed \n\nor internal tools \n\nScope of agent’s \n\nactions \n\nWhether an agent can only read \n\nfrom or modify the data and systems \n\nit has access to \n\nRead vs write: Agent that can only \n\nread from a database vs being able \n\nto write to it \n\nMany tools vs a few: Agent that can \n\nonly choose from a few pre -defined \n\ntools, vs an agent who has unlimited \n\naccess to a browser tool 10 \n\nReversibility of \n\nagent’s actions \n\nIf the agent can modify data and \n\nsystems, whether such \n\nmodifications are easily reversed \n\nAgent that schedules meetings vs \n\nagent that sends email \n\ncommunications to external parties \n\nFactors affecting likelihood \n\nFactor  Description  Illustration \n\nAgent’s level of \n\nautonomy \n\nWhether the agent can define the \n\nentire workflow or must follow a \n\nwell -defined procedure. \n\nA higher level of autonomy can \n\nresult in higher unpredictability, \n\nincreasing likelihood of error. \n\nAgent is provided with a SOP and \n\ninstructed to follow it when carrying \n\nout a task, vs agent is instructed to \n\nuse its best judgment to select and \n\nexecute every step \n\nTask complexity  How complex the task is, in relation \n\nto the number of steps required to \n\ncomplete it and the level of analysis \n\nrequired at each step. \n\nA higher level of complexity similarly \n\nincreases unpredictability and the \n\nlikelihood of error. \n\nAgent is required to extract key \n\naction points from a meeting \n\ntranscript, vs agent is tasked to \n\nfollow a nuanced data sharing policy \n\nwhen handling external requests for \n\ninformation \n\nAgent’s access to \n\nexternal systems \n\nWhether the agent is exposed to \n\nexternal systems, and who \n\nmaintains these systems. \n\nA higher level of exposure makes the \n\nagent more vulnerable to prompt \n\ninjections and cyberattacks. \n\nAgent can only access an internal \n\nknowledge base which is \n\nmaintained by trusted internal \n\nteams, vs an agent who can access \n\nthe web containing untrusted data \n\nThreat modelling also makes risk assessment more \n\nrigorous by systematically identifying specific ways \n\nin which an attacker may take to compromise the \n\nsystem. Common security threats to agentic systems \n\ninclude memory poisoning, tool misuse, and privilege \n\ncompromise. 15  As agentic systems (especially multi -\n\nagent systems) can become very complex, it is often \n\nuseful to use a method called taint tracing to map out \n\nall the workflows and interactions to track how \n\nuntrusted data can move through the system. For more \n\ninformatio n on how to perform threat modelling and \n\ntaint tracing for agentic systems, organisations may \n\nrefer to  CSA’s Draft Addendum on Securing Agentic AI .                \n\n> 15 For a more comprehensive coverage of potential security threats to agentic AI systems , see OWASP,\n> Agentic AI –Threats and Mitigations .\n> The relationship between threat modelling\n> and risk assessment\n> Threat modelling augments the risk assessment\n> process by generating contextualised threat\n> events with well -described sequence of actions,\n> activities and scenarios that the attacker may\n> take to compromise the system. With more\n> relevant threat events, risk a ssessments will be\n> more rigorous and robust, resulting in more\n> targeted controls and effective layered defence.\n> Since risk assessment is continuous, the threat\n> model should be regularly updated.\n> Adapted from CSA, Guide to Cyber Threat\n> Modelling\n\n11 \n\n# 2.1.2 Bound risks through design by defining agents limits and permissions \n\nHaving selected an appropriate agent use case, organisations can further bound the risks by \n\ndefining appropriate limits and permission policies for each agent. \n\n## Agent limits \n\nOrganisations should consider defining limits on: \n\n• Agent’s access to tools and systems: Define policies that give agents only the minimum \n\ntools and data access needed for it to complete its task. 16  For example, a coding assistant \n\nmay not require access to a web search tool, especially if it already has curated access to \n\nthe latest software documentation. \n\n• Agent’s autonomy: For process -driven tasks, SOPs and protocols are frequently used to \n\nimprove consistency and reduce unpredictability.  17  Define similar SOPs for agentic \n\nworkflows that an agent is constrained to follow, rather than giving the agent the freedom to \n\ndefine every step of the workflow. \n\n• Agent’s area of impact: Design mechanisms and procedures to take agents offline and limit \n\ntheir potential scope of impact when they malfunction. This can include running agents in \n\nself -contained environments with limited network and data access, particularly when they \n\nare carrying out high -risk tasks such as code execution. 18 \n\n## Agent identity \n\nIdentity management and access control is one of the key means in which organisations enable \n\ntraceability and accountability today for humans. As agents become more autonomous, identity \n\nmanagement has to be extended to agents as well to track individual agent behaviour and establish \n\nwho holds accountability for each agent. \n\nThis is an evolving space, and gaps exist today in terms of handling agent identity robustly . For \n\nexample, current authorisation systems typically have pre -defined, static scopes. However, to \n\noperate safely in more complex scenarios, agents require fine -grained permissions that may change \n\ndynamically depending on the context, risk levels, and tas k objectives. Current authentication \n\nsystems are also typically based on a single, unique individual. Such systems face difficulty in \n\nhandling complex agent setups, such as when agents act for multiple human users with different \n\npermissions, or recursive delegation scenarios where agents spin up multiple sub -agents. 19                    \n\n> 16 See PwC ,The rise –and risks –of agentic AI .\n> 17 Grab introduced anLLM agent framework leverag ing on Standard Operating Procedures (SOPs) to\n> guide AI -driven execution (see Introducing the SOP -driven LLM agent frameworks ).\n> 18 See McKinsey, Deploying agentic AI with safety and security: A playbook for technology leaders .\n> 19 For a more comprehensive treatment of how current identity systems may face challenges when\n> catering to agentic AI, see OpenID, Identity Management for Agentic AI .\n\n12 \n\nSolutions are being developed to address these issues, such as integrating well -established \n\nstandards like OAuth 2.0 into MCP. 20  The industry is also developing new standards and solutions for \n\nagents, such as decentralised identity management and dynamic access control. 21 \n\nIn the interim, organisations should consider these best practices to enable agent control and \n\ntraceability: \n\n• Identification: An agent should have its own unique identity, such that it can identify itself to \n\nthe organisation, its human user, or other agents. However, an agent’s identity may need to \n\nbe tied to a supervising agent, a human user, or an organisational department for \n\naccountability and tracking. Additionally, the different capacities in which an agent acts (e.g. \n\nindependently or on behalf of a specified human user) should also be recorded. \n\n• Authorisation: An agent can have pre -defined permissions based on its role or the task at \n\nhand, or its permissions may be dynamically set by its authorising human user, or a \n\ncombination of both. As a rule of thumb, the human user should not be able to set \n\npermissions fo r the agent greater than what the human user is himself authorised to do. Such \n\ndelegations of authority should be clearly recorded.         \n\n> 20 See MCP specifications for Authentication support ,Authorisation support .\n> 21 See proposed framework for agentic identity by Cloud Security Alliance, Agentic AI Identity & Access\n> Management: A New Approach .\n> Evaluating the residual risks\n> Residual risk is the risk that remains after mitigation measures have been applied. It is important to note\n> that there will always be a level of risk remaining, even after efforts are taken to identify appropriate agentic\n> use cases and define limits on any agents, especially given how quickly agentic AI is evolving. Ultimately,\n> organisations should evaluate and determine if the residual risk for their agentic deployment is of a\n> tolerable level and can be accepted.\n\n13 \n\n# 2.2 Make humans meaningfully accountable \n\nThe organisations that deploy agents and the humans who oversee them remain accountable for the \n\nagents’ behaviours and actions. But it can be challenging to fulfil this accountability when agent \n\nactions emerge dynamically and adaptively from interactions instead of fixed logic. Multiple \n\nstakehol ders may also be involved in different parts of the agent lifecycle, diffusing accountability. \n\nFinally, automation bias, or the tendency to over -trust a n automated system , especially when it has \n\nperformed reliably in the past, becomes a bigger concern as humans supervise increasingly capable \n\nagents. \n\nTo address these challenges to human accountability, organisations should consider: \n\n• Clear allocation of responsibilities within and outside the organisation , by establishing \n\nchains of accountability across the agent value chain and lifecycle, while emphasising \n\nadaptive governance, so that the organisation is set up to quickly understand new \n\ndevelopments and update their approach as the technology evolves. \n\n• Measures to enable meaningful human oversight of agents , such as requiring human \n\napproval at significant checkpoints, auditing the effectiveness of human approvals, and \n\ncomplementing these measures with automated monitoring .\n\n# 2.2.1 Clear allocation of responsibilities within and outside the organisation \n\nAs deployers, organisations and humans remain accountable for the decisions and actions of \n\nagents. However, as with AI, the value chain for agentic AI involves multiple actors. Organisations \n\nshould consider the allocation of responsibility both within their organisation, and vis -à-vis other \n\norganisations along the value chain. \n\nSimplified agentic AI value chain 22    \n\n> 22 For a more comprehensive list of potential stakeholders involved in the agentic AI ecosystem, see CSA\n> and FAR.AI, Securing Agentic AI: A Discussion Paper .\n\nModel developers \n\nModels that agents \n\ncan be built on  Agentic AI system \n\nproviders \n\nProviding platforms to \n\nbuild agents on or full \n\nSaaS solutions \n\nDeploying \n\norganisation \n\nMay also develop \n\nagents in -house \n\nEnd users \n\nInteracts with \n\nand uses \n\nagents \n\nTooling providers \n\ne.g. MCP, APIs \n\nAllow agents to \n\nconnect to external \n\nsystems 14 \n\n## Within the organisation \n\nWithin the organisation, organisations should allocate responsibilities for different teams \n\nacross the agent lifecycle. While each organisation is structured differently, this is an illustration \n\nof how such responsibilities may be allocated across different teams: \n\nKey decision \n\nmakers \n\nProduct teams \n\nCybersecurity \n\nteams \n\nWho: Leaders who define strategic decisions and high -level policies \n\nfor the organisation e.g. board members, C -suite executives, \n\nmanaging directors, or department leaders. \n\nKey responsibilities can include: \n\n• Setting high -level goals for use of agents \n\n• Defining permitted operational use cases for agents, \n\nincluding limits on agent’s data access \n\n• Setting the overall governance approach, including risk \n\nmanagement frameworks and escalation processes \n\nWho: These roles oversee the translation of stakeholder needs or \n\nbusiness goals into a technical agentic solution e.g. Product \n\nManagers, UI / UX Designers, AI Engineers, Software Engineers \n\nKey responsibilities can include: \n\n• Defining the design and requirements for agents, as well as \n\nany feature controls or phased rollouts \n\n• Reliable implementation of agents i.e. development, pre -\n\ndeployment testing and post -deployment monitoring \n\nacross the agent lifecycle \n\n• Educating users on responsible use of agentic product \n\nWho: These roles oversee the protection of agentic systems from \n\ncyber threats, by implementing and managing security measures, \n\nidentifying vulnerabilities, and responding to incidents e.g. Chief \n\nSecurity Officer, Cyber Security Specialist, Penetration Tester \n\nKey responsibilities can include: \n\n• Defining baseline security guardrails and secure -by -design \n\ntemplates that technical teams should implement or adapt \n\nto the agentic system being deployed \n\n• Conducting regular red teaming and threat modelling 15 \n\nUsers \n\n## Outside the organisation \n\nOrganisations may also need to work with external parties when deploying agents e.g. model \n\ndevelopers, agentic AI providers, or hosts of external MCP servers or tools. \n\nIn these cases, organisations should similarly ensure that there are measures in place to fulfil its \n\nown accountability. Some agent -specific considerations are: \n\n• Clarify distribution of obligations in any terms and conditions or contracts between the \n\norganisation and the external party. In particular, organisations should consider provisions \n\nto address any security arrangements, performance guarantees, or data protection and \n\nconfidentiality. Where th ere are gaps, the organisation should reassess if the agentic \n\ndeployment meets its risk tolerance. \n\n• Features to maintain security and control. Organisations should consider if the external \n\nparty’s product offers features for the organisation to maintain a sufficient level of security \n\nor control. This includes strong authentication measures such as scoped API keys, per -agent \n\nidentity tokens, and robust observability such as the logging of tool calls and access history. \n\nWhere such features are lacking, organisations should consider alternative or in -house \n\nsolutions, or scoping down the agentic use case, suc h as restricting access to sensitive data. \n\n## End users \n\nOrganisations may deploy agents to users within or outside their organisation. In doing so, \n\norganisations should ensure that users are provided sufficient information to hold the organisation \n\naccountable, as well as any information relating to the user’s own responsibilities. More information \n\ncan be found in  Enabling end -user responsibility  below. \n\nWho: Any individual who utilises the output of the agents to contribute to \n\nan organisational goal e.g. company employees making decisions or \n\nautomating workflows and practices. \n\nKey responsibilities can include: \n\n• Ethical and responsible usage of agents \n\n• Attending required training, complying with usage policies, \n\ntimely reporting of bugs or issues with agents \n\n> Developing internal capabilities for adaptive governance\n> All teams involved in the agentic AI lifecycle should also develop internal capabilities to\n> understand agentic AI. As the technology is quickly evolving, being aware of the improvements\n> and limitations of new agentic developments, such as new modalities li ke computer use agents,\n> or new evaluation frameworks for agents, allow organisations to quickly adapt their governance\n> approach to new developments.\n\n16 \n\n# 2.2.2 Design for meaningful human oversight \n\nSetting up a system for effective human supervision \n\nOrganisations should define significant checkpoints or action boundaries that require human \n\napproval , especially before sensitive actions are executed. This can include: 23 \n\n• High -stakes actions and decisions e.g. editing of sensitive data, final decisions in high -risk \n\ndomains (such as healthcare or legal), actions that may trigger liability \n\n• Irreversible actions e.g. permanently deleting data, sending communications, making \n\npayments \n\n• Outlier or atypical behaviour e.g. when agent accesses a system or database outside of its \n\nwork scope, when agent selects a delivery route that is twice as long as the median distance \n\n• User -defined . Agents may act on behalf of users who have different risk appetites. Beyond \n\norganisation -defined boundaries, users may be given the option to define their own \n\nboundaries e.g. requiring approval for purchases above a certain amount \n\nApart from considering when approvals are required, organisations should also consider what \n\nform approvals should take. These considerations include: \n\n• Keep approval requests contextual and digestible. When asking humans for approval, \n\nkeep the request short and clear, instead of providing long logs or raw data that may be \n\nchallenging to decipher and understand. \n\n• Consider the form of human input required. For straightforward actions such as accessing \n\na database, the human user can simply approve or reject. For more complex cases, such as \n\nreviewing an agent’s plan before execution, it may be more productive for the human to edit \n\nthe plan before giving the ag ent the go -ahead. \n\nOrganisations should implement measures to ensure continued effectiveness of human \n\noversight , particularly as humans remain susceptible to alert fatigue and automation bias. These \n\nmeasures can include:  \n\n> 23 For further examples of where human involvement may be considered, see Partnership on AI,\n> Prioritising real -time failure detection in AI agents ).\n\nDefine significant \n\ncheckpoints or action \n\nboundaries that require \n\nhuman approval \n\nTrain humans to evaluate \n\nthese requests for approval \n\neffectively, and audit these \n\napprovals \n\nComplement this with \n\nautomated monitoring \n\nmechanisms and \n\npredefined alert thresholds 17 \n\n• Training humans to identify common failure modes e.g. inconsistent agent reasoning, \n\nagents referring to outdated policies \n\n• Regularly auditing the effectiveness of human oversight \n\nFinally, human oversight should be complemented with automated real -time monitoring to \n\nescalate any unexpected or anomalous behaviour . This can be done by implementing alerts for \n\ncertain logged events (e.g. attempted unauthorised access or multiple failed attempts to call a tool), \n\nusing data science techniques to identify anomalous agent trajectories, or using agents to monitor \n\nother ag ents. For more information, see  Continuous testing and monitoring  below. 18 \n\n# 2.3 Implement technical controls and processes \n\nThe agentic components that differentiate agents from simple LLM -based applications necessitate \n\nadditional controls during the key stages of the implementation lifecycle. \n\nOrganisations should consider: \n\n• During design and development, design and implement technical controls . The new \n\ncomponents and capabilities of agents also necessitate new and tailored controls. \n\nDepending on the agent design, implement controls such as tool guardrails and plan \n\nreflections. Further, limit the agent’s impact on the external environment by enf orcing least -\n\nprivilege access to tools and data. \n\n• Pre -deployment, test agents for safety and security. As with all software, testing before \n\ndeployment ensures that the system behaves as expected. Specifically for agents, test for \n\nnew dimensions such as overall task execution, policy adherence and tool use accuracy, \n\nand test at different levels and across v aried datasets to capture the full spectrum of agent \n\nbehaviour. \n\n• When deploying, gradually roll out agents and continuously monitor them in production. \n\nThe autonomous nature of agents and the changing environment makes it challenging to \n\naccount for and test all possible outcomes before deployment. Hence it is recommended to \n\nroll out agents gradually, supported with real -time monitoring post -deployment to ensure \n\nthat agents function safely. \n\n# 2.3.1 During design and development, use technical controls \n\nOrganisations should design and implement technical controls in the agentic AI system to \n\nmitigate identified risks. For agents specifically, in addition to baseline software and LLM controls, \n\nconsider adding controls for: \n\n• New agentic components, such as planning and reasoning and tools \n\n• Increased security concerns from the larger attack surface and new protocols \n\nFor illustration, these are some sample controls for agents. For a more comprehensive list, \n\norganisations can refer to CSA’s  Draft Addendum on Securing Agentic AI  and GovTech’s  Agentic Risk \n\nand Capability Framework .\n\nPlanning  • Prompt agent to reflect on whether its plan adheres to user instructions \n\n• Prompt the agent to summarise its understanding and request clarification \n\nfrom the user before proceeding \n\n• Log the agent’s plan and reasoning for the user to evaluate and verify \n\nTools  • Configure tools to require strict input formats \n\n• Apply the principle of least privilege to limit tools available to each agent, \n\nenforced through robust authentication and authorisation \n\n• For data -related tools: \n\no Do not grant agent write access to tables in sensitive databases unless \n\nstrictly required 19 \n\no Configure agent to let user take over control when keying in sensitive \n\ndata (e.g. passwords, API keys) \n\nProtocols  • Use standardised protocols where applicable (e.g. agentic commerce \n\nprotocols when agent is handling a financial transaction) \n\n• For MCP servers: \n\no Whitelist trusted servers and only allow agent to interact with servers \n\non that whitelist \n\no Sandbox any code execution \n\n# 2.3.2 Before deploying, test agents \n\nOrganisations should test agents for safety and security before deployment. This provides \n\nconfidence that the agents work as expected and controls are effective. Best practices on software \n\nand LLM testing are still relevant, such as unit and integration testing for software systems, as well \n\nas selecting representative datasets, an d useful metrics and evaluators for LLM testing. \n\nOrganisations can refer to previous guidance, such as the Starter Kit for testing of LLM -based apps \n\nfor safety and reliability. \n\nHowever, organisations should adapt their testing approaches for agents. Some considerations \n\ninclude: \n\n• Testing for new risks: Beyond producing incorrect outputs, agents can take unsafe or \n\nunintended actions through tools. Organisations can consider testing for: 24 \n\no Overall task execution : Whether agent can complete task accurately \n\no Policy compliance : Whether an agent follows defined SOPs and routes for human \n\napproval when required \n\no Tool calling : Whether an agent calls the right tools, with the right permissions, with \n\nthe right inputs and in the right order \n\no Robustness : As agents are expected to react and adapt to real -world situations, test \n\nfor their response to errors and edge cases \n\n• Testing entire agent workflows: Agents can take multiple steps in sequence without human \n\ninvolvement. Thus, beyond testing an agent’s final output, agents should be tested across \n\ntheir entire workflow, including reasoning and tool calling. \n\n• Testing agents individually and together: Beyond individual agents, testing should be \n\ncarried out at the multi -agent system level, to understand any emergent risks and behaviours \n\nwhen agents collaborate, such as competitive behaviours or the impact on other agents \n\nwhen one agent has been compromis ed. \n\n• Testing in real or realistic environments: As agents may be expected to navigate real -world \n\nsituations, testing should occur in a properly configured execution environment that mirrors \n\nproduction as closely as possible, such as using tool integrations, external APIs, and \n\nsandboxes that behave as th ey would in deployment. However, organisations should    \n\n> 24 For an example of new agentic aspects to test for, see Microsoft Foundry, Agent evaluators .\n\n20 \n\ncalibrate the need for realism against the risk of prematurely allowing agents to access tools \n\nthat affect the real world. \n\n• Testing repeatedly and across varied datasets: Agent behaviour is inherently stochastic \n\nand context -dependent. Testing should thus be done at scale and across varied datasets to \n\nobserve any unexpected low -probability behaviours, especially if they are high -impact. This \n\nrequires generating test dataset s that cover different conditions that agents may encounter \n\nand running these tests multiple times, including minor perturbations where needed. \n\n• Evaluating test results at scale: Reliably evaluating test results at scale is a known \n\nchallenge for LLM testing. Agents add a further layer of complexity as their workflows can be \n\nlong and contain unstructured information that cannot be easily processed by humans or \n\nautomated scripts. Org anisations may consider using different evaluation methods for \n\ndifferent parts of the agentic workflow (e.g. deterministic tests for structured tool calls vs \n\nLLM or human evaluation for unstructured agent reasoning). Howev er, there is still a need to \n\nevaluate agents holistically, so that agent patterns across steps can be evaluated. Current \n\nindustry solutions thus include defining LLMs or agents to evaluate other agents. 25 \n\n# 2.3.3 When deploy ing , continuous ly monitor and test \n\nAs agents are adaptive and autonomous , organisations should consider mechanisms to respond to \n\nunexpected or emergent risks when deploying agents .\n\n## Gradual deployment of agents \n\nOrganisations should consider gradually rolling out agents into production to control the \n\namount of risk exposure . Such rollouts can be controlled based on :\n\n• User s of agents e.g. rolling out to trained or experienced users first \n\n• Tools and protocols available to agent e.g. restricting agents to more secure, whitelisted \n\nMCP servers first \n\n• Systems exposed to agent e.g. using agents in lower -risk internal systems first \n\n## Continuous test ing and monitor ing \n\nOrganisations should continuously m onitor and log agent behaviour post -deployment , and \n\nes tablish reporting and failsafe mechanisms for agent failures or unexpected behaviours . This \n\nallows the organisation to :\n\n• Intervene in real -time : When potential failures are detected, s top agent workflow and \n\nescalate to a human supervisor e.g. if agent attempts unauthorised access \n\n• Debug when incidents happen : Logging and tracing each step of an agent workflow and \n\nagent -to -agent interactions help to identify points of failure \n\n• Audit at regular intervals: This ensure s that the system is performing as expected .    \n\n> 25 For an example of agent evaluation solutions, see AWS Labs ,Agent Evaluation .\n\n21 \n\nMonitoring and observability are not new concept s, but agents introduce some challenges . As \n\nagents execute multiple actions at machine speed, organisations face the issue of extracting \n\nmeaningful insights from the voluminous logs generated by monitoring systems. This becomes more \n\ndifficult when high -risk anomalies are expected to be detected in real -time and surfaced as early as \n\npossible .\n\nKey considerations when setting up a monitoring system include: \n\n• What to log : Organisations should determine their objectives for monitoring (e.g. real -time \n\nintervention, debugging , integration between components ) to identify what to log. In doing \n\nso, prioritise monitoring for high -risk activities such as updating database records or \n\nfinancial transactions. \n\n• How to effectively monitor logs : Organisations can consider approaches such as: \n\no Defining alert thresholds: \n\n▪ Programmatic, threshold -based : Define alerts when agents trigger \n\nthresholds e.g. agent attempts unauthorised access or makes too many \n\nrepeated tool calls within a specified timeframe .\n\n▪ Outlier / anomaly detection : Use data science or deep learning technique s\n\nto process agent signals and identify anomalous behaviour that may indicate \n\nmalfunctions. \n\n▪ Agents monitoring other agents : Design agents to monitor other agents in \n\nreal -time, flagging any anomalies or inconsistencies. \n\no Defining specific interventions : For each alert type, consider what the level of \n\nintervention should be. Some degree of human review should be incorporated, \n\nproportionate to the risk level. For example, lower -priority alerts can be flagged for \n\nreview at a scheduled time, whereas higher -priority ones might require temporarily \n\nhalting agent execution until a human reviewer can assess. In the event of \n\ncatastrophic agent ic malfunction or compromis e, comm en surate measures such as \n\ntermination and fallback solutions should be considered .\n\nFinally, continuously test the agentic system even post -deployment to ensure that it works as \n\nexpected and is not affected by model drift or other changes in the environment. 22 \n\n# 2.4 Enabl e end -user responsibility \n\nUltimately, end users are the ones who use and rely on agents , and human accountability also \n\nextends to these users. Organisations should provide sufficient information to end users to \n\npromote trust and enable responsible use. \n\nOrganisations should consider: \n\n• Transparency : Users should be informed of the agents’ capabilities (e.g. scope of agent’s \n\naccess to user’s data, actions the agent can take) and the contact points whom users can \n\nescalate to if the agent malfunctions .\n\n• Education : Users should be educated on proper use and oversight of agents (e.g. training \n\nshould be provided on an agent’s range of actions, common failure modes like hallucinations, \n\nusage policies for data), as well as the potential loss of trade craft i.e. as agents take over \n\nmore functions, basic operational knowledge could be eroded. Hence sufficient training \n\n(espe cially in areas where agents are prevalent) should be provided to ensure that humans \n\nretain core skills. \n\n# 2.4.1 Different users, different needs \n\nOrganisations should cater to different users with different information needs, to enable such \n\nusers to use AI responsibly. Broadly, there are two main archetypes of end -users – those who \n\ninteract with agents, and those who integrate agents into their work processes or oversee them .\n\nUsers who interact with agents \n\ne.g. customer service, HR agents –\n\nmostly external -facing \n\nUsers who integrate agents into \n\ntheir work processes \n\ne.g. coding assistants, enterprise \n\nworkflows – mostly internal -facing \n\nFocus on transparency  Layer on education and training 23 \n\n# 2.4.2 User s who interact with agents \n\nSuch users usually interact with agents that act on behalf of the organisation, e.g. customer \n\nservice or sales agents. These agents tend to be external facing , although they can also be deployed \n\nwithin the organisation e.g. a human resource agent that interacts with other users in the \n\norganisation. \n\nFor these users , focus on transparency . Organisations should share pertinent information to \n\nfoster trust and facilitate proper usage of agents. Such information can include: \n\n• User’s responsibilities : Clearly define the user’s responsibilities, such as asking the user to \n\ndouble -check all information provided by the agent. \n\n• Interaction : Declare upfront that the user s are interacting with agent s.\n\n• Agent s’ range of actions an d decisions : Inform the user s on the range of actions and \n\ndecisions that the agent is authorised to perform and make .\n\n• Data : Be clear on how user data is collected, stored, and used by the agents, in accordance \n\nwith the organi sation's data privacy policies. Where necessary, obtain explicit consent from \n\nusers before collecting or using their data for the agents. \n\n• Human accountability and escalation : Provide user s with the respective human contact \n\npoint s who are responsible for the agent s, whom the user s can alert if the agent s malfunction \n\nor if they are dissatisfied with a decision. \n\n# 2.4.3 Users who integrate agents into their work processes \n\nSuch users typically utili se agents as part of their internal work flows e.g. coding assistants, \n\nautomation of enterprise processes . The agent acts for and on behalf of the user. \n\nFor these users , in addition to the information in the previous section , layer on education and \n\ntraining so that users can use the agents responsibly . Key aspects include education and training \n\non: \n\n• Foundational knowledge on agents \n\no Relevant use cases , so that the user s understand how to best integrate the agents into \n\ntheir day -to -day work, and the scenarios under which the use of agent s should be \n\nrestricted (e.g. do not use an agent for confidential data) \n\no Instructing the agents e.g. general best practices in prompting, glossary of keywords to \n\nelicit specific responses \n\no Agents’ range of actions , so that the user is aware of their capabilities and potential \n\nimpact \n\n• Effective oversight of agents \n\no Common agent failure modes , such as hallucinations, getting stuck in loops after errors ,\n\nso that the user can identify and flag out issues .\n\no Ongoing support , such as regular refreshers to update users on latest features and \n\ncommon user mistakes \n\n• Potential impact on tradecraft 24 \n\no As agents take over entry level tasks, which typically serve as the training ground for new \n\nstaff, this could le ad to loss of basic operational knowledge for the users. \n\no Organisations should identify core capabilities of each job and provide s ufficient training \n\nand work exposure so that users retain foundational skills. 25 \n\n# Annex A: Further resources \n\n1.  Introduction to Agentic AI \n\nWhat is Agentic \n\nAI? \n\n• AWS , Agentic AI Security Scoping Matrix: A framework for securing \n\nautonomous AI systems \n\n• WEF , AI Agents in Action: Foundations for Evaluation and \n\nGovernance \n\n• Anthropic,  Building effective agents \n\n• IBM,  The 2026 Guide to AI Agents \n\n• McKinsey , What is an AI agent? \n\nRisks of Agentic AI  • GovTech , Agentic Risk & Capability Framework \n\n• CSA , Draft Addendum on Securing Agentic AI \n\n• OWASP , Multi -Agentic System Threat Modelling Guide \n\n• IBM,  AI agents: Opportunities, risks, and mitigations \n\n• Infosys,  Agentic AI risks to the enterprise, and its mitigations \n\n2.  MGF for Agentic AI \n\nAssess and bound \n\nthe risks upfront \n\nAgentic governance in general \n\n• EY , Building a risk framework for Agentic AI \n\n• McKinsey , Deploying agentic AI with safety and security: A playbook \n\nfor technology leaders \n\n• Bain,  Building the Foundation for Agentic AI \n\n• OWASP,  State of Agentic AI Security and Governance 1.0 \n\nRisk assessment and threat modelling \n\n• OWASP , Agentic AI – Threats & Mitigations \n\n• OWASP , Multi -Agentic System Threat Modelling Guide \n\n• Cloud Security Alliance,  Agentic AI: Understanding Its Evolution, \n\nRisks, and Security Challenges \n\n• EY,  Building a risk framework for Agentic AI \n\nAgent limits and agent identity \n\n• Meta,  Agents Rule of Two: A Practical Approach to AI Agent Security \n\n• OpenID , Identity Management for Agentic AI \n\nMake humans \n\nmeaningfully \n\naccountable \n\nAllocating responsibility within and outside an organisation \n\n• Carnegie Mellon University,  The ‘Who’, ‘What’, and ‘How’ of \n\nResponsible AI Governance \n\n• CSA and FAR.AI,  Securing Agentic AI: A Discussion Paper \n\n• McKinsey,  Accountability by design in the agentic organization \n\nDesigning for meaningful human oversight \n\n• Partnership on AI , Prioritizing real -time failure detection in AI agents 26 \n\n• Permit.IO , Human -in -the -Loop for AI Agents: Best Practices, \n\nFrameworks, Use Cases, and Demo \n\nImplement \n\ntechnical controls \n\nand processes \n\nTechnical controls \n\n• GovTech , Agentic Risk & Capability Framework \n\n• CSA , Draft Addendum on Securing Agentic AI \n\nTesting and evaluation \n\n• Microsoft , Microsoft Agent Evaluators \n\n• AWS , AWS Agent Evaluation \n\n• Anthropic,  Demystifying evals for AI agents \n\n• IBM,  What is AI Agent Evaluation? \n\nMonitoring and observability \n\n• Microsoft,  Top 5 agent observability best practices for reliable AI \n\nEnabling end -user \n\nresponsibility \n\n• Zendesk , What is AI transparency? A comprehensive guide \n\n• HR Brew , Salesforce’s head of talent growth and development \n\nshares how the tech giant is training its 72,000 employees on \n\nagentic AI \n\n• Harvard Business Review , The Perils of Using AI to Replace Entry -\n\nLevel Jobs 27 \n\n# Annex B: Call for feedback and case studies \n\nCall for feedback: This is a living document, and we invit e suggestions on how the framework can \n\nbe updated or refined. The following questions can be used as a guide: \n\n• Introduction to Agentic AI : Are the descriptions of agentic AI systems accurate and \n\nsufficiently comprehensive for readers to obtain a clear overview of the governance \n\nchallenges of agentic AI ? Are there other risks that should be included? \n\n• Proposed Model Governance Framework : Are the four dimensions of the framework \n\npractical and applicable? Are there any other dimensions that should be included? For \n\neach dimension, are there specific governance and technical challenges and best \n\npractices that should be included? \n\nCall for case studies: We also invite organisations to submit their own agentic governance \n\nexperiences as case studies on how specific aspects of the framework can be implemented, to serve \n\nas practical examples of responsible deploymen t that other organisations can refer to . Case studies \n\nshould ideally involve an organisation’s deployment of an agentic use case that demonstrates one \n\nof the dimensions of the framework . While not exhaustive, we are specifically interested in case \n\nstudies that demonstra te good practices in :\n\nDimension  Example case studies \n\nAssess and bound \n\nthe risks upfront \n\n• Defining use cases to reduce risk but maximise benefits of \n\nagents \n\n• Defining limits on agent’s autonomy through defined SOPs and \n\nworkflows \n\n• Defining limits on agent’s access to tools and systems \n\n• How identity is implemented for agents, and how it interacts with \n\nhuman identities in an organisation \n\nMake humans \n\nmeaningfully \n\naccountable \n\n• Allocating responsibility across the organisation for agentic \n\ndeployment \n\n• Assessing when human approvals are required in an agentic use \n\ncase, and how requests for such approvals are implemented \n\nImplement technical \n\ncontrols and \n\nprocesses \n\n• Designing and implementing technical controls for agents \n\n• How agentic safety testing is carried out \n\n• How monitoring and observability mechanisms are set up, \n\nincluding defining alert thresholds and processing large volumes \n\nof agent -related data \n\nEnable end -user \n\nresponsibility \n\n• Making information available to internal and external \n\nstakeholders who interact with and use agents \n\n• Training human overseers to exercise effective oversight \n\nFor an example of what a case study may look like , please refer to those in our previous  Model \n\nGovernance Framework for AI .\n\nPlease note that a ny feedback and case studies may be incorporated into an updated version of the \n\nframework, and contributors will be acknowledged accordingly . Please submit your feedback and \n\ncase studies at this link : https://go.gov.sg/mgfagentic -feedback .",
  "fetched_at_utc": "2026-02-08T18:51:47Z",
  "sha256": "d729cf2c58b90a9dcd0f9a35309e5fc837197030f026dd114a85ffac77296768",
  "meta": {
    "file_name": "Singapore - Governance for Agentic AI.pdf",
    "file_size": 1078313,
    "relative_path": "pdfs\\Singapore - Governance for Agentic AI.pdf",
    "jina_status": 20000,
    "jina_code": 200,
    "usage": {
      "tokens": 12742
    }
  }
}