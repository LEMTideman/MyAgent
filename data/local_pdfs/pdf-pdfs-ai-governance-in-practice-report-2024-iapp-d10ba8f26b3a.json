{
  "doc_id": "pdf-pdfs-ai-governance-in-practice-report-2024-iapp-d10ba8f26b3a",
  "source_type": "local_pdf",
  "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Governance in Practice Report 2024 - IAPP.pdf",
  "title": "AI Governance in Practice Report 2024 - IAPP",
  "text": "AI Governance in Practice Report 2024 AI Governance in Practice Report 2024  | 2\n\n# Table of \n\n# contents \n\n## What's inside? \n\nExecutive summary  3\n\nPart I. Understanding AI and governance  . . . . . . . . . . . . . . . . . . .  6\n\nPart II. The data challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  15 \n\nPart III. The  privacy and data protection challenge  23 \n\nPart IV. The  transparency, explainability \n\nand interpretability challenge  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  32 \n\nPart V. The bias, discrimination and fairness challenge  41 \n\nPart VI. The  security and robustness challenge  . . . . . . . . . . . . .  50 \n\nPart VII. AI  safety  55 \n\nPart VIII. The  copyright challenge  61 \n\nPart IX. Third-party AI assurance  . . . . . . . . . . . . . . . . . . . . . . . . .  65 \n\nConclusion  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  69 \n\nContacts  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  70 AI Governance in Practice Report 2024  | 3\n\n# Executive \n\n# summary \n\n## Recent and rapidly advancing \n\n## breakthroughs in machine \n\n## learning technology have forever \n\n## transformed the landscape of AI. \n\nAI systems have become powerful engines capable of \n\nautonomous learning across vast swaths of information and \n\ngenerating entirely new data. As a result, society is in the midst \n\nof significant disruption with the surge in AI sophistication and \n\nthe emergence of a new era of technological innovation. \n\nAs businesses grapple with a future in which the boundaries of \n\nAI only continue to expand, their leaders face the responsibility \n\nof managing the various risks and harms of AI, so its benefits \n\ncan be realized in a safe and responsible manner. \n\nCritically, these benefits are accompanied by serious \n\nconsiderations and concerns about the safety of this technology \n\nand the potential for it to disrupt the world and negatively \n\nimpact individuals when left unchecked. Confusion about how \n\nthe technology works, the introduction and proliferation of bias \n\nin algorithms, dissemination of misinformation, and privacy \n\nrights violations represent only a sliver of the potential risks. \n\nThe practice of  AI governance  is designed to tackle these \n\nissues. It encompasses the growing combination of principles, \n\nlaws, policies, processes, standards, frameworks, industry \n\nbest practices and other tools incorporated across the design, \n\ndevelopment, deployment and use of AI. → Executive summary    \n\n> AI Governance in Practice Report 2024 |4\n> TABLE OF CONTENTS ↑\n\nWhile relatively new, the field of AI governance \n\nis maturing, with government authorities \n\naround the world beginning to develop targeted \n\nregulatory requirements and governance \n\nexperts supporting the creation of accepted \n\nprinciples, such as the Organisation for \n\nEconomic Co-Operation and Development's  AI \n\nPrinciples , emerging best practices and tools for \n\nvarious uses of AI in different domains. \n\nThere are many challenges and potential \n\nsolutions for AI governance, each with unique \n\nproximity and significance based on an \n\norganization's role, footprint, broader risk-\n\ngovernance profile and maturity. This report \n\naims to inform the growing, increasingly \n\nempowered and increasingly important \n\ncommunity of AI governance professionals \n\nabout the most common and significant \n\nchallenges to be aware of when building \n\nand maturing an AI governance program. \n\nIt offers actionable, real-world insights \n\ninto applicable law and policy, a variety of \n\ngovernance approaches, and tools used to \n\nmanage risk. Indeed, some of the challenges \n\nto AI governance overlap and run through \n\na range of themes. Therefore, an emerging \n\nsolution for one thematic challenge may \n\nalso be leveraged for another. Conversely, in \n\ncertain circumstances, specific challenges and \n\nassociated solutions may conflict and require \n\nreconciliation  with other approaches. Some of \n\nthese potential overlaps and conflicts have been \n\nidentified throughout the report. \n\nGlobal AI private investment \n\n(USD billion, 2021) 2013 2014 2015 2016 2017 2018 2019 2020 2021 4810 18 22 38 42 47 94 → Executive summary \n\nAI Governance in Practice Report 2024  | 5\n\nTABLE OF CONTENTS  ↑\n\n# Questions about whether and when \n\n# organizations should prioritize \n\n# AI governance are being answered: \n\n# \"yes\" and \"now,\" respectively. \n\nQuestions about whether and when organizations should prioritize \n\nAI governance are being answered: \"yes\" and \" now ,\" respectively. \n\nThis  report is, therefore, focused on how organizations can approach, \n\nbuild and leverage AI governance in the context of the increasingly \n\nvoluminous and complex applicable landscape. \n\nJoe Jones \n\nIAPP Director of Research \n\nand Insights \n\nAshley Casovan \n\nIAPP AI Governance Center \n\nManaging Director \n\nUzma Chaudhry \n\nIAPP AI Governance \n\nCenter Research Fellow \n\nNina Bryant \n\nFTI Technology Senior \n\nManaging Director \n\nLuisa Resmerita \n\nFTI Technology \n\nSenior Director \n\nMichael Spadea \n\nFTI Technology Senior \n\nManaging Director AI Governance in Practice Report 2024  | 6\n\n# Part I. \n\n# Understanding \n\n# AI and \n\n# governance \n\n## Components of an AI system and \n\n## their governance \n\nTo understand how to govern an AI system, it is important to first \n\nunderstand what an AI system is. The EU AI Act, for example, \n\ndefines an AI system as \"a machine-based system that is designed \n\nto operate with varying levels of autonomy and that may exhibit \n\nadaptiveness after deployment, and that, for explicit or implicit \n\nobjectives, infers, from the input it receives, how to generate outputs \n\nsuch as predictions, content, recommendations, or decisions that \n\ncan influence physical or virtual environments.\" \n\nAs indicated in the OECD's Framework for the Classification of AI \n\nsystems, AI systems are comprised of data used to train and operate \n\na system, model, output and context. While a model is a fundamental \n\nbuilding block of an AI system, a single model seldom operates in \n\nisolation. Instead, multiple AI models come together and interact \n\nwith each other to form complex AI systems. Additionally, AI systems \n\nare often designed to interact with other systems for sharing data, \n\nfacilitating seamless integration into real-world environments. \n\nThis  results in a network of AI systems, each with its specialized \n\nmodels, working  together to achieve a  larger goal. \n\nWith AI poised to revolutionise many aspects of our \n\nlives, fresh cooperative governance approaches are \n\nessential. Effective collaboration between regulatory \n\nportfolios, within nations as well as across borders, \n\nis crucial: both to safeguard people from harm and to \n\nfoster innovation and growth. \n\nKate Jones \n\nU.K. Digital Regulation Cooperation Forum CEO → Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 7 \n\n> TABLE OF CONTENTS ↑\n\nAI governance is about to get a lot \n\nharder. The internal complexity of \n\ngoverning AI is growing as more \n\ninternal teams adopt AI, new AI \n\nfeatures are built, and the systems \n\nget complex, but at the same time, \n\nthe external complexity is also set to \n\ngrow rapidly with new regulations, \n\ncustomer demands, and safety \n\nresearch evolving. \n\nThe organizations who have invested \n\nin structured AI governance already \n\nhave a leg up and will continue to \n\nhave a competitive advantage. \n\nAndrew Gamino-Cheong \n\nTrustible AI Co-founder and Chief Technology Officer \n\nNavigating AI governance sources \n\nGiven the complexity and transformative \n\nnature of AI, significant work has been done \n\nby law and policymakers on what is now a \n\nvast and growing body of principles, laws, \n\npolicies, frameworks, declarations, voluntary \n\ncommitments, standards and emerging best \n\npractices that can be challenging to navigate. \n\nMany of these various sources interact with \n\neach other, either directly or by virtue of the \n\nissues covered. \n\nAI principles, such as the OECD's AI Principles \n\nor UNESCO's Recommendation on the Ethics \n\nof AI, can shape global standards, especially \n\nwhen national governments pledge to \n\nvoluntarily incorporate such guidance into \n\ntheir domestic  AI governance initiatives. \n\nThey  provide a nonbinding, principled \n\napproach to guide legal, policy and industry \n\nefforts toward tackling thematic challenges. \n\nAlgorithm Watch created an inventory of these \n\nprinciples, identifying 167  reports .\n\nLaws and regulations include existing \n\nlegislation that is not specific but is \n\nnonetheless applicable to AI, as well as \n\nemerging legislation that more specifically \n\naddresses the governance of AI systems, such \n\nas the EU AI Act. The EU AI Act is the world's \n\nfirst comprehensive AI regulation. Although \n\njurisdictional variations can be observed across \n\nthe emerging global AI regulatory landscape, \n\nmany draft regulations adopt a risk-based \n\napproach similar to the EU AI Act. \n\nThe EU AI Act mandates AI governance standards \n\nbased on the risk classification of AI systems and \n\nthe organization's role as an AI actor. Certain \n\nAI systems are deemed to pose unacceptable \n\nrisk and are prohibited by law, subject to very \n\nnarrow exceptions. The bulk of the requirements \n\nimposed by the act apply to providers of high-risk \n\nAI systems, although deployers and resellers, \n\nnamely distributers and importers, are are also \n\nsubject to direct obligations. \n\nThe act imposes regulatory obligations at \n\nenterprise, product and operational levels, \n\nsuch as establishing appropriate accountability \n\nstructures, assessing system impact, providing \n\ntechnical documentation, establishing risk \n\nmanagement protocols and monitoring \n\nperformance, among other key requirements. \n\nIn  the context of the growing variety of \n\ngenerative AI use cases and adoption of \n\nsolutions embedding generative AI such as MS \n\nCopilot, general purpose AI-specific provisions \n\nare another crucial component of the EU AI \n\nAct. Depending on their capabilities, reach and \n\ncomputing power, certain GPAI systems are \n\nconsidered to present systemic risk and attract \n\nbroadly similar obligations to those applicable \n\nto high-risk AI systems. → Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 8 \n\n> TABLE OF CONTENTS ↑\n\nIn addition to binding legislation, voluntary \n\nAI frameworks, such as the National Institute \n\nof Standards and Technology's AI Risk \n\nManagement Framework and the International \n\nOrganization for Standardization's AI Standards, \n\noffer structured and actionable guidance \n\nstakeholders can elect to use to support \n\ntheir work on implementing AI governance. \n\nVoluntary commitments are often developed to \n\nbring different stakeholders closer to a shared \n\nunderstanding of identifying, assessing and \n\nmanaging risks. Standards serve as benchmarks \n\nthat can demonstrate compliance with \n\nregulatory requirements. \n\nInternational declarations and commitments \n\nmemorialize shared commitments, often between \n\ngovernments, to specific aspects or broad \n\nswathes of AI governance. While not binding, \n\nsuch commitments can, at a minimum, indicate \n\na country's support for and intention to advance \n\nAI  governance in particular or general ways, even \n\nat the highest of levels. \n\nNavigating a growing body of draft AI laws, \n\nregulations, standards and frameworks can \n\nbe challenging for organizations pioneering \n\nwith AI. By understanding their unique AI risk \n\nprofile and adopting a risk-based approach, \n\norganizations can build a robust and scalable \n\nAI governance framework that can be deployed \n\nacross jurisdictions. \n\nIAPP Global AI Law and Policy Tracker  \n\n> This map shows the jurisdictions in focus and covered by the IAPP Global AI Law and Policy Tracker . It does not represent the extent to which jurisdictions\n> around the world are active on AI governance legislation. Tracker last updated January 2024.\n\n→ Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 9\n\nTABLE OF CONTENTS  ↑\n\nThe following are  examples  of some of the most prominent and consequential  AI governance efforts :\n\nPrinciples \n\n→ OECD AI Principles \n\n→ European Commission's Ethics Guidelines for Trustworthy AI \n\n→ UNESCO Recommendation on the Ethics of AI \n\n→ The White House Blueprint for an AI Bill of Rights \n\n→ G7 Hiroshima Principles \n\nLaws and \n\nregulations \n\n→ EU AI Act \n\n→ EU Product Liability Directive, proposed \n\n→ EU General Data Protection Regulation \n\n→ Canada – AI and Data Act, proposed \n\n→ U.S. AI Executive Order 14110 \n\n→ Sectoral U.S. legislation for employment, housing and consumer finance \n\n→ U.S. state laws, such as Colorado AI Act, Senate Bill 24-205 \n\n→ China's Interim Measures for the Management of Generative AI Services \n\n→ The United Arab Emirates Amendment to Regulation 10 to include new rules on \n\nProcessing Personal Data through Autonomous and Semi-autonomous Systems \n\n→ Digital India Act \n\nAI frameworks \n\n→ OECD Framework for the classification of AI Systems \n\n→ NIST AI RMF \n\n→ NIST Special Publication 1270: Towards a Standard for Identifying and Managing Bias in AI \n\n→ Singapore AI Verify \n\n→ The Council of Europe's Human Rights, Democracy, and the Rule of Law Assurance \n\nFramework for AI systems \n\nDeclarations \n\nand voluntary \n\ncommitments \n\n→ Bletchley Declaration \n\n→ The Biden-Harris Administration's voluntary commitments from leading AI companies \n\n→ Canada's guide on the use of generative AI \n\nStandards \n\nefforts \n\n→ ISO/IEC JTC 1 SC 42 \n\n→ The Institute of Electrical and Electronics Engineers Standards Association P7000 \n\n→ The European Committee for Electrotechnical Standardization  AI standards for EU AI Act \n\n→ The  VDE Association's AI Quality and Testing Hub \n\n→ The British Standards Institution and  Alan Turing Institute AI Standards Hub \n\n→ Canada's  AI and Data Standards Collaborative \n\nIt is important to take an \n\necosystem approach to AI \n\ngovernance. Policy makers and \n\nindustry need to work together \n\nat platforms such as the AI Verify \n\nFoundation to make sense of the \n\nopportunities and risks that this \n\ntechnology brings. The aim is to \n\nfind common guardrails to manage \n\nkey risks in order to create a \n\ntrusted ecosystem that promotes \n\nmaximal innovation. \n\nDenise Wong \n\nSingapore Infocomm Media Development Authority Assistant Chief \n\nExecutive, Data Innovation & Protection Group → Part I. Understanding AI and governance    \n\n> AI Governance in Practice Report 2024 |10\n> TABLE OF CONTENTS ↑\n\nThe AI governance imperative \n\nWith private investment, global adoption rates \n\nand regulatory activity on the rise, as well as \n\nthe growing maturity of the technology, AI is \n\nincreasingly becoming a strategic priority for \n\norganizations and governments worldwide. \n\nOrganizations of all sizes and industries are \n\nincreasingly engaging with AI systems at various \n\nstages of the technology product supply  chain. \n\nThe exceptional dependence on high volumes \n\nof data and endless practical applicability that \n\nmake AI technology a disruptive opportunity \n\ncan also generate uniquely multifaceted risks for \n\nbusinesses and individuals. These include legal, \n\nregulatory, reputational and/or financial risks to \n\norganizations, but also risks to individuals and \n\nthe wider society. \n\nAI Risks \n\nREPUTATIONAL \n\n> Risk of\n> damage to\n> reputation\n> and market\n> competitiveness\n\nFINANCIAL \n\n> Risk of financial\n> implications,\n> e.g., fines, legal\n> or operational\n> costs, or\n> lost profit\n\nLEGAL AND \n\nREGULATORY \n\n> Risk of\n> noncompliance\n> with legal and\n> contractual\n> obligations\n\nINDIVIDUALS \n\nAND SOCIETY     \n\n> Risk of bias\n> or other\n> detrimental\n> impact on\n> individuals →Part I. Understanding AI and governance\n> AI Governance in Practice Report 2024 |11\n> TABLE OF CONTENTS ↑\n\nEnterprise governance \n\nAI governance starts with defining the corporate \n\nstrategy for AI by documenting: \n\n→ Target operating models to set out clear roles \n\nand responsibilities for AI risk. \n\n→ Compliance assessments to establish \n\nprogram maturity and remediation priorities. \n\n→ Accountability processes to record and \n\ndemonstrate compliance. \n\n→ Policies and procedures to formulate policy \n\nstandards and operational procedures. \n\n→ Horizon scanning to enhance and \n\nalign  the program with ongoing \n\nregulatory  developments. \n\nProduct governance \n\nAI governance also requires enterprise policy \n\nstandards to be applied at the product level. \n\nOrganizations can ensure their AI products \n\nmatch their enterprise strategy by using: \n\n→ System impact assessments to identify and \n\naddress risk prior to product development \n\nor  deployment. \n\n→ Quality management procedures tailored \n\nto the software development life cycle to \n\naddress risk by design. \n\n→ Risk and controls frameworks to define \n\nAI risk and treatment based on widely \n\nrecognised standards such as ISO and NIST. \n\n→ Conformity assessments and declarations to \n\ndemonstrate their products are compliant. \n\n→ Technical documentation including \n\nstandardized instructions of use and \n\ntechnical product specifications. \n\n→ Post-market monitoring plans to monitor \n\nproduct compliance following market launch. \n\n→ Third-party due diligence assessments \n\nto identify possible external risk and \n\ninform  selection. → Part I. Understanding AI and governance    \n\n> AI Governance in Practice Report 2024 |12\n> TABLE OF CONTENTS ↑\n\nOperational governance \n\nThe organization's AI strategy must ultimately be operationalized \n\nthroughout the business through the development of: \n\n→ Performance monitoring protocols to ensure systems perform \n\nadequately for their intended purposes. \n\n→ Transparency and human oversight initiatives to ensure \n\nindividuals are aware and can make informed choices when they \n\ninteract with AI systems or when AI-powered decisions are made. \n\n→ Incident management plans to identify, escalate and respond to \n\nserious incidents, malfunctions and national risks impacting AI \n\nsystems and their operation. \n\n→ Communication strategies to ensure transparency toward \n\ninternal and external stakeholders in relation to the \n\norganization's AI practices. \n\n→ Training and awareness programs to enable staff with roles and \n\nresponsibilities for AI governance to help them understand and \n\nperform their respective roles. \n\n→ Skills and capabilities development to assess human resources \n\ncapabilities and review or design job requirements. → Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 13 \n\nTABLE OF CONTENTS  ↑\n\nAn effective AI governance \n\nmodel is about collective \n\nresponsibility and collective \n\nbusiness responsibility, \n\nwhich should encompass \n\noversight mechanisms such \n\nas privacy, accountability, \n\ncompliance, among others. \n\nThis responsibility should be \n\nshared by every stakeholder \n\nwho is part of the AI \n\ngovernance chain. \n\nVishal Parmar \n\nBritish Airways Global Lead Privacy Counsel \n\nand Data Protection Officer \n\nUnderstanding that AI systems, like all \n\nproducts, follow a life cycle is important as \n\nthere are governance considerations across \n\nthe life cycle. The  NIST AI RMF  sets out a \n\ncomprehensive articulation of the AI system \n\nlife  cycle and includes considerations for \n\ntesting, evaluation, validation, verification \n\nand key stakeholders for each phase. A more \n\nsimplified sample life cycle is included above, \n\nalong with some top-level considerations. \n\nThe AI life cycle \n\nPLANNING \n\n→ Plan and document the \n\nsystem's concept and \n\nobjectives. \n\n→ Plan for legal and \n\nregulatory compliance.  DESIGN \n\n→ Gather data and check \n\nfor data quality. \n\n→ Document and \n\nassess metadata and \n\ncharacteristics of \n\nthe  dataset. \n\n→ Consider legal and \n\nregulatory requirements. \n\nDEVELOPMENT \n\n→ Select the algorithm. \n\n→ Train the model. \n\n→ Carry out testing, \n\nvalidation and \n\nverification. \n\n→ Calibrate. \n\n→ Carry out output \n\ninterpretation. \n\nDEPLOYMENT \n\n→ Pilot and perform \n\ncompatibility checks. \n\n→ Verify legal and \n\nregulatory compliance. \n\n→ Monitor performance \n\nand mitigate risks post \n\ndeployment. → Part I. Understanding AI and governance    \n\n> AI Governance in Practice Report 2024 |14\n> TABLE OF CONTENTS ↑\n\n## → HOW TO \n\n## Navigate developers from deployers \n\nVarious contractual and regulatory obligations may arise \n\ndepending on whether an organization is a vendor or buyer, \n\nor  if it sources external services such as hardware, cloud or  data \n\ncollection, for the development and operations of  its  AI system. \n\nPrior IAPP  research  found more than 70% of organizations \n\nrely  at least somewhat on third-party AI, so the responsibility \n\nfor ensuring the AI system is safe and responsible may be \n\nspread  across multiple roles. \n\nIn both  current legislation  and  proposed legislation  we are starting to see \n\ndifferent obligations for those who provide and supply AI versus those who \n\ndeploy AI. Understanding whether you are a developer and/or deployer is \n\nimportant to ensuring you meet compliance obligations. Once this is understood, \n\nit is possible to establish AI-governance processes for  procurement , including \n\nevaluations and contracts to avoid taking on additional liabilities. \n\n→ The World Economic Forum put together a useful  toolkit \n\nto help those who are procuring AI systems. AI Governance in Practice Report 2024  | 15 \n\n# Part II. \n\n# The data \n\n# challenge \n\n## Data is an integral part of training \n\n## and operating an AI system. \n\nMost AI requires sizeable amounts of high-quality data, \n\nespecially during the training phase to maximize the model's \n\nperformance, as well as to ensure the desired and accurate \n\noutput. With the advancement of new AI technologies, models \n\nare requiring increasingly more data, which may come from \n\na variety of sources. Given the importance of the data used to \n\nfuel the AI system, it is important to understand what data is \n\nbeing used; how, where and by whom it was collected; from \n\nwhom it was collected; if it is the right data for the desired \n\noutcome; and how it will be managed throughout the life cycle. \n\nAccessing data and identifying data  sources \n\nUnderstanding where data comes from and how it is collected is \n\nnot only necessary for AI systems, but also for building trust in AI \n\nby ensuring the lawfulness of data collection and processing. Such \n\ndocumentation can assist with  data transparency  and improve the \n\nAI system's auditability as well. \n\nAlthough data may originate from multiple sources, it can be \n\nbroadly categorized into three  types: first-party data, public \n\ndata  and third-party data. → Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 16  \n\n> TABLE OF CONTENTS ↑\n\nFirst-party data \n\nThis refers to data collected directly from \n\nindividuals by an organization through their own \n\ninteractions and transactions. Such data may \n\noriginate from sources such as website visits, \n\ncustomer feedback and surveys, subscriptions, \n\nand customer relationship management systems, \n\namong others. This data is extremely valuable for \n\norganizations as it provides direct and firsthand \n\ninsights into individuals' behavior. \n\nFirst-party data can be collected from various \n\nsources. Identifying the data channels and \n\ndocumenting the source will not only help the \n\norganization determine what types of data, e.g., \n\ntext, numerical, image or audio, will be collected \n\nfrom each source, but also alert the legal team \n\nabout where legal compliance will be required \n\non  the organization's part. \n\nPublic data \n\nThis refers to data that is available to the wider \n\npublic and encompasses a range of sources, \n\nsuch as publicly available government records, \n\npublications, and open source and web-scraped \n\ndata. Public data is a valuable resource for \n\nresearchers and innovators as it provides readily \n\navailable information. Public data can come \n\nfrom multiple sources. \n\nWhile it is arduous and cumbersome to \n\nmaintain data lineage for public datasets, \n\nit  is important for upholding organizational \n\nreputation and fostering user trust, legal \n\ncompliance and AI safety overall. A lack of \n\nunderstanding of where data comes from \n\neventually leads to a lack of understanding of \n\nthe training dataset and model performance, \n\nwhich can reinforce the black-box problem. \n\nTherefore, in the interest of transparency, \n\ntracking and documenting public-data sources \n\nas much as possible may prove beneficial for \n\nthe organization, as it can later support other \n\ntransparency efforts, such as drawing up data, \n\nmodel or system cards. \n\nMoreover, without knowledge of public-data \n\nsources, the organization may inadvertently \n\ntrain the AI system on personal, sensitive \n\nor proprietary data. From the privacy \n\nstandpoint, this can be problematic in cases \n\nof  data leakage , where personally identifiable \n\ndata may be exposed. AI security challenges \n\nmay also be amplified if data was procured \n\nfrom unsafe public sources, as that carries \n\nthe risk of introducing malicious bugs into \n\nthe system. It may also lead to biases in \n\nthe  AI system. \n\nEthical development \n\npractices start with \n\nresponsible data \n\nacquisition and \n\nmanagement systems, \n\nas well as review \n\nprocesses that track \n\nthe lineage of \n\nsourced data. \n\nChristina Montgomery \n\nIBM Vice President and \n\nChief Privacy and Trust Officer → Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 17  \n\n> TABLE OF CONTENTS ↑\n\nAn organization can begin by establishing a \n\nclear understanding of how and why public \n\ndata is being collected, how it aligns with the \n\npurposes the AI system will fulfil, if and how \n\nsystem accuracy will be affected by using \n\npublic data, what the trustworthy sources for \n\ngathering public data are, if the organization \n\nhas rights to use the public data, and other legal \n\nconsiderations that may have to be taken into \n\naccount, particularly given that public data is \n\ntreated differently across jurisdictions. \n\nThird-party data \n\nThis refers to data obtained or licensed by the \n\norganization from external entities that collect \n\nand sell data, such as data brokers. Datasets \n\npurchased from brokers are webbed together \n\nfrom a wide range of sources. While this may \n\nhave the benefit of providing insights into \n\na wider user base, the insights may not be \n\naccurate or may be missing key data. It may \n\nlack direct insights into customer behavior, as \n\nbrokers do not interact with the organization's \n\ncustomer base. \n\nThird-party data can also include open-source \n\ndata, available through open-source data \n\ncatalogues. Sometimes these databases are \n\nprovided by government or academic institutions \n\nwith a clear understanding of how the data was \n\ncollected and how it can be used, including a \n\nclear use license. Open-source data collected \n\nthrough other community efforts may not follow \n\nthe same collection and distribution practices. \n\nAs when using all data, it is important to know \n\nwhere the data came from, how it was collected, \n\nin which context it is meant to be used and what \n\nrights you have to use it. \n\nData quality \n\nThe quality of data that AI is trained and tested \n\non directly impacts the quality of the outputs and \n\nperformance, so ensuring the data is high quality \n\ncan help lay the initial foundations for a safe and \n\nresponsible AI system. Measuring  data quality \n\noften includes a few  baseline considerations .\n\nAccuracy confirms the correctness of data. \n\nThat is, whether the data collected is based \n\non real-world insights. Completeness refers to \n\nchecking for missing values, determining the \n\nusability of the data, and looking for any over \n\nor underrepresentation in the data sample. \n\nValidity ensures data is in a format that is \n\ncompatible with intended use. This may include \n\nvalid data types, metadata, ranges and patterns. \n\nConsistency refers to the relationships between \n\ndata from multiple sources and includes \n\nchecking if the data shows consistent trends \n\nand values it represents. Ideally, this process of \n\nensuring data quality is documented to support \n\ntransparency, explainability, data fairness, \n\nauditability, understanding of the data phase \n\nof  the life cycle and system performance. \n\nWithout understanding the quality \n\nof the data being ingested into an \n\nAI model, you may not know the \n\nquality of the output. Companies \n\nmust establish and define what ‘data \n\nquality’ involves and consists of, as this \n\ndetermination is highly contextual for \n\nany organization, and can depend on \n\nbusiness goals, use cases, focus areas \n\nand fitness for purpose. \n\nRegardless of context, there are \n\nminimum baseline attributes which can \n\nand should be established: accuracy, \n\ncompleteness, consistency and \n\nvalidity. Timeliness and uniqueness \n\nmay also be important to establishing \n\nfitness for purpose. \n\nDera Nevin \n\nFTI Technology Managing Director → Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 18 \n\nTABLE OF CONTENTS  ↑\n\nAppropriate use \n\nOne of the most significant challenges when designing and \n\ndeveloping AI systems is ensuring the data used is appropriate \n\nfor the intended purpose. Often data is collected with one \n\nintention in mind or within a specific demographic area, and, \n\nwhile it might appear to be a useful dataset, upon further \n\nanalysis it might include data that does not match the industry or \n\ngeographic area of operation. When data is not fit for purpose, it \n\ncan skew the AI system's predictions or outcomes. \n\nWhen thinking about appropriate use, consider the \n\nproportionality of data required for the desired outcome. \n\nOften,  there are occurrences of collecting or acquiring more \n\ndata than necessary to achieve the outcome. It is important \n\nto understand if it is even necessary to collect and use certain \n\ndata  in your AI system. \n\nManaging unnecessary data, especially data that may contain \n\nsensitive attributes, can increase an organization's risk of a \n\nbreach or harm resulting from the use of AI. \n\nLaw and policy considerations \n\nApproaches can be categorized according to how the \n\ndata  was  collected. \n\n# Managing unnecessary data \n\n# can increase an organization's \n\n# risk of a breach or harm \n\n# resulting from the use of AI. → Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 19  \n\n> TABLE OF CONTENTS ↑\n\nFirst-party data \n\nWhere first-party data amounts to personal or \n\nsensitive data, relevant provisions may be triggered \n\nunder the data protection and privacy legislation of \n\nthe jurisdictions where the organization carries out \n\nits business, where the processing takes place or \n\nwhere the individuals concerned are located. \n\nThe EU General Data Protection Regulation, \n\nfor instance, has a default prohibition against \n\nprocessing of personal data, unless such \n\nprocessing falls under one of the  six bases  for \n\nlawful processing under Article 6(1): consent, \n\ncontractual performance, vital interest, legal \n\nobligation, public task and legitimate interest \n\npursued by a controller or third party. \n\nPublic data \n\nWeb scraping may involve compliance with the \n\nterms of service and privacy policies of websites. \n\nOtherwise, when an organization is aware the \n\npublic dataset contains personal or sensitive \n\ninformation, lawfulness of use may require \n\ncompliance with relevant data protection or \n\nprivacy laws, such as by acquiring valid consent. \n\nWhile web scraping, it is possible for copyrighted \n\ndata to be collected to train AI systems. \n\nAnother type of public data is open-source \n\ndata, which is publicly available software that \n\nmay include both code and datasets. Although \n\naccessible to the public, open-source software is \n\noften made available by the organization through \n\nvarious  open-source licensing schema . In addition \n\nto complying with the terms of the licenses, \n\norganizations using open-source data may also \n\nconsider conducting their own due diligence to \n\nensure the datasets were acquired lawfully, are \n\nsafe to use and were assessed for bias mitigation. \n\nThird-party data \n\nAs organizations have neither proximity to how \n\nthird-party data was first collected nor direct \n\ncontrol over the data governance practices of \n\nthird parties, an organization can benefit from \n\ncarrying out its own legal due diligence and \n\nthird-party risk management. The extent and \n\nintensity of this exercise will largely depend on \n\nthe organization's broader governance and risk-\n\nmanagement approach and the relevant facts. \n\nLegal due diligence may include verification \n\nof the personal data's lawful collection by the \n\ndata broker, review of contractual obligations \n\nand licenses, and identification of protected \n\nintellectual property interests. When data is \n\nlicensed, the organization will first have to \n\nlawfully procure rights to use data through a \n\nlicensing agreement. This will help maintain data \n\nprovenance and a clear understanding of data \n\nownership. The lawful and informed use of such \n\ndata at subsequent stages of the AI life cycle will \n\nalso be governed by the license. \n\nWith growing public concerns \n\nand increased regulation aimed \n\nat developing trustworthy, \n\ntransparent and performative \n\nAI systems, an internal data \n\ngovernance program is \n\nintegral to understanding and \n\ndocumenting metadata prior to \n\nusage, and to identifying risks \n\nassociated with lawful data use. \n\nChristina Montgomery \n\nIBM Vice President and \n\nChief Privacy and Trust Officer → Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 20  \n\n> TABLE OF CONTENTS ↑\n\n## → SPOTLIGHT \n\n## Joint statement by international data protection and privacy authorities on web scraping \n\nIn August 2023, 12 international data protection and privacy authorities released a  joint statement  to address data scraping on \n\nsocial media platforms and other publicly accessible websites. \n\nThe joint statement outlined: \n\n→ Key privacy risks associated with data scraping, such as targeted \n\ncyberattacks, identity fraud, monitoring and profiling individuals, \n\nunauthorized political or intelligence gathering, and unwanted direct \n\nmarketing or spam. \n\n→ How social media companies and other websites should protect \n\nindividuals' personal information from unlawful data scraping, such \n\nas through data security measures and multilayered technical and \n\nprocedural controls to mitigate the risk. \n\n→ Steps individuals can take to minimize the privacy risks of scraping, \n\nincluding reading a website's privacy policy, limiting information \n\nposted online, and understanding and managing privacy settings. \n\nSome key takeaways from the joint statement include: \n\n→ Publicly accessible personal information is still subject to data \n\nprotection and privacy laws in most jurisdictions. \n\n→ Social media companies and other website operators hosting publicly \n\naccessible personal data have legal obligations to protect personal \n\ninformation on their platforms from unlawful data scraping. \n\n→ Accessing personal information through mass data scraping can \n\nconstitute reportable data breaches in many jurisdictions. \n\n→ Individuals can take steps to prevent their personal information from \n\nbeing scraped, and social media companies have a role to play in \n\nempowering users to engage with social media services in a manner \n\nthat upholds privacy. → Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 21  \n\n> TABLE OF CONTENTS ↑\n\nImplementing AI governance \n\nNumerous strategies are being leveraged to \n\nmanage data in the context of AI. \n\nData management plans \n\nAlongside ensuring the lawfulness of data \n\nacquisition, there are numerous measures an \n\norganization can take to keep track of where the \n\ndata used to train AI systems comes from. Such \n\norganizational practices are especially important \n\nwith the advent of generative AI, where training \n\ndata is merged from numerous sources. \n\nDeveloping a comprehensive plan for how data is \n\nmanaged across an organization is a foundational \n\nelement to managing all AI systems. Some \n\nconsiderations for data management plans include \n\nunderstanding what data is being used in which \n\nsystem; how it is collected, retained and disposed; \n\nif there is lawful consent to use the data; and who is \n\nresponsible for ensuring the appropriate oversight. \n\nIt is likely your organization is already keeping \n\ntrack of the data used across the organization. \n\nWhile there are additional considerations involved \n\nwhen using data for AI systems as discussed above, \n\nit is possible to add to your existing data workflows \n\nor management practices. It is important to \n\nconsider the use and management of data used for \n\nAI systems at every stage of the life cycle as there \n\nare different concerns and implications to consider \n\nduring different stages. If your organization does \n\nnot already have a data management practice, \n\nresources such as those from  Harvard Biomedical \n\nData Management  can help you get started. \n\nAdditionally, the data management plan should \n\nidentify relevant data standards, such as  ISO \n\n8000  for data quality, to set appropriate controls \n\nand targets for your organization to meet. Data \n\nstandards for aspects of AI are under development \n\nthrough various initiatives at the NIST, ISO/IEC \n\nand other national standards bodies. \n\nIBM believes it is essential for \n\ndata management practices \n\ntied to AI development to \n\ninclude advanced filtering \n\nand curation techniques \n\nto identify untrustworthy, \n\nprotected/sensitive, explicit, \n\nbiased/nonrepresentative or \n\notherwise unwanted data. \n\nChristina Montgomery \n\nIBM Vice President and \n\nChief Privacy and Trust Officer → Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 22  \n\n> TABLE OF CONTENTS ↑\n\nData labels \n\nGrowing in importance,  data labels  are tools that can require \n\norganizations to provide information on how data was collected and \n\nused to train AI models. They are transparency artifacts of AI datasets \n\nthat explain the processes and rationale for using certain data and \n\nexplain how it was used in training, design, development and use. \n\nThis will help explain if the data being used is fit for purpose, if it is \n\nrepresentative of the demographics being served with the AI system \n\nand if the data meets relevant data quality standards. \n\nIdeally data labels are requirements of a robust data management \n\nprocess, which includes data quality and data impact assessments. \n\nWhile data labels are intended to provide documentation and \n\nawareness of the data being used, they can also assist with the \n\nassessment and review process. These tools should be aligned \n\nwhere  possible within the organization to avoid redundant efforts. \n\nData-source maintenance through documentation and inventories \n\ncan help organizations keep track of where the data is acquired \n\nand carry out relevant legal due diligence at first-party or \n\nthird-party levels. \n\nDedicated processes and functions \n\nWhen third-party data is used, it is important to follow the terms of \n\nservice and provide attribution where possible. This will also help \n\ninform users of the AI system where the data originated. Where \n\npossible, when data is being used from a third party, an appropriate \n\ndata sharing agreement with clear terms of use for both parties is \n\nhighly recommended. This helps to resolve any liability issues that \n\nmay arise as a result of using the system. \n\nINDUSTRY EXAMPLE \n\nWhen third-party organizations use publicly \n\navailable data, processes can be put into \n\nplace. Meta's  External Data Misuse  team \n\ndetects, blocks and deters web scraping. \n\nSome actions taken by the EDM team include \n\ndisabling accounts, serving cease-and-desist \n\nnotices, using CAPTCHAs for bot detection \n\nand blocking IP addresses where data \n\nscraping is identified. OpenAI has put in place \n\nan  opt-out process  for organizations that do \n\nnot want GPTbot to access their websites for \n\nthe purpose of web crawling. AI Governance in Practice Report 2024  | 23 \n\n# Part III. \n\n# The privacy and \n\n# data protection \n\n# challenge \n\nGiven that AI is a data-dependent enterprise and that privacy \n\nlaw governs the processing of personal data,  privacy laws  have \n\nemerged as a prominent mechanism for managing the key AI \n\ngovernance challenges. After all, information privacy seeks to \n\nprovide a framework \" for making ethical choices about how \n\nwe use new technologies .\"\n\nIndeed, national  data protection authorities  have been \n\namong the first to intervene and bring enforcement actions \n\nwhen AI-based products were thought to harm consumers. \n\nFor example, Italy's data protection authority, the Garante, \n\nimposed a  temporary ban  on ChatGPT after concluding the \n\nservice was in violation of the GDPR for lacking a legal basis \n\nfor processing and age-verification mechanism. \n\n## Privacy and data protection \n\n## governance practices are woven \n\n## into the AI life cycle. \n\nThe enforcement landscape for AI governance is \n\nincredibly unsettled. Which regulators will lead on what \n\nand how they will collaborate or conflict is subject \n\nto heavy debate and will differ by country, creating \n\nheightened uncertainty for organizations. Whether or \n\nnot privacy regulators have the lead remit, they will play \n\na key role given the centrality of data to AI governance. \n\nCaitlin Fennessy \n\nIAPP Vice President and Chief Knowledge Officer → Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 24  \n\n> TABLE OF CONTENTS ↑\n\nLaw and policy considerations \n\nThe OECD's  Guidelines Governing the \n\nProtection of Privacy and Transborder Flows \n\nof Personal Data  — developed in 1980 and \n\nrevised in 2013 — enshrine eight principles that \n\nhave served as the  foundation  for most global \n\nprivacy and data protection laws written over \n\nthe past several decades, including landmark \n\nlegislation such as the GDPR. These eight \n\nprinciples include collection limitation, data \n\nquality, purpose specification, use limitation, \n\nsecurity safeguards, openness, individual \n\nparticipation and accountability. \n\nMany DPAs around the world already put forth \n\nguidance  on how AI systems can work to align \n\nthemselves with these foundational principles of \n\ninformation privacy. Yet, as Australia's Office of \n\nthe Victorian Information Commissioner noted \n\nin a  resource  on issues and challenges of AI and \n\nprivacy, \"AI presents challenges to the underlying \n\nprinciples upon which the (OECD Privacy) \n\nGuidelines are based.\" To better understand \n\nwhere these challenges currently exist, each of \n\nthese principles is discussed below in the context \n\nof their applicability to — and potential conflict \n\nwith — the development of AI. \n\nCollection limitation \n\nThe principle of collection limitation states, \n\n\"There should be limits to the collection \n\nof personal data and any such data should \n\nbe obtained by lawful and fair means and, \n\nwhere appropriate, with the knowledge or \n\nconsent of the data subject.\" It most readily \n\ntranslates to the concept and practice of data \n\nminimization. GDPR Article 5(1)(c) emanates \n\nfrom this idea that data, at the collection stage, \n\nshould have some predefined limit or upper \n\nbound. Specifically, data collection should be \n\n\"... limited to what is necessary in relation to \n\nthe purposes for which they are processed.\" \n\nAs many observers have noted, this is one of \n\nthe privacy principles for which there appears \n\nto be an \" inherent conflict \" with AI systems \n\nthat rely on the collection and analysis of large \n\ndatasets. Performing adequate AI  bias testing ,\n\nfor example, requires collecting more data than \n\nmight otherwise be collected. \n\nAt Mastercard, we are testing \n\ninnovative tools and technologies \n\nto address some of the potential \n\ntensions between privacy and \n\nAI governance. For instance, we \n\nknow that a lot of data is needed, \n\nincluding sometimes sensitive \n\ndata, for AI to produce unbiased, \n\naccurate and fair outcomes. \n\nHow do you reconcile this with the \n\nprinciple of data minimization and \n\nthe need for individual's explicit \n\nconsent? We are exploring how \n\nthe creation of synthetic data can \n\nhelp, so as to achieve all desired \n\nobjectives at the same time. \n\nCaroline Louveaux \n\nMastercard Chief Privacy and Data Responsibility Officer → Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |25\n> TABLE OF CONTENTS ↑\n\nData quality \n\nThis is the principle that \"Personal data should be relevant to the \n\npurposes for which they are to be used, and, to the extent necessary for \n\nthose purposes, should be accurate, complete and kept up-to-date.\" Data \n\nquality is the privacy principle with which AI may be most in synchrony. \n\nThe  accuracy  of AI model outputs depends significantly on the quality of \n\ntheir inputs. A breakdown in  AI governance  can lead to data becoming \n\ninconsistent and error-laden, underscoring the need for AI-based \n\nsystems to orient themselves around the principle of data quality. Data \n\nbrokers and other companies can become the target of  enforcement \n\nactions  for failing to ensure the accuracy of the data they collect and sell. \n\nPurpose specification \n\nThe principle of purpose specification states, \"The purposes for which \n\npersonal data are collected should be specified ... and the subsequent \n\nuse limited to the fulfilment of those purposes ...\" Indeed, as the \n\nU.K. Information Commissioner's Office explained in the context \n\nof its  consultation  on purpose limitation in the generative AI life \n\ncycle, purposes of data processing \"must be specified and explicit: \n\norganizations need to be clear about why they are processing personal \n\ndata.\" This need for clarity applies not only to internal documentation \n\nand governance structures, but in communication with the people to \n\nwhom the personal data relates. In sum, organizations should be able \n\nto explain what personal data they process at each stage and why it is \n\nneeded to meet the specified purpose. \n\nA conflict with the purpose specification principle can arise if and \n\nwhen a developer wants to use the same training dataset to train \n\nmultiple models. The ICO advises developers reusing training data to \n\nconsider whether the purpose of training a new model is compatible \n\nwith the original purpose of collecting the training data. Considering \n\nthe reasonable expectations of those whose data is being reused can \n\nhelp an organization make a compatibility assessment. Currently, the \n\nICO considers collating repositories of web-scraped data, developing \n\na generative AI model and developing an application based on such a \n\nmodel to constitute different purposes under data protection law. → Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 26  \n\n> TABLE OF CONTENTS ↑\n\nUse limitation \n\nRelated to purpose specification, use \n\nlimitation  is the principle that states personal \n\ndata \"should not be disclosed, made available \n\nor otherwise used for purposes other than \n\nthose specified,\" except with the consent of \n\nthe data subject or by the authority of law. \n\nPurposes of use  must be specified at or before \n\nthe time of the collection, and subsequent \n\nuses must not be incompatible with the initial \n\npurposes of collection. \n\nThis is another principle that is challenged by \n\nAI systems, with potential  regulatory gaps  left \n\nby both the EU GDPR and EU AI Act. Proposals \n\nto address these gaps have included restricting \n\nthe training of models only to stated purposes \n\nand requiring alignment between training data \n\ncollection and the purpose of a model. \n\nSecurity safeguards \n\nUniting the fields of privacy, data protection \n\nand cybersecurity for decades is the principle \n\nthat \"Personal data should be protected \n\nby reasonable security safeguards against \n\nsuch risks as loss or unauthorized access, \n\ndestruction, use, modification or disclosure of \n\ndata.\" Ensuring the security of personal data \n\ncollected and processed is a key to building and \n\nmaintaining trust within the digital economy. \n\nRemedying problems of security and safety \n\nis and will remain a critical challenge for AI. \n\nEnsuring the actions of an AI system align \n\n\"with the values and preferences of humans\" is \n\ncentral to keeping these systems  safe . Yet, many \n\nAI systems remain susceptible to hacking and \n\nso-called \" adversarial attacks ,\" which are inputs \n\ndesigned to deceive an AI system, as well as data \n\npoisoning, evasion attacks and model extraction. \n\nExamples include forcing chatbots to provide \n\nanswers to responses to  harmful prompts \n\nor getting a self-driving vehicle's cameras to \n\nmisclassify a stop sign as a speed-limit sign. \n\nOpenness \n\nThe  right to be informed  and the principle of \n\ntransparency  are touchstones of global privacy \n\nand data protection laws. Beginning at the \n\ncollection stage and enduring throughout the life \n\ncycle of processing, these rights form the basis \n\nof organization's transparency obligations. They \n\noften require organizations to disclose various \n\ntypes of information, from the types of data \n\ncollected and how it is used to the availability of \n\ndata subjects' rights and how to exercise them to \n\nthe logic involved and potential consequences of \n\nany automated decision-making or profiling the \n\norganization engages in. The \"black-box\" nature \n\nof many AI systems can make this principle \n\nchallenging to navigate and adhere to. \n\nAs all AI and machine learning \n\nmodels are 100% data dependent, \n\nthe models must be fed high-\n\nquality, valid, verifiable data with \n\nthe appropriate velocity. As obvious \n\nas that may be, the challenges \n\naround establishing the governance \n\nrequirements that ensure the \n\nappropriate use of private data \n\nmay be far more complex. Modelers \n\nshould absolutely be applying the \n\nminimization principle of identifiable \n\ndata as they train. Adding private \n\ndata that could leak or cause bias \n\nneeds to be thought through early in \n\nthe design process. \n\nScott Margolis \n\nFTI Technology Managing Director → Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |27\n> TABLE OF CONTENTS ↑\n\nIndividual rights \n\nIndividual rights in privacy law commonly include the rights to \n\naccess, opt in/opt out, erasure, rectification and data portability, \n\namong others. Many privacy laws contain rights for individuals to \n\nopt-out of automated decision-making underpinned by AI systems. \n\nAccountability \n\nAccountability is arguably one of the most important principles when \n\nit comes to operationalizing organizational governance. Accountability \n\nis based on the idea that there should be a person and/or entity that is \n\nultimately responsible for any harm resulting from the use of the data, \n\nalgorithm and AI system's underlying processes. \n\nImplementing AI governance \n\nThe practice and professionalization of AI governance is a highly \n\nspecialized, stand-alone field requiring multidisciplinary expertise. \n\nA holistic approach to AI governance requires support from \n\nestablished subject-matter areas, including data protection and \n\ninformation governance practitioners. Data from past IAPP  research \n\nshows 73% of organizations are leveraging their existing privacy \n\nexpertise to manage AI governance. This is not surprising, as data is \n\na critical component of AI. Good AI governance weaves privacy and \n\ndata governance practices into the AI life cycle alongside AI-specific \n\nissues. This chapter demonstrates the overlapping nature of privacy \n\nand AI governance. \n\nApproaching the implementation of AI governance by adapting \n\nexisting governance structures and processes enables organizations \n\nto move forward quickly, responsibly and with minimal disruption to \n\ninnovation and the wider business. Target processes that may already \n\nbe established by organization's data protection program include: \n\naccountability, inventories, privacy by design and risk management. → Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |28\n> TABLE OF CONTENTS ↑\n\nAccountability \n\nPrivacy compliance programs are likely to \n\nhave established roles and responsibilities for \n\nthose with direct and indirect responsibility for \n\nprivacy compliance. These are likely supported \n\nby policies and procedures to help individuals \n\nfulfil the expectations of their role. Senior \n\nmanagement contributions are likely channeled \n\nthrough privacy committees, with mechanisms in \n\nplace to support risk-based escalation, reporting \n\non key metrics and decision-making. \n\nPrivacy leaders often have a direct line to CEOs \n\nand boards of directors, as well as a matrixed \n\nstructure of privacy champions across the \n\norganization to enable a multidisciplinary \n\napproach to privacy governance and ensure data \n\nprotection needs are considered by product and \n\nservice teams. This structure is well-suited to, and \n\ncan be leveraged for, AI governance given the need \n\nfor leadership engagement and skills spanning \n\nlegal, design, product and technical disciplines. \n\nWhere AI systems process personal data, \n\nthose  with accountability for privacy \n\ncompliance will need to ensure their existing \n\nprivacy compliance processes are set up to \n\naddress the intersection between AI and privacy. \n\nThis will include considering data inventory, \n\ntraining, privacy by design and other topics \n\nfurther outlined in this section. \n\nInventories \n\nPersonal data inventories have long been \n\nthe foundation of establishing a successful \n\nprivacy program and a key requirement of \n\nprivacy regulations. Knowing your data, \n\nhow it is collected and used, and being able \n\nto demonstrate this remains a core part of \n\naccountability. Organizations have also matured \n\nin their approaches, from lengthy spreadsheets \n\nto technology-enabled approaches. \n\nWhere AI systems use personal data, the data \n\ninventory can play a crucial role. Organizations \n\nthat have captured additional privacy compliance \n\nmetadata alongside the minimum regulatory \n\nrequirements may find their personal data \n\ninventories particularly useful in the age of AI. \n\nAdditional uses of this metadata could include a \n\nsingle source of truth for lawful basis to identify \n\nif additional use within AI models is permitted, \n\naccuracy metrics on personal data to support AI \n\nmodels to make accurate inferences based on \n\nthe latest personal data and a top-down view on \n\nprocesses relying on automated decision-making \n\nthat can be aligned with AI registries. → Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 29  \n\n> TABLE OF CONTENTS ↑\n\nEffective AI governance is underpinned by \n\nAI inventories with similar functionalities to \n\nthose of data inventories. AI registers can help \n\norganizations keep track of their AI development \n\nand deployment. Some functional requirements \n\nthat overlap with data inventories include the \n\nability to connect into the system-development \n\nlife cycle, maintenance and regular updates \n\nby  multiple users, and logging capability \n\nto  ensure integrity. \n\nPrivacy by design \n\nBy embedding privacy at the outset, privacy \n\nby design continues to be a critical part of \n\nhow organizations address privacy concerns. \n\nIn implementing privacy by design, privacy \n\nfunctions may take steps to map and embed \n\nprivacy into areas such as system-development \n\nlife cycles, project initiation and development \n\napproaches within an organization, risk \n\nmanagement and approval workflows, \n\nand  stage  gates. \n\nSteps may include developing AI-specific \n\nrisk-assessment workflows into existing \n\nrisk-assessment processes, enhancing existing \n\ncontrol catalogs with AI and privacy controls, \n\nor updating approval workflows to include \n\nstakeholders with AI accountabilities. \n\nAdditionally, the growing maturity of privacy \n\nenhancing technologies and their increasing \n\ntraction as technical measures within \n\norganizations may have benefits for the \n\ndevelopment of AI. With some PETs potentially \n\nhelping organizations reduce inherent risk \n\nof data use, an organization may be able to \n\nmaximize the strategic use of its data. Examples \n\ninclude using differential privacy in training \n\nmachine-learning models, federated learning \n\nand synthetic data. \n\nRisk management \n\nThe risk-based approach often adopted by \n\nglobal privacy regulations has been distilled into \n\norganizational risk-management efforts, which \n\nput privacy impact assessments at the heart of \n\ndeciding whether an organization can reduce \n\nharm from personal data processing through the \n\nimplementation of organizational and technical \n\nmeasures. Privacy risk can also stem from wider \n\nprivacy compliance activities and lessons learned \n\nin areas such as vendor risk, incident management \n\nand data subject requests management. \n\nLegal professionals need to keep an \n\nopen and flexible mind — technology \n\nbrings new challenges but also \n\nnew solutions. General counsel \n\nshould position themselves as \n\nthe center of a multidisciplinary \n\nteam of stakeholders across \n\ntheir organizations, including \n\nproduct design, compliance, data \n\nand privacy, which can deploy \n\nto manage multifaceted data \n\nrisks. Companies that strive for \n\nestablished best privacy practice \n\nwill more easily be able to comply \n\nwith the rising standards of global \n\nprivacy laws. \n\nTim de Sousa \n\nFTI Technology, Managing Director, Australia → Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 30 \n\nTABLE OF CONTENTS  ↑\n\nPrivacy risk may already feed into wider enterprise risk-\n\nmanagement programs, such as information technology and \n\ncybersecurity risk and control frameworks. These can be enhanced \n\nto accommodate the complex types and sources of AI risk into \n\na unified risk-management framework at the enterprise level. \n\nThis approach can also facilitate crucial visibility across different \n\nsubject-matter practice areas across the business and enable a more \n\neffective analysis and treatment of  AI  risk. \n\nAs AI risk-management approaches mature, AI governance \n\nprofessionals face choices between embedding algorithmic impact \n\nassessments alongside or within PIAs. The need to align AI risk \n\nmanagement with broader enterprise risk-management efforts is \n\nof  equal importance. AI governance professionals will likely need \n\nto update enterprise risk-management strategies and frameworks to \n\nclearly factor in AI-related risks and document ongoing AI risks and \n\nremediations in a formal risk register. \n\nRisk-assessments \n\nA wide range of AI risk assessments are often talked about in the \n\nemerging global AI governance landscape. \n\nSome of these assessments are required by existing data \n\nprotection legislation, such as the GDPR, while others may \n\nemerge from AI-specific laws, policies and voluntary frameworks. \n\nFor the latter, laws and policies often provide AI governance \n\nsolutions with knowledge of the overlap. \n\n# Privacy risk may \n\n# already feed into wider \n\n# enterprise risk management \n\n# programs, such as IT and \n\n# cybersecurity risk \n\n# and control frameworks. → Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |31\n> TABLE OF CONTENTS ↑\n\n## → SPOTLIGHT \n\n## AI governance assessments: A closer look at EU DPIAs and FRIAs \n\nGDPR: DPIAs \n\nData protection impact assessments are required \n\nunder GDPR Article 35. DPIAs are particularly \n\nimportant where systematic and extensive evaluation \n\nof personal or sensitive aspects of natural persons \n\nthrough automated systems or profiling leads to legal \n\nconsequences for that person. Incorporating these \n\nassessments within the AI-governance life cycle can \n\nhelp organizations identify, analyze and minimize \n\ndata-related risks and demonstrate accountability. \n\nDPIAs at a minimum contain: \n\n→ A systematic description of the anticipated \n\nprocessing, its purpose and pursued legitimate \n\ninterest. \n\n→ A necessity and proportionality assessment in \n\nrelation to the intended purpose for processing. \n\n→ An assessment of the risks to fundamental \n\nrights and freedoms. \n\n→ Measures to be taken to safeguard security \n\nrisks and protect personal data. \n\nEU AI Act: FRIAs \n\nUnder the EU AI Act, FRIAS are required to be carried \n\nout in accordance with Article 27 by: \n\n→ Law enforcement when they use real-time \n\nremote biometric identification AI systems, which \n\nare a prohibited AI practice under Article 5. \n\n→ Deployers of high-risk AI systems that are \n\ngoverned by public law, private operators that \n\nprovide public services and operators deploying \n\ncertain high-risk AI systems referred to in \n\nAnnex III, point 5 (b) and (c), such as banking or \n\ninsurance entities. \n\nFRIAs are required only for the first use of the \n\nhigh-risk AI system, and the act permits deployers \n\nto rely on previously conducted FRIAs, provided all \n\ninformation about the system is up to date. FRIAs \n\nmust consist of: \n\n→ Descriptions of the deployer's processes in \n\nline with intended use and purpose of the \n\nhigh-risk AI system. \n\n→ Descriptions of the period and frequency of the \n\nhigh-risk AI system's use. \n\n→ Categories of individuals or groups likely to be \n\naffected by the high-risk system. \n\n→ Specific risks of harm that are likely to affect \n\nindividuals or groups. \n\n→ Descriptions of the human oversight measures \n\nin place according to instructions of use. \n\n→ Measures to be taken when risk \n\nmaterializes into harm, including \n\narrangements for internal governance \n\nand complaint mechanisms. \n\nHowever, AI governance solutions often foresee \n\nthe overlap with existing practices, and this is no \n\ndifferent under the EU AI Act. FRIAs, for instance, \n\ndo not need to be conducted for aspects covered \n\nunder existing legislation. As such, if a DPIA and \n\nFRIA have an overlapping aspect, that aspect \n\nneed only be covered under DPIA. AI Governance in Practice Report 2024  | 32 \n\n# Part IV. \n\n# The transparency, \n\n# explainability and \n\n# interpretability \n\n# challenge \n\n## The black-box problem \n\nOne reason for the lack of trust associated with AI systems is \n\nthe inability of users, and often creators, of AI systems to have \n\na clear understanding of how AI works. How does it arrive at a \n\ndecision? How do we know the prediction is accurate? This is \n\noften referred to as the \" black-box problem \" because the model \n\nis either too complex for human comprehension or it is closed \n\nand safeguarded by  intellectual property. \n\nAI techniques, such as deep learning, are becoming increasingly \n\ncomplex as they learn from terabytes of data, and the number \n\nof  parameters  has grown exponentially over the years. In July \n\n2023, Meta released its  Llama 2 model  with a parameter count \n\nat 70 billion. Google's  PaLM  parameter count is reported to be \n\nas large as 540 billion. Due to the self-learning abilities of AI, \n\nincluding their size and complexity, the black-box problem is \n\nincreasingly difficult to solve and often requires a trade-off to \n\nsimplify aspects of the system. \n\nTransparency is a term of broad scope, which can include the \n\nneed for technical and nontechnical documentation across the \n\nlife cycle. Having strong product documentation in place can \n\nalso provide commercial benefits by supporting the product \n\nsales cycle and helping providers to navigate prospective \n\nclients' due diligence protocols. → Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 33 \n\nTABLE OF CONTENTS  ↑\n\nIn the open-source context, transparency can also refer to providing \n\naccess to code or datasets in the open-source community to be used \n\nby AI systems. Transparency objectives can also include informing \n\nusers when they are interacting with an AI system or identifying \n\nwhen content was AI generated. Independent of how the term is \n\nused, transparency is a key tenet of AI governance due to the desire \n\nto understand how AI systems are built, managed and maintained. \n\nIt  is crucial that clear and comprehensive documentation is available \n\nto those who design and use these systems to ensure trust and help \n\nidentify where an error was made if an issue occurs. \n\nExplainability  refers to the understanding of how a black-box model, \n\ni.e., an incomprehensible or proprietary model, works. While useful, \n\nthe difficulty with black-box models is that the explanation may not \n\nbe entirely accurate or faithful to the underlying model, given its \n\nincomprehensibility. When full explainability is not possible due \n\nto  the factors mentioned above, an alternative is interpretability. \n\nInterpretability , on the other hand, refers to designing models that \n\ninherently make the reasoning process of the model understandable. \n\nIt encourages designing models that are not black boxes, with \n\ndecision or prediction processes that are comprehensible to domain \n\nexperts. In other words, interpretability is applied ante hoc. While \n\nit does away with the problems of explainable models, interpretable \n\nmodels are often domain specific and require significant effort to \n\ndevelop in terms of domain expertise. \n\nLaw and policy considerations \n\nOne proposed solution to the black-box challenge has been codifying \n\napproaches to and requirements for transparency, explainability and \n\ninterpretability in law or policy initiatives. Regulatory and voluntary \n\ngovernance tools that have established requirements for tackling the \n\nblack-box problem through transparency and explainability include \n\nthe EU GDPR and AI Act, NIST AI RMF, U.S. Executive Order 14110, \n\nChina's Interim Measures for the Management of Generative AI \n\nServices, and Singapore's AI Verify. \n\n# One reason for the lack of trust \n\n# associated with AI systems is the \n\n# inability for users, and often creators, \n\n# of AI systems to have a clear \n\n# understanding of how AI works. → Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 34  \n\n> TABLE OF CONTENTS ↑\n\nEU GDPR \n\nArguably one of the first legislative requirements \n\nfor AI governance,  GDPR  Articles 13(2)(f), 14(2) \n\n(g) and 15(1)(h) refer to providing meaningful \n\ninformation about the logic underpinning \n\nautomated decisions, as well as information \n\nabout the significance and envisaged \n\nconsequences of the automated decision-\n\nmaking for the individual. This is further \n\nsupported by Article 22 and Recital 71, which \n\nstate such decision-making should be subject to \n\nsafeguards , such as through the right to obtain \n\nan explanation to challenge an assessment. \n\nEU AI Act \n\nThe EU  AI Act  takes a risk-based approach to \n\ntransparency, with documentary and disclosure \n\nrequirements attaching to high-risk and general-\n\npurpose AI systems. \n\nIt mandates drawing up technical \n\ndocumentation for high-risk AI systems, and \n\nrequires high-risk AI systems to come with \n\ninstructions for use that disclose various \n\ninformation, including characteristics, \n\ncapabilities and performance limitations. \n\nTo  make high-risk AI systems more traceable, \n\nit also requires AI systems to be able to \n\nautomatically allow for the maintenance of \n\nlogs  throughout the AI life cycle. \n\nSimilarly, the AI Act places documentation \n\nobligations on providers of general-purpose \n\nAI systems with and without systemic risks. \n\nThis includes maintenance of technical \n\ndocumentation, including results from training, \n\ntesting and evaluation. It also requires  up-to-\n\ndate information and documentation to be \n\nmaintained for providers of AI systems who \n\nintend to integrate GPAI into their system. \n\nProviders of GPAI systems with systemic risks \n\nmust also publicly disclose sufficiently detailed \n\nsummaries of the content used for training GPAI. \n\nWith certain exceptions, the EU AI Act provides \n\nindividuals with the right to an explanation from \n\ndeployers of individual decision-making \"on the \n\nbasis of the output from a high-risk AI system \n\n... which produces legal effects or similarly \n\nsignificantly affects that person in a way that \n\nthey consider to have an adverse impact on \n\ntheir  health, safety or fundamental rights.\" \n\nIn addition to the documentary and disclosure \n\nrequirements, the AI Act seeks to foster \n\ntransparency by mandating machine-readable \n\nwatermarks. Article 50(2) requires machine-\n\nreadable watermarks for certain AI systems \n\nand  GPAI systems, so content can be detected \n\nas  AI generated or to inform users when they \n\nare  interacting with AI. \n\nThe EU is first out of the gate \n\nwith comprehensive AI legislation \n\nbut the EU AI Act is just the \n\ntip of the regulatory iceberg. \n\nMore guidance is coming and \n\nmany laws enacted since the \n\nearly 2000s, and under the recent \n\nEuropean Data Strategy, will have \n\nto be considered in AI governance \n\nprograms. The EU will continue \n\nto promote its approach to \n\nregulating AI on the global stage, \n\nfurthering the Brussels effect on \n\ndigital regulation. \n\nIsabelle Roccia \n\nIAPP Managing Director, Europe → Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 35  \n\n> TABLE OF CONTENTS ↑\n\nNIST AI RMF \n\nThe  NIST AI RMF  sees transparency, explainability and \n\ninterpretability as distinct characteristics of AI systems that \n\nsupport each other. Under the RMF, transparency is meant to \n\nanswer the \"what,\" explainability the \"how\" and interpretability \n\nthe  \"why\" of a decision. \n\n→ Accountability and transparency:  The RMF defines \n\ntransparency as the extent to which information about an \n\nAI system and its outputs are made available to individuals \n\ninteracting with AI, regardless of whether they are aware \n\nof it. Meaningful transparency includes the disclosure of \n\nappropriate levels of information at different stages of the \n\nAI  life cycle, tailored to the knowledge or role of the individual \n\ninteracting with the system. This could include design \n\ndecisions, the model's training data and structure, intended \n\nuse-cases, and how and when deployment, post-deployment \n\nor end-user decisions were made and by whom. The RMF \n\nrequires AI transparency to consider human-AI interaction, \n\nsuch as by  notifying the human if a potential or actual adverse \n\noutcome  is  detected. \n\n→ Explainable and interpretable AI:  The RMF defines \n\nexplainability as a representation of the underlying \n\nmechanisms of the AI system's operation, while it defines \n\ninterpretability as the meanings assigned to the AI outputs \n\nin the context of their designed functional purpose. \n\nLack  of explainability can be managed by describing how \n\nthe system functions by tailoring such descriptions to the \n\nknowledge, roles and skills of the individual, whereas lack \n\nof interpretability can be managed by describing why the \n\nAI  system gave a specific output. \n\nIt's important to align on a set of ethical AI principles \n\nthat are operationalized through tangible responsible AI \n\npractices, rooted in regulations, e.g. EU AI Act, and best \n\npractice frameworks, e.g. NIST AI RMF, when developing \n\nAI features. At Workday, we take a risk-based approach to \n\nresponsible AI governance. \n\nOur scalable risk evaluation dictates relevant guidelines \n\nsuch as requirements to map, measure, and manage \n\nunintended consequences including bias. Within the \n\nWorkday AI Feature Fact Sheets, Workday provides \n\ntransparency to customers on each feature such as, \n\nwhere relevant, how they were assessed for bias. \n\nThese safeguards are intended to document our efforts \n\nto develop AI features that are safe and secure, human \n\ncentered, and transparent and explainable. \n\nBarbara Cosgrove \n\nWorkday Vice President, Chief Privacy Officer → Part IV. The transparency, explainability and interpretability challenge    \n\n> AI Governance in Practice Report 2024 |36\n> TABLE OF CONTENTS ↑\n\nU.S. Executive Order 14110 \n\nU.S. Executive Order 14110  approaches \n\ntransparency through an AI safety perspective. \n\nUnder Section 4, the safety and security of AI \n\ntechnology is to be ensured through certain \n\ntransparency measures, such as the requirements \n\nto share results of safety tests and other \n\nimportant information with the U.S. government, \n\nthat have been imposed on developers of the \n\nmost powerful AI systems. Watermarks to label \n\nAI-generated content are also required under the \n\norder, with the purpose of protecting Americans \n\nfrom AI-enabled fraud and deception. \n\nChina's Interim Measures for the \n\nManagement of Generative AI Services \n\nArticle 10 of China's  Interim Measures for the \n\nManagement of Generative AI Services  requires \n\nproviders of AI services to clarify and disclose \n\nthe uses of the services to user groups and to \n\nguide their scientific understanding and lawful \n\nuse of generative AI. Watermarking AI-generated \n\ncontent is also a requirement under Article 11. \n\nSingapore's AI Verify \n\nSingapore's  AI Verify  is a voluntary testing \n\nframework  on AI governance for organizational \n\nuse comprised of two  parts : a testing framework \n\ngrounded in 11 internationally accepted \n\nprinciples grouped into five pillars and a \n\ntoolkit  to execute technical tests. \n\nTransparency and explainability themes \n\nare among the 11 principles embedded in \n\nAI Verify. The framework addresses the \n\ntransparency problem by providing impacted \n\nindividuals with appropriate information \n\nabout AI use in a technological system so they \n\ncan make informed decisions on whether to \n\nuse that AI enabled system. Explainability, \n\non the other hand, is achieved through an \n\nunderstanding of how an AI model reaches \n\na decision, so individuals are aware of the \n\nfactors that contributed to a resulting output. \n\nTransparency  is assessed through documentary \n\nevidence and explainability is assessed through \n\ntechnical tests. → Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 37 \n\nTABLE OF CONTENTS  ↑\n\nImplementing AI governance \n\nOrganizations have been active in coming up with tools \n\nand techniques to address the black-box transparency and \n\nexplainability challenge. \n\nModel and system cards \n\nModel cards are short documents that accompany an AI model to \n\nprovide  transparent  model reporting  by disclosing information about \n\nthe model. Information may include explanations about intended \n\nuse, performance metrics and benchmarked evaluation in various \n\nconditions such as across different cultures, demographics or race. \n\nIn addition to providing transparency, model cards are also meant to \n\ndiscourage use of models outside their  intended uses . At the industry \n\nlevel, use of model cards is becoming more prominent as evidenced \n\nby publicly accessible model cards for Meta and Microsoft's  Llama 2 ,\n\nOpenAI's  GPT-3  and Google's  face-detection model .\n\nIt may not always be easy to explain a model in a short document. \n\nModel cards are to serve a broad audience and, therefore, \n\nstandardizing explanations may prove either too simplistic for one \n\naudience or too complicated for another. Moreover, organizations \n\nshould also be mindful of how much information they reveal in the \n\ncards to prevent adversarial attacks and mitigate security risks. \n\nAI models are often part of a larger system comprised of a group \n\nof  models and technologies that work together to give outputs. \n\nAs  a result, model cards can fall short of providing a more nuanced \n\npicture of how different models interact together within the system. \n\nThat is where  system cards  can help achieve better insights. \n\n# Organizations have been active \n\n# in coming up with tools \n\n# and techniques to address \n\n# the black-box transparency \n\n# and explainability challenge. → Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 38  \n\n> TABLE OF CONTENTS ↑\n\nSystem cards explain how a group of AI models \n\nand other AI and non-AI technologies work \n\ntogether as part of an AI system to achieve \n\nspecific tasks. Meta released  22 system cards \n\nexplaining how AI powers its Facebook and \n\nInstagram platforms. Each card has four \n\nsections that detail: \n\n→ An overview of the AI system. \n\n→ How the system works by summarizing the \n\nsteps involved in creating experiences on \n\nFacebook and Instagram. \n\n→ How the shown content can be customized. \n\n→ How AI delivers content as part of the \n\nwhole  system. \n\nAI systems learn from their environments and \n\nconstantly evolve, so the way they work also \n\nchanges over time, requiring updates to the \n\nsystem cards. Like  with model cards, reducing \n\ntechnical concepts to a standardized language \n\nthat serves all audiences can be challenging for \n\nsystem cards, and system cards can also attract \n\nsecurity threats based on the amount and type \n\nof  information shared. \n\nThe utility of model and system cards can go \n\nbeyond meeting transparency challenges. \n\nMaintaining standardized records about the \n\nmodel itself can facilitate communication and \n\ncollaboration between various stakeholders \n\nthroughout the life cycle. This can also help \n\nwith bias and security-risk mitigation. They are \n\nalso useful for making comparisons with future \n\nversions of the models to track improvements. \n\nThe cards provide a documented record of \n\ndesign, development and deployment, so they can \n\nfacilitate attribution of responsibility for various \n\ndecisions and outcomes related to the model or \n\nsystem. Auditors can use them not only to gain \n\na holistic understanding of the system itself, but \n\nalso to zoom in on the processes and decisions \n\nmade during different phases of the life cycle. \n\nINDUSTRY EXAMPLE \n\nMeta released  22 system cards \n\nexplaining how AI powers \n\nits Facebook and Instagram \n\nplatforms. Each card has four \n\nsections that detail: \n\n1. An overview of the AI system. \n\n2. How the system works by \n\nsummarizing the steps involved \n\nin creating experiences on \n\nFacebook and Instagram. \n\n3. How the shown content can \n\nbe customized. \n\n4. How AI delivers content as part \n\nof the whole system. → Part IV. The transparency, explainability and interpretability challenge    \n\n> AI Governance in Practice Report 2024 |39\n> TABLE OF CONTENTS ↑\n\nOpen-source AI \n\nAnother approach to addressing the black-box \n\nchallenge is making AI open source. This requires \n\nmaking the source code public and allowing users \n\nto view, modify and distribute it freely. Open \n\naccess can be especially useful for researchers \n\nand developers, as there is more potential for \n\nscrutiny by a wider, diverse and collaborative \n\ncommunity of experts. In turn, that can lead to \n\nimprovements to the transparency of algorithms, \n\nthe detection of risks and the offering of solutions \n\nif things go wrong. Open-source AI can also \n\nimprove technology access and drive collaborative \n\ninnovation, which may otherwise be limited by \n\nproprietary algorithms. \n\nWatermarking \n\nWith the rise of generative AI, it is becoming \n\nincreasingly difficult to distinguish AI-generated \n\ncontent from human-created content. To ensure \n\ntransparency, the watermarking or labeling of \n\nAI generated content has been legally mandated \n\nunder the EU AI Act, U.S. Executive Order 14110 \n\nand state-level requirements, and China's Interim \n\nMeasures for Management of Generative AI. \n\nAt IBM, we believe that \n\nopen technology and \n\ncollaboration are essential \n\nto further the responsible \n\nadoption of AI. An open \n\napproach can support \n\nefforts to develop and \n\nimplement leading \n\ntechnical methods, such \n\nas those used during the \n\ntesting and evaluation of \n\ndata and AI systems. \n\nChristina Montgomery \n\n> IBM Vice President and Chief Privacy and Trust Officer\n\nINDUSTRY EXAMPLE \n\nMeta's Llama is open source, and it \n\naims to power innovation through \n\nthe open-source community. Given \n\nthe safety and security concerns \n\nassociated with generative AI models, \n\nLlama comes with a  responsible-use \n\nguide  and an  acceptable-use policy .\n\nOn the other hand,  OpenAI  has taken \n\na closed approach toward its large \n\nlanguage models, such as GPT-3 and \n\nGPT-4, in the interest of maintaining \n\nits competitive advantage, as well as \n\nto ensure AI safety. → Part IV. The transparency, explainability and interpretability challenge    \n\n> AI Governance in Practice Report 2024 |40\n> TABLE OF CONTENTS ↑\n\nINDUSTRY EXAMPLE \n\nGoogle uses a technology called  SynthID , which directly \n\nembeds watermarks into Google's text-to-image generator \n\nImagen. Meta has moved toward labeling \n\nAI-generated images  on Facebook, Instagram and \n\nThreads. Although Meta already adds the label \"Imagined \n\nwith AI\" on images generated through its AI feature, \n\nit now also aims to work with industry partners on \n\ncommon standards to add multilingual labels on synthetic \n\ncontent generated with tools of other companies that \n\nusers post on Meta's platforms. Specifically, Meta \n\nis relying on Partnership on AI's  best practices , the \n\nCoalition for Content Provenance and Authenticity's \n\nTechnical Specifications  and the International Press \n\nTelecommunications Council's  Technical Standards  to add \n\ninvisible markers at scale to label AI generated content by \n\ntools of companies such as Google, Microsoft and OpenAI. \n\nWatermarking is gaining traction as a way for organizations to \n\npromote transparency and ensure safety against harmful content, \n\nsuch as misinformation and disinformation. Companies are \n\nembedding watermarks on AI-generated content. Watermarks are \n\ninvisible to the human eye, but are machine readable and can be \n\ndetected by computers as AI generated. \n\nWhile watermarking is becoming a popular technique for \n\ntransparency, it is still not possible to label all AI generated content. \n\nMoreover,  techniques  to break watermarks also exist. \n\nFocusing on the building blocks of AI governance \n\n— like appropriate documentation of AI models and \n\nsystems — is important because those foundations \n\nare necessary to enable risk management, impact \n\nassessment, and third-party auditing. \n\nMiranda Bogen \n\n> Center for Democracy and Technology\n> AI Governance Lab Director\n\nAI Governance in Practice Report 2024  | 41 \n\n# Part V. The bias, \n\n# discrimination \n\n# and fairness \n\n# challenge \n\nBias, discrimination and fairness are among the most \n\nimportant challenges of AI governance, given their \n\npotentially very significant real-world impacts on individuals \n\nand communities. Leaving this challenge unaddressed can \n\nlead to discriminatory outcomes and perpetuate inequalities \n\nat scale. Healthy AI governance must promote legal and \n\nethical norms including human rights, professional \n\nresponsibility, human-centered design and control of \n\ntechnology, community development and nondiscrimination. \n\nWhile the automation of human tasks using AI has the \n\nadvantages of scalability, efficiency and accuracy, it is \n\naccompanied by the challenge of  algorithmic bias , whereby \n\na systematic error manifests through an inaccuracy in the \n\nalgorithm. It occurs when an algorithm systematically or \n\nrepeatedly misses certain groups of people more than others. \n\nWith transparency challenges around how or why an input \n\nturns into a particular output, biases in the algorithm can be \n\ndifficult to trace and identify. \n\nInstances of algorithmic bias have been well documented in \n\npolicing , criminal sentencing  and  hiring . Algorithmic bias can \n\nimpact even the most well-intentioned AI systems, and it can \n\nenter a  model  or system in numerous ways. \n\n## Hidden and harmful biases may lurk \n\n## within an AI system. → Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 42  \n\n> TABLE OF CONTENTS ↑\n\nWays biases may get into the AI system \n\nBiases may get into the AI system in multiple \n\nways during the input, training and output stages. \n\nAt the input stage \n\n→ Historical data.  If historical data used \n\nto train algorithms is biased, then the \n\nalgorithm may learn those biases and \n\nperpetuate them. For example, if an AI \n\nrecruitment tool is trained on historical \n\ndata  containing gender or racial biases, \n\nthose biases will be reflected in the tool's \n\nhiring decisions or predictions. \n\n→ Representation bias.  Biases can also enter \n\nthe algorithm through data that either \n\noverrepresents or underrepresents social \n\ngroups. This can make the algorithmic \n\ndecisions less accurate and create \n\ndemographic or social disparities. \n\n→ Inaccurate data.  The accuracy of data \n\ncan be impaired if it is outdated or \n\ninsufficient. Such data falls short of fully \n\nrepresenting current realities, leading to \n\ninaccurate results, which may also lead to \n\nreinforcement of historical biases. \n\nAt the training stage \n\n→ Model.  Biases can arise when they are an \n\nintrinsic part of the model itself. For example, \n\nmodels developed through traditional \n\nprogramming, i.e., those manually coded by \n\nhuman designers, can have  intrinsic biases \n\nif they are not based on real-world insights. \n\nAn algorithm assisting with university \n\nadmissions may be biased if the human \n\ndesigner programmed it to give a higher \n\npreference to students from private schools \n\nover students from public schools. Intrinsic \n\nbiases may be difficult to spot in AI models, \n\nas they are a result of self-learning and make \n\ncorrelations across billions of data points, \n\nwhich are often part of a black box. \n\n→ Parameters.  The model adjusts its \n\nparameters, such as  weights and biases \n\nin neural networks, during the training \n\nprocess based on the training data. Bias \n\ncan manifest when the values assigned to \n\nthese parameters inadvertently reinforce \n\nthe bias present in the training data or the \n\ndecisions made by the designers during \n\narchitecture selection. In an algorithm for \n\nuniversity admissions, for example, the \n\nattributes of leadership and competitiveness \n\ncan reflect a gender stereotype present \n\nin the training data with the algorithm \n\nfavoring male candidates over female \n\nones. However, bias in parameters can also \n\nmanifest more stealthily, such as through \n\nproxies. In absence of certain data, the \n\nalgorithm will make correlations to make \n\nsense of the missing data. An algorithm \n\nfor loan approval, for example, may \n\ndisproportionately assign more weight \n\nto certain zip codes and the model may \n\ninadvertently perpetuate racial or ethnic \n\nbias by rejecting loan applications using \n\nzip  codes as a proxy. \n\nAt Microsoft, we are steadfast \n\nin our commitment to \n\ndeveloping AI technologies \n\nthat are not only innovative \n\nbut also trustworthy, safe, and \n\nsecure. We believe that the \n\ntrue measure of our progress is \n\nnot just in the capabilities we \n\nunlock, but in the assurance \n\nthat the digital experiences \n\nwe create will enhance \n\nrather than compromise the \n\nhuman experience. \n\nJulie Brill, \n\nMicrosoft Chief Privacy Officer, Corporate Vice President → Part V. The bias, discrimination and fairness challenge    \n\n> AI Governance in Practice Report 2024 |43\n> TABLE OF CONTENTS ↑\n\nAt the output stage \n\n→ Self-reinforcing biases.  A feedback loop is a process through \n\nwhich the AI system continues to learn based on the outputs \n\nit generates. The output goes back into the system as an input, \n\nwhich can influence the system's behavior or performance in \n\nsome positive or negative way. While feedback loops can foster \n\ncontinuous learning and allow the system to adapt to its deployed \n\nenvironment, they can also lead to  self-reinforcing biases  if the \n\noutputs of the algorithm itself are biased. For example, if an \n\nalgorithm consistently rejects loan applications for women and \n\nconsistently approves them for men, there may be a gender bias at \n\nplay, and the algorithm could fall into a loop where it learns from \n\nthe biased outputs and continues to reinforce the biased pattern. \n\n→ Human oversight.  Although it is necessary to have humans in \n\nthe loop throughout the life cycle of the AI system, there is a \n\nrisk that human biases can reenter the algorithm. For example, \n\nhuman control over a system's final output is necessary, but \n\nbias can externally impact the output based on the human \n\ninterpretation  applied to that final output. \n\n→ Automation bias.  Automation bias refers to the human \n\ntendency to overly rely on automated outputs. This leads \n\nto people trusting the recommendations of algorithms \n\nwithout questioning or verifying their accuracy or being \n\nmindful of the system's limitations and errors. This can be \n\nespecially dangerous when confirmation bias about protected \n\ncharacteristics is at play. That is, users are more likely to accept \n\nthe outputs when they align with their preexisting beliefs. \n\nBias detection and mitigation is particularly challenging \n\nin the  context of foundation models due to their size and \n\ncomplex architectures. → Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 44  \n\n> TABLE OF CONTENTS ↑\n\nLaw and policy considerations \n\nMany existing equalities and antidiscrimination laws apply to AI \n\nsystems and many emerging initiatives specific to AI governance \n\ninclude provisions on bias. \n\nDepending on the jurisdiction where the organization operates, \n\nliability could also fall under relevant civil rights, human rights \n\nor  constitutional freedoms of that jurisdiction. \n\nIn the U.S., civil rights can be protected through private rights \n\nof action by individuals. For example, according to the guidance \n\nprovided by the U.S. Equal Employment Opportunity Commission, \n\nprivate rights of action against discrimination through algorithms \n\ncould occur under the  Americans with Disability Act  and  Title VII \n\nof the Civil Rights Act . Under both the ADA and Title VII, employers \n\ncan be exposed to liability even where their algorithmic decision-\n\nmaking tools are designed or administered by another entity. When \n\nindividuals think their rights under either of those laws have been \n\nviolated, they can file a charge of discrimination with EEOC. \n\nOECD AI Principles \n\nThe principle of \"human-centered values and fairness\" in the OECD \n\nAI Principles  requires respect for the rule of law, human rights and \n\ndemocratic values across the life cycle of the AI system, through \n\nrespect for nondiscrimination and equality, diversity, fairness, and \n\nsocial justice. This is to be implemented through safeguards, like \n\ncontext-appropriate human determination that is consistent with \n\nthe state of the art. The OECD AI Policy Observatory maintains \n\na catalogue on  tools and metrics  for practically aligning AI with \n\nOECD's principles, including  bias and fairness .\n\n→ SPOTLIGHT \n\nJoint statement by US Federal Agencies \n\nIn April 2023, a  joint statement  made by four federal \n\nagencies, namely the EEOC, Department of Justice, \n\nFederal Trade Commission and Consumer Financial \n\nProtection Bureau, reiterated the U.S.'s commitment to \n\nthe principles of fairness, equality and justice, which \n\nare deeply embedded in federal laws. In April 2024, five \n\nadditional cabinet-level agencies joined that pledge. \n\nThe joint statement now includes the Department of \n\nEducation, Department of Health and Human Services, \n\nDepartment of Homeland Security, Department of \n\nHousing and Urban Development, and Department of \n\nLabor. The Consumer Protection Branch of the Justice \n\nDepartment's Civil Division also joined the pledge. \n\nEnforcement of discrimination through automated \n\nmeans is managed by these federal agencies. → Part V. The bias, discrimination and fairness challenge    \n\n> AI Governance in Practice Report 2024 |45\n> TABLE OF CONTENTS ↑\n\nUNESCO Recommendations on the Ethics of AI \n\nPrinciples 28, 29 and 30 of UNESCO's  Recommendations on the \n\nEthics of AI  encourage AI actors to promote access to technology \n\nto diverse groups, minimize the reinforcement or perpetuation \n\nof discriminatory or biased outcomes throughout the life cycle \n\nof the AI systems, and reduce the global digital divide. Among \n\nthe tools provided by UNESCO for the practical implementation \n\nof its recommendations is the  ethical impact assessment , which \n\nis designed primarily for government officials involved in the \n\nprocurement of AI systems but can also be used by companies to \n\nassess if an AI system aligns with UNESCO's Recommendations. \n\nEU AI Act \n\nThe EU  AI Act  provides a relevant framework for data governance \n\nof high-risk AI systems under Article 10, which permits training, \n\nvalidation and testing of datasets to examine the possibility of biases \n\nthat affect the health and safety of persons and negatively impact \n\nfundamental rights or lead to discrimination. To deal with the challenge \n\nof self-reinforcing biases, Article 15(4) also requires the elimination \n\nor reduction of biases emanating from feedback loops in high-risk AI \n\nsystems after they have been put on the market or into service. The \n\nEU AI Act calls for consideration of the European Commission's  Ethics \n\nGuidelines for Trustworthy AI , which are voluntary guidelines seeking \n\nto promote \"diversity, non-discrimination and fairness.\" \n\nSingapore \n\nSingapore's AI Verify tackles bias via the principle of \"ensuring \n\nfairness.\" This principle is made up of the pillars of data governance \n\nand fairness. While there are no specific tests for data governance \n\nin the toolkit, if the model is not giving biased outputs based on \n\nprotected characteristics, fairness can be ensured by checking the \n\nmodel against  ground truth . Process checks include the verification of \n\ndocumentary evidence that there is a strategy for fairness metrics and \n\nthat the definition of sensitive attributes is consistent with the law. → Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 46  \n\n> TABLE OF CONTENTS ↑\n\n## → SPOTLIGHT \n\n## US FTC Enforcement priorities and concerns \n\nWe have made no secret of our enforcement priorities \n\nand concerns. \n\n# 1.  There is no AI exemption from the laws on the books and \n\nbusinesses need to develop and deploy AI tools in ways that \n\nallow for an open and competitive market and protect consumers \n\nfrom potential harms. \n\n# 2.  We are scrutinizing existing and emerging bottlenecks across the AI \n\ndesign stack to ensure that businesses aren't using monopoly power \n\nto block innovation and competition. \n\n# 3.  We are acutely aware that behavioral advertising, brought on by \n\nweb 2.0, fuels the endless collection of user data and recognize \n\nthat model training is emerging as another feature that could further \n\nincentivize surveillance. \n\n# 4.  We are squarely focused on aligning liability with capability and \n\ncontrol, looking upstream and across layers of the AI stack to \n\npinpoint which actor is driving or enabling the lawbreaking. \n\n# 5.  We are focused on crafting effective remedies in cases \n\nthat establish bright-line rules on the development, use and \n\nmanagement of AI inputs, such as prohibiting the uses of inaccurate \n\nor highly-sensitive data when training models. \n\nSamuel Levine \n\nFTC Bureau of Consumer Protection Director → Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 47  \n\n> TABLE OF CONTENTS ↑\n\nThe US \n\nIn the U.S., antidiscrimination laws that also \n\nextend to AI are scattered across various sectors, \n\nsuch as employment, housing and civil rights. \n\n→ Employment.  Under the  Americans with \n\nDisabilities Act , employers are prohibited from \n\nusing algorithmic decision-making tools that \n\ncould violate the act, such as in not providing \n\nreasonable accommodations, intentionally \n\nor  unintentionally screening out an individual \n\nwith a disability, or adopting disability-related \n\ninquiries and medical examinations. \n\n→ Housing.  In 2023, to ensure fairness in \n\nhousing, the Biden-Harris Administration \n\nissued a proposed rule against racial bias in \n\nalgorithmic  home valuations , empowering \n\nconsumers to take action against appraisal \n\nbias, increasing transparency and leveraging \n\nfederal data to inform policy and improve \n\nenforcement against appraisal bias. \n\n→ Consumer finance.  The  CFPB  confirmed \n\ncompanies are not absolved of their legal \n\nresponsibilities under existing legislation, \n\nsuch as the  Equal Credit Opportunity Act ,\n\nwhen they use AI models to make lending \n\ndecisions. Remedies include compensating \n\nthe victim, providing injunctive relief to \n\nstop unlawful conduct, or banning persons \n\nor companies from future participation in \n\nthe marketplace. \n\n→ Voluntary frameworks.  The  NIST \n\nSpecial  Publication 1270: Towards a \n\nStandard for Identifying and Managing \n\nBias in AI  lays down governance standards \n\nfor managing  AI bias. These include \n\nmonitoring the system for biases, \n\nmaking  feedback channels available \n\nso users can flag incorrect or harmful \n\nresults  for which they can seek recourse, \n\nputting policies and procedures in \n\nplace for every stage of the life cycle, \n\nmaintaining model documentation to \n\nensure accountability, and embedding \n\nAI governance within the culture of \n\nthe  organization. \n\nAs we navigate the \n\ntransformative potential of \n\nAI, it is imperative that we \n\nanchor our journey in our \n\ncollective ability to protect \n\nfundamental rights and the \n\nenduring values of safety \n\nand accountability. \n\nJulie Brill \n\nMicrosoft Chief Privacy Officer, \n\nCorporate Vice President → Part V. The bias, discrimination and fairness challenge    \n\n> AI Governance in Practice Report 2024 |48\n> TABLE OF CONTENTS ↑\n\n## → SPOTLIGHT \n\n## FTC enforcement action against Rite Aid \n\nOn 19 Dec. 2023, the FTC issued an  enforcement action  against \n\nRite Aid's  discriminatory use  of facial recognition technology. \n\nRite  Aid deployed facial recognition surveillance systems for theft \n\ndeterrence without assessing the accuracy or bias of the system. \n\nRite Aid recorded thousands of false-match alerts, and the FTC's gender-based \n\nanalysis revealed Black, Asian, Latino and women consumers were more likely to \n\nbe harmed by Rite Aid's surveillance technology. \n\nThe FTC placed a five-year moratorium on Rite Aid's use of facial recognition, \n\nand if after five years Rite Aid chooses to use this technology again, it will have \n\nto implement the FTC's governance plan detailed in the  order . The enforcement \n\ndecision also included an order for  disgorgement , that is, to delete or destroy any \n\nphotos and videos including any data, models or algorithms used for surveillance. \n\nThis case serves as good indication of the nature and intensity \n\nof liability that deployers and providers of AI in the U.S. may \n\nbe exposed to for deploying discriminatory AI systems. → Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 49  \n\n> TABLE OF CONTENTS ↑\n\nImplementing AI governance \n\nOne overarching practice used to mitigate biases is \n\nthe promotion of diversity and inclusivity among \n\nteams working across the life cycle of the AI \n\nsystem. Personnel composition is often supported \n\nby organization-level principles for safe and \n\nresponsible AI, many of which internal AI ethics \n\npolicies, e.g.,  Google , IBM  and  Microsoft .\n\nBias testing \n\nOne way to minimize bias in AI systems is by \n\ntesting the systems. While there are  numerous \n\nways  to test for bias in AI systems, it is important \n\nto understand what is being evaluated. \n\nDemographic parity may be different than equality \n\nobjectives. Establish goals based on the desired \n\nsystem outcomes to start, and then establish an \n\nappropriate technique for testing bias within the \n\nsystem. For example, does fairness mean an equal \n\nnumber of males and females will be screened for \n\na new position on your team, or that candidates \n\nwith the most distinguished resumes are identified \n\nas ideal applicants independent of their gender, \n\nrace, experience, etc. \n\nIt is important to note that often testing for bias \n\nwill require the use of personal information \n\nto determine if fairness objectives are being \n\nmet. As such, there may be a  privacy-bias \n\ntrade-off , as safeguarding privacy through data \n\nminimization creates  challenges  for mitigating \n\nbiases in AI systems. Some considerations when \n\nbalancing privacy while mitigating bias include: \n\n→ Intentionally collecting sensitive data \n\ndirectly in the design phase so it is ready \n\nfor the testing phase. This can be done by \n\nprocuring consent from data subjects and \n\ndisclosing the purpose for the collection and \n\nprocessing of their data. \n\n→ Creating intentional proxies to test how the \n\nsystem makes correlations without sensitive \n\ndata, such as for demographic features. \n\n→ Buying missing data from data brokers, \n\npublic data or other datasets in compliance \n\nwith privacy and data governance policies. \n\nFairness tests and debiasing \n\nmethods are not created \n\nequally — as an AI deployer \n\nor governance professional, \n\nit is critically important to \n\nuse tools and methods that \n\nfundamentally align with \n\nequality and nondiscrimination \n\nlaw in your jurisdiction. \n\nBrent Mittelstadt \n\nUniversity of Oxford Internet Institute Director of Research, \n\nAssociate Professor and Senior Research Fellow AI Governance in Practice Report 2024  | 50 \n\n# Part VI. \n\n# The security \n\n# and robustness \n\n# challenge \n\nCompromises to the security of AI could result in manipulation \n\nof outputs, stolen sensitive information or interference with \n\nsystem operations. Unsecured AI can result in financial losses, \n\nreputational damage and even physical harm. For example, \n\nexploiting the vulnerabilities of medical AI could lead to a \n\nmisdiagnosis, and adversarial attacks on autonomous vehicles \n\ncould lead to road traffic accidents. \n\nAlthough AI security overlaps with and suffers from traditional \n\ncybersecurity risks, cybersecurity is often about protecting \n\ncomputer systems and networks from attacks, whereas AI \n\nsecurity is about guarding the AI system's components, namely \n\nthe data, model and outputs. When it comes to AI security, \n\nmalicious actors can enable adversarial attacks by exploiting \n\nthe  inherent limitations  of AI algorithms. \n\n## The pace, scale, and reach of AI \n\n## development and integration \n\n## demands strong security. → Part VI. The security and robustness challenge    \n\n> AI Governance in Practice Report 2024 |51\n> TABLE OF CONTENTS ↑\n\nAdversarial attacks \n\nAdversarial attacks are a deliberate attempt to \n\nmanipulate models in a way that leads to incorrect \n\nor harmful outputs. The intention behind \n\nthe attack could be to lead the model toward \n\nmisclassification or cause harm, and all it may take \n\nto trick the model is a slight switching of pixels or \n\nadding a bit of noise. Some types of adversarial \n\nattacks include: \n\n→ Evasion attacks.  The aim of evasion \n\nattacks is to deceive the model into \n\nmisclassifying data, such as by adding a \n\nsmall perturbation to the input image, as \n\nin the  MIT example , leading to an incorrect \n\noutput with high  confidence. \n\n→ Data poisoning.  This can happen in various \n\nways, such as by switching the labels of \n\nlabeled data or injecting entirely new data \n\ninto the dataset. However, for this to work, \n\nthe adversary will have to first gain access \n\nto training data.  Data poisoning  can also \n\nhelp attackers create  backdoors  so they can \n\nmanipulate model behavior in the future. \n\n→ Model extraction.  The aim of model \n\nextraction is model theft by reverse \n\nengineering to reveal the hidden mechanism \n\nof the model or sensitive information, or \n\nto make the model vulnerable to further \n\nattacks. This is done by feeding carefully \n\ncrafted  queries  to a black-box model to \n\nanalyze its outputs and steal its functionality. \n\nThis can help the adversary copy the model \n\nand make financial gains. \n\nAI vulnerabilities can also be exploited through \n\nopen-source software and third-party risks. \n\n→ Open-source software can be manipulated \n\nin many ways, such as through supply-chain \n\nattacks, in which open-source AI libraries \n\nare targeted by malicious code that is planted \n\nas a legitimate update or functionality. \n\nAlthough open-source software suggests \n\neverything has been made publicly available, \n\nthe original developers can restrict access \n\nto some parts of the software in the license \n\nagreement. In such cases, hackers may resort \n\nto model extraction. Even if an AI  system \n\nis not open source, the project may rely on \n\na complex ecosystem of open-source tools, \n\nexposing itself to a potential attack surface \n\nthat malicious actors can exploit. \n\n→ A lack of control and visibility over third-\n\nparty governance practices makes risk \n\nmitigation more difficult, including with \n\nrespect to security. Third-party vendors \n\nmay have weaker security standards and \n\npractices, making them more vulnerable \n\nto data breaches, supply chain attacks and \n\nsystem hacks, among other security risks. → Part VI. The security and robustness challenge    \n\n> AI Governance in Practice Report 2024 |52\n> TABLE OF CONTENTS ↑\n\nLaw and policy considerations \n\nRegulatory and voluntary governance tools that \n\nhave established requirements for tackling AI \n\nsecurity issues include the NIS2 Directive, U.S. \n\nExecutive Order 14110, the NIST AI RMF and the \n\nNIST Cybersecurity Framework. \n\nNIS2 \n\nThe  NIS2  Directive replaces the EU Network \n\nand Information Security Directive from 2016. \n\nIt  aims to boost resilience and incident-response \n\ncapacities in public and private sectors through \n\nrisk management and reporting obligations. \n\nSome cybersecurity requirements under Article \n\n21 include policies on risk analysis and system \n\nsecurity, incident handling, supply-chain \n\nsecurity, policies and procedures to assess \n\ncybersecurity risk-management effectiveness, \n\ncyber hygiene practices, policies on the use of \n\ncryptography, and encryption. \n\nEU AI Act \n\nAs with most AI themes, under the EU  AI Act ,\n\nsecurity takes a risk-based approach. As such, \n\nsecurity and robustness requirements vary based \n\non if the system is high risk or if it is a GPAI system \n\nwith systemic risks. \n\n→ High-risk AI systems.  The EU AI Act \n\nlays down detailed security obligations \n\nfor accuracy, security and robustness \n\nof high-risk AI systems. Technical and \n\norganizational measures are to be placed to \n\nensure high-risk systems are resilient toward \n\nerrors, faults and inconsistencies. Possible \n\nsolutions include back-up or fail-safe plans. \n\nThe act also foresees risks emerging at the \n\nthird-party level, requiring resilience against \n\nunauthorized third-party attempts to alter \n\nuse, outputs or performance by exploiting \n\nthe system vulnerabilities. Technical \n\nsolutions to handle such security risks must \n\nbe appropriate to circumstances and risk. \n\nThese can include measures to prevent, \n\ndetect, respond to, resolve and control data \n\npoisoning, model poisoning, adversarial \n\nexamples and model evasion, confidentiality \n\nattacks, or model flaws. \n\nAdditionally, the EU AI Act obliges providers \n\nof high-risk AI systems to ensure they \n\nundergo conformity assessments that \n\ndemonstrate compliance with requirements \n\nfor high-risk systems. \n\n→ Obligations for providers of GPAI systems \n\nwith systemic risks.  The EU AI Act lists the \n\nsecurity requirements for high-impact AI \n\nsystems. Requirements include: \n\n• Evaluating models in accordance with \n\nstandardized protocols, such as conducting \n\nand documenting adversarial testing to \n\nidentify and mitigate systemic risks. \n\n• Monitoring, documenting and reporting \n\nserious incidents to the AI Office. \n\n• Ensuring GPAI models with systemic risks \n\nand their physical infrastructures have \n\nadequate levels of cybersecurity. → Part VI. The security and robustness challenge \n\nAI Governance in Practice Report 2024  | 53  \n\n> TABLE OF CONTENTS ↑\n\nUS  Executive  Order 14110 \n\nThe U.S.  AI Executive Order 14110  calls on \n\ndevelopers of the most powerful AI systems \n\nto share their safety results and other critical \n\ninformation with the U.S. government. It also \n\ncalls on the NIST to develop rigorous standards \n\nfor extensive red-team testing to ensure safety \n\nbefore public release. \n\nNIST AI RMF \n\nThe NIST  AI RMF  identifies common security \n\nconcerns such as data poisoning and exfiltration \n\nof models, training data or other intellectual \n\nproperty through AI system endpoints. Under \n\nthe AI RMF, a system is said to be secure when \n\nit can maintain confidentiality, integrity and \n\navailability through protection mechanisms that \n\nprevent unauthorized access and use. Practical \n\nimplementation can be achieved through the \n\nNIST Cybersecurity Framework and  RMF .\n\nNIST Cybersecurity Framework \n\nThe NIST  Cybersecurity Framework  is a voluntary \n\nframework that provides standards, guidelines \n\nand best practices for organizations to mitigate \n\ncybersecurity risks. The framework is organized \n\nunder five  key functions : identify, protect, detect, \n\nrespond and recover. \n\nThere is an urgent need to \n\nrespond to the complex \n\nchallenges of AI governance \n\nby professionalizing the field. \n\nA professionalized workforce \n\ncan take AI governance from \n\ntheory to practice, spread \n\ntrustworthy and standardized \n\npractices across industries \n\nand borders, and remain \n\nadaptable to swiftly changing \n\ntechnologies and risks. \n\nJ. Trevor Hughes \n\nIAPP President and CEO → Part VI. The security and robustness challenge \n\nAI Governance in Practice Report 2024  | 54  \n\n> TABLE OF CONTENTS ↑\n\nImplementing AI governance \n\nDue diligence in the identification of security \n\nrisks throughout the life cycle of the system is \n\nan important activity, especially when a third-\n\nparty vendor is involved. Due diligence can only \n\never inform. With appropriate information, an \n\norganization can seek contract terms with third-\n\nparty vendors that mandate: \n\n→ Making the vendor's security practices \n\ncompatible with the organization's \n\nown  standards. \n\n→ Monitoring system robustness regularly \n\nthrough security assessments or audits to \n\nidentify third-party risks and ensure the \n\nvendor is complying with the organization's \n\nsecurity standards. \n\n→ Limiting access to third-party vendors only \n\nfor the services they need to perform. \n\nRed teaming \n\nRed teaming is the process of testing the security \n\nof an AI system through an  adversarial lens  by \n\nremoving defender bias. It involves the simulation \n\nof adversarial attacks on the model to evaluate \n\nit against certain benchmarks, \" jailbreak \" it and \n\nmake it behave in unintended ways. Red teaming \n\nreveals security risks, model flaws, biases, \n\nmisinformation and other harms, and the results \n\nof such testing are passed along to the model \n\ndevelopers for remediation. Developers use red \n\nteaming to bolster and secure their product before \n\nreleasing it to the public. \n\nSecure data sharing practices \n\nDifferential privacy is primarily a  privacy-\n\nenhancing technique  that also has security \n\nbenefits, it analyzes group data while \n\npreserving individual privacy by adding \n\ncontrolled noise to the data and blurring \n\nindividual details. So, even if an attacker were \n\nto steal this data, they would not be able to \n\nlink it back to specific individuals, minimizing \n\nharm. As such, differential privacy can limit \n\nthe utility of stolen data. However, that impact \n\nto the utility of the data can also impact \n\norganizations with lawful and legitimate \n\ninterests in processing the data. Moreover, \n\ndifferential privacy can also be a costly \n\ntechnique to implement, especially where \n\nlarge  datasets are concerned. \n\nHITL \n\nHuman in the loop refers to incorporating \n\nhuman expertise and oversight into the \n\nalgorithmic decision-making process. Although \n\nHITL may provide a gateway for human \n\nbiases to reenter the algorithm when making \n\njudgements about final outputs, in the context \n\nof  AI security, HITL can make incident detection \n\nand response more efficient. This is especially \n\ntrue where subtle manipulations or attacks that \n\nthe model may not have been trained to identify \n\nare involved.  HITL  allows for continuous \n\nmonitoring and verification, however, optimal \n\nuse of this approach rests on balancing the \n\ncontradictions that may arise to address bias \n\nor  safety and security. \n\nINDUSTRY EXAMPLE \n\nOpenAI's latest text-to-video \n\nmodel,  Sora , was red teamed. \n\nIn preparation for the U.K. AI \n\nSafety Summit, Meta released \n\na document  detailing the \n\nsafety of its Llama 2 model, \n\nas well as the benchmarks and \n\npotential attack vectors it \n\nwas red teamed for. AI Governance in Practice Report 2024  | 55 \n\n# Part VII. \n\n# AI safety \n\nVarious themes, particularly value alignment, transparency \n\nand AI security, eventually culminate into the broader \n\ntheme of AI safety. Given that safety is an all-encompassing \n\ntheme, it has no settled global definition. It may include \n\npreventing so-called existential risks posed by artificial \n\ngeneral intelligence. For some, such as the  Center for AI \n\nSafety , AI risk is categorized based on malicious use, the AI \n\nrace, rogue behavior and organizational risks. For others, \n\nsuch as the country signatories to the  Bletchley Declaration ,\n\nand most recently, for parties to the  Seoul Declaration for \n\nSafe, Innovative and Inclusive AI , it is about managing risks \n\nand being prepared for unexpected risks that may arise from \n\nfrontier AI. AI safety can also be the term used to describe \n\nminimizing AI harms from misinformation, disinformation \n\nand deepfakes, and the unintended behavior of an AI system, \n\nespecially advanced AI systems. \n\n## AI safety is a cornerstone but \n\n## somewhat mercurial principle for \n\n## realizing safe and responsible AI. → Part VII. AI safety    \n\n> AI Governance in Practice Report 2024 |56\n> TABLE OF CONTENTS ↑\n\nLaw and policy considerations \n\nThe importance of AI safety is reflected in the fact \n\nthat, for some jurisdictions, it has been embedded \n\nas a main theme in national strategies toward \n\nAI. The Biden-Harris Administration's Executive \n\nOrder 14110 focuses on developing \"Safe, Secure \n\nand Trustworthy\" AI. In 2023, the U.K. brought \n\nworld leaders together for first AI Safety Summit, \n\nand the country's approach toward AI is focused \n\non the safety of advanced AI systems, or \"frontier \n\nAI.\" Safety is also an important factor under the \n\nEU AI Act, which is reflected in the security and \n\nrobustness requirements for high-impact GPAI \n\nsystems and  high-risk AI systems. \n\nAI safety institutes \n\nRecently, the NIST announced it would establish \n\nthe  U.S. AI Safety Institute . To support this \n\ninstitute, the NIST also created an  AI Safety \n\nInstitute Consortium , which brought more than \n\n200 organizations together to develop guidelines \n\nand standards for AI measurement and policy \n\nthat can lay the foundation for AI safety globally. \n\nAmong many security- and safety-related \n\ninitiatives, the AISIC is tasked with enabling \n\ncollaborative and interdisciplinary research \n\nand establishing a knowledge and data sharing \n\nspace for AI stakeholders. More specifically, \n\nthe AISIC will develop new guidelines, tools, \n\nmethods, protocols and best practices to \n\nfacilitate the evolution of industry standards \n\nfor AI safety. The AISIC will also develop \n\nbenchmarks for evaluating AI capabilities, \n\nespecially harmful ones. \n\nThe U.K. government established an  AI Safety \n\nInstitute  to build a sociotechnical infrastructure \n\nthat can minimize risks emerging from \n\nunexpected advancements in AI technology. \n\nThe institute has been entrusted with three \n\nmain functions: developing and conducting \n\nevaluations on advanced AI systems, driving \n\nfoundational AI research, and facilitating the \n\nexchange of information. → Part VII. AI safety \n\nAI Governance in Practice Report 2024  | 57  \n\n> TABLE OF CONTENTS ↑\n\nBletchley Declaration \n\nThe 2023 U.K. AI Safety Summit brought \n\ntogether international governments, leading \n\nAI  companies and civil society groups to discuss \n\nfrontier AI risks and ways to promote AI safety. \n\nAs a demonstration of their commitments to \n\nAI safety, participating nations also signed \n\nthe Bletchley Declaration, which makes \n\nvarious affirmations to cooperate globally \n\non innovation, sustainable development, \n\neconomic growth, protection of human rights \n\nand fundamental freedoms, and building public \n\ntrust and confidence in AI technology. \n\nEU AI Act \n\nThe security requirements for general-purpose \n\nAI systems under the AI Act are also focused \n\non regulating \"systemic risks.\" The EU AI act \n\ndefines this risk as one emerging from high-\n\nimpact general purpose models that \"significantly \n\nimpact the internal market, and with actual or \n\nreasonably foreseeable negative effects on public \n\nhealth, safety, public security, fundamental \n\nrights, or the society as a whole, that can be \n\npropagated at  scale across the value chain.\" \n\nAI Safety Standards \n\nISO/IEC Guide 51:2014  provides requirements \n\nand recommendations for drafters of standards \n\nto include safety aspects in those standards. It \n\napplies to safety aspects pertaining to people, \n\nenvironments or both. \n\nWe are generating 2.5 \n\nquintillion bytes of data \n\nglobally per day. Much of this \n\nis flowing into our internet. \n\nTherefore, generative AI \n\nmodels are dynamic and the \n\napplications that are built on \n\ntop of them will move. It is up \n\nto the organizations to ensure \n\nthat the movement meets \n\ntheir standards. \n\nDominique Shelton Leipzig \n\nMayer Brown Partner, Cybersecurity & Data Privacy \n\nand Leader, Global Data Innovation & AdTech → Part VII. AI safety    \n\n> AI Governance in Practice Report 2024 |58\n> TABLE OF CONTENTS ↑\n\n## → SPOTLIGHT \n\n## Compute governance \n\nOn a broader level, AI safety also refers to regulating compute, i.e., the \n\npower source of AI systems, as regulating AI at its source increases the \n\nvisibility of its technical capabilities. Unlike AI models, which can be \n\nreplicated exponentially and without control, compute must be purchased \n\nand is quantifiable. As computing chips are manufactured through highly \n\nconcentrated  supply chains  and dominated by only a few companies, \n\nregulatory interventions can be more focused. Such  regulation  can \n\npurposefully occur with AI safety in mind to control the allocation of \n\nresources for AI projects by subsidizing or limiting access to compute \n\nor by building guardrails into hardware. \n\nWith compute governance gaining traction because of advanced AI \n\nsystems, compute thresholds, i.e., numerical measures of computing \n\npower, are also being set legally, which helps distinguish AI systems with \n\nhigh capabilities from other AI systems. \n\nFor instance, U.S. Executive Order 14110 requires models using computing \n\npower greater than 10 26  integer and models using biological sequence \n\ndata and computing power greater than 10 23  integer to provide the \n\ngovernment with information and reports on the models testing and \n\nsecurity on an ongoing basis. \n\nSimilarly, under the EU AI Act, GPAI is presumed to have high-impact \n\ncapabilities when cumulative compute used for training is greater than \n\n10 25  floating-point operations. When a model meets this threshold, the \n\nprovider must notify the Commission, as meeting the threshold leads to \n\nthe presumption that this is a GPAI system with systemic risk. This means \n\nthe model can have a significant impact on the internal market, and actual \n\nor reasonably foreseeable negative effects on health, safety, fundamental \n\nrights or society. Providers need to comply with requirements on model \n\nevaluation, adversarial testing, assessing and mitigating systemic risks, \n\nand reporting any serious incidents. → Part VII. AI safety \n\nAI Governance in Practice Report 2024  | 59  \n\n> TABLE OF CONTENTS ↑\n\nImplementing AI governance \n\nThe organizational practices for security and \n\nrobustness discussed in this report, such as \n\nred teaming for adversarial testing, HITL and \n\nprivacy-preserving technologies, can apply to \n\nAI safety. Similarly, organizational practices and \n\nlaws requiring transparency and explainability, \n\nspecifically watermarks, also apply to AI safety. \n\nPrompt engineering \n\nOne of OpenAI's  safety practices  includes \n\nprompt engineering  to help generative AI \n\nunderstand prompts in a given context. This \n\npractice is aimed at minimizing harmful and \n\nundesired outputs from generative AI, and it \n\nhelps developers exercise more control over \n\nuser interactions with AI to reduce misuse at \n\nthe user level. Moreover, as part of  product \n\nsafety standards , OpenAI also has put in \n\nplace  usage policies .\n\nReports and complaints \n\nAnother safety practice of OpenAI is allowing \n\nusers to report issues that can be monitored \n\nand responded to by human operators. This is \n\nnot yet a popular practice. A 2023 study carried \n\nout by  TrustibleAI  found out of 100 random \n\norganizations, three provided an individual \n\nappeals process between the individual and \n\nthe company. It is possible internal governance \n\nand complaint mechanisms may become \n\nmore common post-EU AI Act, given that, \n\nunder Article 27 (f), deployers of AI systems \n\nmust carry out FRIAs of internal governance \n\nand complaint mechanisms where a  risk has \n\nmaterialized into a harm. \n\nSafety by design \n\nTo combat abusive AI-generated content, \n\nMicrosoft is focused on building strong safety \n\narchitecture through the  safety by design \n\napproach, which can be applied at the AI \n\nplatform, model and application levels. Some \n\nefforts include red teaming, pr eemptive \n\nclassifiers, blocking abusive prompts, automated \n\ntesting and rapid bans of users who abuse the \n\nsystem. With regard to balancing freedom of \n\nspeech against abusive content, Microsoft is \n\nalso committed to identifying and removing \n\ndeceptive and abusive content on LinkedIn, \n\nMicrosoft Gaming Network and other  services. \n\nHumans control AI, not the \n\nother way around. Generative \n\nAI models drift. The only way \n\nfor companies to know when/ \n\nhow they are drifting is to \n\ncontinuously test, monitor and \n\naudit the AI applications for high \n\nrisk use cases- every second of \n\nevery minute of every day. This \n\nis the only way to ensure that \n\nthe model output comports with \n\nthe organization’s pre-installed \n\nguardrails for accuracy, health \n\nand safety, privacy, bias. \n\nDominique Shelton Leipzig \n\nMayer Brown Partner, Cybersecurity & Data Privacy and Leader, \n\nGlobal Data Innovation & AdTech → Part VII. AI safety    \n\n> AI Governance in Practice Report 2024 |60\n> TABLE OF CONTENTS ↑\n\nSafety policies \n\nIn preparation for the U.K. AI Safety Summit, Meta released an \n\noverview of its  AI safety policies , specifically in relation to its \n\ngenerative AI Llama model. In addition to model evaluations and \n\nred-team analysis, the policy also detailed Meta's model reporting \n\nand sharing, reporting structure for vulnerabilities found after \n\nmodel release, post-deployment monitoring for patterns of misuse, \n\nidentifiers of AI generated material, data input controls and audits, \n\nand priority research on societal, safety and security risks. \n\nIndustry best practices \n\nPartnership on AI has invested extensively in AI safety research and \n\nresources. Some of its work includes  Guidance for Safe Foundation \n\nModel Deployment . This framework is a living document targeted at \n\nmodel providers on ways to operationalize AI safety for responsible \n\ndeployment. The framework provides  custom guidance  providers of \n\nfoundation models can follow throughout the deployment process \n\nthat is appropriate for their model's capabilities. Another resource \n\nis PAI's  SafeLife , which is a benchmark focused on avoiding negative \n\nside effects in complex environments.  SafeLife  is a reinforcement \n\nlearning environment that tests the \"safety of reinforcement \n\nlearning agents and the algorithms that train them.\" It allows \n\nagents to navigate a complex environment to accomplish a primary \n\ntask. The aim is to create a \"space for comparisons and improving \n\ntechniques for training non-destructive agents.\" AI Governance in Practice Report 2024  | 61 \n\n# Part VIII. \n\n# The copyright \n\n# challenge \n\nCopyright  refers to the rights that creators have over the \n\nexpression of their artistic or intellectual works. Although it is \n\nnot possible to provide an exhaustive list of \"works\" covered by \n\ncopyright legislation, globally copyright protection has been \n\nextended to include a wide range of works, such as literature, \n\nmusic, architecture and film. In the context of modern \n\ntechnology, computer software programs, e-books, online \n\njournal publications and the content of websites such as news \n\nreports and databases are also copyrightable. \n\n## Generative AI is raising new \n\n## challenges for copyright law. \n\nThe clear establishment of intellectual property \n\nrights around both inputs and outputs for generative \n\nAI models is of crucial importance to creative artists \n\nand the creative industries. In the face of dramatically \n\ngrowing machine capabilities, we need to make sure that \n\nincentives for human creation remain strong.\" \n\nLord Tim Clement-Jones \n\nU.K. House of Lords Liberal Democrat Peer and \n\nSpokesperson for Science, Innovation and Technology → Part VIII. The copyright challenge \n\nAI Governance in Practice Report 2024  | 62  \n\n> TABLE OF CONTENTS ↑\n\nLaw and policy considerations \n\nIn  most countries , and especially those party \n\nto the  Berne Convention , copyright protection \n\nis obtained automatically upon creation of the \n\nwork. In other words, copyright registration is \n\nnot necessary for proprietarily safeguarding \n\nartistic and intellectual works. Regardless, \n\nwhile offering automatic copyright protection, \n\nmany countries, including the U.S., also allow \n\nvoluntary  copyright registration. \n\nCopyright provides owners two types of rights: \n\neconomic rights, through which the owner can \n\nmake financial gains by authorizing use of their \n\nwork by others through a license, and moral \n\nrights, which include noneconomic interests such \n\nas the right to claim authorship of a work or to \n\noppose changes to a work that could harm the \n\nowner's reputation. \n\nCopyright  protects artistic and intellectual works \n\nby preventing others from copying, adapting, \n\ndistributing, performing or publicly displaying \n\nthe work, or creating derivative works. When \n\nan individual does any of these without the \n\nauthorization of the rights' owner, this may \n\nconstitute copyright infringement. \n\nThe use of copyright protected content requires \n\nthe authorization of the original author, unless a \n\nstatutory copyright exception applies. A legitimate \n\nexception to copyright infringement in some \n\njurisdictions is fair use or fair dealing. This is a \n\nlimitation on the exclusive rights of a copyright \n\nholder, which sometimes allows the use of the \n\nwork without the right holder's permission. \n\nIn the U.S., fair use is statutorily defined \n\nunder  17 U.S. Code § 107 , and four \n\nfactors assist courts in making a fair-use \n\ndetermination. These include purpose and \n\ncharacter of use, nature of copyrighted work, \n\nsubstantiality of use, and impact of use on \n\nthe potential market of the copyrighted \n\nwork. Similarly, Singapore's  Copyright Act \n\nof 2021  also includes a fair-use exemption \n\nand takes into account the same four factors \n\nas the U.S. courts. Singapore's old copyright \n\nlaw also had a fifth factor, which considered \n\nthe possibility of obtaining a work within a \n\nreasonable time at an ordinary commercial \n\nprice. However, under the new law, the fifth \n\nfactor may be considered by courts only \n\nwhen relevant. \n\nThe U.K. also has a permitted exemption \n\nto copyright infringement termed  fair \n\ndealing . There is no statutory definition for \n\nfair dealing as, depending on the case, it \n\nwill always be a matter of fact, degree and \n\nimpression. Other factors the U.K. courts \n\npreviously considered to determine fair \n\ndealing include the effect on the market for \n\nthe original work and whether the amount of \n\nwork copied was reasonable and  appropriate. \n\nCommon remedies that can be granted by \n\na court ruling on copyright infringement \n\ninclude injunctions, damages for the loss \n\nsuffered, statutory damages, infringer's \n\nprofits, surrender or destruction of infringing \n\narticles, and attorney fees and costs. \n\nThough copyright has emerged \n\nas one of the first and foremost \n\nfrontiers between AI and intellectual \n\nproperty, the full gamut of IP \n\nrights are engaged by AI, and \n\nspecifically generative AI: design \n\nrights, performers’ rights, patents \n\nand trademarks. Anthropocentric \n\napproaches to IP will butt up against \n\nAI’s learning techniques, its scale \n\nand the nature of its outputs, leaving \n\nmuch uncertainty, complexity and \n\nvariety in the implementation of AI \n\nand IP governance. \n\nJoe Jones \n\nIAPP Director of Research and Insights → Part VIII. The copyright challenge    \n\n> AI Governance in Practice Report 2024 |63\n> TABLE OF CONTENTS ↑\n\n## → SPOTLIGHT \n\n## Generative AI copyright litigation in the U.S. \n\nTwo main lines of argument are emerging in ongoing \n\nAI  copyright litigation in the U.S. \n\nPetitioners are arguing that: \n\n→ Defendants made copies of copyrighted works when ingesting them \n\nfor training foundation models. \n\n→ As the generated outputs were trained on copyrighted material, \n\nthe outputs themselves are also infringing derivative works. \n\nMore specifically, in a lawsuit against OpenAI, the New York Times argued \n\nthat OpenAI and Microsoft's generative AI tools were built by copying years \n\nof journalistic work without permission or payment, and both companies \n\nare making high profits through their generative AI tools, which now \n\ncompete with the news outlet as reliable sources of information. \n\nOpenAI's motion to dismiss the lawsuit provides background on fair \n\nuse law, and it argues courts have historically used fair use to protect \n\nuseful innovations and copyright is not a veto right over transformative \n\ntechnologies that leverage existing work internally. \n\nThe assessment of fair use is likely to include an \n\nevaluation of exactly what was or is being copied, \n\nwhether ingestion of copyrighted material amounts to \n\ntransformative use, the substantiality of the copying and \n\nthe economic harm caused by using copyrighted material \n\nin developing generative AI models on the potential \n\nmarket for the copyrighted work. \n\nSimilarly, in Tremblay v. OpenAI, various authors alleged copyright \n\ninfringement based on the ingestion of training data that copied \n\nthe works of the authors without consent, credit or compensation. \n\nA California court  recently  rejected  claims on vicarious copyright \n\ninfringements, Digital Millennium Copyright Act violations, negligence \n\nand unjust enrichment. → Part VIII. The copyright challenge \n\nAI Governance in Practice Report 2024  | 64  \n\n> TABLE OF CONTENTS ↑\n\nImplementing AI governance \n\nNumerous  copyright-safety solutions  and \n\nharm-mitigation strategies  are emerging, \n\nnotwithstanding the uncertainty present due \n\nto  pending litigation. \n\n→ Opt outs.  As foundation models are \n\ntrained on vast amounts of data online, \n\norganizations may not be aware that their \n\ncopyrighted material is used for training. \n\nIn those scenarios, when organizations \n\nare concerned about their webpages being \n\nscraped, an opt-out process, like that of \n\nOpenAI , may be a workable strategy to \n\nmitigate the risk of unwanted scraping. \n\n→ Liability considerations.  Given the fear of \n\npotentially becoming a copyright infringer \n\nas a user of generative AI, commercial \n\nusers may avoid engaging with providers \n\nof  generative AI services. \n\n→ Explore technical guardrails. \n\nOrganizations can also make use of \n\ntechnical guardrails that help them \n\nrespect  the copyrights of authors. Microsoft \n\nincorporated guardrails such as content \n\nfilters, operational monitoring, classifiers, \n\nabuse detection and other technologies to \n\nreduce the likelihood of Copilot returning \n\ncopyright-infringing content. \n\n→ Generative AI requirements.  To increase \n\ntransparency around data used to train \n\ngenerative AI models, including copyrighted \n\ndata, certain jurisdictions such as the \n\nEU require system providers to publish \n\ndetailed summaries of the content used for \n\ntraining their models. Further, with respect \n\nto copyright compliance, the EU AI Act \n\nrequires providers to implement a copyright \n\npolicy mandating protocols to identify and \n\nobserve applicable copyright laws. \n\nINDUSTRY EXAMPLE \n\nMicrosoft  committed to absolve \n\nits users of liability by assuming \n\nvicarious responsibility for \n\ninfringements when use of its \n\nCopilot service leads to legal issues \n\nfor their commercial customers. \n\nMicrosoft also requires customers \n\nto use content filters and other \n\nbuilt-in product safety features, \n\nand asks customers to not generate \n\ninfringing content, such as by \n\nproviding Copilot with inputs they \n\ndo not have the right to use. AI Governance in Practice Report 2024  | 65 \n\n# Part IX. \n\n# Third-party \n\n# AI assurance \n\nIn a recent  report  released by the U.K. government, assurance \n\nis defined as \"the process of measuring, evaluating and \n\ncommunicating something about a system or process, \n\ndocumentation, a product, or an organisation.\" Many of the \n\nAI governance implementation mechanisms discussed in this \n\nreport are forms of assurance. \n\nWhile establishing core competencies within an organization \n\nis beneficial to create strong AI-governance foundations \n\nacross the different  lines of defense , utilization of third-party \n\nAI assurance mechanisms may be an important or necessary \n\nconsideration depending on the type of AI used and the \n\norganization's knowledge and capacity. \n\nIntegrating third-party assurance into an AI-governance \n\nstrategy is a consideration at various stages of the life cycle. \n\n## AI assurance methods are crucial \n\n## for demonstrating accountability \n\n## and establishing trust. → Part IX. Third-party AI assurance \n\nAI Governance in Practice Report 2024  | 66  \n\n> TABLE OF CONTENTS ↑\n\nTypes of third-party assurance \n\nSome of the most practical tools for the realization \n\nof safe and responsible AI are emerging from \n\nthird-party AI assurance methods. \n\nAssessment \n\nAssessments are key mechanisms to evaluate \n\nvarious aspects of an AI system, including to \n\ndetermine the risk of a system or identify the \n\nsource of bias or determine the reason a system \n\nis making inaccurate predictions. Various \n\nservices and off-the-shelf products can be \n\nintegrated into AI governance practices based on \n\nwhat an organization is trying to determine from \n\nits assessment. \n\nCertain assessments must be conducted by the \n\nthird party providing the system to their customers, \n\nsuch as conformity assessments and impact \n\nassessments focusing on the impacts of the datasets \n\nused and the model itself. From a deployer's \n\nperspective, third-party due diligence enquiries \n\nshould be integrated into the organization's existing \n\nthird-party risk management program and include \n\nscreening at both the vendor enterprise and \n\nproduct levels. \n\nTesting and validation \n\nTesting techniques such as statistical tests to \n\nevaluate demographic fairness, assess system \n\nperformance or detect generative AI that may \n\nlead to copyright breaches are becoming widely \n\navailable through various third-party vendors. \n\nBefore choosing a vendor, it is important to have \n\na clear understanding of what the test is for \n\nand whether the context — which includes the \n\ntype of AI used, applicable jurisdictions and the \n\ndomain operating in — will impact the types of \n\ntests to run. \n\nConformity assessments \n\nConformity assessments are reviews completed \n\nby internal or external review functions to \n\nevaluate whether a product, system, process \n\nor individual adheres to an established set of \n\nrequirements. This is typically performed in \n\nadvance of a product or system being placed on \n\nthe market. While most assessments focus on \n\nevaluating aspects of AI systems, conformity \n\nassessments have been designed to evaluate \n\nquality-management systems , a set of processes \n\nfor those who build and deploy AI systems, and \n\nindividuals  who are involved in the development, \n\nmanagement or auditing of AI systems. \n\nFrom a deployer's perspective, the third-party \n\ndue diligence process should include vendor \n\ninquiries into product documentation, such as \n\ntechnical specifications, user guides, conformity \n\nassessments and impact assessments. \n\nRisk assessments should be done \n\nat several phases of development, \n\nstarting with the proposal/idea phase. \n\nIt's easier to incorporate some \n\n‘responsible by design' features early \n\non, rather than tack them on at the \n\nend. For example, filtering for toxic \n\ncontent in your training data, before \n\na model is trained, can be more \n\neffective than trying to catch toxic \n\ngenerated content afterwards. \n\nIn contrast, a full impact assessment \n\nshould be done once a model is fully \n\ndeveloped and evaluated, because it's \n\nhard to assess the impact without a lot \n\nof information about the final system. \n\nAndrew Gamino-Cheong \n\nTrustible AI Co-founder and Chief Technology Officer → Part IX. Third-party AI assurance \n\nAI Governance in Practice Report 2024  | 67  \n\n> TABLE OF CONTENTS ↑\n\nImpact assessments \n\nThe risk profile of AI systems can vary widely \n\nbased on the technical capabilities and intended \n\npurposes of the system, as well as the particular \n\ncontext of their implementation. Evaluating and \n\nmitigating the impacts of an AI system is therefore \n\na shared responsibility that must be owned by \n\nproviders and deployers alike in practice. The \n\norganization deploying a third-party AI system will \n\nhave a closer understanding of the specific context \n\nand impacts of deploying the system. Similarly, \n\nthe third-party vendor is best placed to evaluate \n\nthe impacts of the training, testing and validation \n\ndatasets, the model and infrastructure used to \n\ndesign and develop the system. \n\nAI/algorithmic auditing \n\nWhile there is not yet a formal audit practice as \n\nseen in financial services, there is a growing call \n\nfor those who audit AI systems to demonstrate \n\na common set of competencies, such as with a \n\ncertification or formal designation. These audits \n\nmay incorporate other third-party mechanisms \n\ndiscussed above to evaluate AI systems and ensure \n\nthey are safe, secure, legally compliant and meet \n\nrequisite standards, among other things. The \n\nNational Telecommunications and Information \n\nAdministration released  recommendations  for \n\nfederal agencies to use audit and auditors for the \n\nuse of high-risk AI systems. \n\nCanada's proposed Bill C-27, the Digital Charter \n\nImplementation Act, identifies that the Minister \n\nof Innovation, Science and Industry can issue an \n\nindependent audit if they have reasonable grounds \n\nto believe requirements outlined in the act have \n\nnot been met. This may encourage organizations \n\nto ensure compliance via preventative third-\n\nparty audits. Additionally, Canada identified the \n\nimportance of international standards to help \n\nsupport the desired objectives of the act. \n\nCertifications \n\nCertifications are marks or declarations provided \n\nafter evaluations or audits are performed against \n\nstandards or conformity assessments . The mark \n\nindicates the AI system adheres to certain specified \n\nrequirements. It is important to note certifications \n\ncan also be provided to quality-management \n\nsystems used throughout the life cycle of an AI \n\nsystem or to individuals, demonstrating that they \n\nmet a set of competencies. \n\nOrganizations need a clear \n\nunderstanding of how AI risk will \n\naffect their business through \n\nthird-party relationships. \n\nThey should proactively review their \n\ninventory of vendors and identify \n\nthose that provide AI solutions or \n\ncomponents. They also need to be \n\naware of the development plans for \n\nall third-party products, including \n\nwhether, how, and when AI will be \n\nintegrated. With that understanding, \n\npartners, vendors and their products \n\nmay need to be reassessed to \n\naccount for AI risk with updated due \n\ndiligence processes. \n\nAmber Gosney \n\nFTI Technology Managing Director → Part IX. Third-party AI assurance    \n\n> AI Governance in Practice Report 2024 |68\n> TABLE OF CONTENTS ↑\n\n## → SPOTLIGHT \n\n## Algorithmic audits as airplane cockpits \n\nAt ORCAA, we use the analogy of an airplane cockpit \n\nto talk about algorithmic audits. In an airplane cockpit, \n\nthe dials and gauges take measurements that relate to \n\npossible failure modes. \n\nFor instance, the fuel gauge says if the plane is about to run out of gas, \n\nand the attitude indicator says if it is going to dive or roll. These dials \n\nhave 'redlines': threshold values that, if exceeded, mean the pilot needs to \n\nintervene. The auditor's job is to design a 'cockpit' for a given algorithmic \n\nsystem. This involves identifying failure modes -- how the system could \n\nresult in harm to various stakeholders -- and building 'dials' that measure \n\nconditions that lead to failures. At ORCAA, we have developed frameworks \n\nfor doing these critical tasks. \n\nSome other aspects of this analogy are worth noting. A cockpit identifies \n\nproblems but does not fix them. An indicator light will say an engine is out, \n\nbut it won't say how to repair or restart the engine. Likewise, an algorithmic \n\ncockpit should indicate when a failure is imminent, but it is the job of the \n\nsystem deployer, the 'pilot,' to intervene. A cockpit is a critical piece of \n\nairplane safety, but it's not the whole picture. Planes are tested extensively \n\nbefore being put into service, both during the design phase and when \n\nthey roll off the assembly line and are periodically taken out of service for \n\nregular inspections and maintenance. \n\nLikewise, algorithmic cockpits, which are critical \n\nfor safety while the system is deployed, should be \n\ncomplemented by predeployment testing and regular \n\ninspections and maintenance during deployment.\" \n\nCathy O'Neil \n\n> O'Neil Risk Consulting & Algorithmic Auditing CEO\n\nAI Governance in Practice Report 2024  | 69 \n\n# Conclusion \n\nOrganizations may seek to leverage existing organizational \n\nrisk frameworks to tackle AI risk at enterprise, product and \n\noperational levels. Tailoring their approach to AI governance \n\nto their specific AI product risks, business needs and \n\nbroader strategic objectives can help organizations establish \n\nthe building blocks of trustworthy and responsible AI. A key \n\ngoal of the AI governance program is to facilitate responsible \n\ninnovation. Flexibly adapting existing governance processes \n\ncan help businesses to move forward with exploring the \n\ndisruptive competitive opportunities that AI technologies \n\npresent, while minimizing associated financial, operational \n\nand reputational risks. \n\n## Bringing it all together and \n\n## putting it into action. Uzma Chaudhry \n\nIAPP AI Governance Center \n\nResearch Fellow \n\nuchaudhry@iapp.org \n\nJoe Jones \n\nIAPP Director of Research \n\nand Insights \n\njjones @iapp.org \n\nAshley Casovan \n\nIAPP AI Governance Center \n\nManaging Director \n\nacasovan@iapp.org \n\nLynsey Burke \n\nIAPP Research and Insights \n\nProject Specialist \n\nlburke@iapp.org \n\nNina Bryant \n\nFTI Technology Senior \n\nManaging Director \n\nnina.bryant@fticonsulting.com \n\nLuisa Resmerita \n\nFTI Technology \n\nSenior Director \n\nluisa.resmerita@fticonsulting.com \n\nMichael Spadea \n\nFTI Technology Senior \n\nManaging Director \n\nmicheal.spadea@fticonsulting.com \n\nFollow the IAPP on social media \n\nD C Q E\n\nPublished June 2024. \n\nIAPP disclaims all warranties, expressed or implied, with respect to \n\nthe contents of this document, including any warranties of accuracy, \n\nmerchantability, or fitness for a particular purpose. Nothing herein \n\nshould be construed as legal advice. \n\n© 2024 IAPP. All rights reserved. \n\n# Contacts \n\n## Connect with the team",
  "fetched_at_utc": "2026-02-08T19:06:59Z",
  "sha256": "d10ba8f26b3a8a5f315538e4f40dcd3ba10b228963f59e2d2c3a180ca8b56e2b",
  "meta": {
    "file_name": "AI Governance in Practice Report 2024 - IAPP.pdf",
    "file_size": 39709902,
    "relative_path": "pdfs\\AI Governance in Practice Report 2024 - IAPP.pdf",
    "jina_status": 20000,
    "jina_code": 200,
    "usage": {
      "tokens": 29464
    }
  }
}