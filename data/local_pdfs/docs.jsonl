{"doc_id": "pdf-pdfs-a-practical-guide-to-ai-and-copyright-oliver-patel-308aa353afd3", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\A Practical Guide to AI and Copyright - Oliver Patel.pdf", "title": "A Practical Guide to AI and Copyright - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel \n\nhttps://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright  2/20 This free newsletter delivers practical, actionable and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! For more frequent updates, be sure to follow me on LinkedIn .This special edition of Enterprise AI Governance contains everything you need to know to navigate the thorny issue of AI and copyright . It is packed full of practical resources, including: \n\nâœ… 5 practical steps to mitigate risks \n\nâœ… A primer on AI and copyright \n\nâœ… International regulatory snapshot  ðŸ‡ºðŸ‡¸  ðŸ‡ªðŸ‡º  ðŸ‡¬ðŸ‡§  ðŸ‡¯ðŸ‡µ  ðŸ‡¨ðŸ‡³  ðŸ‡°ðŸ‡· \n\nâœ… Relevant litigation and fair use arguments \n\nâœ… Generative AI and Copyright Cheat Sheet ( developed in partnership with Copyright Clearance Center ) - scroll to the bottom for this free subscriber only pdf download!   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 3/20\n\nIf this topic interests you, then check out this LinkedIn Live event I am speaking at today (Thursday 27 th February) at 3pm GMT / 10am EST. Itâ€™s a fireside chat on â€˜ Bridging Innovation and Integrity in Responsible AIâ€™ , hosted by Copyright Clearance Center . Weâ€™ll be covering how to build and implement an enterprise AI governance programme, as well as copyright and AI challenges. 800+ people have signed up so far and it would be great to see you there! \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week. \n\nFrequent readers of this newsletter know that I try to front-load value and actionable advice. Therefore, before my deeper analysis of the AI and copyright issue, here are 5 practical steps organisations can take today to mitigate copyright risks. \n\n1. Licensing: use data and content explicitly licensed for use in AI systems. Where appropriate, renegotiate contracts to enable this. \n\n5 practical steps to mitigate risks   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 4/20\n\n2. Contractual safeguards: negotiate robust clauses, such as IP indemnity protection, in contracts with AI providers. However, be wary that these indemnity clauses do not apply in all circumstances, nor do they offer complete protection. \n\n3. Data lineage: ensure copyright considerations are a core part of AI risk assessments. These assessments should track and record the lineage of all data used for AI training and development. \n\n4. AI literacy: scale employee training and awareness, particularly on document and multimedia uploads and prompting. Now that AI is at the fingertips of all employees, anyone can exacerbate copyright infringement risk. \n\n5. EU AI Act compliance: General-Purpose AI (GPAI) model providers must prepare for their obligation to publish training data summaries. This is applicable from August 2025 (or 2027 for models already on the market). The forthcoming GPAI Model Code of Practice and associated template will outline how to do this. AI deployers and downstream providers (which integrate GPAI models into new AI systems) should conduct robust due diligence and choose their providers wisely. \n\nWhat is copyright? \n\nA primer on AI and copyright   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 5/20\n\nCopyright is a form of intellectual property that protects original works of authorship, including articles and books, to encourage and promote culture, science, and innovation. Under copyright law, copyright owners, such as authors and publishers, have exclusive rights regarding how their work is used, shared, or reproduced. Copyright laws attempt to strike a balance between ensuring fair compensation for creators of original work, whilst enabling wider society to benefit from that work, for example by permitting engagement, commentary, and transformation. Throughout history, technological developments, such as the advent of radio, television, and the internet, have fundamentally altered how content is developed, distributed, and consumed. Each new wave of technological change has been accompanied with meaningful changes in copyright laws and the way they are applied. \n\nAI and copyright: what is the issue? \n\nRapid advances in the field of machine learning, and generative AI in particular, raise important legal questions and challenges relating to copyright.   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 6/20\n\nThese challenges are relevant for all organisations which use AI. Copyright infringement is a significant risk of AI development and deployment. This is because of the vast amount of published material protected by copyright, such as books, videos, music, news articles, and software, required to train certain AI models. Those AI models can also reproduce copyrighted material in their outputs. If AI-generated outputs closely resemble original works (which formed part of the training data) this can infringe copyright. Copyrighted material can also be usedâ€“whether intentionally or unintentionallyâ€“to prompt, augment, or fine-tune AI. Therefore, although the large foundation model developers are currently in the spotlight, many other organisations could get caught in the crosshairs of copyright litigation. Organisations must also consider whether their AI-generated output (e.g., content) is protected by copyright. In most jurisdictions, the key factor is the level and nature of human involvement in generating this output. AI-assisted works may be copyrightable, but purely AI-generated works may not be.   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 7/20\n\nThe copyright risks of AI are apparent in the dozens of lawsuits copyright owners have brought against unauthorised use of their protected works in AI systems. \n\nTraining AI models \n\nTraining AI models, such as Large Language Models (LLMs), often involves the collection and processing of large amounts of copyright protected material. LLM training involves breaking down this text into tokens , each of which is assigned a unique integer ID (i.e., a mathematical representation of the text, which the machine can process). During training, the model processes these tokens, identifying patterns and structures, effectively compressing and retaining representations of the training data, including potentially copyrighted content, within its parameters. During inference, the model predicts the next token which should appear in the sequence, based upon the statistical patterns and structures it has been exposed to. This process allows the model to generate new text based on learned patterns. The key question is whether the copying and use of this data, for AI training and   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 8/20\n\ninference, infringes on copyright. There are strong arguments both for and against, which I summarise below. \n\nCopyright infringement risk across the AI lifecycle AI model training : many AI models are trained on large datasets, often incorporating copyrighted material. \n\nAI model fine-tuning: the fine-tuning and adjustment of pre-trained AI models can be done leveraging copyright protected material. \n\nRetrieval-augmented generation: RAG applications can incorporate or reproduce copyrighted material as part of their retrieval corpus or augmented outputs. \n\nInputs and prompts: users can input or upload copyrighted material as part of prompts, such as images, documents, or text excerpts. \n\nAI inference & outputs: AI-generated content may reproduce or closely resemble copyrighted material, potentially infringing. \n\nAI-generated outputs   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 9/20\n\nWhilst some organisations will primarily be concerned with whether their AI development activities are copyright infringing, many others will be keen to understand whether their AI-generated outputs are copyright protected (i.e., copyrightable). The extent to which AI-generated outputs can be copyright protected varies. In most jurisdictions, human authorship is required .This is because copyright is a legal right afforded to humans. Therefore, without human authorship, there is no copyright. This has been reinforced in several legal cases , which determined that AI itself cannot be the author of a copyright protected work. In other words, AI-generated work is not copyrightable if no human was involved. â€˜AI-assistedâ€™ works may be copyrightable, whereas purely AI-generated works are most likely not. What matters is the nature and degree of human involvement and creativity. For example, the U.S. Copyright Officeâ€™s position is that merely inputting a prompt is insufficient for copyright protection. This is because simple prompting does not usually enable the user to exert meaningful control over the nature of the output.   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 10/20\n\nHowever, this changes if the humanâ€™s work or creativity is perceptible in the AI output, or if the human can modify, arrange, or shape the AI-generated output in a meaningful way. In such cases, copyright protection may be granted for a human-assisted AI generated work. This will usually be determined on a case-by-case basis. There is no global copyright law. Rather, there is significant global divergence in copyright laws and their application to AI. Also, the applicable legal framework is determined by the jurisdiction in which the copying happened. Therefore, it is not possible to make blanket claims about whether AI development or deployment infringes copyright, as it will always depend on the relevant jurisdiction, the material which has been used, and exactly how that material has been used. Crucially, there are exceptions in copyright law, such as â€˜fair useâ€™ in the U.S . These exceptions stipulate that, in specific scenarios, copyrighted material can be used without permission. \n\nInternational regulatory snapshot   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 11/20\n\nGiven that there is broad international consensus on the requirement for human authorship for AI output to be copyrightable, the below will instead focus on whether the use of copyright protected material in AI training, development, and deployment is permitted under applicable exceptions. \n\nðŸ‡ºðŸ‡¸  USA: There is no specific law or regulation regarding AI and copyright, so existing exclusive rights (like the copyright holderâ€™s right of reproduction) and exceptions and limitations (like fair use) apply. Whether AI model training constitutes fair use (and is therefore not copyright infringing) is determined by courts on a case-by-case basis. When evaluating whether an activity constitutes fair use, U.S. courts consider four key factors: \n\n1. Purpose and character of the use. \n\n2. Nature of the copyrighted work. \n\n3. Amount of the portion taken. \n\n4. Market effects. \n\nðŸ‡ªðŸ‡º  EU: The EU Copyright Directive includes two text and data mining (TDM) exceptions â€”one for scientific research and another that allows broader use (including for commercial purposes), unless the copyright holder opts out. The TDM exception could apply to AI training, but this has not been tested in courts.   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 12/20\n\nThe EU AI Act reinforces that existing EU copyright law applies to AI systems and introduces transparency requirements regarding the training data used. Specifically, providers of general-purpose AI models will be obliged to publish summaries of their training data and implement a copyright compliance policy. \n\nðŸ‡¬ðŸ‡§  UK: The UK launched a Copyright and AI consultation in December 2024, which outlined a proposal to adopt a framework akin to the EUâ€™s (i.e., with a data mining exception covering commercial use). This would create a more permissive regime than is currently in place. The consultation closed earlier this week. The governmentâ€™s proposals were widely criticised by artists and publishers, but backed by AI companies. \n\nðŸ‡¯ðŸ‡µ  Japan: Japan has a nuanced approach . While Article 30-4 of the Copyright Act permits AI training in some cases, this is not a blanket exemption, and it has limitations. For example, if copying the material for \"enjoyment purposes\" or prejudice to rights holders are involved. Such prejudice includes â€œmaterial impact on the relevant marketsâ€. Furthermore, AI-generated outputs that are \"similar\" and \"dependent\" on copyrighted works can still be infringing. \n\nðŸ‡¨ðŸ‡³  China: China has a stricter AI and copyright legal framework. There are no explicit exemptions for TDM or AI training. Enforcement by Chinese courts is expected to be relatively strict.   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 13/20\n\nðŸ‡°ðŸ‡·  Republic of Korea: The Korean Copyright Commission encourages AI companies to proactively secure licenses, prior to using copyrighted material for AI training. There are dozens of ongoing legal cases around the world adjudicating AI and copyright disputes. \n\nOne tracker lists 39 relevant cases, including Getty Images v Stability AI, The New York Times v Microsoft and OpenAI, and Farnsworth v Meta. Many cases are in the U.S., where courts are determining whether various uses of copyrighted material in AI systems are infringing or constitute fair use. There are strong arguments on both sides of the debate. Those arguing that AI training does not meet the criteria for fair use highlight that original copyright protected materials (e.g., text) are copied for training and then stored, as mathematical representations, in model parameters, which is why LLMs can â€˜memoriseâ€™ and reproduce training data content, thereby infringing the right to reproduction. \n\nRelevant litigation and fair use arguments", "fetched_at_utc": "2026-02-08T18:49:15Z", "sha256": "308aa353afd3ceb132f6a93d8bf7fd6bc90ffd1e39a38378c315b9ea54002fa0", "meta": {"file_name": "A Practical Guide to AI and Copyright - Oliver Patel.pdf", "file_size": 910317, "relative_path": "pdfs\\A Practical Guide to AI and Copyright - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 3146}}}
{"doc_id": "pdf-pdfs-ai-governance-in-practice-report-2024-iapp-d10ba8f26b3a", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Governance in Practice Report 2024 - IAPP.pdf", "title": "AI Governance in Practice Report 2024 - IAPP", "text": "AI Governance in Practice Report 2024 AI Governance in Practice Report 2024  | 2\n\n# Table of \n\n# contents \n\n## What's inside? \n\nExecutive summary  3\n\nPart I. Understanding AI and governance  . . . . . . . . . . . . . . . . . . .  6\n\nPart II. The data challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  15 \n\nPart III. The  privacy and data protection challenge  23 \n\nPart IV. The  transparency, explainability \n\nand interpretability challenge  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  32 \n\nPart V. The bias, discrimination and fairness challenge  41 \n\nPart VI. The  security and robustness challenge  . . . . . . . . . . . . .  50 \n\nPart VII. AI  safety  55 \n\nPart VIII. The  copyright challenge  61 \n\nPart IX. Third-party AI assurance  . . . . . . . . . . . . . . . . . . . . . . . . .  65 \n\nConclusion  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  69 \n\nContacts  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  70 AI Governance in Practice Report 2024  | 3\n\n# Executive \n\n# summary \n\n## Recent and rapidly advancing \n\n## breakthroughs in machine \n\n## learning technology have forever \n\n## transformed the landscape of AI. \n\nAI systems have become powerful engines capable of \n\nautonomous learning across vast swaths of information and \n\ngenerating entirely new data. As a result, society is in the midst \n\nof significant disruption with the surge in AI sophistication and \n\nthe emergence of a new era of technological innovation. \n\nAs businesses grapple with a future in which the boundaries of \n\nAI only continue to expand, their leaders face the responsibility \n\nof managing the various risks and harms of AI, so its benefits \n\ncan be realized in a safe and responsible manner. \n\nCritically, these benefits are accompanied by serious \n\nconsiderations and concerns about the safety of this technology \n\nand the potential for it to disrupt the world and negatively \n\nimpact individuals when left unchecked. Confusion about how \n\nthe technology works, the introduction and proliferation of bias \n\nin algorithms, dissemination of misinformation, and privacy \n\nrights violations represent only a sliver of the potential risks. \n\nThe practice of  AI governance  is designed to tackle these \n\nissues. It encompasses the growing combination of principles, \n\nlaws, policies, processes, standards, frameworks, industry \n\nbest practices and other tools incorporated across the design, \n\ndevelopment, deployment and use of AI. â†’ Executive summary    \n\n> AI Governance in Practice Report 2024 |4\n> TABLE OF CONTENTS â†‘\n\nWhile relatively new, the field of AI governance \n\nis maturing, with government authorities \n\naround the world beginning to develop targeted \n\nregulatory requirements and governance \n\nexperts supporting the creation of accepted \n\nprinciples, such as the Organisation for \n\nEconomic Co-Operation and Development's  AI \n\nPrinciples , emerging best practices and tools for \n\nvarious uses of AI in different domains. \n\nThere are many challenges and potential \n\nsolutions for AI governance, each with unique \n\nproximity and significance based on an \n\norganization's role, footprint, broader risk-\n\ngovernance profile and maturity. This report \n\naims to inform the growing, increasingly \n\nempowered and increasingly important \n\ncommunity of AI governance professionals \n\nabout the most common and significant \n\nchallenges to be aware of when building \n\nand maturing an AI governance program. \n\nIt offers actionable, real-world insights \n\ninto applicable law and policy, a variety of \n\ngovernance approaches, and tools used to \n\nmanage risk. Indeed, some of the challenges \n\nto AI governance overlap and run through \n\na range of themes. Therefore, an emerging \n\nsolution for one thematic challenge may \n\nalso be leveraged for another. Conversely, in \n\ncertain circumstances, specific challenges and \n\nassociated solutions may conflict and require \n\nreconciliation  with other approaches. Some of \n\nthese potential overlaps and conflicts have been \n\nidentified throughout the report. \n\nGlobal AI private investment \n\n(USD billion, 2021) 2013 2014 2015 2016 2017 2018 2019 2020 2021 4810 18 22 38 42 47 94 â†’ Executive summary \n\nAI Governance in Practice Report 2024  | 5\n\nTABLE OF CONTENTS  â†‘\n\n# Questions about whether and when \n\n# organizations should prioritize \n\n# AI governance are being answered: \n\n# \"yes\" and \"now,\" respectively. \n\nQuestions about whether and when organizations should prioritize \n\nAI governance are being answered: \"yes\" and \" now ,\" respectively. \n\nThis  report is, therefore, focused on how organizations can approach, \n\nbuild and leverage AI governance in the context of the increasingly \n\nvoluminous and complex applicable landscape. \n\nJoe Jones \n\nIAPP Director of Research \n\nand Insights \n\nAshley Casovan \n\nIAPP AI Governance Center \n\nManaging Director \n\nUzma Chaudhry \n\nIAPP AI Governance \n\nCenter Research Fellow \n\nNina Bryant \n\nFTI Technology Senior \n\nManaging Director \n\nLuisa Resmerita \n\nFTI Technology \n\nSenior Director \n\nMichael Spadea \n\nFTI Technology Senior \n\nManaging Director AI Governance in Practice Report 2024  | 6\n\n# Part I. \n\n# Understanding \n\n# AI and \n\n# governance \n\n## Components of an AI system and \n\n## their governance \n\nTo understand how to govern an AI system, it is important to first \n\nunderstand what an AI system is. The EU AI Act, for example, \n\ndefines an AI system as \"a machine-based system that is designed \n\nto operate with varying levels of autonomy and that may exhibit \n\nadaptiveness after deployment, and that, for explicit or implicit \n\nobjectives, infers, from the input it receives, how to generate outputs \n\nsuch as predictions, content, recommendations, or decisions that \n\ncan influence physical or virtual environments.\" \n\nAs indicated in the OECD's Framework for the Classification of AI \n\nsystems, AI systems are comprised of data used to train and operate \n\na system, model, output and context. While a model is a fundamental \n\nbuilding block of an AI system, a single model seldom operates in \n\nisolation. Instead, multiple AI models come together and interact \n\nwith each other to form complex AI systems. Additionally, AI systems \n\nare often designed to interact with other systems for sharing data, \n\nfacilitating seamless integration into real-world environments. \n\nThis  results in a network of AI systems, each with its specialized \n\nmodels, working  together to achieve a  larger goal. \n\nWith AI poised to revolutionise many aspects of our \n\nlives, fresh cooperative governance approaches are \n\nessential. Effective collaboration between regulatory \n\nportfolios, within nations as well as across borders, \n\nis crucial: both to safeguard people from harm and to \n\nfoster innovation and growth. \n\nKate Jones \n\nU.K. Digital Regulation Cooperation Forum CEO â†’ Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 7 \n\n> TABLE OF CONTENTS â†‘\n\nAI governance is about to get a lot \n\nharder. The internal complexity of \n\ngoverning AI is growing as more \n\ninternal teams adopt AI, new AI \n\nfeatures are built, and the systems \n\nget complex, but at the same time, \n\nthe external complexity is also set to \n\ngrow rapidly with new regulations, \n\ncustomer demands, and safety \n\nresearch evolving. \n\nThe organizations who have invested \n\nin structured AI governance already \n\nhave a leg up and will continue to \n\nhave a competitive advantage. \n\nAndrew Gamino-Cheong \n\nTrustible AI Co-founder and Chief Technology Officer \n\nNavigating AI governance sources \n\nGiven the complexity and transformative \n\nnature of AI, significant work has been done \n\nby law and policymakers on what is now a \n\nvast and growing body of principles, laws, \n\npolicies, frameworks, declarations, voluntary \n\ncommitments, standards and emerging best \n\npractices that can be challenging to navigate. \n\nMany of these various sources interact with \n\neach other, either directly or by virtue of the \n\nissues covered. \n\nAI principles, such as the OECD's AI Principles \n\nor UNESCO's Recommendation on the Ethics \n\nof AI, can shape global standards, especially \n\nwhen national governments pledge to \n\nvoluntarily incorporate such guidance into \n\ntheir domestic  AI governance initiatives. \n\nThey  provide a nonbinding, principled \n\napproach to guide legal, policy and industry \n\nefforts toward tackling thematic challenges. \n\nAlgorithm Watch created an inventory of these \n\nprinciples, identifying 167  reports .\n\nLaws and regulations include existing \n\nlegislation that is not specific but is \n\nnonetheless applicable to AI, as well as \n\nemerging legislation that more specifically \n\naddresses the governance of AI systems, such \n\nas the EU AI Act. The EU AI Act is the world's \n\nfirst comprehensive AI regulation. Although \n\njurisdictional variations can be observed across \n\nthe emerging global AI regulatory landscape, \n\nmany draft regulations adopt a risk-based \n\napproach similar to the EU AI Act. \n\nThe EU AI Act mandates AI governance standards \n\nbased on the risk classification of AI systems and \n\nthe organization's role as an AI actor. Certain \n\nAI systems are deemed to pose unacceptable \n\nrisk and are prohibited by law, subject to very \n\nnarrow exceptions. The bulk of the requirements \n\nimposed by the act apply to providers of high-risk \n\nAI systems, although deployers and resellers, \n\nnamely distributers and importers, are are also \n\nsubject to direct obligations. \n\nThe act imposes regulatory obligations at \n\nenterprise, product and operational levels, \n\nsuch as establishing appropriate accountability \n\nstructures, assessing system impact, providing \n\ntechnical documentation, establishing risk \n\nmanagement protocols and monitoring \n\nperformance, among other key requirements. \n\nIn  the context of the growing variety of \n\ngenerative AI use cases and adoption of \n\nsolutions embedding generative AI such as MS \n\nCopilot, general purpose AI-specific provisions \n\nare another crucial component of the EU AI \n\nAct. Depending on their capabilities, reach and \n\ncomputing power, certain GPAI systems are \n\nconsidered to present systemic risk and attract \n\nbroadly similar obligations to those applicable \n\nto high-risk AI systems. â†’ Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 8 \n\n> TABLE OF CONTENTS â†‘\n\nIn addition to binding legislation, voluntary \n\nAI frameworks, such as the National Institute \n\nof Standards and Technology's AI Risk \n\nManagement Framework and the International \n\nOrganization for Standardization's AI Standards, \n\noffer structured and actionable guidance \n\nstakeholders can elect to use to support \n\ntheir work on implementing AI governance. \n\nVoluntary commitments are often developed to \n\nbring different stakeholders closer to a shared \n\nunderstanding of identifying, assessing and \n\nmanaging risks. Standards serve as benchmarks \n\nthat can demonstrate compliance with \n\nregulatory requirements. \n\nInternational declarations and commitments \n\nmemorialize shared commitments, often between \n\ngovernments, to specific aspects or broad \n\nswathes of AI governance. While not binding, \n\nsuch commitments can, at a minimum, indicate \n\na country's support for and intention to advance \n\nAI  governance in particular or general ways, even \n\nat the highest of levels. \n\nNavigating a growing body of draft AI laws, \n\nregulations, standards and frameworks can \n\nbe challenging for organizations pioneering \n\nwith AI. By understanding their unique AI risk \n\nprofile and adopting a risk-based approach, \n\norganizations can build a robust and scalable \n\nAI governance framework that can be deployed \n\nacross jurisdictions. \n\nIAPP Global AI Law and Policy Tracker  \n\n> This map shows the jurisdictions in focus and covered by the IAPP Global AI Law and Policy Tracker . It does not represent the extent to which jurisdictions\n> around the world are active on AI governance legislation. Tracker last updated January 2024.\n\nâ†’ Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 9\n\nTABLE OF CONTENTS  â†‘\n\nThe following are  examples  of some of the most prominent and consequential  AI governance efforts :\n\nPrinciples \n\nâ†’ OECD AI Principles \n\nâ†’ European Commission's Ethics Guidelines for Trustworthy AI \n\nâ†’ UNESCO Recommendation on the Ethics of AI \n\nâ†’ The White House Blueprint for an AI Bill of Rights \n\nâ†’ G7 Hiroshima Principles \n\nLaws and \n\nregulations \n\nâ†’ EU AI Act \n\nâ†’ EU Product Liability Directive, proposed \n\nâ†’ EU General Data Protection Regulation \n\nâ†’ Canada â€“ AI and Data Act, proposed \n\nâ†’ U.S. AI Executive Order 14110 \n\nâ†’ Sectoral U.S. legislation for employment, housing and consumer finance \n\nâ†’ U.S. state laws, such as Colorado AI Act, Senate Bill 24-205 \n\nâ†’ China's Interim Measures for the Management of Generative AI Services \n\nâ†’ The United Arab Emirates Amendment to Regulation 10 to include new rules on \n\nProcessing Personal Data through Autonomous and Semi-autonomous Systems \n\nâ†’ Digital India Act \n\nAI frameworks \n\nâ†’ OECD Framework for the classification of AI Systems \n\nâ†’ NIST AI RMF \n\nâ†’ NIST Special Publication 1270: Towards a Standard for Identifying and Managing Bias in AI \n\nâ†’ Singapore AI Verify \n\nâ†’ The Council of Europe's Human Rights, Democracy, and the Rule of Law Assurance \n\nFramework for AI systems \n\nDeclarations \n\nand voluntary \n\ncommitments \n\nâ†’ Bletchley Declaration \n\nâ†’ The Biden-Harris Administration's voluntary commitments from leading AI companies \n\nâ†’ Canada's guide on the use of generative AI \n\nStandards \n\nefforts \n\nâ†’ ISO/IEC JTC 1 SC 42 \n\nâ†’ The Institute of Electrical and Electronics Engineers Standards Association P7000 \n\nâ†’ The European Committee for Electrotechnical Standardization  AI standards for EU AI Act \n\nâ†’ The  VDE Association's AI Quality and Testing Hub \n\nâ†’ The British Standards Institution and  Alan Turing Institute AI Standards Hub \n\nâ†’ Canada's  AI and Data Standards Collaborative \n\nIt is important to take an \n\necosystem approach to AI \n\ngovernance. Policy makers and \n\nindustry need to work together \n\nat platforms such as the AI Verify \n\nFoundation to make sense of the \n\nopportunities and risks that this \n\ntechnology brings. The aim is to \n\nfind common guardrails to manage \n\nkey risks in order to create a \n\ntrusted ecosystem that promotes \n\nmaximal innovation. \n\nDenise Wong \n\nSingapore Infocomm Media Development Authority Assistant Chief \n\nExecutive, Data Innovation & Protection Group â†’ Part I. Understanding AI and governance    \n\n> AI Governance in Practice Report 2024 |10\n> TABLE OF CONTENTS â†‘\n\nThe AI governance imperative \n\nWith private investment, global adoption rates \n\nand regulatory activity on the rise, as well as \n\nthe growing maturity of the technology, AI is \n\nincreasingly becoming a strategic priority for \n\norganizations and governments worldwide. \n\nOrganizations of all sizes and industries are \n\nincreasingly engaging with AI systems at various \n\nstages of the technology product supply  chain. \n\nThe exceptional dependence on high volumes \n\nof data and endless practical applicability that \n\nmake AI technology a disruptive opportunity \n\ncan also generate uniquely multifaceted risks for \n\nbusinesses and individuals. These include legal, \n\nregulatory, reputational and/or financial risks to \n\norganizations, but also risks to individuals and \n\nthe wider society. \n\nAI Risks \n\nREPUTATIONAL \n\n> Risk of\n> damage to\n> reputation\n> and market\n> competitiveness\n\nFINANCIAL \n\n> Risk of financial\n> implications,\n> e.g., fines, legal\n> or operational\n> costs, or\n> lost profit\n\nLEGAL AND \n\nREGULATORY \n\n> Risk of\n> noncompliance\n> with legal and\n> contractual\n> obligations\n\nINDIVIDUALS \n\nAND SOCIETY     \n\n> Risk of bias\n> or other\n> detrimental\n> impact on\n> individuals â†’Part I. Understanding AI and governance\n> AI Governance in Practice Report 2024 |11\n> TABLE OF CONTENTS â†‘\n\nEnterprise governance \n\nAI governance starts with defining the corporate \n\nstrategy for AI by documenting: \n\nâ†’ Target operating models to set out clear roles \n\nand responsibilities for AI risk. \n\nâ†’ Compliance assessments to establish \n\nprogram maturity and remediation priorities. \n\nâ†’ Accountability processes to record and \n\ndemonstrate compliance. \n\nâ†’ Policies and procedures to formulate policy \n\nstandards and operational procedures. \n\nâ†’ Horizon scanning to enhance and \n\nalign  the program with ongoing \n\nregulatory  developments. \n\nProduct governance \n\nAI governance also requires enterprise policy \n\nstandards to be applied at the product level. \n\nOrganizations can ensure their AI products \n\nmatch their enterprise strategy by using: \n\nâ†’ System impact assessments to identify and \n\naddress risk prior to product development \n\nor  deployment. \n\nâ†’ Quality management procedures tailored \n\nto the software development life cycle to \n\naddress risk by design. \n\nâ†’ Risk and controls frameworks to define \n\nAI risk and treatment based on widely \n\nrecognised standards such as ISO and NIST. \n\nâ†’ Conformity assessments and declarations to \n\ndemonstrate their products are compliant. \n\nâ†’ Technical documentation including \n\nstandardized instructions of use and \n\ntechnical product specifications. \n\nâ†’ Post-market monitoring plans to monitor \n\nproduct compliance following market launch. \n\nâ†’ Third-party due diligence assessments \n\nto identify possible external risk and \n\ninform  selection. â†’ Part I. Understanding AI and governance    \n\n> AI Governance in Practice Report 2024 |12\n> TABLE OF CONTENTS â†‘\n\nOperational governance \n\nThe organization's AI strategy must ultimately be operationalized \n\nthroughout the business through the development of: \n\nâ†’ Performance monitoring protocols to ensure systems perform \n\nadequately for their intended purposes. \n\nâ†’ Transparency and human oversight initiatives to ensure \n\nindividuals are aware and can make informed choices when they \n\ninteract with AI systems or when AI-powered decisions are made. \n\nâ†’ Incident management plans to identify, escalate and respond to \n\nserious incidents, malfunctions and national risks impacting AI \n\nsystems and their operation. \n\nâ†’ Communication strategies to ensure transparency toward \n\ninternal and external stakeholders in relation to the \n\norganization's AI practices. \n\nâ†’ Training and awareness programs to enable staff with roles and \n\nresponsibilities for AI governance to help them understand and \n\nperform their respective roles. \n\nâ†’ Skills and capabilities development to assess human resources \n\ncapabilities and review or design job requirements. â†’ Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 13 \n\nTABLE OF CONTENTS  â†‘\n\nAn effective AI governance \n\nmodel is about collective \n\nresponsibility and collective \n\nbusiness responsibility, \n\nwhich should encompass \n\noversight mechanisms such \n\nas privacy, accountability, \n\ncompliance, among others. \n\nThis responsibility should be \n\nshared by every stakeholder \n\nwho is part of the AI \n\ngovernance chain. \n\nVishal Parmar \n\nBritish Airways Global Lead Privacy Counsel \n\nand Data Protection Officer \n\nUnderstanding that AI systems, like all \n\nproducts, follow a life cycle is important as \n\nthere are governance considerations across \n\nthe life cycle. The  NIST AI RMF  sets out a \n\ncomprehensive articulation of the AI system \n\nlife  cycle and includes considerations for \n\ntesting, evaluation, validation, verification \n\nand key stakeholders for each phase. A more \n\nsimplified sample life cycle is included above, \n\nalong with some top-level considerations. \n\nThe AI life cycle \n\nPLANNING \n\nâ†’ Plan and document the \n\nsystem's concept and \n\nobjectives. \n\nâ†’ Plan for legal and \n\nregulatory compliance.  DESIGN \n\nâ†’ Gather data and check \n\nfor data quality. \n\nâ†’ Document and \n\nassess metadata and \n\ncharacteristics of \n\nthe  dataset. \n\nâ†’ Consider legal and \n\nregulatory requirements. \n\nDEVELOPMENT \n\nâ†’ Select the algorithm. \n\nâ†’ Train the model. \n\nâ†’ Carry out testing, \n\nvalidation and \n\nverification. \n\nâ†’ Calibrate. \n\nâ†’ Carry out output \n\ninterpretation. \n\nDEPLOYMENT \n\nâ†’ Pilot and perform \n\ncompatibility checks. \n\nâ†’ Verify legal and \n\nregulatory compliance. \n\nâ†’ Monitor performance \n\nand mitigate risks post \n\ndeployment. â†’ Part I. Understanding AI and governance    \n\n> AI Governance in Practice Report 2024 |14\n> TABLE OF CONTENTS â†‘\n\n## â†’ HOW TO \n\n## Navigate developers from deployers \n\nVarious contractual and regulatory obligations may arise \n\ndepending on whether an organization is a vendor or buyer, \n\nor  if it sources external services such as hardware, cloud or  data \n\ncollection, for the development and operations of  its  AI system. \n\nPrior IAPP  research  found more than 70% of organizations \n\nrely  at least somewhat on third-party AI, so the responsibility \n\nfor ensuring the AI system is safe and responsible may be \n\nspread  across multiple roles. \n\nIn both  current legislation  and  proposed legislation  we are starting to see \n\ndifferent obligations for those who provide and supply AI versus those who \n\ndeploy AI. Understanding whether you are a developer and/or deployer is \n\nimportant to ensuring you meet compliance obligations. Once this is understood, \n\nit is possible to establish AI-governance processes for  procurement , including \n\nevaluations and contracts to avoid taking on additional liabilities. \n\nâ†’ The World Economic Forum put together a useful  toolkit \n\nto help those who are procuring AI systems. AI Governance in Practice Report 2024  | 15 \n\n# Part II. \n\n# The data \n\n# challenge \n\n## Data is an integral part of training \n\n## and operating an AI system. \n\nMost AI requires sizeable amounts of high-quality data, \n\nespecially during the training phase to maximize the model's \n\nperformance, as well as to ensure the desired and accurate \n\noutput. With the advancement of new AI technologies, models \n\nare requiring increasingly more data, which may come from \n\na variety of sources. Given the importance of the data used to \n\nfuel the AI system, it is important to understand what data is \n\nbeing used; how, where and by whom it was collected; from \n\nwhom it was collected; if it is the right data for the desired \n\noutcome; and how it will be managed throughout the life cycle. \n\nAccessing data and identifying data  sources \n\nUnderstanding where data comes from and how it is collected is \n\nnot only necessary for AI systems, but also for building trust in AI \n\nby ensuring the lawfulness of data collection and processing. Such \n\ndocumentation can assist with  data transparency  and improve the \n\nAI system's auditability as well. \n\nAlthough data may originate from multiple sources, it can be \n\nbroadly categorized into three  types: first-party data, public \n\ndata  and third-party data. â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 16  \n\n> TABLE OF CONTENTS â†‘\n\nFirst-party data \n\nThis refers to data collected directly from \n\nindividuals by an organization through their own \n\ninteractions and transactions. Such data may \n\noriginate from sources such as website visits, \n\ncustomer feedback and surveys, subscriptions, \n\nand customer relationship management systems, \n\namong others. This data is extremely valuable for \n\norganizations as it provides direct and firsthand \n\ninsights into individuals' behavior. \n\nFirst-party data can be collected from various \n\nsources. Identifying the data channels and \n\ndocumenting the source will not only help the \n\norganization determine what types of data, e.g., \n\ntext, numerical, image or audio, will be collected \n\nfrom each source, but also alert the legal team \n\nabout where legal compliance will be required \n\non  the organization's part. \n\nPublic data \n\nThis refers to data that is available to the wider \n\npublic and encompasses a range of sources, \n\nsuch as publicly available government records, \n\npublications, and open source and web-scraped \n\ndata. Public data is a valuable resource for \n\nresearchers and innovators as it provides readily \n\navailable information. Public data can come \n\nfrom multiple sources. \n\nWhile it is arduous and cumbersome to \n\nmaintain data lineage for public datasets, \n\nit  is important for upholding organizational \n\nreputation and fostering user trust, legal \n\ncompliance and AI safety overall. A lack of \n\nunderstanding of where data comes from \n\neventually leads to a lack of understanding of \n\nthe training dataset and model performance, \n\nwhich can reinforce the black-box problem. \n\nTherefore, in the interest of transparency, \n\ntracking and documenting public-data sources \n\nas much as possible may prove beneficial for \n\nthe organization, as it can later support other \n\ntransparency efforts, such as drawing up data, \n\nmodel or system cards. \n\nMoreover, without knowledge of public-data \n\nsources, the organization may inadvertently \n\ntrain the AI system on personal, sensitive \n\nor proprietary data. From the privacy \n\nstandpoint, this can be problematic in cases \n\nof  data leakage , where personally identifiable \n\ndata may be exposed. AI security challenges \n\nmay also be amplified if data was procured \n\nfrom unsafe public sources, as that carries \n\nthe risk of introducing malicious bugs into \n\nthe system. It may also lead to biases in \n\nthe  AI system. \n\nEthical development \n\npractices start with \n\nresponsible data \n\nacquisition and \n\nmanagement systems, \n\nas well as review \n\nprocesses that track \n\nthe lineage of \n\nsourced data. \n\nChristina Montgomery \n\nIBM Vice President and \n\nChief Privacy and Trust Officer â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 17  \n\n> TABLE OF CONTENTS â†‘\n\nAn organization can begin by establishing a \n\nclear understanding of how and why public \n\ndata is being collected, how it aligns with the \n\npurposes the AI system will fulfil, if and how \n\nsystem accuracy will be affected by using \n\npublic data, what the trustworthy sources for \n\ngathering public data are, if the organization \n\nhas rights to use the public data, and other legal \n\nconsiderations that may have to be taken into \n\naccount, particularly given that public data is \n\ntreated differently across jurisdictions. \n\nThird-party data \n\nThis refers to data obtained or licensed by the \n\norganization from external entities that collect \n\nand sell data, such as data brokers. Datasets \n\npurchased from brokers are webbed together \n\nfrom a wide range of sources. While this may \n\nhave the benefit of providing insights into \n\na wider user base, the insights may not be \n\naccurate or may be missing key data. It may \n\nlack direct insights into customer behavior, as \n\nbrokers do not interact with the organization's \n\ncustomer base. \n\nThird-party data can also include open-source \n\ndata, available through open-source data \n\ncatalogues. Sometimes these databases are \n\nprovided by government or academic institutions \n\nwith a clear understanding of how the data was \n\ncollected and how it can be used, including a \n\nclear use license. Open-source data collected \n\nthrough other community efforts may not follow \n\nthe same collection and distribution practices. \n\nAs when using all data, it is important to know \n\nwhere the data came from, how it was collected, \n\nin which context it is meant to be used and what \n\nrights you have to use it. \n\nData quality \n\nThe quality of data that AI is trained and tested \n\non directly impacts the quality of the outputs and \n\nperformance, so ensuring the data is high quality \n\ncan help lay the initial foundations for a safe and \n\nresponsible AI system. Measuring  data quality \n\noften includes a few  baseline considerations .\n\nAccuracy confirms the correctness of data. \n\nThat is, whether the data collected is based \n\non real-world insights. Completeness refers to \n\nchecking for missing values, determining the \n\nusability of the data, and looking for any over \n\nor underrepresentation in the data sample. \n\nValidity ensures data is in a format that is \n\ncompatible with intended use. This may include \n\nvalid data types, metadata, ranges and patterns. \n\nConsistency refers to the relationships between \n\ndata from multiple sources and includes \n\nchecking if the data shows consistent trends \n\nand values it represents. Ideally, this process of \n\nensuring data quality is documented to support \n\ntransparency, explainability, data fairness, \n\nauditability, understanding of the data phase \n\nof  the life cycle and system performance. \n\nWithout understanding the quality \n\nof the data being ingested into an \n\nAI model, you may not know the \n\nquality of the output. Companies \n\nmust establish and define what â€˜data \n\nqualityâ€™ involves and consists of, as this \n\ndetermination is highly contextual for \n\nany organization, and can depend on \n\nbusiness goals, use cases, focus areas \n\nand fitness for purpose. \n\nRegardless of context, there are \n\nminimum baseline attributes which can \n\nand should be established: accuracy, \n\ncompleteness, consistency and \n\nvalidity. Timeliness and uniqueness \n\nmay also be important to establishing \n\nfitness for purpose. \n\nDera Nevin \n\nFTI Technology Managing Director â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 18 \n\nTABLE OF CONTENTS  â†‘\n\nAppropriate use \n\nOne of the most significant challenges when designing and \n\ndeveloping AI systems is ensuring the data used is appropriate \n\nfor the intended purpose. Often data is collected with one \n\nintention in mind or within a specific demographic area, and, \n\nwhile it might appear to be a useful dataset, upon further \n\nanalysis it might include data that does not match the industry or \n\ngeographic area of operation. When data is not fit for purpose, it \n\ncan skew the AI system's predictions or outcomes. \n\nWhen thinking about appropriate use, consider the \n\nproportionality of data required for the desired outcome. \n\nOften,  there are occurrences of collecting or acquiring more \n\ndata than necessary to achieve the outcome. It is important \n\nto understand if it is even necessary to collect and use certain \n\ndata  in your AI system. \n\nManaging unnecessary data, especially data that may contain \n\nsensitive attributes, can increase an organization's risk of a \n\nbreach or harm resulting from the use of AI. \n\nLaw and policy considerations \n\nApproaches can be categorized according to how the \n\ndata  was  collected. \n\n# Managing unnecessary data \n\n# can increase an organization's \n\n# risk of a breach or harm \n\n# resulting from the use of AI. â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 19  \n\n> TABLE OF CONTENTS â†‘\n\nFirst-party data \n\nWhere first-party data amounts to personal or \n\nsensitive data, relevant provisions may be triggered \n\nunder the data protection and privacy legislation of \n\nthe jurisdictions where the organization carries out \n\nits business, where the processing takes place or \n\nwhere the individuals concerned are located. \n\nThe EU General Data Protection Regulation, \n\nfor instance, has a default prohibition against \n\nprocessing of personal data, unless such \n\nprocessing falls under one of the  six bases  for \n\nlawful processing under Article 6(1): consent, \n\ncontractual performance, vital interest, legal \n\nobligation, public task and legitimate interest \n\npursued by a controller or third party. \n\nPublic data \n\nWeb scraping may involve compliance with the \n\nterms of service and privacy policies of websites. \n\nOtherwise, when an organization is aware the \n\npublic dataset contains personal or sensitive \n\ninformation, lawfulness of use may require \n\ncompliance with relevant data protection or \n\nprivacy laws, such as by acquiring valid consent. \n\nWhile web scraping, it is possible for copyrighted \n\ndata to be collected to train AI systems. \n\nAnother type of public data is open-source \n\ndata, which is publicly available software that \n\nmay include both code and datasets. Although \n\naccessible to the public, open-source software is \n\noften made available by the organization through \n\nvarious  open-source licensing schema . In addition \n\nto complying with the terms of the licenses, \n\norganizations using open-source data may also \n\nconsider conducting their own due diligence to \n\nensure the datasets were acquired lawfully, are \n\nsafe to use and were assessed for bias mitigation. \n\nThird-party data \n\nAs organizations have neither proximity to how \n\nthird-party data was first collected nor direct \n\ncontrol over the data governance practices of \n\nthird parties, an organization can benefit from \n\ncarrying out its own legal due diligence and \n\nthird-party risk management. The extent and \n\nintensity of this exercise will largely depend on \n\nthe organization's broader governance and risk-\n\nmanagement approach and the relevant facts. \n\nLegal due diligence may include verification \n\nof the personal data's lawful collection by the \n\ndata broker, review of contractual obligations \n\nand licenses, and identification of protected \n\nintellectual property interests. When data is \n\nlicensed, the organization will first have to \n\nlawfully procure rights to use data through a \n\nlicensing agreement. This will help maintain data \n\nprovenance and a clear understanding of data \n\nownership. The lawful and informed use of such \n\ndata at subsequent stages of the AI life cycle will \n\nalso be governed by the license. \n\nWith growing public concerns \n\nand increased regulation aimed \n\nat developing trustworthy, \n\ntransparent and performative \n\nAI systems, an internal data \n\ngovernance program is \n\nintegral to understanding and \n\ndocumenting metadata prior to \n\nusage, and to identifying risks \n\nassociated with lawful data use. \n\nChristina Montgomery \n\nIBM Vice President and \n\nChief Privacy and Trust Officer â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 20  \n\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## Joint statement by international data protection and privacy authorities on web scraping \n\nIn August 2023, 12 international data protection and privacy authorities released a  joint statement  to address data scraping on \n\nsocial media platforms and other publicly accessible websites. \n\nThe joint statement outlined: \n\nâ†’ Key privacy risks associated with data scraping, such as targeted \n\ncyberattacks, identity fraud, monitoring and profiling individuals, \n\nunauthorized political or intelligence gathering, and unwanted direct \n\nmarketing or spam. \n\nâ†’ How social media companies and other websites should protect \n\nindividuals' personal information from unlawful data scraping, such \n\nas through data security measures and multilayered technical and \n\nprocedural controls to mitigate the risk. \n\nâ†’ Steps individuals can take to minimize the privacy risks of scraping, \n\nincluding reading a website's privacy policy, limiting information \n\nposted online, and understanding and managing privacy settings. \n\nSome key takeaways from the joint statement include: \n\nâ†’ Publicly accessible personal information is still subject to data \n\nprotection and privacy laws in most jurisdictions. \n\nâ†’ Social media companies and other website operators hosting publicly \n\naccessible personal data have legal obligations to protect personal \n\ninformation on their platforms from unlawful data scraping. \n\nâ†’ Accessing personal information through mass data scraping can \n\nconstitute reportable data breaches in many jurisdictions. \n\nâ†’ Individuals can take steps to prevent their personal information from \n\nbeing scraped, and social media companies have a role to play in \n\nempowering users to engage with social media services in a manner \n\nthat upholds privacy. â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 21  \n\n> TABLE OF CONTENTS â†‘\n\nImplementing AI governance \n\nNumerous strategies are being leveraged to \n\nmanage data in the context of AI. \n\nData management plans \n\nAlongside ensuring the lawfulness of data \n\nacquisition, there are numerous measures an \n\norganization can take to keep track of where the \n\ndata used to train AI systems comes from. Such \n\norganizational practices are especially important \n\nwith the advent of generative AI, where training \n\ndata is merged from numerous sources. \n\nDeveloping a comprehensive plan for how data is \n\nmanaged across an organization is a foundational \n\nelement to managing all AI systems. Some \n\nconsiderations for data management plans include \n\nunderstanding what data is being used in which \n\nsystem; how it is collected, retained and disposed; \n\nif there is lawful consent to use the data; and who is \n\nresponsible for ensuring the appropriate oversight. \n\nIt is likely your organization is already keeping \n\ntrack of the data used across the organization. \n\nWhile there are additional considerations involved \n\nwhen using data for AI systems as discussed above, \n\nit is possible to add to your existing data workflows \n\nor management practices. It is important to \n\nconsider the use and management of data used for \n\nAI systems at every stage of the life cycle as there \n\nare different concerns and implications to consider \n\nduring different stages. If your organization does \n\nnot already have a data management practice, \n\nresources such as those from  Harvard Biomedical \n\nData Management  can help you get started. \n\nAdditionally, the data management plan should \n\nidentify relevant data standards, such as  ISO \n\n8000  for data quality, to set appropriate controls \n\nand targets for your organization to meet. Data \n\nstandards for aspects of AI are under development \n\nthrough various initiatives at the NIST, ISO/IEC \n\nand other national standards bodies. \n\nIBM believes it is essential for \n\ndata management practices \n\ntied to AI development to \n\ninclude advanced filtering \n\nand curation techniques \n\nto identify untrustworthy, \n\nprotected/sensitive, explicit, \n\nbiased/nonrepresentative or \n\notherwise unwanted data. \n\nChristina Montgomery \n\nIBM Vice President and \n\nChief Privacy and Trust Officer â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 22  \n\n> TABLE OF CONTENTS â†‘\n\nData labels \n\nGrowing in importance,  data labels  are tools that can require \n\norganizations to provide information on how data was collected and \n\nused to train AI models. They are transparency artifacts of AI datasets \n\nthat explain the processes and rationale for using certain data and \n\nexplain how it was used in training, design, development and use. \n\nThis will help explain if the data being used is fit for purpose, if it is \n\nrepresentative of the demographics being served with the AI system \n\nand if the data meets relevant data quality standards. \n\nIdeally data labels are requirements of a robust data management \n\nprocess, which includes data quality and data impact assessments. \n\nWhile data labels are intended to provide documentation and \n\nawareness of the data being used, they can also assist with the \n\nassessment and review process. These tools should be aligned \n\nwhere  possible within the organization to avoid redundant efforts. \n\nData-source maintenance through documentation and inventories \n\ncan help organizations keep track of where the data is acquired \n\nand carry out relevant legal due diligence at first-party or \n\nthird-party levels. \n\nDedicated processes and functions \n\nWhen third-party data is used, it is important to follow the terms of \n\nservice and provide attribution where possible. This will also help \n\ninform users of the AI system where the data originated. Where \n\npossible, when data is being used from a third party, an appropriate \n\ndata sharing agreement with clear terms of use for both parties is \n\nhighly recommended. This helps to resolve any liability issues that \n\nmay arise as a result of using the system. \n\nINDUSTRY EXAMPLE \n\nWhen third-party organizations use publicly \n\navailable data, processes can be put into \n\nplace. Meta's  External Data Misuse  team \n\ndetects, blocks and deters web scraping. \n\nSome actions taken by the EDM team include \n\ndisabling accounts, serving cease-and-desist \n\nnotices, using CAPTCHAs for bot detection \n\nand blocking IP addresses where data \n\nscraping is identified. OpenAI has put in place \n\nan  opt-out process  for organizations that do \n\nnot want GPTbot to access their websites for \n\nthe purpose of web crawling. AI Governance in Practice Report 2024  | 23 \n\n# Part III. \n\n# The privacy and \n\n# data protection \n\n# challenge \n\nGiven that AI is a data-dependent enterprise and that privacy \n\nlaw governs the processing of personal data,  privacy laws  have \n\nemerged as a prominent mechanism for managing the key AI \n\ngovernance challenges. After all, information privacy seeks to \n\nprovide a framework \" for making ethical choices about how \n\nwe use new technologies .\"\n\nIndeed, national  data protection authorities  have been \n\namong the first to intervene and bring enforcement actions \n\nwhen AI-based products were thought to harm consumers. \n\nFor example, Italy's data protection authority, the Garante, \n\nimposed a  temporary ban  on ChatGPT after concluding the \n\nservice was in violation of the GDPR for lacking a legal basis \n\nfor processing and age-verification mechanism. \n\n## Privacy and data protection \n\n## governance practices are woven \n\n## into the AI life cycle. \n\nThe enforcement landscape for AI governance is \n\nincredibly unsettled. Which regulators will lead on what \n\nand how they will collaborate or conflict is subject \n\nto heavy debate and will differ by country, creating \n\nheightened uncertainty for organizations. Whether or \n\nnot privacy regulators have the lead remit, they will play \n\na key role given the centrality of data to AI governance. \n\nCaitlin Fennessy \n\nIAPP Vice President and Chief Knowledge Officer â†’ Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 24  \n\n> TABLE OF CONTENTS â†‘\n\nLaw and policy considerations \n\nThe OECD's  Guidelines Governing the \n\nProtection of Privacy and Transborder Flows \n\nof Personal Data  â€” developed in 1980 and \n\nrevised in 2013 â€” enshrine eight principles that \n\nhave served as the  foundation  for most global \n\nprivacy and data protection laws written over \n\nthe past several decades, including landmark \n\nlegislation such as the GDPR. These eight \n\nprinciples include collection limitation, data \n\nquality, purpose specification, use limitation, \n\nsecurity safeguards, openness, individual \n\nparticipation and accountability. \n\nMany DPAs around the world already put forth \n\nguidance  on how AI systems can work to align \n\nthemselves with these foundational principles of \n\ninformation privacy. Yet, as Australia's Office of \n\nthe Victorian Information Commissioner noted \n\nin a  resource  on issues and challenges of AI and \n\nprivacy, \"AI presents challenges to the underlying \n\nprinciples upon which the (OECD Privacy) \n\nGuidelines are based.\" To better understand \n\nwhere these challenges currently exist, each of \n\nthese principles is discussed below in the context \n\nof their applicability to â€” and potential conflict \n\nwith â€” the development of AI. \n\nCollection limitation \n\nThe principle of collection limitation states, \n\n\"There should be limits to the collection \n\nof personal data and any such data should \n\nbe obtained by lawful and fair means and, \n\nwhere appropriate, with the knowledge or \n\nconsent of the data subject.\" It most readily \n\ntranslates to the concept and practice of data \n\nminimization. GDPR Article 5(1)(c) emanates \n\nfrom this idea that data, at the collection stage, \n\nshould have some predefined limit or upper \n\nbound. Specifically, data collection should be \n\n\"... limited to what is necessary in relation to \n\nthe purposes for which they are processed.\" \n\nAs many observers have noted, this is one of \n\nthe privacy principles for which there appears \n\nto be an \" inherent conflict \" with AI systems \n\nthat rely on the collection and analysis of large \n\ndatasets. Performing adequate AI  bias testing ,\n\nfor example, requires collecting more data than \n\nmight otherwise be collected. \n\nAt Mastercard, we are testing \n\ninnovative tools and technologies \n\nto address some of the potential \n\ntensions between privacy and \n\nAI governance. For instance, we \n\nknow that a lot of data is needed, \n\nincluding sometimes sensitive \n\ndata, for AI to produce unbiased, \n\naccurate and fair outcomes. \n\nHow do you reconcile this with the \n\nprinciple of data minimization and \n\nthe need for individual's explicit \n\nconsent? We are exploring how \n\nthe creation of synthetic data can \n\nhelp, so as to achieve all desired \n\nobjectives at the same time. \n\nCaroline Louveaux \n\nMastercard Chief Privacy and Data Responsibility Officer â†’ Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |25\n> TABLE OF CONTENTS â†‘\n\nData quality \n\nThis is the principle that \"Personal data should be relevant to the \n\npurposes for which they are to be used, and, to the extent necessary for \n\nthose purposes, should be accurate, complete and kept up-to-date.\" Data \n\nquality is the privacy principle with which AI may be most in synchrony. \n\nThe  accuracy  of AI model outputs depends significantly on the quality of \n\ntheir inputs. A breakdown in  AI governance  can lead to data becoming \n\ninconsistent and error-laden, underscoring the need for AI-based \n\nsystems to orient themselves around the principle of data quality. Data \n\nbrokers and other companies can become the target of  enforcement \n\nactions  for failing to ensure the accuracy of the data they collect and sell. \n\nPurpose specification \n\nThe principle of purpose specification states, \"The purposes for which \n\npersonal data are collected should be specified ... and the subsequent \n\nuse limited to the fulfilment of those purposes ...\" Indeed, as the \n\nU.K. Information Commissioner's Office explained in the context \n\nof its  consultation  on purpose limitation in the generative AI life \n\ncycle, purposes of data processing \"must be specified and explicit: \n\norganizations need to be clear about why they are processing personal \n\ndata.\" This need for clarity applies not only to internal documentation \n\nand governance structures, but in communication with the people to \n\nwhom the personal data relates. In sum, organizations should be able \n\nto explain what personal data they process at each stage and why it is \n\nneeded to meet the specified purpose. \n\nA conflict with the purpose specification principle can arise if and \n\nwhen a developer wants to use the same training dataset to train \n\nmultiple models. The ICO advises developers reusing training data to \n\nconsider whether the purpose of training a new model is compatible \n\nwith the original purpose of collecting the training data. Considering \n\nthe reasonable expectations of those whose data is being reused can \n\nhelp an organization make a compatibility assessment. Currently, the \n\nICO considers collating repositories of web-scraped data, developing \n\na generative AI model and developing an application based on such a \n\nmodel to constitute different purposes under data protection law. â†’ Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 26  \n\n> TABLE OF CONTENTS â†‘\n\nUse limitation \n\nRelated to purpose specification, use \n\nlimitation  is the principle that states personal \n\ndata \"should not be disclosed, made available \n\nor otherwise used for purposes other than \n\nthose specified,\" except with the consent of \n\nthe data subject or by the authority of law. \n\nPurposes of use  must be specified at or before \n\nthe time of the collection, and subsequent \n\nuses must not be incompatible with the initial \n\npurposes of collection. \n\nThis is another principle that is challenged by \n\nAI systems, with potential  regulatory gaps  left \n\nby both the EU GDPR and EU AI Act. Proposals \n\nto address these gaps have included restricting \n\nthe training of models only to stated purposes \n\nand requiring alignment between training data \n\ncollection and the purpose of a model. \n\nSecurity safeguards \n\nUniting the fields of privacy, data protection \n\nand cybersecurity for decades is the principle \n\nthat \"Personal data should be protected \n\nby reasonable security safeguards against \n\nsuch risks as loss or unauthorized access, \n\ndestruction, use, modification or disclosure of \n\ndata.\" Ensuring the security of personal data \n\ncollected and processed is a key to building and \n\nmaintaining trust within the digital economy. \n\nRemedying problems of security and safety \n\nis and will remain a critical challenge for AI. \n\nEnsuring the actions of an AI system align \n\n\"with the values and preferences of humans\" is \n\ncentral to keeping these systems  safe . Yet, many \n\nAI systems remain susceptible to hacking and \n\nso-called \" adversarial attacks ,\" which are inputs \n\ndesigned to deceive an AI system, as well as data \n\npoisoning, evasion attacks and model extraction. \n\nExamples include forcing chatbots to provide \n\nanswers to responses to  harmful prompts \n\nor getting a self-driving vehicle's cameras to \n\nmisclassify a stop sign as a speed-limit sign. \n\nOpenness \n\nThe  right to be informed  and the principle of \n\ntransparency  are touchstones of global privacy \n\nand data protection laws. Beginning at the \n\ncollection stage and enduring throughout the life \n\ncycle of processing, these rights form the basis \n\nof organization's transparency obligations. They \n\noften require organizations to disclose various \n\ntypes of information, from the types of data \n\ncollected and how it is used to the availability of \n\ndata subjects' rights and how to exercise them to \n\nthe logic involved and potential consequences of \n\nany automated decision-making or profiling the \n\norganization engages in. The \"black-box\" nature \n\nof many AI systems can make this principle \n\nchallenging to navigate and adhere to. \n\nAs all AI and machine learning \n\nmodels are 100% data dependent, \n\nthe models must be fed high-\n\nquality, valid, verifiable data with \n\nthe appropriate velocity. As obvious \n\nas that may be, the challenges \n\naround establishing the governance \n\nrequirements that ensure the \n\nappropriate use of private data \n\nmay be far more complex. Modelers \n\nshould absolutely be applying the \n\nminimization principle of identifiable \n\ndata as they train. Adding private \n\ndata that could leak or cause bias \n\nneeds to be thought through early in \n\nthe design process. \n\nScott Margolis \n\nFTI Technology Managing Director â†’ Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |27\n> TABLE OF CONTENTS â†‘\n\nIndividual rights \n\nIndividual rights in privacy law commonly include the rights to \n\naccess, opt in/opt out, erasure, rectification and data portability, \n\namong others. Many privacy laws contain rights for individuals to \n\nopt-out of automated decision-making underpinned by AI systems. \n\nAccountability \n\nAccountability is arguably one of the most important principles when \n\nit comes to operationalizing organizational governance. Accountability \n\nis based on the idea that there should be a person and/or entity that is \n\nultimately responsible for any harm resulting from the use of the data, \n\nalgorithm and AI system's underlying processes. \n\nImplementing AI governance \n\nThe practice and professionalization of AI governance is a highly \n\nspecialized, stand-alone field requiring multidisciplinary expertise. \n\nA holistic approach to AI governance requires support from \n\nestablished subject-matter areas, including data protection and \n\ninformation governance practitioners. Data from past IAPP  research \n\nshows 73% of organizations are leveraging their existing privacy \n\nexpertise to manage AI governance. This is not surprising, as data is \n\na critical component of AI. Good AI governance weaves privacy and \n\ndata governance practices into the AI life cycle alongside AI-specific \n\nissues. This chapter demonstrates the overlapping nature of privacy \n\nand AI governance. \n\nApproaching the implementation of AI governance by adapting \n\nexisting governance structures and processes enables organizations \n\nto move forward quickly, responsibly and with minimal disruption to \n\ninnovation and the wider business. Target processes that may already \n\nbe established by organization's data protection program include: \n\naccountability, inventories, privacy by design and risk management. â†’ Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |28\n> TABLE OF CONTENTS â†‘\n\nAccountability \n\nPrivacy compliance programs are likely to \n\nhave established roles and responsibilities for \n\nthose with direct and indirect responsibility for \n\nprivacy compliance. These are likely supported \n\nby policies and procedures to help individuals \n\nfulfil the expectations of their role. Senior \n\nmanagement contributions are likely channeled \n\nthrough privacy committees, with mechanisms in \n\nplace to support risk-based escalation, reporting \n\non key metrics and decision-making. \n\nPrivacy leaders often have a direct line to CEOs \n\nand boards of directors, as well as a matrixed \n\nstructure of privacy champions across the \n\norganization to enable a multidisciplinary \n\napproach to privacy governance and ensure data \n\nprotection needs are considered by product and \n\nservice teams. This structure is well-suited to, and \n\ncan be leveraged for, AI governance given the need \n\nfor leadership engagement and skills spanning \n\nlegal, design, product and technical disciplines. \n\nWhere AI systems process personal data, \n\nthose  with accountability for privacy \n\ncompliance will need to ensure their existing \n\nprivacy compliance processes are set up to \n\naddress the intersection between AI and privacy. \n\nThis will include considering data inventory, \n\ntraining, privacy by design and other topics \n\nfurther outlined in this section. \n\nInventories \n\nPersonal data inventories have long been \n\nthe foundation of establishing a successful \n\nprivacy program and a key requirement of \n\nprivacy regulations. Knowing your data, \n\nhow it is collected and used, and being able \n\nto demonstrate this remains a core part of \n\naccountability. Organizations have also matured \n\nin their approaches, from lengthy spreadsheets \n\nto technology-enabled approaches. \n\nWhere AI systems use personal data, the data \n\ninventory can play a crucial role. Organizations \n\nthat have captured additional privacy compliance \n\nmetadata alongside the minimum regulatory \n\nrequirements may find their personal data \n\ninventories particularly useful in the age of AI. \n\nAdditional uses of this metadata could include a \n\nsingle source of truth for lawful basis to identify \n\nif additional use within AI models is permitted, \n\naccuracy metrics on personal data to support AI \n\nmodels to make accurate inferences based on \n\nthe latest personal data and a top-down view on \n\nprocesses relying on automated decision-making \n\nthat can be aligned with AI registries. â†’ Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 29  \n\n> TABLE OF CONTENTS â†‘\n\nEffective AI governance is underpinned by \n\nAI inventories with similar functionalities to \n\nthose of data inventories. AI registers can help \n\norganizations keep track of their AI development \n\nand deployment. Some functional requirements \n\nthat overlap with data inventories include the \n\nability to connect into the system-development \n\nlife cycle, maintenance and regular updates \n\nby  multiple users, and logging capability \n\nto  ensure integrity. \n\nPrivacy by design \n\nBy embedding privacy at the outset, privacy \n\nby design continues to be a critical part of \n\nhow organizations address privacy concerns. \n\nIn implementing privacy by design, privacy \n\nfunctions may take steps to map and embed \n\nprivacy into areas such as system-development \n\nlife cycles, project initiation and development \n\napproaches within an organization, risk \n\nmanagement and approval workflows, \n\nand  stage  gates. \n\nSteps may include developing AI-specific \n\nrisk-assessment workflows into existing \n\nrisk-assessment processes, enhancing existing \n\ncontrol catalogs with AI and privacy controls, \n\nor updating approval workflows to include \n\nstakeholders with AI accountabilities. \n\nAdditionally, the growing maturity of privacy \n\nenhancing technologies and their increasing \n\ntraction as technical measures within \n\norganizations may have benefits for the \n\ndevelopment of AI. With some PETs potentially \n\nhelping organizations reduce inherent risk \n\nof data use, an organization may be able to \n\nmaximize the strategic use of its data. Examples \n\ninclude using differential privacy in training \n\nmachine-learning models, federated learning \n\nand synthetic data. \n\nRisk management \n\nThe risk-based approach often adopted by \n\nglobal privacy regulations has been distilled into \n\norganizational risk-management efforts, which \n\nput privacy impact assessments at the heart of \n\ndeciding whether an organization can reduce \n\nharm from personal data processing through the \n\nimplementation of organizational and technical \n\nmeasures. Privacy risk can also stem from wider \n\nprivacy compliance activities and lessons learned \n\nin areas such as vendor risk, incident management \n\nand data subject requests management. \n\nLegal professionals need to keep an \n\nopen and flexible mind â€” technology \n\nbrings new challenges but also \n\nnew solutions. General counsel \n\nshould position themselves as \n\nthe center of a multidisciplinary \n\nteam of stakeholders across \n\ntheir organizations, including \n\nproduct design, compliance, data \n\nand privacy, which can deploy \n\nto manage multifaceted data \n\nrisks. Companies that strive for \n\nestablished best privacy practice \n\nwill more easily be able to comply \n\nwith the rising standards of global \n\nprivacy laws. \n\nTim de Sousa \n\nFTI Technology, Managing Director, Australia â†’ Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 30 \n\nTABLE OF CONTENTS  â†‘\n\nPrivacy risk may already feed into wider enterprise risk-\n\nmanagement programs, such as information technology and \n\ncybersecurity risk and control frameworks. These can be enhanced \n\nto accommodate the complex types and sources of AI risk into \n\na unified risk-management framework at the enterprise level. \n\nThis approach can also facilitate crucial visibility across different \n\nsubject-matter practice areas across the business and enable a more \n\neffective analysis and treatment of  AI  risk. \n\nAs AI risk-management approaches mature, AI governance \n\nprofessionals face choices between embedding algorithmic impact \n\nassessments alongside or within PIAs. The need to align AI risk \n\nmanagement with broader enterprise risk-management efforts is \n\nof  equal importance. AI governance professionals will likely need \n\nto update enterprise risk-management strategies and frameworks to \n\nclearly factor in AI-related risks and document ongoing AI risks and \n\nremediations in a formal risk register. \n\nRisk-assessments \n\nA wide range of AI risk assessments are often talked about in the \n\nemerging global AI governance landscape. \n\nSome of these assessments are required by existing data \n\nprotection legislation, such as the GDPR, while others may \n\nemerge from AI-specific laws, policies and voluntary frameworks. \n\nFor the latter, laws and policies often provide AI governance \n\nsolutions with knowledge of the overlap. \n\n# Privacy risk may \n\n# already feed into wider \n\n# enterprise risk management \n\n# programs, such as IT and \n\n# cybersecurity risk \n\n# and control frameworks. â†’ Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |31\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## AI governance assessments: A closer look at EU DPIAs and FRIAs \n\nGDPR: DPIAs \n\nData protection impact assessments are required \n\nunder GDPR Article 35. DPIAs are particularly \n\nimportant where systematic and extensive evaluation \n\nof personal or sensitive aspects of natural persons \n\nthrough automated systems or profiling leads to legal \n\nconsequences for that person. Incorporating these \n\nassessments within the AI-governance life cycle can \n\nhelp organizations identify, analyze and minimize \n\ndata-related risks and demonstrate accountability. \n\nDPIAs at a minimum contain: \n\nâ†’ A systematic description of the anticipated \n\nprocessing, its purpose and pursued legitimate \n\ninterest. \n\nâ†’ A necessity and proportionality assessment in \n\nrelation to the intended purpose for processing. \n\nâ†’ An assessment of the risks to fundamental \n\nrights and freedoms. \n\nâ†’ Measures to be taken to safeguard security \n\nrisks and protect personal data. \n\nEU AI Act: FRIAs \n\nUnder the EU AI Act, FRIAS are required to be carried \n\nout in accordance with Article 27 by: \n\nâ†’ Law enforcement when they use real-time \n\nremote biometric identification AI systems, which \n\nare a prohibited AI practice under Article 5. \n\nâ†’ Deployers of high-risk AI systems that are \n\ngoverned by public law, private operators that \n\nprovide public services and operators deploying \n\ncertain high-risk AI systems referred to in \n\nAnnex III, point 5 (b) and (c), such as banking or \n\ninsurance entities. \n\nFRIAs are required only for the first use of the \n\nhigh-risk AI system, and the act permits deployers \n\nto rely on previously conducted FRIAs, provided all \n\ninformation about the system is up to date. FRIAs \n\nmust consist of: \n\nâ†’ Descriptions of the deployer's processes in \n\nline with intended use and purpose of the \n\nhigh-risk AI system. \n\nâ†’ Descriptions of the period and frequency of the \n\nhigh-risk AI system's use. \n\nâ†’ Categories of individuals or groups likely to be \n\naffected by the high-risk system. \n\nâ†’ Specific risks of harm that are likely to affect \n\nindividuals or groups. \n\nâ†’ Descriptions of the human oversight measures \n\nin place according to instructions of use. \n\nâ†’ Measures to be taken when risk \n\nmaterializes into harm, including \n\narrangements for internal governance \n\nand complaint mechanisms. \n\nHowever, AI governance solutions often foresee \n\nthe overlap with existing practices, and this is no \n\ndifferent under the EU AI Act. FRIAs, for instance, \n\ndo not need to be conducted for aspects covered \n\nunder existing legislation. As such, if a DPIA and \n\nFRIA have an overlapping aspect, that aspect \n\nneed only be covered under DPIA. AI Governance in Practice Report 2024  | 32 \n\n# Part IV. \n\n# The transparency, \n\n# explainability and \n\n# interpretability \n\n# challenge \n\n## The black-box problem \n\nOne reason for the lack of trust associated with AI systems is \n\nthe inability of users, and often creators, of AI systems to have \n\na clear understanding of how AI works. How does it arrive at a \n\ndecision? How do we know the prediction is accurate? This is \n\noften referred to as the \" black-box problem \" because the model \n\nis either too complex for human comprehension or it is closed \n\nand safeguarded by  intellectual property. \n\nAI techniques, such as deep learning, are becoming increasingly \n\ncomplex as they learn from terabytes of data, and the number \n\nof  parameters  has grown exponentially over the years. In July \n\n2023, Meta released its  Llama 2 model  with a parameter count \n\nat 70 billion. Google's  PaLM  parameter count is reported to be \n\nas large as 540 billion. Due to the self-learning abilities of AI, \n\nincluding their size and complexity, the black-box problem is \n\nincreasingly difficult to solve and often requires a trade-off to \n\nsimplify aspects of the system. \n\nTransparency is a term of broad scope, which can include the \n\nneed for technical and nontechnical documentation across the \n\nlife cycle. Having strong product documentation in place can \n\nalso provide commercial benefits by supporting the product \n\nsales cycle and helping providers to navigate prospective \n\nclients' due diligence protocols. â†’ Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 33 \n\nTABLE OF CONTENTS  â†‘\n\nIn the open-source context, transparency can also refer to providing \n\naccess to code or datasets in the open-source community to be used \n\nby AI systems. Transparency objectives can also include informing \n\nusers when they are interacting with an AI system or identifying \n\nwhen content was AI generated. Independent of how the term is \n\nused, transparency is a key tenet of AI governance due to the desire \n\nto understand how AI systems are built, managed and maintained. \n\nIt  is crucial that clear and comprehensive documentation is available \n\nto those who design and use these systems to ensure trust and help \n\nidentify where an error was made if an issue occurs. \n\nExplainability  refers to the understanding of how a black-box model, \n\ni.e., an incomprehensible or proprietary model, works. While useful, \n\nthe difficulty with black-box models is that the explanation may not \n\nbe entirely accurate or faithful to the underlying model, given its \n\nincomprehensibility. When full explainability is not possible due \n\nto  the factors mentioned above, an alternative is interpretability. \n\nInterpretability , on the other hand, refers to designing models that \n\ninherently make the reasoning process of the model understandable. \n\nIt encourages designing models that are not black boxes, with \n\ndecision or prediction processes that are comprehensible to domain \n\nexperts. In other words, interpretability is applied ante hoc. While \n\nit does away with the problems of explainable models, interpretable \n\nmodels are often domain specific and require significant effort to \n\ndevelop in terms of domain expertise. \n\nLaw and policy considerations \n\nOne proposed solution to the black-box challenge has been codifying \n\napproaches to and requirements for transparency, explainability and \n\ninterpretability in law or policy initiatives. Regulatory and voluntary \n\ngovernance tools that have established requirements for tackling the \n\nblack-box problem through transparency and explainability include \n\nthe EU GDPR and AI Act, NIST AI RMF, U.S. Executive Order 14110, \n\nChina's Interim Measures for the Management of Generative AI \n\nServices, and Singapore's AI Verify. \n\n# One reason for the lack of trust \n\n# associated with AI systems is the \n\n# inability for users, and often creators, \n\n# of AI systems to have a clear \n\n# understanding of how AI works. â†’ Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 34  \n\n> TABLE OF CONTENTS â†‘\n\nEU GDPR \n\nArguably one of the first legislative requirements \n\nfor AI governance,  GDPR  Articles 13(2)(f), 14(2) \n\n(g) and 15(1)(h) refer to providing meaningful \n\ninformation about the logic underpinning \n\nautomated decisions, as well as information \n\nabout the significance and envisaged \n\nconsequences of the automated decision-\n\nmaking for the individual. This is further \n\nsupported by Article 22 and Recital 71, which \n\nstate such decision-making should be subject to \n\nsafeguards , such as through the right to obtain \n\nan explanation to challenge an assessment. \n\nEU AI Act \n\nThe EU  AI Act  takes a risk-based approach to \n\ntransparency, with documentary and disclosure \n\nrequirements attaching to high-risk and general-\n\npurpose AI systems. \n\nIt mandates drawing up technical \n\ndocumentation for high-risk AI systems, and \n\nrequires high-risk AI systems to come with \n\ninstructions for use that disclose various \n\ninformation, including characteristics, \n\ncapabilities and performance limitations. \n\nTo  make high-risk AI systems more traceable, \n\nit also requires AI systems to be able to \n\nautomatically allow for the maintenance of \n\nlogs  throughout the AI life cycle. \n\nSimilarly, the AI Act places documentation \n\nobligations on providers of general-purpose \n\nAI systems with and without systemic risks. \n\nThis includes maintenance of technical \n\ndocumentation, including results from training, \n\ntesting and evaluation. It also requires  up-to-\n\ndate information and documentation to be \n\nmaintained for providers of AI systems who \n\nintend to integrate GPAI into their system. \n\nProviders of GPAI systems with systemic risks \n\nmust also publicly disclose sufficiently detailed \n\nsummaries of the content used for training GPAI. \n\nWith certain exceptions, the EU AI Act provides \n\nindividuals with the right to an explanation from \n\ndeployers of individual decision-making \"on the \n\nbasis of the output from a high-risk AI system \n\n... which produces legal effects or similarly \n\nsignificantly affects that person in a way that \n\nthey consider to have an adverse impact on \n\ntheir  health, safety or fundamental rights.\" \n\nIn addition to the documentary and disclosure \n\nrequirements, the AI Act seeks to foster \n\ntransparency by mandating machine-readable \n\nwatermarks. Article 50(2) requires machine-\n\nreadable watermarks for certain AI systems \n\nand  GPAI systems, so content can be detected \n\nas  AI generated or to inform users when they \n\nare  interacting with AI. \n\nThe EU is first out of the gate \n\nwith comprehensive AI legislation \n\nbut the EU AI Act is just the \n\ntip of the regulatory iceberg. \n\nMore guidance is coming and \n\nmany laws enacted since the \n\nearly 2000s, and under the recent \n\nEuropean Data Strategy, will have \n\nto be considered in AI governance \n\nprograms. The EU will continue \n\nto promote its approach to \n\nregulating AI on the global stage, \n\nfurthering the Brussels effect on \n\ndigital regulation. \n\nIsabelle Roccia \n\nIAPP Managing Director, Europe â†’ Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 35  \n\n> TABLE OF CONTENTS â†‘\n\nNIST AI RMF \n\nThe  NIST AI RMF  sees transparency, explainability and \n\ninterpretability as distinct characteristics of AI systems that \n\nsupport each other. Under the RMF, transparency is meant to \n\nanswer the \"what,\" explainability the \"how\" and interpretability \n\nthe  \"why\" of a decision. \n\nâ†’ Accountability and transparency:  The RMF defines \n\ntransparency as the extent to which information about an \n\nAI system and its outputs are made available to individuals \n\ninteracting with AI, regardless of whether they are aware \n\nof it. Meaningful transparency includes the disclosure of \n\nappropriate levels of information at different stages of the \n\nAI  life cycle, tailored to the knowledge or role of the individual \n\ninteracting with the system. This could include design \n\ndecisions, the model's training data and structure, intended \n\nuse-cases, and how and when deployment, post-deployment \n\nor end-user decisions were made and by whom. The RMF \n\nrequires AI transparency to consider human-AI interaction, \n\nsuch as by  notifying the human if a potential or actual adverse \n\noutcome  is  detected. \n\nâ†’ Explainable and interpretable AI:  The RMF defines \n\nexplainability as a representation of the underlying \n\nmechanisms of the AI system's operation, while it defines \n\ninterpretability as the meanings assigned to the AI outputs \n\nin the context of their designed functional purpose. \n\nLack  of explainability can be managed by describing how \n\nthe system functions by tailoring such descriptions to the \n\nknowledge, roles and skills of the individual, whereas lack \n\nof interpretability can be managed by describing why the \n\nAI  system gave a specific output. \n\nIt's important to align on a set of ethical AI principles \n\nthat are operationalized through tangible responsible AI \n\npractices, rooted in regulations, e.g. EU AI Act, and best \n\npractice frameworks, e.g. NIST AI RMF, when developing \n\nAI features. At Workday, we take a risk-based approach to \n\nresponsible AI governance. \n\nOur scalable risk evaluation dictates relevant guidelines \n\nsuch as requirements to map, measure, and manage \n\nunintended consequences including bias. Within the \n\nWorkday AI Feature Fact Sheets, Workday provides \n\ntransparency to customers on each feature such as, \n\nwhere relevant, how they were assessed for bias. \n\nThese safeguards are intended to document our efforts \n\nto develop AI features that are safe and secure, human \n\ncentered, and transparent and explainable. \n\nBarbara Cosgrove \n\nWorkday Vice President, Chief Privacy Officer â†’ Part IV. The transparency, explainability and interpretability challenge    \n\n> AI Governance in Practice Report 2024 |36\n> TABLE OF CONTENTS â†‘\n\nU.S. Executive Order 14110 \n\nU.S. Executive Order 14110  approaches \n\ntransparency through an AI safety perspective. \n\nUnder Section 4, the safety and security of AI \n\ntechnology is to be ensured through certain \n\ntransparency measures, such as the requirements \n\nto share results of safety tests and other \n\nimportant information with the U.S. government, \n\nthat have been imposed on developers of the \n\nmost powerful AI systems. Watermarks to label \n\nAI-generated content are also required under the \n\norder, with the purpose of protecting Americans \n\nfrom AI-enabled fraud and deception. \n\nChina's Interim Measures for the \n\nManagement of Generative AI Services \n\nArticle 10 of China's  Interim Measures for the \n\nManagement of Generative AI Services  requires \n\nproviders of AI services to clarify and disclose \n\nthe uses of the services to user groups and to \n\nguide their scientific understanding and lawful \n\nuse of generative AI. Watermarking AI-generated \n\ncontent is also a requirement under Article 11. \n\nSingapore's AI Verify \n\nSingapore's  AI Verify  is a voluntary testing \n\nframework  on AI governance for organizational \n\nuse comprised of two  parts : a testing framework \n\ngrounded in 11 internationally accepted \n\nprinciples grouped into five pillars and a \n\ntoolkit  to execute technical tests. \n\nTransparency and explainability themes \n\nare among the 11 principles embedded in \n\nAI Verify. The framework addresses the \n\ntransparency problem by providing impacted \n\nindividuals with appropriate information \n\nabout AI use in a technological system so they \n\ncan make informed decisions on whether to \n\nuse that AI enabled system. Explainability, \n\non the other hand, is achieved through an \n\nunderstanding of how an AI model reaches \n\na decision, so individuals are aware of the \n\nfactors that contributed to a resulting output. \n\nTransparency  is assessed through documentary \n\nevidence and explainability is assessed through \n\ntechnical tests. â†’ Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 37 \n\nTABLE OF CONTENTS  â†‘\n\nImplementing AI governance \n\nOrganizations have been active in coming up with tools \n\nand techniques to address the black-box transparency and \n\nexplainability challenge. \n\nModel and system cards \n\nModel cards are short documents that accompany an AI model to \n\nprovide  transparent  model reporting  by disclosing information about \n\nthe model. Information may include explanations about intended \n\nuse, performance metrics and benchmarked evaluation in various \n\nconditions such as across different cultures, demographics or race. \n\nIn addition to providing transparency, model cards are also meant to \n\ndiscourage use of models outside their  intended uses . At the industry \n\nlevel, use of model cards is becoming more prominent as evidenced \n\nby publicly accessible model cards for Meta and Microsoft's  Llama 2 ,\n\nOpenAI's  GPT-3  and Google's  face-detection model .\n\nIt may not always be easy to explain a model in a short document. \n\nModel cards are to serve a broad audience and, therefore, \n\nstandardizing explanations may prove either too simplistic for one \n\naudience or too complicated for another. Moreover, organizations \n\nshould also be mindful of how much information they reveal in the \n\ncards to prevent adversarial attacks and mitigate security risks. \n\nAI models are often part of a larger system comprised of a group \n\nof  models and technologies that work together to give outputs. \n\nAs  a result, model cards can fall short of providing a more nuanced \n\npicture of how different models interact together within the system. \n\nThat is where  system cards  can help achieve better insights. \n\n# Organizations have been active \n\n# in coming up with tools \n\n# and techniques to address \n\n# the black-box transparency \n\n# and explainability challenge. â†’ Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 38  \n\n> TABLE OF CONTENTS â†‘\n\nSystem cards explain how a group of AI models \n\nand other AI and non-AI technologies work \n\ntogether as part of an AI system to achieve \n\nspecific tasks. Meta released  22 system cards \n\nexplaining how AI powers its Facebook and \n\nInstagram platforms. Each card has four \n\nsections that detail: \n\nâ†’ An overview of the AI system. \n\nâ†’ How the system works by summarizing the \n\nsteps involved in creating experiences on \n\nFacebook and Instagram. \n\nâ†’ How the shown content can be customized. \n\nâ†’ How AI delivers content as part of the \n\nwhole  system. \n\nAI systems learn from their environments and \n\nconstantly evolve, so the way they work also \n\nchanges over time, requiring updates to the \n\nsystem cards. Like  with model cards, reducing \n\ntechnical concepts to a standardized language \n\nthat serves all audiences can be challenging for \n\nsystem cards, and system cards can also attract \n\nsecurity threats based on the amount and type \n\nof  information shared. \n\nThe utility of model and system cards can go \n\nbeyond meeting transparency challenges. \n\nMaintaining standardized records about the \n\nmodel itself can facilitate communication and \n\ncollaboration between various stakeholders \n\nthroughout the life cycle. This can also help \n\nwith bias and security-risk mitigation. They are \n\nalso useful for making comparisons with future \n\nversions of the models to track improvements. \n\nThe cards provide a documented record of \n\ndesign, development and deployment, so they can \n\nfacilitate attribution of responsibility for various \n\ndecisions and outcomes related to the model or \n\nsystem. Auditors can use them not only to gain \n\na holistic understanding of the system itself, but \n\nalso to zoom in on the processes and decisions \n\nmade during different phases of the life cycle. \n\nINDUSTRY EXAMPLE \n\nMeta released  22 system cards \n\nexplaining how AI powers \n\nits Facebook and Instagram \n\nplatforms. Each card has four \n\nsections that detail: \n\n1. An overview of the AI system. \n\n2. How the system works by \n\nsummarizing the steps involved \n\nin creating experiences on \n\nFacebook and Instagram. \n\n3. How the shown content can \n\nbe customized. \n\n4. How AI delivers content as part \n\nof the whole system. â†’ Part IV. The transparency, explainability and interpretability challenge    \n\n> AI Governance in Practice Report 2024 |39\n> TABLE OF CONTENTS â†‘\n\nOpen-source AI \n\nAnother approach to addressing the black-box \n\nchallenge is making AI open source. This requires \n\nmaking the source code public and allowing users \n\nto view, modify and distribute it freely. Open \n\naccess can be especially useful for researchers \n\nand developers, as there is more potential for \n\nscrutiny by a wider, diverse and collaborative \n\ncommunity of experts. In turn, that can lead to \n\nimprovements to the transparency of algorithms, \n\nthe detection of risks and the offering of solutions \n\nif things go wrong. Open-source AI can also \n\nimprove technology access and drive collaborative \n\ninnovation, which may otherwise be limited by \n\nproprietary algorithms. \n\nWatermarking \n\nWith the rise of generative AI, it is becoming \n\nincreasingly difficult to distinguish AI-generated \n\ncontent from human-created content. To ensure \n\ntransparency, the watermarking or labeling of \n\nAI generated content has been legally mandated \n\nunder the EU AI Act, U.S. Executive Order 14110 \n\nand state-level requirements, and China's Interim \n\nMeasures for Management of Generative AI. \n\nAt IBM, we believe that \n\nopen technology and \n\ncollaboration are essential \n\nto further the responsible \n\nadoption of AI. An open \n\napproach can support \n\nefforts to develop and \n\nimplement leading \n\ntechnical methods, such \n\nas those used during the \n\ntesting and evaluation of \n\ndata and AI systems. \n\nChristina Montgomery \n\n> IBM Vice President and Chief Privacy and Trust Officer\n\nINDUSTRY EXAMPLE \n\nMeta's Llama is open source, and it \n\naims to power innovation through \n\nthe open-source community. Given \n\nthe safety and security concerns \n\nassociated with generative AI models, \n\nLlama comes with a  responsible-use \n\nguide  and an  acceptable-use policy .\n\nOn the other hand,  OpenAI  has taken \n\na closed approach toward its large \n\nlanguage models, such as GPT-3 and \n\nGPT-4, in the interest of maintaining \n\nits competitive advantage, as well as \n\nto ensure AI safety. â†’ Part IV. The transparency, explainability and interpretability challenge    \n\n> AI Governance in Practice Report 2024 |40\n> TABLE OF CONTENTS â†‘\n\nINDUSTRY EXAMPLE \n\nGoogle uses a technology called  SynthID , which directly \n\nembeds watermarks into Google's text-to-image generator \n\nImagen. Meta has moved toward labeling \n\nAI-generated images  on Facebook, Instagram and \n\nThreads. Although Meta already adds the label \"Imagined \n\nwith AI\" on images generated through its AI feature, \n\nit now also aims to work with industry partners on \n\ncommon standards to add multilingual labels on synthetic \n\ncontent generated with tools of other companies that \n\nusers post on Meta's platforms. Specifically, Meta \n\nis relying on Partnership on AI's  best practices , the \n\nCoalition for Content Provenance and Authenticity's \n\nTechnical Specifications  and the International Press \n\nTelecommunications Council's  Technical Standards  to add \n\ninvisible markers at scale to label AI generated content by \n\ntools of companies such as Google, Microsoft and OpenAI. \n\nWatermarking is gaining traction as a way for organizations to \n\npromote transparency and ensure safety against harmful content, \n\nsuch as misinformation and disinformation. Companies are \n\nembedding watermarks on AI-generated content. Watermarks are \n\ninvisible to the human eye, but are machine readable and can be \n\ndetected by computers as AI generated. \n\nWhile watermarking is becoming a popular technique for \n\ntransparency, it is still not possible to label all AI generated content. \n\nMoreover,  techniques  to break watermarks also exist. \n\nFocusing on the building blocks of AI governance \n\nâ€” like appropriate documentation of AI models and \n\nsystems â€” is important because those foundations \n\nare necessary to enable risk management, impact \n\nassessment, and third-party auditing. \n\nMiranda Bogen \n\n> Center for Democracy and Technology\n> AI Governance Lab Director\n\nAI Governance in Practice Report 2024  | 41 \n\n# Part V. The bias, \n\n# discrimination \n\n# and fairness \n\n# challenge \n\nBias, discrimination and fairness are among the most \n\nimportant challenges of AI governance, given their \n\npotentially very significant real-world impacts on individuals \n\nand communities. Leaving this challenge unaddressed can \n\nlead to discriminatory outcomes and perpetuate inequalities \n\nat scale. Healthy AI governance must promote legal and \n\nethical norms including human rights, professional \n\nresponsibility, human-centered design and control of \n\ntechnology, community development and nondiscrimination. \n\nWhile the automation of human tasks using AI has the \n\nadvantages of scalability, efficiency and accuracy, it is \n\naccompanied by the challenge of  algorithmic bias , whereby \n\na systematic error manifests through an inaccuracy in the \n\nalgorithm. It occurs when an algorithm systematically or \n\nrepeatedly misses certain groups of people more than others. \n\nWith transparency challenges around how or why an input \n\nturns into a particular output, biases in the algorithm can be \n\ndifficult to trace and identify. \n\nInstances of algorithmic bias have been well documented in \n\npolicing , criminal sentencing  and  hiring . Algorithmic bias can \n\nimpact even the most well-intentioned AI systems, and it can \n\nenter a  model  or system in numerous ways. \n\n## Hidden and harmful biases may lurk \n\n## within an AI system. â†’ Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 42  \n\n> TABLE OF CONTENTS â†‘\n\nWays biases may get into the AI system \n\nBiases may get into the AI system in multiple \n\nways during the input, training and output stages. \n\nAt the input stage \n\nâ†’ Historical data.  If historical data used \n\nto train algorithms is biased, then the \n\nalgorithm may learn those biases and \n\nperpetuate them. For example, if an AI \n\nrecruitment tool is trained on historical \n\ndata  containing gender or racial biases, \n\nthose biases will be reflected in the tool's \n\nhiring decisions or predictions. \n\nâ†’ Representation bias.  Biases can also enter \n\nthe algorithm through data that either \n\noverrepresents or underrepresents social \n\ngroups. This can make the algorithmic \n\ndecisions less accurate and create \n\ndemographic or social disparities. \n\nâ†’ Inaccurate data.  The accuracy of data \n\ncan be impaired if it is outdated or \n\ninsufficient. Such data falls short of fully \n\nrepresenting current realities, leading to \n\ninaccurate results, which may also lead to \n\nreinforcement of historical biases. \n\nAt the training stage \n\nâ†’ Model.  Biases can arise when they are an \n\nintrinsic part of the model itself. For example, \n\nmodels developed through traditional \n\nprogramming, i.e., those manually coded by \n\nhuman designers, can have  intrinsic biases \n\nif they are not based on real-world insights. \n\nAn algorithm assisting with university \n\nadmissions may be biased if the human \n\ndesigner programmed it to give a higher \n\npreference to students from private schools \n\nover students from public schools. Intrinsic \n\nbiases may be difficult to spot in AI models, \n\nas they are a result of self-learning and make \n\ncorrelations across billions of data points, \n\nwhich are often part of a black box. \n\nâ†’ Parameters.  The model adjusts its \n\nparameters, such as  weights and biases \n\nin neural networks, during the training \n\nprocess based on the training data. Bias \n\ncan manifest when the values assigned to \n\nthese parameters inadvertently reinforce \n\nthe bias present in the training data or the \n\ndecisions made by the designers during \n\narchitecture selection. In an algorithm for \n\nuniversity admissions, for example, the \n\nattributes of leadership and competitiveness \n\ncan reflect a gender stereotype present \n\nin the training data with the algorithm \n\nfavoring male candidates over female \n\nones. However, bias in parameters can also \n\nmanifest more stealthily, such as through \n\nproxies. In absence of certain data, the \n\nalgorithm will make correlations to make \n\nsense of the missing data. An algorithm \n\nfor loan approval, for example, may \n\ndisproportionately assign more weight \n\nto certain zip codes and the model may \n\ninadvertently perpetuate racial or ethnic \n\nbias by rejecting loan applications using \n\nzip  codes as a proxy. \n\nAt Microsoft, we are steadfast \n\nin our commitment to \n\ndeveloping AI technologies \n\nthat are not only innovative \n\nbut also trustworthy, safe, and \n\nsecure. We believe that the \n\ntrue measure of our progress is \n\nnot just in the capabilities we \n\nunlock, but in the assurance \n\nthat the digital experiences \n\nwe create will enhance \n\nrather than compromise the \n\nhuman experience. \n\nJulie Brill, \n\nMicrosoft Chief Privacy Officer, Corporate Vice President â†’ Part V. The bias, discrimination and fairness challenge    \n\n> AI Governance in Practice Report 2024 |43\n> TABLE OF CONTENTS â†‘\n\nAt the output stage \n\nâ†’ Self-reinforcing biases.  A feedback loop is a process through \n\nwhich the AI system continues to learn based on the outputs \n\nit generates. The output goes back into the system as an input, \n\nwhich can influence the system's behavior or performance in \n\nsome positive or negative way. While feedback loops can foster \n\ncontinuous learning and allow the system to adapt to its deployed \n\nenvironment, they can also lead to  self-reinforcing biases  if the \n\noutputs of the algorithm itself are biased. For example, if an \n\nalgorithm consistently rejects loan applications for women and \n\nconsistently approves them for men, there may be a gender bias at \n\nplay, and the algorithm could fall into a loop where it learns from \n\nthe biased outputs and continues to reinforce the biased pattern. \n\nâ†’ Human oversight.  Although it is necessary to have humans in \n\nthe loop throughout the life cycle of the AI system, there is a \n\nrisk that human biases can reenter the algorithm. For example, \n\nhuman control over a system's final output is necessary, but \n\nbias can externally impact the output based on the human \n\ninterpretation  applied to that final output. \n\nâ†’ Automation bias.  Automation bias refers to the human \n\ntendency to overly rely on automated outputs. This leads \n\nto people trusting the recommendations of algorithms \n\nwithout questioning or verifying their accuracy or being \n\nmindful of the system's limitations and errors. This can be \n\nespecially dangerous when confirmation bias about protected \n\ncharacteristics is at play. That is, users are more likely to accept \n\nthe outputs when they align with their preexisting beliefs. \n\nBias detection and mitigation is particularly challenging \n\nin the  context of foundation models due to their size and \n\ncomplex architectures. â†’ Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 44  \n\n> TABLE OF CONTENTS â†‘\n\nLaw and policy considerations \n\nMany existing equalities and antidiscrimination laws apply to AI \n\nsystems and many emerging initiatives specific to AI governance \n\ninclude provisions on bias. \n\nDepending on the jurisdiction where the organization operates, \n\nliability could also fall under relevant civil rights, human rights \n\nor  constitutional freedoms of that jurisdiction. \n\nIn the U.S., civil rights can be protected through private rights \n\nof action by individuals. For example, according to the guidance \n\nprovided by the U.S. Equal Employment Opportunity Commission, \n\nprivate rights of action against discrimination through algorithms \n\ncould occur under the  Americans with Disability Act  and  Title VII \n\nof the Civil Rights Act . Under both the ADA and Title VII, employers \n\ncan be exposed to liability even where their algorithmic decision-\n\nmaking tools are designed or administered by another entity. When \n\nindividuals think their rights under either of those laws have been \n\nviolated, they can file a charge of discrimination with EEOC. \n\nOECD AI Principles \n\nThe principle of \"human-centered values and fairness\" in the OECD \n\nAI Principles  requires respect for the rule of law, human rights and \n\ndemocratic values across the life cycle of the AI system, through \n\nrespect for nondiscrimination and equality, diversity, fairness, and \n\nsocial justice. This is to be implemented through safeguards, like \n\ncontext-appropriate human determination that is consistent with \n\nthe state of the art. The OECD AI Policy Observatory maintains \n\na catalogue on  tools and metrics  for practically aligning AI with \n\nOECD's principles, including  bias and fairness .\n\nâ†’ SPOTLIGHT \n\nJoint statement by US Federal Agencies \n\nIn April 2023, a  joint statement  made by four federal \n\nagencies, namely the EEOC, Department of Justice, \n\nFederal Trade Commission and Consumer Financial \n\nProtection Bureau, reiterated the U.S.'s commitment to \n\nthe principles of fairness, equality and justice, which \n\nare deeply embedded in federal laws. In April 2024, five \n\nadditional cabinet-level agencies joined that pledge. \n\nThe joint statement now includes the Department of \n\nEducation, Department of Health and Human Services, \n\nDepartment of Homeland Security, Department of \n\nHousing and Urban Development, and Department of \n\nLabor. The Consumer Protection Branch of the Justice \n\nDepartment's Civil Division also joined the pledge. \n\nEnforcement of discrimination through automated \n\nmeans is managed by these federal agencies. â†’ Part V. The bias, discrimination and fairness challenge    \n\n> AI Governance in Practice Report 2024 |45\n> TABLE OF CONTENTS â†‘\n\nUNESCO Recommendations on the Ethics of AI \n\nPrinciples 28, 29 and 30 of UNESCO's  Recommendations on the \n\nEthics of AI  encourage AI actors to promote access to technology \n\nto diverse groups, minimize the reinforcement or perpetuation \n\nof discriminatory or biased outcomes throughout the life cycle \n\nof the AI systems, and reduce the global digital divide. Among \n\nthe tools provided by UNESCO for the practical implementation \n\nof its recommendations is the  ethical impact assessment , which \n\nis designed primarily for government officials involved in the \n\nprocurement of AI systems but can also be used by companies to \n\nassess if an AI system aligns with UNESCO's Recommendations. \n\nEU AI Act \n\nThe EU  AI Act  provides a relevant framework for data governance \n\nof high-risk AI systems under Article 10, which permits training, \n\nvalidation and testing of datasets to examine the possibility of biases \n\nthat affect the health and safety of persons and negatively impact \n\nfundamental rights or lead to discrimination. To deal with the challenge \n\nof self-reinforcing biases, Article 15(4) also requires the elimination \n\nor reduction of biases emanating from feedback loops in high-risk AI \n\nsystems after they have been put on the market or into service. The \n\nEU AI Act calls for consideration of the European Commission's  Ethics \n\nGuidelines for Trustworthy AI , which are voluntary guidelines seeking \n\nto promote \"diversity, non-discrimination and fairness.\" \n\nSingapore \n\nSingapore's AI Verify tackles bias via the principle of \"ensuring \n\nfairness.\" This principle is made up of the pillars of data governance \n\nand fairness. While there are no specific tests for data governance \n\nin the toolkit, if the model is not giving biased outputs based on \n\nprotected characteristics, fairness can be ensured by checking the \n\nmodel against  ground truth . Process checks include the verification of \n\ndocumentary evidence that there is a strategy for fairness metrics and \n\nthat the definition of sensitive attributes is consistent with the law. â†’ Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 46  \n\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## US FTC Enforcement priorities and concerns \n\nWe have made no secret of our enforcement priorities \n\nand concerns. \n\n# 1.  There is no AI exemption from the laws on the books and \n\nbusinesses need to develop and deploy AI tools in ways that \n\nallow for an open and competitive market and protect consumers \n\nfrom potential harms. \n\n# 2.  We are scrutinizing existing and emerging bottlenecks across the AI \n\ndesign stack to ensure that businesses aren't using monopoly power \n\nto block innovation and competition. \n\n# 3.  We are acutely aware that behavioral advertising, brought on by \n\nweb 2.0, fuels the endless collection of user data and recognize \n\nthat model training is emerging as another feature that could further \n\nincentivize surveillance. \n\n# 4.  We are squarely focused on aligning liability with capability and \n\ncontrol, looking upstream and across layers of the AI stack to \n\npinpoint which actor is driving or enabling the lawbreaking. \n\n# 5.  We are focused on crafting effective remedies in cases \n\nthat establish bright-line rules on the development, use and \n\nmanagement of AI inputs, such as prohibiting the uses of inaccurate \n\nor highly-sensitive data when training models. \n\nSamuel Levine \n\nFTC Bureau of Consumer Protection Director â†’ Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 47  \n\n> TABLE OF CONTENTS â†‘\n\nThe US \n\nIn the U.S., antidiscrimination laws that also \n\nextend to AI are scattered across various sectors, \n\nsuch as employment, housing and civil rights. \n\nâ†’ Employment.  Under the  Americans with \n\nDisabilities Act , employers are prohibited from \n\nusing algorithmic decision-making tools that \n\ncould violate the act, such as in not providing \n\nreasonable accommodations, intentionally \n\nor  unintentionally screening out an individual \n\nwith a disability, or adopting disability-related \n\ninquiries and medical examinations. \n\nâ†’ Housing.  In 2023, to ensure fairness in \n\nhousing, the Biden-Harris Administration \n\nissued a proposed rule against racial bias in \n\nalgorithmic  home valuations , empowering \n\nconsumers to take action against appraisal \n\nbias, increasing transparency and leveraging \n\nfederal data to inform policy and improve \n\nenforcement against appraisal bias. \n\nâ†’ Consumer finance.  The  CFPB  confirmed \n\ncompanies are not absolved of their legal \n\nresponsibilities under existing legislation, \n\nsuch as the  Equal Credit Opportunity Act ,\n\nwhen they use AI models to make lending \n\ndecisions. Remedies include compensating \n\nthe victim, providing injunctive relief to \n\nstop unlawful conduct, or banning persons \n\nor companies from future participation in \n\nthe marketplace. \n\nâ†’ Voluntary frameworks.  The  NIST \n\nSpecial  Publication 1270: Towards a \n\nStandard for Identifying and Managing \n\nBias in AI  lays down governance standards \n\nfor managing  AI bias. These include \n\nmonitoring the system for biases, \n\nmaking  feedback channels available \n\nso users can flag incorrect or harmful \n\nresults  for which they can seek recourse, \n\nputting policies and procedures in \n\nplace for every stage of the life cycle, \n\nmaintaining model documentation to \n\nensure accountability, and embedding \n\nAI governance within the culture of \n\nthe  organization. \n\nAs we navigate the \n\ntransformative potential of \n\nAI, it is imperative that we \n\nanchor our journey in our \n\ncollective ability to protect \n\nfundamental rights and the \n\nenduring values of safety \n\nand accountability. \n\nJulie Brill \n\nMicrosoft Chief Privacy Officer, \n\nCorporate Vice President â†’ Part V. The bias, discrimination and fairness challenge    \n\n> AI Governance in Practice Report 2024 |48\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## FTC enforcement action against Rite Aid \n\nOn 19 Dec. 2023, the FTC issued an  enforcement action  against \n\nRite Aid's  discriminatory use  of facial recognition technology. \n\nRite  Aid deployed facial recognition surveillance systems for theft \n\ndeterrence without assessing the accuracy or bias of the system. \n\nRite Aid recorded thousands of false-match alerts, and the FTC's gender-based \n\nanalysis revealed Black, Asian, Latino and women consumers were more likely to \n\nbe harmed by Rite Aid's surveillance technology. \n\nThe FTC placed a five-year moratorium on Rite Aid's use of facial recognition, \n\nand if after five years Rite Aid chooses to use this technology again, it will have \n\nto implement the FTC's governance plan detailed in the  order . The enforcement \n\ndecision also included an order for  disgorgement , that is, to delete or destroy any \n\nphotos and videos including any data, models or algorithms used for surveillance. \n\nThis case serves as good indication of the nature and intensity \n\nof liability that deployers and providers of AI in the U.S. may \n\nbe exposed to for deploying discriminatory AI systems. â†’ Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 49  \n\n> TABLE OF CONTENTS â†‘\n\nImplementing AI governance \n\nOne overarching practice used to mitigate biases is \n\nthe promotion of diversity and inclusivity among \n\nteams working across the life cycle of the AI \n\nsystem. Personnel composition is often supported \n\nby organization-level principles for safe and \n\nresponsible AI, many of which internal AI ethics \n\npolicies, e.g.,  Google , IBM  and  Microsoft .\n\nBias testing \n\nOne way to minimize bias in AI systems is by \n\ntesting the systems. While there are  numerous \n\nways  to test for bias in AI systems, it is important \n\nto understand what is being evaluated. \n\nDemographic parity may be different than equality \n\nobjectives. Establish goals based on the desired \n\nsystem outcomes to start, and then establish an \n\nappropriate technique for testing bias within the \n\nsystem. For example, does fairness mean an equal \n\nnumber of males and females will be screened for \n\na new position on your team, or that candidates \n\nwith the most distinguished resumes are identified \n\nas ideal applicants independent of their gender, \n\nrace, experience, etc. \n\nIt is important to note that often testing for bias \n\nwill require the use of personal information \n\nto determine if fairness objectives are being \n\nmet. As such, there may be a  privacy-bias \n\ntrade-off , as safeguarding privacy through data \n\nminimization creates  challenges  for mitigating \n\nbiases in AI systems. Some considerations when \n\nbalancing privacy while mitigating bias include: \n\nâ†’ Intentionally collecting sensitive data \n\ndirectly in the design phase so it is ready \n\nfor the testing phase. This can be done by \n\nprocuring consent from data subjects and \n\ndisclosing the purpose for the collection and \n\nprocessing of their data. \n\nâ†’ Creating intentional proxies to test how the \n\nsystem makes correlations without sensitive \n\ndata, such as for demographic features. \n\nâ†’ Buying missing data from data brokers, \n\npublic data or other datasets in compliance \n\nwith privacy and data governance policies. \n\nFairness tests and debiasing \n\nmethods are not created \n\nequally â€” as an AI deployer \n\nor governance professional, \n\nit is critically important to \n\nuse tools and methods that \n\nfundamentally align with \n\nequality and nondiscrimination \n\nlaw in your jurisdiction. \n\nBrent Mittelstadt \n\nUniversity of Oxford Internet Institute Director of Research, \n\nAssociate Professor and Senior Research Fellow AI Governance in Practice Report 2024  | 50 \n\n# Part VI. \n\n# The security \n\n# and robustness \n\n# challenge \n\nCompromises to the security of AI could result in manipulation \n\nof outputs, stolen sensitive information or interference with \n\nsystem operations. Unsecured AI can result in financial losses, \n\nreputational damage and even physical harm. For example, \n\nexploiting the vulnerabilities of medical AI could lead to a \n\nmisdiagnosis, and adversarial attacks on autonomous vehicles \n\ncould lead to road traffic accidents. \n\nAlthough AI security overlaps with and suffers from traditional \n\ncybersecurity risks, cybersecurity is often about protecting \n\ncomputer systems and networks from attacks, whereas AI \n\nsecurity is about guarding the AI system's components, namely \n\nthe data, model and outputs. When it comes to AI security, \n\nmalicious actors can enable adversarial attacks by exploiting \n\nthe  inherent limitations  of AI algorithms. \n\n## The pace, scale, and reach of AI \n\n## development and integration \n\n## demands strong security. â†’ Part VI. The security and robustness challenge    \n\n> AI Governance in Practice Report 2024 |51\n> TABLE OF CONTENTS â†‘\n\nAdversarial attacks \n\nAdversarial attacks are a deliberate attempt to \n\nmanipulate models in a way that leads to incorrect \n\nor harmful outputs. The intention behind \n\nthe attack could be to lead the model toward \n\nmisclassification or cause harm, and all it may take \n\nto trick the model is a slight switching of pixels or \n\nadding a bit of noise. Some types of adversarial \n\nattacks include: \n\nâ†’ Evasion attacks.  The aim of evasion \n\nattacks is to deceive the model into \n\nmisclassifying data, such as by adding a \n\nsmall perturbation to the input image, as \n\nin the  MIT example , leading to an incorrect \n\noutput with high  confidence. \n\nâ†’ Data poisoning.  This can happen in various \n\nways, such as by switching the labels of \n\nlabeled data or injecting entirely new data \n\ninto the dataset. However, for this to work, \n\nthe adversary will have to first gain access \n\nto training data.  Data poisoning  can also \n\nhelp attackers create  backdoors  so they can \n\nmanipulate model behavior in the future. \n\nâ†’ Model extraction.  The aim of model \n\nextraction is model theft by reverse \n\nengineering to reveal the hidden mechanism \n\nof the model or sensitive information, or \n\nto make the model vulnerable to further \n\nattacks. This is done by feeding carefully \n\ncrafted  queries  to a black-box model to \n\nanalyze its outputs and steal its functionality. \n\nThis can help the adversary copy the model \n\nand make financial gains. \n\nAI vulnerabilities can also be exploited through \n\nopen-source software and third-party risks. \n\nâ†’ Open-source software can be manipulated \n\nin many ways, such as through supply-chain \n\nattacks, in which open-source AI libraries \n\nare targeted by malicious code that is planted \n\nas a legitimate update or functionality. \n\nAlthough open-source software suggests \n\neverything has been made publicly available, \n\nthe original developers can restrict access \n\nto some parts of the software in the license \n\nagreement. In such cases, hackers may resort \n\nto model extraction. Even if an AI  system \n\nis not open source, the project may rely on \n\na complex ecosystem of open-source tools, \n\nexposing itself to a potential attack surface \n\nthat malicious actors can exploit. \n\nâ†’ A lack of control and visibility over third-\n\nparty governance practices makes risk \n\nmitigation more difficult, including with \n\nrespect to security. Third-party vendors \n\nmay have weaker security standards and \n\npractices, making them more vulnerable \n\nto data breaches, supply chain attacks and \n\nsystem hacks, among other security risks. â†’ Part VI. The security and robustness challenge    \n\n> AI Governance in Practice Report 2024 |52\n> TABLE OF CONTENTS â†‘\n\nLaw and policy considerations \n\nRegulatory and voluntary governance tools that \n\nhave established requirements for tackling AI \n\nsecurity issues include the NIS2 Directive, U.S. \n\nExecutive Order 14110, the NIST AI RMF and the \n\nNIST Cybersecurity Framework. \n\nNIS2 \n\nThe  NIS2  Directive replaces the EU Network \n\nand Information Security Directive from 2016. \n\nIt  aims to boost resilience and incident-response \n\ncapacities in public and private sectors through \n\nrisk management and reporting obligations. \n\nSome cybersecurity requirements under Article \n\n21 include policies on risk analysis and system \n\nsecurity, incident handling, supply-chain \n\nsecurity, policies and procedures to assess \n\ncybersecurity risk-management effectiveness, \n\ncyber hygiene practices, policies on the use of \n\ncryptography, and encryption. \n\nEU AI Act \n\nAs with most AI themes, under the EU  AI Act ,\n\nsecurity takes a risk-based approach. As such, \n\nsecurity and robustness requirements vary based \n\non if the system is high risk or if it is a GPAI system \n\nwith systemic risks. \n\nâ†’ High-risk AI systems.  The EU AI Act \n\nlays down detailed security obligations \n\nfor accuracy, security and robustness \n\nof high-risk AI systems. Technical and \n\norganizational measures are to be placed to \n\nensure high-risk systems are resilient toward \n\nerrors, faults and inconsistencies. Possible \n\nsolutions include back-up or fail-safe plans. \n\nThe act also foresees risks emerging at the \n\nthird-party level, requiring resilience against \n\nunauthorized third-party attempts to alter \n\nuse, outputs or performance by exploiting \n\nthe system vulnerabilities. Technical \n\nsolutions to handle such security risks must \n\nbe appropriate to circumstances and risk. \n\nThese can include measures to prevent, \n\ndetect, respond to, resolve and control data \n\npoisoning, model poisoning, adversarial \n\nexamples and model evasion, confidentiality \n\nattacks, or model flaws. \n\nAdditionally, the EU AI Act obliges providers \n\nof high-risk AI systems to ensure they \n\nundergo conformity assessments that \n\ndemonstrate compliance with requirements \n\nfor high-risk systems. \n\nâ†’ Obligations for providers of GPAI systems \n\nwith systemic risks.  The EU AI Act lists the \n\nsecurity requirements for high-impact AI \n\nsystems. Requirements include: \n\nâ€¢ Evaluating models in accordance with \n\nstandardized protocols, such as conducting \n\nand documenting adversarial testing to \n\nidentify and mitigate systemic risks. \n\nâ€¢ Monitoring, documenting and reporting \n\nserious incidents to the AI Office. \n\nâ€¢ Ensuring GPAI models with systemic risks \n\nand their physical infrastructures have \n\nadequate levels of cybersecurity. â†’ Part VI. The security and robustness challenge \n\nAI Governance in Practice Report 2024  | 53  \n\n> TABLE OF CONTENTS â†‘\n\nUS  Executive  Order 14110 \n\nThe U.S.  AI Executive Order 14110  calls on \n\ndevelopers of the most powerful AI systems \n\nto share their safety results and other critical \n\ninformation with the U.S. government. It also \n\ncalls on the NIST to develop rigorous standards \n\nfor extensive red-team testing to ensure safety \n\nbefore public release. \n\nNIST AI RMF \n\nThe NIST  AI RMF  identifies common security \n\nconcerns such as data poisoning and exfiltration \n\nof models, training data or other intellectual \n\nproperty through AI system endpoints. Under \n\nthe AI RMF, a system is said to be secure when \n\nit can maintain confidentiality, integrity and \n\navailability through protection mechanisms that \n\nprevent unauthorized access and use. Practical \n\nimplementation can be achieved through the \n\nNIST Cybersecurity Framework and  RMF .\n\nNIST Cybersecurity Framework \n\nThe NIST  Cybersecurity Framework  is a voluntary \n\nframework that provides standards, guidelines \n\nand best practices for organizations to mitigate \n\ncybersecurity risks. The framework is organized \n\nunder five  key functions : identify, protect, detect, \n\nrespond and recover. \n\nThere is an urgent need to \n\nrespond to the complex \n\nchallenges of AI governance \n\nby professionalizing the field. \n\nA professionalized workforce \n\ncan take AI governance from \n\ntheory to practice, spread \n\ntrustworthy and standardized \n\npractices across industries \n\nand borders, and remain \n\nadaptable to swiftly changing \n\ntechnologies and risks. \n\nJ. Trevor Hughes \n\nIAPP President and CEO â†’ Part VI. The security and robustness challenge \n\nAI Governance in Practice Report 2024  | 54  \n\n> TABLE OF CONTENTS â†‘\n\nImplementing AI governance \n\nDue diligence in the identification of security \n\nrisks throughout the life cycle of the system is \n\nan important activity, especially when a third-\n\nparty vendor is involved. Due diligence can only \n\never inform. With appropriate information, an \n\norganization can seek contract terms with third-\n\nparty vendors that mandate: \n\nâ†’ Making the vendor's security practices \n\ncompatible with the organization's \n\nown  standards. \n\nâ†’ Monitoring system robustness regularly \n\nthrough security assessments or audits to \n\nidentify third-party risks and ensure the \n\nvendor is complying with the organization's \n\nsecurity standards. \n\nâ†’ Limiting access to third-party vendors only \n\nfor the services they need to perform. \n\nRed teaming \n\nRed teaming is the process of testing the security \n\nof an AI system through an  adversarial lens  by \n\nremoving defender bias. It involves the simulation \n\nof adversarial attacks on the model to evaluate \n\nit against certain benchmarks, \" jailbreak \" it and \n\nmake it behave in unintended ways. Red teaming \n\nreveals security risks, model flaws, biases, \n\nmisinformation and other harms, and the results \n\nof such testing are passed along to the model \n\ndevelopers for remediation. Developers use red \n\nteaming to bolster and secure their product before \n\nreleasing it to the public. \n\nSecure data sharing practices \n\nDifferential privacy is primarily a  privacy-\n\nenhancing technique  that also has security \n\nbenefits, it analyzes group data while \n\npreserving individual privacy by adding \n\ncontrolled noise to the data and blurring \n\nindividual details. So, even if an attacker were \n\nto steal this data, they would not be able to \n\nlink it back to specific individuals, minimizing \n\nharm. As such, differential privacy can limit \n\nthe utility of stolen data. However, that impact \n\nto the utility of the data can also impact \n\norganizations with lawful and legitimate \n\ninterests in processing the data. Moreover, \n\ndifferential privacy can also be a costly \n\ntechnique to implement, especially where \n\nlarge  datasets are concerned. \n\nHITL \n\nHuman in the loop refers to incorporating \n\nhuman expertise and oversight into the \n\nalgorithmic decision-making process. Although \n\nHITL may provide a gateway for human \n\nbiases to reenter the algorithm when making \n\njudgements about final outputs, in the context \n\nof  AI security, HITL can make incident detection \n\nand response more efficient. This is especially \n\ntrue where subtle manipulations or attacks that \n\nthe model may not have been trained to identify \n\nare involved.  HITL  allows for continuous \n\nmonitoring and verification, however, optimal \n\nuse of this approach rests on balancing the \n\ncontradictions that may arise to address bias \n\nor  safety and security. \n\nINDUSTRY EXAMPLE \n\nOpenAI's latest text-to-video \n\nmodel,  Sora , was red teamed. \n\nIn preparation for the U.K. AI \n\nSafety Summit, Meta released \n\na document  detailing the \n\nsafety of its Llama 2 model, \n\nas well as the benchmarks and \n\npotential attack vectors it \n\nwas red teamed for. AI Governance in Practice Report 2024  | 55 \n\n# Part VII. \n\n# AI safety \n\nVarious themes, particularly value alignment, transparency \n\nand AI security, eventually culminate into the broader \n\ntheme of AI safety. Given that safety is an all-encompassing \n\ntheme, it has no settled global definition. It may include \n\npreventing so-called existential risks posed by artificial \n\ngeneral intelligence. For some, such as the  Center for AI \n\nSafety , AI risk is categorized based on malicious use, the AI \n\nrace, rogue behavior and organizational risks. For others, \n\nsuch as the country signatories to the  Bletchley Declaration ,\n\nand most recently, for parties to the  Seoul Declaration for \n\nSafe, Innovative and Inclusive AI , it is about managing risks \n\nand being prepared for unexpected risks that may arise from \n\nfrontier AI. AI safety can also be the term used to describe \n\nminimizing AI harms from misinformation, disinformation \n\nand deepfakes, and the unintended behavior of an AI system, \n\nespecially advanced AI systems. \n\n## AI safety is a cornerstone but \n\n## somewhat mercurial principle for \n\n## realizing safe and responsible AI. â†’ Part VII. AI safety    \n\n> AI Governance in Practice Report 2024 |56\n> TABLE OF CONTENTS â†‘\n\nLaw and policy considerations \n\nThe importance of AI safety is reflected in the fact \n\nthat, for some jurisdictions, it has been embedded \n\nas a main theme in national strategies toward \n\nAI. The Biden-Harris Administration's Executive \n\nOrder 14110 focuses on developing \"Safe, Secure \n\nand Trustworthy\" AI. In 2023, the U.K. brought \n\nworld leaders together for first AI Safety Summit, \n\nand the country's approach toward AI is focused \n\non the safety of advanced AI systems, or \"frontier \n\nAI.\" Safety is also an important factor under the \n\nEU AI Act, which is reflected in the security and \n\nrobustness requirements for high-impact GPAI \n\nsystems and  high-risk AI systems. \n\nAI safety institutes \n\nRecently, the NIST announced it would establish \n\nthe  U.S. AI Safety Institute . To support this \n\ninstitute, the NIST also created an  AI Safety \n\nInstitute Consortium , which brought more than \n\n200 organizations together to develop guidelines \n\nand standards for AI measurement and policy \n\nthat can lay the foundation for AI safety globally. \n\nAmong many security- and safety-related \n\ninitiatives, the AISIC is tasked with enabling \n\ncollaborative and interdisciplinary research \n\nand establishing a knowledge and data sharing \n\nspace for AI stakeholders. More specifically, \n\nthe AISIC will develop new guidelines, tools, \n\nmethods, protocols and best practices to \n\nfacilitate the evolution of industry standards \n\nfor AI safety. The AISIC will also develop \n\nbenchmarks for evaluating AI capabilities, \n\nespecially harmful ones. \n\nThe U.K. government established an  AI Safety \n\nInstitute  to build a sociotechnical infrastructure \n\nthat can minimize risks emerging from \n\nunexpected advancements in AI technology. \n\nThe institute has been entrusted with three \n\nmain functions: developing and conducting \n\nevaluations on advanced AI systems, driving \n\nfoundational AI research, and facilitating the \n\nexchange of information. â†’ Part VII. AI safety \n\nAI Governance in Practice Report 2024  | 57  \n\n> TABLE OF CONTENTS â†‘\n\nBletchley Declaration \n\nThe 2023 U.K. AI Safety Summit brought \n\ntogether international governments, leading \n\nAI  companies and civil society groups to discuss \n\nfrontier AI risks and ways to promote AI safety. \n\nAs a demonstration of their commitments to \n\nAI safety, participating nations also signed \n\nthe Bletchley Declaration, which makes \n\nvarious affirmations to cooperate globally \n\non innovation, sustainable development, \n\neconomic growth, protection of human rights \n\nand fundamental freedoms, and building public \n\ntrust and confidence in AI technology. \n\nEU AI Act \n\nThe security requirements for general-purpose \n\nAI systems under the AI Act are also focused \n\non regulating \"systemic risks.\" The EU AI act \n\ndefines this risk as one emerging from high-\n\nimpact general purpose models that \"significantly \n\nimpact the internal market, and with actual or \n\nreasonably foreseeable negative effects on public \n\nhealth, safety, public security, fundamental \n\nrights, or the society as a whole, that can be \n\npropagated at  scale across the value chain.\" \n\nAI Safety Standards \n\nISO/IEC Guide 51:2014  provides requirements \n\nand recommendations for drafters of standards \n\nto include safety aspects in those standards. It \n\napplies to safety aspects pertaining to people, \n\nenvironments or both. \n\nWe are generating 2.5 \n\nquintillion bytes of data \n\nglobally per day. Much of this \n\nis flowing into our internet. \n\nTherefore, generative AI \n\nmodels are dynamic and the \n\napplications that are built on \n\ntop of them will move. It is up \n\nto the organizations to ensure \n\nthat the movement meets \n\ntheir standards. \n\nDominique Shelton Leipzig \n\nMayer Brown Partner, Cybersecurity & Data Privacy \n\nand Leader, Global Data Innovation & AdTech â†’ Part VII. AI safety    \n\n> AI Governance in Practice Report 2024 |58\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## Compute governance \n\nOn a broader level, AI safety also refers to regulating compute, i.e., the \n\npower source of AI systems, as regulating AI at its source increases the \n\nvisibility of its technical capabilities. Unlike AI models, which can be \n\nreplicated exponentially and without control, compute must be purchased \n\nand is quantifiable. As computing chips are manufactured through highly \n\nconcentrated  supply chains  and dominated by only a few companies, \n\nregulatory interventions can be more focused. Such  regulation  can \n\npurposefully occur with AI safety in mind to control the allocation of \n\nresources for AI projects by subsidizing or limiting access to compute \n\nor by building guardrails into hardware. \n\nWith compute governance gaining traction because of advanced AI \n\nsystems, compute thresholds, i.e., numerical measures of computing \n\npower, are also being set legally, which helps distinguish AI systems with \n\nhigh capabilities from other AI systems. \n\nFor instance, U.S. Executive Order 14110 requires models using computing \n\npower greater than 10 26  integer and models using biological sequence \n\ndata and computing power greater than 10 23  integer to provide the \n\ngovernment with information and reports on the models testing and \n\nsecurity on an ongoing basis. \n\nSimilarly, under the EU AI Act, GPAI is presumed to have high-impact \n\ncapabilities when cumulative compute used for training is greater than \n\n10 25  floating-point operations. When a model meets this threshold, the \n\nprovider must notify the Commission, as meeting the threshold leads to \n\nthe presumption that this is a GPAI system with systemic risk. This means \n\nthe model can have a significant impact on the internal market, and actual \n\nor reasonably foreseeable negative effects on health, safety, fundamental \n\nrights or society. Providers need to comply with requirements on model \n\nevaluation, adversarial testing, assessing and mitigating systemic risks, \n\nand reporting any serious incidents. â†’ Part VII. AI safety \n\nAI Governance in Practice Report 2024  | 59  \n\n> TABLE OF CONTENTS â†‘\n\nImplementing AI governance \n\nThe organizational practices for security and \n\nrobustness discussed in this report, such as \n\nred teaming for adversarial testing, HITL and \n\nprivacy-preserving technologies, can apply to \n\nAI safety. Similarly, organizational practices and \n\nlaws requiring transparency and explainability, \n\nspecifically watermarks, also apply to AI safety. \n\nPrompt engineering \n\nOne of OpenAI's  safety practices  includes \n\nprompt engineering  to help generative AI \n\nunderstand prompts in a given context. This \n\npractice is aimed at minimizing harmful and \n\nundesired outputs from generative AI, and it \n\nhelps developers exercise more control over \n\nuser interactions with AI to reduce misuse at \n\nthe user level. Moreover, as part of  product \n\nsafety standards , OpenAI also has put in \n\nplace  usage policies .\n\nReports and complaints \n\nAnother safety practice of OpenAI is allowing \n\nusers to report issues that can be monitored \n\nand responded to by human operators. This is \n\nnot yet a popular practice. A 2023 study carried \n\nout by  TrustibleAI  found out of 100 random \n\norganizations, three provided an individual \n\nappeals process between the individual and \n\nthe company. It is possible internal governance \n\nand complaint mechanisms may become \n\nmore common post-EU AI Act, given that, \n\nunder Article 27 (f), deployers of AI systems \n\nmust carry out FRIAs of internal governance \n\nand complaint mechanisms where a  risk has \n\nmaterialized into a harm. \n\nSafety by design \n\nTo combat abusive AI-generated content, \n\nMicrosoft is focused on building strong safety \n\narchitecture through the  safety by design \n\napproach, which can be applied at the AI \n\nplatform, model and application levels. Some \n\nefforts include red teaming, pr eemptive \n\nclassifiers, blocking abusive prompts, automated \n\ntesting and rapid bans of users who abuse the \n\nsystem. With regard to balancing freedom of \n\nspeech against abusive content, Microsoft is \n\nalso committed to identifying and removing \n\ndeceptive and abusive content on LinkedIn, \n\nMicrosoft Gaming Network and other  services. \n\nHumans control AI, not the \n\nother way around. Generative \n\nAI models drift. The only way \n\nfor companies to know when/ \n\nhow they are drifting is to \n\ncontinuously test, monitor and \n\naudit the AI applications for high \n\nrisk use cases- every second of \n\nevery minute of every day. This \n\nis the only way to ensure that \n\nthe model output comports with \n\nthe organizationâ€™s pre-installed \n\nguardrails for accuracy, health \n\nand safety, privacy, bias. \n\nDominique Shelton Leipzig \n\nMayer Brown Partner, Cybersecurity & Data Privacy and Leader, \n\nGlobal Data Innovation & AdTech â†’ Part VII. AI safety    \n\n> AI Governance in Practice Report 2024 |60\n> TABLE OF CONTENTS â†‘\n\nSafety policies \n\nIn preparation for the U.K. AI Safety Summit, Meta released an \n\noverview of its  AI safety policies , specifically in relation to its \n\ngenerative AI Llama model. In addition to model evaluations and \n\nred-team analysis, the policy also detailed Meta's model reporting \n\nand sharing, reporting structure for vulnerabilities found after \n\nmodel release, post-deployment monitoring for patterns of misuse, \n\nidentifiers of AI generated material, data input controls and audits, \n\nand priority research on societal, safety and security risks. \n\nIndustry best practices \n\nPartnership on AI has invested extensively in AI safety research and \n\nresources. Some of its work includes  Guidance for Safe Foundation \n\nModel Deployment . This framework is a living document targeted at \n\nmodel providers on ways to operationalize AI safety for responsible \n\ndeployment. The framework provides  custom guidance  providers of \n\nfoundation models can follow throughout the deployment process \n\nthat is appropriate for their model's capabilities. Another resource \n\nis PAI's  SafeLife , which is a benchmark focused on avoiding negative \n\nside effects in complex environments.  SafeLife  is a reinforcement \n\nlearning environment that tests the \"safety of reinforcement \n\nlearning agents and the algorithms that train them.\" It allows \n\nagents to navigate a complex environment to accomplish a primary \n\ntask. The aim is to create a \"space for comparisons and improving \n\ntechniques for training non-destructive agents.\" AI Governance in Practice Report 2024  | 61 \n\n# Part VIII. \n\n# The copyright \n\n# challenge \n\nCopyright  refers to the rights that creators have over the \n\nexpression of their artistic or intellectual works. Although it is \n\nnot possible to provide an exhaustive list of \"works\" covered by \n\ncopyright legislation, globally copyright protection has been \n\nextended to include a wide range of works, such as literature, \n\nmusic, architecture and film. In the context of modern \n\ntechnology, computer software programs, e-books, online \n\njournal publications and the content of websites such as news \n\nreports and databases are also copyrightable. \n\n## Generative AI is raising new \n\n## challenges for copyright law. \n\nThe clear establishment of intellectual property \n\nrights around both inputs and outputs for generative \n\nAI models is of crucial importance to creative artists \n\nand the creative industries. In the face of dramatically \n\ngrowing machine capabilities, we need to make sure that \n\nincentives for human creation remain strong.\" \n\nLord Tim Clement-Jones \n\nU.K. House of Lords Liberal Democrat Peer and \n\nSpokesperson for Science, Innovation and Technology â†’ Part VIII. The copyright challenge \n\nAI Governance in Practice Report 2024  | 62  \n\n> TABLE OF CONTENTS â†‘\n\nLaw and policy considerations \n\nIn  most countries , and especially those party \n\nto the  Berne Convention , copyright protection \n\nis obtained automatically upon creation of the \n\nwork. In other words, copyright registration is \n\nnot necessary for proprietarily safeguarding \n\nartistic and intellectual works. Regardless, \n\nwhile offering automatic copyright protection, \n\nmany countries, including the U.S., also allow \n\nvoluntary  copyright registration. \n\nCopyright provides owners two types of rights: \n\neconomic rights, through which the owner can \n\nmake financial gains by authorizing use of their \n\nwork by others through a license, and moral \n\nrights, which include noneconomic interests such \n\nas the right to claim authorship of a work or to \n\noppose changes to a work that could harm the \n\nowner's reputation. \n\nCopyright  protects artistic and intellectual works \n\nby preventing others from copying, adapting, \n\ndistributing, performing or publicly displaying \n\nthe work, or creating derivative works. When \n\nan individual does any of these without the \n\nauthorization of the rights' owner, this may \n\nconstitute copyright infringement. \n\nThe use of copyright protected content requires \n\nthe authorization of the original author, unless a \n\nstatutory copyright exception applies. A legitimate \n\nexception to copyright infringement in some \n\njurisdictions is fair use or fair dealing. This is a \n\nlimitation on the exclusive rights of a copyright \n\nholder, which sometimes allows the use of the \n\nwork without the right holder's permission. \n\nIn the U.S., fair use is statutorily defined \n\nunder  17 U.S. Code Â§ 107 , and four \n\nfactors assist courts in making a fair-use \n\ndetermination. These include purpose and \n\ncharacter of use, nature of copyrighted work, \n\nsubstantiality of use, and impact of use on \n\nthe potential market of the copyrighted \n\nwork. Similarly, Singapore's  Copyright Act \n\nof 2021  also includes a fair-use exemption \n\nand takes into account the same four factors \n\nas the U.S. courts. Singapore's old copyright \n\nlaw also had a fifth factor, which considered \n\nthe possibility of obtaining a work within a \n\nreasonable time at an ordinary commercial \n\nprice. However, under the new law, the fifth \n\nfactor may be considered by courts only \n\nwhen relevant. \n\nThe U.K. also has a permitted exemption \n\nto copyright infringement termed  fair \n\ndealing . There is no statutory definition for \n\nfair dealing as, depending on the case, it \n\nwill always be a matter of fact, degree and \n\nimpression. Other factors the U.K. courts \n\npreviously considered to determine fair \n\ndealing include the effect on the market for \n\nthe original work and whether the amount of \n\nwork copied was reasonable and  appropriate. \n\nCommon remedies that can be granted by \n\na court ruling on copyright infringement \n\ninclude injunctions, damages for the loss \n\nsuffered, statutory damages, infringer's \n\nprofits, surrender or destruction of infringing \n\narticles, and attorney fees and costs. \n\nThough copyright has emerged \n\nas one of the first and foremost \n\nfrontiers between AI and intellectual \n\nproperty, the full gamut of IP \n\nrights are engaged by AI, and \n\nspecifically generative AI: design \n\nrights, performersâ€™ rights, patents \n\nand trademarks. Anthropocentric \n\napproaches to IP will butt up against \n\nAIâ€™s learning techniques, its scale \n\nand the nature of its outputs, leaving \n\nmuch uncertainty, complexity and \n\nvariety in the implementation of AI \n\nand IP governance. \n\nJoe Jones \n\nIAPP Director of Research and Insights â†’ Part VIII. The copyright challenge    \n\n> AI Governance in Practice Report 2024 |63\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## Generative AI copyright litigation in the U.S. \n\nTwo main lines of argument are emerging in ongoing \n\nAI  copyright litigation in the U.S. \n\nPetitioners are arguing that: \n\nâ†’ Defendants made copies of copyrighted works when ingesting them \n\nfor training foundation models. \n\nâ†’ As the generated outputs were trained on copyrighted material, \n\nthe outputs themselves are also infringing derivative works. \n\nMore specifically, in a lawsuit against OpenAI, the New York Times argued \n\nthat OpenAI and Microsoft's generative AI tools were built by copying years \n\nof journalistic work without permission or payment, and both companies \n\nare making high profits through their generative AI tools, which now \n\ncompete with the news outlet as reliable sources of information. \n\nOpenAI's motion to dismiss the lawsuit provides background on fair \n\nuse law, and it argues courts have historically used fair use to protect \n\nuseful innovations and copyright is not a veto right over transformative \n\ntechnologies that leverage existing work internally. \n\nThe assessment of fair use is likely to include an \n\nevaluation of exactly what was or is being copied, \n\nwhether ingestion of copyrighted material amounts to \n\ntransformative use, the substantiality of the copying and \n\nthe economic harm caused by using copyrighted material \n\nin developing generative AI models on the potential \n\nmarket for the copyrighted work. \n\nSimilarly, in Tremblay v. OpenAI, various authors alleged copyright \n\ninfringement based on the ingestion of training data that copied \n\nthe works of the authors without consent, credit or compensation. \n\nA California court  recently  rejected  claims on vicarious copyright \n\ninfringements, Digital Millennium Copyright Act violations, negligence \n\nand unjust enrichment. â†’ Part VIII. The copyright challenge \n\nAI Governance in Practice Report 2024  | 64  \n\n> TABLE OF CONTENTS â†‘\n\nImplementing AI governance \n\nNumerous  copyright-safety solutions  and \n\nharm-mitigation strategies  are emerging, \n\nnotwithstanding the uncertainty present due \n\nto  pending litigation. \n\nâ†’ Opt outs.  As foundation models are \n\ntrained on vast amounts of data online, \n\norganizations may not be aware that their \n\ncopyrighted material is used for training. \n\nIn those scenarios, when organizations \n\nare concerned about their webpages being \n\nscraped, an opt-out process, like that of \n\nOpenAI , may be a workable strategy to \n\nmitigate the risk of unwanted scraping. \n\nâ†’ Liability considerations.  Given the fear of \n\npotentially becoming a copyright infringer \n\nas a user of generative AI, commercial \n\nusers may avoid engaging with providers \n\nof  generative AI services. \n\nâ†’ Explore technical guardrails. \n\nOrganizations can also make use of \n\ntechnical guardrails that help them \n\nrespect  the copyrights of authors. Microsoft \n\nincorporated guardrails such as content \n\nfilters, operational monitoring, classifiers, \n\nabuse detection and other technologies to \n\nreduce the likelihood of Copilot returning \n\ncopyright-infringing content. \n\nâ†’ Generative AI requirements.  To increase \n\ntransparency around data used to train \n\ngenerative AI models, including copyrighted \n\ndata, certain jurisdictions such as the \n\nEU require system providers to publish \n\ndetailed summaries of the content used for \n\ntraining their models. Further, with respect \n\nto copyright compliance, the EU AI Act \n\nrequires providers to implement a copyright \n\npolicy mandating protocols to identify and \n\nobserve applicable copyright laws. \n\nINDUSTRY EXAMPLE \n\nMicrosoft  committed to absolve \n\nits users of liability by assuming \n\nvicarious responsibility for \n\ninfringements when use of its \n\nCopilot service leads to legal issues \n\nfor their commercial customers. \n\nMicrosoft also requires customers \n\nto use content filters and other \n\nbuilt-in product safety features, \n\nand asks customers to not generate \n\ninfringing content, such as by \n\nproviding Copilot with inputs they \n\ndo not have the right to use. AI Governance in Practice Report 2024  | 65 \n\n# Part IX. \n\n# Third-party \n\n# AI assurance \n\nIn a recent  report  released by the U.K. government, assurance \n\nis defined as \"the process of measuring, evaluating and \n\ncommunicating something about a system or process, \n\ndocumentation, a product, or an organisation.\" Many of the \n\nAI governance implementation mechanisms discussed in this \n\nreport are forms of assurance. \n\nWhile establishing core competencies within an organization \n\nis beneficial to create strong AI-governance foundations \n\nacross the different  lines of defense , utilization of third-party \n\nAI assurance mechanisms may be an important or necessary \n\nconsideration depending on the type of AI used and the \n\norganization's knowledge and capacity. \n\nIntegrating third-party assurance into an AI-governance \n\nstrategy is a consideration at various stages of the life cycle. \n\n## AI assurance methods are crucial \n\n## for demonstrating accountability \n\n## and establishing trust. â†’ Part IX. Third-party AI assurance \n\nAI Governance in Practice Report 2024  | 66  \n\n> TABLE OF CONTENTS â†‘\n\nTypes of third-party assurance \n\nSome of the most practical tools for the realization \n\nof safe and responsible AI are emerging from \n\nthird-party AI assurance methods. \n\nAssessment \n\nAssessments are key mechanisms to evaluate \n\nvarious aspects of an AI system, including to \n\ndetermine the risk of a system or identify the \n\nsource of bias or determine the reason a system \n\nis making inaccurate predictions. Various \n\nservices and off-the-shelf products can be \n\nintegrated into AI governance practices based on \n\nwhat an organization is trying to determine from \n\nits assessment. \n\nCertain assessments must be conducted by the \n\nthird party providing the system to their customers, \n\nsuch as conformity assessments and impact \n\nassessments focusing on the impacts of the datasets \n\nused and the model itself. From a deployer's \n\nperspective, third-party due diligence enquiries \n\nshould be integrated into the organization's existing \n\nthird-party risk management program and include \n\nscreening at both the vendor enterprise and \n\nproduct levels. \n\nTesting and validation \n\nTesting techniques such as statistical tests to \n\nevaluate demographic fairness, assess system \n\nperformance or detect generative AI that may \n\nlead to copyright breaches are becoming widely \n\navailable through various third-party vendors. \n\nBefore choosing a vendor, it is important to have \n\na clear understanding of what the test is for \n\nand whether the context â€” which includes the \n\ntype of AI used, applicable jurisdictions and the \n\ndomain operating in â€” will impact the types of \n\ntests to run. \n\nConformity assessments \n\nConformity assessments are reviews completed \n\nby internal or external review functions to \n\nevaluate whether a product, system, process \n\nor individual adheres to an established set of \n\nrequirements. This is typically performed in \n\nadvance of a product or system being placed on \n\nthe market. While most assessments focus on \n\nevaluating aspects of AI systems, conformity \n\nassessments have been designed to evaluate \n\nquality-management systems , a set of processes \n\nfor those who build and deploy AI systems, and \n\nindividuals  who are involved in the development, \n\nmanagement or auditing of AI systems. \n\nFrom a deployer's perspective, the third-party \n\ndue diligence process should include vendor \n\ninquiries into product documentation, such as \n\ntechnical specifications, user guides, conformity \n\nassessments and impact assessments. \n\nRisk assessments should be done \n\nat several phases of development, \n\nstarting with the proposal/idea phase. \n\nIt's easier to incorporate some \n\nâ€˜responsible by design' features early \n\non, rather than tack them on at the \n\nend. For example, filtering for toxic \n\ncontent in your training data, before \n\na model is trained, can be more \n\neffective than trying to catch toxic \n\ngenerated content afterwards. \n\nIn contrast, a full impact assessment \n\nshould be done once a model is fully \n\ndeveloped and evaluated, because it's \n\nhard to assess the impact without a lot \n\nof information about the final system. \n\nAndrew Gamino-Cheong \n\nTrustible AI Co-founder and Chief Technology Officer â†’ Part IX. Third-party AI assurance \n\nAI Governance in Practice Report 2024  | 67  \n\n> TABLE OF CONTENTS â†‘\n\nImpact assessments \n\nThe risk profile of AI systems can vary widely \n\nbased on the technical capabilities and intended \n\npurposes of the system, as well as the particular \n\ncontext of their implementation. Evaluating and \n\nmitigating the impacts of an AI system is therefore \n\na shared responsibility that must be owned by \n\nproviders and deployers alike in practice. The \n\norganization deploying a third-party AI system will \n\nhave a closer understanding of the specific context \n\nand impacts of deploying the system. Similarly, \n\nthe third-party vendor is best placed to evaluate \n\nthe impacts of the training, testing and validation \n\ndatasets, the model and infrastructure used to \n\ndesign and develop the system. \n\nAI/algorithmic auditing \n\nWhile there is not yet a formal audit practice as \n\nseen in financial services, there is a growing call \n\nfor those who audit AI systems to demonstrate \n\na common set of competencies, such as with a \n\ncertification or formal designation. These audits \n\nmay incorporate other third-party mechanisms \n\ndiscussed above to evaluate AI systems and ensure \n\nthey are safe, secure, legally compliant and meet \n\nrequisite standards, among other things. The \n\nNational Telecommunications and Information \n\nAdministration released  recommendations  for \n\nfederal agencies to use audit and auditors for the \n\nuse of high-risk AI systems. \n\nCanada's proposed Bill C-27, the Digital Charter \n\nImplementation Act, identifies that the Minister \n\nof Innovation, Science and Industry can issue an \n\nindependent audit if they have reasonable grounds \n\nto believe requirements outlined in the act have \n\nnot been met. This may encourage organizations \n\nto ensure compliance via preventative third-\n\nparty audits. Additionally, Canada identified the \n\nimportance of international standards to help \n\nsupport the desired objectives of the act. \n\nCertifications \n\nCertifications are marks or declarations provided \n\nafter evaluations or audits are performed against \n\nstandards or conformity assessments . The mark \n\nindicates the AI system adheres to certain specified \n\nrequirements. It is important to note certifications \n\ncan also be provided to quality-management \n\nsystems used throughout the life cycle of an AI \n\nsystem or to individuals, demonstrating that they \n\nmet a set of competencies. \n\nOrganizations need a clear \n\nunderstanding of how AI risk will \n\naffect their business through \n\nthird-party relationships. \n\nThey should proactively review their \n\ninventory of vendors and identify \n\nthose that provide AI solutions or \n\ncomponents. They also need to be \n\naware of the development plans for \n\nall third-party products, including \n\nwhether, how, and when AI will be \n\nintegrated. With that understanding, \n\npartners, vendors and their products \n\nmay need to be reassessed to \n\naccount for AI risk with updated due \n\ndiligence processes. \n\nAmber Gosney \n\nFTI Technology Managing Director â†’ Part IX. Third-party AI assurance    \n\n> AI Governance in Practice Report 2024 |68\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## Algorithmic audits as airplane cockpits \n\nAt ORCAA, we use the analogy of an airplane cockpit \n\nto talk about algorithmic audits. In an airplane cockpit, \n\nthe dials and gauges take measurements that relate to \n\npossible failure modes. \n\nFor instance, the fuel gauge says if the plane is about to run out of gas, \n\nand the attitude indicator says if it is going to dive or roll. These dials \n\nhave 'redlines': threshold values that, if exceeded, mean the pilot needs to \n\nintervene. The auditor's job is to design a 'cockpit' for a given algorithmic \n\nsystem. This involves identifying failure modes -- how the system could \n\nresult in harm to various stakeholders -- and building 'dials' that measure \n\nconditions that lead to failures. At ORCAA, we have developed frameworks \n\nfor doing these critical tasks. \n\nSome other aspects of this analogy are worth noting. A cockpit identifies \n\nproblems but does not fix them. An indicator light will say an engine is out, \n\nbut it won't say how to repair or restart the engine. Likewise, an algorithmic \n\ncockpit should indicate when a failure is imminent, but it is the job of the \n\nsystem deployer, the 'pilot,' to intervene. A cockpit is a critical piece of \n\nairplane safety, but it's not the whole picture. Planes are tested extensively \n\nbefore being put into service, both during the design phase and when \n\nthey roll off the assembly line and are periodically taken out of service for \n\nregular inspections and maintenance. \n\nLikewise, algorithmic cockpits, which are critical \n\nfor safety while the system is deployed, should be \n\ncomplemented by predeployment testing and regular \n\ninspections and maintenance during deployment.\" \n\nCathy O'Neil \n\n> O'Neil Risk Consulting & Algorithmic Auditing CEO\n\nAI Governance in Practice Report 2024  | 69 \n\n# Conclusion \n\nOrganizations may seek to leverage existing organizational \n\nrisk frameworks to tackle AI risk at enterprise, product and \n\noperational levels. Tailoring their approach to AI governance \n\nto their specific AI product risks, business needs and \n\nbroader strategic objectives can help organizations establish \n\nthe building blocks of trustworthy and responsible AI. A key \n\ngoal of the AI governance program is to facilitate responsible \n\ninnovation. Flexibly adapting existing governance processes \n\ncan help businesses to move forward with exploring the \n\ndisruptive competitive opportunities that AI technologies \n\npresent, while minimizing associated financial, operational \n\nand reputational risks. \n\n## Bringing it all together and \n\n## putting it into action. Uzma Chaudhry \n\nIAPP AI Governance Center \n\nResearch Fellow \n\nuchaudhry@iapp.org \n\nJoe Jones \n\nIAPP Director of Research \n\nand Insights \n\njjones @iapp.org \n\nAshley Casovan \n\nIAPP AI Governance Center \n\nManaging Director \n\nacasovan@iapp.org \n\nLynsey Burke \n\nIAPP Research and Insights \n\nProject Specialist \n\nlburke@iapp.org \n\nNina Bryant \n\nFTI Technology Senior \n\nManaging Director \n\nnina.bryant@fticonsulting.com \n\nLuisa Resmerita \n\nFTI Technology \n\nSenior Director \n\nluisa.resmerita@fticonsulting.com \n\nMichael Spadea \n\nFTI Technology Senior \n\nManaging Director \n\nmicheal.spadea@fticonsulting.com \n\nFollow the IAPP on social media \n\nD C Q E\n\nPublished June 2024. \n\nIAPP disclaims all warranties, expressed or implied, with respect to \n\nthe contents of this document, including any warranties of accuracy, \n\nmerchantability, or fitness for a particular purpose. Nothing herein \n\nshould be construed as legal advice. \n\nÂ© 2024 IAPP. All rights reserved. \n\n# Contacts \n\n## Connect with the team", "fetched_at_utc": "2026-02-08T18:49:30Z", "sha256": "d10ba8f26b3a8a5f315538e4f40dcd3ba10b228963f59e2d2c3a180ca8b56e2b", "meta": {"file_name": "AI Governance in Practice Report 2024 - IAPP.pdf", "file_size": 39709902, "relative_path": "pdfs\\AI Governance in Practice Report 2024 - IAPP.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 29464}}}
{"doc_id": "pdf-pdfs-ai-risk-management-singapore-aa4ced6d9f7b", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Risk Management - Singapore.pdf", "title": "AI Risk Management - Singapore", "text": "ARTIFICIAL INTELLIGENCE \n\n# MODEL RISK \n\n# MAN AGEMENT \n\n# OBSERVATIONS FROM A THEMATIC REVIEW  \n\n> Information Paper\n> December 2024\n\nArtificial Intelligence Model Risk Management | 2\n\n## Contents \n\n1. Overview 3\n\n2. Background 3\n\n3. Objectives and Key Focus Areas 7\n\n4. Governance and Oversight 10 \n\n5. Key Risk Management Systems and Processes 12 \n\n5.1  Identification 12 \n\n5.2 Inventory 13 \n\n5.3 Risk Materiality Assessment 15 \n\n6 Development and Deployment 16 \n\n6.1 Standards and Processes 16 \n\n6.2 Data Management 18 \n\n6.3 Development 21 \n\n6.4 Validation 30 \n\n6.5 Deployment, Monitoring and Change Management 31 \n\n7 Other Key Areas 36 \n\n7.1 Generative AI 36 \n\n7.2 Third -Party AI 43 \n\n8 Conclusion 44 \n\nAnnex A - Definitions 46 \n\nAnnex B - Useful References 49 Artificial Intelligence Model Risk Management | 3\n\n# 1. Overview \n\n1.1  This information paper sets out good practices relating to Artificial Intelligence \n\n(AI) (including Generative AI)  1 model risk management (MRM)  2 that were \n\nobserved during a recent thematic review of selected banks . The information \n\npaper focuses on the following key areas 3 : AI governance and oversight ; AI \n\nidentification, inventorisation and risk materiality assessment; as well as AI \n\ndevelopment, validation, deployment, monitoring and change management. \n\n1.2  While the thematic review focused on selected banks, the good practices \n\nhighlighted in this information paper should generally appl y to other financial \n\ninstitutions (FIs), which should take reference from these when developing and \n\ndeploying AI. \n\n# 2. Background \n\nIndustry use of AI and Generative AI and associated risks \n\n2.1  The launch of ChatGPT in November 2022 and recent advancements in AI, \n\nparticularly Generative AI, has led to an increased interest in leveraging AI and \n\nGenerative AI in the banking and broader financial sector. Prior to these \n\ndevelopments, FIs have used AI in a wide range of areas and use cases. Key areas \n\nwhere we observed significant use of AI by banks during the thematic review \n\ninclude risk management, customer engagement and servicing , as well as to  \n\n> 1\n\nGenerative AI is a subset of AI , and a n AI or Generative AI system can comprise one or more AI or Generative AI models and \n\nother machine -based components . For the purposes of this paper, AI generally refers to both AI and Generative AI models and \n\nsystems . Where a point pertains specifically to an AI model or an AI system, or to Generative AI , we will use the respective \n\nterm s explicitly in the paper. We define the terms AI and Generative AI, as well as AI model, system and use case in greater \n\ndetail in Annex A.  \n\n> 2\n\nIn line with the footnote above and r ecognising that the AI MRM is intrinsically linked to the risk management of systems in \n\nwhich AI models are used, when we refer to AI MRM or AI risk management in this paper, it generally refers to the risk \n\nmanagement of AI models and systems.  \n\n> 3\n\nThe aim of this information paper is not to cover all aspects of model risk management, but to focus on good practices in \n\nareas that are more relevant to AI MRM .Artificial Intelligence Model Risk Management | 4\n\nsupport internal operational processes . For example, we have seen banks use AI, \n\nparticularly decision tree -based machine learning (ML) models such as XGBoost, \n\nLightGBM and CatBoost  4 , in financial risk management to detect abnormal \n\nfinancial market movements, or to estimate loan prepayment rates. They are also \n\ncommonly used in anti -money laundering (AML) systems to detect suspicious \n\ntransactions , and in fraud detection systems . In customer engagement and \n\nservicing, banks use AI to predict customer preferences, personalise financial \n\nproduct recommendations and manage customer feedback. AI is also widely \n\nused to support internal operational processes across a wide range of business \n\nfunctions, for example, to automate checking and verification processes (e.g. , for \n\ncustomer information ), prioritise incident management (e.g. , triaging IT incidents \n\nfor attention ), or forecast demand for services (e.g. , ATM cash withdrawals) .\n\n2.2  While the use of AI in these areas can enhance operational efficiency , facilitate \n\nrisk management and enhance financial services , they can also increase risk \n\nexposure if not developed or deployed responsibly . Po tential risks include: \n\nâ€¢ Financial risks , e.g.,  poor accuracy of AI used for risk management could  lead \n\nto poor risk assessments and consequent financial losses. \n\nâ€¢ Operational risks , e.g., unexpected behaviour of  AI used to automate financial \n\noperations could lead to  operational disruptions or errors in critical processes. \n\nâ€¢ Regulatory risks , e.g., poor performance of AI used to support AML efforts \n\ncould lead to non -compliance with regulations. \n\nâ€¢ Reputational risk s, e.g., wrong or inappropriate information from AI -based \n\ncustomer -facing systems , such as chatbots , could lead to customer complaints \n\nand negative media attention, and consequent reputational damage.                 \n\n> 4Decision tree -based ML models make predictions based on a tree -like structure learnt from data. Models such as X GBoost,\n> LightGBM and CatBoost utilise a series of decision trees together with a boosting technique . Each decision tree in the series\n> focuses on the errors made by aprior decision tree to improve predictions .Such models are also explainable as the relative\n> importance of different features to model predictions can be extracted.\n\nArtificial Intelligence Model Risk Management | 5\n\n2.3  While natural language processing (NLP) 5 and computer vision (CV) 6 techniques \n\nwere already in use in the financial sector prior to the emergence of Generative \n\nAI 7 for text or image -related tasks, recent Generative AI models such as OpenAIâ€™s \n\nGPT 8 large language models (LLM s) and D ALL -E9 image generation models , or \n\nAnthropicâ€™s Claude LLM s10  offer better performance in tasks such as cluster ing \n\ndocuments . They have also enable d new use cases , e.g. , to generat e text content \n\nand image s for marketing , or to process multimodal data 11  for financial analysis .\n\n2.4  Based on the thematic review, use of Generative AI in banks appear s to still be at \n\nan early stage . The current focus is on the use of Generative AI to assist or \n\naugment humans for productivity enhancements , and not in applying Generative \n\nAI to direct customer facing applications . Use cases being explored by banks \n\ninclude risk management (e.g., detecting emerging risks in text information );\n\ncustomer engagement and service (e.g., summarising customer interactions or \n\ngenerating marketing content ); and research and reporting (e.g., investment \n\nanalyses ). Banks are also exploring the use of Generative AI in copilots 12  to \n\nsupport staff, for example , in coding, or for general text -related tasks such as \n\nsummarisation and answering queries based on information in internal \n\nknowledge repositories. \n\n2.5  With Generative AI, existing risks associated with AI may be amplified 13 . For \n\nexample, Generative AI's potential for hallucinations and unpredictable  \n\n> 5\n\nNatural language processing (NLP) is commonly used to refer to techniques that process , analyse , make predictions or \n\ngenerate outputs relating to human language, both in its written and spoken forms.  \n\n> 6\n\nComputer vision (CV) is commonly used to refer to techniques that enable machines to process and generate outputs based \n\non visual information from the world.  \n\n> 7\n\nFor example, for news sentiment analysis, information extraction, clustering documents based on underlying topics, or \n\ndigitising physical documents . \n\n> 8\n\nGenerative Pre -trained Transformers (GPT) are a family of Generative AI models developed by OpenAI , that include s models \n\nsuch GPT 4 and GPT -4o.  \n\n> 9\n\nDALL -E is a Generative AI model that generates images from text prompts or descriptions.  \n\n> 10\n\nClaude models are a family of Generative AI models developed by Anthropic and include models such as Claude 3.5 Haiku \n\nand Sonnet.  \n\n> 11\n\nMultimodal data refers to datasets that comprise multiple types of data, e.g., text, images, audio or video.  \n\n> 12\n\nIn the context of Generative AI, the term copilot is typically used to refer to Generative AI being used to assist or augment \n\nhumans on specific tasks . \n\n> 13\n\nMore details on risks associated with Generative AI have already been covered extensively in Project Mind Forge â€™s white \n\npaper on â€œEmerging Risks and Opportunities of Generative AI for Banks â€ and will not be repeated in this information paper. \n\nThe whitepaper can be accessed at  https://www.mas.gov.sg/schemes -and -initiatives/project -mindforge .Artificial Intelligence Model Risk Management | 6\n\nbehaviours may pose significant risks  if Genera tive AI is used in mission -critical \n\nareas. The complexity of Gen erative AI models and lack of established \n\nexplainability techniques also create s challenges for understanding and \n\nexplaining decisions , while the d iverse and often opaque data sources used in \n\nGen erative AI training, coupled with difficulties in evaluating bias of Generative \n\nAI outputs , could lead to unfair decisions .\n\nMASâ€™ Efforts on Responsible AI for the Financial Sector \n\n2.6  Alongside the growing use of AI in the financial sector and such associated risks ,\n\nMAS ha d established key principles to guide financial institutions in their \n\nresponsible use of AI. \n\n2.7  In 2018, MAS co -created the principles of Fairness, Ethics, Accountability and \n\nTransparency (FEAT) with the financial industry to promote the deployment of AI \n\nand data analytics in a responsible manner. To provide guidance to FIs in \n\nimplementing FEAT, MAS started working with an industry consortium on the \n\nVeritas Initiative 14  in November 2019. The Veritas Initiative aimed to support FIs \n\nin incorporating the FEAT Principles into their AI and data analytics solutions, and \n\nhas released assessment methodologies, a toolkit , and accompanying case \n\nstudies. \n\n2.8  With the emergence of Generative AI, Project Mind Forge 15 , which is also driven \n\nby the Veritas Initiative, was established to examine the risk s and opportunities \n\nof Generative AI. The first phase of Project Mind Forge was supported by a \n\nconsortium of banks and released a risk framework for Generative AI in \n\nNovember 2023. \n\n2.9  More recently, MAS released an information paper relating to Generative AI risks \n\nin July 2024 16 . The paper provides an overview of key cyber threats arising from \n\nGenerative AI, the risk implications, and mitigation measures that FIs could take         \n\n> 14 See https://www.mas.gov.sg/schemes -and -initiatives/veritas\n> 15 See https://www.mas.gov.sg/schemes -and -initiatives/project -mindforge\n> 16 See https://www.mas.gov.sg/regulation/circulars/cyber -risks -associated -with -generative -artificial -intelligence Artificial Intelligence Model Risk Management |7\n\nto address such risks. The paper covered areas enabled by Generative AI, such as \n\ndeepfakes, phishing and malware, as well as threats to deployed Generative AI, \n\nsuch as data leakage and model manipulation. \n\n# 3. Objectives and Key Focus Area s\n\n3.1  This information paper , which focus es on AI MRM, is part of MASâ€™ incremental \n\nefforts to ensure responsible use of AI in the financial sector . A key difference \n\nbetween an AI -based system and other system s is the use of one or more AI \n\nmodels within the system, which potentially increases uncertainties in outcomes .\n\nRobust MRM of such AI models is important to support the responsible use of AI .\n\n3.2  As the maturity of AI MRM may vary significantly across different FIs, MAS \n\nconducted a thematic review of selected banksâ€™ AI MRM practices in mid -2024 .\n\nThe objective was to gather good practices for sharing across the industry .\n\n3.3  Based on information gathered during the review, MAS observed good practices \n\nby banks in the se key focus areas 17 :\n\nâ€¢ Section 4: Oversight and Governance of AI \n\n- Updating of existing policies and procedures of relevant risk management \n\nfunctions to strengthen AI governance ;\n\n- Establish ing cross -functional oversight forums to ensure that evolving AI \n\nrisks are appropriately managed across the bank; \n\n- Articulat ing clear statements and principles to govern areas such as the \n\nfair, ethical, accountable and transparent use of AI ; and           \n\n> 17 For the purposes of the subsequent parts of this information paper, the good practices relating to AI would also apply to\n> Generative AI as practicable. Specific considerations relating to Generative AI will be covered in Section 7.1 .Artificial Intelligence Model Risk Management |8\n\n- Building capabilities in AI across the bank to support both innovation and \n\nrisk managemen t. \n\nâ€¢ Section 5: Key Risk Management Systems and Processes \n\n- Identif ying AI usage and risks across the bank so that commensurate risk \n\nmanagement can be applied ;\n\n- Utilis ing AI inventories, which provide a central view of AI usage across \n\nthe bank to support oversight ; and \n\n- Assessing the materiality of risks that AI poses using key risk dimensions so \n\nthat relevant controls can be applied proportionately .\n\nâ€¢ Section 6: Development and Deployment of AI \n\n- Establishing standards and processes for k ey areas that are important for \n\nthe d evelopment of AI , such as data management, robustness and \n\nstability, explainability and fairness , reproducibility and auditability ;\n\n- Conducting independent validation or peer review s 18  of AI before \n\ndeployment based on risk materialities ; and \n\n- Instituting p re -deployment checks and monitoring of deployed AI to \n\nensure that it behave s as intended, and application of appropriate change \n\nmanagement standards and processes where necessary .       \n\n> 18 The terms validation sand reviews are usually used interchangeably by banks to refer to assessments or checks of the AI\n> model development process, whether by an independent party, or another peer developer. More details on such validations\n> and reviews are provided in Section 6. 4.Artificial Intelligence Model Risk Management |9\n> Overview of Key Thematic Focus Areas\n\n3.4  These key focus areas are generally also applicable to Generative AI, as well as AI \n\n(including Generative AI ) from third -party providers. Nonetheless, there may be \n\nadditional considerations for Generative AI , as well as AI from third -party \n\nproviders. Hence, additional observations on good practices relating to \n\nGenerative AI and third -party AI are also outlined in Sections 7 .1 and 7.2 of this \n\ninformation paper respectively .\n\n3.5  The risks posed by AI and Generative AI extend beyond MRM and relate to non -\n\nAI specific areas such as general data governance and management, technology \n\nand cyber risk management, as well as third party risk management . These are \n\nnot covered in this information paper, and existing regulatory requirements and \n\nsupervisory expectations , including but not limited to notice s, guidelines or \n\ninformation papers on data governance, technology and outsourcing risk \n\nmanagement would apply , where relevant 19 .      \n\n> 19 Links to relevant publications are provided in Annex B.Artificial Intelligence Model Risk Management |10\n\n# 4. Governance and Oversight \n\nOve rview \n\nWhile existing control functions continue to play key roles in AI risk management , most \n\nbanks have update d governance structures, roles and responsibilities, as well as policies \n\nand processes to address AI risks and keep pace with AI developments . Good practices \n\ninclude :\n\nâ€¢ establishing cross -functional oversight forums to avoid gaps in AI risk \n\nmanagement ;\n\nâ€¢ updating control standards, policies and procedures, and clearly set ting out roles \n\nand responsibilities to address AI risks ;\n\nâ€¢ developing clear statements and guidelines to govern areas such as fair, ethical, \n\naccountable and transparent use of AI across the bank ; and \n\nâ€¢ building capabilities in AI across the bank to support both innovation and risk \n\nmanagement .\n\nExisting governance structures and such good practices are important to help support \n\nBoard and Senior Management in exercis ing oversight over the bankâ€™s use of AI , and \n\nensure that the bankâ€™s risk managemen t is robust and commensurate with its state of \n\nuse of AI .\n\n4.1 While existing risk governance framework s and structure s 20  continue to be \n\nrelevant for AI governance and risk management , a number of banks have \n\nestablish ed cross -functional AI oversight forums . Such forums serve as key \n\nplatform s for coordinating governance and oversight of AI usage across various \n\nfunctions . They also play an important role in addressing emerging challenges \n\nand potential gaps in risk management as the AI landscape evolves , and ensuring \n\nthat standards and processes, such as relevant AI development and deployment \n\nstandards , are aligned across the bank.                \n\n> 20 Aside from MRM, risk governance frameworks and structures from other areas that are usually relevant to AI risk\n> management include (but are not limited to )data, technology and cyber ,third -party risk management, as well as legal and\n> compliance .Artificial Intelligence Model Risk Management |11\n\n4.2 The mandates of these forums often include establishing a consistent and \n\ncomprehensive framework for managing AI risks, evaluating use cases that \n\nrequire broader cross -functional inputs, and reviewing AI governance \n\nrequirements to ensure they keep pace with the state of AI usage in the bank. \n\nData and analytics, r isk management, legal and compliance, technology, audit, as \n\nwell as other relevant busines s and corporate functions , are typically represented \n\nat such cross -functional oversight forums .\n\n4. 3 A number of banks have also found value in compiling policies and procedures \n\nthat are relevant to AI into a central guide to ensure that consistent standards \n\nfor AI are applied across the bank.  As more AI use cases are rolled out in banks, \n\nand the state of AI technology evolves, the use of AI may accentuate existing risks \n\nor introduce new risks. Hence, most banks have  reviewed and , where necessary, \n\nupdated existing policies and procedures to keep pace with the increasing use of \n\nAI across the bank , or new AI developments, e.g., updat ing policies and \n\nprocedures relating to performance testing of AI for new use cases , or \n\nestablishing new policies and procedures for AI models that are dynamically \n\nupdated based on new data .\n\n4. 4 Given the broad range of use cases for AI, and the potential for inappropriate \n\nuse, most banks have set out central statements and principles on how they \n\nintend to use AI responsibly, including developing guidelines to govern areas such \n\nas fair, ethical, accountable, and transparent use of AI  21  . Such efforts are \n\nimportant in setting the tone and establishing clear guidance on how AI should \n\nbe used appropriately across the bank, and to prevent potential harms to \n\nconsumers and other stakeholders arising from the use of AI . In addition to \n\ncentral statement s and principles , some banks have also taken steps to \n\noperationalise such central statement s and principles by mapping them to key                                    \n\n> 21 More details on these areas can be found in MAS â€™publications relating to the FEAT principles under the Veritas Initiative .\n> Similar principles covering areas relating to the responsible or ethical use of AI in the financial sector have also been published\n> in other jurisdictions ,e.g. ,the Hong Kong Monetary Authority (HKMA) issued guiding principles for the use of big data analytics\n> and AI covering governance and accountability, fairness, transparency and disclosure, and data privacy and protection in 2019;\n> De Nederlandsche Bank (DNB) issued the SAFEST principles on s oundness, accountability, fairness, ethics, skills, and\n> transparency in 2019. Artificial Intelligence Model Risk Management |12\n\ncontrols , which are in turn mapped to the relevant functions responsible for \n\nthese controls .\n\n4. 5 Given the growing interest in AI, b anks also recognised the need to develop AI \n\ncapabilities and have established plans to upskill both their staff and senior \n\nexecutives . Aside from building awareness, banks have developed AI training that \n\nfacilitate staff in leveraging an d using AI in a n effective and responsible manner. \n\nSome banks have also set up AI Centre s of Excellence to drive innovation ,\n\npromote best practices and build AI capabilities across the bank .\n\n# 5. Key Risk Management Systems and \n\n# Processes \n\nOverview \n\nMost banks have recognised the need to establish or updat e key risk management \n\nsystems and processes for AI , particularly in the following areas: \n\nâ€¢ policies and procedures for identifying AI usage and risks across the bank , so that \n\ncommensurate risk management can be applied ;\n\nâ€¢ systems and processes to ensure the completeness of AI inventories, which \n\ncapture the approved scope of use and provide a central view of AI usage to \n\nsupport oversight ; and \n\nâ€¢ assessment of the risk materiality of AI that cover s key risk dimensions, such as \n\nAIâ€™s impact on the bank and stakeholders , the complexity of AI used , and the \n\nbankâ€™s reliance on AI , so that relevant controls can be applied proportionately. \n\n5.1  Identification \n\n5.1.1  Identif ying where AI is used is important so that the relevant governance and risk \n\nmanagement controls can be applied . Even when using widely accepted \n\ndefinition s, such as the Organisation for Economic Co -operation and Artificial Intelligence Model Risk Management | 13 \n\nDevelopmentâ€™s  definition of AI 22 , considerable ambiguity remain s around the \n\ndefinition of AI due to its broad and evolving scope .\n\n5.1.2  Most banks leverage d definitions in existing MRM policies and procedures as a\n\nfoundation for identifying AI models 23 , and extended or adapted these definitions \n\nto account for AI -specific characteristics . Some banks shared that the uncertainty \n\nof model outputs is a common source of risk for both AI and conventional \n\nmodels 24 , and that the presence of such uncertainties was a key feature that was \n\nusually considered when identifying AI. MRM control functions also typically play \n\na key role in AI identification , often serving as the key control function \n\nresponsible for AI identification systems and processes , e.g., setting up \n\nattestation processes, or acting as the final arbiter in determin ing whether AI is \n\nbeing used . Some banks have also developed tools or portals to facilitate the \n\nprocess of identifying and classifying AI across the bank in a consistent manner. \n\n5.2 Inventory \n\n5.2.1  Banks mostly maintain a formal AI inventory 25  with a comprehensive record of \n\nwhere AI is used in the bank . A key area that an AI inventory supports, alongside \n\nthe relevant policies, procedures and systems, is to ensure that AI are only used \n\nwithin the scope in which they have been approved for use, e.g. , the purpose ,\n\njurisdiction, use case, application, system, and other conditions for which they \n\nhave been developed, validated and deployed. This is critical because \n\nunapproved usage of AI, particularly in higher -risk use cases, can lead to  \n\n> 22\n\nThe OECDâ€™s definition of AI: An AI system is a machine -based system that, for explicit or implicit objectives, infers, from the \n\ninput it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence \n\nphysic al or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.  \n\n> 23\n\nThis could entail step -by -step guide to facilitate the identification of techniques that meet the bankâ€™s definition of AI.  \n\n> 24\n\nModel s usually refer to quantitative algorithms, methods or techniques that process input data into quantitative estimates \n\nwhich may be used for analysis or decision making. Apart from AI models, which typically refer to machine or deep learning \n\nmodels, banks also routinely utilise conventional models, such as economic, financial, or statistical models. Some quantitative \n\nalgorithms, methods or techniques , such as logistic regressions , are commonly regarded as both AI and statistical models. A\n\nmore detailed definition of models can be found in Annex A.  \n\n> 25\n\nMost banks have established software system s for their AI inventor ies that not only record where AI is used in the bank, but \n\nmay also include additional features outlined above, such as automated tracking of approvals and issues , and identification of \n\ninter -dependences between AI . A small number of banks still rely on spreadsheets for their AI inventories, but this approach is \n\nmore prone to operational issues, e.g., outdated records, and would not allow for the additional features outlined above. Artificial Intelligence Model Risk Management | 14 \n\nunintended consequences. For example,  AI approved for use in one jurisdiction \n\nshould not automatically be treated as approved for use in others as the data, \n\nassumptions and considerations may not be similar , and the AI may not perform \n\nas expected in a different context .\n\n5.2.2  A few banks also utilise d their AI inventory system to track the use of AI through \n\ntheir lifecycle, and to establish checkpoints for different risk management \n\nprocesses at the various stages of the AI lifecycle. A few banks also use d the AI \n\ninventory to support the identification and monitoring of aggregate AI risks and \n\ninterdependencies across different AI models and systems . The AI inventory may \n\nalso serv e as a central repository for AI artifacts needed for model maintenance, \n\nvalidation and incident or issue management .\n\n5.2.3  Most banks have established clear policies on the scope of AI assets to be \n\ninventoried, the roles responsible for maintaining the inventory, and the \n\nprocesses for updating it. AI models are typically included within regular model \n\ninventories but specific tags or fields added to identify AI and capture AI -relevant \n\nattributes. One bank built an AI use case inventory that aggregate d information \n\nfrom the AI model inventory and other inventories or repositories relating to \n\nassets and controls in areas such as data, technology and operational \n\nmanagement . This provided the bank with a comprehensive and clear view of the \n\nlinkages between AI models and other relevant assets and controls. \n\n5.2.4  Across banks, AI inventories generally capture key attributes such as the AIâ€™s \n\npurpose and description , scope of use, jurisdiction, model type, model output 26 ,\n\nupstream and downstream dependencies , model status, risk materiality rating, \n\napprovals obtained for validation and deployment , responsible AI requirements ,\n\nwaiver or dispensation details 27 , use of personally identifiable information (PII) 28 ,\n\npersonnel responsible such as owner s, sponsor s, users, developer s, and \n\nvalidator s. For third -party AI , additional attributes such as the AI provider, model              \n\n> 26 Model output refers to the type of output generated by the AI model. For example, the model output attribute could be the\n> likelihood of customer attrition, or the credit score of a customer.\n> 27 Waiver or d ispensation details refer to information about exceptions/special permissions granted, regarding the\n> development or deployment of AI, that deviate from the bank's standard policies and procedures .\n> 28 For example, full name, national identification number, personal mobile number. Artificial Intelligence Model Risk Management |15\n\nversion, endpoints utilised , as well as other details from the AI developers 29  may \n\nalso be included. \n\n5.3 Risk Materiality Assessment \n\n5.3.1.  Risk materiality assessment s are critical for banks to calibrate their approach to \n\nrisk management of AI across the diverse areas in which AI can be used (e.g., to \n\nmap the risk materiality of AI to the depth and scope of validation and monitoring \n\nrequired ). In assessing risk materiality , most banks considered both quantitative \n\nand qualitative risk dimensions that could generally be grouped into three broad \n\ncategories :\n\na.  Impact on the bank , its customers or other stakeholders , including but not \n\nlimited to financial, operational, regulatory and reputational impact . A few \n\nbanks developed granular, function -specific definitions of impact to provide \n\ngreater clarity .\n\nb.  Complexity due to the nature of the AI model or system, or the novelty of the \n\narea or use case in which AI is being applied .\n\nc.  Reliance on AI, which takes into account the autonomy granted to the AI, or \n\nthe involvement of human s in the loop as risk mitigant s.\n\n5.3. 2 Most banks have also established processes to review that risk materialit ies \n\nassigned to AI remain appropriate over time. Similarly, quantitative and \n\nqualitative measures and methods use d to assign risk materialities were also \n\nreviewed , e.g., measures used to quant ify financial impact would be updated if \n\nthe nature of the business in which AI was used had evolved.             \n\n> 29 These may be provided in AI or AI m odel cards , which are documents or information usually released alongside open -source\n> AI models that facilitate transparency and accountability by providing essential information on key areas such as the AI modelâ€™s\n> purpose, performance, limitations, ethical considerations .More information on details that may be included in such cards are\n> available in papers such as https://link.springer.com/chapter/10.1007/978 -3-031 -68024 -3_3 .Artificial Intelligence Model Risk Management |16\n\n# 6 Development and Deployment \n\nOverview \n\nMost banks have established standards and processes for development, validation, and \n\ndeployment of AI to address key risks. \n\nâ€¢ For development of AI , key a reas that banks paid greater attention to include data \n\nmanagement, model selection, robustness and stability, explainability and \n\nfairness , as well as reproducibility and auditability .\n\nâ€¢ For validation , banks required independent validation s or review s of AI of higher \n\nrisk materiality prior to deployment , to ensure that development and deployment \n\nstandards have been adhered to. For AI of lower risk materiality, most banks \n\nconducted peer reviews that are calibrated to the risks posed by the use of AI \n\nprior to deployment .\n\nâ€¢ To ensure that AI would behave as intended when deployed and that any data \n\nand model drifts are detected and addressed, banks performed pre -deployment \n\nchecks, closely monitor ed deployed AI based on appropriate metrics , and appl ied \n\nappropriate change management standards and processes. \n\n6.1 Standards and Processes \n\n6.1.1.  To support robust risk management of AI across its lifecycle, banks have \n\nestablished standards and processes in the key areas of development, validation, \n\ndeployment, monitoring and change management . Most banks built upon \n\nexisting MRM standards and processes for development, validation, deployment, \n\nmonitoring and change management , but updated these standards and \n\nprocesses to address risks posed by AI. \n\n6.1.2.  Key standards and processes relating to conventional model development ,\n\nvalidation , deployment , monitoring and change management that banks Artificial Intelligence Model Risk Management | 17 \n\ngenerally  regard as relevant to AI are listed below 30 . Observations on key areas \n\nof focus for AI, and how banks have adapted or updated these standards and \n\nprocesses in these areas to address AI risks will be outlined in the subsequent \n\nsections. \n\na.  Data management - Determining suitability of data, such as the \n\nrepresentativeness of data for the intended objective, assessment of \n\ncompleteness , reliability , quality, and relevance of data, and approaches for \n\ndetermining train ing and test ing datasets. \n\nb.  Model selection - Defining the intended objective of the model and justifying \n\nhow the selection and design of the model is relevant and appropriate for \n\nachieving the desired objective , including the selection of architectures 31  and \n\ntechniques 32  that are appropriate for the use case and objecti ve .\n\nc.  Performance evaluation - Setting appropriate evaluation approaches and \n\nthresholds, and assessing the modelâ€™s ability to perform under a range of \n\nconditions in accordance with its intended usage and objective .\n\nd.  Documentation - Providing sufficient detail to facilitate reproducibility by an \n\nindependent party, including details on data sources , lineage , and processing \n\nsteps ; model architecture and techniques ; evaluation and testing approaches \n\nand results.  \n\n> 30\n\nAs highlighted previously, even prior to the use of AI models, banks already utilise d conventional models, such as economic, \n\nfinancial, or statistical models , and would have instituted model risk management standards and processes for such models .\n\nWhile these standards and processes may have precede d the use of AI model s in the bank , their general principles and \n\nconsiderations may also be applicable to AI models.  \n\n> 31\n\nModel architecture, in the context of AI, relates to the underlying structure and design of the model . It could involve choosing \n\nbetween decision tree -based models such as XGBoost, which were previously described in Section 2, or neural network -based \n\nmodels such as recurrent neural network or transformer models , based on various considerations. For example, d ecision tree -\n\nbased models may be more suitable for structured data, such as tabular data, while recurrent neural network or transformer \n\nmodels may be more suitable for text or time -series data as they are designed for sequential data.  \n\n> 32\n\nTechniques may include methods that are used to train a model from the data . In the context of AI, these may include \n\nsupervised learning techniques that use labelled data during training to learn how to generate predictions , or unsupervised \n\nlearning techniques which learn general patterns from unlabelled data . For more details on supervised and unsupervised \n\nlearning , please refer to Annex A. Artificial Intelligence Model Risk Management | 18 \n\ne.  Validation - Setting out the d epth of review expected of validators across the \n\nareas above ; frameworks for determining the prioritisation and frequency of \n\nvalidation (including any revalidation conducted on deployed models ).\n\nf.  Mitigating model limitations - Frameworks and processes for testing key \n\nassumptions, identifying limitations and their expected impact, and \n\nestablishing appropriate mitigants which are commensurate with the impact \n\nof the limitations. \n\ng.  Monitoring and change management - Setting appropriate tests and \n\nthresholds to evaluate the ongoing performance of a deployed model, \n\nincluding the frequency of monitoring; as well as the processes to be followed \n\n(e.g. , additional validations and approvals) for changes made to a deployed \n\nmodel. \n\n6.1.3.  When implementing standards and processes for risk management of AI , most \n\nbanks establish ed baseline standards and processes that applied to all AI across \n\nthe bank , regardless of risk materiality. For AI that were of greater risk \n\nmateriality, or where there were requirements specific to the use case , baseline \n\nstandards and processes would be supplemented by enhanced standards and \n\nprocesses . For example, additional evaluation or enhanced validation standards \n\nand processes could apply to AI used for risk and regulatory use cases where \n\nthere may be heightened requirements on performance evaluation or \n\nthresholds . The alignment of baseline standards and processes across the bank \n\nhelped ensure that key model risks were addressed consistently for AI with \n\nsimilar characteristics and risks regardless of where they were used in the bank. \n\n6.2 Data Management \n\n6.2.1  Robust data management is essential to support the development and \n\ndeployment of AI . G eneral bank -wide data governance and management Artificial Intelligence Model Risk Management | 19 \n\nstandards and processes 33  would apply to data used for AI. F or example, whether \n\ndata was used for reporting purposes or for AI systems , the same data \n\ngovernance committees generally oversee approvals and management of data \n\nissues. Similarly,  standards and processes for key data management controls such \n\nas basic data quality checks would also apply . However , to address AI -specific \n\nrequirements, all banks had established additional data management standards \n\nand processes to ensure that data used for AI development and deployment are \n\nfit for purpose . An overview of key data management areas for AI development \n\nand deployment that most banks generally focus ed on are listed below. \n\nStandards or processes relating to data management that are specific to AI \n\ndevelopment, validation, deployment, monitoring or change management are \n\ncovered in the subsequent sections. \n\na.  Appropriateness of data for AI use cases - Ensuring data used for \n\ndevelopment and deployment of AI are suitable for the context in which the \n\nAI is used , including assessing the use of such data against fairness and ethical \n\nconsideration s.\n\nb.  Representativeness of data for development - Ensuring data selected for \n\ntraining and testing AI models are representative of the real -world conditions ,\n\nincluding stressed conditions, under which the AI would be used .\n\nc.  Robust data engineering during development - Ensuring data processing \n\nsteps , 34  which may include additional data quality checks  35  , feature                               \n\n> 33 Please see MASâ€™ information paper on Data Governance and Management Practices for more details on general data\n> governance and management standards and processes . The paper covered governance and oversight, data management\n> function, data quality and data issue smanagement, which would also apply to data used for AI. Other relevant regulations and\n> publications include t he Personal Data Protection Act (PDPA) , which comprises various requirements on data privacy governing\n> the collection, use, disclosure and care of personal data, and provides a baseline standard of protection for personal data in\n> Singapore ;and Advisory Guidelines on Use of Personal Data in AI Recommendation and Decision Systems issued by the\n> Personal Data Protection Commission (PDPC) in March 2024 . Please refer to Annex B for the relevant links.\n> 34 Examples of data processing steps include missing value imputation, replacement of outlier values and standardi sation or\n> normalisation of data values .\n> 35 To ensure data quality, key areas such as data relevance, accuracy, completeness and recency may be assessed.\n\nArtificial Intelligence Model Risk Management | 20 \n\nengineering 36 , augmentation and labelling 37  of datasets , are robust and free \n\nof b ias , and that the integrity and lineage of data are che cked and tracked \n\nacross these data engineering steps .\n\nd.  Robust data pipelines for deployment - Establishing r obust controls around \n\ndata pipelines for deployment , includ ing continuous monitoring of the quality \n\nof data passed to deployed AI , as well as checks for anomalies, drift s, and \n\npotential bias that may have an impact on performance or fairness .\n\ne.  Documentation of data -related aspects for reproducibility and auditability -\n\nEnsuring key data management steps, such as data sourc ing , data selection, \n\ndata lineage, data processing , approvals and remediation actions taken for \n\ndata issues are documented to enable reproducibility and auditability. \n\n6.2.2  Some banks have also established additional data management standards and \n\nprocesses in the areas below: \n\na.  To ensure that data is being used appropriately when developing or deploying \n\nAI, a few banks have required approvals to be obtained for high -risk data use \n\ncases , such as data use where a third party may have access to the bankâ€™s \n\ninternal data , use of employee data for monitoring, or the collection of \n\nbiometric data to identify individuals .                                     \n\n> 36 Features refer to the attributes of data points in a dataset, e.g., for data relating to a loan, the income of the obligor and\n> outstanding value of the loan are two possible attributes or features. Feature engineering refers to the process of selecting,\n> modifying or creating new features from the original attributes of a data set to improve a n AI modelâ€™s performance , e.g.,\n> normalising income of the obligor and outstanding value of the loan to a common scale ranging from 0 to 1; or creating new\n> derived features, such as a debt -to -income ratio , from existing attributes .\n> 37 When training AI models for a specific task, such as predicting a credit default or recommending a suitable financial product\n> to a customer ,we need data that includes the input variables (e.g. ,data relating to a past loan ,or customer history ), as well\n> as a target variable (e.g. ,whether there was a credit default for the loan, or a recommendation that the customer accepted ).\n> Data labelling refers to the process of assigning such target variables, typically based on past historical data or via human\n> annotation.\n\nArtificial Intelligence Model Risk Management | 21 \n\nb.  To support data reusability and reduce the time needed for feature \n\nengineering across the bank, as well as enhance consistency and accuracy in \n\nmodel development , a few banks have also built feature marts 38 .\n\nc.  To account for the greater use of unstructured data 39 , there were also ongoing \n\nefforts to more effectively manage such unstructured data , such as improving \n\nmetadata management and tagging for unstructured data to enable better \n\ndata governance 40 . Most of the data management areas outlined in paragraph \n\n6.2.1 are also generally appli cable to unstructured data , where relevant .\n\n6.3  Development \n\nModel Selection \n\n6.3.1  Given the trade -offs of adopting more complex AI models (e.g., higher \n\nuncertainties, limited explainability), most banks required developers to justify \n\ntheir selection of a more complex AI model over a conventional model or a\n\nsimpler AI model  41  , (e.g., balancing the need for performance against \n\nexplainability for a specific use -case ). Some banks required developers to go \n\nbeyond qualitative justifications, and develop challenger models (which could be \n\neither conventional or simpler AI models) to explicitly demonstrate the \n\nperformance uplift of the AI model over the challenger model as part of this \n\njustification. \n\n> 38\n\nA feature mart is a centralised repository or database that stores curated, pre -processed and reusable features (variables or \n\nattributes) that can be used for training models . Aside from supporting data reusability , feature marts may also help improve \n\ndata governance by maintaining metadata on each feature, including details on its source s, transformations, lineage and \n\nquality. Feature marts may also allow for v ersion control, ensuring that any updates to features are tracked. \n\n> 39\n\nUnstructured data refers to information that does not follow a predefined format or organised structure, making it more \n\ndifficult to store and analyse using traditional databases or methods for structured data . Unstructured data typically includes \n\ndata types such as text, images, videos, and audio. While the use of unstructured data is not new to banks , e.g., using \n\nsurveillance videos from cameras at ATMs, the use of such data is growing due to Generative AI .\n\n> 40\n\nThese may also include updating and adapting other areas such as data discovery and classification, access rights , data \n\nlifecycle management, data saniti sation and validation , and security controls for unstructured data . \n\n> 41\n\nFor example, a developer who wishes to use a more complex neural network -based deep learning model may be required \n\nto justify the need for such an AI model over a simpler tree -based machine learning model or a logistic regression model, and \n\nconsider the trade -offs based on the use case requirements. Artificial Intelligence Model Risk Management | 22 \n\nRobustness and Stability \n\n6.3.2  In assessing the overall suitability of AI models , banks placed heavy focus on \n\nensuring that AI models were both robust and stable 42 , and accordingly paid \n\nsignificant attention to i) the selection and processing of datasets used for \n\ntraining and testing AI models ; ii) determining appropriate approaches, measures \n\nand thresholds for evaluating AI models ; and iii) mitigating overfitting risk s43  that \n\noften arise due to the complexity of AI models . We outline some of the practices \n\nin these key areas below. \n\nSelection and Processing of Datasets for Training and Testing \n\n6.3.3  Datasets chosen for training and testing or evaluati on  44  of AI models were \n\nexpected to be representative of the full range of input values and environments \n\nunder which the AI model was intended to be used. Training and testing datasets \n\nwere also checked to ensure that their dist ribu tions or characteristics are \n\nsimilar 45 .\n\n6.3.4  Most banks also invested efforts in collecting testing datasets that allowed \n\npredictions or outputs from AI models to be tested or evaluated in the bankâ€™s \n\ncontext as far as possible . For example, curating datasets that allow ed for AI \n\nmodel generated answers to queries from customers to be compared against \n\nanswers from in -house human expert s, or getting actual feedback from the \n\nbankâ€™s customers on the quality of these AI model generated answers .\n\nEvaluation Approaches, Measures and Thresholds                                                      \n\n> 42 The concepts of robustness and stability in AI systems often overlap and what these terms cover can vary. For the purpose\n> of this information paper, r obustness refers to AIâ€™s ability to achieve its desired level of performance under real -world\n> conditions ,while stability refers to the consistent performance of AI across arepresentative range of real -world scenarios.\n> These concepts are also related to the reliability of the AI system or model.\n> 43 Overfitting is when an AI model learns the training data overly well , to the point where it perform sextremely well on training\n> data but very poorly on new data that it has not seen in the training dataset .Intuitively, t his may mean that the model has\n> memorised the training examples rather than learning general patterns, resulting in poor performance in real -world conditions .\n> 44 The terms â€œtesting â€and â€œevaluat ion â€of AI models are commonly used interchangeably to refer to the assessment of the\n> performance of AI models on datasets that it had not been trained on .\n> 45 This issue is also commonly referred to as training -testing skew, which are discrepancies between the distribution of data\n> used to train an AI model and the distribution of data it encounters during testing .Artificial Intelligence Model Risk Management |23\n\n6.3.5  Given that AI is developed to meet specific business needs or objectives, banksâ€™ \n\nstandards and processes on the robustness and stability of AI models generally \n\nrequired test ing or evaluation approaches to be aligned with the intended \n\noutcomes that the AI models were meant to support. The exact approaches \n\nselected could differ depending on the nature of the AI model s, as well as the \n\nneeds of the use case. For example, assessing a fraud detection modelâ€™s ability to \n\nflag out known fraud cases by comparing against ground truth in historical data, \n\nor the usefulness of a financial product recommendation model through human \n\nfeedback. \n\n6.3.6  Correspondingly, w hile there are many established performance measures for AI \n\nmodels 46 , banks pa id significant attention to aligning the choice of performance \n\nmeasures with the intended outcomes that the AI models were meant to \n\nsupport. In some cases, this could involve trade -offs between different \n\nperformance measures. For example, if the intended outcome was to detect as \n\nmany instances of fraud as possible, performance measurement would need to \n\nfocus more on the proportion of false negatives (i.e. fraudulent instances that \n\nwere not detected) , even though this may come at the expense of a higher \n\nproportion of false positives (i.e. instances falsely flagged by the model as being \n\nfraudulent) .\n\n6.3.7  Other tests that banks may include to ensure robustness and stability 47  include \n\nthe following :\n\na.  Sensitivity analysis to understand how predictions or outputs of AI models \n\nchange under different permutations of data inputs . This also helps to identify \n\nimportant features that significantly influence predictions or outputs , and \n\nfacilitate explanations of the behaviour of AI models .                   \n\n> 46 There are a wide range of performance measures for AI models, and these are often specific to the task or use -case ;for\n> example, recall, precision, or F1 for classification tasks, mean absolute error or root mean squared error for regression tasks,\n> mean average precision or mean reciprocal rank for recommendation tasks.\n> 47 The objectives of some of these tests overlap, and may also relate to data management aspects that we outlined earlier .\n> Nonetheless, we list all the tests that we observed across the banks for completeness. Artificial Intelligence Model Risk Management |24\n\nb.  Stability analysis to compare the stability of data distribution s and predictions \n\nor outputs, e.g ., assessing whether the distribution of a training dataset from \n\nan earlier period matches the distribution of testing dataset s from more \n\nrecent period s, and how differences affect the performance of AI models .\n\nc.  Sub -population analysis , which are evaluations of how AI model s perform \n\nacross different sub -populations or subsets within the datasets (e.g., to \n\nidentify any significant differences in performance between different \n\ncustomer segments). Such analysis of sub -populations or subsets within the \n\ndatasets help to identify potential issues that might not be obvious in the \n\naggregated testing dataset, as well as potential sources of bias, which could \n\nsupport fairness assessments of AI models where necessary (e.g., wh ere sub -\n\npopulations relate to protected features or attributes such as race or gender ). \n\nd.  Error analysis to identify potential patterns in prediction errors (e.g., \n\nmisclassified instances), which help s to understand the limitations of AI \n\nmodel s.\n\ne.  Stress testing the response of AI models to edge cases or inputs outside the \n\ntypical range of values used in training. This allowed banks to better \n\ndetermine performance boundaries and identify limitations of AI model s.\n\nSome banks also tested the behaviour of AI model s in the context of \n\nunexpected inputs or conditions. Examples included adversarial testing or red \n\nteaming types of exercises . Such testing is especially important in the context \n\nof AI models used in high risk or customer -facing applications , as it allowed \n\nthe bank to establish conditions under which AI models would not perform as \n\nexpected or could introduce potential security or ethical concerns .\n\n6.3.8  Most banks would establish criteria or thresholds for performance measures , to \n\ndefine what was considered acceptable performance. Such thresholds need to \n\nbe clearly defined and documented , as well as mutually agreed upon by \n\ndevelopers and validators. Such thresholds were usually use case specific, and \n\ncould also be used subsequently to facilitate validation, pre -deployment checks, \n\nas well as monitoring and change management. Artificial Intelligence Model Risk Management | 25 \n\nMitigating Overfitting Risks \n\n6.3.9  The large number of parameters and inherent complexity of AI models increases \n\nthe risks of them overfitting on training data (in -sample data) and hence \n\nperforming poorly when deployed on out -of -sample data. Banks employed a \n\nvariety of mitigants to address this risk: \n\na.  Model selection â€“ Generally f avouring AI models of lower complexity unless \n\nthere are clear justifications to do otherwise ; or adopting approaches that \n\nconstrained the complex ity of AI models 48 .\n\nb.  Feature selection - Applying explainability methods to identify the key input \n\nfeatures or attributes that are important for the AI model predictions or \n\noutputs 49  and assess ing that they are intuitive from a business and/or user \n\nperspective 50 .\n\nc.  Model evaluation - Additional performance testing requirements to test the \n\nperformance of AI model s on unseen data where possible, such as cross -\n\nvalidation techniques 51  and testing against more out -of -sample/out -of -time 52 \n\ndatasets. \n\nExplainability \n\n6.3.10  All banks identified explainability as a key area of focus for AI, particularly for use \n\ncases where end -users or customers need to understand key features or \n\nattributes in the data influencing predictions of AI models . For example, \n\nexplainability would be more important in higher risk materiality use cases w here  \n\n> 48\n\nExamples include regulari sation techniques or limiting the number and depth of trees for gradient boosting trees. Such \n\ntechniques generally try to limit the number of parameters used so that the trained model is less complex. For example, some \n\nregularisation  techniques  force less important parameter s to values of zero.  \n\n> 49\n\nAs discussed in the next section on explainability methods.  \n\n> 50\n\nAdditional justification would typically be required to retain features or attributes that were not intuitive, or which did n ot \n\nmeaningfully contribute to the overall performance of the models. Such data may introduce more noise, and cause the AI \n\nmodel to overfit on the noise, leading to poor performance in real world conditions.  \n\n> 51\n\nCross -validation generally refers to techniques to evaluate models by resampling the dataset for training and testing . An \n\nexample would be K-fold cross -validation (which involve s splitting the dataset into K parts for K training and testing rounds) . \n\n> 52\n\nAn out -of -sample testing dataset is a sub set of data not used in model training , whereas an out -of -time testing dataset is a\n\nsubset of data obtained from a time period distinct from the time period of the subset of data used in training the model. Artificial Intelligence Model Risk Management | 26 \n\nbank staff making decisions based on predictions  of AI models need to \n\nunderstand the key features or attributes 53  contributing to the prediction; or in \n\nuse cases where a customer may ask for reasons for being denied a financial \n\nservice. Hence, development standards for AI across all banks ha d been \n\nexpanded to include a section on explainability. \n\n6.3.11  Explainability requirements in banksâ€™ standards and processes generally required \n\ndevelopers to apply global and/or local 54  explainability methods to identify the \n\nkey features or attributes used as inputs to AI model s and their relative \n\nimportance ; assess whether these features or attributes were intuitive from a \n\nbusiness and/or user perspective ; and provide additional justification for \n\nretaining features o r attributes which were not intuitive. Such methods could \n\nalso help identify the usage of potentially sensitive features as part of fairness \n\nassessments. Some banks had set out a list of global and local explainability \n\nmethods that could be applied to explain the outputs  55  of AI models . Such \n\nmethods could be directly applied during development as part of the feature \n\nselection process, or used within explainability tools developed as part of the AI \n\nsystem so that either global and/or local explanations can be provided alongside \n\npredictions or outputs generated by AI models post -deployment. \n\n6.3.12  In terms of the level of explainability required for different use cases, some banks \n\nestablished standards and processes to clearly define the minimum level of global \n\nand/or local explainability required for different use cases. For these banks,  \n\n> 53\n\nAn example of a feature or attribute in this context could be the income of the customer.  \n\n> 54\n\nGlobal explainability is the ability to understand the overall functioning of the model by identifying how input features drive \n\nmodel outputs at an overall model level. Local explainability is the ability to identify how input feature s drive the model output \n\nfor a specific observation or instance. Taking a fraud detection model as an example, global explainability methods allow for \n\nidentification of the most important features , such as the high values of transactions, used to detect fraudulent transactions \n\nfor the model in general . However, the key features that are important for a specific transaction (i.e. the local instance) may \n\nnot necessarily be the same, e.g., the value of the transaction may be small for a specific instance but the transaction is still \n\ndetected as a fraudulent transaction due to specific characteristics of the parties involved in the transaction , such as an \n\nunfamiliar geographic location of one of the parties . Local explainability methods help to identify such features for the local \n\ninstance.  \n\n> 55\n\nCommon examples of explainability methods include SHAP (for global and local explainability) and LIME (for local \n\nexplainability). SHAP generates Shapley values for each feature based on its contribution to a given model output. A global -\n\nlevel explanation can be generated by generating a summary plot of the Shapley values of the key features, across the entire \n\nset of model outputs . LIME is based on training a separate model for the local instance that needs to be explained . The \n\nexplanation that is generated is based on the separately trained model .Artificial Intelligence Model Risk Management | 27 \n\nfactors considered  when applying a higher standard of global and/or local \n\nexplainability included risk materiality or the extent to which AI -driven decisions \n\nwere likely to require explanations (e.g. , to the bankâ€™s customers) for the use \n\ncase . For example, AI model s used for credit decisioning could require the most \n\nexacting standards for global and local explainability, requiring developers to \n\ncarefully consider all features used as inputs and provide justifications for their \n\nuse , as well as the ability for users to easily identify key features influencing any \n\ngiven prediction post -deployment. Other banks required global and/or local \n\nexplainability to be explored across all AI, but allowed users and owners to decide \n\non the acceptable level of explainability , and justify their decision based on the \n\nuse case .\n\nFairness \n\n6.3.13  The outputs of AI models are inherently influenced by the patterns learnt from \n\nits training data. If th e training data contain ed biases that unfairly represent or \n\ndisadvantage specific groups of individuals, AI model s may perpetuate these \n\nunfair biases in its predictions or outputs. This could lead to decisions or \n\nrecommendations that disproportionately and unfairly impact certain \n\ndemographic groups. \n\n6.3.14  The earlier section on data management had outlined the need for fairness to be \n\nconsidered during development, and for checks and monitoring of potential \n\nbiases during deployment. More specifically, d uring AI development, for use \n\ncases that could have a significant impact on individuals, most banks would \n\nundertake a formal assessment on whether specific groups of individuals could \n\nbe systematically disadvantaged by AI -driven decisions. The scope of such \n\nassessments could vary between banks depending on the relevant rules, \n\nregulations or expectations applicable to the bank 56 , and between use cases \n\ndepending on the risk materiality of the AI .    \n\n> 56 Examples of such expectations on fairness for AI used by banks across jurisdictions include the Principles to Promote Fairness,\n> Ethics, Accountability and Transparency (FEAT) in the use of Artificial Intelligence and Data Analytics in Singaporeâ€™s Financial\n> Sector , published by MAS in 2018; General Principles for the use of Artificial Intelligence in the Financial Sector , published by\n\nArtificial Intelligence Model Risk Management | 28 \n\n6.3.15  Generally, the approach for assessing fairness used by banks involved the \n\nfollowing steps: \n\na.  Defining a list of protected features or attributes, for which use of such \n\nfeatures or attributes in AI models would require additional analysis and \n\njustification. Common examples of such protected features or attributes \n\ninclude gender, race or age. \n\nb.  Determin ing whether such features or attributes 57  were used in training AI \n\nmodel s. Based on this assessment, to define groups of individuals at risk of \n\nbeing systematically disadvantaged by the AI -driven decisions (at -risk groups) .\n\nc.  Where necessary, determining the extent to which AI -driven decisions \n\nsystematically disadvantaged against at -risk groups. The was usually assessed \n\nvia fairness measures (e.g., fairness measures that are available in the toolkit \n\nreleased by the Veritas Initiative) .\n\nd.  Where necessary, providing adequate justifications on the use of protected \n\nfeatures or attributes in AI models (e.g. , trade -offs against the intended \n\nobjectives of the AI model 58 ). \n\nReproducibility and Auditability \n\n6.3.16  Reproducibility and auditability 59  of AI development are essential for ensuring \n\naccountability and building trust in AI systems . To facilitate reproducibility and \n\nauditability of AI, most banks expanded existing documentation requirements to \n\nincorporate the relevant AI development processes and considerations. A list of                 \n\n> De Nederlandsche Bank in 2019; and the High -level Principles on Artificial Intelligence , published by the Hong Kong Monetary\n> Authority in 2019.\n> 57 These could include proxy attributes that are heavily correlated with such protected attributes .\n> 58 This could be supported by, for example, analysis on the difference in performance between an AI model which included\n> these protected features or attributes, and an AI model which did not. An informed assessment could then be made on whether\n> this differenc e in performance was necessary to achieving the model's intended objective, taking into consideration the level\n> of potential harm done to at -risk groups arising from the use of the AI model.\n> 59 Reproducibility refers to â€œthe ability of an independent verification team to produce the same results using the same AI\n> method based on the documentation made by the organisation â€, while audibility refers to â€œthe readiness of an AI system to\n> undergo an assessment of its algorithms, data and design processes â€ ( Model AI Governance Framework, IMDA Singapore. )\n\nArtificial Intelligence Model Risk Management | 29 \n\nkey documentation requirements  for AI commonly seen across banks are as \n\nfollow s:\n\na.  Data - Documentation of key data management steps is important to facilitate \n\nreproducibility and auditability. During development, key information that \n\nwould usually be documented include datasets and data sources used for \n\nmodel development and evaluation, details of how these datasets were \n\nassessed as fit -for -purpose, processed ahead of model training, and split into \n\nrelevant training, testing and/or validation 60  datasets. \n\nb.  Model training - Details of how the AI model was trained or fit to the training \n\ndataset. Such details could include codes (along with software \n\npackages/environment used and their relevant versions), key settings (e.g. ,\n\nhyperparameters 61  used and the approach for selecting hyperparameters 62 ), \n\nrandom seed values 63  and any other configuration s required for a third party \n\nto reproduce the training process. \n\nc.  Model selection - Details of how the performance of the AI model was \n\nevaluated and how the final model was selecte d. Such details could include \n\nthe evaluation approaches, thresholds and datasets applied  64  and the \n\ncorresponding results, comparison s of performance across multiple AI models \n\nand justification s for selecting the final model. \n\nd.  Explainability - Global and/or local explainability methods used , feature \n\nselection process, analysis of results , as well as description of key features \n\nselected and additional justifications for inclusion of certain key features (e.g .,\n\nfeatures that may not have appear ed to be important to a human expert ).                          \n\n> 60 Testing and validation datasets refer to datasets used to evaluate the performance of the model outside of the dataset used\n> to train the model. This should be distinguished from independent validation, which is the process of independently assessing\n> the overall suitability of the model.\n> 61 E.g. ,number of trees and maximum tree depth for gradient boosted trees.\n> 62 E.g. ,grid search, random search of hyperparameters .\n> 63 AI models usually need to be in itialised with a random set of numbers (e.g., for the model parameters) before training , and\n> documenting the random seed value that is used to initialise the AI models is necessary to reproduce the AI modelâ€™s behaviour\n> and results.\n> 64 As detailed in the earlier sub -section on Robustness & Stability. Artificial Intelligence Model Risk Management |30\n\ne.  Fairness - Metrics and associated thresholds, results of fairness assessments \n\nand justifications for the use of any protected features or attributes .\n\n6.3.17  Alongside documentation requirements in the relevant standards and processes, \n\nmost banks also set up documentation templates that developers were required \n\nto follow for consistency . Such templates were typically designed by the bankâ€™s \n\nMRM function. Templates could differ between business domains (as different \n\nperforma nce tests or metrics could apply) or between AI of different risk \n\nmaterialities (as documentation requirements could be higher for AI of higher \n\nrisk materiality ). \n\n6.4 Validation \n\n6.4.1  Independent validation provides an objective and unbiased assessment of the \n\nsuitability, performance and limitations of AI. It acts as a n important challenge to \n\ndevelo pers , and ensures that the relevant standards and processes have been \n\nadhered to when developing AI. \n\n6.4.2  The validation process typically involves an independent unit 65  reviewing the AI \n\ndevelopment process and documentation, assessing that AI performs and \n\nbehaves as intended, and undertaking pre -deployment checks. Actions to \n\naddress issues identified during validation, such as the application of suitable \n\nadjustments or other mitig ating or compensat ory controls, would typically be \n\nproposed by developers and agreed to by validators before deploying AI. \n\n6.4.3  Building on their conventional MRM processes, banks have equipped \n\nindependent validation functions with the skills and incentives needed to \n\nconduct independent review of AI used in the bank , which include invest ments \n\nin efforts to ensure that independent validation staff have the relevant technical \n\nexpertise for AI.   \n\n> 65 For example, t he Federal Reserve/Office of the Comptroller of the Currencyâ€™s SR Letter 11 -7 on Supervisory Guidance on\n> Model Risk Management states that validation should generally be done by individuals not responsible for development or use\n> and do not have a stake in whether a model is determined to be valid.\n\nArtificial Intelligence Model Risk Management | 31 \n\n6.4.4  Banks adopted a range of approaches in establishing independent validation \n\nrequirements across different AI. One bank required all AI to be subject to \n\nindependent validation, with the depth and rigour of validation varying based on \n\nthe AIâ€™s risk materiality rating. Most other banks required independent validation \n\nonly for AI of higher risk materiality , with other AI subject only to peer review 66 .\n\nEven for AI of lower risk materiality, the involvement of either an independent \n\nvalidator or peer reviewer allowed for some degree of challenge that helped to \n\nbetter manage the added uncertainties and risks posed by AI, and check that such \n\nAI was developed in accordance with the bankâ€™s standards and processes. \n\n6.5 Deployment , Monitoring and Change Management \n\nPre -Deployment Checks \n\n6.5.1.  Aside from checks during the validation process, p re -deployment checks and \n\ntests are important to ensure that the AI has been correctly implemented and \n\nproduces the intended results before being deployed for use . Banks placed \n\nsignificant focus on implementing controls for the deployment of AI to ensure \n\nthat the AI functions as intended in the production environment 67 . These controls \n\nwere usually based on existing technology risk management guidelines . F or \n\nexample, b anks would apply standard software development lifecycle (SDLC) \n\nprocesses to ensure that the AI application or system was secure, free from error \n\nand perform ed as intended before deployment 68 . Some banks also conduct ed \n\nadditional check s to ensure that the deployed AI â€™s scope, output and \n\npe rformance , and associated controls align with that of the validated AI :\n\na.  Additional tests , such as :                 \n\n> 66 As compared to independent validation, peer reviews were usually conducted by a non -independent function (e.g. ,a\n> different development team in the same unit/reporting line as the original model developers).\n> 67 A production environment is alive operational setting where deployed systems, such as deployed AI models, are run under\n> real world condit ions to deliver services or perform tasks for end -users.\n> 68 Please see MASâ€™ Technology Risk Management Guidelines for further details on the adoption of sound and robust practices\n> for the management of technology risk in these areas: https://www.mas.gov.sg/regulation/guidelines/technology -risk -\n> management -guidelines\n\nArtificial Intelligence Model Risk Management | 32 \n\ni.  forward testing , which are experimental runs using a limited set of \n\nproduction data or with a limited set of users, for selected high materiality \n\nuse cases to assess the behaviour of AI in an environment similar to when \n\nthe AI is fully deployed ; and \n\nii.  live edge case test ing to assess how AI handles edge cases in the \n\nproduction environment , which helps to verify that AI can handle a variety \n\nof improbable but plausible scenarios when deployed .\n\nb.  Automated pipelines , such as setting up automated deployment and \n\ncontinuous integration/continuous deployment (CI/CD ) pipelines  69  to \n\nminimi se human error and maintain ing a consistent process for how AI is \n\ndeployed , monitored, and maintained , which is important for AI given the \n\nneed for regular data and model updates .\n\nc.  Process management , which includes checks to ensure that key processes \n\nimportant for the deployed AI, such as human oversight , backup models , and \n\nother appropriate controls and contingencies , are in place ; and business \n\nprocess change management, such as training users to understand AI \n\nlimitations and to use AI appropriately .\n\n6.5.2.  Non -AI specific pre -deployment checks  70  remain relevant, hence key control \n\nfunctions, such as those in the areas of technology, data, legal and compliance, \n\nthird -party and outsourcing , would also confirm that the checks have been \n\nundertaken and sign off before AI is deployed into production.               \n\n> 69 Continuous integration/continuous deployment (CI/CD) pipelines automate the process of building, testing, and deploying\n> code changes , and reduce the potential of errors arising from manual interventions .Approvals and checks are also usually\n> integrated into the CI/CD process to ensure that new code pushed into production are checked for errors .More details on\n> CI/CD, as well as other related terms such as MLOps and AIOps are provided in Annex A.\n> 70 For example, checks relating to cyber -security , or compliance with outsour cing policies .Artificial Intelligence Model Risk Management |33\n\nMonitoring Metrics and Thresholds \n\n6.5.3.  Monitoring is particularly critical for AI given their dynamic nature and the \n\npotential for AI model staleness due to drifts  71  in either data or the model \n\nbehaviour over time. All b anks pa id significant focus to the ongoing monitoring \n\nof their AI to ensure that they continue to operate as intended post -deployment. \n\nKey measures that were monitored generally follow those that were covered \n\nduring development and validation, and include robustness , stability, data \n\nquality, and fairness measures .\n\n6.5.4.  Measures used for monitoring were tracked against predefined thresholds ,\n\nusually determined at the development and validation stages,  to ensure models \n\nperform within acceptable boundaries . Some banks have also implemented \n\ntiered thresholds, for example, additional  early warning thresholds to pre -empt \n\nmodel deterioration, and different thresholds to determine when retraining or a \n\nfull redevelopment of the AI may be necessary. \n\n6.5.5.  Most banks also have a process or system for reporting , tracking and resol ving \n\nissues or incidents if breaches or anomalies arise from the monitoring process. \n\nBanks generally track issues or incidents from discovery to resolution, and \n\nincorporate a relevant escalation process based on the materiality of the issue or \n\nincident. The resolution process may include AI model retrai ning ,\n\nredevelopment , or decommissioning as possible outcomes. Where a major \n\nredevelopment was undertaken , revalidation and approval would be needed \n\nbefore the updated model could be redeployed .   \n\n> 71 AI models can perform poorly when they become stale due to factors such as data drift, concept drift or model drift, which\n> are essentially due to changes in the data distributions, relationships between input data and predictions/outputs, or the\n> general e nvironment in which the AI model is being used. More details on data, concept and model drifts are provided in Annex\n> A. Artificial Intelligence Model Risk Management |34\n\nContingency Plans \n\n6.5.6.  All banks would generally have standards and processes relating to contingency \n\nplans for AI, particularly those supporting high -risk or critical functions 72 . These \n\nplans , which may not be specific to AI, typically outline fallback options, such as \n\nalternative systems or manual processes, and would be subject to regular \n\nreviews and testing to ensure readiness for rapid activation when necessary. For \n\nmission -critical AI applications 73 , a few banks may also have kill switches in place. \n\nKill switches are used to deactivate AI if they exceed risk tolerances , and require \n\nclear contingency plans to be quickly rolled out .\n\nReview and Re validations \n\n6.5.7.  Aside from ongoing monitoring, banks also conduct ed periodic reviews of their \n\nportfolio of AI . Key aspects that that were usually reviewed include changes in \n\nthe model sâ€™ materiality, risks, scope and usage, performance, assumptions and \n\nlimitations, and identification and remediation of issues .\n\n6.5.8.  Banks also have standards and processes for ongoing re validations of AI in \n\nproduction, with the intensity and frequency based on the materiality of the AI .\n\nIn general, AI deemed critical to risk management, regulatory compliance, \n\nbusiness operations, or customer outcomes are revalidated more frequently and \n\nintensely .\n\nChange Management \n\n6.5.9.  Standards and processes relating to AI change management are needed to \n\nensure that what constitutes a change is clearly defined, and that the appropriate \n\ndevelopment and validation requirements are applied. M ost banks require d    \n\n> 72 Such contingency plans may not apply specifically to AI, but to technology systems in general. Nonetheless, they may require\n> additional considerations in the case of AI, e.g., AI -specific performance monitoring thresholds to determine when to trigger\n> the contingency plan, or a backup plan that involves another AI system or model.\n> 73 For example, for AI that are used for trading .\n\nArtificial Intelligence Model Risk Management | 35 \n\nsignificant  or material changes 74  to AI in production to be reviewed and approved \n\nby the control functions prior to implementation , so as to  ensure that any \n\nmodifications made to the model do not negatively impact its performance . To \n\nmanage changes to AI , b anks have also established systems and processes for \n\nversion control of both internal and third -party AI (which do not only cover code \n\nrelating to AI , but also data and other artifacts such as hyperparameters and the \n\ntrained model parameters or weights ). Version control enable s banks to track \n\nchanges across different aspects of AI and roll -back to previous version s of AI \n\nwhere ne cessary 75 . Most b anks have also set up processes for third -party AI \n\nproviders to provide notification s of version updates 76 .\n\n6.5.10.  AI for certain use cases, such as fraud detection, may need to be changed or \n\nupdated more frequently 77 , due to drifts in the data or the behaviour of the AI \n\nmodel over time . To deal with such frequent changes, some banks have \n\nestablished systems and processes for the automatic updating of such AI. Such \n\nAI, which some banks refer to as â€œdynamic AI â€, need to be subject to enhanced \n\nrequirements and controls to ensure that change management is well governed .\n\nKey additional requirements and controls include justification s for enabling \n\nautomatic updating of AI , clearly defining what can be updated automatically , for \n\nexample, restricting changes to the retraining of AI model with more recent \n\ndatasets , but not allowing for changes to AI model architectures or \n\nhyperparameters . Such dynamic AI would also be subject to enhanced risk  \n\n> 74\n\nExample s of significant or material change s include fundamental changes to AI model architecture s or training technique s.\n\nSuch changes may necessitate an in -depth revalidation, compared to less significant changes , such as retraining the AI model \n\nwith more recent data , which may only require checks on AI performance to ensure the AI is still behaving as expected.  \n\n> 75\n\nWhile we cover version control here under change management whe re the AI is already deployed , it is important to note \n\nthat version control for AI also plays a key role during the development and validation stages. For example, v ersion controls \n\nare needed to support iterative improvements and collaboration during development , and also help to ensure reproducibility \n\nand auditability during validation.  \n\n> 76\n\nWhile banks generally try to require third -party providers to notify them of any changes to the AI model or service, there \n\nmay be circumstances where such notifications may not happen, e.g., the third -party provider may not notify end -users on \n\nchanges that they view as immaterial. We have observed banks trying to address this by setting out clearer terms in their legal \n\nagreement s, for example, adding a clause that requires the third -party provider to notify banks on an y upcoming change s to \n\nthe AI model or system.  \n\n> 77\n\nFor example, if we compare a fraud detection use case with a n NLP use case such as summarisation of customer call \n\ntranscripts, data relating to the behaviour of scammers would usually change much more frequently than data relating to \n\ncustomer calls due to the active efforts of scammers to evade detection. Artificial Intelligence Model Risk Management | 36 \n\nmanagement requirements, such as enhanced  data management standards, e.g., \n\nadditional checks on data quality and drifts , as well as enhanced performance \n\nmonitoring requirements , e.g., more stringent monitoring notification \n\nthresholds .\n\n# 7 Other Key Areas \n\n7.1 Generative AI \n\nOverview \n\nWhile t he use of Generative AI in banks is still in the early stages , banks generally try to \n\napply existing governance and risk management structures and processes where relevant \n\nand practicable, and balance innovation and risk management by adopting :\n\nâ€¢ Strategies and approaches , where they leverage on the general -purpose nature of \n\nGenerative AI by focusing on the development of key enabling modules or services; \n\nlimit the current scope of Generative AI to use cases for assistin g/ augmenting \n\nhumans or improving internal operational efficiencies that are not direct customer \n\nfacing; and buil ding capacity and capabilities by establishing pilot and \n\nexperimentation frameworks ;\n\nâ€¢ Process controls , such as setting up cross -functional risk control checks at key \n\nstages of the Generative AI lifecycle ; establishing more detailed development and \n\nvalidation guidelines for different Generative AI task archetypes ; requir ing human \n\noversight for Generative AI decisions ; and paying close attention to user education \n\nand training on the limitations of Generative AI tools ; and \n\nâ€¢ Technical controls , such as selection, testing and evaluation of Generative AI \n\nmodels in the context of the bankâ€™s use cases; developing reusable modules to \n\nfacilitate testing and evaluatio n; assessing different aspects of Generative AI model \n\nperformance and risks; establish ing input and output filters as guardrails to address \n\ntoxicity, bias and privacy issues ; and mitigating data security risks via measures \n\nsuch as the use of private clouds or on -premise servers, data loss prevention tools, \n\nand limiting the access of Generative AI to more sensi tive information. Artificial Intelligence Model Risk Management | 37 \n\n7.1.1.  In addition to the k ey areas highlighted in the prior sections, there are some \n\naspects relating to Generative AI (compared to conventional AI) that require \n\nfurther consideration: \n\na.  Higher uncertainties associated with Generative AI â€“ The risks of \n\nhallucinations and unexpected behaviours by Generative AI given its greater \n\ncomplexity may lead to less robust and stable performance , and was a key \n\nconcern highlighted by banks . This concern was particularly pronounced for \n\nuse cases of higher risk materiality or those that are directly customer -facing ,\n\nwhere greater reliability was required .\n\nb.  Difficulties in evaluating/testing Generative AI and mitigating its limitations \n\nâ€“ Compared to conventional AI, which were typically used by banks for \n\nspecific use cases that the AI models had been trained for, Generative AI are \n\nmore general -purpose in nature and can be used in a wider range of use cases \n\nin the bank. However, there may be a lack of easily available ground truths 78 \n\nin some of these newer use cases to evaluate and test Generative AI. Use \n\ncases involving Generative AI also typically involve unstructured data , such as \n\ntext data , for which there are significantly more possible permutations ,\n\ncompared to structured data usually used for conventional AI . This makes it \n\nchallenging to foresee all potential scenarios and perform comprehensive \n\ntesting and evaluations 79 .\n\nc.  Lack of transparency from Generative AI providers - Unlike conventional AI \n\nmodels, which are often developed and trained internally by the bankâ€™s \n\ndevelopers, Generative AI used by banks were pre -dominantly based on pre -\n\ntrained models from external providers. As disclosure standards relating to \n\nsuch AI are still evolving globally, banks may lack full access to essential risk          \n\n> 78 Ground truth refers to reliable or factual information that serves as a standard against which the outputs or predictions of\n> AI models, including Generative AI models, can be evaluated.\n> 79 For example, it is significantly harder to evaluate the quality of a summary or of an image generated by Generative AI,\n> compared to evaluating the accuracy of a simple yes/no prediction from conventional AI. It is also harder to foresee all possible\n> permutations of text or images that may be used as inputs to Generative AI, as well as all possible permutations of text or\n> images that may be generated by Generative AI.\n\nArtificial Intelligence Model Risk Management | 38 \n\nmanagement information, such as details about the underlying data used in \n\nmodel training and testing, as well as the extent of evaluation or testing \n\napplied to these models. \n\nd.  Challenges in explainability and fairness with Generative AI â€“ The lack of \n\ntransparency from external providers may also contribute to challenges in \n\nunderstanding and explaining the outputs and behaviour of Generative AI ,\n\nand ensuring that the outputs generated by Generative AI are fair. There is \n\nalso a general lack of established methods currently for explain ing Generative \n\nAI outputs and assessing their fairness .\n\n7.1.2.  Most b anks are in the process of reviewing and updating parts of their AI model \n\nrisk management framework for Generative AI to balance the benefits and risks \n\nof its use. \n\n7.1.3. The subsequent paragraph s outline observations from the thematic on key \n\napproaches and controls that banks have adopted to balance innovation and risks \n\nbased on the current state of use of Generative AI . It should be noted that these \n\napproaches and controls will need to be updated as Generative AI technology \n\nevolves , and that risk management efforts will need to be scaled accordingly \n\nbased on the state of Generative AI use across the institution .\n\nStrategie s and Approaches \n\n7.1.4. Some banks have invested significant effort in identifying and building key \n\nenabling services and modules for Generative AI that can be utilised across \n\nmultiple use cases, e.g., vector databases 80 , retrieval systems 81 , evaluation and           \n\n> 80 Data, particularly unstructured data, such as text and images, need to be encoded into numerical representations before\n> they can be used for AI or Gen erative AI. Such numerical representations are commonly referred to as vectors. Vector\n> databases are speciali sed database systems designed to store, index, and efficiently query such data.\n> 81 Retrieval systems help to search information repositories and retrieve the most relevant information for a specific task. For\n> example, to help answer a query relating to information in a corporate information repository, the retrieval system will help\n> to search for the most relevant pieces of information in the corporate information repository . The retrieved information is then\n> usually used as context for the Generative AI model to generate an answer from. Artificial Intelligence Model Risk Management |39\n\ntesting modules 82 . Such an approach enables scalability, reduces time and costs \n\nfor implementation, and facilitates the development of more robust and stable \n\nGenerative AI. \n\n7.1.5. To manage the potential impact of Generative AI risks, such as hallucinations, \n\nmost banks have started with a more limited scope of use, focusing on the use of \n\nGenerative AI for assisting or augmenting humans, or improving inte rnal \n\noperational efficiencies, rather than deploying Generative AI in direct customer -\n\nfacing applications without a human -in -the -loop. Banks felt that such an \n\napproach would allow them to learn how to utilise Generative AI effectively and \n\nunderstand its limitations, while managing the potential impact of risks posed by \n\nGenerative AI. \n\n7.1.6. Similarly , to gain greater comfort with the use of Generative AI, most banks have \n\nestablished clear policies and procedures for Generative AI pilots and \n\nexperimentation frameworks. Aside from helping the bank to build capacity and \n\ncapabilities while managing risks a ssociated with Generative AI, such pilots and \n\nexperimentation frameworks are needed to evaluate and test Generative AI in \n\nreal -world scenarios and understand how Generative AI would behave when \n\ndepl oyed . Such pilots are typically bound by time and user lim its 83 .\n\nProcess Controls \n\n7.1.7. To address the cross -cutting nature of Generative AI use cases and risks, as well \n\nas the fast -evolving landscape, some banks have instituted cross -functional risk \n\ncontrol checks at key stages of the Generative AI lifecycle. \n\n7.1.8. As most Generative AI use cases usually fall within a few task archetypes, e.g., \n\nsummarisation, information extraction, conversational agents, question \n\nanswering, one bank established detailed development and validation guidelines                       \n\n> 82 An example of such a module could be a separately trained AI model that estimates the probability of an answer generated\n> by an LLM being a hallucination.\n> 83 Aside from setting time and user limits, other requirements that may apply to such pilots or experiments include setting\n> clear criteria for success at the end of the pilot ,conditions on the terms of use for owners and end -users , and close monitoring\n> of usage patterns and outputs for anomalies and to ensure compliance with the limited scope of usage. Artificial Intelligence Model Risk Management |40\n\nspecific to different Generative AI task archetypes to support development and \n\nvalidation processes. \n\n7.1.9. Due to the uncertainties associated with Generative AI, banks continue to require \n\nhuman oversight or have a human -in -the -loop when using Generative AI to aid in \n\ndecision -making . Extensive user education and training on the limitations of \n\nGenerative AI tools was another key area of focus. \n\nTechnical Controls \n\n7.1.10. As most Generative AI models used by banks, whether closed or open -source, \n\noriginate from third parties, selection of the appropriate model continues to be \n\nan important step for most banks. To assess suitability , some banks would \n\ntypically start by conduct ing significant research on the capabilities of these \n\nmodels for their needs , including utilising public benchmarks and the latest \n\nresearch papers to guide decisions. Testing and evaluation of Generative AI \n\nmodels in the context of the bankâ€™s use cases was also an important area of focus. \n\n7.1.11. More advanced banks would undertake a range of assessments, from \n\nstandalone, functional to end -to -end assessments. Standalone assessments \n\ninvolve the evaluation of the Generative AI model itself. This is usually based on \n\npublicly available data or resources , such as evaluation results in research \n\narticles , model leaderboards, or using open -source evaluation datasets .\n\nFunctional assessments involve evaluati ons of Generative AI model performance \n\non tasks and contexts specific to the bank, e.g., evaluating the performance of a \n\nGenerative AI model when used for retrieval of information from the bankâ€™s \n\nrepository . Finally, end -to -end assessments would evaluate the performance of \n\nthe entire Generative AI system, which may involve multiple Generative AI or AI \n\nmodels .\n\n7.1.12. Such banks also paid significant attention to establishing methods for assessing \n\ndifferent aspects of Generative AI model performance such as accuracy, Artificial Intelligence Model Risk Management | 41 \n\nrelevance, and bias 84 , as well as creating reusable modules to facilitate testing \n\nand evaluation .\n\n7.1.13. The more advanced banks also paid significant attention to curating testing \n\ndatasets that were specific to the use cases and tasks that Generative AI models \n\nwere being used for in the bank. Such testing datasets were critical to ensuring \n\nthat Generative AI models and systems were fit -for -purpose in the bank â€™s \n\ncontext . For example, if Generative AI was used for summarising complaints from \n\nthe bankâ€™s customers, the performance of Generative AI on general \n\nsummarisation tasks may not be indicative of its performance in the bankâ€™s \n\ncontext as it may not have been trained on such complaints that are not in the \n\npublic domain , and the complaints may also contain information specific to the \n\nbank, e.g., the bankâ€™s services. To ensure the proper evaluation of Generative AI \n\nin the bankâ€™s context , t he bank will need to curate bank -specific testing datasets \n\nfrom the bankâ€™s internal historical data, or use expert human annotators to \n\ngenerate good quality summaries for a set of customer complaints to evaluate \n\nagainst . Such testing datasets are also important for monitoring the ongoing \n\nperformance of Generative AI models, and for evaluating newer Generative AI \n\nmodels as part of the onboarding process. Other key tests that banks adopted \n\ninclude d model vulnerability testing to assess cyber security risks 85 , as well as \n\nstability and sensitivity testing to ensure consistent performance. Human \n\nfeedback also play ed a key role in testing, evaluating and monitoring Generative \n\nAI performance .\n\n7.1.14. Most banks have established input and output guardrails that utilise filters to \n\nmanage risks relating to areas such as toxicity, biasness, or leakage of sensitive \n\ninformation . Such filters may use rules or AI to detect such undesired or \n\ninappropriate information. For example, input  filters may be used to reject \n\nrequests with toxic language , or replace PII information in requests with generic                        \n\n> 84 In this context, a ccuracy refers to whether the generated text aligns with factual informatio n; r elevance refers to how\n> pertinent the generated text is to the specific query ; and b ias refers to scenarios where the generated text may be biased to\n> specific groups of people , e.g., the generated content may favour one gender over another.\n> 85 These were discussed at length in MASâ€™ information paper on Cyber Risks Associated with Generative Artificial Intelligence\n> and will not be repeated here. See Annex B for link to the paper.\n\nArtificial Intelligence Model Risk Management | 42 \n\nplaceholders. O utput filters may be used to detect biasness or toxic language in \n\nthe outputs of Generative AI and trigger a review by a human or another \n\nGenerative AI model , or redact PII information in the outputs of Generative AI \n\nbefore they are presented to the user . Similarly, some banks also focused efforts \n\non developing guardrails that were reusable. \n\n7.1.15. Banks mitigate d data security risks when using Generative AI by either using \n\nprivate cloud solutions for Generative AI models, or open -source models on -\n\npremise , which  keep sensitive data within controlled environments (either \n\ndedicated cloud resources not shared with other organisations, or on -premise \n\nserve rs) which can reduce the risks of exposure of data to external parties . Legal \n\nagreements with solution providers, data loss prevention tools, as well as limits \n\non the classification of data that could be used in Generat ive AI were also \n\nimportant to mitigate data security risks. \n\n7.1.16. Anoth er common area that banks were exploring to address Generative AI risks \n\nwere grou nding methods  86  such as retrieval augmented generation ( RAG ) 87 \n\nwhere the outputs of Generative AI models are constrained based on internal \n\nknowledge bases , and source citations are provided to allow end -users to check \n\nfor the accuracy of Generative AI outputs.                      \n\n> 86 Grounding methods help to ground or anchor the Generative AI outputs to factual, verifiable information, which can help\n> reduc ehallucinations and improv erobustness.\n> 87 Retrieval -Augmented Generation (RAG) methods typically retrieve relevant information from a pre -defined knowledge base,\n> and provide the retrieved information as context to the Generative AI model for the generation of outputs. For example, to\n> generate an answer to a question, information relevant to the questi on would be first retrieved, and the retrieved information\n> would then be provided as context to an LLM. The LLM would usually be instructed to answer the question based on the\n> retrieved information .Links to t he retrieved information could also be provided as source citations in the answer. There is\n> however still the possibility of hallucinations occurring even with such approaches. Artificial Intelligence Model Risk Management |43\n\n7.2 Third -Party AI \n\nOverview \n\nExisting third -party risk management standards and processes 88  continue to play an \n\nimportant role in banksâ€™ efforts to mitigat e risks associated with third -party AI. As far as \n\npracticable, most banks also extended controls for internally developed AI to third -party \n\nAI. When considering the use of third -party AI, b anks would weigh the potential benefits \n\nagainst the risks of using third -party AI. To address the additional risks arising from third -\n\nparty AI , banks were exploring areas such as :\n\nâ€¢ conducting c ompensatory testing ;\n\nâ€¢ enhancing c ontingency planning ;\n\nâ€¢ updating l egal agreements ; and \n\nâ€¢ investing in training and other a wareness efforts .\n\n7.2.1  The u se of third -party AI is increasingly common among banks , particularly in the \n\ncontext of Generative AI where most banks utilise Generative AI models that \n\nwere pre -trained by an external party. However, the use of such third -party AI \n\nand Generative AI presents additional risks , such as unknown biases from pre -\n\ntraining data, data protection concerns , as well as concentration risks due to \n\nincreased interdependencies , e.g., from multiple FIs or even third -party providers \n\nrelying on common underlying Generative AI model s. The lack of transparency is \n\noften cited as a key challenge in managing such third -party risks. Third -party AI \n\nproviders may be reluctant to disclose proprietary information about their \n\ntraining data or algorithms, hindering banksâ€™ efforts in risk assessment and \n\nongoing monitoring. \n\n7.2.2  To mitigate these additional risks, banks were exploring various approaches , such \n\nas:                  \n\n> 88 This includes processes required to comply with MASâ€™ Notice and Guidelines on Outsourcing (refer to\n> https://www.mas.gov.sg/regulation/third -party -risk -management ).Artificial Intelligence Model Risk Management |44\n\na.  Compensatory testing - conduct ing rigorous testing of third -party AI models \n\nusing various datasets and scenarios to verify the modelâ€™s robustness and \n\nstability in the bankâ€™s context , and to detect potential biases. \n\nb.  Contingency planning - develop ing robust contingency plan s to address \n\npotential failures, unexpected behaviour of third -party AI, or discontinuing of \n\nsupport by vendors. This can include having backup systems or manual \n\nprocesses in place to ensure business continuity. \n\nc.  Legal agreements - updat ing contracts with third -party AI providers to include \n\nclauses such as those pertaining to performance guarantee s, data protection, \n\nthe right to audit, and notification when AI is introduced (or not incorporating \n\nAI without the bankâ€™s agreement) in existing third -party providers â€™ solutions. \n\nSuch clauses could facilitate clear er expectations and responsibilities. \n\nd.  Awareness efforts â€“ invest ing in training of staff on AI literacy and risk \n\nawareness to improve understanding and mitigation of risks; conduct ing \n\nsurveys with third -party providers to gather more information about whether \n\nAI is being used in their products or services, and third -party providersâ€™ \n\npractices , including their AI development and risk management processes. \n\n# 8 Conclusion \n\n8.1.  Robust oversight and governance of AI, supported by comprehensive \n\nidentification , inventorisation of AI and appropriate risk materiality assessment, \n\nas well as rigorous development, validation and deployment standards and \n\nprocesses are important areas that FIs need to focus on when using AI. As the AI \n\nlandscape continues to evolve , AI MRM framework s will need to be regularly \n\nreviewed and updated , and risk management efforts scale d up based on the state \n\nof AI use. Aside from AI MRM, controls in non AI -speci fic areas such as general \n\ndata governance and management, technology, cyber and third party risk \n\nmanagement, and legal and compliance will also need to be reviewed to take AI \n\ndevelopments into account. Artificial Intelligence Model Risk Management | 45 \n\n8.2.  As the AI landscape continues to evolve, MAS will continue to work with the \n\nindustry to help facilitate and uplift AI and Gen erative AI governance and risk \n\nmanagement efforts across the financial industry, through information sharing \n\nefforts such as this paper to promulgat e industry best practices, and industry \n\ncollaborations such as Project Mind Forge. MAS is also considering supervisory \n\nguidance for all FIs next year, building upon the focus areas covered in this \n\ninformation paper. Artificial Intelligence Model Risk Management | 46 \n\n# Annex A - Definitions \n\nâ€¢ Model â€“ A model  is a method, system or approach which converts assumptions and \n\ninput data into quantitative estimates, decisions, or decision recommendations \n\n(based on the Global Associate of Ris k Professionals â€™ definition of a model ). Apart \n\nfrom AI models , which typically refer to machine or deep learning models which we \n\ndefine below , banks also routinely utilise conventional models , such as economic, \n\nfinancial, or statistical models. Some models , such as logistic regression model s, are \n\ncommonly used in both statistical and AI fields and may be regarded as both AI and \n\nconventional model s. \n\n> â€¢\n\nArtificial Intelligence (AI) â€“ An  AI system is a machine -based system that, for explicit \n\nor implicit objectives, infers, from the input it receives, how to generate outputs such \n\nas predictions, content, recommendations, or decisions that can influence physical \n\nor virtual environments. Different AI systems vary in their levels of autonomy and \n\nadaptiveness after deployment  (based on the Organisation for Economic Co -\n\noperation and Development â€™s definition of AI ). Such a definition would include \n\nGenerative AI. An  AI  or Generative AI  system  can be based on one or multiple AI or \n\nGenerative AI models and may also involve other machine -based components. \n\nâ€¢ AI Use Case â€“ An AI or Generative AI use case usually refers to a  specific real -world \n\ncontext that the AI or Generative AI model or system is applied to. For example, an \n\nAI recommendation model or system that is applied to a financial product \n\nrecommendation use case. \n\nâ€¢ Machine learning â€“ Machine learning is a subset of AI where the AI directly learn s\n\nfrom data. The machine learning model learns model parameters (or model weights) \n\nto transform inputs into estimates or outputs from the data by updating these \n\nparameters iteratively based on an objective. For example, the machine learning \n\nmodel may be provided with histori cal data that consists of the information on \n\ncustomer s, e.g., income and existing value of debt (which we refer to as input data) ,\n\nand whether the customer had defaulted on a loan obligation (which we refer to as \n\nthe target variable or label) . The machine learning model can then be trained by \n\nlearning model parameters that allow it to transform input data to target variables \n\nor labels with maximum accuracy (or minimum error). Artificial Intelligence Model Risk Management | 47 \n\nâ€¢ Deep learning â€“ Deep learning is a subset of machine learning, usually based on \n\nneural networks (that were inspired by how neurons in the brain recognise complex \n\npatterns in data) that comprise multiple layers of neurons. Deep learning models are \n\nable to learn more comple x patterns due to the many layers of neurons in the model. \n\nâ€¢ Discriminative versus Generative AI models â€“ AI models that generate predictions, \n\ne.g., predicting a credit default based on customer information, or recommending a \n\nfinancial product based on customer information, are usually referred to as \n\ndiscriminative AI models. This is in contrast to Generative AI models that are usually \n\nused to generat e content such as text, images, audio or videos .\n\nâ€¢ CI/CD , DevOps, MLOps, AIOps, LLMOps â€“ Continuous integration/continuous \n\ndeployment (CI/CD) or DevOps pipelines automate the process of building, testing, \n\nand deploying code changes. These terms are closely related to the term MLOps, \n\nwhich is used to describe tools and systems that help to automate the process of \n\nbuilding, testing, deploying and monitoring the performance of machine learning \n\nsystems. More recent terms such as AIOps and LLMOps have also been used to \n\ndescribe such tools and systems for AI in g eneral or for Large Language Models \n\n(LLM). \n\nâ€¢ Data Drift - This occurs when the statistical properties of the distribution of the data \n\nchanges. For example, the underlying distribution of customer data may have drifted \n\nor changed over time due to changes in the lifestyles of customers . Hence, an AI \n\nmodel that was trained on data from a more distant time period may not perform \n\nas well on data from a more recent time period due to data drift . A common measure \n\nof how much a population distribution has changed over time is the Population \n\nStability Index (PSI). \n\nâ€¢ Concept Drift - This occurs when the underlying relationships between the features \n\nin input data and what the AI model is being used to predict or generate changes .\n\nFor example, customer preferences for financial products may have shifted due to \n\nbroad industry changes ( e.g., a shift in the relationships between customer \n\ninformation and their preferences for financial products ), and an AI model used to \n\ngenerate financial product recommendations may no longer perform as well due to Artificial Intelligence Model Risk Management | 48 \n\nsuch  concept drift s. A common measure of concept drift is the Characteristic \n\nStability Index (CSI). \n\nâ€¢ Model Drift - Model drift is a broader term that usually encompasses both data drift \n\nand concept drift, as well as other factors that can cause a model's performance to \n\ndegrade over time. Aside from measures such as PSI and CSI, monitoring the \n\nstatistical characteristics of AI predictions can also be used to detect drifts in general. \n\nâ€¢ Supervised learning â€“ Supervised learning is a machine learning approach where a\n\nmodel is trained on a labelled dataset. In this process , each data point  includes input \n\nfeatures paired with the corresponding output (label).  The model learns to map \n\ninputs to outputs  by comparing its predictions with the actual labels and updating \n\nthe model parameters iteratively . Classification , which involves the prediction of \n\nclasses or categories, and regression , which involves the prediction of continuous \n\nvalues, are common examples of supervised learning. \n\nâ€¢ Unsupervised learning â€“ Unsupervised learning is a machine learning approach \n\nwhere a model discovers patterns in data without the use of label s. An e xample of \n\nunsupervised learning is clustering, where data points are grouped together based \n\non their inherent similarities or dissimilarities .Artificial Intelligence Model Risk Management | 49 \n\n# Annex B - Useful References \n\nPublications for the Financial Sector issued by MAS \n\nâ€¢ MAS FEAT Principles:  https://www.mas.gov.sg/publications/monographs -or -information -\n\npaper/2018/feat \n\nâ€¢ Veritas Initiative : https://www.mas.gov.sg/schemes -and -initiatives/veritas \n\nâ€¢ Project MindForge: https://www.mas.gov.sg/schemes -and -initiatives/project -mindforge \n\nâ€¢ Information Paper on Implementation of Fairness Principles in Financial Institutionsâ€™ use of \n\nArtificial Intelligence/Machine Learning:  https://www.mas.gov.sg/publications/monographs -or -\n\ninformation -paper/2022/implementation -of -fairness -principles -in -financial -institutions -use -of -\n\nartificial -intelligence -and -machine -learning \n\nâ€¢ Information Paper on Cyber Risks Associated with Generative Artificial Intelligence: \n\nhttps://www.mas.gov.sg/regulation/circulars/cyber -risks -associated -with -generative -artificial -\n\nintelligence \n\nâ€¢ Information Paper on Data Governance and Management Practices: \n\nhttps://www.mas.gov.sg/publications/monographs -or -information -paper/2024/data -\n\ngovernance -and -management -practices \n\nâ€¢ Technology Risk Management Guidelines :\n\nhttps://www.mas.gov.sg/regulation/guidelines/technology -risk -management -guidelines \n\nâ€¢ Business Continuity Management Guidelines :\n\nhttps://www.mas.gov.sg/regulation/guidelines/guidelines -on -business -continuity -management \n\nâ€¢ Notice and Guidelines on Third -Party Risk Management :\n\nhttps://www.mas.gov.sg/regulation/third -party -risk -management \n\nâ€¢ Information Paper on Operational Risk Management - Management of Third Party Arrangements: \n\nhttps://www.mas.gov.sg/publications/monographs -or -information -paper/2022/operational -risk -\n\nmanagement ---management -of -third -party -arrangements Artificial Intelligence Model Risk Management | 50 \n\nNon -Financial Sector Specific Publications \n\nâ€¢ AI Verify : AI governance testing framework and software toolkit: \n\nhttps://www.aiverifyfoundation.sg/what -is -ai -verify/ \n\nâ€¢ Project Moonshot:  https://www.aiverifyfoundation.sg/project -moonshot/ \n\nâ€¢ Model Governance Framework for Generative AI :\n\nhttps://www.aiverifyfoundation.sg/resources/mgf -gen -ai/ \n\nâ€¢ Trusted Data Sharing Framework:  https://www.imda.gov.sg/how -we -can -help/data -\n\ninnovation/trusted -data -sharing -framework \n\nâ€¢ Personal Data Protection Act (PDPA):  https://www.pdpc.gov.sg/overview -of -pdpa/the -\n\nlegislation/personal -data -protection -act \n\nâ€¢ Advisory Guidelines on use of Personal Data in AI Recommendation and Decision Systems :\n\nhttps://www.pdpc.gov.sg/guidelines -and -consultation/2024/02/advisory -guidelines -on -use -of -\n\npersonal -data -in -ai -recommendation -and -decision -systems \n\nâ€¢ Guidelines and Companion Guide on Securing AI Systems : https://www.csa.gov.sg/Tips -\n\nResource/publications/2024/guidelines -on -securing -ai", "fetched_at_utc": "2026-02-08T18:49:34Z", "sha256": "aa4ced6d9f7b41ac579e07dea2f87952b5eb3158f3bd0de54ba0162322b0fc36", "meta": {"file_name": "AI Risk Management - Singapore.pdf", "file_size": 765210, "relative_path": "pdfs\\AI Risk Management - Singapore.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 22410}}}
{"doc_id": "pdf-pdfs-ai-risk-management-framework-nist-92288e1fcca5", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Risk Management Framework - NIST.pdf", "title": "AI Risk Management Framework - NIST", "text": "# NIST AI 100-1 \n\n# Artificial Intelligence Risk Management Framework (AI RMF 1.0) NIST AI 100-1 \n\n# Artificial Intelligence Risk Management Framework (AI RMF 1.0) \n\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1 January 2023 \n\nU.S. Department of Commerce \n\nGina M. Raimondo, Secretary \n\nNational Institute of Standards and Technology \n\nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology Certain commercial entities, equipment, or materials may be identified in this document in order to describe \n\nan experimental procedure or concept adequately. Such identification is not intended to imply recommenda-tion or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that \n\nthe entities, materials, or equipment are necessarily the best available for the purpose. \n\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1 Update Schedule and Versions \n\nThe Artificial Intelligence Risk Management Framework (AI RMF) is intended to be a living document. NIST will review the content and usefulness of the Framework regularly to determine if an update is appro-priate; a review with formal input from the AI community is expected to take place no later than 2028. The Framework will employ a two-number versioning system to track and identify major and minor changes. The first number will represent the generation of the AI RMF and its companion documents (e.g., 1.0) and will change only with major revisions. Minor revisions will be tracked using â€œ.nâ€ after the generation number (e.g., 1.1). All changes will be tracked using a Version Control Table which identifies the history, including version number, date of change, and description of change. NIST plans to update the AI RMF Playbook frequently. Comments on the AI RMF Playbook may be sent via email to AIframework@nist.gov at any time and will be reviewed and integrated on a semi-annual basis. Table of Contents \n\nExecutive Summary 1Part 1: Foundational Information 41 Framing Risk 4\n\n1.1 Understanding and Addressing Risks, Impacts, and Harms 41.2 Challenges for AI Risk Management 51.2.1 Risk Measurement 51.2.2 Risk Tolerance 71.2.3 Risk Prioritization 71.2.4 Organizational Integration and Management of Risk 8\n\n2 Audience 93 AI Risks and Trustworthiness 12 \n\n3.1 Valid and Reliable 13 3.2 Safe 14 3.3 Secure and Resilient 15 3.4 Accountable and Transparent 15 3.5 Explainable and Interpretable 16 3.6 Privacy-Enhanced 17 3.7 Fair â€“ with Harmful Bias Managed 17 \n\n4 Effectiveness of the AI RMF 19 Part 2: Core and Profiles 20 5 AI RMF Core 20 \n\n5.1 Govern 21 5.2 Map 24 5.3 Measure 28 5.4 Manage 31 \n\n6 AI RMF Profiles 33 Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3 35 Appendix B: How AI Risks Differ from Traditional Software Risks 38 Appendix C: AI Risk Management and Human-AI Interaction 40 Appendix D: Attributes of the AI RMF 42 \n\n# List of Tables \n\nTable 1 Categories and subcategories for the GOVERN function. 22 Table 2 Categories and subcategories for the MAP function. 26 Table 3 Categories and subcategories for the MEASURE function. 29 Table 4 Categories and subcategories for the MANAGE function. 32 iNIST AI 100-1 AI RMF 1.0 \n\n# List of Figures \n\nFig. 1 Examples of potential harms related to AI systems. Trustworthy AI systems and their responsible use can mitigate negative risks and contribute to bene-fits for people, organizations, and ecosystems. 5Fig. 2 Lifecycle and Key Dimensions of an AI System. Modified from OECD (2022) OECD Framework for the Classification of AI systems â€” OECD Digital Economy Papers. The two inner circles show AI systemsâ€™ key di-mensions and the outer circle shows AI lifecycle stages. Ideally, risk man-agement efforts start with the Plan and Design function in the application context and are performed throughout the AI system lifecycle. See Figure 3 for representative AI actors. 10 Fig. 3 AI actors across AI lifecycle stages. See Appendix A for detailed descrip-tions of AI actor tasks, including details about testing, evaluation, verifica-tion, and validation tasks. Note that AI actors in the AI Model dimension (Figure 2) are separated as a best practice, with those building and using the models separated from those verifying and validating the models. 11 Fig. 4 Characteristics of trustworthy AI systems. Valid & Reliable is a necessary condition of trustworthiness and is shown as the base for other trustworthi-ness characteristics. Accountable & Transparent is shown as a vertical box because it relates to all other characteristics. 12 Fig. 5 Functions organize AI risk management activities at their highest level to govern, map, measure, and manage AI risks. Governance is designed to be a cross-cutting function to inform and be infused throughout the other three functions. 20  \n\n> Page ii NIST AI 100-1\n\nAI RMF 1.0 \n\n# Executive Summary \n\nArtificial intelligence (AI) technologies have significant potential to transform society and peopleâ€™s lives â€“ from commerce and health to transportation and cybersecurity to the envi-ronment and our planet. AI technologies can drive inclusive economic growth and support scientific advancements that improve the conditions of our world. AI technologies, how-ever, also pose risks that can negatively impact individuals, groups, organizations, commu-nities, society, the environment, and the planet. Like risks for other types of technology, AI risks can emerge in a variety of ways and can be characterized as long- or short-term, high-or low-probability, systemic or localized, and high- or low-impact. The AI RMF refers to an AI system as an engineered or machine-based system that can, for a given set of objectives, generate outputs such as predictions, recommenda-tions, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy (Adapted from: OECD Recommendation on AI:2019; ISO /IEC 22989:2022). While there are myriad standards and best practices to help organizations mitigate the risks of traditional software or information-based systems, the risks posed by AI systems are in many ways unique (See Appendix B). AI systems, for example, may be trained on data that can change over time, sometimes significantly and unexpectedly, affecting system function-ality and trustworthiness in ways that are hard to understand. AI systems and the contexts in which they are deployed are frequently complex, making it difficult to detect and respond to failures when they occur. AI systems are inherently socio-technical in nature, meaning they are influenced by societal dynamics and human behavior. AI risks â€“ and benefits â€“ can emerge from the interplay of technical aspects combined with societal factors related to how a system is used, its interactions with other AI systems, who operates it, and the social context in which it is deployed. These risks make AI a uniquely challenging technology to deploy and utilize both for orga-nizations and within society. Without proper controls, AI systems can amplify, perpetuate, or exacerbate inequitable or undesirable outcomes for individuals and communities. With proper controls, AI systems can mitigate and manage inequitable outcomes. AI risk management is a key component of responsible development and use of AI sys-tems. Responsible AI practices can help align the decisions about AI system design, de-velopment, and uses with intended aim and values. Core concepts in responsible AI em-phasize human centricity, social responsibility, and sustainability. AI risk management can drive responsible uses and practices by prompting organizations and their internal teams who design, develop, and deploy AI to think more critically about context and potential or unexpected negative and positive impacts. Understanding and managing the risks of AI systems will help to enhance trustworthiness, and in turn, cultivate public trust.  \n\n> Page 1 NIST AI 100-1\n\nAI RMF 1.0 \n\nSocial responsibility can refer to the organizationâ€™s responsibility â€œfor the impacts of its decisions and activities on society and the environment through transparent and ethical behaviorâ€ ( ISO 26000:2010). Sustainability refers to the â€œstate of the global system, including environmental, social, and economic aspects, in which the needs of the present are met without compromising the ability of future generations to meet their own needsâ€ ( ISO /IEC TR 24368:2022). Responsible AI is meant to result in technology that is also equitable and accountable. The expectation is that organizational practices are carried out in accord with â€œ professional responsibility ,â€ defined by ISO as an approach that â€œaims to ensure that professionals who design, develop, or deploy AI systems and applications or AI-based products or systems, recognize their unique position to exert influence on people, society, and the future of AIâ€ ( ISO /IEC TR 24368:2022). As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283), the goal of the AI RMF is to offer a resource to the organizations designing, developing, deploying, or using AI systems to help manage the many risks of AI and promote trustwor-thy and responsible development and use of AI systems. The Framework is intended to be \n\nvoluntary , rights-preserving, non-sector-specific, and use-case agnostic, providing flexibil-ity to organizations of all sizes and in all sectors and throughout society to implement the approaches in the Framework. The Framework is designed to equip organizations and individuals â€“ referred to here as \n\nAI actors â€“ with approaches that increase the trustworthiness of AI systems, and to help foster the responsible design, development, deployment, and use of AI systems over time. AI actors are defined by the Organisation for Economic Co-operation and Development (OECD) as â€œthose who play an active role in the AI system lifecycle, including organiza-tions and individuals that deploy or operate AIâ€ [OECD (2019) Artificial Intelligence in Societyâ€”OECD iLibrary] (See Appendix A). The AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies continue to develop, and to be operationalized by organizations in varying degrees and capacities so society can benefit from AI while also being protected from its potential harms. The Framework and supporting resources will be updated, expanded, and improved based on evolving technology, the standards landscape around the world, and AI community ex-perience and feedback. NIST will continue to align the AI RMF and related guidance with applicable international standards, guidelines, and practices. As the AI RMF is put into use, additional lessons will be learned to inform future updates and additional resources. The Framework is divided into two parts. Part 1 discusses how organizations can frame the risks related to AI and describes the intended audience. Next, AI risks and trustworthi-ness are analyzed, outlining the characteristics of trustworthy AI systems, which include  \n\n> Page 2 NIST AI 100-1\n\nAI RMF 1.0 valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy enhanced, and fair with their harmful biases managed. Part 2 comprises the â€œCoreâ€ of the Framework. It describes four specific functions to help organizations address the risks of AI systems in practice. These functions â€“ GOVERN ,\n\n> MAP\n\n, MEASURE , and MANAGE â€“ are broken down further into categories and subcate-gories. While GOVERN applies to all stages of organizationsâ€™ AI risk management pro-cesses and procedures, the MAP , MEASURE , and MANAGE functions can be applied in AI system-specific contexts and at specific stages of the AI lifecycle. Additional resources related to the Framework are included in the AI RMF Playbook, which is available via the NIST AI RMF website: https://www.nist.gov/itl/ai-risk-management-framework. Development of the AI RMF by NIST in collaboration with the private and public sec-tors is directed and consistent with its broader AI efforts called for by the National AI Initiative Act of 2020, the National Security Commission on Artificial Intelligence recom-mendations, and the Plan for Federal Engagement in Developing Technical Standards and Related Tools. Engagement with the AI community during this Frameworkâ€™s development â€“ via responses to a formal Request for Information, three widely attended workshops, public comments on a concept paper and two drafts of the Framework, discussions at mul-tiple public forums, and many small group meetings â€“ has informed development of the AI RMF 1.0 as well as AI research and development and evaluation conducted by NIST and others. Priority research and additional guidance that will enhance this Framework will be captured in an associated AI Risk Management Framework Roadmap to which NIST and the broader community can contribute.  \n\n> Page 3 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Part 1: Foundational Information \n\n# 1. Framing Risk \n\nAI risk management offers a path to minimize potential negative impacts of AI systems, such as threats to civil liberties and rights, while also providing opportunities to maximize positive impacts. Addressing, documenting, and managing AI risks and potential negative impacts effectively can lead to more trustworthy AI systems. \n\n1.1 Understanding and Addressing Risks, Impacts, and Harms \n\nIn the context of the AI RMF, risk refers to the composite measure of an eventâ€™s probability of occurring and the magnitude or degree of the consequences of the corresponding event. The impacts, or consequences, of AI systems can be positive, negative, or both and can result in opportunities or threats (Adapted from: ISO 31000:2018). When considering the negative impact of a potential event, risk is a function of 1) the negative impact, or magni-tude of harm, that would arise if the circumstance or event occurs and 2) the likelihood of occurrence (Adapted from: OMB Circular A-130:2016). Negative impact or harm can be experienced by individuals, groups, communities, organizations, society, the environment, and the planet. â€œRisk management refers to coordinated activities to direct and control an organiza-tion with regard to riskâ€ (Source: ISO 31000:2018). While risk management processes generally address negative impacts, this Framework of-fers approaches to minimize anticipated negative impacts of AI systems and identify op-portunities to maximize positive impacts. Effectively managing the risk of potential harms could lead to more trustworthy AI systems and unleash potential benefits to people (individ-uals, communities, and society), organizations, and systems/ecosystems. Risk management can enable AI developers and users to understand impacts and account for the inherent lim-itations and uncertainties in their models and systems, which in turn can improve overall system performance and trustworthiness and the likelihood that AI technologies will be used in ways that are beneficial. The AI RMF is designed to address new risks as they emerge. This flexibility is particularly important where impacts are not easily foreseeable and applications are evolving. While some AI risks and benefits are well-known, it can be challenging to assess negative impacts and the degree of harms. Figure 1 provides examples of potential harms that can be related to AI systems. AI risk management efforts should consider that humans may assume that AI systems work â€“ and work well â€“ in all settings. For example, whether correct or not, AI systems are often perceived as being more objective than humans or as offering greater capabilities than general software.  \n\n> Page 4 NIST AI 100-1\n\nAI RMF 1.0  \n\n> Fig. 1. Examples of potential harms related to AI systems. Trustworthy AI systems and their responsible use can mitigate negative risks and contribute to benefits for people, organizations, and ecosystems.\n\n1.2 Challenges for AI Risk Management \n\nSeveral challenges are described below. They should be taken into account when managing risks in pursuit of AI trustworthiness. \n\n1.2.1 Risk Measurement \n\nAI risks or failures that are not well-defined or adequately understood are difficult to mea-sure quantitatively or qualitatively. The inability to appropriately measure AI risks does not imply that an AI system necessarily poses either a high or low risk. Some risk measurement challenges include: \n\nRisks related to third-party software, hardware, and data: Third-party data or systems can accelerate research and development and facilitate technology transition. They also may complicate risk measurement. Risk can emerge both from third-party data, software or hardware itself and how it is used. Risk metrics or methodologies used by the organization developing the AI system may not align with the risk metrics or methodologies uses by the organization deploying or operating the system. Also, the organization developing the AI system may not be transparent about the risk metrics or methodologies it used. Risk measurement and management can be complicated by how customers use or integrate third-party data or systems into AI products or services, particularly without sufficient internal governance structures and technical safeguards. Regardless, all parties and AI actors should manage risk in the AI systems they develop, deploy, or use as standalone or integrated components. \n\nTracking emergent risks: Organizationsâ€™ risk management efforts will be enhanced by identifying and tracking emergent risks and considering techniques for measuring them.  \n\n> Page 5 NIST AI 100-1\n\nAI RMF 1.0 AI system impact assessment approaches can help AI actors understand potential impacts or harms within specific contexts. \n\nAvailability of reliable metrics: The current lack of consensus on robust and verifiable measurement methods for risk and trustworthiness, and applicability to different AI use cases, is an AI risk measurement challenge. Potential pitfalls when seeking to measure negative risk or harms include the reality that development of metrics is often an institu-tional endeavor and may inadvertently reflect factors unrelated to the underlying impact. In addition, measurement approaches can be oversimplified, gamed, lack critical nuance, be-come relied upon in unexpected ways, or fail to account for differences in affected groups and contexts. Approaches for measuring impacts on a population work best if they recognize that contexts matter, that harms may affect varied groups or sub-groups differently, and that communities or other sub-groups who may be harmed are not always direct users of a system. \n\nRisk at different stages of the AI lifecycle: Measuring risk at an earlier stage in the AI lifecycle may yield different results than measuring risk at a later stage; some risks may be latent at a given point in time and may increase as AI systems adapt and evolve. Fur-thermore, different AI actors across the AI lifecycle can have different risk perspectives. For example, an AI developer who makes AI software available, such as pre-trained mod-els, can have a different risk perspective than an AI actor who is responsible for deploying that pre-trained model in a specific use case. Such deployers may not recognize that their particular uses could entail risks which differ from those perceived by the initial developer. All involved AI actors share responsibilities for designing, developing, and deploying a trustworthy AI system that is fit for purpose. \n\nRisk in real-world settings: While measuring AI risks in a laboratory or a controlled environment may yield important insights pre-deployment, these measurements may differ from risks that emerge in operational, real-world settings. \n\nInscrutability: Inscrutable AI systems can complicate risk measurement. Inscrutability can be a result of the opaque nature of AI systems (limited explainability or interpretabil-ity), lack of transparency or documentation in AI system development or deployment, or inherent uncertainties in AI systems. \n\nHuman baseline: Risk management of AI systems that are intended to augment or replace human activity, for example decision making, requires some form of baseline metrics for comparison. This is difficult to systematize since AI systems carry out different tasks â€“ and perform tasks differently â€“ than humans.  \n\n> Page 6 NIST AI 100-1\n\nAI RMF 1.0 \n\n1.2.2 Risk Tolerance \n\nWhile the AI RMF can be used to prioritize risk, it does not prescribe risk tolerance. Risk tolerance refers to the organizationâ€™s or AI actorâ€™s (see Appendix A) readiness to bear the risk in order to achieve its objectives. Risk tolerance can be influenced by legal or regula-tory requirements (Adapted from: ISO GUIDE 73). Risk tolerance and the level of risk that is acceptable to organizations or society are highly contextual and application and use-case specific. Risk tolerances can be influenced by policies and norms established by AI sys-tem owners, organizations, industries, communities, or policy makers. Risk tolerances are likely to change over time as AI systems, policies, and norms evolve. Different organiza-tions may have varied risk tolerances due to their particular organizational priorities and resource considerations. Emerging knowledge and methods to better inform harm/cost-benefit tradeoffs will con-tinue to be developed and debated by businesses, governments, academia, and civil society. To the extent that challenges for specifying AI risk tolerances remain unresolved, there may be contexts where a risk management framework is not yet readily applicable for mitigating negative AI risks. The Framework is intended to be flexible and to augment existing risk practices which should align with applicable laws, regulations, and norms. Organizations should follow existing regulations and guidelines for risk criteria, tolerance, and response established by organizational, domain, discipline, sector, or professional requirements. Some sectors or industries may have established definitions of harm or established documentation, reporting, and disclosure requirements. Within sectors, risk management may depend on existing guidelines for specific applications and use case settings. Where established guidelines do not exist, organizations should define reasonable risk tolerance. Once tolerance is defined, this AI RMF can be used to manage risks and to document risk management processes. \n\n1.2.3 Risk Prioritization \n\nAttempting to eliminate negative risk entirely can be counterproductive in practice because not all incidents and failures can be eliminated. Unrealistic expectations about risk may lead organizations to allocate resources in a manner that makes risk triage inefficient or impractical or wastes scarce resources. A risk management culture can help organizations recognize that not all AI risks are the same, and resources can be allocated purposefully. Actionable risk management efforts lay out clear guidelines for assessing trustworthiness of each AI system an organization develops or deploys. Policies and resources should be prioritized based on the assessed risk level and potential impact of an AI system. The extent to which an AI system may be customized or tailored to the specific context of use by the AI deployer can be a contributing factor.  \n\n> Page 7 NIST AI 100-1\n\nAI RMF 1.0 When applying the AI RMF, risks which the organization determines to be highest for the AI systems within a given context of use call for the most urgent prioritization and most thorough risk management process. In cases where an AI system presents unacceptable negative risk levels â€“ such as where significant negative impacts are imminent, severe harms are actually occurring, or catastrophic risks are present â€“ development and deployment should cease in a safe manner until risks can be sufficiently managed. If an AI systemâ€™s development, deployment, and use cases are found to be low-risk in a specific context, that may suggest potentially lower prioritization. Risk prioritization may differ between AI systems that are designed or deployed to directly interact with humans as compared to AI systems that are not. Higher initial prioritization may be called for in settings where the AI system is trained on large datasets comprised of sensitive or protected data such as personally identifiable information, or where the outputs of the AI systems have direct or indirect impact on humans. AI systems designed to interact only with computational systems and trained on non-sensitive datasets (for example, data collected from the physical environment) may call for lower initial prioritization. Nonethe-less, regularly assessing and prioritizing risk based on context remains important because non-human-facing AI systems can have downstream safety or social implications. \n\nResidual risk â€“ defined as risk remaining after risk treatment (Source: ISO GUIDE 73) â€“ directly impacts end users or affected individuals and communities. Documenting residual risks will call for the system provider to fully consider the risks of deploying the AI product and will inform end users about potential negative impacts of interacting with the system. \n\n1.2.4 Organizational Integration and Management of Risk \n\nAI risks should not be considered in isolation. Different AI actors have different responsi-bilities and awareness depending on their roles in the lifecycle. For example, organizations developing an AI system often will not have information about how the system may be used. AI risk management should be integrated and incorporated into broader enterprise risk management strategies and processes. Treating AI risks along with other critical risks, such as cybersecurity and privacy, will yield a more integrated outcome and organizational efficiencies. The AI RMF may be utilized along with related guidance and frameworks for managing AI system risks or broader enterprise risks. Some risks related to AI systems are common across other types of software development and deployment. Examples of overlapping risks include: privacy concerns related to the use of underlying data to train AI systems; the en-ergy and environmental implications associated with resource-heavy computing demands; security concerns related to the confidentiality, integrity, and availability of the system and its training and output data; and general security of the underlying software and hardware for AI systems.  \n\n> Page 8 NIST AI 100-1\n\nAI RMF 1.0 Organizations need to establish and maintain the appropriate accountability mechanisms, roles and responsibilities, culture, and incentive structures for risk management to be ef-fective. Use of the AI RMF alone will not lead to these changes or provide the appropriate incentives. Effective risk management is realized through organizational commitment at senior levels and may require cultural change within an organization or industry. In addi-tion, small to medium-sized organizations managing AI risks or implementing the AI RMF may face different challenges than large organizations, depending on their capabilities and resources. \n\n# 2. Audience \n\nIdentifying and managing AI risks and potential impacts â€“ both positive and negative â€“ re-quires a broad set of perspectives and actors across the AI lifecycle. Ideally, AI actors will represent a diversity of experience, expertise, and backgrounds and comprise demograph-ically and disciplinarily diverse teams. The AI RMF is intended to be used by AI actors across the AI lifecycle and dimensions. The OECD has developed a framework for classifying AI lifecycle activities according to five key socio-technical dimensions, each with properties relevant for AI policy and gover-nance, including risk management [OECD (2022) OECD Framework for the Classification of AI systems â€” OECD Digital Economy Papers]. Figure 2 shows these dimensions, slightly modified by NIST for purposes of this framework. The NIST modification high-lights the importance of test, evaluation, verification, and validation (TEVV) processes throughout an AI lifecycle and generalizes the operational context of an AI system. AI dimensions displayed in Figure 2 are the Application Context, Data and Input, AI Model, and Task and Output. AI actors involved in these dimensions who perform or manage the design, development, deployment, evaluation, and use of AI systems and drive AI risk management efforts are the primary AI RMF audience. Representative AI actors across the lifecycle dimensions are listed in Figure 3 and described in detail in Appendix A. Within the AI RMF, all AI actors work together to manage risks and achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific expertise are integrated throughout the AI lifecycle and are especially likely to benefit from the Framework. Performed regularly, TEVV tasks can provide insights relative to technical, societal, legal, and ethical standards or norms, and can assist with anticipating impacts and assessing and tracking emergent risks. As a regular process within an AI lifecycle, TEVV allows for both mid-course remediation and post-hoc risk management. The People & Planet dimension at the center of Figure 2 represents human rights and the broader well-being of society and the planet. The AI actors in this dimension comprise a separate AI RMF audience who informs the primary audience. These AI actors may in-clude trade associations, standards developing organizations, researchers, advocacy groups,  \n\n> Page 9 NIST AI 100-1\n\nAI RMF 1.0  \n\n> Fig. 2. Lifecycle and Key Dimensions of an AI System. Modified from OECD (2022) OECD Framework for the Classification of AI systems â€” OECD Digital Economy Papers. The two inner circles show AI systemsâ€™ key dimensions and the outer circle shows AI lifecycle stages. Ideally, risk management efforts start with the Plan and Design function in the application context and are performed throughout the AI system lifecycle. See Figure 3 for representative AI actors.\n\nenvironmental groups, civil society organizations, end users, and potentially impacted in-dividuals and communities. These actors can: â€¢ assist in providing context and understanding potential and actual impacts; â€¢ be a source of formal or quasi-formal norms and guidance for AI risk management; â€¢ designate boundaries for AI operation (technical, societal, legal, and ethical); and â€¢ promote discussion of the tradeoffs needed to balance societal values and priorities related to civil liberties and rights, equity, the environment and the planet, and the economy. Successful risk management depends upon a sense of collective responsibility among AI actors shown in Figure 3. The AI RMF functions, described in Section 5, require diverse perspectives, disciplines, professions, and experiences. Diverse teams contribute to more open sharing of ideas and assumptions about the purposes and functions of technology â€“ making these implicit aspects more explicit. This broader collective perspective creates opportunities for surfacing problems and identifying existing and emergent risks.  \n\n> Page 10 NIST AI 100-1\n\nAI RMF 1.0  \n\n> Fig. 3.  AI actors across AI lifecycle stages. See Appendix A for detailed descriptions of AI actor tasks, including details about testing, evaluation, verification, and validation tasks. Note that AI actors in the AI Model dimension (Figure 2) are separated as a best practice, with those building and using the models separated from those verifying and validating the models.\n> Page 11 NIST AI 100-1\n\nAI RMF 1.0 \n\n# 3. AI Risks and Trustworthiness \n\nFor AI systems to be trustworthy, they often need to be responsive to a multiplicity of cri-teria that are of value to interested parties. Approaches which enhance AI trustworthiness can reduce negative AI risks. This Framework articulates the following characteristics of trustworthy AI and offers guidance for addressing them. Characteristics of trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and trans-parent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. Creating trustworthy AI requires balancing each of these characteristics based on the AI systemâ€™s context of use. While all characteristics are socio-technical system at-tributes, accountability and transparency also relate to the processes and activities internal to an AI system and its external setting. Neglecting these characteristics can increase the probability and magnitude of negative consequences.  \n\n> Fig. 4. Characteristics of trustworthy AI systems. Valid & Reliable is a necessary condition of trustworthiness and is shown as the base for other trustworthiness characteristics. Accountable & Transparent is shown as a vertical box because it relates to all other characteristics.\n\nTrustworthiness characteristics (shown in Figure 4) are inextricably tied to social and orga-nizational behavior, the datasets used by AI systems, selection of AI models and algorithms and the decisions made by those who build them, and the interactions with the humans who provide insight from and oversight of such systems. Human judgment should be employed when deciding on the specific metrics related to AI trustworthiness characteristics and the precise threshold values for those metrics. Addressing AI trustworthiness characteristics individually will not ensure AI system trust-worthiness; tradeoffs are usually involved, rarely do all characteristics apply in every set-ting, and some will be more or less important in any given situation. Ultimately, trustwor-thiness is a social concept that ranges across a spectrum and is only as strong as its weakest characteristics. When managing AI risks, organizations can face difficult decisions in balancing these char-acteristics. For example, in certain scenarios tradeoffs may emerge between optimizing for interpretability and achieving privacy. In other cases, organizations might face a tradeoff between predictive accuracy and interpretability. Or, under certain conditions such as data sparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions  \n\n> Page 12 NIST AI 100-1\n\nAI RMF 1.0 about fairness and other values in certain domains. Dealing with tradeoffs requires tak-ing into account the decision-making context. These analyses can highlight the existence and extent of tradeoffs between different measures, but they do not answer questions about how to navigate the tradeoff. Those depend on the values at play in the relevant context and should be resolved in a manner that is both transparent and appropriately justifiable. There are multiple approaches for enhancing contextual awareness in the AI lifecycle. For example, subject matter experts can assist in the evaluation of TEVV findings and work with product and deployment teams to align TEVV parameters to requirements and de-ployment conditions. When properly resourced, increasing the breadth and diversity of input from interested parties and relevant AI actors throughout the AI lifecycle can en-hance opportunities for informing contextually sensitive evaluations, and for identifying AI system benefits and positive impacts. These practices can increase the likelihood that risks arising in social contexts are managed appropriately. Understanding and treatment of trustworthiness characteristics depends on an AI actorâ€™s particular role within the AI lifecycle. For any given AI system, an AI designer or developer may have a different perception of the characteristics than the deployer. Trustworthiness characteristics explained in this document influence each other. Highly secure but unfair systems, accurate but opaque and uninterpretable systems, and inaccurate but secure, privacy-enhanced, and transparent systems are all unde-sirable. A comprehensive approach to risk management calls for balancing tradeoffs among the trustworthiness characteristics. It is the joint responsibility of all AI ac-tors to determine whether AI technology is an appropriate or necessary tool for a given context or purpose, and how to use it responsibly. The decision to commission or deploy an AI system should be based on a contextual assessment of trustworthi-ness characteristics and the relative risks, impacts, costs, and benefits, and informed by a broad set of interested parties. \n\n3.1 Valid and Reliable \n\nValidation is the â€œconfirmation, through the provision of objective evidence, that the re-quirements for a specific intended use or application have been fulfilledâ€ (Source: ISO \n\n9000:2015). Deployment of AI systems which are inaccurate, unreliable, or poorly gener-alized to data and settings beyond their training creates and increases negative AI risks and reduces trustworthiness. \n\nReliability is defined in the same standard as the â€œability of an item to perform as required, without failure, for a given time interval, under given conditionsâ€ (Source: ISO /IEC TS \n\n5723:2022). Reliability is a goal for overall correctness of AI system operation under the conditions of expected use and over a given period of time, including the entire lifetime of the system.  \n\n> Page 13 NIST AI 100-1\n\nAI RMF 1.0 Accuracy and robustness contribute to the validity and trustworthiness of AI systems, and can be in tension with one another in AI systems. \n\nAccuracy is defined by ISO /IEC TS 5723:2022 as â€œcloseness of results of observations, computations, or estimates to the true values or the values accepted as being true.â€ Mea-sures of accuracy should consider computational-centric measures (e.g., false positive and false negative rates), human-AI teaming, and demonstrate external validity (generalizable beyond the training conditions). Accuracy measurements should always be paired with clearly defined and realistic test sets â€“ that are representative of conditions of expected use â€“ and details about test methodology; these should be included in associated documen-tation. Accuracy measurements may include disaggregation of results for different data segments. \n\nRobustness or generalizability is defined as the â€œability of a system to maintain its level of performance under a variety of circumstancesâ€ (Source: ISO /IEC TS 5723:2022). Ro-bustness is a goal for appropriate system functionality in a broad set of conditions and circumstances, including uses of AI systems not initially anticipated. Robustness requires not only that the system perform exactly as it does under expected uses, but also that it should perform in ways that minimize potential harms to people if it is operating in an unexpected setting. Validity and reliability for deployed AI systems are often assessed by ongoing testing or monitoring that confirms a system is performing as intended. Measurement of validity, accuracy, robustness, and reliability contribute to trustworthiness and should take into con-sideration that certain types of failures can cause greater harm. AI risk management efforts should prioritize the minimization of potential negative impacts, and may need to include human intervention in cases where the AI system cannot detect or correct errors. \n\n3.2 Safe \n\nAI systems should â€œnot under defined conditions, lead to a state in which human life, health, property, or the environment is endangeredâ€ (Source: ISO /IEC TS 5723:2022). Safe operation of AI systems is improved through: â€¢ responsible design, development, and deployment practices; â€¢ clear information to deployers on responsible use of the system; â€¢ responsible decision-making by deployers and end users; and â€¢ explanations and documentation of risks based on empirical evidence of incidents. Different types of safety risks may require tailored AI risk management approaches based on context and the severity of potential risks presented. Safety risks that pose a potential risk of serious injury or death call for the most urgent prioritization and most thorough risk management process.  \n\n> Page 14 NIST AI 100-1\n\nAI RMF 1.0 Employing safety considerations during the lifecycle and starting as early as possible with planning and design can prevent failures or conditions that can render a system dangerous. Other practical approaches for AI safety often relate to rigorous simulation and in-domain testing, real-time monitoring, and the ability to shut down, modify, or have human inter-vention into systems that deviate from intended or expected functionality. AI safety risk management approaches should take cues from efforts and guidelines for safety in fields such as transportation and healthcare, and align with existing sector- or application-specific guidelines or standards. \n\n3.3 Secure and Resilient \n\nAI systems, as well as the ecosystems in which they are deployed, may be said to be re-silient if they can withstand unexpected adverse events or unexpected changes in their envi-ronment or use â€“ or if they can maintain their functions and structure in the face of internal and external change and degrade safely and gracefully when this is necessary (Adapted from: ISO /IEC TS 5723:2022). Common security concerns relate to adversarial examples, data poisoning, and the exfiltration of models, training data, or other intellectual property through AI system endpoints. AI systems that can maintain confidentiality, integrity, and availability through protection mechanisms that prevent unauthorized access and use may be said to be secure . Guidelines in the NIST Cybersecurity Framework and Risk Manage-ment Framework are among those which are applicable here. Security and resilience are related but distinct characteristics. While resilience is the abil-ity to return to normal function after an unexpected adverse event, security includes re-silience but also encompasses protocols to avoid, protect against, respond to, or recover from attacks. Resilience relates to robustness and goes beyond the provenance of the data to encompass unexpected or adversarial use (or abuse or misuse) of the model or data. \n\n3.4 Accountable and Transparent \n\nTrustworthy AI depends upon accountability. Accountability presupposes transparency. \n\nTransparency reflects the extent to which information about an AI system and its outputs is available to individuals interacting with such a system â€“ regardless of whether they are even aware that they are doing so. Meaningful transparency provides access to appropriate levels of information based on the stage of the AI lifecycle and tailored to the role or knowledge of AI actors or individuals interacting with or using the AI system. By promoting higher levels of understanding, transparency increases confidence in the AI system. This characteristicâ€™s scope spans from design decisions and training data to model train-ing, the structure of the model, its intended use cases, and how and when deployment, post-deployment, or end user decisions were made and by whom. Transparency is often necessary for actionable redress related to AI system outputs that are incorrect or otherwise lead to negative impacts. Transparency should consider human-AI interaction: for exam- \n\n> Page 15 NIST AI 100-1\n\nAI RMF 1.0 ple, how a human operator or user is notified when a potential or actual adverse outcome caused by an AI system is detected. A transparent system is not necessarily an accurate, privacy-enhanced, secure, or fair system. However, it is difficult to determine whether an opaque system possesses such characteristics, and to do so over time as complex systems evolve. The role of AI actors should be considered when seeking accountability for the outcomes of AI systems. The relationship between risk and accountability associated with AI and tech-nological systems more broadly differs across cultural, legal, sectoral, and societal contexts. When consequences are severe, such as when life and liberty are at stake, AI developers and deployers should consider proportionally and proactively adjusting their transparency and accountability practices. Maintaining organizational practices and governing structures for harm reduction, like risk management, can help lead to more accountable systems. Measures to enhance transparency and accountability should also consider the impact of these efforts on the implementing entity, including the level of necessary resources and the need to safeguard proprietary information. Maintaining the provenance of training data and supporting attribution of the AI systemâ€™s decisions to subsets of training data can assist with both transparency and accountability. Training data may also be subject to copyright and should follow applicable intellectual property rights laws. As transparency tools for AI systems and related documentation continue to evolve, devel-opers of AI systems are encouraged to test different types of transparency tools in cooper-ation with AI deployers to ensure that AI systems are used as intended. \n\n3.5 Explainable and Interpretable \n\nExplainability refers to a representation of the mechanisms underlying AI systemsâ€™ oper-ation, whereas interpretability refers to the meaning of AI systemsâ€™ output in the context of their designed functional purposes. Together, explainability and interpretability assist those operating or overseeing an AI system, as well as users of an AI system, to gain deeper insights into the functionality and trustworthiness of the system, including its out-puts. The underlying assumption is that perceptions of negative risk stem from a lack of ability to make sense of, or contextualize, system output appropriately. Explainable and interpretable AI systems offer information that will help end users understand the purposes and potential impact of an AI system. Risk from lack of explainability may be managed by describing how AI systems function, with descriptions tailored to individual differences such as the userâ€™s role, knowledge, and skill level. Explainable systems can be debugged and monitored more easily, and they lend themselves to more thorough documentation, audit, and governance.  \n\n> Page 16 NIST AI 100-1\n\nAI RMF 1.0 Risks to interpretability often can be addressed by communicating a description of why an AI system made a particular prediction or recommendation. (See â€œFour Principles of Explainable Artificial Intelligenceâ€ and â€œPsychological Foundations of Explainability and Interpretability in Artificial Intelligenceâ€ found here.) Transparency, explainability, and interpretability are distinct characteristics that support each other. Transparency can answer the question of â€œwhat happenedâ€ in the system. Ex-plainability can answer the question of â€œhowâ€ a decision was made in the system. Inter-pretability can answer the question of â€œwhyâ€ a decision was made by the system and its meaning or context to the user. \n\n3.6 Privacy-Enhanced \n\nPrivacy refers generally to the norms and practices that help to safeguard human autonomy, identity, and dignity. These norms and practices typically address freedom from intrusion, limiting observation, or individualsâ€™ agency to consent to disclosure or control of facets of their identities (e.g., body, data, reputation). (See The NIST Privacy Framework: A Tool for Improving Privacy through Enterprise Risk Management.) Privacy values such as anonymity, confidentiality, and control generally should guide choices for AI system design, development, and deployment. Privacy-related risks may influence security, bias, and transparency and come with tradeoffs with these other characteristics. Like safety and security, specific technical features of an AI system may promote or reduce privacy. AI systems can also present new risks to privacy by allowing inference to identify individuals or previously private information about individuals. Privacy-enhancing technologies (â€œPETsâ€) for AI, as well as data minimizing methods such as de-identification and aggregation for certain model outputs, can support design for privacy-enhanced AI systems. Under certain conditions such as data sparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions about fairness and other values in certain domains. \n\n3.7 Fair â€“ with Harmful Bias Managed \n\nFairness in AI includes concerns for equality and equity by addressing issues such as harm-ful bias and discrimination. Standards of fairness can be complex and difficult to define be-cause perceptions of fairness differ among cultures and may shift depending on application. Organizationsâ€™ risk management efforts will be enhanced by recognizing and considering these differences. Systems in which harmful biases are mitigated are not necessarily fair. For example, systems in which predictions are somewhat balanced across demographic groups may still be inaccessible to individuals with disabilities or affected by the digital divide or may exacerbate existing disparities or systemic biases.  \n\n> Page 17 NIST AI 100-1\n\nAI RMF 1.0 Bias is broader than demographic balance and data representativeness. NIST has identified three major categories of AI bias to be considered and managed: systemic, computational and statistical, and human-cognitive. Each of these can occur in the absence of prejudice, partiality, or discriminatory intent. Systemic bias can be present in AI datasets, the orga-nizational norms, practices, and processes across the AI lifecycle, and the broader society that uses AI systems. Computational and statistical biases can be present in AI datasets and algorithmic processes, and often stem from systematic errors due to non-representative samples. Human-cognitive biases relate to how an individual or group perceives AI sys-tem information to make a decision or fill in missing information, or how humans think about purposes and functions of an AI system. Human-cognitive biases are omnipresent in decision-making processes across the AI lifecycle and system use, including the design, implementation, operation, and maintenance of AI. Bias exists in many forms and can become ingrained in the automated systems that help make decisions about our lives. While bias is not always a negative phenomenon, AI sys-tems can potentially increase the speed and scale of biases and perpetuate and amplify harms to individuals, groups, communities, organizations, and society. Bias is tightly asso-ciated with the concepts of transparency as well as fairness in society. (For more informa-tion about bias, including the three categories, see NIST Special Publication 1270, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.)  \n\n> Page 18 NIST AI 100-1\n\nAI RMF 1.0 \n\n# 4. Effectiveness of the AI RMF \n\nEvaluations of AI RMF effectiveness â€“ including ways to measure bottom-line improve-ments in the trustworthiness of AI systems â€“ will be part of future NIST activities, in conjunction with the AI community. Organizations and other users of the Framework are encouraged to periodically evaluate whether the AI RMF has improved their ability to manage AI risks, including but not lim-ited to their policies, processes, practices, implementation plans, indicators, measurements, and expected outcomes. NIST intends to work collaboratively with others to develop met-rics, methodologies, and goals for evaluating the AI RMFâ€™s effectiveness, and to broadly share results and supporting information. Framework users are expected to benefit from: â€¢ enhanced processes for governing, mapping, measuring, and managing AI risk, and clearly documenting outcomes; â€¢ improved awareness of the relationships and tradeoffs among trustworthiness char-acteristics, socio-technical approaches, and AI risks; â€¢ explicit processes for making go/no-go system commissioning and deployment deci-sions; â€¢ established policies, processes, practices, and procedures for improving organiza-tional accountability efforts related to AI system risks; â€¢ enhanced organizational culture which prioritizes the identification and management of AI system risks and potential impacts to individuals, communities, organizations, and society; â€¢ better information sharing within and across organizations about risks, decision-making processes, responsibilities, common pitfalls, TEVV practices, and approaches for continuous improvement; â€¢ greater contextual knowledge for increased awareness of downstream risks; â€¢ strengthened engagement with interested parties and relevant AI actors; and â€¢ augmented capacity for TEVV of AI systems and associated risks.  \n\n> Page 19 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Part 2: Core and Profiles \n\n# 5. AI RMF Core \n\nThe AI RMF Core provides outcomes and actions that enable dialogue, understanding, and activities to manage AI risks and responsibly develop trustworthy AI systems. As illus-trated in Figure 5, the Core is composed of four functions: GOVERN , MAP , MEASURE ,\n\nand MANAGE . Each of these high-level functions is broken down into categories and sub-categories. Categories and subcategories are subdivided into specific actions and outcomes. Actions do not constitute a checklist, nor are they necessarily an ordered set of steps. \n\nFig. 5. Functions organize AI risk management activities at their highest level to govern, map, measure, and manage AI risks. Governance is designed to be a cross-cutting function to inform and be infused throughout the other three functions. \n\nRisk management should be continuous, timely, and performed throughout the AI system lifecycle dimensions. AI RMF Core functions should be carried out in a way that reflects diverse and multidisciplinary perspectives, potentially including the views of AI actors out-side the organization. Having a diverse team contributes to more open sharing of ideas and assumptions about purposes and functions of the technology being designed, developed,  \n\n> Page 20 NIST AI 100-1\n\nAI RMF 1.0 deployed, or evaluated â€“ which can create opportunities to surface problems and identify existing and emergent risks. An online companion resource to the AI RMF, the NIST AI RMF Playbook, is available to help organizations navigate the AI RMF and achieve its outcomes through suggested tactical actions they can apply within their own contexts. Like the AI RMF, the Playbook is voluntary and organizations can utilize the suggestions according to their needs and interests. Playbook users can create tailored guidance selected from suggested material for their own use and contribute their suggestions for sharing with the broader community. Along with the AI RMF, the Playbook is part of the NIST Trustworthy and Responsible AI Resource Center. Framework users may apply these functions as best suits their needs for managing AI risks based on their resources and capabilities. Some organizations may choose to select from among the categories and subcategories; others may choose and have the capacity to apply all categories and subcategories. Assuming a governance struc-ture is in place, functions may be performed in any order across the AI lifecycle as deemed to add value by a user of the framework. After instituting the outcomes in \n\n> GOVERN\n\n, most users of the AI RMF would start with the MAP function and con-tinue to MEASURE or MANAGE . However users integrate the functions, the process should be iterative, with cross-referencing between functions as necessary. Simi-larly, there are categories and subcategories with elements that apply to multiple functions, or that logically should take place before certain subcategory decisions. \n\n5.1 Govern \n\nThe GOVERN function: â€¢ cultivates and implements a culture of risk management within organizations design-ing, developing, deploying, evaluating, or acquiring AI systems; â€¢ outlines processes, documents, and organizational schemes that anticipate, identify, and manage the risks a system can pose, including to users and others across society â€“ and procedures to achieve those outcomes; â€¢ incorporates processes to assess potential impacts; â€¢ provides a structure by which AI risk management functions can align with organi-zational principles, policies, and strategic priorities; â€¢ connects technical aspects of AI system design and development to organizational values and principles, and enables organizational practices and competencies for the individuals involved in acquiring, training, deploying, and monitoring such systems; and â€¢ addresses full product lifecycle and associated processes, including legal and other issues concerning use of third-party software or hardware systems and data.  \n\n> Page 21 NIST AI 100-1\n\nAI RMF 1.0  \n\n> GOVERN\n\nis a cross-cutting function that is infused throughout AI risk management and enables the other functions of the process. Aspects of GOVERN , especially those related to compliance or evaluation, should be integrated into each of the other functions. Attention to governance is a continual and intrinsic requirement for effective AI risk management over an AI systemâ€™s lifespan and the organizationâ€™s hierarchy. Strong governance can drive and enhance internal practices and norms to facilitate orga-nizational risk culture. Governing authorities can determine the overarching policies that direct an organizationâ€™s mission, goals, values, culture, and risk tolerance. Senior leader-ship sets the tone for risk management within an organization, and with it, organizational culture. Management aligns the technical aspects of AI risk management to policies and operations. Documentation can enhance transparency, improve human review processes, and bolster accountability in AI system teams. After putting in place the structures, systems, processes, and teams described in the GOV - \n\n> ERN\n\nfunction, organizations should benefit from a purpose-driven culture focused on risk understanding and management. It is incumbent on Framework users to continue to ex-ecute the GOVERN function as knowledge, cultures, and needs or expectations from AI actors evolve over time. Practices related to governing AI risks are described in the NIST AI RMF Playbook. Table 1 lists the GOVERN functionâ€™s categories and subcategories. Table 1: Categories and subcategories for the GOVERN function.  \n\n> GOVERN\n\n1: \n\nPolicies, processes, procedures, and practices across the organization related to the mapping, measuring, and managing of AI risks are in place, transparent, and implemented effectively.  \n\n> GOVERN\n\n1.1: Legal and regulatory requirements involving AI are understood, managed, and documented.  \n\n> GOVERN\n\n1.2: The characteristics of trustworthy AI are inte-grated into organizational policies, processes, procedures, and practices.  \n\n> GOVERN\n\n1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based on the organizationâ€™s risk tolerance.  \n\n> GOVERN\n\n1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 22 NIST AI 100-1\n\nAI RMF 1.0 Table 1: Categories and subcategories for the GOVERN function. (Continued)  \n\n> GOVERN\n\n1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned and or-ganizational roles and responsibilities clearly defined, including determining the frequency of periodic review.  \n\n> GOVERN\n\n1.6: Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities.  \n\n> GOVERN\n\n1.7: Processes and procedures are in place for decom-missioning and phasing out AI systems safely and in a man-ner that does not increase risks or decrease the organizationâ€™s trustworthiness.  \n\n> GOVERN\n\n2: \n\nAccountability structures are in place so that the appropriate teams and individuals are empowered, responsible, and trained for mapping, measuring, and managing AI risks.  \n\n> GOVERN\n\n2.1: Roles and responsibilities and lines of communi-cation related to mapping, measuring, and managing AI risks are documented and are clear to individuals and teams throughout the organization.  \n\n> GOVERN\n\n2.2: The organizationâ€™s personnel and partners receive AI risk management training to enable them to perform their du-ties and responsibilities consistent with related policies, proce-dures, and agreements.  \n\n> GOVERN\n\n2.3: Executive leadership of the organization takes re-sponsibility for decisions about risks associated with AI system development and deployment.  \n\n> GOVERN\n\n3: \n\nWorkforce diversity, equity, inclusion, and accessibility processes are prioritized in the mapping, measuring, and managing of AI risks throughout the lifecycle.  \n\n> GOVERN\n\n3.1: Decision-making related to mapping, measuring, and managing AI risks throughout the lifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, expe-rience, expertise, and backgrounds).  \n\n> GOVERN\n\n3.2: Policies and procedures are in place to define and differentiate roles and responsibilities for human-AI configura-tions and oversight of AI systems.  \n\n> GOVERN\n\n4: \n\nOrganizational teams are committed to a culture  \n\n> GOVERN\n\n4.1: Organizational policies and practices are in place to foster a critical thinking and safety-first mindset in the design, development, deployment, and uses of AI systems to minimize potential negative impacts. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 23 NIST AI 100-1\n\nAI RMF 1.0 Table 1: Categories and subcategories for the GOVERN function. (Continued) that considers and communicates AI risk.  \n\n> GOVERN\n\n4.2: Organizational teams document the risks and po-tential impacts of the AI technology they design, develop, deploy, evaluate, and use, and they communicate about the impacts more broadly.  \n\n> GOVERN\n\n4.3: Organizational practices are in place to enable AI testing, identification of incidents, and information sharing.  \n\n> GOVERN\n\n5: \n\nProcesses are in place for robust engagement with relevant AI actors.  \n\n> GOVERN\n\n5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks.  \n\n> GOVERN\n\n5.2: Mechanisms are established to enable the team that developed or deployed AI systems to regularly incorporate adjudicated feedback from relevant AI actors into system design and implementation.  \n\n> GOVERN\n\n6: Policies and procedures are in place to address AI risks and benefits arising from third-party software and data and other supply chain issues.  \n\n> GOVERN\n\n6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of in-fringement of a third-partyâ€™s intellectual property or other rights.  \n\n> GOVERN\n\n6.2: Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be high-risk. \n\nCategories Subcategories 5.2 Map \n\nThe MAP function establishes the context to frame risks related to an AI system. The AI lifecycle consists of many interdependent activities involving a diverse set of actors (See Figure 3). In practice, AI actors in charge of one part of the process often do not have full visibility or control over other parts and their associated contexts. The interdependencies between these activities, and among the relevant AI actors, can make it difficult to reliably anticipate impacts of AI systems. For example, early decisions in identifying purposes and objectives of an AI system can alter its behavior and capabilities, and the dynamics of de-ployment setting (such as end users or impacted individuals) can shape the impacts of AI system decisions. As a result, the best intentions within one dimension of the AI lifecycle can be undermined via interactions with decisions and conditions in other, later activities.  \n\n> Page 24 NIST AI 100-1\n\nAI RMF 1.0 This complexity and varying levels of visibility can introduce uncertainty into risk man-agement practices. Anticipating, assessing, and otherwise addressing potential sources of negative risk can mitigate this uncertainty and enhance the integrity of the decision process. The information gathered while carrying out the MAP function enables negative risk pre-vention and informs decisions for processes such as model management, as well as an initial decision about appropriateness or the need for an AI solution. Outcomes in the  \n\n> MAP\n\nfunction are the basis for the MEASURE and MANAGE functions. Without contex-tual knowledge, and awareness of risks within the identified contexts, risk management is difficult to perform. The MAP function is intended to enhance an organizationâ€™s ability to identify risks and broader contributing factors. Implementation of this function is enhanced by incorporating perspectives from a diverse internal team and engagement with those external to the team that developed or deployed the AI system. Engagement with external collaborators, end users, potentially impacted communities, and others may vary based on the risk level of a particular AI system, the makeup of the internal team, and organizational policies. Gathering such broad perspec-tives can help organizations proactively prevent negative risks and develop more trustwor-thy AI systems by: â€¢ improving their capacity for understanding contexts; â€¢ checking their assumptions about context of use; â€¢ enabling recognition of when systems are not functional within or out of their in-tended context; â€¢ identifying positive and beneficial uses of their existing AI systems; â€¢ improving understanding of limitations in AI and ML processes; â€¢ identifying constraints in real-world applications that may lead to negative impacts; â€¢ identifying known and foreseeable negative impacts related to intended use of AI systems; and â€¢ anticipating risks of the use of AI systems beyond intended use. After completing the MAP function, Framework users should have sufficient contextual knowledge about AI system impacts to inform an initial go/no-go decision about whether to design, develop, or deploy an AI system. If a decision is made to proceed, organizations should utilize the MEASURE and MANAGE functions along with policies and procedures put into place in the GOVERN function to assist in AI risk management efforts. It is incum-bent on Framework users to continue applying the MAP function to AI systems as context, capabilities, risks, benefits, and potential impacts evolve over time. Practices related to mapping AI risks are described in the NIST AI RMF Playbook. Table 2 lists the MAP functionâ€™s categories and subcategories.  \n\n> Page 25 NIST AI 100-1\n\nAI RMF 1.0 Table 2: Categories and subcategories for the MAP function.  \n\n> MAP\n\n1: Context is established and understood.  \n\n> MAP\n\n1.1: Intended purposes, potentially beneficial uses, context-specific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and docu-mented. Considerations include: the specific set or types of users along with their expectations; potential positive and negative im-pacts of system uses to individuals, communities, organizations, society, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or product AI lifecycle; and related TEVV and system metrics.  \n\n> MAP\n\n1.2: Interdisciplinary AI actors, competencies, skills, and capacities for establishing context reflect demographic diversity and broad domain and user experience expertise, and their par-ticipation is documented. Opportunities for interdisciplinary col-laboration are prioritized.  \n\n> MAP\n\n1.3: The organizationâ€™s mission and relevant goals for AI technology are understood and documented.  \n\n> MAP\n\n1.4: The business value or context of business use has been clearly defined or â€“ in the case of assessing existing AI systems â€“ re-evaluated.  \n\n> MAP\n\n1.5: Organizational risk tolerances are determined and documented.  \n\n> MAP\n\n1.6: System requirements (e.g., â€œthe system shall respect the privacy of its usersâ€) are elicited from and understood by rel-evant AI actors. Design decisions take socio-technical implica-tions into account to address AI risks.  \n\n> MAP\n\n2: \n\nCategorization of the AI system is performed.  \n\n> MAP\n\n2.1: The specific tasks and methods used to implement the tasks that the AI system will support are defined (e.g., classifiers, generative models, recommenders).  \n\n> MAP\n\n2.2: Information about the AI systemâ€™s knowledge limits and how system output may be utilized and overseen by humans is documented. Documentation provides sufficient information to assist relevant AI actors when making decisions and taking subsequent actions. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 26 NIST AI 100-1\n\nAI RMF 1.0 Table 2: Categories and subcategories for the MAP function. (Continued)  \n\n> MAP\n\n2.3: Scientific integrity and TEVV considerations are iden-tified and documented, including those related to experimental design, data collection and selection (e.g., availability, repre-sentativeness, suitability), system trustworthiness, and construct validation.  \n\n> MAP\n\n3: AI capabilities, targeted usage, goals, and expected benefits and costs compared with appropriate benchmarks are understood.  \n\n> MAP\n\n3.1: Potential benefits of intended AI system functionality and performance are examined and documented.  \n\n> MAP\n\n3.2: Potential costs, including non-monetary costs, which result from expected or realized AI errors or system functionality and trustworthiness â€“ as connected to organizational risk toler-ance â€“ are examined and documented.  \n\n> MAP\n\n3.3: Targeted application scope is specified and docu-mented based on the systemâ€™s capability, established context, and AI system categorization.  \n\n> MAP\n\n3.4: Processes for operator and practitioner proficiency with AI system performance and trustworthiness â€“ and relevant technical standards and certifications â€“ are defined, assessed, and documented.  \n\n> MAP\n\n3.5: Processes for human oversight are defined, assessed, and documented in accordance with organizational policies from the GOVERN function.  \n\n> MAP\n\n4: Risks and benefits are mapped for all components of the AI system including third-party software and data.  \n\n> MAP\n\n4.1: Approaches for mapping AI technology and legal risks of its components â€“ including the use of third-party data or soft-ware â€“ are in place, followed, and documented, as are risks of in-fringement of a third partyâ€™s intellectual property or other rights.  \n\n> MAP\n\n4.2: Internal risk controls for components of the AI sys-tem, including third-party AI technologies, are identified and documented.  \n\n> MAP\n\n5: Impacts to individuals, groups, communities, organizations, and society are characterized.  \n\n> MAP\n\n5.1: Likelihood and magnitude of each identified impact (both potentially beneficial and harmful) based on expected use, past uses of AI systems in similar contexts, public incident re-ports, feedback from those external to the team that developed or deployed the AI system, or other data are identified and documented. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 27 NIST AI 100-1\n\nAI RMF 1.0 Table 2: Categories and subcategories for the MAP function. (Continued)  \n\n> MAP\n\n5.2: Practices and personnel for supporting regular en-gagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented. \n\nCategories Subcategories 5.3 Measure \n\nThe MEASURE function employs quantitative, qualitative, or mixed-method tools, tech-niques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related impacts. It uses knowledge relevant to AI risks identified in the MAP function and informs the MANAGE function. AI systems should be tested before their deployment and regu-larly while in operation. AI risk measurements include documenting aspects of systemsâ€™ functionality and trustworthiness. Measuring AI risks includes tracking metrics for trustworthy characteristics, social impact, and human-AI configurations. Processes developed or adopted in the MEASURE function should include rigorous software testing and performance assessment methodologies with associated measures of uncertainty, comparisons to performance benchmarks, and formal-ized reporting and documentation of results. Processes for independent review can improve the effectiveness of testing and can mitigate internal biases and potential conflicts of inter-est. Where tradeoffs among the trustworthy characteristics arise, measurement provides a trace-able basis to inform management decisions. Options may include recalibration, impact mitigation, or removal of the system from design, development, production, or use, as well as a range of compensating, detective, deterrent, directive, and recovery controls. After completing the MEASURE function, objective, repeatable, or scalable test, evaluation, verification, and validation (TEVV) processes including metrics, methods, and methodolo-gies are in place, followed, and documented. Metrics and measurement methodologies should adhere to scientific, legal, and ethical norms and be carried out in an open and trans-parent process. New types of measurement, qualitative and quantitative, may need to be developed. The degree to which each measurement type provides unique and meaningful information to the assessment of AI risks should be considered. Framework users will en-hance their capacity to comprehensively evaluate system trustworthiness, identify and track existing and emergent risks, and verify efficacy of the metrics. Measurement outcomes will be utilized in the MANAGE function to assist risk monitoring and response efforts. It is in-cumbent on Framework users to continue applying the MEASURE function to AI systems as knowledge, methodologies, risks, and impacts evolve over time.  \n\n> Page 28 NIST AI 100-1\n\nAI RMF 1.0 Practices related to measuring AI risks are described in the NIST AI RMF Playbook. Table 3 lists the MEASURE functionâ€™s categories and subcategories. Table 3: Categories and subcategories for the MEASURE function.  \n\n> MEASURE\n\n1: \n\nAppropriate methods and metrics are identified and applied.  \n\n> MEASURE\n\n1.1: Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for imple-mentation starting with the most significant AI risks. The risks or trustworthiness characteristics that will not â€“ or cannot â€“ be measured are properly documented.  \n\n> MEASURE\n\n1.2: Appropriateness of AI metrics and effectiveness of existing controls are regularly assessed and updated, including reports of errors and potential impacts on affected communities.  \n\n> MEASURE\n\n1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are in-volved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance.  \n\n> MEASURE\n\n2: AI systems are evaluated for trustworthy characteristics.  \n\n> MEASURE\n\n2.1: Test sets, metrics, and details about the tools used during TEVV are documented.  \n\n> MEASURE\n\n2.2: Evaluations involving human subjects meet ap-plicable requirements (including human subject protection) and are representative of the relevant population.  \n\n> MEASURE\n\n2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented.  \n\n> MEASURE\n\n2.4: The functionality and behavior of the AI sys-tem and its components â€“ as identified in the MAP function â€“ are monitored when in production.  \n\n> MEASURE\n\n2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability be-yond the conditions under which the technology was developed are documented. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 29 NIST AI 100-1\n\nAI RMF 1.0 Table 3: Categories and subcategories for the MEASURE function. (Continued)  \n\n> MEASURE\n\n2.6: The AI system is evaluated regularly for safety risks â€“ as identified in the MAP function. The AI system to be de-ployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics re-flect system reliability and robustness, real-time monitoring, and response times for AI system failures.  \n\n> MEASURE\n\n2.7: AI system security and resilience â€“ as identified in the MAP function â€“ are evaluated and documented.  \n\n> MEASURE\n\n2.8: Risks associated with transparency and account-ability â€“ as identified in the MAP function â€“ are examined and documented.  \n\n> MEASURE\n\n2.9: The AI model is explained, validated, and docu-mented, and AI system output is interpreted within its context â€“ as identified in the MAP function â€“ to inform responsible use and governance.  \n\n> MEASURE\n\n2.10: Privacy risk of the AI system â€“ as identified in the MAP function â€“ is examined and documented.  \n\n> MEASURE\n\n2.11: Fairness and bias â€“ as identified in the MAP \n\nfunction â€“ are evaluated and results are documented.  \n\n> MEASURE\n\n2.12: Environmental impact and sustainability of AI model training and management activities â€“ as identified in the  \n\n> MAP\n\nfunction â€“ are assessed and documented.  \n\n> MEASURE\n\n2.13: Effectiveness of the employed TEVV met-rics and processes in the MEASURE function are evaluated and documented.  \n\n> MEASURE\n\n3: \n\nMechanisms for tracking identified AI risks over time are in place.  \n\n> MEASURE\n\n3.1: Approaches, personnel, and documentation are in place to regularly identify and track existing, unanticipated, and emergent AI risks based on factors such as intended and ac-tual performance in deployed contexts.  \n\n> MEASURE\n\n3.2: Risk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 30 NIST AI 100-1\n\nAI RMF 1.0 Table 3: Categories and subcategories for the MEASURE function. (Continued)  \n\n> MEASURE\n\n3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into AI system evaluation metrics.  \n\n> MEASURE\n\n4: \n\nFeedback about efficacy of measurement is gathered and assessed.  \n\n> MEASURE\n\n4.1: Measurement approaches for identifying AI risks are connected to deployment context(s) and informed through consultation with domain experts and other end users. Ap-proaches are documented.  \n\n> MEASURE\n\n4.2: Measurement results regarding AI system trust-worthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI ac-tors to validate whether the system is performing consistently as intended. Results are documented.  \n\n> MEASURE\n\n4.3: Measurable performance improvements or de-clines based on consultations with relevant AI actors, in-cluding affected communities, and field data about context-relevant risks and trustworthiness characteristics are identified and documented. \n\nCategories Subcategories 5.4 Manage \n\nThe MANAGE function entails allocating risk resources to mapped and measured risks on a regular basis and as defined by the GOVERN function. Risk treatment comprises plans to respond to, recover from, and communicate about incidents or events. Contextual information gleaned from expert consultation and input from relevant AI actors â€“ established in GOVERN and carried out in MAP â€“ is utilized in this function to decrease the likelihood of system failures and negative impacts. Systematic documentation practices established in GOVERN and utilized in MAP and MEASURE bolster AI risk management efforts and increase transparency and accountability. Processes for assessing emergent risks are in place, along with mechanisms for continual improvement. After completing the MANAGE function, plans for prioritizing risk and regular monitoring and improvement will be in place. Framework users will have enhanced capacity to man-age the risks of deployed AI systems and to allocate risk management resources based on assessed and prioritized risks. It is incumbent on Framework users to continue to apply the MANAGE function to deployed AI systems as methods, contexts, risks, and needs or expectations from relevant AI actors evolve over time.  \n\n> Page 31 NIST AI 100-1\n\nAI RMF 1.0 Practices related to managing AI risks are described in the NIST AI RMF Playbook. Table 4 lists the MANAGE functionâ€™s categories and subcategories. Table 4: Categories and subcategories for the MANAGE function.  \n\n> MANAGE\n\n1: AI risks based on assessments and other analytical output from the  \n\n> MAP\n\nand MEASURE \n\nfunctions are prioritized, responded to, and managed.  \n\n> MANAGE\n\n1.1: A determination is made as to whether the AI system achieves its intended purposes and stated objectives and whether its development or deployment should proceed.  \n\n> MANAGE\n\n1.2: Treatment of documented AI risks is prioritized based on impact, likelihood, and available resources or methods.  \n\n> MANAGE\n\n1.3: Responses to the AI risks deemed high priority, as identified by the MAP function, are developed, planned, and doc-umented. Risk response options can include mitigating, transfer-ring, avoiding, or accepting.  \n\n> MANAGE\n\n1.4: Negative residual risks (defined as the sum of all unmitigated risks) to both downstream acquirers of AI systems and end users are documented.  \n\n> MANAGE\n\n2: \n\nStrategies to maximize AI benefits and minimize negative impacts are planned, prepared, implemented, documented, and informed by input from relevant AI actors.  \n\n> MANAGE\n\n2.1: Resources required to manage AI risks are taken into account â€“ along with viable non-AI alternative systems, ap-proaches, or methods â€“ to reduce the magnitude or likelihood of potential impacts.  \n\n> MANAGE\n\n2.2: Mechanisms are in place and applied to sustain the value of deployed AI systems.  \n\n> MANAGE\n\n2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identified.  \n\n> MANAGE\n\n2.4: Mechanisms are in place and applied, and respon-sibilities are assigned and understood, to supersede, disengage, or deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use.  \n\n> MANAGE\n\n3: AI risks and benefits from third-party entities are managed.  \n\n> MANAGE\n\n3.1: AI risks and benefits from third-party resources are regularly monitored, and risk controls are applied and documented.  \n\n> MANAGE\n\n3.2: Pre-trained models which are used for develop-ment are monitored as part of AI system regular monitoring and maintenance. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 32 NIST AI 100-1\n\nAI RMF 1.0 Table 4: Categories and subcategories for the MANAGE function. (Continued)  \n\n> MANAGE\n\n4: Risk treatments, including response and recovery, and communication plans for the identified and measured AI risks are documented and monitored regularly.  \n\n> MANAGE\n\n4.1: Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and eval-uating input from users and other relevant AI actors, appeal and override, decommissioning, incident response, recovery, and change management.  \n\n> MANAGE\n\n4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular engage-ment with interested parties, including relevant AI actors.  \n\n> MANAGE\n\n4.3: Incidents and errors are communicated to relevant AI actors, including affected communities. Processes for track-ing, responding to, and recovering from incidents and errors are followed and documented. \n\nCategories Subcategories \n\n# 6. AI RMF Profiles \n\nAI RMF use-case profiles are implementations of the AI RMF functions, categories, and subcategories for a specific setting or application based on the requirements, risk tolerance, and resources of the Framework user: for example, an AI RMF hiring profile or an AI RMF fair housing profile . Profiles may illustrate and offer insights into how risk can be managed at various stages of the AI lifecycle or in specific sector, technology, or end-use applications. AI RMF profiles assist organizations in deciding how they might best manage AI risk that is well-aligned with their goals, considers legal/regulatory requirements and best practices, and reflects risk management priorities. AI RMF temporal profiles are descriptions of either the current state or the desired, target state of specific AI risk management activities within a given sector, industry, organization, or application context. An AI RMF Current Profile indicates how AI is currently being managed and the related risks in terms of current outcomes. A Target Profile indicates the outcomes needed to achieve the desired or target AI risk management goals. Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk management objectives. Action plans can be developed to address these gaps to fulfill outcomes in a given category or subcategory. Prioritization of gap mitigation is driven by the userâ€™s needs and risk management processes. This risk-based approach also enables Framework users to compare their approaches with other approaches and to gauge the resources needed (e.g., staffing, funding) to achieve AI risk management goals in a cost-effective, prioritized manner.  \n\n> Page 33 NIST AI 100-1\n\nAI RMF 1.0 AI RMF cross-sectoral profiles cover risks of models or applications that can be used across use cases or sectors. Cross-sectoral profiles can also cover how to govern, map, measure, and manage risks for activities or business processes common across sectors such as the use of large language models, cloud-based services or acquisition. This Framework does not prescribe profile templates, allowing for flexibility in implemen-tation.  \n\n> Page 34 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3 \n\nAI Design tasks are performed during the Application Context and Data and Input phases of the AI lifecycle in Figure 2. AI Design actors create the concept and objectives of AI systems and are responsible for the planning, design, and data collection and processing tasks of the AI system so that the AI system is lawful and fit-for-purpose. Tasks include ar-ticulating and documenting the systemâ€™s concept and objectives, underlying assumptions, context, and requirements; gathering and cleaning data; and documenting the metadata and characteristics of the dataset. AI actors in this category include data scientists, do-main experts, socio-cultural analysts, experts in the field of diversity, equity, inclusion, and accessibility, members of impacted communities, human factors experts (e.g., UX/UI design), governance experts, data engineers, data providers, system funders, product man-agers, third-party entities, evaluators, and legal and privacy governance. \n\nAI Development tasks are performed during the AI Model phase of the lifecycle in Figure 2. AI Development actors provide the initial infrastructure of AI systems and are responsi-ble for model building and interpretation tasks, which involve the creation, selection, cali-bration, training, and/or testing of models or algorithms. AI actors in this category include machine learning experts, data scientists, developers, third-party entities, legal and privacy governance experts, and experts in the socio-cultural and contextual factors associated with the deployment setting. \n\nAI Deployment tasks are performed during the Task and Output phase of the lifecycle in Figure 2. AI Deployment actors are responsible for contextual decisions relating to how the AI system is used to assure deployment of the system into production. Related tasks include piloting the system, checking compatibility with legacy systems, ensuring regu-latory compliance, managing organizational change, and evaluating user experience. AI actors in this category include system integrators, software developers, end users, oper-ators and practitioners, evaluators, and domain experts with expertise in human factors, socio-cultural analysis, and governance. \n\nOperation and Monitoring tasks are performed in the Application Context/Operate and Monitor phase of the lifecycle in Figure 2. These tasks are carried out by AI actors who are responsible for operating the AI system and working with others to regularly assess system output and impacts. AI actors in this category include system operators, domain experts, AI designers, users who interpret or incorporate the output of AI systems, product developers, evaluators and auditors, compliance experts, organizational management, and members of the research community. \n\nTest, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout the AI lifecycle. They are carried out by AI actors who examine the AI system or its components, or detect and remediate problems. Ideally, AI actors carrying out verification  \n\n> Page 35 NIST AI 100-1\n\nAI RMF 1.0 and validation tasks are distinct from those who perform test and evaluation actions. Tasks can be incorporated into a phase as early as design, where tests are planned in accordance with the design requirement. â€¢ TEVV tasks for design, planning, and data may center on internal and external vali-dation of assumptions for system design, data collection, and measurements relative to the intended context of deployment or application. â€¢ TEVV tasks for development (i.e., model building) include model validation and assessment. â€¢ TEVV tasks for deployment include system validation and integration in production, with testing, and recalibration for systems and process integration, user experience, and compliance with existing legal, regulatory, and ethical specifications. â€¢ TEVV tasks for operations involve ongoing monitoring for periodic updates, testing, and subject matter expert (SME) recalibration of models, the tracking of incidents or errors reported and their management, the detection of emergent properties and related impacts, and processes for redress and response. \n\nHuman Factors tasks and activities are found throughout the dimensions of the AI life-cycle. They include human-centered design practices and methodologies, promoting the active involvement of end users and other interested parties and relevant AI actors, incor-porating context-specific norms and values in system design, evaluating and adapting end user experiences, and broad integration of humans and human dynamics in all phases of the AI lifecycle. Human factors professionals provide multidisciplinary skills and perspectives to understand context of use, inform interdisciplinary and demographic diversity, engage in consultative processes, design and evaluate user experience, perform human-centered evaluation and testing, and inform impact assessments. \n\nDomain Expert tasks involve input from multidisciplinary practitioners or scholars who provide knowledge or expertise in â€“ and about â€“ an industry sector, economic sector, con-text, or application area where an AI system is being used. AI actors who are domain experts can provide essential guidance for AI system design and development, and inter-pret outputs in support of work performed by TEVV and AI impact assessment teams. \n\nAI Impact Assessment tasks include assessing and evaluating requirements for AI system accountability, combating harmful bias, examining impacts of AI systems, product safety, liability, and security, among others. AI actors such as impact assessors and evaluators provide technical, human factor, socio-cultural, and legal expertise. \n\nProcurement tasks are conducted by AI actors with financial, legal, or policy management authority for acquisition of AI models, products, or services from a third-party developer, vendor, or contractor. \n\nGovernance and Oversight tasks are assumed by AI actors with management, fiduciary, and legal authority and responsibility for the organization in which an AI system is de- \n\n> Page 36 NIST AI 100-1\n\nAI RMF 1.0 signed, developed, and/or deployed. Key AI actors responsible for AI governance include organizational management, senior leadership, and the Board of Directors. These actors are parties that are concerned with the impact and sustainability of the organization as a whole. \n\nAdditional AI Actors Third-party entities include providers, developers, vendors, and evaluators of data, al-gorithms, models, and/or systems and related services for another organization or the or-ganizationâ€™s customers or clients. Third-party entities are responsible for AI design and development tasks, in whole or in part. By definition, they are external to the design, devel-opment, or deployment team of the organization that acquires its technologies or services. The technologies acquired from third-party entities may be complex or opaque, and risk tolerances may not align with the deploying or operating organization. \n\nEnd users of an AI system are the individuals or groups that use the system for specific purposes. These individuals or groups interact with an AI system in a specific context. End users can range in competency from AI experts to first-time technology end users. \n\nAffected individuals/communities encompass all individuals, groups, communities, or organizations directly or indirectly affected by AI systems or decisions based on the output of AI systems. These individuals do not necessarily interact with the deployed system or application. \n\nOther AI actors may provide formal or quasi-formal norms or guidance for specifying and managing AI risks. They can include trade associations, standards developing or-ganizations, advocacy groups, researchers, environmental groups, and civil society organizations .\n\nThe general public is most likely to directly experience positive and negative impacts of AI technologies. They may provide the motivation for actions taken by the AI actors. This group can include individuals, communities, and consumers associated with the context in which an AI system is developed or deployed.  \n\n> Page 37 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Appendix B: How AI Risks Differ from Traditional Software Risks \n\nAs with traditional software, risks from AI-based technology can be bigger than an en-terprise, span organizations, and lead to societal impacts. AI systems also bring a set of risks that are not comprehensively addressed by current risk frameworks and approaches. Some AI system features that present risks also can be beneficial. For example, pre-trained models and transfer learning can advance research and increase accuracy and resilience when compared to other models and approaches. Identifying contextual factors in the MAP \n\nfunction will assist AI actors in determining the level of risk and potential management efforts. Compared to traditional software, AI-specific risks that are new or increased include the following: â€¢ The data used for building an AI system may not be a true or appropriate representa-tion of the context or intended use of the AI system, and the ground truth may either not exist or not be available. Additionally, harmful bias and other data quality issues can affect AI system trustworthiness, which could lead to negative impacts. â€¢ AI system dependency and reliance on data for training tasks, combined with in-creased volume and complexity typically associated with such data. â€¢ Intentional or unintentional changes during training may fundamentally alter AI sys-tem performance. â€¢ Datasets used to train AI systems may become detached from their original and in-tended context or may become stale or outdated relative to deployment context. â€¢ AI system scale and complexity (many systems contain billions or even trillions of decision points) housed within more traditional software applications. â€¢ Use of pre-trained models that can advance research and improve performance can also increase levels of statistical uncertainty and cause issues with bias management, scientific validity, and reproducibility. â€¢ Higher degree of difficulty in predicting failure modes for emergent properties of large-scale pre-trained models. â€¢ Privacy risk due to enhanced data aggregation capability for AI systems. â€¢ AI systems may require more frequent maintenance and triggers for conducting cor-rective maintenance due to data, model, or concept drift. â€¢ Increased opacity and concerns about reproducibility. â€¢ Underdeveloped software testing standards and inability to document AI-based prac-tices to the standard expected of traditionally engineered software for all but the simplest of cases. â€¢ Difficulty in performing regular AI-based software testing, or determining what to test, since AI systems are not subject to the same controls as traditional code devel-opment.  \n\n> Page 38 NIST AI 100-1\n\nAI RMF 1.0 â€¢ Computational costs for developing AI systems and their impact on the environment and planet. â€¢ Inability to predict or detect the side effects of AI-based systems beyond statistical measures. Privacy and cybersecurity risk management considerations and approaches are applicable in the design, development, deployment, evaluation, and use of AI systems. Privacy and cybersecurity risks are also considered as part of broader enterprise risk management con-siderations, which may incorporate AI risks. As part of the effort to address AI trustworthi-ness characteristics such as â€œSecure and Resilientâ€ and â€œPrivacy-Enhanced,â€ organizations may consider leveraging available standards and guidance that provide broad guidance to organizations to reduce security and privacy risks, such as, but not limited to, the NIST Cy-bersecurity Framework, the NIST Privacy Framework, the NIST Risk Management Frame-work, and the Secure Software Development Framework. These frameworks have some features in common with the AI RMF. Like most risk management approaches, they are outcome-based rather than prescriptive and are often structured around a Core set of func-tions, categories, and subcategories. While there are significant differences between these frameworks based on the domain addressed â€“ and because AI risk management calls for addressing many other types of risks â€“ frameworks like those mentioned above may inform security and privacy considerations in the MAP , MEASURE , and MANAGE functions of the AI RMF. At the same time, guidance available before publication of this AI RMF does not compre-hensively address many AI system risks. For example, existing frameworks and guidance are unable to: â€¢ adequately manage the problem of harmful bias in AI systems; â€¢ confront the challenging risks related to generative AI; â€¢ comprehensively address security concerns related to evasion, model extraction, mem-bership inference, availability, or other machine learning attacks; â€¢ account for the complex attack surface of AI systems or other security abuses enabled by AI systems; and â€¢ consider risks associated with third-party AI technologies, transfer learning, and off-label use where AI systems may be trained for decision-making outside an organiza-tionâ€™s security controls or trained in one domain and then â€œfine-tunedâ€ for another. Both AI and traditional software technologies and systems are subject to rapid innovation. Technology advances should be monitored and deployed to take advantage of those devel-opments and work towards a future of AI that is both trustworthy and responsible.  \n\n> Page 39 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Appendix C: AI Risk Management and Human-AI Interaction \n\nOrganizations that design, develop, or deploy AI systems for use in operational settings may enhance their AI risk management by understanding current limitations of human-AI interaction. The AI RMF provides opportunities to clearly define and differentiate the various human roles and responsibilities when using, interacting with, or managing AI systems. Many of the data-driven approaches that AI systems rely on attempt to convert or represent individual and social observational and decision-making practices into measurable quanti-ties. Representing complex human phenomena with mathematical models can come at the cost of removing necessary context. This loss of context may in turn make it difficult to understand individual and societal impacts that are key to AI risk management efforts. Issues that merit further consideration and research include: 1. Human roles and responsibilities in decision making and overseeing AI systems need to be clearly defined and differentiated. Human-AI configurations can span from fully autonomous to fully manual. AI systems can autonomously make deci-sions, defer decision making to a human expert, or be used by a human decision maker as an additional opinion. Some AI systems may not require human oversight, such as models used to improve video compression. Other systems may specifically require human oversight. 2. Decisions that go into the design, development, deployment, evaluation, and use of AI systems reflect systemic and human cognitive biases. AI actors bring their cognitive biases, both individual and group, into the process. Biases can stem from end-user decision-making tasks and be introduced across the AI lifecycle via human assumptions, expectations, and decisions during design and modeling tasks. These biases, which are not necessarily always harmful, may be exacerbated by AI system opacity and the resulting lack of transparency. Systemic biases at the organizational level can influence how teams are structured and who controls the decision-making processes throughout the AI lifecycle. These biases can also influence downstream decisions by end users, decision makers, and policy makers and may lead to negative impacts. 3. Human-AI interaction results vary. Under certain conditions â€“ for example, in perceptual-based judgment tasks â€“ the AI part of the human-AI interaction can am-plify human biases, leading to more biased decisions than the AI or human alone. When these variations are judiciously taken into account in organizing human-AI teams, however, they can result in complementarity and improved overall perfor-mance.  \n\n> Page 40 NIST AI 100-1\n\nAI RMF 1.0 4. Presenting AI system information to humans is complex. Humans perceive and derive meaning from AI system output and explanations in different ways, reflecting different individual preferences, traits, and skills. The GOVERN function provides organizations with the opportunity to clarify and define the roles and responsibilities for the humans in the Human-AI team configurations and those who are overseeing the AI system performance. The GOVERN function also creates mechanisms for organizations to make their decision-making processes more explicit, to help counter systemic biases. The MAP function suggests opportunities to define and document processes for operator and practitioner proficiency with AI system performance and trustworthiness concepts, and to define relevant technical standards and certifications. Implementing MAP function cat-egories and subcategories may help organizations improve their internal competency for analyzing context, identifying procedural and system limitations, exploring and examining impacts of AI-based systems in the real world, and evaluating decision-making processes throughout the AI lifecycle. The GOVERN and MAP functions describe the importance of interdisciplinarity and demo-graphically diverse teams and utilizing feedback from potentially impacted individuals and communities. AI actors called out in the AI RMF who perform human factors tasks and activities can assist technical teams by anchoring in design and development practices to user intentions and representatives of the broader AI community, and societal values. These actors further help to incorporate context-specific norms and values in system design and evaluate end user experiences â€“ in conjunction with AI systems. AI risk management approaches for human-AI configurations will be augmented by on-going research and evaluation. For example, the degree to which humans are empowered and incentivized to challenge AI system output requires further studies. Data about the fre-quency and rationale with which humans overrule AI system output in deployed systems may be useful to collect and analyze.  \n\n> Page 41 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Appendix D: Attributes of the AI RMF \n\nNIST described several key attributes of the AI RMF when work on the Framework first began. These attributes have remained intact and were used to guide the AI RMFâ€™s devel-opment. They are provided here as a reference. The AI RMF strives to: 1. Be risk-based, resource-efficient, pro-innovation, and voluntary. 2. Be consensus-driven and developed and regularly updated through an open, trans-parent process. All stakeholders should have the opportunity to contribute to the AI RMFâ€™s development. 3. Use clear and plain language that is understandable by a broad audience, including senior executives, government officials, non-governmental organization leadership, and those who are not AI professionals â€“ while still of sufficient technical depth to be useful to practitioners. The AI RMF should allow for communication of AI risks across an organization, between organizations, with customers, and to the public at large. 4. Provide common language and understanding to manage AI risks. The AI RMF should offer taxonomy, terminology, definitions, metrics, and characterizations for AI risk. 5. Be easily usable and fit well with other aspects of risk management. Use of the Framework should be intuitive and readily adaptable as part of an organizationâ€™s broader risk management strategy and processes. It should be consistent or aligned with other approaches to managing AI risks. 6. Be useful to a wide range of perspectives, sectors, and technology domains. The AI RMF should be universally applicable to any AI technology and to context-specific use cases. 7. Be outcome-focused and non-prescriptive. The Framework should provide a catalog of outcomes and approaches rather than prescribe one-size-fits-all requirements. 8. Take advantage of and foster greater awareness of existing standards, guidelines, best practices, methodologies, and tools for managing AI risks â€“ as well as illustrate the need for additional, improved resources. 9. Be law- and regulation-agnostic. The Framework should support organizationsâ€™ abilities to operate under applicable domestic and international legal or regulatory regimes. 10. Be a living document. The AI RMF should be readily updated as technology, under-standing, and approaches to AI trustworthiness and uses of AI change and as stake-holders learn from implementing AI risk management generally and this framework in particular. \n\n> Page 42\n\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1", "fetched_at_utc": "2026-02-08T18:49:38Z", "sha256": "92288e1fcca5951721db4482b109fad7dd2e72207c9e9994d5fed41491c176f8", "meta": {"file_name": "AI Risk Management Framework - NIST.pdf", "file_size": 1946127, "relative_path": "pdfs\\AI Risk Management Framework - NIST.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 20922}}}
{"doc_id": "pdf-pdfs-ai-security-concerns-in-a-nutshell-fdafff40e294", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Security Concerns in a Nutshell.pdf", "title": "AI Security Concerns in a Nutshell", "text": "AI SECURITY CONCERNS IN A NUTSHELL Document history \n\nVersion  Dat e Editor  Description \n\n1.0  09.03.2023  TK24  First Release \n\nFederal Office for Information Security \n\nP.O. Box 20 03 63 \n\n53133 Bonn \n\nE-Mail: ki-kontakt@bsi.bund.de \n\nInternet: https://www.bsi.bund.de \n\nÂ© Federal Office for Information Security 2023 Table of Contents  \n\n> Federal Office for Information Security 3\n\n# Table of Contents \n\n1 Introduction ............................................................................................................................................................................................ 4 \n\n2 General Measures for IT Security of AI-Systems ................................................................................................................... 5 \n\n3 Evasion Attacks ...................................................................................................................................................................................... 6 \n\n3.1 Construction of Adversarial Examples ............................................................................................................................ 6 \n\n3.2 Evasion Attacks in Transfer Learning .............................................................................................................................. 6 \n\n3.3 Defending against Evasion Attacks ................................................................................................................................... 7 \n\n4 Information Extraction Attacks ..................................................................................................................................................... 8 \n\n4.1 Model Stealing Attacks ............................................................................................................................................................ 8 \n\n4.2 Membership Inference Attacks ........................................................................................................................................... 8 \n\n4.3 Attribute Inference Attacks................................................................................................................................................... 8 \n\n4.4 Model Inversion Attacks......................................................................................................................................................... 8 \n\n4.5 Defending against Information Extraction Attacks .................................................................................................. 9 \n\n5 Poisoning and Backdoor Attacks................................................................................................................................................... 9 \n\n5.1 Poisoning Attacks ...................................................................................................................................................................... 9 \n\n5.2 Backdoor Attacks ..................................................................................................................................................................... 10 \n\n5.3 Defending against Poisoning and Backdoor Attacks .............................................................................................. 10 \n\n6 Limitations ............................................................................................................................................................................................. 11 \n\nBibliography .................................................................................................................................................................................................... 12 Introduction  \n\n> Federal Office for Information Security 4\n\n# 1 Introduction \n\nThis guideline introduces developers to the most relevant attacks on machine learning systems and potential complementary defences. It does not claim to be comprehensive and can only offer a first introduction to the topic. \n\nIn many applications, machine learning models use sensitive information as training data or make decisions that affect people in critical areas, like autonomous driving, cancer detection, and biometric authentication. The possible impact of attacks increase as machine learning is used more and more in critical applications. Attacks that either aim at extracting data from the models or manipulating their decisions are threats that need to be considered during a risk assessment. Using pre-trained models or publicly available datasets from external sources lowers the resources needed for developing AI systems, but may also enable a variety of attacks. The datasets or models could be prepared maliciously to induce a specific behaviour during deployment, unknown to the AI developer. Furthermore, overfitting, a state in which a model has memorized the training data and does not generalize well to previously unseen data, can increase the chances of extracting private information from models or facilitate more effective evasion attacks. \n\nApart from malicious attacks on machine learning models, a lack of comprehension of their decision-making process poses a threat. The models could be learning spurious correlations from faulty or insufficient training data. Therefore, it is helpful to understand their decision process before deploying them to real-world use cases. The following chapters introduce three broad categories of possible attacks: Evasion Attacks, Information Extraction Attacks and Backdoor Attacks. Additionally a set of possible first defences for each category is introduced. 2 General Measures for IT Security of AI-Systems  \n\n> Federal Office for Information Security 5\n\n# 2 General Measures for IT Security of AI-Systems \n\nAI systems exhibit some unique characteristics that give rise to novel attacks, which are treated extensively in the following sections. AI systems are IT systems, meaning classical measures can be applied to increase IT security. Moreover, AI systems, in practice, do not operate in isolation but are embedded in a more extensive IT system consisting of various components. They can introduce additional layers of defence, e.g., by making side conditions unfavourable for attackers, beyond the level of the AI system itself (which is the last line of defence). \n\nClassical IT security measures address a wide array of topics. In-depth recommendations by the BSI can be found in the IT-Grundschutz [1]. One important measure is the documentation of all relevant facts and developer choices during the systemâ€™s development and the way the system operates. Log files should be used to monitor the systemâ€™s operation and should be regularly checked for anomalies. The responsibilities within the development process and the subsequent operation should be clearly distributed, and emergency plans should be in place. \n\nIn addition, technical protection measures on various levels should be applied. This includes classical network security, e.g., by using firewalls. To thwart attacks, it is also essential to protect the input and output of the AI system from tampering, using measures on the hardware, operating system, and software level (in particular, installing security patches as soon as possible) as appropriate for the respective threat level. Access control should be used for the AI system during development and inference time. Furthermore, access rights should be bound to authentication at an appropriate level of assurance. \n\nApart from generic classical measures, other general measures can also help address AI-specific threats. A possible safeguard for the AI system development process is to mandate background checks of the (core) developers. Another measure is to document and protect important information cryptographically for the whole AI life cycle. This can include the used data sets, pre-processing steps, pre-trained models, and the training procedure itself. Cryptographic protection can be applied using hash functions and digital signatures, which allow for verifying that no tampering has occurred at intermediate steps [2]. The amount of effort required for documentation and protection can vary greatly and should be appropriate for the use case. \n\nThe robustness of the outputs of the AI system can be increased and its susceptibility to attacks be reduced by operating multiple AI systems using different architectures or different training data redundantly. Further information may also be gleaned from other sources and allow for detecting attacks. For example, biometric fakes can be detected using additional sensors in biometric authentication. In cases where this is feasible, an additional layer of human supervision - constantly present or acting on request in cases of ambiguity - can also improve security. \n\nAttacks that aim to extract information via queries to the model can be hampered by supplying only relevant information, ignoring invalid queries, or imposing and enforcing limits on the number of allowed queries. Evasion Attacks  \n\n> 6Federal Office for Information Security\n\n# 3 Evasion Attacks \n\nWithin an evasion attack, an attacker aims to cause a misclassification during the inference phase of a machine learning model. The attacker constructs a malicious input, which is typically close to a benign sample, to conceal the attack. These inputs, denoted as adversarial examples, are generated by adding a perturbation to the input that fools the model or reduces its accuracy. Evasion attacks can be separated into targeted attacks, where the attacker forces the model to predict the desired target value, and untargeted attacks that cause a general reduction in model accuracy or prediction confidence. Evasion attacks can take place in the physical or digital world. For example, certain patterns could cause an automated car to mistake traffic signs, or a biometric camera system to mistake somebodyâ€™s identity. These patterns or perturbations may not be perceptible to humans. \n\nIn the following, a brief overview of methods to create such attacks and possible defences are outlined. The article provides key ideas instead of covering all existing methods and details. For a more in-depth reading, we refer an interested reader to the study [3] or other up-to-date research surveys on the topic. \n\n# 3.1 Construction of Adversarial Examples \n\nA popular approach to creating adversarial examples is the Fast Gradient Method (FGM) [4], which creates adversarial examples by relying on the model's gradient. The method needs white-box access, which means access to the model, including its structure, internal weights and gradients. For the attack, a perturbation pattern is calculated from the gradient of the loss function with respect to the input. It is scaled by Epsilon ðœ–ðœ– ,which describes the amount of perturbation, and added to the original sample, creating an adversarial one. The adversarial sample increases the result of the cost function for the correct label, which can result in a completely different prediction while staying visually close to the original sample. Depending on the magnitude of ðœ–ðœ– , the manipulated images are more or less noticeable to the human observer. In the case of image recognition tasks, the larger the epsilon, the easier it is for a human observer to spot the perturbation. \n\nFigure 1  shows a perturbation of ðœ–ðœ– = 0.2 added to a sample. As a result, the modelâ€™s prediction confidence decreases, and some samples are misclassified. Apart from white-box attacks like FGM, there exist black-box attacks that require only access to the model, meaning the attacker can only query the model as an oracle for confidence scores or output labels, see e.g. [5]. \n\n# 3.2 Evasion Attacks in Transfer Learning \n\nTransfer learning describes a technique in which (parts of) an existing machine learning model, called the teacher model, are retrained for a different target domain. The resulting model is called the student model. \n\nFigure 1: A perturbation of epsilon = 0.2 is added to inputs, creating adversarial examples. 3 Evasion Attacks  \n\n> Federal Office for Information Security 7\n\nThe retraining might require only a small training data set, and the computational effort might be modest in comparison to a model trained from scratch. \n\nRegarding evasion attacks, the main concern is that evasion attacks on the teacher model might also be applicable to a student model. If the teacher model is openly available, it could be misused for this purpose by an attacker. \n\n# 3.3 Defending against Evasion Attacks \n\nGiven a concrete task, a risk analysis should be performed to determine the criticality and applicability of evasion attacks. In the following, several defence methods are outlined. It is encouraged to simulate concrete attacks on your system to check the vulnerability to attacks and effectiveness of selected defence mechanisms. \n\nAdversarial Retraining \n\nAdversarial retraining consists of iteratively generating adversarial examples and repeatedly training the model on them. As a result, the robustness of the model against the selected attack methods increases. \n\nGeneralization \n\nUsing a diverse and qualitative training data set is a good way to reduce the susceptibility of the model to certain adversarial examples. If the AIâ€™s decision barriers enclose the known class too closely it may be easy to sample visually close inputs, which are detected as different class [6]. Additionally, random transformations within the bounds of the natural feature distribution, like omitting input pixels (dropout), tilting, compression, or filters can be used to increase the size and variety of the training data. \n\nDefending against Adversarial Attacks based on a teacher model \n\nThe success of adversarial attacks transferred from a teacher model to a student model may be reduced by lowering the similarity between the teacher and the student model [7]. For this purpose, the weights in the different layers of the student model need to be changed. A disadvantage is the computing time required for the adjustment. This procedure can be applied without affecting classification accuracy significantly. However, black-box attacks on the student model are still possible [7]. Information Extraction Attacks  \n\n> 8Federal Office for Information Security\n\n# 4 Information Extraction Attacks \n\nInformation extraction attacks, which are also referred to as privacy or reconstruction attacks, summarize all attacks that aim at reconstructing the model or information from its training data. They include model stealing attacks, attribute inference attacks, membership inference attacks, and model inversion attacks. Information extraction attacks often require prior knowledge about the training dataset or access to its publicly available parts. \n\n# 4.1 Model Stealing Attacks \n\nFor organizations who invested significant resources in the development of a commercial AI model, model stealing is a threat. Attackers can try to steal the modelâ€™s architecture or reconstruct it by querying the original model and feeding the answers back into their own shadow model. Model stealing can serve as a stepping stone for other attacks, e.g. generating transferable adversarial attacks based on the shadow model. \n\n# 4.2 Membership Inference Attacks \n\nIn membership inference attacks, the attacker tries to determine whether a data sample was part of a modelâ€™s training data. From a privacy perspective, determining the membership of an individualâ€™s data in a dataset or restoring its attributes can be sensitive [8]. The attack utilizes differences in model behaviour on new input data and data used for training. One possibility to implement such an attack is to train an attack model to recognize such differences [8]. For this purpose, the attacker requires at least black-box access to the predicted label, e.g. API access. For some attacks, background knowledge about the population from which the target modelâ€™s training dataset was drawn is required [8]. \n\n# 4.3 Attribute Inference Attacks \n\nIn attribute inference attacks, the attacker seeks to breach the confidentiality of the modelâ€™s training data by determining the value of a sensitive attribute associated with a specific individual or identity in the training data [9, 10]. The attacker requires access to the model and a publicly available part of the victimâ€™s dataset. Such attack methods utilize the statistical correlation of sensitive (non-public attributes) and non-sensitive (public) attributes as well as the general distribution of attributes [11]. An example for an attribute inference attack in general, is to infer sensitive attributes, e.g. the home address of a user, by using publicly available information in social networks [12]. Although the sensitive attribute might not be publicly available directly, it might be deduced combining different sources of public knowledge. \n\n# 4.4 Model Inversion Attacks \n\nModel inversion attacks aim to recover features that characterize classes from the training data. As a result, the attacker can create a representative sample for a class, which is not from the training set but shows features of the class it represents. Attacks based on generative adversarial networks (GANs) typically require only black-box access to the model, which makes the target architectures irrelevant [9]. However, attacks based on â€œDeepInversionâ€ [13, 14] require black-box access to the batch normalization layers of a neural network, which contain the average and variance of the activations. Therefore, they are architecture-dependent. The basic idea of each attack version is to search for input features, which maximize the modelâ€™s output probability for the attacked class. By gaining knowledge of the distribution of input features, the attacker is able to narrow down the search space for high-dimensional input features In GAN-based attacks, the attacker can train a GAN with a surrogate training set that shares a similar feature distribution with the actual training data. As a result, the GAN generates high-probability samples ( Figure 2 ) for a chosen class [9]. A possible attack scenario could be to recover a personâ€™s face only by having access to the outputs of a classifier trained to recognize this person. As the GAN-based attack only needs surrogate training data with e.g., sample faces, knowledge of the victimâ€™s face is not required for the attack. 5 Poisoning and Backdoor Attacks  \n\n> Federal Office for Information Security 9\n\n# 4.5 Defending against Information Extraction Attacks \n\nIt is encouraged to simulate concrete attacks on your system to check the vulnerability to attacks and effectiveness of selected defence mechanisms. \n\nDecrease Model Output \n\nAs many information extraction attacks use model confidence scores as the basis for an attack, reducing the scope of the modelâ€˜s output values or their precision might increase the effort for attackers [15]. However, as for example seen in the case of membership inference attacks, there might be attack methods just relying on class labels circumventing such a measure. \n\nData Sanitization \n\nRemoving all the sensitive parts of the data before using it for training makes it impossible for intruders to extract the data from the trained model. \n\nAvoid Overfitting \n\nPrivacy attacks benefit from the overfitting of a model. Consequently, good model generalization mitigates the risk of successful privacy attacks. This might be achieved by a large and diverse training set as well as techniques such as regularization, dropout, or dataset condensation [16]. However, effectiveness of the used methods might depend on the concrete setting at hand. \n\nDifferential Privacy \n\nDifferential privacy is a concept that helps to describe and quantify privacy in the processing of data. It demands, â€œNothing about an individual should be learnable from the database that cannot be learned without access to the databaseâ€ [17]. Differential privacy is often measured by a parameter Îµ, with lower values corresponding to greater privacy. Given a concrete application, the correct choice of Îµ is difficult to determine because there is a trade-off between privacy and the accuracy of the algorithm or model that uses the database. Finding suitable parameters might be costly in terms of computational effort. A model trained with differential private data might still be susceptible to attribute inference attacks since DP does not explicitly aim to protect attribute privacy [9]. \n\n# 5 Poisoning and Backdoor Attacks \n\n# 5.1 Poisoning Attacks \n\nThe attack goals of data poisoning are the malfunctioning or performance degradation of machine learning models [18]. Therefore, the adversary manipulates the training dataset used by a machine learning model. \n\nA computationally inexpensive poisoning attack consists of flipping the label of an input to the desired class. Subsequently, the poisoned samples are injected into the training set and used during model training. This attack method may be effective with only a small number of poisoned samples. However, the flipped training sample labels might be discoverable by manual inspection of the training dataset [19]. \n\nFigure 2: A horse from the CIFAR10 dataset on the left vs. an artificial one created by a GAN trained on a surrogate dataset on the right. Poisoning and Backdoor Attacks  \n\n> 10 Federal Office for Information Security\n\n# 5.2 Backdoor Attacks \n\nBackdoor attacks are targeted poisoning attacks. A backdoor attack aims at creating a predetermined response to a trigger in an input while maintaining the systemâ€™s performance in its absence. In the image domain, attack triggers can take the form of patterns or hard-to-see projections onto the input images [18]. \n\nThe trigger is implanted in a subset of training data. The subset is labelled with the adversaryâ€™s chosen class. The key idea is that the model learns to connect the trigger with a class determined by the adversary ( Figure 3). An attack is successful when a backdoored model behaves normally when encountering benign images but predicts the adversaryâ€™s chosen label when presented with triggered images [19]. The success rate of backdoor attacks depends on the modelâ€™s architecture, the number and rate of triggered images, and the trigger patterns chosen by the attacker. A trigger with a high success rate in a model does not necessarily negatively influence the overall model performance on benign inputs. Therefore, backdoored models are hard to detect by inspecting their performance alone [18]. Research shows that backdoor attacks can be successful with only a small number of triggered training samples. In addition, when using pretrained models from public sources, it should be noted that through transfer learning ( 3.2 ), risks like built-in backdoors could also be transferred. \n\n# 5.3 Defending against Poisoning and Backdoor Attacks \n\nIt is encouraged to simulate concrete attacks on your system to check the vulnerability to attacks and effectiveness of selected defence mechanisms. \n\nUse Trusted Sources \n\nDepending on the security requirements for the use case, it is essential to make adequate efforts to ensure that the supply chain and the sources of training data, models, and code are known and trustworthy. Publicly available models could contain backdoors. \n\nSearch for Triggers \n\nThe triggers used for backdoors rely on logical shortcuts between the target class and the input. To find a shortcut, one must determine the minimal input change required to shift the modelâ€™s prediction. If such a change is minimal, a backdoor may have been found [20]. Another method for the image domain is to randomly mask parts of an input image and examine how the modelâ€˜s prediction changes. If the input image contains a trigger, masking it will change the modelâ€™s prediction [21]. \n\nRetraining \n\nRetraining a model with benign training samples, if available, reduces the probability of backdoors being successful [18]. The degree of success depends on the size and quality of the clean dataset [22]. Research suggests that even with a small retraining dataset, the vulnerability of a model to backdoor attacks significantly drops, while its accuracy may be slightly reduced [22]. \n\nNetwork Pruning \n\nFor network pruning, benign data samples are fed into the trained neural network, and their average activation is measured. Neurons without a high level of activation can be trimmed without substantially \n\nFigure 3: The trigger (left) is placed in the training set in a picture of a bird labelled as a cat. A model trained with these triggered examples is likely to classify pictures containing the trigger as cats instead of birds during inference. 6 Limitations  \n\n> Federal Office for Information Security 11\n\nreducing the modelâ€™s accuracy. In the process, potential backdoors can be removed as well. Similar to retraining, the complete success of the measure cannot be guaranteed [20]. \n\nAutoencoder Detection \n\nAn autoencoder is trained with a benign dataset whose feature distribution is close to the training dataset. As a result, the trained autoencoder may be able to detect manipulated data samples that lie outside of the learned distribution [22]. \n\nRegularization \n\nRegularization can lower the success rate of backdoor attacks without significantly degrading the baseline performance on benign inputs [18]. \n\n# 6 Limitations \n\nThe introduced defences can help counter attacks on machine learning models but can also adversely affect other aspects of the model. They often require more computational time. Moreover, an increased attack resilience can lower the general performance of the model. It is advisable to balance attack resilience and performance, as well as other relevant aspects, based on the expected risk of the overall AI system. Adaptive attacks on machine learning models might circumvent existing defence methods. However, the named defence methods can increase the attack effort, be it through higher computational costs or a larger attack budget needed. \n\nFor further reading on attacks on machine learning, we refer the reader to the study [3] or other up-to-date publications like [23] and [24]. Limitations  \n\n> 12 Federal Office for Information Security\n\n# Bibliography \n\n[1]  Bundesamt fÃ¼r Sicherheit in der Informationstechnik, â€žIT-Grundschutz-Kompendium,â€œ Bonn, Germany, 2022. \n\n[2]  C. Berghoff, â€žProtecting the integrity of the training procedure of neural networks,â€œ Bundesamt fÃ¼r Sicherheit in der Informationstechnik, Bonn, Germany, 2020. \n\n[3]  Federal Office for Information Security, â€žSecurity of AI-Systems: Fundamentals,â€œ Bonn, Germany, 2022. \n\n[4]  I. J. Goodfellow, J. Shlens und C. Szegedy, â€žExplaining and Harnessing Adversarial Examples,â€œ in 3rd International Conference on Learning Representations , San Diego, CA, USA, 2015. \n\n[5]  J. Chen, M. I. Jordan und M. J. Wainwright, â€žHopSkipJumpAttack: A Query-Efficient Decision-Based Attack,â€œ in IEEE Symposium on Security and Privacy , San Francisco, CA, USA, 2020. \n\n[6]  D. Stutz, M. Hein und B. Schiele, â€žDisentangling Adversarial Robustness and Generalization,â€œ in IEEE Conference on Computer Vision and Pattern Recognition , Long Beach, CA, USA, 2019. \n\n[7]  B. Wang, Y. Yao, B. Viswanath, H. Zheng und B. Y. Zhao, â€žWith Great Training Comes Great Vulnerability: Practical Attacks against Transfer Learning,â€œ in 27th USENIX Security Symposium, USENIX Security , Baltimore, MD, USA, 2018. \n\n[8]  R. Shokri, M. Stronati, C. Song und V. Shmatikov, â€žMembership Inference Attacks Against Machine Learning Models,â€œ in IEEE Symposium on Security and Privacy , San Jose, CA, USA, 2017. \n\n[9]  Y. Zhang, R. Jia, H. Pei, W. Wang, B. Li und D. Song, â€žThe Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks,â€œ in IEEE/CVF: Conference on Computer Vision and Pattern Recognition , Seattle, WA, USA, 2020. \n\n[10]  B. Z. H. Zhao, A. Agrawal, C. Coburn, H. J. Asghar, R. Bhaskar, M. A. Kaafar, D. Webb und P. Dickinson, â€žOn the (In)Feasibility of Attribute Inference Attacks on Machine Learning Models,â€œ in IEEE European Symposium on Security and Privacy, EuroS&P , Vienna, Austria, 2021. \n\n[11]  S. Mehnaz, S. V. Dibbo, E. Kabir, N. Li und E. Bertino, â€žAre Your Sensitive Attributes Private? Novel Model Inversion Attribute Inference Attacks on Classification Models,â€œ in 31st USENIX Security Symposium , Boston, MA, USA, 2022. \n\n[12]  N. Z. Gong und B. Liu, â€žAttribute Inference Attacks in Online Social Networks,â€œ ACM Trans. Priv. Secur. 21, pp. 3:1--3:30, 2018. \n\n[13]  H. Yin, P. Molchanov, J. M. Alvarez und Z. Li, â€žDreaming to Distill: Data-free Knowledge Transfer via DeepInversion,â€œ in Conference on Computer Vision and Pattern Recognition , Seattle, WA, USA, 2020. \n\n[14]  A. Chawla, H. Yin, P. Molchanov und J. Alvarez, â€žData-free Knowledge Distillation for Object Detection,â€œ in Winter Conference on Applications of Computer Vision , Waikoloa, HI, USA, 2021. \n\n[15]  M. Fredrikson, S. Jha und T. Ristenpart, â€žModel inversion attacks that exploit confidence information and basic countermeasures,â€œ in Proceedings of the 22nd ACM Conference on Computer and Communications Security , Denver, CO, USA, 2015. \n\n[16]  T. Dong, B. Zhao und L. Lyu, â€žPrivacy for Free: How does Dataset Condensation Help Privacy?,â€œ in \n\nProceedings of the 39th International Conference on Machine Learning , Baltimore, MD, USA, 2022. 6 Limitations  \n\n> Federal Office for Information Security 13\n\n[17]  T. Dalenius, â€žTowards a Methodology for Statistical Disclosure Control,â€œ Statistik Tidskrift 15, p. 429â€“ 444, 1977. \n\n[18]  L. Truong, C. Jones, B. Hutchinson, A. August, B. Praggastis, R. Jasper, N. Nichols und A. Tuor, â€žSystematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers,â€œ in IEEE/CVF Conference on Computer Vision and Pattern Recognition , Seattle, WA, USA, 2020. \n\n[19]  X. Chen, C. Liu, B. Li, K. Lu und D. Song, â€žTargeted Backdoor Attacks on Deep Learning,â€œ CoRR, 2017. \n\n[20]  B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng und B. Y. Zhao, â€žNeural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks,â€œ in IEEE Symposium on Security and Privacy , San Francisco, CA, USA, 2019. \n\n[21]  S. Udeshi, S. Peng, G. Woo, L. Loh, L. Rawshan und S. Chattopadhyay, â€žModel Agnostic Defence Against Backdoor Attacks in Machine Learning,â€œ in IEEE Transactions on Reliability , 2022. \n\n[22]  Y. Liu, Y. Xie und A. Srivastava, â€žNeural Trojans,â€œ in IEEE International Conference on Computer Design ,Boston, MA, USA, 2017. \n\n[23]  NCSA, â€žAI systems: develop them securely,â€œ 15 02 2023. [Online]. Available: https://english.aivd.nl/latest/news/2023/02/15/ai-systems-develop-them-securely. \n\n[24]  A. Malatras, I. Agrafiotis und M. Adamczyk, â€žSecuring machine learning algorithms,â€œ ENISA, 2021.", "fetched_at_utc": "2026-02-08T18:50:12Z", "sha256": "fdafff40e29442b44abb99854848c38b0b29309a168eb93f1e83204f966d45b9", "meta": {"file_name": "AI Security Concerns in a Nutshell.pdf", "file_size": 440346, "relative_path": "pdfs\\AI Security Concerns in a Nutshell.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 6040}}}
{"doc_id": "pdf-pdfs-artificial-intelligence-systems-and-the-gdpr-belgium-8072688f58c2", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Artificial Intelligence Systems and the GDPR - Belgium.pdf", "title": "Artificial Intelligence Systems and the GDPR - Belgium", "text": "(Original version â€“ version December 2024) \n\n> 1\n> Artificial Intelligence Systems and the GDPR\n> A Data Protection Perspective\n\n# Data Protection Authority of \n\n# Belgium \n\n# General Secretariat \n\n# Artificial Intelligence Systems and the GDPR \n\n# A Data Protection Perspective (Original version â€“ version December 2024) \n\n2\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nEXECUTIVE SUMMARY  ................................ ................................ ................................ .................... 3\n\nOBJECTIVE OF THIS INFORMATION BROCHURE  ................................ ................................ .... 4\n\nAUDIENCE FOR THIS INFORMATION BROCHURE  ................................ ................................ ... 5\n\nWHAT IS AN AI SYSTEM ?  ................................ ................................ ................................ ............... 6\n\nGDPR & AI ACT REQUIREMENTS  ................................ ................................ ................................ .. 8\n\nLAWFUL , FAIR , AND TRANSPARENT PROCESSING ................................ ................................ ................................ ........ 8\n\nPURPOSE LIMITATION AND DATA MINIMISATION  ................................ ................................ ................................ ......... 9\n\nDATA ACCURACY AND UP -TO -DATENESS  ................................ ................................ ................................ ....................... 9\n\nSTORAGE LIMITATION  ................................ ................................ ................................ ................................ ............................ 9\n\nAUTOMATED DECISION -MAKING  ................................ ................................ ................................ ................................ ...... 10 \n\nSECURITY OF PROCESSING  ................................ ................................ ................................ ................................ ................. 11 \n\nDATA SUBJECT RIGHTS  ................................ ................................ ................................ ................................ ....................... 13 \n\nACCOUNTABILITY  ................................ ................................ ................................ ................................ ................................ .. 14 \n\nMAKING COMPLIANCE STRAIGHTFORWARD: USER STORIES FOR AI SYSTEMS IN \n\nLIGHT OF GDPR AND AI ACT REQUIREMENTS  ................................ ................................ ...... 16 \n\nREQUIREMENTS OF LAWFUL , FAIR , AND TRANSPARENT PROCESSING  ................................ ............................... 16 \n\nREQUIREMENTS OF PURPOSE LIMITATION AND DATA MINIMIZATION  ................................ ................................ . 17 \n\nREQUIREMENTS OF DATA ACCURACY AND UP -TO -DATENESS  ................................ ................................ .............. 18 \n\nREQUIREMENT OF SECURE PROCESSING  ................................ ................................ ................................ ....................... 19 \n\nREQUIREMENT OF (THE ABILITY OF DEMONSTRATING ) ACCOUNTABILITY  ................................ ....................... 20 \n\nREFERENCES ................................ ................................ ................................ ................................ ..... 21 (Original version â€“ version December 2024) \n\n3\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Executive summary \n\nThis information brochure outlines the complex interplay between the General Data \n\nProtection Regulation (GDPR) i and the Artificial Intelligence (AI) Act ii in the context of AI \n\nsystem development. The document emphasizes the importance of aligning AI systems \n\nprocessing personal data with data protection principles while addressing the unique \n\nchallenges posed by AI technologies. \n\nKey points include: \n\nâ€¢ GDPR and AI Act alignment: the brochure highlights the complementary nature of \n\nthe GDPR and AI Act in ensuring lawful, fair, and transparent processing of personal \n\ndata in AI systems. \n\nâ€¢ AI system definition: the document provides a clear definition of AI systems and \n\noffers illustrative examples to clarify the concept. \n\nâ€¢ data protection principles: the brochure delves into core GDPR principles such as \n\nlawfulness, fairness, transparency, purpose limitation, data minimization, accuracy, \n\nstorage limitation, and data subject rights in the context of AI systems. \n\nâ€¢ accountability: the importance of accountability is emphasized, with specific \n\nrequirements outlined for both the GDPR and AI Act. \n\nâ€¢ security: the document highlights the need for robust technical and organizational \n\nmeasures, to protect personal data processed by AI systems. \n\nâ€¢ human oversight: The crucial role of human oversight in AI system development \n\nand operation is emphasized, particularly for high -risk AI systems. \n\nBy providing insights into the legal framework and practical guidance, this information \n\nbrochure aims to empower legal professionals, data protection officers, technical \n\nstakeholders, including controllers and processors, to understand and comply with the \n\nGDPR and AI Act requirements when developing and deploying AI systems. (Original version â€“ version December 2024) \n\n4\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Objective of this information brochure \n\nThe General Secretariat of the Belgian Data Protection Authority monitors social, \n\neconomic, and technological developments that impact the protection of personal data iii .\n\nIn recent years, AI technologies have experienced exponential growth, revolutionizing \n\nvarious industries and significantly impacting the way data is collected, processed, and \n\nutilized. However, this rapid advancement has brought about complex challenges \n\nre garding data privacy, transparency, and accountability. \n\nIn this context, the General Secretariat of the Belgian Data Protection Authority publishes \n\nthis information brochure to provide insights on data protection and the development and \n\nimplementation of AI systems. \n\nUnderstanding and adhering to the GDPR principles and provisions is crucial for ensuring \n\nthat AI systems operate ethically, responsibly, and in compliance with legal standards. This \n\ninformation brochure aims to elucidate the GDPR requirements specifically applicable to \n\nAI systems that process personal data , offering more clarity and useful insights to \n\nstakeholders involved in the development, implementation, and (internal) regulation of AI \n\ntechnologies. \n\nIn addition to the GDPR, the Artificial Intelligence Act (AI Act), which entered into force on \n\n1st of August 2024 , will also significantly impact the regulation of AI system development \n\nand use. This information brochure will also address the requirements of the AI Act. \n\nThe examples included in this brochure serve a purely pedagogical purpose, they are \n\nsometimes hypothetical and do not take into account certain exceptions iv and \n\nimperfections v in the regulation. (Original version â€“ version December 2024) \n\n5\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Audience for this information brochure \n\nThis information brochure is intended for a diverse audience comprising legal \n\nprofessionals, Data Protection Officers (DPOs), and individuals with technical backgrounds \n\nincluding business analysts, architects, and developers. It also targets controllers and \n\nprocessors involved in the development and deployment of AI systems. Given the \n\nintersection of legal and technical considerations inherent in the application of the G DPR \n\nto AI systems, this information brochure seeks to bridge the gap between legal \n\nrequirements and technical implementation. \n\nLegal professionals and DPOs play a crucial role in ensuring organizational compliance with \n\nGDPR obligations, specifically those relevant to AI systems that process personal data. By \n\nproviding insights into GDPR requirements specific to AI, this information brochure equips \n\nlegal professionals and DPOs with useful knowledge to navigate the complexities of AI -\n\nrelated data processing activities, assess risks, and implement appropriate measures. \n\nAt the same time, individuals with technical backgrounds such as business analysts, \n\narchitects, and developers are integral to the design, development, and deployment of AI \n\nsystems. Recognizing their pivotal role, this information brochure aims to elucidate GDPR \n\nrequirements in a manner accessible to technical stakeholders. \n\nConcrete examples are incorporated into the text to illustrate how GDPR principles \n\ntranslate into practical considerations during the lifecycle of AI projects. By offering \n\nrelatively simple and actionable insights, this information brochure empowers \n\nprofessionals with various backgrounds to design AI systems that are compliant with \n\nGDPR obligations , embed data protection -by -design principles, and mitigate potential legal \n\nand ethical risks. (Original version â€“ version December 2024) \n\n6\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# What is an AI system ? \n\nThe term \"AI system\" encompasses a wide range of interpretations. \n\nThis information brochure will not delve into the intricacies and nuances that distinguish \n\nthese various definitions. \n\nInstead, we will begin by examining the definition of an AI system as outlined in the AI Act vi :\n\nFor the purposes of this Regulation, the following definitions apply: \n\n(1) â€˜AI systemâ€™ means a machine -based system that is designed to operate with varying \n\nlevels of autonomy and that may exhibit adaptiveness after deployment, and that, for \n\nexplicit or implicit objectives, infers, from the input it receives, how to generate outputs \n\nsuch as predictions, content, recommendations, or decisions that can influence physical \n\nor virtual environments; \n\nIn other terms : \n\nAn AI system is a computer system specifically designed to analyze data, identify patterns, \n\nand use that knowledge to make informed decisions or predictions. \n\nIn some cases, AI systems can learn from data and adapt over time. This learning capability \n\nallows them to improve their performance, identify complex patterns across different data \n\nsets, and make more accurate or nuanced decisions. \n\nExamples of AI systems in everyday life: \n\nSpam filters in email : spam filters analyze incoming emails and identify patterns that \n\ndistinguish spam messages from legitimate emails. Over time, as people mark emails as \n\nspam or not spam, the AI system can learn and improve its filtering accuracy. This is an \n\nexample of an AI system that meets the criteria of an AI system :\n\nâ€¢ machine -based system: it's a computer program. \n\nâ€¢ analyzes data: it analyzes the content of emails. \n\nâ€¢ identifies patterns: it identifies patterns in emails that suggest spam. \n\nâ€¢ makes decisions: it decides whether to categorize an email as spam or not. \n\nRecommendation systems on streaming services : movie streaming services utilize AI \n\nsystems to generate recommendations for users. These systems analyze a user's past \n\nviewing habits, along with the habits of similar users, to recommend content they might be \n\ninterested in. This is another example of an AI system : (Original version â€“ version December 2024) \n\n7\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nâ€¢ machine -based system: it's a computer program. \n\nâ€¢ analyzes data: it analyzes a user's viewing/listening history. \n\nâ€¢ identifies patterns: it identifies patterns in user preferences and those of similar \n\nusers. \n\nâ€¢ makes recommendations: it recommends content based on the identified patterns. \n\nVirtual assistants : virtual assistants respond to voice commands and complete tasks like \n\nsetting alarms, playing music, or controlling smart home devices. These systems use \n\nspeech recognition and natural language processing to understand user requests and take \n\naction. This is again an example of an AI system :\n\nâ€¢ machine -based system: it's a computer program. \n\nâ€¢ analyzes data: it analyzes user voice commands. \n\nâ€¢ identifies patterns: it identifies patterns in speech to understand user requests. \n\nâ€¢ makes decisions: it decides how to respond to commands based on its \n\nunderstanding. \n\nâ€¢ may exhibit adaptiveness: some virtual assistants can learn user preferences and \n\nadapt their responses over time. \n\nAI -powered medical imaging analysis : many hospitals and healthcare providers are utilizing \n\nAI systems to assist doctors in analyzing medical images, such as X -rays, CT scans, and \n\nMRIs. These systems are trained on vast datasets of labeled medical images, allowing them \n\nto identify patterns and potential abnormalities. \n\nâ€¢ machine -based system: it's a computer program. \n\nâ€¢ analyzes data: it analyzes the digital medical images. \n\nâ€¢ identifies patterns: it identifies patterns in the images that might indicate the \n\npresence of a disease or abnormality. \n\nâ€¢ supports decision -making: the system highlights potential areas of concern in the \n\nimages, which can help doctors make more informed diagnoses. (Original version â€“ version December 2024) \n\n8\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# GDPR & AI Act requirements \n\n## Lawful, fair, and transparent processing \n\nThe GDPR requires lawfulness, fairness and transparency. \n\nLeveraging GDPR lawfulness of processing : The GDPR establishes six legal bases for \n\nprocessing personal data in Article 6 (consent, contract, legal obligation, vital interests, \n\npublic interest , and legitimate interest svii ). These same legal bases remain applicable for AI \n\nsystems that process personal data under the AI Act. \n\nProhibited AI Systems : t he AI Act introduces additional prohibitions beyond the GDPR for \n\ncertain high -risk AI systems. While the GDPR focuses on protecting personal data through \n\nvarious principles, the AI Act directly prohibits specific types of high -risk AI applications. \n\nHere ar e some examples: \n\nâ€¢ Social scoring systems: these systems assign a score to individuals based on \n\nvarious factors, potentially leading to discrimination and limitations on \n\nopportunities. \n\nâ€¢ AI systems for real time remote biometric identification for the purpose of law \n\nenforcement in public places (with limited exceptions) : these systems raise \n\nconcerns about privacy, freedom of movement, and potential misuse for mass \n\nsurveillance. \n\nFairness: \n\nâ€¢ While the AI Act doesn't have a dedicated section titled â€œfairnessâ€, it builds upon \n\nthe GDPR's principle of fair processing (art. 5.1.a) as the AI Act focuses on \n\nmitigating bias and discrimination in the development, deployment, and use of AI \n\nsystems. \n\nTransparency: \n\nâ€¢ the AI Act requires a baseline level of transparency for certain AI systems. This \n\nmeans users should be informed that they're interacting with an AI system. For \n\ninstance, a chatbot could begin an interaction with a message like \"Hello, I am \n\nNelson, a chatbot . How can I assist you today?\" \n\nâ€¢ the AI Act requires a higher transparency level for high -risk AI systems. This \n\nincludes providing clear and accessible information about how data is used in these \n\nsystems, particularly regarding the decision -making process. Users should \n\nunderstand the factors influencing AI -based decisions and how potential bias is \n\nmitigated. (Original version â€“ version December 2024) \n\n9\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Purpose limitation and data minimi sation \n\nThe GDPR requires purpose limitation (art. 5.1.b) and data minimi sation (art. 5.1.c) . This \n\nmeans personal data must be collected for specific and legitimate purposes, and limited to \n\nwhat is necessary for those purposes. Th ese principles ensure that AI systems don't use \n\npersonal data for purposes beyond their intended function or collect excessive data .\n\nThe AI Act strengthens the principle of purpose limitation â€“ from the GDPR â€“ for high -risk \n\nAI systems by emphasizing the need for a well -defined and documented intended purpose. \n\nHypothetical e xample : A loan approval AI system of a financial institution , in addition to \n\nstandard identification data and credit bureau information, also utilizes geolocation data \n\n(e.g., past locations visited) and social media data (e.g., friends' profiles and interests) of a \n\ndata subject . This extensive data collection, including geolocation and social media data, \n\nraises concerns about the system's compliance with the GDPR. \n\n## Data accuracy and up-to -dateness \n\nThe GDPR requires personal data to be accurate and, where necessary, kept up -to -date \n\n(art. 5.1.d) . Organizations must take reasonable steps to ensure this. The AI Act builds upon \n\nthis principle by requiring high -risk AI systems to use high -quality and unbiased data to \n\nprevent discriminatory outcomes. \n\nHypothetical example : a financial institution develops a n AI system to automate loan \n\napprovals. The system analyzes various data points about loan applicants, including credit \n\nhistory, income, and demographics (postal code). However, t he training data for the AI \n\nsystem unknowingly reflects historical biases : the data stems from a period when loans \n\nwere more readily granted in wealthier neighborhoods ( with a higher average income) . The \n\nAI system perpetuates these biases as l oan applicants from lower -income neighborhoods \n\nmight be systematically denied loans, even if they are financially qualified. This results in a \n\ndiscriminatory outcome , and might raise serious concerns about the system's compliance \n\nwith the AI Act. \n\n## Storage limitation \n\nThe GDPR requires personal data to be stored only for as long as necessary to achieve the \n\npurposes for which it was collected (art. 5.1.e) . The AI Act doesn't explicitly introduce \n\nan other or an extra requirement on storage limitation for high -risk AI systems. (Original version â€“ version December 2024) \n\n10 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Automated decision -making \n\nThe GDPR and the AI Act both address the importance of human involvement in automated \n\ndecision -making processes that impact individuals. However, they differ in their focus: \n\nâ€¢ The GDPR grants individuals the right not to be subject solely to automated \n\nprocessing for decisions that produce legal effects concerning them ( art . 22). This \n\nmeans data subjects have the right to request a reconsideration of an automated \n\ndecision by a human decision -maker. This functions as an individual right to \n\nchallenge decisions perceived as unfair or inaccurate. \n\nâ€¢ The AI Act strengthens the focus on human involvement by requiring meaningful \n\nhuman oversight throughout the development, deployment, and use of high -risk AI \n\nsystems. This acts as a governance measure to ensure responsible AI development \n\nand use. Human ove rsight under the AI Act encompasses a broader range of \n\nactivities than just reconsideration of individual decisions. It includes, for example, \n\nreviewing the AI system's training data and algorithms for potential biases, \n\nmonitoring the system's performance, and intervening in critical decision -making \n\npathways. \n\nIn essence, the GDPR empowers individuals to object to solely automated decisions, while \n\nthe AI Act requires proactive human oversight for high -risk AI systems to safeguard \n\nagainst potential biases and ensure responsible development and use of such systems. \n\nHypothetical example : a government agency uses an AI system to assess eligibility for \n\nsocial welfare benefits based on income, employment status, and family situation .\n\nFollowing the GDPR, i ndividuals have the right not to be subject solely to automated \n\nprocessing for social welfare benefits eligibility ( art. 22). This means they can request a\n\nreconsideration of an automated decision by a human decision -maker .\n\nFollowing the AI Act, this AI system is classified as an high -risk system (as it has a \n\nsignificant impact on individuals' livelihoods). This requires the government agency to \n\nimplement human oversight throughout the development, deployment, and use of the AI \n\nsystem. (Original version â€“ version December 2024) \n\n11 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Security of Processing \n\nBoth the G DPR and the AI Act emphasize the importance of securing personal data \n\nthroughout its processing lifecycle. However, AI systems introduce specific risks that \n\nrequire additional security measures beyond traditional data protection practices. \n\nThe GDPR requires organizations to implement technical and organizational measures \n\n(TOMs) that are appropriate to the risk associated with their data processing activities. This \n\ninvolves conducting risk assessments to identify potential threats and vulnera bilities. The \n\nselected TOMs should mitigate these risks and ensure a baseline level of security for \n\npersonal data. \n\nThe AI Act builds upon this foundation by mandating robust security measures for high -\n\nrisk AI systems. This is because AI systems introduce specific risks that go beyond \n\ntraditional data processing, such as: \n\nâ€¢ potential bias in training data: biased training data can lead to biased decisions by \n\nthe AI system, impacting individuals unfairly. \n\nâ€¢ manipulation by unauthorized individuals: for example, a hacker could potentially \n\nmanipulate the AI system's training data to influence its decisions in a harmful way. \n\nImagine a system trained to approve loan applications being tricked into rejecting \n\nqualified applicants based on irrelevant factors. \n\nTo address these unique risks, the AI Act emphasizes proactive measures such as: \n\nâ€¢ identifying and planning for potential problems: This involves brainstorming what \n\ncould go wrong with the AI system and how likely it is to happen (risk assessment). \n\nThis is a core practice under both the GDPR and AI Act. \n\nâ€¢ continuous monitoring and testing: This involves regularly evaluating the AI \n\nsystem's performance for several aspects including: \n\no security flaws: identifying vulnerabilities in the system's code or design that \n\ncould be exploited by attackers. \n\no bias: checking for potential biases in the system's training data or decision -\n\nmaking processes. \n\nâ€¢ human oversight: the AI Act emphasizes the importance of meaningful human \n\noversight throughout the development, deployment, and use of high -risk AI \n\nsystems. This ensures that humans are involved in critical decisions and (Original version â€“ version December 2024) \n\n12 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nunderstand the system's vulnerabilities. Human oversight under the AI Act goes \n\nbeyond just security processes and encompasses various aspects, such as: \n\no reviewing training data and algorithms for potential biases. \n\no monitoring the system's performance for fairness, accuracy, and potential \n\nunintended behaviour .\n\no intervening in critical decision -making pathways, especially when they \n\ncould significantly impact individuals. \n\nExample : AI -powered Lung Cancer Diagnosis System .\n\nAn AI system used by a hospital to diagnose lung cancer exemplifies a high -risk AI system \n\ndue to several factors: \n\nâ€¢ highly sensitive data: it processes highly sensitive personal data, including patients' \n\nhealth information (lungs) and diagnoses (special category data under article 9 of \n\nthe GDPR) ;\n\nâ€¢ data breach impact: a data breach could expose critical health information about \n\npatients, potentially leading to privacy violations and reputational harm for the \n\nhospital ;\n\nâ€¢ life -altering decisions: the system's output directly impacts patients' lives. A \n\ndiagnosis based on inaccurate or compromised data could have serious \n\nconsequences for their health and well -being. \n\nBoth the GDPR and the AI Act emphasize the importance of security measures for data \n\nprocessing activities, especially those involving sensitive data. \n\nâ€¢ the GDPR establishes a foundation for data security: It requires organizations to \n\nimplement appropriate technical and organizational measures (TOMs) to protect \n\npersonal data based on a risk assessment. For health data, these measures would \n\nbe particularly strong due to its sensitive nature. Examples under the GDPR could \n\ninclude: \n\no data encryption: encrypting patient data at rest and in transit ensures its \n\nconfidentiality even if a breach occurs ;\n\no access controls: implementing strict access controls limits who can access \n\nand modify patient data ;\n\no penetration testing: regularly conducting penetration tests helps identify \n\nand address vulnerabilities in the system's security posture ;\n\no logging and auditing: maintaining detailed logs of system activity allows for \n\nmonitoring and investigation of any suspicious behavior. \n\nâ€¢ The AI Act builds upon this foundation for high -risk AI systems: recognizing the \n\nspecific risks of AI, the AI Act mandates robust security measures. These might (Original version â€“ version December 2024) \n\n13 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\ninclude additional measures tailored to the  specific vulnerabilities of the AI system, \n\nsuch as data validation and quality assurance : t he AI Act emphasizes the \n\nimportance of ensuring the quality and integrity of the data used to train and \n\noperate the AI system. This could involve techniques for: \n\no data provenance: tracking the origin of data to identify potential sources of \n\nbias or manipulation in the training data, such as incorrect X -ray labeling. \n\no anomaly detection: identifying and flagging unusual patterns in the training \n\ndata that might indicate malicious tampering, such as a sudden influx of X -\n\nrays with unrealistic characteristics. \n\no human review of high -risk data points: Having healthcare professionals \n\nreview critical X -rays before they are used to train the AI system, especially \n\nthose that show unusual features or could significantly impact patient \n\noutcomes. \n\nBy implementing these security measures the hospital can mitigate the risks associated \n\nwith the AI -powered lung cancer diagnosis system and ensure patient privacy, data \n\nsecurity, and ultimately, the best possible patient outcomes. \n\n## Data Subject Rights \n\nThe GDPR grants natural persons data subject rights, empowering them to control their \n\npersonal data and how it's used. These rights include access (seeing what data is \n\nprocessed, art. 15 ), rectification (correcting inaccurate data and completing data , art. 16 ), \n\nerasure (requesting data deletion , art. 17 ), restriction of processing (limiting how data is \n\nused , art. 18 ), and data portability (transferring data to another service , art. 20 ). \n\nTo effectively exercise these rights, natural persons need to understand how their data is \n\nbeing processed . The AI Act reinforces this by emphasizing the importance of clear \n\nexplanations about how data is used in certain AI systems. With this transparency, \n\nindividuals can make informed decisions about their data and utilize their data subject \n\nrights more effectively. \n\nExample : an AI system used to determine premiums for life insurance assigns a relatively \n\nhigh premium to a particular customer (data subject). The AI Act entitles this customer to \n\na clear explanation of how their premium is calculated. For example, the insurer (data \n\ncontroller) could explain that various data points were u sed, such as medical problems \n\ncustomers have faced in the past. This information, in turn, allows the customer to exercise \n\ntheir data subject rights under the GDPR , such as the right to rectification (correction of \n\ninaccurate personal data or completion of personal data ). (Original version â€“ version December 2024) \n\n14 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Accountability \n\nThe GDPR requires (organizations to demonstrate ) accountability for personal data \n\nprocessing through several measures , such as : \n\nâ€¢ Transparent processing: individuals must understand how their data is collected, \n\nused, stored and shared (f.e. by a clear and concise data protection statement , by \n\ndata subject access rights , â€¦) . This transparency allows them to see if their data is \n\nbeing handled lawfully and fairly ;\n\nâ€¢ Policies and procedures for handling personal data: documented policies ensure \n\nconsistent data handling practices across the organization ;\n\nâ€¢ Documented legal basis for processing : for each data processing activity, \n\norganizations need documented proof of the lawful justification (consent, contract, \n\nlegitimate interest, etc.) ;\n\nâ€¢ Keeping different records (like the Register Of Processing Activities ( ROPA ), data \n\nsubject requests, data breaches) is required: maintaining accurate records \n\ndemonstrates a commitment to accountability and allows organizations to prove \n\ncompliance during audits or investigations ;\n\nâ€¢ Security measures : implementing and correctly maintaining appropriate technical \n\nand organizational measures (TOMs) to protect personal data is crucial for \n\ndemonstrating accountability ;\n\nâ€¢ A Data Protection Impact Assessment ( DPIAs ) is required in some cases: these are \n\nmandatory when processing high -risk data or implementing new technologies ;\n\nâ€¢ A Data Protection Officer ( DPO ) is required in some cases: f.e. governmental \n\norganizations, regardless of their core activities, are required to have a DPO. \n\nWhile the AI Act doesn't have a dedicated section on demonstrating accountability, it \n\nbuilds upon the GDPR's principles. The AI Act requires organizations to implement : \n\nâ€¢ a two -step risk management approach for AI systems. First, there's an initial \n\nclassification process that categorizes the risk the AI poses to individuals (ranging \n\nfrom minimal to high). \n\nFor high -risk systems, a more in -depth risk assessment is required in some cases .\n\nThis dives deeper into the specific risks and identifies potential harms associated \n\nwith the AI system , and is also called a FRIA (Fundamental Rights Impact \n\nAssessment) ;(Original version â€“ version December 2024) \n\n15 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nâ€¢ clear documentation of the design and implementation of AI systems ;\n\nâ€¢ processes dealing with human oversight in high -risk AI systems. This could involve \n\nhuman intervention or approval for critical decisions made by the AI system ;\n\nâ€¢ a formal incident reporting process for reporting incidents related to AI system \n\nmalfunctions or unintended behaviour .(Original version â€“ version December 2024) \n\n16 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Making compliance straightforward: user \n\n# stories for AI systems in light of GDPR and AI Act \n\n# requirements \n\nTranslating regulatory requirements into technical specifications for AI systems presents \n\nsignificant challenges. This document focuses on using user stories to bridge the gap \n\nbetween legal obligations and system development. \n\nUser stories offer a practical approach to understanding and addressing regulatory \n\nrequirements in the context of AI system design. By adopting a user -centric perspective, \n\norganizations can effectively translate legal obligations into actionable steps. \n\nThis document uses a life insurance premium calculation system as an example to illustrate \n\nthe application of user stories in the AI domain. \n\n## Requirements of lawful, fair, and transparent processing \n\nUser story : ensuring lawfulness â€“ correct legal basis \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremium s, we need to conduct a thorough legal basis assessment to determine the most \n\nappropriate legal justification for collecting and using customer data in our AI system. This \n\nis important to comply with the GDPR principle of lawfulness. \n\nUser story : ensuring lawfulness - prohibited data \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure our system complies with the GDPR and AI Act prohibitions \n\non processing certain types of personal data. This includes special categories of personal \n\ndata such as racial or ethnic origin, political opinions, religious beliefs, health, etc. This is \n\nimportant to comply with the GDPR's protection of sensitive personal data and the AI Act's \n\nemphasis on preventing discriminatory outcomes. \n\nUser story : ensuring fairness \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure fair and non -discriminatory processing of customer data. \n\nThis is important to comply with the GDPR principle of fairness and the specific AI Act's \n\nfocus on preventing biased outcomes that could disadvantage certain groups. (Original version â€“ version December 2024) \n\n17 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nThe life insurance company can achieve fairness by: \n\nâ€¢ data source review: analyze the data sources used to train the AI system to identify \n\nand mitigate potential biases based on factors like postal code, gender, age , â€¦ .\n\nEnsure these factors are used in a way that is relevant and necessary for premium \n\ncalculations , avoiding any discriminatory outcomes. \n\nâ€¢ fairness testing: regularly test the AI system for potential biases in its outputs. This \n\nmight involve comparing life insurance premium calculations for similar customer \n\nprofiles to identify any unexplainable disparities. \n\nâ€¢ human oversight: implement a human review process for high -impact decisions \n\nmade by the AI system, such as significant life insurance premium increases or \n\neven policy denials .\n\nUser story : ensuring transparency \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to be transparent about how our customers' data is used. This is \n\nimportant to comply with the general GDPR principle of transparency and the specific AI \n\nActâ€™s focus on transparency for high -risk AI systems. \n\nThe life insurance company can achieve transparency by : \n\nâ€¢ a data protection statement : clearly explain in the company's data protection \n\nstatement how customer data is collected, used, and stored in the AI system for \n\npremium calculations .\n\nâ€¢ easy -to -understand explanations : provide customer -friendly explanations of the \n\nAI premium calculations process. This could involve using simple language, visuals, \n\nor FAQs to demystify the AI's role in determining life insurance premiums. \n\nâ€¢ right to access information : implement mechanisms for customers to easily access \n\ninformation about the data points used in their specific premium calculations. \n\n## Requirements of purpose limitation and data minimization \n\nUser story : ensuring purpose limitation \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure that the data we collect from our customers is limited to what \n\nis strictly necessary for the accurate premium calculations . This is important to comply with \n\nthe principle of purpose limitation under the GDPR. (Original version â€“ version December 2024) \n\n18 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nUser story : ensuring data minimization \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to implement a data minimization strategy to ensure we only collect \n\nand use the minimum amount of customer data necessary for the accurate premium \n\ncalculations . This is important to comply with the principle of data minimization under the \n\nGDPR. \n\n## Requirements of data accuracy and up -to -dateness \n\nUser story : ensuring data accuracy and up -to -dateness \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to implement processes to ensure the accuracy and up -to -dateness of \n\ncustomer data used in the system. This is important to comply with the principle of data \n\naccuracy under the GDPR. \n\nThe life insurance company can achieve accuracy and up -to -dateness of customer data by: \n\nâ€¢ data verification mechanisms: offer customers easy -to -use mechanisms to verify \n\nand update their personal data within the life insurance system. This could be \n\nthrough an online portal, mobile app, or dedicated phone line. \n\nâ€¢ regular data refresh: establish procedures for regularly refreshing customer data \n\nused in the AI system. This might involve requesting customers to update their \n\ninformation periodically or integrating with external data sources to automatically \n\nupdate relevant data points. \n\nâ€¢ data quality alerts: implement alerts for missing or potentially inaccurate data \n\npoints in customer profiles. This allows the company to proactively reach out to \n\ncustomers and request updates. \n\nâ€¢ clearly communicate to customers their right to rectification under the GDPR. This \n\nright allows them to request corrections of any inaccurate personal data or \n\ncompletion of missing data used in the premium calculations system. \n\nUser story : ensuring use of unbiased data \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure that the data used to train and operate the system is of free \n\nfrom bias. This is important to comply with the specific AI Act's focus on preventing biased \n\noutcomes that could disadvantage certain groups. (Original version â€“ version December 2024) \n\n19 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nThe life insurance company can achieve u nbiased data for fair AI premium calculations by : \n\nâ€¢ data source evaluation: Analyze the sources of data used to train the AI system. \n\nIdentify potential biases based on factors like socioeconomic background in the \n\ndata collection process. \n\nâ€¢ regular monitoring and bias testing: Continuously monitor the AI system's \n\nperformance for potential biases in its outputs. Conduct regular bias testing to \n\nidentify and address any discriminatory outcomes in premium calculations .\n\nâ€¢ human oversight: implement a human review process for high -impact decisions \n\nmade by the AI system, such as significant life insurance premium increases or \n\neven policy denials . This allows human intervention to prevent biased out comes. \n\nâ€¢ transparency with customers: Inform customers in the data protection statement \n\nabout the company's commitment to using high -quality, unbiased data in the AI \n\nsystem. \n\n## Requirement of secure processing \n\nUser story : implementing appropriate security measures for life insurance AI \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to conduct a thorough risk assessment to identify potential threats and \n\nvulnerabilities that could impact our customer data . This assessment will consider various \n\nfactors, including the type of data ( health data vs. basic customer information), processing \n\nactivities, and potential impact of a security breach. Based on this assessment, we will \n\nimplement appropriate technical and organizational measures (TOMs) to mitigate these \n\nrisks and ensure the security of our customer data. This is important to comply with the \n\nrequirement of security of the processing under the GDPR .\n\nExamples of TOMs may include: \n\nâ€¢ data encryption: encrypting customer data at rest and in transit to protect \n\nconfidentiality ;\n\nâ€¢ access controls: implementing strict access controls to limit who can access and \n\nmodify customer data ;\n\nâ€¢ regular penetration testing: conducting penetration tests to identify and address \n\nvulnerabilities in the system's security posture ;\n\nâ€¢ logging and auditing: maintaining detailed logs of system activity for monitoring \n\nand investigation of any suspicious behavior .(Original version â€“ version December 2024) \n\n20 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nUser story : implementing specific security measures for life insurance AI \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we recognize that AI systems introduce specific risks beyond traditional data \n\nprocessing. These risks might include potential bias in training data or manipulation by \n\nunauthorized actors. To address these specific risks we will implement additional \n\nmeasures in conjunction with the baseline GDPR -compliant TOMs. This is important to \n\ncomply with the requirement of security of the processing under the AI Act .\n\nExamples of these additional measures may include: \n\nâ€¢ data validation and quality assurance: implementing processes to ensure the \n\nquality and integrity of the data used to train and operate the AI system. This could \n\ninvolve data provenance tracking and anomaly detection to identify potential \n\nbiases or manipulation attempts. \n\nâ€¢ human oversight: establishing a framework for human oversight throughout the AI \n\nsystem's lifecycle. This could involve human review of high -risk data points, \n\nmonitoring the system's performance for fairness and accuracy, and intervening in \n\ncritical decision -making pathways. \n\n## Requirement of (the ability of demonstrating) accountability \n\nUser story : documenting the legal basis \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to have a clear and concise record of the legal basis for collecting and \n\nusing customer data in the AI system. This is important to comply with the GDPR principle \n\nof (demonstrating) accountability (also in the context of audits or investigations). \n\nUser story : conducting a Fundamental Rights Impact Assessment (FRIA) \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to develop and maintain a comprehensive FRIA (Fundamental Rights \n\nImpact Assessment) to proactively identify and mitigate potential risks associated with \n\nthis AI system. This is important to comply with the AI Act's requirements for high -risk AI \n\nsystems and promote fair and non -discriminatory premium calculations for our customers. \n\nThis obligation is in addition to the GDPR rules on data protection impact assessment. \n\n* * *(Original version â€“ version December 2024) \n\n21 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# References  \n\n> 0\n\nThis paper also utilized spelling and grammar checking, and a large language model, as a \n\ntool for refining and correcting initial text section s.  \n\n> i\n\nRegulation (EU)2016/679 of the European Parliament and of the Council of 27 April 2016 \n\non the protection of natural persons with regard to the processing of personal data and on \n\nthe free movement of such data, and repealing Directive 95/46/EC (General Data \n\nProtection Regulation),  Official Journal of the European Union L 119/1, 4.5.2016, p. 1 â€“88. \n\nii  Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June \n\n2024 laying down harmonised rules on artificial intelligence  (Artificial Intelligence Act), \n\nOfficial Journal of the European Union L 199/1, 12.7.2024, p. 1 â€“120. \n\niii  Art. 20, Â§1, 1Â°, Data Protection Authority Act of 3 December 2017, amended by the Act of \n\n25 December 2023. \n\niv In the examples discussing high -risk AI systems listed in Annex III of the AI act, the \n\npossible exceptions referred to in section 6.3 of the AI act are not taken into account. \n\nv In the examples addressing life insurance, possible gaps in the legal basis (see decision \n\n109/2024 of the Litigation Chamber of the Belgian DPA) are not taken into account. \n\nvi  Artificial Intelligence Act, Article 3 (1) \n\nvii For more information on the legal basis â€˜legitimate interestâ€™, see the following opinion of \n\nthe European Data Protection Board: Opinion 28/2024 on certain data protection aspects \n\nrelated to the processing  of personal data in the context of AI models \n\n(https://www.edpb.europa.eu/news/news/2024/edpb -opinion -ai -models -gdpr -principles -\n\nsupport -responsible -ai_en )", "fetched_at_utc": "2026-02-08T18:50:15Z", "sha256": "8072688f58c2242ab3989cdf7d78db7dfec1eb82dd1953c8ab68f0227e59f3d3", "meta": {"file_name": "Artificial Intelligence Systems and the GDPR - Belgium.pdf", "file_size": 489694, "relative_path": "pdfs\\Artificial Intelligence Systems and the GDPR - Belgium.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 8308}}}
{"doc_id": "pdf-pdfs-bsi-eu-ai-act-whitepaper-final-2-9-24-aa10c02636c1", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\BSI_EU_AI_Act_Whitepaper_Final_2_9_24.pdf", "title": "BSI_EU_AI_Act_Whitepaper_Final_2_9_24", "text": "Artificial Intelligence Act What AI providers and  deployers need to know. @2024 BSI. All rights reserved. \n\nDisclaimer \n\nThe AI Act text used for the analysis is the text voted in the Plenary of the European Parliament on the 13th of Mar 2024  (P9_ TA(2024)0138 Artificial Intelligence Act). This paper is BSIâ€™s interpretation of the AI Act and is currently not legally binding. Artificial Intelligence Act (AI Act) What AI providers and deployers need to know. Authors: Alex Zaretsky *,Daniela Seneca *, Inma PÃ©rez *,Sarah Mathew *, Aris Tzavaras ** .* Regulatory Lead, Artificial Intelligence Notified Body, BSI group. ** Head of Artificial Intelligence Notified Body, BSI group. @2024 BSI. All rights reserved. We did not wake up to a world where Artificial Intelligence (AI) was just born. The genesis of AI as an idea is evident from ancient times in the form of myths. 1However, the term â€œAIâ€ more recently has been attributed to John McCarthy of the Massachusetts Institute of Technology (MIT) who first suggested the concept at a 1956 conference at Dartmouth College. The technological evolution permitted AI to become accessible , both in terms of computation power as well as in terms of the tooling and availability of digital data for facilitating development of AI systems. \n\nSince 1956, AI has shown significant progress \n\nin performing â€œnarrowâ€ tasks, in most cases, better than the average human and, in some, better than experts . A landmark victory of AIâ€™s progress became clear when the Deep Blue expert system played chess against the world champion Garry Kasparov in the 90s. 2  \n\n> 1Stanford researcher examines earliest concepts of artificial intelligence, robots in ancient myths 220 Years after Deep Blue: How AI Has Advanced Since Conquering Chess\n\nNow, why is there an increasing global concern to regulate and/or control AI? The short answer to this question is that we may lose to an opponent we created. Society cannot afford to leave AI \n\nunregulated as this could lead to the misuse of this technology. AI also needs vast amounts of data to become increasingly intelligent, and there is the risk that fundamental rights would be violated. For example, an algorithm that processes profiles to evaluate candidates for a job position, may be biased against people of a certain ethnicity, limiting exposure of their profiles for opportunities. These algorithmic biases have serious real-world implications. In this context, to prevent any form of manipulation or biased outcome, several regions are leaning towards AI regulation. The need for Artificial Intelligence legislation @2024 BSI. All rights reserved. \n\nLetâ€™s not forget about the geopolitics linked to regulating AI. The AI industry is growing at an extremely rapid pace and AI has become of strategic importance for governments across the world. Countries are competing to win the â€œAI raceâ€ and those able to successfully lead on AI innovation will be well positioned in global affairs. \n\nRegulating a technology sector is not something new. Typical examples are regulations bestowed on the pharmaceutical and medical industries, as well as on more abstract technologies like those processing our digital data. 3 Justifications behind market \n\nregulation includes market acceleration and harmonization as well as protecting consumers \n\nfrom the negative effects of such technologies. In the context of AI systems, harms may arise from the deployment of AI and its after effects. Therefore, such justifications can be used to limit AI. 4\n\nReasons to regulate AI may differ across regions. \n\nThe European Unionâ€™s (EU) approach, for instance, has been characterized by its focus on the protection of human rights â€“ or as it is called in Europe, fundamental rights. Those rights are enshrined in the Charter of Fundamental Rights of the EU and in other binding legislation such as the General Data Protection Regulation (GDPR). As technology is becoming an ever more central part of citizensâ€™ life, the EU understands that trust, is a prerequisite for AI uptake in Europe . We use, as consumers, products and services coming from â€œunfamiliar sourcesâ€ and we need to have the assurance that those products are safe, trustworthy, and ethical for us to consume. Regulations set the 3 â€˜Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation)â€™, OJ L 119 (2016). 4 Chesterman, Simon, From Ethics to Law: Why, When, and How to Regulate AI (April 29, 2023). Forthcoming in The Handbook of the Ethics of AI edited by David J. Gunkel (Edward Elgar Publishing Ltd.), NUS Law Working Paper No. 2023/014 5 Smuha, Nathalie A., Beyond a Human Rights-based approach to AI Governance: Promise, Pitfalls, Plea (February 1, 2020). Published in Philosophy & Technology, 2020 6 Idem from footnote 4, p.5 7 High-Level Expert Group on AI, â€˜Ethics Guidelines for Trustworthy AIâ€™, 8 April 2019 8 European Parliament, â€˜EU AI Act: first regulation on artificial intelligenceâ€™ basic â€œrules of the gameâ€, ensuring consumers that unfamiliar sources deliver a product or service that obeys those rules. In most cases, legislation goes beyond the first entry of a product to the market, they additionally dictate the need for monitoring the product while in use and to deliver feedback to relevant authorities and action when something goes wrong; this is known as Post Market Surveillance (PMS). The use of AI comes with more sophisticated and nuanced challenges: some philosophical, some practical. Due to the increasing concerns about the adverse impact that AI systems may have on individuals, EU lawmakers have the \n\nchallenge of ensuring that AI is used for good. But what is â€˜â€™good AIâ€™â€™? This quest is relatively new, however, defining what is â€œgoodâ€ has been a long-standing question with different answers depending on the moral theory that you consider. 5 For this reason, since human rights are universally recognized, the EU decided to take a human-centric based approach to AI governance 6 and, in April 2019, the European Commission published its conceptualization of â€˜â€™good AIâ€™â€™: the Ethics Guidelines for Trustworthy AI. 7 The non-binding nature of these guidelines were criticized, however, in 2021, the EU Commission published the proposal for an Artificial Intelligence Act \n\n(AI Act) that largely codifies the ethics \n\nrequirements proposed by the High-Level Expert Group on AI in its Guidelines. 8 Since then, the final text has gained political agreement and has been voted by the EU Parliament in March 2024. @2024 BSI. All rights reserved. The AI Act is a â€œhorizontalâ€ legislation as it does not target a specific industry sector but rather any industry that uses AI. The AI Act sets requirements that products must comply with, as well as obligations for all parties involved (economic operators). The horizontal nature of this legislation is envisioned to â€œbuild onâ€ sectorial legislations, regulating only the AI aspects of those products. Furthermore, because the AI Act is technology agnostic, it does not prescribe specific rules for specific types of AI techniques, with the exemption of General-Purpose AI systems (GPAI). To determine if the upcoming legislation is applicable to oneâ€™s product, one must first define whether their product is or uses AI. Unfortunately, the definition of AI has been the â€œholy grailâ€ of the last decades, as there is no globally acceptable definition of â€œintelligence.â€ The above-mentioned â€œAI raceâ€ has forced governments as well as supranational and intergovernmental organizations to attempt to find a common definition of AI. 9 The OECD defines AI systems as follows: â€œAn AI system is a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy.â€ See OECD, â€˜Recommendation of the Council on Artificial Intelligenceâ€™ 10  See Recital 12 of AI Act For instance in the AI Act, the EU ultimately suggested a definition aligned to the Organization for Economic Co-operation and Development (OECD)â€™s AI definition, 9 ensuring the text \n\nâ€œdistinguish[es] it from simpler traditional software systems or programming approaches and should not \n\ncover systems that are based on the rules defined solely \n\nby natural persons to automatically execute operations.â€ 10 The AI Act defines an AI system in Article 3 as: â€œa machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, \n\nrecommendations, or decisions that can influence \n\nphysical or virtual environments. â€@2024 BSI. All rights reserved. AI Act scope and territorial implications \n\nThere are two key actors caught under the scope of the AI Act which are providers 11 and deployers 12 of AI systems. Moreover, certain obligations have been introduced for importers, distributors, product manufacturers and authorized representatives of providers. The AI Act states 13 that the regulation applies to providers and deployers of AI systems established in the EU, as well as externally, to any provider and deployer of AI systems outside the EU, if the output of the AI system is used within the EU. What â€œoutputâ€ means is not defined under the AI Act, however, the definition of AI system refers to outputs in the form of content (generative AI systems) including text, video, or image 14 and \n\npredictions, recommendations, or decisions that can \n\ninfluence physical or virtual environments. 15 The key thing to ask here is whether the impact of the AI system occurs within the EU, regardless of where the provider and deployer is established. \n\n11  Article 3(3) of P9_TA(2024)0138 AI Act defines providers as â€œa natural or legal person, public authority, agency or other body that develops an AI system or a general purpose AI model or that has an AI system or a general purpose AI model developed and places them on the market or puts the system into service under its own name or trademark, whether for payment or free of chargeâ€ \n\n12  Article 3(4) of AI Act defines deployers as â€œany natural or legal person, public authority, agency or other body using an AI system under its authority except where the AI system is used in the course of a personal non-professional activityâ€ 13  See Article 2 of AI Act 14  Throughout the text we see that the AIA includes image, audio or video as example of AI-generated content. 15  See Article 3 and Recital 12 of AI Act 16  Bradford, Anu. (2020). The Brussels Effect: How the European Union Rules the World. Similar to the GDPR, one of the most important consequences of the AI Act extraterritorial scope is that it will impose significant obligations on non-EU businesses, even if they do not have a legal presence in the EU. The rationale behind this approach is linked to the EUâ€™s growing concern on how authoritarian governments use AI and its potential impact on the rights and freedoms of individuals. Consequently, the AI Act aims to level the playing field and make the AI Act applicable in a non-discriminatory manner. \n\nFurthermore, it is important to mention that the AI Act will potentially become another example of the phenomenon called the \n\nâ€œBrussels Effect,â€ a concept originally coined by Anu Bradford, 16 a professor at Columbia University. The â€œBrussels effectâ€ refers to â€œthe EUâ€™s unilateral power to regulate global markets.â€ @2024 BSI. All rights reserved. Some may argue that the upcoming AI Act Regulation will hamper AI innovation in Europe as it is more stringent in comparison to other more flexible countries that encourage self-regulation and voluntary commitments. However, for economic operators, it is more beneficial to adopt a uniform global standard rather than adhering to multiple, including laxer, regulatory standards, as this brings legal certainty. This would be the case for those organizations operating globally, who have multiple production locations where it is not legally, technically, or even economically viable, for the company to comply with multiple regulatory regimes. 17 This business approach would explain why so many large non-EU companies follow the GDPR and many other EU environmental regulations across their global operations. \n\nLetâ€™s not also forget that the EU was the first \n\nmover when it comes to regulating AI. It is true that due to the slow pace of the EU legislative process, there have been delays in the AI Act negotiations and other jurisdictions have adopted comprehensive regulation for parts of the AI ecosystem before the EU. 18 For example, China has \n\nbeen one of the first countries to implement AI \n\nregulations, including new rules on the use of recommendation algorithms. 19 Despite this, the AI Act has been the first draft to be published and this may be the reason why other jurisdictions have sped up their legislative processes. 17  Bradford, Anu, The Brussels Effect (2012). Northwestern University Law Review, Vol. 107, No. 1, 2012, Columbia Law and Economics Working Paper No. 533 18  Siegmann, Charlotte, Anderljung, Markus, The Brussels effect and Artificial Intelligence: How EU regulation will impact the global AI market (2022). Centre for the Governance of AI. P.39 19  See translation of the text here Translation: Internet Information Service Algorithmic Recommendation Management Provisions â€“ Effective March 1, 2022 (stanford.edu)   \n\n> 20 Siegmann, Charlotte, Anderljung, Markus, The Brussels effect and Artificial Intelligence: How EU regulation will impact the global AI market (2022). Centre for the Governance of AI. P.3 21 See interview to Bradford What is the Brussels Effect, and what does it mean for global regulation? (microsoft.com)\n\nTaking the success of the GDPR adoption as an example, the EU plans to promote its blueprint on AI globally. 20 It is fair to assume that the AI Actâ€™s human-centered approach, strong focus on ethics, transparency and fundamental rights, will serve as an inspiration to like-minded countries. \n\nEspecially after seeing over 100 countries today with GDPR-like data privacy rules. 21 @2024 BSI. All rights reserved. To sum up, the AI Act will have an extra-territorial impact on AI providers and deployers in non-EU jurisdictions, if their AI systems and/or outputs are used within the EU. Moreover, its existing extraterritorial scope has the potential to become the gold standard when it comes to AI governance. Experience has proved that European values have a broad appeal, and it is fair to assume that the AI Act will be globally widespread. Companies operating globally may be glad to follow only one set of rules, even if they are more stringent. However, the extraterritorial scope of the AI Act might raise compliance challenges, especially to those AI providers established in third countries if they are not aware that the output of their AI system will be used in the EU. Therefore, all relevant operators need to understand which role they play along the AI value chain and properly determine the scope of their AI systems to see if they fall within the scope of the AI Act. @2024 BSI. All rights reserved. As we have seen in this whitepaper, the AI Actâ€™s \n\ndefinition of AI is close to the one proposed by \n\nthe OECD and this seems to be an advantage as it maintains a semantic alignment with international partners. The EU believes that this definition gives a clear criterion for differentiating AI systems from traditional software, thus ensuring a proportionate regulatory approach. However, this definition has \n\ndrawn much criticism for still remaining too broad . In any case, it is important to understand that not all AI technologies defined as an AI system under the AI Act will be subject to obligations, however one must consider the degree of risk they pose to the health, safety, and fundamental rights of individuals.   \n\n> 22 European Commission, White Paper on Artificial Intelligence - A European approach to excellence and trust, COM(2020) 65 final, 2020. P. 17 23 See Article 5 of AI Act\n\nThe EU believes that a risk-based approach is impor-tant to help ensure that the regulatory intervention is proportionate. 22 To that end, the AI Act distin-guishes between AI systems posing (i) unacceptable risk, (ii) high risk, (iii) limited risk, and (iv) low or minimal risk. \n\nThe Commission judges the level of risk by the likelihood that the system may harm the health \n\nand safety of specific individuals, and/or \n\npotentially violate their fundamental rights. The obligations imposed on such systems range from prohibitions to the voluntary codes of conduct. The AI Act proposes prohibitions on AI applications that pose â€œunacceptable risksâ€ to peopleâ€™s safety, health and rights. 23 \n\n# AI products falling under the AI Act @2024 BSI. All rights reserved. The AI Act considers these practices to be harmful and abusive and should be prohibited because they contradict Union values. 24 Accordingly, these systems would be prohibited to be placed on the market, put into service, or used in the EU: 1 AI systems that deploy harmful manipulative â€œsubliminal techniques.â€ 25 \n\nâ€¢ Example: â€œAn inaudible sound is played in truck driversâ€™ cabins to push them to drive longer than \n\nhealthy and safe. AI is used to find the frequency maximising this effect on drivers.â€ 26 2 AI systems that exploit specific vulnerable \n\ngroups (due to their age, physical or mental disabilities). 27 \n\nâ€¢ Example: â€œA doll with an integrated voice assistant encourages a minor to engage in progressively dangerous behaviour or challenges in the guise of a fun or cool game.â€ 28 3 AI systems used for social scoring purposes for public and private purposes- in particular, to classify the reliability of people based on their social behaviour or personality traits. 29 \n\nâ€¢ Example: â€œAn AI system identifies at-risk children in need of social care based on insignificant or irrele -vant social â€˜misbehaviourâ€™ of parents, e.g. missing a doctorâ€™s appointment or divorce.â€ 30 4 Biometric categorization of natural persons based on biometric data to deduce or infer their race, political opinions, trade union, membership, religious or philosophical beliefs, sex life or sexual orientation. 31 \n\nâ€¢ Example: â€œAI systems that infer â€˜criminalityâ€™ based on data about peopleâ€™s facial structure or biological characteristics, for example, the colour of the skin.â€            \n\n> 24 See recital 28 of AI Act 25 See Article 5(1)(a) of AI Act 26 For the sake of clarity, the Commission has presented some examples of the above prohibitions. Some argue that these are borderline fantastical, however, being AI such an innovative technology, who knows where it will take us. See https://cor.europa.eu/en/events/Documents/SEDEC/FINAL%20PDF%20AI%20Presentatiofor%20COR%20Sedec%20Committee%20 meeting%2023%2006%2021.pdf 27 See Article 5(1)(b) of AI Act 28 See https://cor.europa.eu/en/events/Documents/SEDEC/FINAL%20PDF%20AI%20Presentatiofor%20COR%20Sedec%20Committee%20meeting%2023%2006%2021.pdf 29 See Article 5(1)(c) of AI Act 30 See https://cor.europa.eu/en/events/Documents/SEDEC/FINAL%20PDF%20AI%20 Presentatiofor%20COR%20Sedec%20Committee%20meeting%2023%2006%2021.pdf 31 See Article 5(1)(g) of AI Act 32 See Article 5(1)(h) of AI Act 33 See Article 5(1)(d) of AI Act 34 See Recital 42 of AI Act\n\n5 Real-time remote biometric identification \n\nin publicly accessible spaces by law enforce-ment. 32 \n\nâ€¢ Example: â€œAll faces captured live in a public space by video cameras checked, in real time, against a database to identify a criminal in the crowd.â€ \n\n6 Individual predictive policing; except for law enforcement if based on objective and \n\nverifiable facts. 33 \n\nâ€¢ Example: â€œAI-predicted behaviour based solely on \n\ntheir profiling, personality traits or characteristics, \n\nsuch as nationality, place of birth, place of residence, number of children, debt, their type of car, without a reasonable suspicion of that person being involved \n\nin a criminal activity based on objective verifiable \n\nfacts and without human assessment thereof.â€ 34 @2024 BSI. All rights reserved. 7 Emotion recognition in the workplace and education institutions, unless for medical or safety reasons (i.e. AI systems used in detecting the state of fatigue of professional pilots or drivers for the purpose of preventing accidents). 35 \n\nâ€¢ Example : â€AI recruitment tools that assess a candidateâ€™s emotional state or truthfulness through analysis of facial expressions, voice modulation, or body language during interviews.â€ \n\n8 AI systems using indiscriminate scraping of biometric data from the internet or CCTV footage to create facial recognition data-bases. 36 \n\nâ€¢ Example: â€œAI system that collects facial images \n\nfrom social media without any specific targeting or \n\nconsent, amassing a vast database of faces.â€ \n\nHowever, it is important to mention that the use of real-time remote biometric identification in point 5 above has some exceptions related to the safety of society as a whole. In particular, the use of real-time remote biometric identification systems (such as facial recognition) in public spaces for law enforce-ment purposes will be allowed when the use of such systems can be justified by â€œthree exhaustively listed and narrowly defined situations.â€ These narrowly 35  See Article 5(1)(f) of AI Act 36  See Article 5(1)(e) of AI Act 37  See Article 5(1)(h)(i) of AI Act 38  See Article 5(1)(h)(ii) of AI Act 39  The list of the 16 crimes in Annex II of AI Act: Terrorism; Trafficking in human beings; Sexual exploitation of children and child sexual abuse material; Illicit trafficking in narcotic drugs and psychotropic substances; Illicit trafficking in weapons, munitions and explosives; Murder; Grievous bodily injury; Illicit trade in human organs and tissue; Illicit trafficking in nuclear or radioactive materials; Kidnapping, illegal restraint and hostage-taking; Crimes within the jurisdiction of the International Criminal Court; Unlawful seizure of aircraft/ships; Rape; Environmental crime; Organised or armed robbery; Sabotage, participation in a criminal organisation involved in one or more crimes listed above.   \n\n> 40 See Article 5(1)(h)(iii) of AI Act 41 See Article 26 (10) of AI Act\n\ndefined exceptions cover a rather broad range of situations: â€¢ a â€œtargeted search for specific victims of abduction, trafficking in human beings or sexual exploitation \n\nof human beings, as well as searching for missing persons.â€ 37 â€¢ the â€œprevention of a specific, substantial and \n\nimminent threat to the life or physical safety of natural persons or (...) of a terrorist attack.â€ 38 â€¢ the â€œlocalisation or identificationâ€ of a person suspected of having committed a crime 39 with a maximum sentence of at least 4 years that would allow for the issuing of a European Arrest Warrant. 40 \n\nTherefore, if the above is fulfilled, real-time biome-tric identification by law enforcement authorities may be permitted, if it is accompanied by safeguards for fundamental rights, including the ex-ante involvement of judicial authorities and prior funda-mental rights impact assessment (unless in duly justified situations of urgency). \n\nThese safeguards will also be mandatory for the usage of AI systems for post remote biometric \n\nidentification of persons under investigation. 41 An example of this would be the use of biometric surveillance to analyse footage during a protest to identify an individual that has committed a crime. \n\nIt is important to state that the above prohibition does not ban actors from using remote biometric \n\nidentification for non-law enforcement purposes .This means that private entities may use such systems (e.g. marketplaces, public transport and even schools) if they go through a third-party conformity assessment or comply with harmonized European standards that are to be published later on. @2024 BSI. All rights reserved. Moving on to the next category under the AI Act, â€œhigh-risk AI systemsâ€ are those systems that create adverse impact on peopleâ€™s health and safety or their fundamental rights in a number of defined applications, products and sectors. This is the main focus of the regulation. \n\nBefore going into more detail, it is important to clarify that AI systems can be used on a stand-alone basis or as a component of a product ,irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). 42 \n\nThe high-risk regime is based on the intended purpose of the AI system, in line with the New Legislative Framework (NLF), a common EU approach to the regulation of certain products such as machinery, lifts, medical devices, personal protec-tive equipment and toys. 43 The AI Act distinguishes between two categories of high-risk AI systems: 1 AI systems that are products or safety compo-nents of products covered by certain Union health and safety harmonization legislation (such as toys, machinery, lifts, or medical devices) and are required to undergo a third party conformity assessment. 44 \n\n2 Stand-alone AI systems deployed in eight \n\nspecific areas: 45 \n\na.  biometric identification, categorization and emotion recognition (outside prohibited categories); \n\nExample: AI systems used for facial recognition. \n\nb.  management and operation of critical infra-structure; \n\nExample: AI systems used in road traffic, the \n\nsupply of water, gas, heating, and electricity.          \n\n> 42 See Recital 12 of AI Act 43 See New legislative framework - European Commission (europa.eu)\n> 44 See Annex I of AI Act with the list of NLF legislation. Annex I (B) is older-style product safety legislation where Title XII introduces new AI Actâ€“related considerations for future delegated acts in those areas. 45 See the list in AI Act 46 See recital 33 of AI Act 47 See Articles 7 and AI Act 48 See Article 6 (3) of AI Act 49 See Article 6 (3) of AI Act 50 See Article 6 (3) of AI Act\n\nc.  educational and vocational training; \n\nExample: AI systems used in evaluating students on \n\ntests required for university admission. \n\nd.  employment, worker management and access to self-employment; \n\nExample: AI systems used to place targeted \n\njob advertisements, to analyse and filter job \n\napplications, and to evaluate candidates. \n\ne.  access to and enjoyment of essential services and benefits; \n\nExample: AI systems used for creditworthiness evaluation of natural persons. \n\nf.  law enforcement; \n\nExample: AI systems used for detection, \n\ninvestigation, or prosecution of criminal offenses. \n\ng.  migration, asylum and border management; \n\nExample: AI systems that identify a person who, \n\nduring an identity check, either refuses to be identified \n\nor is unable to state or prove his or her identity .46 \n\nh.  administration of justice and democracy; \n\nExample: AI systems aimed at helping analyse and interpret facts regarding judicial authority. \n\nThe above list of high-risk AI systems may be updated over time as the EU Commission may modify or add additional use cases if they pose similar risks to the uses currently on the list. However, it can remove areas if they do no longer pose a significant risk to health, safety and funda-mental rights. 47 \n\nA stand-alone AI system can be classified as high-risk if it falls under any of the eight specific \n\nareas. However, as always, the AI Act has excep-tions. If the system does not pose significant harm to the health, safety, or fundamental rights of people, including not materially influencing the outcome of decision-making, it may be exempt. This would be the case of the following systems: 48 â€¢ Intended to perform a narrow procedural task. 49 â€¢ Intended to improve the result of a previously completed human activity. 50 @2024 BSI. All rights reserved. â€¢ Intended to detect decision-making patterns or deviations from prior decision-making patterns. 51 â€¢ Intended to perform a task that is only prepara-tory to an assessment relevant for the purpose of the high-risk use cases in Annex III. 52 It is important to note that an AI system will always be considered at a minimum high-risk if the AI system performs profiling of natural persons. 53 The next type of AI system risk level is â€œlimited riskâ€. \n\nThese are systems that have special disclosure obligations due to their particular interaction with humans given that they may pose the risk of manipulation . These include AI systems that generate or manipulate image, audio or video content (i.e. deep fakes 54 ), AI systems that are intended to interact with people (e.g. chatbots), and AI-powered emotion recognition systems and biometric categorization systems. With this inclusion, the AI Act ensures that EU customers make informed decisions as they are aware that they are interacting with a machine. 55 Finally, the last category is â€œminimal riskâ€. These AI \n\nsystems do not fit in any of the other categories \n\nand present only low or minimal risk. They can be developed and used within the EU without conforming to any additional legal requirements. However, the AI Act envisages the creation of codes of conduct to encourage providers of non high-risk AI systems to voluntarily apply the mandatory requirements for high-risk AI systems. These codes of conduct should be based on clear objectives and key performance indicators to measure the achievement of those objectives. 56 This could include elements around inclusiveness, fairness, transparency, confidentiality, and environmental sustainability. 51  See Article 6 (3) of AI Act 52  See Article 6 (3) of AI Act 53  See Article 6 (3) of AI Act 54  See Article 3(6) of of AI Act 55  See Article 50 of AI Act 56  See Recital 165 of AI Act 57  See Article 95 of AI Act The Commission and Member States will encourage the creation and voluntary compliance with these codes. 57 It is important to underline that existing EU law, such as the GDPR, still applies when the use of AI systems falls within the scope of that law, no matter if classi-fied as no risk under the AI Act. @2024 BSI. All rights reserved. AI systems falling under other EU legislation. Given the horizontal nature of the AI Act, all AI systems across sectors are subject to the same risk-assessment criteria and legal requirements .The AI Act interrelates with other EU legal instru-ments, for example rules on data protection, privacy, civil liability or sectorial law such as the Machinery Regulation or the Medical Devices Regulation (MDR). This horizontal approach prevents companies from â€œshopping aroundâ€ between sectors and thus ensuring that all players conform to the same legal requirements. This approach might seem preferable as it is uniform, stable, and fair across industries. However, it is important to say that, if this approach is not properly addressed, it may lead to conflicting obligations and procedures for AI providers. \n\nThe AI Act draws on the New Legal Framework (NLF) regime, 58 designed to improve the EU internal market , and increase the quality of conformity assessment of certain products such as medical devices, machinery, or toys. As described by Veale and Zuiderveen Borgesius, 59 â€œunder NLF \n\n58  New legislative framework, European Commission 59  Veale, Michael and Zuiderveen Borgesius, Frederik, Demystifying the Draft EU Artificial Intelligence Act (July 31, 2021). Computer Law Review International (2021) 22(4) 97-112, P.6 60  Union harmonization legislation refers to Union legislation that harmonizes the conditions for the marketing of product. This list can be found here and also in Annex II of AI Act 61  See Recital 64 of P9_TA(2024)0138 Artificial Intelligence Act 62  Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (MDR). \n\nregimes, a manufacturer must undertake pre-mar-keting controls undertaken to establish productsâ€™ safety and performance, through conformity assessment to \n\ncertain essential requirements laid out in law. Manufac -turers then mark conforming products with â€œCEâ€; marked products enjoy EU freedom of movement.â€ \n\nThe AI Act acknowledges that a single AI system may be affected by different Union Harmonization legislation. 60 61 For example, a medical device product incorporating AI might present risks not addressed by the Medical Devices Regulation (MDR). 62 This calls for a simultaneous and complementary application of several EU laws. @2024 BSI. All rights reserved. The NLF legal acts are built on the legal concept that \n\nwhenever a matter is regulated by two rules, the \n\nmore specific one should be applied first. 63 With this, it is ensured that products incorporating AI are not subject to a double regulatory burden. This was the intention of the Commission when it proposed the AI Act: \n\nâ€œTo achieve those objectives, this proposal presents a balanced and proportionate horizontal regulatory approach to AI that is limited to the minimum necessary \n\nrequirements to address the risks and problems linked \n\nto AI, without unduly constraining or hindering technological development or otherwise disproportion-ately increasing the cost of placing AI solutions on the market.â€ 64 \n\nIf we take AI-enabled medical devices as an example, the AI Act tries to ensure consistency , avoid duplications, and minimize additional burdens associated with the cumulative application of the AI Act and MDR. It allows AI providers to integrate the necessary measures to comply with the AI Act into the procedures and documents already required under MDR. 65 In practice, this means that the AI-enabled medical device manufacturer would be allowed to integrate the testing and reporting processes, information and documentation required under the AI Act into the already existing documentation and procedures required under the MDR. This is because the MDR is considered the more specific rule and, therefore, will take precedence. However, as previously mentioned, there are certain tensions and inconsistencies that make it difficult to apply the AI Act in conjunction with other Union harmonization laws. For example, as seen in the previous section, the AI Act categorizes as high-risk AI systems those that are products or safety components of products already covered by Union harmonization legislation. For AI-enabled medical devices both, the AI Act and MDR, would be applicable to the same product. The problem here is that there is no 63  Commission notice The â€˜Blue Guideâ€™ on the implementation of EU product rules 2022 (Text with EEA relevance) 2022/C 247/01 C/2022/3637, p. 11 64  Explanatory Memorandum AI Act, p. 3 65  See Recital 64 of AI Act 66  Article 3(14) of AI Act defines safety component, however, we do not see a similar definition under the MDR. 67  Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (GDPR) definition of â€œsafety componentâ€ under the MDR and it is not clear for a medical device what a â€˜safety componentâ€™ is. 66 Additionally, the AI Act interplays not only with Union Harmonization law, but also with other hori-zontal legislation, such as the GDPR. 67 The AI Act makes several references to the GDPR throughout the text and assures that it is without prejudice and complements the GDPR. Accordingly, both regulations apply side by side. In practice, this means that all AI systems must strictly adhere to the GDPR if they use personal data belonging to EU citizens, or plan to be deployed for usage within the EU. However, again there are some tensions when it comes to an AI system processing personal data. @2024 BSI. All rights reserved. For example, the AI Act states that it does not provide legal grounds for processing personal data \n\nand refers back to the GDPR to find a justifiable \n\nground for this processing. 68 However, it does not give clarity on how to apply the GDPRâ€™s requirements for collecting and processing personal data. In this context, it is difficult to find a legal ground for the processing of personal data by Large Language Models (LLMs) as there is no controller/data subject relationship (e.g. contract), nor can the data subject expect their data to be used as training data for an app, and there is no possibility for the data subject to object to such processing (e.g. no explicit consent). In some cases, AI can handle personal data based on the justified grounds of â€œlegitimate interest.â€ 69 Never-theless, this needs to be balanced to ensure that the data subjectâ€™s rights are not compromised. It is expected that the Commission will issue guidelines clarifying how to train AI models \n\nwithout violating personal data protection rules, 68  See Recital 63 of AI Act 69  See Article 6(1)(f) Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (GDPR) 70  Regulation (EU) 2019/881 on ENISA and on information and communications technology cybersecurity certification and repealing Regulation (EU) No 526/2013 (Cybersecurity Act). 71  See Art 42(2) of AI Act 72  Established in 2004 and strengthened by the EU Cybersecurity Act, the European Union Agency for Cybersecurity, ENISA contributes to EU cyber policy, enhances the trustworthiness of ICT products, services and processes with cybersecurity certification schemes. 73  ENISA: AI Cybersecurity Challenges - Threat Landscape for Artificial Intelligence. Artificial Intelligence Cybersecurity Challenges â€” ENISA (europa.eu) 74  See Article 40 of AI Act 75  See Article 41 of AI Act 76  Regulation 2022/0272 (CRA). 77  See Article 8 of Regulation 2022/0272 (CRA). 78  Essential cybersecurity requirements are in Article 10 and Annex I of Regulation 2022/0272 (CRA). including the Data Act, Data Governance Act and the Copyright Directive. Regarding the GDPR, legal grounds for the processing of personal data might require a significant rethink for AI systems. \n\nOn the cybersecurity side, the AI Act also overlaps with the EU Cybersecurity Act 70 , including a presumption of conformity in Article 42(2) of the AI Act. The clause acknowledges that high-risk AI systems that have been certified under a cybersecu-rity scheme created according to the process provided by the Cybersecurity Act â€œshall be presumed to be in \n\ncompliance with the cybersecurity requirements set out in Article 15 of this Regulation.â€ 71 Article 15 and recital 49 of the AI Act state that high-risk AI systems should perform consistently throughout their lifecycle and meet an appropriate level of accuracy, robustness and cybersecurity in accordance with the generally acknowledged state of the art. It is fair to say that Article 15 of the AI Act is very general and does not cover the entirety of potential cyberthreats to AI-powered systems such as those identified by ENISA 72 in its â€œArtificial intelligence and \n\nCybersecurity Challengesâ€ report. 73 Therefore, if AI developers want to ensure the highest level of cybersecurity for their AI system, they will need to either rely on the available cybersecurity schemes or, most likely, apply the harmonized standards 74 or common specifications defined by the Commission, 75 which are not available at this time. On another note, but still within the EU cybersecurity framework, the AI Act will also interplay with the new Cyber Resilience Act (CRA). 76 Like the AI Act, this regulation is expected to enter into force in 2024. In the CRA, 77 we find a presumption of conformity with the AI Act. It states that products with digital elements classified as high-risk AI systems under the AI Act should comply with the essential cybersecurity requirements set out under the CRA. 78 @2024 BSI. All rights reserved. If those high-risk AI systems fulfil the CRAâ€™s cybersecurity essential requirements, they should be deemed compliant with the cybersecurity requirements set out in Article 15 of the AI Act, as long as those requirements are covered by the EU declaration of conformity issued under the CRA. 79 Moreover, the CRA clearly states that the AI Act is the reference act and that the AI Actâ€™s conformity assessment procedures are the ones to be followed . In addition, the CRA clarifies that AI Notified Bodies under the AI Act can also control the conformity of high-risk AI systems with the CRA essential requirements. 80 However, there is an exception to this: if a high-risk AI system also falls under the CRAâ€™s scope as a â€˜critical product with digital elementsâ€™ and to which internal control of the AI Act applies, then the conformity assessment to follow is the one under the CRA insofar as the essential cybersecurity requirements are concerned. The other aspects of the product can still follow the AI Actâ€™s internal control procedure. The reason behind this is that â€˜critical products with digital elementsâ€™ create greater cybersecurity risks and, 79  See Recital 77 of AI Act 80  See Article 8(2) of Regulation 2022/0272 (CRA). therefore, the conformity assessment should always involve a third-party conformity assessment body. \n\nFinally, something important to mention is that high-risk AI providers also need to comply with accessibility requirements , including the EU directives 2016/2102 and 2019/882. The AI Act intends to ensure equal access to technology for all persons with disabilities. Therefore, AI providers will need to ensure compliance with these requirements by design. All the above points have shown that, before placing in the EU market or putting into service a high-risk AI system, AI providers will need to consider multiple horizontal and sector-specific laws if they want to guarantee a holistic compliance of their products to EU law. Despite the contradictions and overlaps between the AI Act and other horizontal and sectorial laws, it is expected that the Commission will perform an in-depth gap analysis where it will provide clarification about the relationship between those laws. @2024 BSI. All rights reserved. For AI systems falling under the high-risk classifica-tion, there are stringent requirements. The legal act \n\ndoes not specify how to fulfill its requirements at \n\ntechnical level â€“ therefore, the AI Act will be supported by a series of technical specifications produced by European Standardization Organiza-tions (ESOs) 81 following a mandate by the Commis-sion. The standards will translate the AI Actâ€™s requirements into actionable steps. Although these standards are not mandatory, AI providers that follow harmonized standards adopted by CEN/ CENELEC will benefit from the â€œpresumption of conformityâ€ with the AI Act. There are still a greater number of AI-specific standards under development. 81  ESO are the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC) and the European Telecommunications Standards Institute (ETSI). 82  See Articles 50(2), 96 & Recitals 116, 121 of AI Act The AI Act lists requirements for high-risk AI systems in Chapter III, Section 2 (articles 9 to 15). \n\nCompliance with the requirements should take into account the AI systemâ€™s intended purpose and what is generally acknowledged as State of the Art (SotA). Table 1 provides a high level summary of Chapter III requirements. SotA is not a well-defined term, neither in the AI Act nor under other relevant NLF legislations. However, we can find multiple references to the SotA in the AI Act â€“ harmonized standards, common specifications, technical standards, and codes of practice. 82 Requirements & Obligations @2024 BSI. All rights reserved. Table 1: High-risk AI systems requirements 83 Requirement Summary \n\nRisk management (RM) system The RM system planned and run throughout the entire lifecycle shall comprise of: \n\nâ€¢ Identification and analysis of the known and foreseeable risks to health, safety, or fundamental rights. â€¢ Evaluation of risks, including the analysis of data gathered from the post-market monitoring system. â€¢ Adoption of appropriate and targeted RM measures. Most appropriate RM measures shall reduce risks as far as technically feasible, with a view to minimizing risks effectively so that each residual risk as well as the overall residual risk are acceptable, considering the context and the deployerâ€™s technical knowledge, experience, education, and training. Specific considerations are made for vulnerable persons and those under the age of 18. \n\nData and data governance Data governance practices shall concern: \n\nâ€¢ Design choices. â€¢ Data collection processes and the original purpose of data collection. â€¢ Data-preparation processing operations such as annotation, labelling, cleaning, updating, enrichment, and aggregation. â€¢ Relevant assumptions on information that the data are supposed to measure. â€¢ Prior assessment of the availability, quantity, and suitability of the needed datasets. â€¢ Biases affecting health and safety or leading to discrimination. â€¢ Being free of errors and complete, to the best extent possible. \n\nTraining, validation and testing data sets shall: \n\nâ€¢ To the best extent possible free of errors and complete, and having the appropriate statistical properties at the level of individual data sets (or a combination thereof), including in regards to the persons or groups on which the system is intended to be used. â€¢ Take into account the intended purpose, the characteristics, or the elements that are particular to the specific geographical, behavioral, or functional setting within which the system is intended to be used. 83  See chapter III, Section 2 of AI Act @2024 BSI. All rights reserved. Requirement Summary \n\nTechnical documentation Technical documentation shall: \n\nâ€¢ Be drawn up before placing on the market or put into service and shall be kept up-to date. â€¢ Provide national competent authorities and notified bodies with all the necessary information in a clear and compre-hensive form to assess the compliance of the AI system with requirements. â€¢ Contain, at a minimum, the elements set out in Annex IV (amendable by Commissionâ€™s delegated acts). In the case of small and medium-sized enterprises (SMEs), including start-ups, any equivalent documentation should meet the same objectives, unless deemed inappropriate by the competent authority. Where a high-risk AI system is placed on the market or put into service, one single technical document shall be drawn up containing all the information required under those legal acts listed in Annex I. Annex IV describes the required content of the technical documentation. \n\nRecord-keeping AI systems shall technically allow for the automatic recording of events (â€˜logsâ€™) over their lifecycle. Logging capabilities shall enable: \n\nâ€¢ The recording of events relevant for identification of situations that may result in risks to health or safety or fundamental rights of persons. â€¢ Post-market monitoring. â€¢ Monitoring of the operations. â€¢ Recording of each use period of the system. â€¢ The reference database against which input data has been checked by the system. â€¢ The input data for which the search has led to a match. â€¢ The identification of the natural persons involved in the verifi-cation of results. \n\nTransparency and provision of information to deployers \n\nAI systems shall be designed and developed to ensure sufficiently transparent operation, achieving compliance with the relevant obligations, and enabling deployers to understand and use the system appropriately. It shall be accompanied by instructions for use (IFU) in an appropriate digital format or otherwise including concise, complete, correct, and clear information, which is relevant, accessible, and comprehensible to deployers. @2024 BSI. All rights reserved. Requirement Summary \n\nHuman oversight AI systems shall be designed to be effectively overseen by natural persons, aiming at preventing or minimizing the risks to health, safety, or fundamental rights. Human oversight shall be ensured through â€œpreâ€ built-in measures (by the provider) and â€œpostâ€ measures (identified by the provider, but implemented by the user), and be enabled to understand capacities and limitations, automation bias, the systemâ€™s output, as well a the ability to override or reverse the output and interrupt the system. For remote biometric identification systems, two natural persons separate verification and confirmation is required. \n\nAccuracy, robustness and cybersecurity AI systems shall be designed and developed to achieve an appropriate level of accuracy, robustness, and cybersecurity, and to perform consistently in those respects throughout their lifecycle. Levels of accuracy and accuracy metrics shall be declared in instructions for use. High-risk AI systems shall be resilient as regards errors, faults, or inconsistencies due to their interaction with natural persons or other systems. The robustness may be achieved through technical redundancy solutions (backup or fail-safe plans). Learning AI systems shall be developed to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (â€˜feedback loopsâ€™). Appropriate cybersecurity solutions to address AI specific vulnerabilities shall include measures to prevent and control for attacks trying to manipulate the training dataset (â€˜data poisoningâ€™), inputs designed to cause the model to make a mistake (â€˜adversarial examplesâ€™), or model flaws. The AI Act goes further than other NLF legislations, as it details obligations not only for economic operators (e.g., providers, importers, distributors), but also obligations for deployers, providers, and deployers of certain AI systems (Chapter IV), providers of GPAI models (Chapter V, Section 2) and GPAI models with systemic risk (Chapter V, Section 3). These obligations are detailed in Table 2. @2024 BSI. All rights reserved. Table 2: AI Act obligations Obligations per economic operators Summary \n\nObligations of providers of high-risk AI systems 84 Providers shall ensure compliance with AI Act requirements, indicate in the AI system the provider information, comply with Article 17 (QMS), keep Article 18 documentation available for 10 years, keep automatic generated logs for at least six months, follow appropriate conformity assessment procedures, comply with registration obligations (Article 49), affix CE and draw up an EU declaration of conformity, ensure compliance with accessibility requirements, and investigate and inform non-conformities and corrective actions to appropriate stakeholders. \n\nAuthorized representatives of providers of high-risk AI systems 85 \n\nEuropean Union providers shall mandate authorized representatives for specific tasks - namely, an EU declaration of conformity and technical documentation verification, keeping the prementioned plus the issued certificate and providing contact details for 10 years, provide national competent authorities with documentation and access to logs, cooperating to reduce/mitigate risks, and where applicable complying with registration obligations. \n\nObligations of importers 86 Importers shall ensure their systems are in conformity with the AI Act, verifying that a conformity assessment according to Article 43 has been carried out, technical documentation, CE marking, EU declaration and IFUs are in place, and that the authorized representative is assigned. Packaging or documentations should indicate the importerâ€™s details. Importers should cooperate with national competent authorities and keep the relevant documentation for 10 years. \n\nObligations of distributors 87 Distributors shall verify the required CE marking, EU declaration of conformity and instruction of use, and that provider and importer have complied with their obligations. They shall inform providers or importers of risks to the health or safety or to fundamental rights of persons, take the corrective actions necessary to bring system into conformity or ensure provider, importer or, operator do, and inform national competent authorities of the non-compliance or any corrective actions taken. Distributors shall cooperate and provide national competent authorities, upon reasoned request, information and documentation. 84  See Article 16 of AI Act 85  See Article 22 of AI Act 86  See Article 23 of AI Act 87  See Article 24 of AI Act @2024 BSI. All rights reserved. Obligations per economic operators Summary \n\nObligations of deployers of high-risk AI systems 88 Deployers shall use the system in accordance with the instructions of use, assign human oversight to natural persons with the necessary competence, training, and authority, and ensure input data is relevant and sufficiently representative. They shall inform the provider or distributor in case of risks or serious incidents and interrupt the use of the system, fulfill rules on internal governance arrangements, processes, and mechanisms pursuant to the relevant financial service legislation in case of financial institutions, and keep logs for at least six months. They are required to comply with the registration obligations, carry out data protection impact assessment (GDPR) and cooperate with national competent authorities. \n\nTransparency obligations for providers and deployers of certain AI systems 89 \n\nAI systems intended to interact directly with natural persons, should be designed to inform interaction with AI, unless this is obvious. Obligations are not applicable to AI systems authorized by law to detect, prevent, investigate, or prosecute criminal offences. AI systems, generating synthetic audio, image, video, or text content shall ensure outputs are marked as AI generated or manipulated. Deployers of an emotion recognition or biometric categorization system, excluding systems permitted by law to detect, prevent, or investigate criminal offences shall inform the natural person on the exposure to the system. \n\nObligations for providers of general-purpose AI models 90 Providers of GPAI models, other than those which are free & open license, should draw and maintain technical documentation according to Annex XI, and draw up information/documentation to be provided to other AI system providers intending to integrate the GPAI model into their system. \n\nObligations for providers of general-purpose AI models with systemic risk 91 \n\nIn addition to GPAI providers obligations, systemic risk GPAI providers need to perform model evaluation including adversarial testing, assessing & mitigating systemic risks at the EU level, reporting appropriate incidents, and ensurng cybersecurity protection of the model and the physical infrastructure. Any distributor, importer, deployer or other third-party that makes a substantial modification of a high-risk AI system OR changes the intended purpose of a non-high risk AI turning it into a high risk one, it will be considered the provider and will be subject to the AI Act providersâ€™ obligations. 92 88  See Article 26 of AI Act 89  See Article 50 of AI Act 90  See Article 53 of AI Act 91  See Article 55 of AI Act 92  See Article 25 of AI Act @2024 BSI. All rights reserved. The AI Actâ€™s regulatory framework introduces a structured ecosystem of entities entrusted with the assessment, certification, and oversight of AI systems. This framework aims to harmonize approaches, ensuring the safe and ethical deployment of AI systems. To unpack this regulatory landscape, we explain and clarifying the definitions of the involved stakeholders: \n\n# AI Act ecosystem @2024 BSI. All rights reserved. National Level \n\nConformity Assessment Body (CAB): 93 A separate legal entity that performs third-party conformity assessment activities including testing, certification, and inspection. The primary objective of a CAB is to ascertain that AI systems meet requirements of the relevant applicable standards. \n\nNotified Body (NB): 94 A specialized form of CABs, NBs undergo formal notification in accordance with the EU AI Act and relevant EU harmonization legislation. Their tasks include conducting conformity assessment activities for high-risk AI systems, adhering to organizational, quality management, resource, process, and cybersecurity requirements. NBs maintain independence from evaluated AI system providers, ensuring impartiality and confidentiality. Articles 31 to 34 of the EU AI Act delineate specific obligations and operational criteria for NBs. \n\nNotifying Authority (NA): 95 Designated within each Member State, NAs manage the procedural framework for assessing, designating, and notifying CABs, alongside ongoing supervision. Operating under a mandate to prevent conflicts of interest, NAs uphold principles of objectivity and impartiality. They are structured to separate decision-making from assessment activi-ties, explicitly prohibiting any commercial or competitive offerings. NAs ensure their personnel are highly qualified in relevant fields, including information technologies, artificial intelligence, and law. \n\nMarket Surveillance Authority (MSA): 96 Designated as the national authority responsible for overseeing market activities to ensure compliance with legal requirements, particularly for high-risk AI systems. They enforce the regulation by monitoring, identifying non-compliance, and oversight of the corrective actions implemented by the AI providers to protect public inter-ests, health, safety, and fundamental rights. \n\nNational Competent Authority (NCA): 97 It includes the NA and the MSA. It represents the authoritative entities designated by EU member states to oversee the regulation and compliance of AI systems within their jurisdictions, focusing on ensuring the safety, security, and rights compliance of AI technologies. EU Level \n\nArtificial Intelligence Office (AIO): 98 An office within the European Commission tasked with monitoring and supervising AI systems, general-purpose AI models, and AI governance. The AIO plays a central role in fostering a coherent regulatory framework for AI across the EU, ensuring compliance with legislative mandates, facilitating enforcement, and overseeing AI governance to safeguard public interest and uphold standards. \n\nEuropean Artificial Intelligence Board (AIB): 99 An advisory and coordinating body established to support the consistent and effective application of the AI Act across the EU. It functions to enhance cooperation among national competent author-ities tasked with the Regulationâ€™s enforcement, share technical and regulatory expertise, and promote best practices among Member States. 93  See Article 3 (21) of AI Act 94  See Article 3 (22) of AI Act 95  See Article 3 (19) of AI Act 96  See Article 3 (26) of AI Act 97  See Article 3 (48) of AI Act 98  See Article 3 (47) of AI Act 99  See Article 65 of AI Act @2024 BSI. All rights reserved. The AI Act lists the responsibilities of \n\nNotified Bodies (NBs): 100 01. Conformity Assessment 101 :\n\nâ€¢ NBs, or CABs acting on their behalf, impartially evaluate the quality management system (AI management system) 102 implemented by the provider of a high-risk AI system and the tech-nical documentation 103 , submitted by that provider. These assessments aim to verify compliance with the AI Actâ€™s harmonized stand-ards and common specifications, focusing on applicable requirements. Each assessment is completed with an audit report 104 detailing the outcomes, identifying areas of compliance, and highlighting non-compliant areas (so-called â€œnon-conformitiesâ€) with the AI Actâ€™s regulatory framework. â€¢ NBs are granted necessary and relevant access to the training, validation, and testing datasets utilized by AI systems, potentially through application programming interfaces (APIs) or other mechanisms facilitating remote access, underpinned by adequate security safeguards. NBs are also granted access to the training and trained models of the AI system, including relevant parameters (e.g., weights, architecture), after alternative conformity verification methods and finding them inadequate. NBs must conduct direct testing if they find the tests provided by the high-risk AI system provider inadequate. 105 \n\n02. Issuance of Certificates: 106 \n\nâ€¢ Upon successful conformity assessment, NBs issue certificates to AI system providers, signi-fying compliance with the AI Act and associated requirements. 100  See Articles 31, 34 of AI Act 101  See Recitals 50,78,86, 123, 125 and Articles 45, 46,57 Section 7 of AI Act 102  See Recital 173, Article 43 and Annex VII, Section 5 of AI Act 103  See Recitals 66,173, Articles 11, 43 Annex VII, Section 4.3 of AI Act 104  See Annex VII, Section 5.3 of AI Act 105  See Annex VII, Sections 4.3-4.5 of AI Act 106  See Article 44 and Annex VII of AI Act 107  See Annex VII, Section 5 of AI Act 108  See Annex VII, Sections 3.4 and 4.7 of AI Act \n\n03. Continuous Monitoring and Surveillance: 107 \n\nâ€¢ Following a successful conformity assessment, NBs undertake ongoing surveillance of EU conformity certificates issued to AI system providers. This surveillance focuses on ensuring continual compliance with the AI Act and includes regular audits of quality management system to verify continuous compliance with applicable regulatory and technical requirements (harmonized standards, common specifications, and industry practices, in the absence of harmo-nized standards and common specifications). â€¢ NBs must be informed by providers of high-risk AI systems of any proposed changes to the AI management system or AI system itself 108 that could impact compliance (and/or intended purpose) and NBs review the relevant documentation to approve changes where compliance is maintained. @2024 BSI. All rights reserved. 04. Cooperation among Notified Bodies: \n\nâ€¢ NBs communicate with other NBs regarding any refusals, suspensions, or withdrawals of quality management system or EU technical documentation approvals. Additionally, upon request, the NB must provide information regarding quality system approvals it has issued. 109 \n\n05. Documentation and Reporting: \n\nâ€¢ NBs maintain records and associated evidence of all assessments, decisions, and certifications, which are made accessible to NCAs for oversight purposes. 110 \n\nThe key responsibilities of NCAs 111 within the regulatory framework under the AI Act entail diverse roles essential for ensuring effective governance and oversight of the national-level implementation of the AI Act: \n\n01. Regulatory Oversight: \n\nâ€¢ NCAs can request and review documentation from providers of non-high risk AI systems and general-purpose AI models (GPAI), ensuring transparency and compliance with AI Act regula-tory requirements. 112 NCAs also request technical documentation from providers of high-risks AI systems (when the NB is not involved). 113 â€¢ NCAs may support the provision of high-quality data for AI system training, which aims to support innovations, as well as the transparency and reliability of the data framework. 114 109  See Article 45, of AI Act 110  See Articles 45 AI Act 111  See Article 28 of AI Act 112  See Article 6, Section 6 and Article 53, Section 1 AI Act 113  See Article 11, Section 1 AI Act 114  See Recital 68 of AI Act 115  See Article 70 of AI Act 116  See Article 30 of AI Act 117  See Article 57 of AI Act 118  See Article 70 of AI Act â€¢ NCAs serve as primary points of contact, facili-tating communication, regulatory, and technical compliance. 115 â€¢ NCAs support the European Commission and exchange information between NCAs. 116 \n\n02. Support for Innovations: \n\nâ€¢ NCAs establish national-level AI regulatory sandboxes to foster innovation and enable testing of AI systems and inform the AIO about the progress. Collaboration between NCAs, the AIO, and other relevant authorities facilitates knowledge exchange and best practices dissemi-nation. 117 â€¢ NCAs can offer SMEs and startups guidance on the AI Act, aligned with the Board and Commis-sion position. For AI systems under other EU laws, relevant authorities must be consulted. 118 @2024 BSI. All rights reserved. 03. Enforcement Actions: 119 \n\nâ€¢ NCAs (specifically MSA) are responsible for oversight of corrective actions implemented by the AI model providers to address incidents with involved AI systems, issuing warnings, imposing fines, and mandating compliance measures to uphold regulatory integrity. NCAs also act as the single point of contact. \n\n04. Designation, Monitoring, and Coordination of \n\nNotified Bodies (NBs): 120 \n\nâ€¢ NCAs designate NBs and ensure their qualifications comply with the AI Actâ€™s requirements for conducting conformity assessments. NCAs monitor and audit NBs, ensuring their compliance with the AI Actâ€™s requirements, which include the maintenance of competence, impartiality, and other legal, regulatory, and technical requirements applicable to the NBs 121. \n\nThe essential duties of the AIO within the regulatory framework established by the AI Act \n\nand its official website 122 are outlined as follows: \n\n01. Central Coordination, Governance, and Oversight of GPAI: \n\nâ€¢ The AIO serves as the central coordinating body among EU member states, providing guidance and support for consistent AI Act implementa-tion. It facilitates best practice exchange and ensures alignment across the EU. The AIO leverages its expertise in establishing EU-level advisory bodies (such as the Artificial Intelligence Board) fostering collaboration to ensure coherent AI Act application across all Member States. 123 119  See Recital 153, Article 89 of AI Act 120  See Article 38 of AI Act 121  See Article 28 of AI Act 122  See https://digital-strategy.ec.europa.eu/en/policies/ai-office 123  See Recital 148 of AI Act 124  See Articles 88, 91, 92 of the EU AI Act â€¢ The AIO supervises GPAI (requirements for providers, authorized representatives, deployers) to ensure compliance with the AI Act, standards, and associated requirements. It develops tools, methodologies, and benchmarks for evaluating the capabilities and reach of GPAI models, classifying models with systemic risks. The AIO also provides oversight of corrective actions taken by GPAI provider in case of non-compliance. The AIO has the power to request documentation and information from the GPAI provider and its authorized representatives, conduct evaluation of GPAI-models and request measures (including to restrict making GPAI available on the market, or to withdraw or recall the model). It also monitors fulfilment of obligations by the providers of GPAI. 124 @2024 BSI. All rights reserved. â€¢ The AIO provides coordination support for joint investigations in case of market surveillance \n\nwith involvement of specific categories of \n\nhigh-risk AI system(s), supervises and monitors the compliance of GPAI, as well as taking necessary action(s) to monitor effective implementation and continuing compliance for providers of GPAI models with the AI Act. 125 \n\n02. Policy Development: \n\nâ€¢ The AIO provides standardized templates for the areas required by the AI Act (e.g., summary of content used for training of the GPAI, summary of the content used codes of practice, questionnaire for deployers). 126 â€¢ The AIO advises on best practices, facilitates access to AI testing environments, and promotes innovative ecosystems to boost EU competitiveness. 127 â€¢ The AIO ensures the regulatory framework adapts to technological advancements and societal needs by engaging with diverse stakeholders, including AI developers, SMEs, and experts. It fosters continuous dialogue to inform policy formulation and develop codes of practice aligned with the AI Act. 128 \n\n03. Cooperation with stakeholders: \n\nâ€¢ The AIO collaborates with a diverse range of stakeholders, such as NCAs, providers of \n\nGPAI, scientific panels (including independent experts), institutions, the European Artificial Intelligence Board, and the European Centre for Algorithmic Transparency (ECAT), to gather and share technical and regulatory expertise, including knowledge gathered from the establishment, running, and oversight of AI sandbox. 129 125  See Recitals 112,114, 160, 161, 162, 164 and Article 75 of AI Act 126  See Recitals 107,108 and Articles 25, 27, 50, 56, 95 of AI Act 127  See Recitals 116, 117, 179 of AI Act 128  See Recitals 113, 151 and Article 90 of AI Act 129  See Recital 111, 116, 163 and Articles 57, 68 of AI Act 130  See Articles 65 and 66 of AI Act 131  See Article 74, Section 11 of AI Act \n\nThe European Artificial Intelligence Board 130 plays a  major role in robust oversight and the application of the AI Act by the Commission and Member states. It achieves this through several key activities: 1 Facilitating coordination among NCAs responsible for enforcing the AI Act and endorsing joint market surveillance activities. 131 2 Maintaining technical expertise and best practices across EU Member States, as well as contributing to a cohesive understanding and application of the AI Act. 3 Delivering strategic advice on the regulationâ€™s enforcement, focusing on GPAI models and aiming to standardize approaches and interpretations. 4 Emphasizing the importance of consistent conformity assessments, the effective use of regulatory sandboxes, and the value of testing AI systems in real-world scenarios. 5 Playing a critical role in advising on the regulationâ€™s implementation and offering recommendations on various fronts including codes of conduct, the evolution of AI standards, and the integration of emerging technologies. 6 Engaging in broad educational efforts to boost AI literacy and fostering the publicâ€™s awareness of AIâ€™s benefits and risks, while facilitating cross-sectoral and international cooperation to enhance the regulationâ€™s global relevance and effectiveness. @2024 BSI. All rights reserved. The development process of the AI Act has been characterized by significant complexity and anticipa-tion over the past several months, as this period was fraught by a series of strategic discussions, uncer-tainties, and widespread speculations regarding the outcome. Nevertheless, the legislative journey succeeded with the final political agreement and the adoption of the Act on 13th March 2024. This result indicates a procedural timeline which is anticipated to take approximately up to two months for the formal publication of the legislation in the Official Journal of the EU. The legislationâ€™s entry into force, the significant milestone for the new AI Act, will be twenty days after its publication in the Official Journal. To operate within the timelines in practical perspective, we analysed associated key milestones and relevant actions to be completed. The results of this analysis are presented in Table 3 .\n\n# AI Act timelines @2024 BSI. All rights reserved. Table 3 â€“ Key timelines of the AI Act implementation Timeline Relevant Action 13 March 2024 â€¢ Adoption on the EU Artificial Intelligence Act in the European Parliamentâ€™s plenary. \n\n22â€“25 April 2024 â€¢ Approval of the AI Act corrigenda in the plenary of the parliament Entry into force 132 \n\n1 August 2024 \n\nâ€¢ 20 days after publication in the Official Journal of the EU. Entry into force + Three months 133 \n\n2 November 2024 \n\nâ€¢ Member States must list, publish, and keep up to date list of public authorities and/or bodies. Entry into force + Six months 134 \n\n2 February 2025 \n\nâ€¢ Date of application for prohibited AI systems to be available on the market (Chapter II) â€¢ Date of application of general provisions (Chapter I) Entry into force + Nine months 135 \n\n2 May 2025 \n\nâ€¢ Readiness of codes of practices to be published by the AIO. Entry into force + 12 months \n\n2 August 2025 \n\nâ€¢ Chapter III (High-Risk AI Systems) Section 4, Chapter V (GPAI Models), Chapter VII, and Chapter XII, except Article 101. 136 â€¢ If the AI Office finds the code of practice insufficient or unfinalized 12 months after Entry into Force, the Commission, via implementing acts, may establish common rules for obligations in Articles 53 and 55, aligning with the examination process of Article 98(2). 137 â€¢ Member states to have implemented rules on penalties, including administrative fines. 138 â€¢ Readiness of NBs and governance structure, including conformity assessments. 139 â€¢ If no code of practice is finalized within 12 months after entry into force, or if deemed inadequate by the AIO, the Commission may issue implementing acts for Articles 53 and 55 obligations, following Article 98(2)â€™s examination procedure. 140 â€¢ Member States must inform the Commission of NAs and MSAs, including their tasks, and provide publicly accessible information on how NCAs (in a form of single point contact) can be contacted. 141 â€¢ The Commission will provide guidance for high-risk AI system providers to comply with obligations of reporting of serious incidents to MSAs. 142 â€¢ Member States must inform the Commission about their NCAâ€™s financial and human resources, assessing their sufficiency, with this being repeated every two years afterwards. The Commission will share this data with the AI Board for analysis and potential advice. 143 â€¢ Following the entry into force and until the delegated powers in Article 97 expire, the Commission is tasked with annually evaluating the necessity for updates to Annex IIIâ€™s list and Article 5â€™s catalogue of banned AI practices. The outcomes of these assessments will be systematically presented to both the European Parliament and the Council. 144 132  See Article 113 of AI Act 133  See Article 77 of AI Act 134  See Article 113 of AI Act 135  See Recital 179 and Article 56, Section 9 of AI Act 136  See Recital 179 and Article 113 of AI Act 137  See Article 56, Section 9 of AI Act 138  See Recital 179 of AI Act 139  See Recital 179 of AI Act 140  See Article 56 of AI Act 141  See Article 70, 59 of AI Act 142  See Article 73 of AI Act 143  See Article 70 of AI Act 144  See Article 112 of AI Act @2024 BSI. All rights reserved. Timeline Relevant Action \n\nEntry into force + 18 months \n\n2 February 2026 \n\nâ€¢ The Commission, as well as the consulting AI Board, are to provide guidelines for the AI Actâ€™s entry, including examples of high/non-high risk AI use cases as required by Article 96. 145 Entry into force + 24 months \n\n2 August 2026 \n\nâ€¢ Application of the AI Act, which includes obligations on high-risk AI systems specifically listed in An-nex III, inlcuding AI systems in biometrics, critical infrastructure, education, employment, access to essential public services, law enforcement, immigration, and the administration of justice to comply with the AI Act. It applies when significant design changes occur from that timeframe, aligning with Article 5 under Article 113(3)(a). 146 â€¢ Member States are mandated to create at least one national AI regulatory sandbox. Implementation of this requirement can be a collaborative effort among different Member States. The EU Commis-sion offers support for sandbox development and operation. States may also join existing sandboxes, provided they ensure equivalent national coverage. 147 Entry into force + 36 months \n\n2 August 2027 \n\nâ€¢ Applicability of Article 6(1) (classification rules for Annex I Union harmonization legislation) and corre-lated obligations of the AI Act. 148 â€¢ GPAI model providers with products placed on the EU market 12 months prior to the AI Actâ€™s entry into force are required to undertake essential measures to meet the relevant obligations of GPAI model providers. 149 Entry into force + 48 months 150 \n\n2 August 2028 \n\nâ€¢ Every 48 months starting from the AI Actâ€™s entry into force, the Commission will analyse and inform the European Parliament and Council on potential amendments to Annex III, Article 50â€™s AI system transparency requirements, and improvements to supervisory and governance frameworks. â€¢ Every 48 months from the AI Actâ€™s entry into force, the Commission will report its evaluation to the Parliament and Council, focusing on enforcement structure and the potential for an EU agency to address gaps. Amendments may be proposed based on these insights. All reports shall be made publicly available. â€¢ The Commission will assess the AI Officeâ€™s performance, examining if it possesses adequate powers and competences for its duties, and if needed enhancing its role and enforcement capabilities, along with increasing its resources. The evaluation report will be submitted to the European Parliament and Council. â€¢ Every 48 months from the AI Actâ€™s entry into force, the Commission reports on the advancement of standardisation in energy-efficient development of general-purpose models. This includes assessing the necessity for additional measures, which are potentially binding. The final evaluation report will be submitted to the European Parliament and the Council and made publicly available. â€¢ The Commission is mandated to assess the influence and efficacy of voluntary codes of conduct every three years afterwards. These evaluations are aimed to optimize adoption of Chapter II, Section 2â€™s requirements for providers of AI systems not classified as high-risk and to explore the potential for integrating additional mandates, notably concerning environmental sustainability. 151 Entry into force + 60 months 152 \n\n2 August 2029 \n\nâ€¢ The Commission is tasked with conducting a review of the AI Act, at intervals of four years and to report the findings to the European Parliament and the Council. An annual assessment is to be provided by the Commission to evaluate potential revisions to the lists of high-risk AI systems and prohibited practices. Entry into force + 72 months \n\n2 August 2030 \n\nâ€¢ Providers and deployers of high-risk AI systems designated for public authority use shall comply with AI Actâ€™s requirements. 153 145  See Article 6 of AI Act 146  See Articles 111 and 113 of AI Act 147  See Article 57 of AI Act 148  See Article 113 of AI Act 149  See Article 111 of AI Act 150  See Article 112 of AI Act 151  See Recital 174 of AI Act 152  See Recital 174 of AI Act 153  See Article 111 of AI Act @2024 BSI. All rights reserved. Timeline Relevant Action 31 December 2030 â€¢ AI systems integrated within large-scale IT frameworks, as specified in Annex X, and operational 36 months before the AI Actâ€™s entry into force must comply with the AI Act. Evaluations of these large-scale IT systems, mandated by the legal acts in Annex X, will incorporate the AI Actâ€™s requirements, especially when these acts undergo revisions or updates. 154 Entry into force + 84 months 155 \n\n2 August 2031 \n\nâ€¢ The Commission is required to execute an assessment of its enforcement. This analysis will be re-ported to the European Parliament, the Council, and the European Economic and Social Committee, reflecting on the initial years of the AI Actâ€™s application. Based on findings, if and when necessary, the final report shall be accompanied by a proposal for AI Actâ€™s amendment regarding the structure of enforcement and changes needed to be implemented by the EU agency to resolve any identified negative findings. 154  See Article 111 of AI Act 155  See Article 112, Section 13 of AI Act @2024 BSI. All rights reserved. The AI Act introduces the term â€œsubstantial modificationâ€, referring to a change to an AI system already placed on the market or put into service which is not foreseen in the initial conformity, and may affect compliance or modify its intended purpose. 156 The AI Act also introduces the term â€œpredetermined changesâ€, a term used to \n\ndescribe predefined changes subject to an initial \n\nconformity assessment of AI systems which are not static but continue to learn or evolve following placement on the market. 157 For high-risk AI systems that have been placed on the market prior to the application of the AI Act, the AI Act applies only if following the AI Actâ€™s application date there are significant changes in the design or intended purpose. Furthermore, the AI Act makes no distinction for the prementioned purpose between the terms â€œsignificant changeâ€ and â€œsubstantial modificationsâ€. 158 156  See Article 3(23) of AI Act 157  See Recital 128 and Article 43(4) of AI Act 158  See Recital 177 of AI Act 159  See Recital 84 of AI Act The AI Act makes an exemption for AI systems which are components of the large-scale IT systems and high-risk AI systems intended to be used by public authorities â€“ compliance of those systems with the AI Act requirements is required by end of 2030, or by six years after the entry into force. \n\nHowever, it is not immediately clear when a high-risk AI system falls under another EU legislation â€“ as in the case of Annex I section A products â€“ which legislation prevails in \n\nterms of substantial modifications. The answer to this question can be found in the recitals of the AI Act. 159 If a change is not consid-ered significant under a more specific EU legislation (e.g., Regulation 2017/745 MDR), then the change should not trigger substantial modification under the relevant clauses of the AI Act. \n\n# AI products already on the market @2024 BSI. All rights reserved. The AI Act brings scrutiny also to sectors that were not previously subject to regulation. Due to its horizontal nature and levels of risk, the AI Act has different obligations for AI providers and deployers to ensure conformity of AI systems. The principal mechanisms of compliance within the AI Act are: \n\nThe use of a quality management system (QMS): 160 Although the recently published standard ISO/IEC 42001:2023 Information Technology Artificial Intelligence Management System (AIMS) is not yet harmonized with the AI Act, it is expected to be the reference standard for conformity with the relevant requirements. The AIMS should cover a strategy for compliance, processes on design, development, data governance, testing and validation of AI systems, risk management, post-market monitoring, and incident reporting. Providers and deployers of high-risk AI systems are obliged to use AIMS, with this also being an obligation for AI providers that are required to follow conformity routes stated in Annex VI (internal control) and Annex VII (assessment of QMS and Technical Documentation). When AI systems are subject to obligations for a QMS under other sectorial EU legislations, AIMS aspects may be covered as part of the sectorial QMS standard. 160  See Article 17 of AI Act 161  See Articles 11 and 53 of AI Act 162  See Articles 8 to 14 of AI Act \n\nThe creation and maintenance of technical documentation: 161 High-risk AI systems that may follow the conformity routes of Annex VI (internal control) and Annex VII (assessment of QMS and technical documentation (TD)), and providers of general-purpose AI models, will require putting in place TD for assessing compliance with the AI Act Chapter III, Section 2 requirements. Annex XI (TD for providers of general-purpose AI models, Article 53(1))) and Annex IV (TD referred to in Article 11(1)), describe the content of the TD. TD will need to be drawn up before placing the AI system on the market. When the AI system is subject to obligations of TD under other sectorial EU legislations, AI Act aspects may be covered as part of the sectorial TD. \n\nConformity with high-risk AI system requirements: 162 Chapter III, Section 2, lists high-risk AI systems requirements. Although the ISO/IEC JTC 1/SC 42 committee has already published multiple standards, none has as of yet been harmonized with the AI Act. The legislation clearly states (Article 40) that conformity with the AI Actâ€™s harmonized standards is a presumption of conformity with AI Act requirements. \n\n# Understanding AI Act conformity @2024 BSI. All rights reserved. Transparency obligations: 163 Providers of AI systems, including general-purpose AI systems, as well as deployers of certain AI systems, need to comply with transparency obligations set in Chapter IV, Article 50. \n\nSandboxes: 164 Sandboxes are established by members states, competent authorities. Sandboxes are controlled environments to facilitate development, training, testing, and validation of innovative AI systems. AI systems might use sandboxes prior to being placed on the market or put into service. The output (exit reports) of the sandboxes may be used to demonstrate compliance with the AI Act, as part of documentation provided for the conformity assessment process, or provided to relevant market surveillance authorities. \n\nRoutes to conformity: 165 â€¢ When a high-risk AI system is subject to other sectorial EU legislations (Annex I, section A) the provider shall follow the relevant conformity assessment procedure as required under those legal acts. Requirements set out in the AI Act apply to these AI systems, and Notified Bodies may request datasets and the AI model for carrying out additional testing (Annex VII). â€¢ Internal control (Annex VI) is available as a conformity assessment route to Annex III high-risk AI systems, however, Annex III point 1 AI systems (Biometric identification) may opt for (or be forced to follow) Annex VII conformity assessments which require the Notified Bodyâ€™s involvement for the assessment of AIMS and TD. 163  See Article 50 of AI Act 164  See Article 57 of AI Act 165  See Article 43 of AI Act 166  See Article 51 and 56 of AI Act 167  See Article 56 of AI Act \n\nGeneral-purpose AI (GPAI) models: The main oversight authority for GPAIs is the AI Office. Article 53 states obligations for GPAI providers and Annex XI describes technical documentation requirements for GPAIs models. Obligations are not applicable for free and open-license providers. There are additional obligations for GPAI models with systemic risks, including model performance evaluation, mitigation measures of systemic risks, and cybersecurity protection. 166 Compliance with codes of practice 167 will be considered sufficient for GPAIs systemic risk obligations, until harmonized standards are released. @2024 BSI. All rights reserved. The AI Act is en route to the Official Journal of the \n\nEuropean Union, and the world is watching. What started out as conjecture has evolved into an emerging and impactful industry, not without its risks. It is evident in the legislative text that not one person or one group of persons can manage this new landscape on its own. It will take the collective effort of providers, deployers, the Commission, the AI Office, the AI Board, member states, National Competent Authorities, the public, and others to ensure the predictability and deployment of these systems is effectively controlled. The momentum of the AI Act has left many wondering what the next steps are to prepare themselves for its impact. We anticipate that this paper will support organizations to take the firsts steps to determine if and how AI systems or models affect their organization, identifying the organiza-tionâ€™s role and obligations, supporting under-standing of AI systems classification, and shedding 168  See Recital 178 of AI Act light on the requirements need to be fulfilled, as well as the methods those requirements are met. Organi-zations should be able to understand conformity routes for their products, allowing them to proac-tively seek accredited or designated organizations under the schemes of interest to consolidate assess-ments where possible. \n\nThe AI Act encourages providers of high-risk AI systems to start the compliance journey on a voluntary basis during the transitional period. 168 BSI shares this view; all stakeholders will face a steep learning curve. BSI deeply values the AI Act and the approach adopted by the community engaged in the development of this regulation, recognizing that the publishing of the AI Act by legislators and achieving compliance are not end goals. Instead, they represent a journey of ongoing enhancement of the regulatory framework, evolving in tandem with technological advancements. \n\n# Conclusion", "fetched_at_utc": "2026-02-08T18:50:18Z", "sha256": "aa10c02636c147ef42826a5d78a3d6a203b40a5cb97638d275bf3b7a0d5b8c76", "meta": {"file_name": "BSI_EU_AI_Act_Whitepaper_Final_2_9_24.pdf", "file_size": 4255354, "relative_path": "pdfs\\BSI_EU_AI_Act_Whitepaper_Final_2_9_24.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 18373}}}
{"doc_id": "pdf-pdfs-debunking-10-common-eu-ai-act-misconceptions-part-1-oliver-patel-70e65b20e395", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel.pdf", "title": "Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1) \n\nhttps://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions  2/21 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! The EU AI Act entered into force in August 2024 and its first provisions became applicable in February 2025. Due to a combination of factors, such as the complexity and novelty of the law, and the lack of guidance and standards, some unhelpful misconceptions have taken hold. This two-part series on Enterprise AI Governance presents and debunks 10 common misconceptions about the AI Act, providing a detailed explanation for each one. The first 5 are covered in part 1 and the second 5 will be covered in part 2. \n\nThe ten misconceptions are: \n\n1. The EU AI Act has a two-year grace period and applies in full from August 2026.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 3/21\n\n2. All open-source AI systems and models are exempt from the EU AI Act. \n\n3. High-risk AI models are explicitly regulated under the EU AI Act. \n\n4. Emotion recognition is prohibited under the EU AI Act. \n\n5. Facial recognition is prohibited under the EU AI Act. \n\n6. Transparency is required for â€˜limited riskâ€™ AI systems. \n\n7. Third-party conformity assessments are required for all high-risk AI systems. \n\n8. Fundamental rights impact assessments are required for all high-risk AI systems. \n\n9. All high-risk AI systems must be registered in the public EU-wide database. \n\n10. Deployers do not need to register their use of high-risk AI systems. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nMisconception 1: The EU AI Act has a two-year grace period and applies in full from August 2026   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 4/21\n\nIt is commonly remarked that the EU AI Act has a two-year grace period, which allows organisations to prepare to be compliant. Although there are grace periods for compliance, they vary in length, and certain provisions are already applicable today. In fairness, Article 113 does state that the AI Act â€œ shall apply from 2 August 2026 â€. However, given the number of exceptions to this, simply claiming there is a â€˜two-year grace periodâ€™ is misleading. The reality is that different provisions become applicable at different times. Although most provisions apply from August 2026, the provisions on AI literacy and prohibited AI practices became applicable in February 2025, and the obligations for providers of new general-purpose AI models become applicable in August 2025. Here is a breakdown of when the most significant provisions apply: 2 February 2025: prohibition of specific AI practices became applicable. 2 February 2025: AI literacy provisions (for deployers and providers of AI systems) became applicable. 2 August 2025: obligations for providers of â€˜newâ€™ general-purpose AI models become applicable (i.e., general-purpose AI models placed on the market from 2 August 2025 onwards).   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 5/21\n\n2 August 2026: many of the AI Actâ€™s provisions become applicable, including obligations and requirements for high-risk AI systems listed in Annex III. 2 August 2027: obligations and requirements for high-risk AI systems which are products, or safety components of products, regulated by specific EU product safety laws (listed in Annex I) become applicable. 2 August 2027: obligations for providers of â€˜oldâ€™ general-purpose AI models become applicable (i.e., general-purpose AI models placed on the market before 2 August 2025). Although the provisions on prohibited AI practices became applicable earlier this year, meaningful enforcement will come later. This is because the applicability of the penalty and governance regime, including the deadline for member states to designate their AI regulators, lands on 2 August 2025. This creates a unique situation where there is a 6-month lag between important provisions becoming applicable and the regulatory enforcement structure and regime being operational. \n\nMisconception 2: All open-source AI systems and models are exempt from the EU AI Act   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 6/21\n\nAlthough there are broad exemptions for open-source AI systems and models, there are also several important ways in which the AI Act regulates them. For example, high-risk AI systems which are open-source are still classified as high-risk, and providers of general-purpose AI models which are open-source must adhere to specific obligations (which are trimmed down in some cases). Article 2(12) states that the AI Act â€œ does not apply to AI systems released under free and open-source licenses, unless they are placed on the market or put into service as high-risk AI systems or as an AI system that falls under Article 5 or Article 50â€. This has the following meaning: Providers and deployers of high-risk AI systems which are open-source must adhere to all the obligations and requirements for high-risk AI systems, despite their AI systemâ€™s open-source nature. The reference to Article 5 means that prohibited AI practices are prohibited, irrespective of whether or not they leverage open-source AI systems. Finally, providers and deployers of open-source AI systems which interact with individuals, generate synthetic content and deep fakes, or perform emotion recognition or biometric categorisation, must adhere to the transparency obligations outlined in Article 50.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 7/21\n\nArticle 53(2) refers to open-source AI models as â€œ models that are released under a free and open-source licence that allows for the access, usage, modification, and distribution of the model, and whose parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly availableâ€. \n\nProviders of open-source general-purpose AI models must adhere to a limited set of obligations, such as publishing a summary of the modelâ€™s training data and implementing a copyright compliance policy. However, providers of these models do not have to adhere to the obligations to produce and make available technical documentation about their general-purpose AI model. Nor do they have to appoint an authorised representative in the EU if they are established in a third country. For providers of open-source general-purpose AI models with systemic risk, the full and extensive set of obligations for general-purpose AI models with systemic risk applies. This includes all the above obligations, as well as performing model evaluations, systemic risk assessment and mitigation, and ensuring adequate cybersecurity protection. In practice, this will mean that despite the broad exemptions for open-source AI, it is   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 8/21\n\nlikely that providers of some of the most advanced and widely used open-source AI models will have extensive compliance obligations. The AI Act regulates the following types of AI, with explicit and targeted provisions: prohibited AI practices; high-risk AI systems; general-purpose AI models; general-purpose AI models with systemic risk; and certain AI systems which require transparency. The AI Act does not explicitly refer to â€˜high-risk AI modelsâ€™ as a regulated category, nor are there specific provisions relating directly to them. Moreover, there are no specific provisions relating to AI models which are not general-purpose AI models. This means that unless an AI model is general-purpose, or part of an AI system or practice which is high-risk, transparency requiring, or prohibited, it is not in scope of \n\nMisconception 3: High-risk AI models are explicitly regulated under the EU AI Act   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 9/21\n\nthe AI Act. However, in most scenarios, an AI model (or AI models) will be one of the most important components of a broader AI system, which could either be high-risk or used for a prohibited practice. Therefore, AI models are regulated in this â€˜indirectâ€™ but consequential sense.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 10/21\n\n1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1) \n\nhttps://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions  11/21 Image credit: This helpful infographic from the team at Digiphile highlights what is and what is not regulated under the AI Act. \n\nInterestingly, the AI Act does not include a legal definition of the term â€˜AI modelâ€™. Article 3 lists 68 different definitions, for terms like â€˜AI systemâ€™, â€˜training dataâ€™, and â€˜general-purpose AI modelâ€™ (see below). Despite there being a definition of an important and common type of AI model (i.e., a general-purpose one), there is no definition of an â€˜AI modelâ€™ itself, which is a broader and arguably more important and foundational concept. In my view, given the centrality of AI models to high-risk AI systems, general-purpose AI models, virtually any type of prohibited AI practice, and the wider field of AI, it would have been helpful for the AI Act to include an official legal definition of an AI model. For context, the AI Act defines an AI system as: â€œa machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environmentsâ€ .  \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 12/21\n\nA general-purpose AI model is defined as: â€œ an AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market â€. The concept of an AI model has been defined in other sources. \n\nNIST defines an AI model as: â€œ a component of an information system that implements AI technology and uses computational, statistical, or machine-learning techniques to produce outputs from a given set of inputs â€. In its paper on the updated definition of an AI system, the OECD refers to an AI model as â€œa core component of an AI system used to make inferences from inputs to produce outputsâ€.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 13/21\n\nImage credit: OECD visual on the definition of an AI system, which highlights that an AI model is an essential component of an AI system. \n\nArticle 5(1) stipulates that the following AI practice is prohibited : placing on the market, putting into service, or using â€œAI systems to infer emotions of a natural person in the areas of workplace and education institutions, except where the use of the AI system is intended to be put in place or into the market for medical or safety reasonsâ€. \n\nMisconception 4: Emotion recognition is prohibited under the EU AI Act   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 14/21\n\nThis means that the use of AI-enabled emotion recognition is only prohibited in specific, pre-defined contexts; namely, the workplace and educational institutions. It also means that AI-enabled emotion recognition could potentially be lawfully used in workplace and educational settings, if it can be demonstrated that this supports medical or safety objectives. For example, using AI to infer and predict the emotional state of a pilot while they are flying a plane, for the sole purpose of determining the pilotâ€™s future bonus or compensation package, would almost certainly be prohibited. However, if the AI system is used solely to initiate safety-critical interventions, which could prevent potentially harmful incidents, then this would likely be permitted under the AI Act. Furthermore, AI systems used for emotion recognition in settings other than the workplace and educational institutions are classified as high-risk, not prohibited. This is clarified in Annex III(1), which lists high-risk AI systems and includes â€œ AI systems intended to be used for emotion recognition â€. This means that providers and deployers developing, making available, or using emotion recognition systems in contexts other than the workplace and educational institutions, as well as emotion recognition systems for safety or medical reasons in the two aforementioned settings, must adhere to the obligations and requirements for high-risk AI systems.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 15/21\n\nTo understand exactly what constitutes an emotion recognition system (that could either be high-risk or prohibited, depending on the context), we need to consult the definition provided in Article 3(39): â€œâ€˜ an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data â€. In practice, this implies that unless it is based on biometric data (e.g., voice or facial expressions), using AI for emotional recognition would not be classified as high-risk or prohibited, and thus may not even be in scope of the AI Act. For example, if I copy and paste a colleagueâ€™s email into a generative AI tool, in advance of an important meeting, and ask it to predict their emotional state at the time of writing, this would most likely not be considered a prohibited AI practice entailing a hefty penalty, given the lack of biometric data being processed. However, if this was done via an AI assessing a video recording of a meeting in which their camera was on, the situation could be quite different. \n\nEuropean Commission guidelines on the topic have confirmed that an AI system inferring emotions from written text (i.e., content/sentiment analysis) is not prohibited as it does not constitute emotion recognition, because it is not based on biometric data.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 16/21\n\nSimilarly, there is no blanket prohibition on the development and use of facial recognition systems, which are already widely deployed in society. However, there are prohibitions on the ways in which law enforcement can use facial recognition and similar technologies to perform remote biometric identification of people in real-time. Also, if facial recognition technology was used for a prohibited AI practice, this would still be prohibited. However, this does not amount to a blanket prohibition. Concretely, law enforcement use of â€˜real-timeâ€™ remote biometric identification systems in public (e.g., facial recognition used to identify and stop flagged people in public) is prohibited, apart from in specific and narrowly defined scenarios. Acceptable scenarios for law enforcement use of such technology includes searching for victims of serious crime, preventing imminent threats to life (e.g., terrorist attacks), and locating suspects or perpetrators of serious crimes (e.g., murder). \n\nMisconception 5: Facial recognition is prohibited under the EU AI Act   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 17/21\n\nBefore AI is used in this way, independent judicial or administrative authorisation must be granted. Also, the use can only occur for a limited time period, with safeguards to protect privacy and fundamental rights. This became a totemic issue during the AI Act trilogue negotiations and legislative process. The European Parliament's initial AI Act proposals called for an outright ban on the use of real-time biometric identification systems in public, like facial recognition. This ban would have applied to law enforcement authorities and any other organisation, with no exceptions. The Council (EU member states) were never going to accept an outright prohibition and a compromise was brokered. Some types of AI-enabled facial recognition, which are permitted, would be classified as a high-risk AI system. Annex III(1) clarifies that â€˜remote biometric identification systemsâ€™, â€˜AI systems intended to be used for biometric categorisationâ€™, and â€˜AI systems intended to be used for emotion recognitionâ€™ are high-risk AI systems. It is also conceivable that a facial recognition system or component could be used as part of any other high-risk AI system listed in the AI Act. However, it is also conceivable that a facial recognition system or component could be   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 18/21\n\npart of an AI system which is not high-risk, nor in scope of the AI Act. This is because AI systems used for â€˜biometric verificationâ€™, which perform the sole function of confirming or authenticating that an individual is who they claim to be, are not high-risk. A facial recognition system used to unlock your phone would not be classified as high-risk, as it is merely confirming that you are who you claim to be (i.e., the owner of the phone). This is in contrast to live facial recognition cameras used by police to identify potential suspects from a crowd, as their facial data is being compared to a larger reference database of faces, with the goal of establishing a match. Finally, as per Article 5(e), it is prohibited to use AI systems to conduct untargeted scraping of facial images, from the internet or CCTV footage, with the goal of creating or expanding databases which are used to develop or operate facial recognition systems. However, this does not prohibit the use of facial recognition in a broader sense, merely a specific data scraping practice. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.", "fetched_at_utc": "2026-02-08T18:50:21Z", "sha256": "70e65b20e395a424a8e0ab0583f0112f8d14da8c1436ab726c56614d5cc47e26", "meta": {"file_name": "Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel.pdf", "file_size": 1601603, "relative_path": "pdfs\\Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4392}}}
{"doc_id": "pdf-pdfs-debunking-10-common-eu-ai-act-misconceptions-part-2-oliver-patel-f6590c49a8c2", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel.pdf", "title": "Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2) \n\nhttps://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070  2/23 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! This two-part series on Enterprise AI Governance presents and debunks 10 common misconceptions about the AI Act, providing a detailed explanation for each one. The first 5 were covered in part 1 and the second 5 are covered in this edition. \n\nThe ten misconceptions are: \n\n1. The EU AI Act has a two-year grace period and applies in full from August 2026. \n\n2. All open-source AI systems and models are exempt from the EU AI Act. \n\n3. High-risk AI models are explicitly regulated under the EU AI Act. \n\n4. Emotion recognition is prohibited under the EU AI Act. \n\n5. Facial recognition is prohibited under the EU AI Act. \n\n6. Transparency is required for â€˜limited riskâ€™ AI systems.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 3/23\n\n7. Third-party conformity assessments are required for all high-risk AI systems. \n\n8. Fundamental rights impact assessments are required for all high-risk AI systems. \n\n9. All high-risk AI systems must be registered in the public EU-wide database. \n\n10. Deployers do not need to register their use of high-risk AI systems. \n\nðŸ§‘â€ðŸŽ“ If you want to dive much deeper, register interest for my EU AI Act Compliance Bootcamp here . This will be an exclusive and intimate masterclass for AI governance leaders, breaking down how to implement AI Act compliance in an enterprise setting. More information will be shared later in the year. Note: if you already registered interest for my AI Usage Policy Bootcamp, you do not need to register here again. Stay tuned for further info. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nMisconception 6. Transparency is required for â€˜limited riskâ€™ AI systems.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 4/23\n\nThis is an important example, because the AI Act does not use the term â€˜limited riskâ€™ to refer to the AI systems for which transparency is required. Therefore, it does not make sense to use this term. It would be like using completely different and inappropriate terminology when referring to â€˜high-risk AI systemsâ€™ or â€˜prohibited AI practicesâ€™. What the AI Act actually says is that for certain AI systems, there are transparency obligations for providers and deployers. Therefore, more accurate terminology than â€˜limited riskâ€™ is â€˜transparency requiring AI systemsâ€™. It may not roll off the tongue as smoothly, but it does the job. All the AI Act risk pyramid images which are widely circulated (including sometimes by EU departments), which use the term â€˜limited riskâ€™ instead of â€˜transparency requiringâ€™ (or something of that nature) are arguably fuelling this misconception. This misconception has taken hold due to a lack of precision regarding the language which is used to talk about the AI Act. This is problematic, because in a complex legal text like this, there are many different terms which, although they may sound conceptually similar, mean different things and can give rise to very different real-world consequences. To accurately and effectively understand, interpret, and comply with a law of this complexity, precision and care with language is key.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 5/23\n\nArticle 50 outlines the transparency obligations for certain AI systems. The AI systems are: AI systems intended to interact directly with people; AI systems that generate synthetic image, audio, video or text content; emotion recognition systems; biometric identification systems; and AI systems which generate or manipulate deep fake content. Providers and deployers of these AI systems are obliged to implement additional transparency measures, such as notification and disclosure, which are detailed in Article 50. These â€˜transparency requiring AI systemsâ€™ can simultaneously be high-risk AI systems, or AI systems which are not high-risk. In the former scenario, all applicable obligations and requirements for high-risk AI systems would also apply. In the latter scenario, only the transparency obligations in Article 50 would apply. This is another reason why the â€˜classicâ€™ yet misleading AI Act risk pyramid doesnâ€™t   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 6/23\n\nwork: it falsely implies that the obligations for high-risk AI systems and â€˜transparency requiring AI systemsâ€™ are mutually exclusive. This point has been observed and elaborated on by other analysts, such as Aleksandr Tiulkanov , who has produced two helpful infographics to help conclusively debunk this misconception. \n\nSee below for the two original images, which have not been modified, and were posted on Aleksandrâ€™s article â€˜ EU AI Act: Getting the Basics Straightâ€™ .  \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 7/23\n\n1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2) \n\nhttps://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070  8/23 Misconception 7: Third-party conformity assessments are required for all high-risk AI systems \n\n1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2) \n\nhttps://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070  9/23 Under the AI Act, providers are obliged to ensure that their high-risk AI systems undergo a conformity assessment before they are placed on the market or put into service. A conformity assessment is an established and longstanding practice for many products regulated by EU product safety laws. The AI Act defines a â€˜conformity assessmentâ€™ as â€œ the process of demonstrating whether the requirements set out in Chapter III, Section 2 relating to a high-risk AI system have been fulfilledâ€. This means the focus of the conformity assessment is largely on the following requirements: risk management system, data and data governance; technical documentation; record-keeping; transparency and provision of information to deployers; human oversight; and accuracy, robustness and cybersecurity.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 10/23\n\nOnce the conformity assessment is completed, the provider must produce the â€˜declaration of conformityâ€™, which can be inspected by other parties, like importers or distributors. However, not all high-risk AI systems are obliged to undergo the same conformity assessment procedure. The key distinction is between the two main categories of high-risk AI system: \n\n1. AI systems which are products, or safety components of products, regulated by one of the EU product safety laws listed in Annex I; and \n\n2. AI systems listed in Annex III. For the first category of high-risk AI system, the requirement to undergo a third-party conformity assessment is already stipulated in the existing laws which are referenced in Annex I. Moreover, the AI systems covered by these existing laws are not classified as high-risk under the AI Act unless the relevant product is already required to undergo a third-party conformity assessment. To avoid burdensome and duplicative compliance work, the AI Act does not create a new and additional conformity assessment regime for these AI systems. What it says is that the existing conformity assessment procedure, performed by a third-party (i.e., a notified body) must continue to be followed, but that it must also   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 11/23\n\nnow consider and include the new requirements outlined in the AI Act. This necessarily entails an update to, and augmentation of, those existing third-party conformity assessment procedures. However, for the second category of high-risk AI systems (i.e., those listed in Annex III), there is no obligation for a third-party conformity assessment. Rather, providers must perform a â€˜self-assessmentâ€™, which does not involve a third-party (i.e., a notified body). Article 43(2) refers to this as the â€œ conformity assessment procedure based on internal control â€. Annex VI provides further information about how providers must perform this self-assessment. EU legislators opted for this approach due to the lack of AI certification expertise and maturity in the wider market, which was deemed an impediment to notified bodies performing conformity assessments across all these domains, at least for now. However, Recital 125 suggests that in future, as the market matures, these conformity assessments may also be performed by notified bodies. The only partial exception to this is high-risk AI systems used for biometrics, including: remote biometric identification; biometric categorisation; and   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 12/23\n\nemotion recognition. In certain scenarios, a third-party conformity is required for these high-risk AI systems. For example, if the provider has not fully applied an official technical standard to demonstrate compliance, or if such a standard does not exist, then the conformity assessment must be performed by a notified body. The procedure for this is outlined in Annex VII. Performing a fundamental rights impact assessment is an important obligation which applies to certain deployers of specific high-risk AI systems. Where applicable, a fundamental rights impact assessment must be performed by the deployer prior to using the AI system. The fundamental rights impact assessment is a formal exercise where deployers must consider, describe, and document how and when they will use the AI system, which people or groups will be impacted by it, the specific harms likely to impact those \n\nMisconception 8: Fundamental rights impact assessments are required for all high-risk AI systems   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 13/23\n\npeople, how human oversight will be implemented, and what will be done by the deployer if any risks materialise or harm arises. It does not apply to providers, nor does it apply to all deployers or all high-risk AI systems. The obligation only applies to the following deployers: bodies governed by public law; private entities providing public services; deployers of AI systems used for life and health insurance risk assessment and pricing; and deployers of AI systems used for creditworthiness evaluation and credit score assessment. For the deployers which are governed by public law, or private entities providing public services, they must only perform a fundamental rights impact assessment before using one of the high-risk AI systems in Annex III. However, this excludes AI systems used for safety management and operation of critical infrastructure, for which no fundamental rights impact assessment is required.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 14/23\n\nIn practice, this means that most businesses will not have to perform a fundamental rights impact assessment prior to using a high-risk AI system. This is either because it does not apply to them as a deployer, or it does not apply to the high-risk AI system they are using, or both. However, for some financial services and insurance firms, as well as companies which provide AI-driven public services in domains like welfare, education, and border control, this will become an important part of their AI governance and compliance work. Once the fundamental rights impact assessment has been completed, the deployer must notify the relevant regulator and, if applicable, summarise its findings in their registration entry in the EU database for high-risk AI systems ( see misconception 10 below ). Whilst providers of all high-risk AI systems are required to perform risk assessments and implement risk management measures (see Article 9)â€”which includes considering the potential risk to fundamental rightsâ€”this is not the same as a dedicated fundamental rights impact assessment. \n\nMisconception 9: All high-risk AI systems must be registered in the public EU-wide database   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 15/23\n\nArticle 71 of the AI Act mandates the European Commission to establish and maintain an EU database for high-risk AI systems. It must be â€œ publicly accessible, free of charge, and easy to navigate â€. Providers of certain high-risk AI systems are obliged to register information about their AI systems and their organisation in this database. They must do this before placing the high-risk AI system on the market or putting it into service. To dispel this misconception, there are four important points you should understand, each of which are explained below: \n\n1. The registration obligation does not apply to all high-risk AI systems. \n\n2. The registration obligation applies to some AI systems which are not technically high-risk. \n\n3. Not all information in the database will be available to the public. \n\n4. Some high-risk AI systems must be registered in a national database, instead of the EU database. \n\n1. The registration obligation does not apply to all high-risk AI systems   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 16/23\n\nThe registration obligation, outlined in Article 49, only applies to high-risk AI systems listed in Annex III. This includes AI systems used for recruitment, health insurance pricing, and educational admissions. However, this excludes AI systems which are products, or safety components of products, regulated by one of the EU safety laws listed in Annex I. This means that AI systems used in critical, regulated domains, like medical devices, vehicles, and aviation do not need to be registered in the EUâ€™s public database. There are various other ways in which the AI Act compliance obligations and requirements differ across these two main categories of high-risk AI system, such as with respect to conformity assessments ( as outlined in misconception 7 above ). \n\n2. The registration obligation applies to some AI systems which are not technically high-risk \n\nArticle 6(3) specifies an important caveat for the classification of high-risk AI systems. It states that high-risk AI systems listed in Annex III are not high-risk if they do not â€œpose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing the outcome of decision making â€. Four potential conditions are provided. If one of these is met, then the AI system does   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 17/23\n\nnot pose this type of risk and is thus not classified as high-risk. For example, one condition is that the AI system performs only a â€œ narrow procedural task â€. Interestingly, even where a provider legitimately determinesâ€”as per the Article 6(3) exception procedureâ€”that their AI system is not high-risk, they must still register that AI system in the EUâ€™s public database. The logic of this is to promote transparency regarding how providers are determining and documenting that AI systems are not high-risk, despite being used in sensitive (Annex III) domains. This could lead to a potentially vast number of AI systems needing to be registered and many organisations being unaware that they are obliged to do so. \n\n3. Not all information in the database will be available to the public \n\nFor certain high-risk AI systems listed in Annex III, including AI systems used for law enforcement, migration, asylum, and border control management, the information will be registered and stored in a â€œ secure non-public section â€ of the EU database. This includes lawful AI systems used for remote biometric identification, biometric categorisation, and emotion recognition, in the context of law enforcement, migration, asylum, and border control management.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 18/23\n\nThese providers are obliged to register less information than the providers of the other high-risk AI systems, and that information can only be viewed by the European Commission and the specific member state regulators who have been designated to lead on AI Act enforcement for those sensitive sectors. Therefore, many of the AI systems which are the most sensitive, and arguably pose the greatest risk to fundamental rights, will not be publicly registered. \n\n4. Some high-risk AI systems must be registered in a national database, instead of the EU database. \n\nProviders of high-risk AI systems that are used as safety components for the operation and management of critical infrastructure, like water and energy, are obliged to register their AI systems, and themselves, at the â€œnational levelâ€. This means that the registration of these AI systems will be in different databasesâ€” maintained by member state regulators and/or governmentsâ€”separate from the public database maintained by the European Commission. The AI Act does not reveal much about these national level databases. However, there is no provision which states that they must be public. This signals an acknowledgement of the sensitivity and secrecy of these domains, because of their importance to national security and economic stability.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 19/23\n\nCertain deployers of specific high-risk AI systems are also obliged to register information about their organisation, and the high-risk AI system they are using, in the EU database. They must do this before using the AI system. This obligation only applies to deployers that are â€œ public authorities, EU institutions, bodies, offices or agencies or persons acting on their behalf â€. The focus is therefore on public sector organisations in the EU using high-risk AI, as opposed to private sector and commercial AI usage. These organisations must register themselves, and select which high-risk AI system they are using, in the EU database. It will not be possible to do this if the provider has not already registered the high-risk AI system. Therefore, this is something which should be checked by the deployer, as part of procurement and partnership due diligence. This also means that the deployer registration obligation only applies to the high-risk AI systems which providers are obliged to register. This includes the high-risk AI \n\nMisconception 10: Deployers do not need to register their use of high-risk AI systems   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 20/23\n\nsystems used for law enforcement, migration, asylum and border control management, however the deployer information will also be registered the â€œ secure non-public section â€ of the EU database, which only the European Commission and certain public authorities can access. There is no explicit provision which stipulates that deployers must register their use of AI systems used for critical infrastructure safety management and operation, which providers must register at the national level instead of the EU-wide database. There is also nothing which indicates that deployers are obliged to register their use of AI systems which are not high-risk, but have nonetheless been registered as they fell under the scope of an Article 6(3) derogation. Finally, there are certain scenarios when deployers can become providers of high-risk AI systems, either intentionally or inadvertently. For example, if they make a â€œsubstantial modificationâ€ to the AI system and it remains high-risk. In these scenarios, the new provider would be required to register the high-risk AI system. \n\nðŸ§‘â€ðŸŽ“ If you found this post useful, register interest for my EU AI Act Compliance Bootcamp here . This will be an exclusive and intimate masterclass for AI governance leaders, breaking down how to implement AI Act compliance in an enterprise setting.", "fetched_at_utc": "2026-02-08T18:50:25Z", "sha256": "f6590c49a8c26344df407e9f9ebc458e9630517cf28bb66045609149b1361557", "meta": {"file_name": "Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel.pdf", "file_size": 1467187, "relative_path": "pdfs\\Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4772}}}
{"doc_id": "pdf-pdfs-eu-ai-act-simplification-oliver-patel-597a6bfd3c39", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\EU AI Act simplification - Oliver Patel.pdf", "title": "EU AI Act simplification - Oliver Patel", "text": "Hey ðŸ‘‹ \n\n1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"? \n\nhttps://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification  2/16 Iâ€™m Oliver Patel , author and creator of Enterprise AI Governance and author of the forthcoming book, Fundamentals of AI Governance (2026). \n\nOn Wednesday 19 November 2025, the European Commission will publish its Digital Omnibus Packageâ€”a sweeping proposal to reform the EUâ€™s digital legislative framework. The EU AI Act is in sharp focus, alongside the GDPR and a host of other flagship EU laws. This article explains how we got here, whatâ€™s at stake, and the potential changes being discussed in Brussels. It will be followed by in-depth analysis of the proposed changes, once they are published. \n\nThe EU has consistently sought to shape global regulatory standards on digital technology, data, and AI. The EU has been open about its policy objectives of protecting citizens and promoting global regulatory convergence towards its high standards, in domains like data protection and privacy. And in some ways, it has succeeded. With GDPR, for example, there has been a demonstrable â€œBrussels effectâ€. Many countries worldwide have enacted data protection laws of a similar flavour and \n\nHow did we get here?   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 3/16\n\ncorporations have prioritised aligning their internal frameworks accordingly. Despite the lack of U.S. federal privacy law (which renders the U.S. a global outlier), almost all major American enterprises are significantly impacted by the GDPR and have implemented dedicated privacy compliance programmes. However, as EU laws covering digital governance have proliferated, the picture has become increasingly complex and convoluted. Keeping track of the EUâ€™s regulations and directives covering the digital sphere is no mean feat. CEPS and Kai Zennerâ€™s dataset of EU digital sector legislation , updated in July 2025, lists 101 different laws , including the EU AI Act and the GDPR. The authors remark that there has been an â€œ absolute explosion â€ of EU digital laws and that â€œeven the best experts struggle to keep up with this torrent of legal and policy instrumentsâ€. To complement the 101 laws, there exists a far greater number of regulatory bodies covering digital issues. Enterprises require large teams of people to make sense of these developments and determine how to be compliant, not least because obtaining deep expertise in just one legislative area takes years. In response, enterprises have also established a range of somewhat separate yet overlapping digital governance capabilities to address key legal obligations and requirements. This includes implementing enterprise AI governance, which has become a core priority in recent years due to both the EU AI   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 4/16\n\nAct and the surge in AI adoption. But you didnâ€™t need me to tell you thatâ€¦ For AI governance professionals, this regulatory complexity is especially acute, as AI systems frequently trigger multiple regulatory requirements simultaneouslyâ€”from GDPRâ€™s rules on automated decision-making and use of sensitive personal data, to the AI Actâ€™s requirements for high-risk and transparency-requiring AI systems, cybersecurity mandates, copyright and intellectual property, and existing sectoral laws. In recent months, discussion regarding simplification of the EUâ€™s digital rulebook has intensifiedâ€”both within the EU and externally. It is widely reported that EU officials and member states have faced sustained lobbying from U.S. industry and the Trump administration. There are even reports of President Trump considering imposing tariffs and sanctions on the EU and its officials, due to the perceived impact of EU digital laws on U.S. companies and citizens. This pressure has been accompanied by increasingly vocal calls from segments of European industry that EU digital regulations need to be reformed, streamlined, simplified, to reduce compliance burdens. For example, in July 2025, dozens of companiesâ€”including industry heavyweights like Mercedes Benz, Deutsche Bank, and Lâ€™Orealâ€” signed an open letter to the EU urging for a two-year delay to the EU AI Actâ€™s key provisions on general-purpose AI (GPAI) models and high-risk AI systems.   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 5/16\n\nThis is coming to a head on Wednesday 19 November, as the European Commission is set to publish its â€œDigital Omnibus Packageâ€ proposal. This will outline, for the first time, the comprehensive set of legislative amendments the Commission proposes, to simplify the EUâ€™s digital rulebook. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nAlthough this situation has evolved over several years, the September 2024 publication of the â€œDraghi reportâ€ on The Future of European Competitiveness was an important milestone in this story. Mario Draghi is the former president of the European Central Bank and former prime minister of Italy. This independent report, commissioned by the EU, is shaping aspects of the European Commissionâ€™s policy agenda. The report presents the EUâ€™s economic challenges, focusing on diminished competitiveness, weakening productivity, and slowing growth. Draghi frames the economic outlook as an â€œexistential challengeâ€ for the EU, arguing that it will be unable \n\nWhat was the Draghi report?   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 6/16\n\nto finance its social model, achieve its environmental ambitions, and deliver the prosperous and fair society that represents its raison dâ€™Ãªtre if it fails to make radical changes. The Draghi report outlines three action areas to â€œreignite growthâ€. These are: \n\n1. Closing the innovation gap with the U.S. and China, especially in advanced technologies. \n\n2. Forging a joint plan for decarbonisation and competitiveness. \n\n3. Increasing security and reducing dependencies. It is the first action area which is most relevant for our focusâ€”EU AI Act simplification. The Draghi report argues that the EUâ€™s complex digital regulatory environment impedes innovation and growth. It elevates â€œreducing the regulatory burdenâ€ as a core priority for transforming the EUâ€™s economic prospects. Examining the â€œinnovation gapâ€ between the U.S. and China on the one hand and the EU on the other, the report claims that innovative European companies attempting to scale up are â€œhindered at every stage by inconsistent and restrictive regulationsâ€. This is why, the report argues, European tech entrepreneurs routinely seek to grow their businesses in the U.S. The report also cites that 55% of SMEs flag â€œregulatory obstacles and administrative burdenâ€ as their greatest challenge, arguing that these â€œregulatory   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 7/16\n\nburdensâ€ are particularly damaging for digital sector SMEs. The report criticises the â€œprecautionary approachâ€ taken by EU digital laws, including the EU AI Act. It specifically calls out the compute threshold for determining whether a general-purpose AI (GPAI) model poses â€œsystemic riskâ€, noting that various frontier AI models already exceed the threshold, despite the EU AI Actâ€™s nascency. It also highlights the increasing difficulty that companies face in navigating the various overlapping laws relevant for AI and the hundreds of regulatory bodies across the EU responsible for digital governance. It is important to note that a vast array of other impediments and challenges are highlightedâ€”from inadequate research talent pipelines to low investment in innovation commercialisationâ€”as contributing factors to the EUâ€™s competitiveness challenge. Therefore, my intention is not to claim that the Draghi report is all about digital â€œregulatory burdensâ€, as that would be misleading. It would be equally misleading to claim the Digital Omnibus Package results solely from the Draghi report. However, the complexity of the EUâ€™s digital legislative framework has undeniably become a totemic issue, which the Draghi report shone a very bright light on. Next, we turn our attention to the European Commissionâ€™s Digital Omnibus Package. Will it deliver the changes that Draghi seeks? And what could this mean for the EU AI Act?   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 8/16\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nThe Digital Omnibus Package is the European Commissionâ€™s much anticipated proposal to reform the EUâ€™s digital legislative framework. It is due to be announced and published on Wednesday 19 November. This follows public consultations earlier in the year. The proposal will feature meaningful amendments to some of the EUâ€™s flagship laws and perhaps even the repeal of specific instruments. The Digital Omnibus Package is expected to focus on the EU AI Act, the GDPR, the ePrivacy Directive, the Data Act, and the NIS2 Directiveâ€”horizontally cutting across the core digital governance domains of AI, data protection and privacy, and cyber security. The purpose of the Digital Omnibus Package is to simplify and streamline the EUâ€™s digital legislative framework, to ease compliance burdens and cut costs for organisations (particularly startups and SMEs), promote the growth and \n\nWhat is the EUâ€™s Digital Omnibus Package?   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 9/16\n\ncompetitiveness of European companies, and boost innovation. It is inevitable that the core threads from the Draghi report will be woven into the European Commissionâ€™s proposal. Before engaging in any further analysis of the imminent proposal, several caveats are required. First, the current discussion, including this article, is based on leaked documents, media reporting, speculation, and rumours. Brussels is a famously leaky city, so this is nothing new. However, until the European Commission officially publishes its proposal, we do not know exactly what the proposal consists of. At the time of writing, nothing official has been published. Second, the Digital Omnibus Package that will be published later this week merely represents the European Commissionâ€™s initial proposal on this controversial set of issues. Therefore, even when we have the proposal, all we will have is the official starting position of one of the EUâ€™s institutions. To amend EU laws in this way will require extensive trilogue negotiations and approval from the European Parliament and EU member states via the Council of the EU (the Council). Third, the aforementioned trilogue negotiationsâ€”and the process of amending and repealing a suite of EU laws in this wayâ€”are bound to be lengthy, complex, and   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 10/16\n\nfraught with drama. Although the precise legislative instrument to operationalise the Digital Omnibus Package is yet to be confirmed, it is most likely to be an EU regulation (which is the same legal instrument as laws like the GDPR and the EU AI Act). To amend existing EU laws via a new regulation, the EUâ€™s â€œ ordinary legislative procedureâ€ must be followed. This necessitates dual approval from both the European Parliament and the Council, following trilogue negotiations where both institutions have several opportunities to amend and update the legislative proposal. On average, it takes the EU 19 months to agree new laws, from the initial Commission proposal to formal adoption. Plainly speaking, what all this means is that: We donâ€™t yet have an official proposal from the European Commission, merely leaks and media speculation. When we do, it will be subject to lengthy and fraught trilogue negotiations, the outcome of which is impossible to predict. However, what can be predicted with a degree of certainty is that there will be a substantial difference between the European Commissionâ€™s initial legislative proposal and the final text that is voted on.   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 11/16\n\nFollowing this, formal approval will be required from both the European Parliament and the Council to pass any regulation that meaningfully amends existing EU laws. It is impossible to predict what the final legislative text will consist of and how long the negotiation and approval process will take. Finally, there is no guarantee that any legislative changes will be approvedâ€” although this does seem unlikely given the various points made above. Caveats aside, the final part of this article will highlight some of the potential changes that are reported to be on the cards for the EU AI Act. Although based primarily on leaks and tip-offs, recent media reporting nonetheless shines a light on what is being discussed in the EUâ€™s corridors of power. Below is a list of the various potential EU AI Act changes that have been reported. A special shout out for the reporting from MLexâ€™s Luca Bertuzzi (who is a must follow for AI governance practitioners), which this list is largely based on: \n\nHow could the EU AI Act change?   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 12/16\n\nDelaying the applicable date for the obligations for providers and deployers of transparency-requiring AI systems. Delaying the applicable date for the obligations for providers and deployers of high-risk AI systems. These obligations apply from 2 August 2026, but this enforcement could reportedly be delayed for one year. This is partly due to delays in finalising technical standards that organisations can use to comply with these provisions. Scrapping the AI literacy obligation for organisations and shifting it to governments, regulators, and EU institutions. Centralising enforcement powers with the European Commissionâ€™s AI Office. This could be done by designating the AI Office as the regulatory authority responsible for supervising AI systems based on GPAI models. This is partly driven by concerns regarding readiness and capacity of EU member state regulators (some of which have not yet been designated), as well as the complexity firms could face in engaging with many different regulatory bodies. Introducing smaller and more proportionate compliance penalties for â€˜small mid-capsâ€™ (defined as companies that employ up to 750 people and have an annual turnover of under â‚¬150 million).   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 13/16\n\nThis would complement and expand the scope of the existing proportionality for compliance penalties for SMEs. Removing the obligation for providers to register, in the EUâ€™s public database, AI systems that are used in a high-risk domain which they have deemed and demonstrated are not high-risk due to the nature of the use. It will be interesting to see which (if any) of these potential changes survive in the Digital Omnibus Package that the European Commission publishes on Wednesday. Given the speculative nature of this, I will not provide any further analysis on these potential changes in this article. One final point to note is that the European Commission does have powers to amend or change aspects of the EU AI Act, via instruments called delegated acts and implementing acts. For example, the European Commission can modify the compute threshold for GPAI models with systemic risk via a delegated act. Where these powers exist, the European Commission is able to drive changes without the need for the ordinary legislative procedure and formal approval from the European Parliament and Council. Although, there is always a degree of oversight from these institutions, who retain veto powers. However, the EU AI Act â€œsimplificationâ€ changes discussed above extend far beyond the scope of the Commissionâ€™s powers to drive changes via delegated and implementing acts.", "fetched_at_utc": "2026-02-08T18:50:31Z", "sha256": "597a6bfd3c390cadfeded04f8589f4aedfe4d341d7d76aca6c51a25f008e7e03", "meta": {"file_name": "EU AI Act simplification - Oliver Patel.pdf", "file_size": 682333, "relative_path": "pdfs\\EU AI Act simplification - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 3502}}}
{"doc_id": "pdf-pdfs-european-union-artificial-intelligence-act-bird-bird-d085ee7a46ea", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\European Union Artificial Intelligence Act - Bird & Bird.pdf", "title": "European Union Artificial Intelligence Act - Bird & Bird", "text": "European Union Artificial Intelligence Act: a guide \n\n# 7 April 2025 21 2 3 4 5 6 7 8 9 10 \n\n# Contents \n\nOVERVIEW, KEY CONCEPTS & TIMING OF IMPLEMENTATION \n\nOverview Key concepts Timeline \n\nGENERAL-PURPOSE AI MODELS \n\nBackground and relevance of general-purpose AI models of personal data Terminology and general-purpose AI value chain Obligations for providers of general-purpose AI models General-purpose AI models with systemic risk \n\nTRANSPARENCY OBLIGATIONS \n\nGeneral transparency obligations Transparency obligations for high-risk AI systems Timing and format Transparency obligations at the national level and codes of practice Relationship with other regulatory frameworks \n\nREGULATORY SANDBOXES \n\nAI regulatory sandboxes Real-world testing of AI systems \n\nENFORCEMENT & GOVERNANCE \n\nOverview Post-marking obligations Market surveillance authorities Procedures for enforcement Authorities protecting fundamental rights General-purpose AI models Penalties Remedies for third parties Governance \n\nAI ACT: WHATâ€™S NEXT \n\nAI Act application deadlines Delegated Acts Implementing Acts Commission Guidelines Codes of conduct and practice Standards Liability \n\nOUR GLOBAL CONTRIBUTORS MATERIAL AND TERRITORIAL SCOPE \n\nMaterial scope Territorial scope Exclusions Relationship with other regulatory frameworks \n\nPROHIBITED AI PRACTICES \n\nProhibited AI practices To whom do the prohibitions apply? Enforcement and Fines \n\nHIGH-RISK AI SYSTEMS \n\nClassification of an AI system as a high-risk AI system Obligations for providers of high-risk AI systems Harmonised standards and conformity assessment procedure for providers of high-risk AI systems Obligations for deployers of high-risk AI systems Obligations for other parties in connection with high-risk AI systems 15678910 234\n\nRanked Tier 1 \n\n# Legal 500 for Artificial Intelligence Distinguished for our client satisfaction 31 2 3 4 5 6 7 8 9 10 Overview, key concepts & timing of implementation \n\n# Overview \n\nThe European Union (EU) stands as a pioneer \n\nin the regulation of artificial intelligence (AI), setting a global benchmark with its proactive approach to ensuring ethical and responsible AI development. Indeed, it seems we may witness a new Brussels effect, reminiscent of the influence wielded by the GDPR. The EUâ€™s comprehensive and precautionary framework prioritises transparency, accountability, and human rights. The AI Act applies beyond the borders of the EU - many of its provisions apply regardless of whether the providers are established or located within the EU or in a third country. The AI Act applies to any provider or entity responsible for deploying an AI system if â€œ the output produced by the system is intended to be used â€ in the EU. Foreign suppliers must appoint an authorised representative in the Union to ensure compliance with the Actâ€™s provisions. However, the AI Act does not apply to public authorities of third countries or to international organisations under police and judicial cooperation agreements with the Union, nor to AI systems placed on the market for military defence or national security purposes. This broad scope aims to ensure comprehensive regulation of AI systems and their uses. \n\nWhat you can expect from this guide \n\nâ€¢ This chapter provides an overview of the whole AI Act, its key concepts and the dates from when its provisions will apply. \n\nâ€¢ Chapter 2 looks at the territorial and material scope of the AI Act. \n\nâ€¢ Chapters 3, 4, 5 and 6 address the \n\nrequirements the AI Act imposes on different types of AI - prohibited practices; high risk systems; general purpose AI; and AI where greater transparency is needed. \n\nâ€¢ Chapter 7 explains the AI Actâ€™s arrangements for testing AI in regulatory sandboxes. Chapter 8 looks at governance and enforcement. \n\nâ€¢ Chapter 9 summarises the numerous further measures that have to follow the adoption of the AI Act. \n\nâ€¢ Last, Chapter 10 includes all the contributors to this guide. \n\nA risk-focused approach \n\nThe EU approach to AI regulation is characterised by its risk-based framework. This regulation adopts a technology-neutral perspective, categorising AI systems based on their risk level, ranging from minimal to high risk. This system ensures that higher-risk AI applications, particularly those that can significantly impact fundamental rights, are either prohibited or subjected to stricter requirements and oversight. The EU places a strong emphasis on promoting the development and use of responsible AI. The AI Act mandates strict measures for data security and user privacy, ensuring that AI systems are designed and deployed with these considerations at the forefront. This includes rigorous requirements for how data is handled and protected, ensuring that usersâ€™ personal information remains secure. Additionally, the AI Act requires comprehensive risk assessments for AI systems. These assessments help identify and mitigate potential risks associated with AI technologies, fostering transparency and accountability among AI providers. By making these evaluations mandatory, the EU ensures that AI developers thoroughly understand and address the implications of their technologies. This proactive approach aims to build public trust in AI technologies by protecting usersâ€™ rights and well-being. By prioritising data security, privacy, and risk management, the EU seeks to reassure the public that AI can be used safely and ethically. This focus on responsible development helps to promote broader acceptance and integration \n\nCHAPTER 1 41 2 3 4 5 6 7 8 9 10 \n\nof AI technologies, ultimately benefiting society as a whole. The AI Act has been developed not only to create laws for AI systems, but also to establish an ethical framework for their use, to ensure that organisations consider the impact of their AI systems on people, other businesses, the environment and many other aspects of our lives. \n\nEthics at the heart of the AI Act \n\nThe AI Act explicitly builds on the Ethical Guidelines on Trustworthy AI, which were published by the European Commission in 2019. While these guidelines remain non-binding, many of their principles have been directly incorporated into the AI Act. The best example of this approach is that in many of its provisions, the AI Act refers directly to the fundamental rights enshrined in the Charter of Fundamental Rights of the European Union. For example, \n\nhigh-risk AI systems are those that have a significant harmful impact on the health, \n\nsafety and fundamental rights of persons \n\nin the Union. The proper application of the AI Act will in many cases require an analysis of the risks to fundamental rights, which includes both legal and ethical issues. It can therefore be said that ethics has been embedded into the AI Act. \n\nGovernance \n\nThe European Union adopts a decentralised supervision model, promoting collaboration with various national authorities. The AI Act establishes the European Artificial Intelligence Office (the AI Office) as an independent entity, serving as the central authority on AI expertise across the EU, and playing a crucial role in implementing of the legal framework. This office will encourage the development of trustworthy AI and support international collaboration. The European Artificial Intelligence Board will be composed of one representative per Member State and the European Data Protection Supervisor shall participate as observer. The AI Office aims to promote and facilitate the creation, review, and adaptation of codes of good practice, considering international approaches. To ensure these codes reflect the current state of the art and incorporate diverse perspectives, the AI Office will collaborate with relevant national authorities and may consult with civil society organisations, stakeholders, and experts, including scientific experts. \n\n# Key concepts \n\nAI systems (see also Chapter 2) \n\nMost of the AI Act applies to â€œAI systemsâ€ ,\n\nwhich the Act defines as â€œ a machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can \n\ninfluence physical or virtual environments â€. It is worth noting that the AI Act does not define \n\nâ€œartificial intelligenceâ€ , but only the term â€œartificial \n\nintelligence systemâ€ . The definition of an AI system is intentionally consistent with the OECD definition of an AI system. The definition does not mention any specific technology or currently known approaches to artificial intelligence systems. With the rapidly evolving nature of AI, this prevents the AI Act from becoming obsolete due to technological developments. A key element of this definition is the AI systemâ€™s ability to â€œinferâ€ . This should allow for a clear distinction between AI systems and traditional software. If a computer program operates according to rules defined in advance by the programmers, it is not an AI system; if a system is built using techniques that allow the program to create rules of its own based on input data or data sets provided to the program, then it is an AI system. The definition of an AI system is discussed further in guidelines published by the Commission on 6 February 2025. \n\nObligations across the supply chain \n\n(see also Chapter 2) \n\nThe AI Act applies to all participants in the supply chain, starting with the â€œproviderâ€ and also \n\nencompassing the â€œimporterâ€ , â€œdistributorâ€ and \n\nâ€œdeployerâ€ of the system. Most responsibilities lie with the provider, and next with the deployer. An importer, distributor or deployer may become a provider of the high-risk AI system if they have put their name or trademark on the system. They may become a provider of a high-risk system (see page 5) if they make substantial modifications to, or modify the intended purpose of the AI system, which renders the system high-risk. 51 2 3 4 5 6 7 8 9 10 \n\nRisk approach to classification of AI systems \n\nThe AI Act defines risk as â€œ the combination of the probability of harm occurring and the severity of that harm.â€ \n\nThe risk-based classification of AI systems is a fundamental aspect of the AI Act, focusing on the potential harm to health, safety, and fundamental human rights that an AI system may cause. This approach categorises AI systems into four distinct risk levels: 1.  Unacceptable risk: AI systems that pose such significant risks are unacceptable and therefore prohibited. 2.  High risk: High-risk AI systems are subject to stringent regulatory requirements. 3.  Limited risk: AI systems in this category pose a limited risk, but have specific transparency obligations. 4.  Minimal or no risk: AI systems that pose minimal or no risk have no regulatory restrictions under the AI Act. \n\nUnacceptable risk: prohibited practices \n\n(see also Chapter 3) \n\nThe AI Act contains a list of prohibited AI practices, which should be understood as a prohibition on placing on the market, putting into service, or using an AI system that employs any of these practices. The list prohibits: \n\nâ€¢ using subliminal techniques or purposefully manipulative or deceptive techniques to materially distort behaviour, leading to significant harm; \n\nâ€¢ exploiting vulnerabilities of an individual or group due to their specific characteristics, leading to significant harm; \n\nâ€¢ social scoring systems i.e. evaluating or classifying of an individual or group based on their social behaviour or personal characteristics, leading to detrimental or unfavourable treatment; \n\nâ€¢ evaluating a personâ€™s likelihood of committing a criminal offence, based solely on profiling or personal characteristics; except when used to support human assessment based on objective and verifiable facts linked to a criminal activity; \n\nâ€¢ facial recognition databases based on untargeted scraping from the internet or CCTV; \n\nâ€¢ inferring emotions in workplaces or educational institutions, except for medical \n\nor safety reasons; \n\nâ€¢ biometric categorisation systems that categorise a person based on their sensitive data, except for labelling or filtering lawfully acquired biometric datasets such as images in the area of law enforcement; \n\nâ€¢ real-time remote biometric identification systems in publicly available spaces for law enforcement purposes, except in narrowly defined circumstances. In some cases, the AI Act contains exceptions that allow these â€œprohibitedâ€ practices to be used in certain situations. A good example is real-time biometric identification, where the Regulation allows its use in exceptional circumstances. The application of these exceptions requires notifications or prior authorisations. The Commission published guidelines on prohibited AI practices on 4 February 2025. \n\nHigh-risk AI systems (see also Chapter 4) \n\nThe extensive regulation of high-risk AI systems constitutes a major part of the AI Act. AI systems are identified as high-risk AI systems if they have a significant harmful impact on the health, safety and fundamental rights of persons in the Union. There are two categories of high-risk AI systems which are regulated differently: \n\nâ€¢ AI systems intended to be used as a product or a safety component of a product which is covered by EU harmonisation legislation, such as civil aviation, vehicle security, marine equipment, radio equipment, toys, lifts, pressure equipment, medical devices, personal protective equipment (listed in Annex I to the AI Act). \n\nâ€¢ AI systems listed in Annex III, such as AI used in education, employment, credit scoring, law enforcement, migration, remote biometric identification systems, and AI systems used as a safety component in critical infrastructure. This list can be amended by the Commission. The first category of high-risk systems is covered by both the harmonisation legislation and the AI Act. 61 2 3 4 5 6 7 8 9 10 \n\nProviders have an option of integrating the requirements of the AI Act into the procedures required under the respective Union harmonisation legislation listed in Section A of Annex I. In addition, only selected provisions of the AI Act apply to high-risk AI systems in relation to products covered by Union harmonisation legislation listed in Section B of Annex I (such as aviation equipment). Practical assistance in the classification of high-risk AI systems will be provided no later than 2 February 2026 by the Commission, to include a comprehensive list of practical examples of use cases of high-risk and non-high-risk AI systems. \n\nExceptions to the qualification of high-risk \n\nAI system \n\nIf a high-risk AI system listed in Annex III does not pose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing the outcome of decision making, it will not be treated as a high-risk AI system. Such situations may only arise in four cases where the AI system is intended to: \n\nâ€¢ perform a narrow procedural task; \n\nâ€¢ improve the result of a previously completed human activity; \n\nâ€¢ detect decision-making patterns or deviations from prior decision-making patterns, and is not meant to replace or influence the previously completed human assessment without proper human review; or \n\nâ€¢ perform a preparatory task to an assessment relevant for the purposes of the use cases listed in Annex III. If, however, the AI system performs profiling of natural persons, it is always considered a high-risk AI system and cannot fall into one of the above exceptions. This exemption is likely to play an important role in practice, as it allows avoiding the obligations and costs associated with placing a high-risk AI system on the market. One of the options is, for example, to carve out those parts of an AI system that can take advantage of this exemption to limit the scope of the high-risk AI system. However, even if a provider relies on the exemption, its assessment of the system must be documented, and the system must still be registered in the EU database for high-risk systems before it is placed on the market or put into service. \n\nExtensive obligations for high-risk AI systems \n\nThe requirements that must be met by providers of high-risk AI systems are strict. These requirements include, in particular, the need to document every stage of the development of the AI system, to meet obligations regarding the use of high-quality data for training, to produce system documentation that provides users with full information about the nature and purpose of the system, or to ensure the accuracy, robustness and cybersecurity of the systems. High-risk AI systems will also have to be registered in an EU database, which will be publicly available. \n\nObligations across the supply chain of AI systems \n\nThe AI Act imposes obligations on all participants in the supply chain of a high-risk system throughout its life cycle. The responsibilities are not only those of the â€˜providerâ€™, but also those of the â€˜importerâ€™, â€˜distributorâ€™ and â€˜deployerâ€™ of the system, although most of the responsibilities lie with the provider and the deployer. The primary duty of the importer and distributor is to verify that the high-risk AI system being imported or distributed meets the requirements of the AI Act. Moreover, an importer, distributor or deployer may become a provider of the high-risk AI system if they have put their name or trademark on the system, made substantial modifications or they have modified the intended purpose of the AI system, which renders the system high-risk. \n\nGeneral-purpose AI models (see also Chapter 5) \n\nThe distinction between AI models and AI systems is crucial for the application of the AI Act. AI models are essential components of AI systems, but they do not constitute AI systems on their own. AI models require the addition of other components, such as a user interface, to become AI systems. The AI Act mostly regulates AI systems, not models. However, it does contain rules on general-purpose AI models. The AI Act provides rules for all general-purpose AI models and additional rules for general-purpose AI models that pose systemic risks. They apply in the following situations: 71 2 3 4 5 6 7 8 9 10 \n\nâ€¢ where the provider of a general-purpose AI model integrates its own model into its own AI system that is made available on the market or put into service; \n\nâ€¢ where the provider of a general-purpose AI model only offers its own model to providers of AI systems. The distinction may be particularly important in cases where a general-purpose AI model of one provider is used in a general-purpose AI system of a second provider, which in turn is integrated into another AI system with a more specific purpose, built by a third provider. \n\nTransparency obligations (see also Chapter 6) \n\nThe AI Act includes transparency obligations for four types of AI systems: \n\nâ€¢ AI systems designed to interact directly with natural persons; \n\nâ€¢ AI systems, including general-purpose AI systems, that generate synthetic audio, image, video or text content; \n\nâ€¢ emotion recognition or biometric categorisation systems; \n\nâ€¢ AI systems that generate or manipulate images, audio or video that are deepfakes. In all these cases, the user must be informed about the use of the AI system. There are also more detailed obligations, for example to mark the output in a machine-readable way so that it can be identified as artificially generated or manipulated. \n\nComplex supervision and enforcement structure (see also Chapter 8) \n\nThe AI Act provides for a complex, multi-level structure for overseeing implementation. It includes both national and EU level entities. At each level there will be several types of bodies, such as notifying authorities and notified bodies, conformity assessment bodies, the European AI Board, the AI Office, national competent authorities and market surveillance authorities. These authorities will not only control compliance, but also support the market by, among other things, developing codes of conduct, organising AI regulatory sandboxes and providing support for SMEs and start-ups. \n\nRole of technical standards, codes of practice and guidelines (see also Chapters 7, 8 and 9) \n\nThe AI Act requires providers of high-risk AI systems to affix a European Conformity (CE) marking. The CE marking will show compliance with the requirements of the AI Act. For the mark to be issued, providers will have to apply harmonised technical standards. In addition, high-risk AI systems or general-purpose AI models which are in conformity with harmonised standards shall be presumed to be in conformity with the requirements of the AI Act to the extent that those standards cover those requirements or obligations. Consequently, the rather general provisions of the AI Act will be complemented by technical standards that will provide the concrete forms of compliance with the AI Act. Thus, we can expect that the CE marking and technical standards will play very important role in practical application of the AI Act. Codes of practice should also form an important role. If they are not prepared by market participants, the Commission may provide the common rules within implementing acts. The Commission can also, by way of an implementing act, approve a code of practice and give it a general validity within the Union. In addition, the Commission has the obligation to develop several guidelines on the practical implementation of the Regulation. The AI Act can therefore be seen as just a framework for more detailed obligation that will result from many further documents and legal acts. \n\nEnforcement (see also Chapter 8) \n\nThe AI Act stipulates significant penalties \n\nfor non-compliance, which vary depending \n\non the nature of the violation and the size \n\nof the entity involved. Actions that may incur high penalties include: \n\nâ€¢ non-compliance with the rules on prohibited AI practices outlined in article 5. Offenders in such cases may face administrative fines of up to â‚¬35,000,000 or up to 7% of annual worldwide turnover, whichever is higher, for undertakings. 81 2 3 4 5 6 7 8 9 10 \n\nâ€¢ violations related to data, data governance, and transparency: AI systems found in breach of these provisions could be fined up to â‚¬20 million or 4% of annual global turnover. \n\nâ€¢ failure to comply with any of the provisions set out in article 99 (e.g. relating to high-risk AI systems), will be subject to administrative fines of up to â‚¬15 million or, if the offender is a company, up to 3% of its global turnover in the preceding financial year, whichever is higher. These penalties underscore the importance of complying with the AI Actâ€™s regulations. It is essential for companies to fully grasp these penalties and ensure that their AI systems meet the Actâ€™s requirements. \n\n# Timeline \n\nThe AI Act becomes applicable on a staggered basis. There are also transitional arrangements for AI systems that had been placed on the market or put into service before certain dates. The AI Act applies to all operators of high-risk AI systems that have been placed on the market or put into service before 2 August 2026, unless those systems are subsequently subject to significant change in design (in which case, the provisions would apply in full with respect to the redesigned system). The relevant dates of application are set out below. \n\n12 July 2024 The AI Act was published in the Official Journal of the EU, triggering the dates for specific provisions in the Regulation becoming applicable. \n\n2 February 2025 Prohibited practices ban applies (Chapter II). AI literacy rules apply (article 4). \n\n2 May 2025 Codes of practice for general-purpose AI must be ready (article 56 (9)). \n\n2 August 2025 National authorities designated (Chapter III Section 4). Obligations for General-purpose AI (GPAI) (Chapter V). Governance (at EU and national level) (Chapter VII). Confidentiality and penalties (other than in relation to gen-AI) \n\n(Chapter XII). \n\n2 August 2026 Start of application of all other provisions of the EU AI Act (unless a later date applies below). \n\n2 August 2027 High-risk categories listed in Annex I. General-purpose AI models placed on the market before 2 August 2025 (article 111). \n\n2 August 2030 High-risk AI systems (other than those listed below), which have been placed on the market or put into service before 2 August 2026 and which are intended to be used by public authorities (article 111). \n\n31 December 2030 Components of large-scale IT systems listed in Annex X, which have \n\nbeen placed on the market or put into service before 2 August 2027 \n\n(article 111). 91 2 3 4 5 6 7 8 9 10 \n\nIf you or your supply chain fall within the scope of the AI Act, check whether any AI systems or AI models fall within one or more of the regulated categories. If you are a provider or deployer of AI systems within the scope of the AI Act, ensure you have taken steps to comply with the Actâ€™s AI literacy requirements. Determine whether you, your suppliers or your customers will be an operator falling within the material and territorial scope of the AI Act. \n\n# To do list At a glance \n\nCHAPTER 2 \n\n# Material and territorial scope \n\nâ€¢ The AI Act covers AI systems, general-purpose AI models and prohibited AI practices. \n\nâ€¢ Obligations can be imposed on six categories of economic actors: providers, importers, distributors, product manufacturers, authorised representatives and deployers. \n\nâ€¢ Economic operators involved with high-risk \n\nAI systems have significant obligations. Providers and deployers of certain \n\ncategories of AI systems are also subject to transparency obligations. \n\nâ€¢ Providers of general-purpose AI models are subject to obligations. \n\nâ€¢ The AI Act applies when an AI system or general-purpose AI model is placed on \n\nthe EU market, put into service in the EU, imported into or distributed in the EU. It also applies where an AI system is used \n\nby a deployer who has their place of establishment or is in the EU. \n\nâ€¢ Providers and deployers of AI systems who fall within scope of the AI Act are subject to AI literacy requirements from 2 February 2025. 10 1 2 3 4 5 6 7 8 9 10 \n\n# Material scope \n\nThe AI Act primarily provides harmonised rules for the placing on the market, the putting into service, and the use of AI systems. It imposes an extensive set of obligations on â€œhigh-riskâ€ AI \n\nsystems and transparency obligations on certain AI systems. It also prohibits certain AI practices and regulates the supply of general-purpose AI models in the EU. The AI Act also sets out rules for market monitoring, market surveillance, governance and enforcement, which includes administrative fines, as well as measures to support innovation, with a particular focus on small and medium enterprises, such as through the operation of AI sandboxes. It also establishes two new bodies: (i) the European Artificial Intelligence Board â€“ which is tasked with advising and assisting the European Commission and EU Member States to facilitate the consistent and effective application of the AI Act; and (ii) the AI Office, which has been established within the European Commission and is tasked with implementing the AI Act, fostering the development and use of trustworthy AI and promoting international cooperation. \n\nRegulated persons: Operators \n\nThe AI Act imposes obligations on six categories of entities: providers, deployers, importers, distributors, product manufacturers and authorised representatives â€“ the term â€œoperatorâ€ \n\nis used to describe all of them. There will always be a provider for an AI system or a general-purpose AI model. Whether there will also be other operators will depend on the way in which the AI system or general-purpose AI model is being supplied and deployed. Most operators are defined with reference to three \n\nkey terms adapted from the EU product legislation referenced in Annex I of the AI Act: \n\nâ€œmaking availableâ€ , â€œplacing on the marketâ€ and \n\nâ€œputting into serviceâ€ .\n\nâ€œmaking availableâ€ is the supply of an AI system or a general-purpose AI model for distribution or use on the EU market in the course of a commercial activity, whether in return for payment or free of charge; \n\nâ€œplacing on the marketâ€ is the first making available of an AI system or a general-purpose AI model on the EU market; and \n\nâ€œputting into serviceâ€ is the supply of an AI system for first use directly to the deployer or for own use in the EU for its intended purposes. The term â€œuseâ€ is not defined in the AI Act. In essence, â€œuseâ€ would be perceived by reference to the key characteristic of an AI system which is to infer, from inputs it receives, how to generate outputs. These three terms are discussed in section 2.3 of the Commissionâ€™s Guidelines on prohibited AI practices, which provides illustrative examples of each activity in the context of the restrictions on prohibited practices. The regulated operators under the AI Act are: \n\nOperator Role Relevant for both AI systems and general-purpose AI models Provider (article 3(3)) \n\nDevelops an AI system or a general-purpose AI model or has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge. Although the definition of â€œplacing on the marketâ€ refers to the \n\nEU market, a person can still be deemed a provider regulated by the AI Act even if they do not place an AI system on the EU market, where the output of the AI system is used in the EU. See \n\nâ€œTerritorial Scopeâ€ further below. A provider can be a natural or legal person, public authority, agency or other body. EU institutions, bodies, offices and agencies may also act as a provider of an AI system. 11 1 2 3 4 5 6 7 8 9 10 \n\nIt is also possible to become a provider where an AI system has already been placed on the market or put into service in the EU by another provider, by taking one of the steps set out in article 25(1) (a)-(c). See further below, under â€œHigh-risk AI systemsâ€ .\n\nAuthorised representative (article 3(5)) \n\nAn EU-established natural or legal person appointed \n\nby a provider established outside the EU to act as their authorised representative. The role includes ensuring that the documentation required by the AI Act is available to the competent authorities and co-operating with those authorities. See article 22 (for high-risk AI systems) and article 54 (for general-purpose AI models). \n\nRelevant for AI systems only Deployer (article 3(4)) \n\nUses an AI system under its authority (excluding use in the course of personal, non-professional activity). A deployer can be a natural or legal person, public authority, agency or other body. EU institutions, bodies, offices and agencies may also act as a deployer of an AI system. \n\nImporter (article 3(6)) \n\nNatural or legal person located or established in the EU that places an AI system bearing the name or trademark of a person not established in the EU on the EU market. \n\nDistributor (article 3(7)) \n\nNatural or legal person in the supply chain, other than the provider or the importer, that makes an AI system available on the EU market. \n\nProduct manufacturer (article 25(3)) \n\nIn certain circumstances, a product manufacturer will be considered the â€œproviderâ€ of a high-risk AI system where: this is a safety component of a product covered by the AI Act (by virtue of being subject to the EU product safety legislation referenced in Section A of Annex I), and the manufacturer places the AI system on the EU market or puts it into service in the EU together with that product and under its own name or trademark. The term â€œproduct manufacturerâ€ is not defined in the AI Act â€“ but Recital 87 clarifies that this is the â€œmanufacturerâ€ defined under the EU product safety legislation referenced in Annex I to the AI Act. \n\nIndirect obligations under the AI Act \n\nThe AI Act imposes indirect obligations on component suppliers to providers of high-risk \n\nAI systems. Those supplying AI systems, \n\ntools, services, components, or processes that are used or integrated in a high-risk AI system are required to enter into a written agreement with the provider of the high-risk AI system and to enable the latter to comply with its obligations under the AI Act (article 25(4)). This obligation does not apply to third parties who make such tools, services, processes or components (other than general-purpose AI models) accessible to the public under a free and open-source licence. \n\nRights granted by the AI Act \n\nUnlike the GDPR, which provides a comprehensive set of rights to individuals, the rights under the AI Act are limited. The AI Act only confers a right to explanation of individual decision-making on affected persons located in the EU (article 86). Affected persons are those who are subject to a decision which has a legal or similarly significant effect on them and which is based on the output of one of the high-risk AI systems identified in Annex III. The wording used here is similar to that used under the automated decision-making provisions of the GDPR (article 22 GDPR); the scope of the two provisions however is not identical. 12 1 2 3 4 5 6 7 8 9 10 \n\nRegulated subject matter: AI systems \n\nAn AI system is defined broadly in article 3(1) as: â€œ a machine-based system that is designed to operate with varying levels of autonomy, and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, \n\nor decisions that can influence physical or \n\nvirtual environmentsâ€ .This definition is intended to align with the definition used by the OECD AI Principles. A key characteristic of AI systems is their capability to infer, i.e. to obtain outputs and to derive models or algorithms, or both, from inputs or data. Instead, traditional software, which executes operations based solely on rules defined by natural persons is not, on its own, considered an AI system. In February 2025, the Commission published guidelines for this definition. These guidelines \n\nprovide further explanations for each aspect of the definition, with a clear emphasis on the â€œability to infer.â€ In a positive sense, the guidelines outline various machine learning approaches that enable this ability. At the same time, they list systems - particularly those primarily based on mathematical or statistical methods - that do not possess this ability and should therefore not fall within the scope of the AI Act. A noteworthy negative example is â€œlogistic regression,â€ which is widely used in the financial sector. An AI system can be used on a standalone basis or as a component of a product, irrespective of whether the AI system is physically integrated into the product or serves the productâ€™s functionality without being integrated into it. Under the AI Act, AI systems fall into the following categories: \n\nâ€¢ high-risk AI systems; \n\nâ€¢ AI systems with transparency risks; and \n\nâ€¢ all other AI systems. An AI system can also form part of a prohibited AI practice. This can be because of certain features of that AI system or because of the way the AI system would be used. \n\nHigh-risk AI systems \n\nSection III of the AI Act regulates high-risk AI systems. These are AI systems that pose a significant risk of harm to the health, safety \n\nand fundamental rights of persons in the EU. \n\nAn AI system may be classified as high-risk in \n\ntwo ways: \n\nâ€¢ Article 6(1): The AI system is used as a \n\nsafety component in a product that is regulated by certain EU product safety legislation (the Union harmonisation legislation listed in Annex I of the AI Act) and is subject to the conformity assessment procedure with a third-party conformity assessment body under such legislation, or constitutes on its own such a product (e.g. an AI system which is used for medical diagnostic purposes will itself be a regulated medical device); or \n\nâ€¢ Article 6(2): The AI system falls within one of the eight categories set out in Annex III of the AI Act â€“ unless the provider can demonstrate and document that such AI system does not pose a significant risk of harm. Most of the obligations regarding high-risk AI systems fall on providers (which includes product manufacturers as we describe further above), whilst a more limited set of obligations is imposed on deployers, on importers and distributors, and where relevant, authorised representatives. See Chapter 4 of this guide for more details. \n\nAI systems with transparency risks \n\nThe AI Act imposes certain transparency \n\nobligations on: \n\nâ€¢ providers of AI systems intended to interact directly with natural persons (article 50(1)); \n\nâ€¢ providers of AI systems generating synthetic audio, image, video or text content (article 50(2)); \n\nâ€¢ deployers of an emotion recognition system or a biometric categorisation system (article 50(3)); and \n\nâ€¢ deployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake (article 50(4)). See Chapter 6 of this guide for more details. 13 1 2 3 4 5 6 7 8 9 10 \n\nAll other AI systems \n\nAll other types of AI systems, which do not fall under the above categories and are not used for prohibited AI practices are not subject to direct legal obligations under the AI Act. Voluntary codes of conduct may be drawn up in future covering this broader category of AI systems and those deploying them (article 95). Providers and deployers may choose to adhere to these codes of conduct. Aside from rules relating to specific categories of AI systems, those qualifying as the provider or deployer of any AI system under the AI Act are required to take AI literacy measures to ensure that their staff and other persons dealing with the operation and use of AI systems on their behalf, have a sufficient level of knowledge, skills and understanding regarding the deployment of AI systems, their opportunities and risks (article 4). This obligation aims to foster the development, operation and use of AI in a trustworthy manner in the EU â€“ however, it is worth noting that this provision refers to voluntary codes of conduct and that administrative fines are not foreseen for failure to comply with the AI literacy obligation. \n\nRegulated subject matter: Prohibited AI practices \n\nThe AI Act prohibits the placing on the market, putting into service and use of AI systems that have certain prohibited features and/or are intended to be used for certain prohibited purposes, e.g. AI systems that create or expand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage. These practices are deemed to be particularly harmful and abusive and contradict EU values and fundamental rights. The prohibited AI practices are listed in article 5 of the AI Act. This list does not affect the prohibitions of AI practices that infringe other EU law (such as data protection, non-discrimination, consumer protection and competition law). See Chapter 3 of this guide for more detail. \n\nRegulated subject matter: general-purpose AI models \n\nA general-purpose AI model is defined in article 3(63) as: â€œ an AI model, including where such an AI model is trained with a large amount of data using self-supervision \n\nat scale, that displays significant generality \n\nand is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market â€\n\nThe AI Act does not provide a definition of an â€œAI model â€; recital 97 notes that although AI models are essential components of AI systems, they do not constitute AI systems on their own and require further components, such as a user interface, to become AI systems. The characteristics of general-purpose AI models are discussed further in recitals 98 and 99. The AI Act regulates general-purpose AI models and imposes additional obligations for general-purpose AI models with systemic risks. The rules apply to providers of general-purpose AI models, once these models are placed on the market: this can be done in various ways, such as through libraries, APIs, as a direct download or as a physical copy. Recital 97 suggests that the rules on general-purpose AI models can also apply when these models are integrated into or form part of an AI system. When the provider of a general-purpose AI model integrates its own model into its own AI system that is made available in the market or put into service, then recital 97 suggests that model will be viewed as being placed on the market and the general-purpose AI model provisions will apply, in addition to those regarding AI systems. \n\n# Where can I find this? \n\nMaterial Scope  article 1  recitals 1-3, 6-8 14 1 2 3 4 5 6 7 8 9 10 \n\nThose who integrate third party general-purpose AI models into their own AI systems are considered â€œdownstream providersâ€ and are granted certain rights under the AI Act. However, the AI Act appears to envisage that a provider who fine-tunes a third party general-purpose AI model and integrates that fine-tuned model into their own AI system (or otherwise places a fine-tuned general-purpose AI model on the market or puts it into service) will be considered the provider of this with respect to that fine-tuning only (see recital109). See Chapter 5 of this guide for more detail. \n\n# Territorial scope \n\nAI System provisions \n\nThe AI Act is intended to have a broad jurisdictional scope for its AI system provisions: these are engaged when an AI system, either on its own or as part of a product covered by the EU product safety legislation in Annex I, is: \n\nâ€¢ placed on the EU market, put into service \n\nin the EU, imported into or distributed in \n\nthe EU; or \n\nâ€¢ used by a deployer who has their place of establishment or is located in the EU. The first point applies applies irrespective of where the provider of the AI system is established. The concept of â€œ establishment â€ is \n\nnot defined in the AI Act. It is expected that this would be interpretated broadly, similar to the use of this term under other EU legislation, such as the GDPR. In addition to those cases, the AI system provisions also apply if the output produced \n\nby an AI system outside the EU is used in the EU. In that case, the non-EU established/ \n\nlocated providers and deployers will also be caught by the scope of the AI Act. Recital 22 clarifies that in those instances the AI Act will apply even though the relevant AI systems are not placed on the market, put into service or used in the EU. \n\nProhibited AI Practices \n\nThe AI Actâ€™s provisions relating to prohibited AI practices apply to the placing on the EU market, putting into service in the EU and use of the relevant AI practices set out in Article 5. As we saw above, the definitions of â€œplacing on the marketâ€ and â€œputting into serviceâ€ refer to the EU \n\nmarket. The AI Act itself does not specify what a prohibited â€œuseâ€ would entail. The Commissionâ€™s Guidelines on prohibited AI practices suggest \n\nthat use â€œshould be understood in a broad manner to cover the use or deployment of the system at any moment of its lifecycle after having been placed on the market or put into serviceâ€ and further that use \n\nâ€œmay also cover the integration of the AI system in the services and processes of the person(s) making use of the AI system, including as part of more complex systems, processes or infrastructure.â€ \n\nGeneral-purpose AI Models \n\nThe AI Actâ€™s general-purpose AI model provisions will be engaged where a provider of a general-purpose AI model places it on the market in the EU or puts it into service in the EU â€“ irrespective of where the provider is located or established. \n\n# Where can I find this? \n\nTerritorial Scope  article 2  recitals 9-11 15 1 2 3 4 5 6 7 8 9 10 \n\n# Exclusions \n\nCertain activities are entirely outside the AI Actâ€™s scope. The AI Act does not apply to: \n\nâ€¢ areas outside the scope of EU law (e.g. activities concerning national security). This is the case irrespective of the type of entity entrusted under national legislation with carrying out the exempted activities. Given the very broad competences of the EU, as set out in the TFEU, this provision will have very limited scope of application in practice; \n\nâ€¢ AI systems placed on the market, put into service, or used with or without modification â€“ or where their output is used in the EU, exclusively for military, defence or national security purposes, regardless of the type of entity carrying out those activities. An AI system placed on the market or put into service for an excluded purpose (military, defence or national security) and one or more non-excluded purposes (e.g. civilian purposes or law enforcement) is subject to the AI Act and providers of those systems should ensure compliance with the AI Act; \n\nâ€¢ public authorities in a third country or international organisations that use AI systems in the framework of international cooperation or agreements for law enforcement and judicial cooperation with the EU or EU member states, provided that such a third country or international organisation provides adequate safeguards for the protection of fundamental rights and freedoms of individuals. The \n\nnational authorities and EU institutions, \n\nbodies, offices and agencies making use of those outputs remain subject to EU law; \n\nâ€¢ AI systems and models, including their output, specifically developed and put into service for the sole purpose of scientific research and development; \n\nâ€¢ research, testing or development of AI systems or models prior to their being placed on the market or put into service, excluding though testing in real world conditions; \n\nâ€¢ deployers who are individuals and use the AI system in the course of a purely personal, non-professional activity. This is similar to \n\nthe GDPRâ€™s â€œhousehold exemptionâ€ â€“ whilst providers of those AI systems continue to be subject to the AI Act; and \n\nâ€¢ AI systems released under free and open-source licences, unless they are placed on the market or put into service as high-risk AI systems, as a prohibited AI system or as a system that is covered by the Actâ€™s transparency obligations. \n\n# Relationship with other regulatory frameworks \n\nâ€¢ As a Regulation, the AI Act is directly applicable in EU Member States without the need for implementing legislation. EU Member States are prevented from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by the AI Act. This is only provided for in limited circumstances: for example, EU member states may introduce more restrictive laws on the use of remote biometric identification systems â€“ some of which constitute prohibited AI practices (article 5(5)) and the use of post-remote biometric identification systems, which constitute high-risk AI systems (article 26(10)). \n\nâ€¢ The AI Actâ€™s provisions on high-risk AI systems are built around the New Legislative Framework for EU products. This is a legislative package that sets out rules for the placing of products on the EU market, enhances market surveillance rules and rules for conformity assessments and CE marking. It also establishes a common legal framework for industrial products in the form of a toolbox of measures for use in future legislation. The AI Act specifies how these tools set out in the New Legislative Framework should apply in the context of AI systems. \n\nâ€¢ In parallel, the AI Act complements Union harmonisation legislation â€“ this is the set of \n\nEU product safety legislation on the basis of which certain AI systems are to be classified \n\nas high-risk. \n\nâ€¢ The obligations of the AI Act apply in addition to and without prejudice to the obligations under GDPR, the e-Privacy Directive and the Law Enforcement Directive. 16 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ Article 5 lists eight prohibited practices \n\nwhich are deemed to pose an unacceptable level of risk. \n\nâ€¢ Prohibitions come into effect on \n\n2 February 2025. \n\nâ€¢ The prohibited practices are: \n\nâ€” Subliminal, manipulative, or deceptive techniques \n\nâ€” Techniques exploiting vulnerable groups in each case which materially distorts behaviour and risks significant harm \n\nâ€” Social scoring in certain use cases \n\nâ€” Predicting criminality based on profiling \n\nâ€” Scraping the web or CCTV for facial recognition databases \n\nâ€” Inferences of emotions at workplaces \n\nor schools \n\nâ€” Biometric categorisation to infer race, political opinion, trade union membership, religious or political beliefs, sex life or sexual \n\norientation \n\nâ€” Real-time remote biometric identification in public spaces for law enforcement purposes. \n\nâ€¢ Many of the prohibitions have exceptions -case by case analysis is needed. \n\nâ€¢ The list is not final: it will be re-assessed annually. \n\nâ€¢ Non-compliance sanctioned by fines up to â‚¬35 million or 7% of total worldwide annual turnover for the proceeding financial year (whichever is higher). \n\nâ€¢ The prohibitions are operator-agnostic \n\nand apply irrespective of the role of the \n\nactor (i.e. whether provider, deployer, distributor or importer). Check for updates to this list annually as the list of prohibited practices may change over time. Consider whether any exceptions apply. The prohibited practices are not absolute; many have exceptions. Check the AI systems you use to see if they fall under the prohibited category. \n\n# To do list At a glance \n\nCHAPTER 3 \n\n# Prohibited AI Practices 17 1 2 3 4 5 6 7 8 9 10 \n\n# Prohibited AI practices \n\nThe AI Act relies on a risk-based approach, so different requirements apply in accordance with the level of risk. This chapter concentrates on prohibited practices i.e. those which conflict with the values of the European Union and are a clear threat to fundamental rights such as freedom, equality and privacy. The prohibitions are an attempt by law makers to respond to transparency and ethics concerns and to guarantee the protection of human rights. The prohibited practices are listed exhaustively in article 5 (and are further explained in recitals 28 â€“ 45 of the Act and by guidelines issued by the Commission on 4 February 2025) and provide a clear framework for what AI can and cannot do within the EU. The prohibitions in Article 5 apply from 2 February 2025 and are therefore the first provisions to come into force, highlighting their importance.The list of prohibited practices in article 5 is exhaustive, but not final. The Commission will assess the need for amendment of the list of prohibited practices annually (article 112) and can submit findings to the European Parliament and Council. So, there may be variations to the list of prohibited practices in due course. There are currently eight prohibited practices, which focus on practices that materially distort peoplesâ€™ behaviour, or raise concerns in democratic societies. Special attention has been given to biometric identification systems. However, there are detailed exceptions to many of the prohibitions and each practice should be considered on a case-by-case basis. \n\nArticle 5(1)(a) Subliminal, manipulative or deceptive techniques \n\nThe first prohibition concerns AI systems deploying subliminal, manipulative or deceptive techniques in cases where: \n\nâ€¢ the techniques either aim to, or actually have, the effect of materially distorting the behaviour of an individual or a group; \n\nâ€¢ by appreciably impairing the ability of individuals to make informed decisions; and \n\nâ€¢ causing them to take decisions they would \n\nnot otherwise have taken, and that either \n\ncause or are reasonably likely to cause them significant harm. The techniques expressly mentioned in recital 29 involve: deployment of subliminal components such as audio, image, video stimuli that persons cannot perceive, or other manipulative or deceptive techniques that subvert or impair a personâ€™s autonomy, decision-making, or free choice, in ways so that people are not consciously aware of those techniques or, where they are aware of them, can still be deceived or are not able to control or resist them. The reference in recital 29 to machine-brain interfaces having the capability to materially distort human behaviour in a significantly harmful manner may also be the Actâ€™s attempt to regulate tools that employ neural data which is currently under discussion in other jurisdictions such as Colorado, California, and Chile. For an AI system to be prohibited, there needs to be a causal link between the deceptive techniques and the significant harm caused. The \n\nthreshold of â€œsignificantâ€ harm was added in the legislative process and makes clear that not all dark patterns would fall under this provision. The provision is open for interpretation and, in particular, the word â€œdeceptiveâ€ will lead to further discussions. According to the Commissionâ€™s guidelines, deceptive techniques could cover presenting false or misleading information with the objective or effect of misleading individuals, if the other requirements of the first prohibition are met. \n\nArticle 5(1)(b) Exploitation of vulnerabilities \n\nThe second category of prohibited AI practices aims to protect vulnerable people. There are three groups: vulnerability due to age, disability, or due to specific social or economic situations. An AI system is only prohibited if it has the objective or the effect of materially distorting the behaviour of an individual and does so in a manner that causes or is likely to cause someone significant harm. An exploitation from a socio-economic perspective does not exist, according to the Commission guidelines on prohibited practices, if the situation may be experienced by any person irrespective of their socio-economic situation (e.g. grievances or loneliness). In such case, however, an exploitation may be covered under Article 5(1)(a) AI Act. 18 1 2 3 4 5 6 7 8 9 10 \n\nAI systems that inadvertently impact socio-disadvantaged groups due to biased training data do not automatically exploit vulnerabilities, as there is no intentional targeting. However, under the Commission guidelines on prohibited practices, if AI providers or deployers are aware that their systems unlawfully discriminate against socio-economically disadvantaged persons and foresee significant harm without taking corrective action, they may still be considered to exploit these vulnerabilities. An exploitation of a personâ€™s economic situation could exist in cases where an AI system is used to find persons in poverty to exploit their weaknesses economically. Organisations using AI systems for marketing and sales should make sure they test their systems against this requirement. The concept of significant harm is common to both subliminal techniques and exploitation of vulnerable groups. In the legislative process, requirements that the harm needed to be physical or psychological were dropped. It seems that a broad approach is intended to be taken to the concept of harm, although recital 29 still gives the examples of important adverse impacts on physical and psychological health, alongside financial interests. The recital also notes that harms can be accumulated over time. This prohibition is not intended to affect lawful medical treatment (e.g. psychological treatment of a mental disease carried out with consent). Recital 29 also implicitly recognises that advertising and some other commercial practices inherently depend on nudging â€“ and states that the intent is not to prohibit common, legitimate and lawful commercial practices, particularly in the field of advertising. Consent can play a crucial role in these scenarios. In persuasive interactions, individuals are aware of the influence attempt and can make choices freely and autonomously. \n\nArticle 5(1)(c) Social scoring \n\nThe third prohibition concerns so-called social scoring, i.e. classifying individuals or groups over a period based on their social behaviour, or known, inferred, or predicted personal characteristics. Social scoring is prohibited in \n\ntwo cases: \n\nâ€¢ if it leads to unfavourable treatment in social contexts that are unrelated to the context in which the data was originally generated; and \n\nâ€¢ if this leads to unfavourable treatment of individuals or groups that is unjustified or disproportionate to their social behaviour or \n\nits gravity. Social scoring is used by several governments around the world. The government in the Netherlands stepped down in 2021 due to a flawed risk-scoring algorithm, which lead to unjustified accusation of fraud for welfare benefits based on personal characteristics and behaviour. The algorithm in that case targeted minorities and people based on their economic situation. Whilst governments might be the first example that comes to mind when thinking about social scoring, the provision is wider and encompasses all social scoring systems in public or private contexts. Many algorithms inherently depend on behavioural scores. However, the AI Act only prohibits those scoring systems resulting in unfavourable treatment in unrelated social contexts. This key restriction targets the consequences of social scoring, preventing unjust outcomes, or discrimination of individuals or groups. The social scoring prohibition under the AI Act therefore depends on the context the data has been obtained from and the context the data is being used. As the Commission guidelines on prohibited practices illustrate, lawful activities, like credit and risk scoring in financial services, are permitted if they improve service quality or prevent fraud. Conversely, an insurance company using spending and other financial data from a bank to set life insurance premiums is provided as an example of unlawful social scoring. \n\nArticle 5(1)(d) Profiling for criminal risk \n\nassessment \n\nThe fourth prohibition is placing on the market, putting into service, or using AI systems that assess or predict the likelihood of a person committing criminal offences based solely on profiling or on assessing the personality traits and characteristics of a person. There is an exception for AI systems used to support human assessment of involvement of a person in a criminal activity, which is based on objective and verifiable facts directly linked to a criminal activity â€“ i.e. detection tools which are factual and supplement, but do not supplant, human decision making. This prohibition aims to avoid the scenario whereby people are treated as guilty for crimes they have not (yet) committed â€“ as illustrated in the film Minority Report . It is tied to human dignity as laid down in article 1 of the Charter of Fundamental Rights. 19 1 2 3 4 5 6 7 8 9 10 \n\nThe Commission guidelines on prohibited practices emphasise that the prohibition can extend to private entities if they act with public authority or assist law enforcement. For instance, a private company analysing data for law enforcement might face prohibition if specific criteria are met. The Commission guidelines also suggest that retrospective human assessments of AI system evaluations can fall outside the scope under certain conditions. This is informed by CJEU case law, which underscores the importance of human review to ensure that AI-driven decisions are based on objective criteria and are non-discriminatory, thus extending beyond the initial exemption in the AI Act. \n\nArticle 5(1)(e) Facial recognition databases \n\nThe fifth prohibited practice is the placing on the market, putting into service for the specific purpose, or use of AI systems to create or expand facial recognition databases through untargeted scraping of facial images from the internet or CCTV footage. Recital 43 considers this practice to add to the feeling of mass surveillance and that it can lead to gross violations of fundamental rights, including the right to privacy. This may be a response to the investigations by supervisory authorities into Clearview AI. The Commission guidelines on prohibited practices regarding facial recognition databases clarify several key points. Such databases can be temporary, centralised, or decentralised, and they fall under Article 5(1)(e), if they can be used for facial recognition, regardless of their primary purpose. Targeted scraping, such as collecting images of specific individuals or using reverse image searches, is allowed, but combining it with untargeted scraping is prohibited. The prohibition does not cover untargeted scraping of other biometric data, like voice samples, or databases not used for recognition, such as those for AI model training without identifying individuals. \n\nArticle 5(1)(f) Inference of emotions in working life and education \n\nThe sixth prohibited practice is the placing on the market, putting into service for this specific purpose, or use of AI systems to infer emotions in workplace or schools, except for safety or medical reasons such as systems intended for therapeutical use. The guidelines clarify that the definition of both the school and workplace should be interpreted widely and in the case of workplace use they should also cover the selection and hiring phases of recruitment. The exception for the safety or medical reasons on the other hand is to be interpreted narrowly. For example, systems measuring burnout or depression in the workplace would not be exempted. Recital 18 distinguishes between emotions or intentions such as happiness, sadness, anger etc. It explains that the notion does not include physical states, such as pain or fatigue (so, systems used in detecting the state of fatigue of professional pilots or drivers for the purpose of preventing accidents would not be affected). It also does not include detection of readily apparent expressions such as a frown or a smile, or gestures such as the movement of hands, arms or head, or characteristics of a personâ€™s voice, such as a raised voice or whispering. However, the guidelines still do not clarify the meaning of â€œintentionâ€ which are also covered by the definition of emotion recognition systems. The AI Act has a defined term of â€œemotion recognition systemâ€ , which means an â€œ AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of biometric data â€. Curiously, article 5(1)(f) does not use this term, and refers to any use of AI systems to infer emotions (i.e. without the requirement that this should be derived from biometric data). However, the Commissionâ€™s guidelines clarified that Article 5(1)(f) should be read as referring to the emotion recognition systems as the defined term under the Act. They further clarified that nonbiometric emotion recognition systems (e.g. text-based) are not prohibited provided they are not used in conjunction with biometric data such as keystroke analysis. The Act references the inaccuracy of biometric emotion recognition systems and their intrusive nature in settings where there is an imbalance of power (such as workplace and schools) as the reason for the prohibition in such settings. However, the AI Act does not explain why it considers non-biometric emotion recognition systems as less intrusive or more accurate than biometric systems. 20 1 2 3 4 5 6 7 8 9 10 \n\nArticle 5(1)(g) Biometric categorisation \n\nThe seventh prohibition is on the use of biometric categorisation systems that categorise individuals based on their biometric data to deduce or infer certain (not all) special category data under the GDPR, namely: race, political opinions, trade union membership, religious or political beliefs, sex life or sexual orientation. Special category data under the GDPR that are not covered in the prohibition are inferences of ethnic origin, health, and genetic data. However, inferring such types of data would likely fall under the high-risk category under Annex III. Additionally, the prohibition does not cover labelling or filtering of lawfully acquired biometric datasets or categorising of biometric data by law enforcement (e.g. sorting of images according to hair colour or eye colour by law enforcement to search for suspects). However as recital 54 suggests that AI systems intended to be used for biometric categorisation according to sensitive attributes or special category data under the GDPR, in so far as they are not prohibited the AI Act, should be classified as high-risk and the guidelines also state that most AI systems that fall under an exception from a prohibition listed in Article 5 AI Act will qualify as high-risk this would suggest that the exempted labelling and filtering systems would fall under the high-risk category. Recital 16 clarifies that biometric categorisation systems do not include purely ancillary features which are linked to another commercial service, where the feature cannot, for objective technical reasons, be used without the main service, and where this is not a circumvention mechanism to evade AI Act rules (e.g. retail try before you buy filters, or social media filters). The guidelines also clarify that the scope of biometric categorisation excludes categorisation according to clothes or accessories, such as scarfs or crosses, or social media activity. \n\nArticle 5(1)(h) Real-time remote biometric \n\nidentification in public spaces \n\nThe eighth and last prohibition is the use \n\nof real-time remote biometric identification systems (â€œRBIâ€) in publicly accessible spaces for law enforcement purposes. RBI systems are AI systems for the purpose of identifying natural persons, without their involvement, typically at a distance, by comparing biometric data with that contained in a reference database. Real-time systems include those where there is a short delay in the comparison. The AI Act does not define how much time amounts to â€œsignificant \n\ndelayâ€ . However, the guidelines suggest that this would likely be the case for when the person is likely to have left the place where the biometric data was taken and not allow for a quick reaction from the law enforcement. Biometric systems used for verification (i.e. confirming that someone is who they claim to be, to access a service, a device, or to have security access to premises) are distinguished from RBI and so not covered by this prohibition (recital 15). The guidelines clarify that the distinction between the identification and verification comes from the active involvement of the individual in the process which may have minor impact on fundamental rights of natural persons. For active involvement, however, it is not sufficient that persons are informed about the presence of cameras, but they need to step actively and consciously in front of a camera that is installed in a way fostering active participation. The AI Act allows (but does not require) member states to permit use of RBI for law enforcement purposes in limited situations where the use of RBI is strictly necessary for: \n\nâ€¢ targeted searches for specific victims of abduction, human trafficking, or sexual exploitation as well as searching for \n\nmissing persons; \n\nâ€¢ the prevention of a specific, substantial, and imminent threat to the life or physical safety, \n\nor a genuine and present or foreseeable threat \n\nof terrorist attack; or \n\nâ€¢ the localisation or identification of a person suspected of having committed a criminal offence, conducting a criminal investigation, prosecution or executing a criminal penalty for serious offences â€“ being those referred to in Annex II and punishable in the Member State concerned by a prison sentence for a maximum period of at least four years. The exemptions only permit RBI used to confirm the identity of the specifically targeted individual. In addition, use of RBI should consider the nature of the situation, in particular the seriousness, probability, and scale of the harm that would be caused if the system were not used, against the consequences of use on the rights and freedoms of the persons concerned. 21 1 2 3 4 5 6 7 8 9 10 \n\nFurther, protections include the need to complete a fundamental rights assessment, registration of the system in an EU database in line with article 49, and prior authorisation of each use case by judicial or administrative authority (subject to urgency measures). In addition, each use of RBI in publicly accessible spaces must be notified to the relevant market surveillance authority and the national data protection authority. The national authorities must then report to the European Commission which, \n\nin turn, prepares an annual state of the nation \n\nreport on usage of RBI in accordance with these provisions. \n\n# To whom do the prohibitions apply? \n\nAs set out in Chapter 2, the AI Act distinguishes between different actors involved in AI systems, attributing specific responsibilities based on their role in relation to the AI model or system. This method ensures that those who have the most influence over the development and implementation of AI technologies adhere to the highest standards. However, the rules on prohibited practices are operator-agnostic. In other words, they apply universally, independent of the specific role of the actor (i.e. whether they are involved in the provision, development, deployment, distribution, or use of AI systems engaging in prohibited practices). This wide-ranging application highlights the Actâ€™s dedication to stopping practices that could infringe on fundamental rights or present intolerable risks, emphasising a comprehensive approach to regulation that covers all types of interaction with harmful AI technologies. \n\n# Enforcement and fines \n\nWhen a practice is prohibited, the AI system in question may not be used in the EU. In the case of an infringement, competent authorities may issue a fine of up to 7% of the total worldwide annual turnover of the offender for the preceding financial year or 35 million EUR, whichever is higher. National market surveillance authorities will be responsible for ensuring compliance with the AI Actâ€™s provisions regarding prohibited AI systems. They will report to the European Commission annually about use of prohibited practices that occurred during the year and about the measures they have taken. 22 1 2 3 4 5 6 7 8 9 10 \n\n# Where can I find this? \n\nSubliminal, manipulative or deceptive techniques  article 5(1)(a)  recitals 28 & 29 \n\nExploitation of vulnerabilities  article 5(1)(b)  recitals 28 & 29 \n\nSocial scoring  article 5(1)(c)  recital 31 \n\nProfiling for criminal risk assessment  article 5(1)(d)  recital 42 \n\nFacial recognition database  article 5(1)(e)  recital 43 \n\nInference of emotions in working life and education  article 5(1)(f)  recitals 44 - 45 \n\nBiometric categorisation  article 5(1)(g)  recital 30 \n\nReal-time remote biometric identification in public spaces  article 5(1)(h)  recitals 32 - 41 \n\nOther useful resources \n\nâ€¢ Commission guidelines on prohibited artificial intelligence practices established by Regulation (EU 2024/1689 (AI Act) \n\nâ€¢ ETHICS GUIDELINES FOR TRUSTWORTHY AI: High-Level Expert Group on Artificial Intelligence (2019) \n\nâ€¢ EDPB Guidelines on Processing Personal Data Through Video Devices \n\nâ€¢ EDPB Guidelines on Use of Facial Recognition Technology In The Area of Law Enforcement \n\nâ€¢ EDPB Guidelines on Automated Decision Making and Profiling \n\nâ€¢ EDPB-EDPS Joint Opinion On The Proposal For The Artificial Intelligence Act \n\nâ€¢ EDPB guidelines on Deceptive Design Patterns in Social Media \n\nâ€¢ Guidelines on dark patterns from the Finnish Market Authority 23 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ AI systems fall within the scope of â€œhigh-riskâ€ if \n\nthey are intended to be used as: \n\nâ€” products, or safety components of products, which must undergo third-party conformity assessment pursuant to the legislation covered by Annex I; or \n\nâ€” for one of the purposes described in \n\nAnnex III. \n\nâ€¢ Providers, deployers, importers, distributors and suppliers to providers of high-risk AI systems have obligations under the AI Act. Market parties can have multiple roles in parallel and need to comply with multiple sets of obligations simultaneously. \n\nâ€¢ Providers of high-risk AI systems have the heaviest compliance burden and need to carry out a conformity assessment before the system can be placed on the market or put into service. \n\nâ€¢ Itâ€™s possible to become the provider of a \n\nhigh-risk AI system (e.g. by placing your own name/trademark on the system, making a substantial modification, or using the system for different purposes than intended by the original provider). Determine your role in the value chain (provider, deployer, importer, distributor, or third-party supplier) and review the corresponding obligations. Determine whether the AI system falls within the scope of high-risk as meant \n\nin article 6, in conjunction with Annexes I and III. \n\n# To do list At a glance \n\nCHAPTER 4 \n\n# High-risk AI systems 24 1 2 3 4 5 6 7 8 9 10 \n\n# Classification of an AI system as a high-risk AI system \n\nâ€¢ marine equipment \n\nâ€¢ rail systems \n\nâ€¢ motor vehicles and their trailers \n\nâ€¢ unmanned aircraft Note that the legislation in Annex I covers the categories above, but can also cover related products. For example, the Machinery Regulation covers lifting accessories and removable mechanical transmission devices as well as machinery. Itâ€™s also the core regulation for robotics, another steadily growing area of AI adoption for which the AI Act and its high-risk requirements will become highly relevant. Safety components fulfil a safety function for a product, where their failure or malfunction would endanger the health and safety of persons or property. You should make an assessment pursuant to the applicable product safety regulation in Annex I to see whether the AI system would have to undergo third-party conformity assessment pursuant to that legislation. For example, in the Medical Device Regulation, medical devices in class IIa and higher are subject to the third-party conformity procedure. If an AI-system qualifies as a safety component of such a medical device, or if it constitutes such a medical device itself, it is a high-risk AI system pursuant to the AI Act. Some of the legislation covered in Annex I also uses terms such as â€œhigh-riskâ€ and â€œmedium-riskâ€ .However, these categories are independent from the classification as high risk under the AI Act. For example, under applicable product safety legislation a product can be classed as â€œmedium-riskâ€ , but if the product has to to undergo third-party conformity assessment, then an AI system that is a safety component of that product, or that itself constitutes such a product, will be high-risk under the AI Act. \n\nCategory B: Annex III systems \n\nThe stand-alone list of high-risk systems currently contains: \n\nâ€¢ Biometrics: remote biometric identification of individuals, biometric categorisation of individuals and/or emotion recognition \n\nof individuals. The main part of the AI Act regulates high-risk \n\nAI systems. These are AI systems that can have \n\na significant harmful impact on the health, \n\nsafety and fundamental rights of persons in \n\nthe EU. There are two main categories of high-risk AI systems: a.  systems which are intended to be used as safety components of products or systems, or which are themselves products or systems, falling within the scope of Union harmonisation legislation listed in Annex I, if required to undergo a third-party conformity assessment pursuant to this legislation; and b.  systems whose intended purpose falls within the scope of the use cases set out in Annex III of the AI Act. \n\nCategory A: Annex I systems \n\nRegarding the first category (a), the product safety legislation listed in Annex I covers the following categories: \n\nâ€¢ machinery \n\nâ€¢ toys \n\nâ€¢ recreational craft and personal watercraft \n\nâ€¢ lifts/elevators \n\nâ€¢ equipment and protective systems for potentially explosive atmospheres \n\nâ€¢ radio equipment \n\nâ€¢ pressure equipment \n\nâ€¢ cableway installations \n\nâ€¢ personal protective equipment \n\nâ€¢ appliances burning gaseous fuels, \n\nmedical devices \n\nâ€¢ in vitro diagnostic medical devices \n\nâ€¢ civil aviation \n\nâ€¢ 2/3-wheel vehicles \n\nâ€¢ agricultural and forestry vehicles 25 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ Management and operation of critical infrastructure: to directly protect physical integrity or health and safety of individuals and property in relation to the management and operation of critical digital infrastructure (e.g., internet exchange points, DNS services, TLD registries, cloud computing services, data centres, content delivery networks, trust service providers, electronic communication networks or services), or in the supply of water, gas, heating, or electricity. \n\nâ€¢ Education and vocational training : decision-making in education and vocational training (e.g. selection, evaluation, assessment and monitoring of students or individuals applying to be students). \n\nâ€¢ Recruitment and HR : decision-making in recruitment and HR (e.g. selection, evaluation, assessment, promotion, termination, task allocation and monitoring of employees and/ or other workers and/or applicants). \n\nâ€¢ Essential services: evaluating the (continued) eligibility of individuals for public assistance benefits (e.g. healthcare services, social security allowances, disability benefits); evaluating creditworthiness of individuals \n\nor establishing their credit score (with the exception of the detection of financial fraud); risk assessment and pricing in relation to individuals in the case of life and health insurance; and evaluating and classifying emergency calls or making decisions in relation to dispatching or prioritisation of the dispatching of emergency first response services (e.g. police, firefighters, medical aid); and emergency healthcare patient triage. \n\nâ€¢ Crime analytics: assessment by/on behalf of/ in support of law enforcement authorities: (i) of the risk of individuals of becoming a victim or (re-)offender; (ii) of personality traits and characteristics; (iii) of past criminal behaviour of individuals or groups; or (iv) consisting of profiling of persons, in the course of the detection, investigation or prosecution of criminal offences. \n\nâ€¢ Evidence gathering and evaluation: \n\nevaluation of reliability of evidence during the investigation or prosecution of criminal offences, or in the course of applications for asylum, visa or residence permits, or with regard to associated complaints; use of polygraphs or similar tools by/on behalf of/ in support of law enforcement authorities or authorities conducting migration, asylum and/ or border control. \n\nâ€¢ Immigrant identification, migration risk and \n\nmigration application assessment: detecting, recognising or identifying individuals (with the exception of verification of travel documents) in the context of migration, asylum or border control management; assessment of risk (e.g. security risk, risk of irregular migration or health risk) posed by individuals who intend to enter or have entered the territory of an EU country and examination of applications for asylum, visa or residence permits and for associated complaints. \n\nâ€¢ Administration of justice: assisting judicial authorities or alternative dispute resolution institutions in researching and interpreting facts and the law and in applying the law to facts. \n\nâ€¢ Democratic processes: influencing the outcome of an election or referendum or voting behaviour of individuals. Note that Annex III may be amended by the Commission (article 7). The intended purpose is defined in article 3(12) as: â€œ the use for which an AI system is intended by the provider, including the \n\nspecific context and conditions of use, as specified in the information supplied by \n\nthe provider in the instructions for use, promotional or sales materials and statements, as well as in the technical documentation.â€ \n\nExceptions: not sufficiently high-risk \n\nArticle 6(3) provides that AI systems whose intended purpose falls within the scope of Annex III, so that they would (absent this provision be high-risk) shall nonetheless not be considered as high-risk if they do not pose a significant risk of harm to the health, safety or fundamental rights of natural persons. The article mentions four criteria. The exemption can be relied upon if one or more of these criteria are fulfilled (article 6(3) and recital 53): \n\nâ€¢ the AI system is intended to perform a narrow procedural task; \n\nâ€” Example: a system which transforms unstructured data into structured data or a system which detects duplicates of documents 26 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ the AI system is intended to improve the result of a previously completed human activity; \n\nâ€” Example: a system which improves the professional tone or academic style of language used in already drafted documents \n\nâ€¢ the AI system is intended to detect decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment, without proper human review; or \n\nâ€” Example: a system which checks flags inconsistencies or anomalies in the grades applied by a teacher, when compared with an existing grading pattern for that teacher \n\nâ€¢ the AI system is intended to perform a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III \n\nâ€” Example: a system for translating documents. The exception does not apply if the AI system involves profiling of natural persons within the meaning of article 4(4) of Regulation (EU) 2016/679 (GDPR) or article 3 (4) of Directive (EU) 2016/680 (Data Protection Enforcement Directive) or article 3, (5) of Regulation (EU) 2018/1725 (Data Protection for EU institutions) (recital 53). Companies deciding to make use of this exemption should note that they carry the burden of proof as to whether the system is high-risk. The assessment under article 6(3) must be documented before the system is placed on the market or put into service and the system must be registered (articles 49(2) and 6(4)). Providers of such systems must provide this documentation to national competent authorities on request. The Commission will provide guidelines specifying the practical implementation of article 6, including a comprehensive list of practical examples of high-risk and non-high-risk use cases of AI systems (article 6(5)). It may also adopt delegated acts adding to or modifying the criteria for article 6(3). The guidelines are expected to be published within six months after entry into force of the AI Act. \n\n# Obligations for providers of high-risk AI systems \n\nThe AI Act provides a detailed list of obligations for providers and deployers of high-risk AI systems as follows in Chapter III, Sections 2, 3 and 4: \n\nObligations for providers on high-risk AI systems Requirements of Section 2 Ensure compliance with requirements of Section 2 (see below). \n\nName of provider and contact information \n\nIndicate on the system (or, if not possible, on its packaging or accompanying documentation) the name of the provider or its brand and its contact information. \n\nQuality management system \n\nHave a quality management system complying with article 17. (Article 17 provides a detailed list of aspects of the system to be documented through policies, procedures and instructions). \n\nDocumentation \n\nKeep the documentation referred to in article 18. The documentation \n\nwill include: \n\nâ€¢ technical documentation (article 11) \n\nâ€¢ documentation concerning the quality management system (article 17) \n\nâ€¢ documentation concerning changes approved by notified bodies, \n\nwhere applicable \n\nâ€¢ decisions and other documents issued by notified bodies, where applicable \n\nâ€¢ the EU declaration of conformity (article 47). 27 1 2 3 4 5 6 7 8 9 10 \n\nLogs \n\nIf the system is under their control, keep logs automatically generated by the system (article 19). Such logs must be kept for a period appropriate to the intended purpose of the high-risk AI system. The period should be at least six months (unless any personal data protection provisions state otherwise). \n\nConformity Assessment \n\nEnsure that the system undergoes the relevant conformity assessment procedure in article 43, prior to being placed on the market or put into service (see below). \n\nDeclaration of conformity \n\nDraw up an EU declaration of conformity (article 47). See below. \n\nCE marking \n\nAffix the CE marking to the high-risk AI system (or, if not possible, on its packaging or accompanying documentation). The CE marking will confirm the conformity of the high-risk AI system with the AI Act as per article 48. See below. \n\nRegistration obligation \n\nComply with EU Database registration obligations (article 49(I)). See below. \n\nCorrective actions / provision of information \n\nIn cases where the system is not in conformity with the AI Act, take the necessary corrective actions, or withdraw, disable, or recall it. Where the system presents a risk to safety, or the fundamental rights \n\nof persons, inform the competent market surveillance authorities \n\nand, where applicable, the notified body that issued a certificate for \n\nthat system (article 79). \n\nDemonstration of conformity \n\nUpon a reasoned request of a national competent authority, demonstrate the conformity of the system with the requirements set out in Section 2 (see above), providing all necessary information and documentation. The duties relating to cooperation with competent authorities are set out in more detail in article 21. Any information shared with a national competent authority shall be treated as confidential. \n\nAccessibility requirements \n\nEnsure the high-risk AI system complies with accessibility requirements in accordance with: \n\nâ€¢ Directive (EU) 2016/2102 (on the accessibility of the websites and mobile applications of public sector bodies); and \n\nâ€¢ Directive (EU) 2019/882 (on the accessibility requirements for products and services). 28 1 2 3 4 5 6 7 8 9 10 \n\n# Harmonised standards and conformity assessment procedure for providers of high-risk AI systems \n\nHarmonised standards \n\nHarmonised standards will be published in the Official Journal of the European Union. If the AI system complies with these standards, there will be a presumption of conformity with the requirements for high-risk AI systems in Chapter III, Section 2 (article 40(1). Harmonised standards are highly relevant in practice. Under traditional product safety laws, â€˜manufacturersâ€™ usually follow them to demonstrate compliance with product safety law requirements. This will be similar under the AI Act. The European Commission issued a (draft) \n\nstandardisation request in accordance with article 40(2) to standardisation bodies CEN/CELENEC, requesting these bodies to draft harmonised standards covering the requirements of Chapter III, Section 2 by 30 April 2025. \n\nConformity assessment procedure \n\nThe conformity assessment procedure for high-risk AI systems under article 43 requires providers to demonstrate compliance with the requirements for high-risk AI systems in Section 2 of Chapter III (overview below). \n\nAnnex III high-risk AI systems \n\nHere, the AI Act outlines two primary procedures for conformity assessment. Most providers of high-risk AI systems in Annex III (i.e. those referred to in points 2 to 8 of Annex III), must follow the internal control procedure specified in Annex VI, without involving a notified body. Providers of high-risk AI systems listed in point 1 of Annex III (biometrics), who have applied harmonised standards or common specifications, as referenced in articles 40 and 41 must also follow the internal control procedure sufficient. However, for providers of high-risk biometric systems who have not done this theinvolvement of a notified body is required. \n\nAnnex I high-risk AI systems \n\nIf a high-risk AI system falls under Union harmonisation legislation listed in Section A of Annex I, the conformity assessment procedures from those legal acts apply. The high-risk AI system requirements of Section 2 in Chapter III are integrated into this assessment, and specific provisions of Annex VII also apply. Notified bodies under these legal acts must comply with certain requirements of the AI Act, to ensure consistent oversight. \n\nNew conformity assessments for substantial \n\nmodifications \n\nSubstantial modifications to high-risk AI systems necessitate a new conformity assessment. However, changes that form part of the systemâ€™s predetermined learning process do not count as substantial modifications. \n\nRequirements for high-risk AI systems Focus on Articles 8-15; requirements for high-risk AI systems Compliance with the requirements (Article 8) \n\nArticle 8 emphasises that high-risk AI systems must meet technical and organisational requirements (articles 9-15) throughout their life cycle, considering the intended use and the status of the technology. Itâ€™s crucial to prioritise requirements impacting humans and if suitable trade-offs are not found, the AI system should not be deployed. \n\nRisk management (Article 9) \n\nArticle 9 requires providers to establish a risk management system. This is an ongoing process to identify, analyse, and mitigate foreseeable risks, including designing risk reduction measures, implementing controls, and providing user information and training. The measures taken must be documented and high-risk AI systems tested at appropriate stages to ensure consistent performance. 29 1 2 3 4 5 6 7 8 9 10 \n\nData governance (Article 10) \n\nRobust data governance is a critical component of the technical and organisational requirements for high-risk AI systems. High-quality, representative, and to the best extent possible error-free and complete training, validation, and testing datasets are required to ensure proper functioning and safety of the system. Providers must also take measures to mitigate biases in datasets that could lead to prohibited discrimination, including by processing special categories of personal data under specific conditions. Certified third-party services can be employed for data integrity verification and to demonstrate compliance with the AI Actâ€™s data governance requirements. \n\nTechnical documentation and record keeping (Articles 11 and 12) \n\nArticles 11 and 12 necessitate detailed technical documentation and record-keeping logs throughout the systemâ€™s lifecycle. Providers must prepare this before deployment and regularly update it. It should cover all aspects of the system, including its characteristics, algorithms, data, training, testing, validation, and risk management. High-risk AI systems should also automatically record usage logs to provide traceability and identify potential risks or needed modifications. \n\nTransparency and provision of information (Article 13) \n\nArticle 13 mandates clear, comprehensive instructions for deployers of high-risk AI systems. These instructions should enable deployers to understand and use the systemâ€™s outputs correctly. The systemâ€™s decision-making must be understandable, and details on its identity, characteristics, limitations, purpose, accuracy, risks, capabilities, oversight, maintenance, and expected lifespan must be provided. All documentation should be tailored to the needs and knowledge level of the intended deployers. \n\nHuman oversight (Article 14) \n\nHuman oversight measures must prevent or minimise risks to health, safety, and rights. These measures must be proportionate to the systemâ€™s risks and level of autonomy. Human operators should also be able to override the system if necessary. Oversight can be achieved through: \n\nâ€¢ Built-in system constraints and responsiveness to human operators. \n\nâ€¢ Provider-identified measures for deployers to help them make informed, autonomous decisions. \n\nâ€¢ Oversight approaches can include human-in-the-loop, human-on-the-loop, or human-in-command, depending on the applicationâ€™s risks. \n\nAccuracy, robustness and cybersecurity (Article 15) \n\nArticle 15 mandates that high-risk AI systems must achieve suitable accuracy, robustness, and cybersecurity levels. Accuracy measures include minimising prediction errors, robustness measures ensure systems can handle errors and inconsistencies. Lastly, cybersecurity measures shall protect against unauthorised system alterations in which case compliance can be demonstrated through the EU Cyber Resilience Act for relevant AI systems subject to the EU Cyber Resilience Act. 30 1 2 3 4 5 6 7 8 9 10 \n\n# Obligations for deployers of high-risk AI systems \n\nThe AI Act provides for obligations for deployers of high-risk AI systems (article 26): \n\nTechnical and organisational measures \n\nDeployers must take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems. \n\nHuman oversight \n\nDeployers must assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support. \n\nInput data \n\nWhere the deployer exercises control over input data, that deployer must ensure that the input data is relevant and sufficiently representative. In other words, this principle states the deployerâ€™s responsibility as to the quality of the input data. \n\nMonitoring high-risk AI system \n\nDeployers must monitor the operation of the high-risk AI system based on the instructions for use. Deployers must inform providers in accordance with article 72 relating to post-marketing activities. If the deployer identifies a risk per article 79(1) it will immediately inform the provider, and then the importer or distributor and the relevant market surveillance authorities and suspend the use of that system. If a serious incident is identified, deployers must also immediately inform the provider, and then the importer or distributor and the relevant market surveillance authorities of that incident. \n\nLogs \n\nDeployers of high-risk AI systems must keep logs automatically generated by that high-risk AI system where these logs are under their control, for a period appropriate to the intended purpose of the high-risk AI system. This period is at least six months, unless provided otherwise in applicable Union or national law, in particular on the protection of personal data. \n\nInformation to the workersâ€™ representatives \n\nDeployers who are employers must inform workersâ€™ representatives and the affected workers that they will be subject to the use of the high-risk AI system. \n\nPublic authority deployers \n\nDeployers of high-risk AI systems who are public authorities, or Union institutions, bodies, offices or agencies must comply with the EU Database registration obligations under article 49. \n\nData protection impact assessment \n\nIf deployers of high-risk AI systems are required to perform a data protection impact assessment under article 35 of Regulation (EU) 2016/679 (GDPR) or article 27 of Directive (EU) 2016/680 (Data Protection Enforcement Directive), they must make use of the information provided by the provider under article 13 of the AI Act. \n\nInvestigation for criminal offences â€“ high-\n\nrisk AI system for post-remote biometric \n\nidentification \n\nWithout prejudice to Directive (EU) 2016/680 (Data Protection Enforcement Directive), in the framework of an investigation for the targeted search of a person suspected or convicted of having committed a criminal offence, someone who wishes to deploy a high-risk AI system for post-remote biometric identification must request an authorisation for this use, ex-ante, or without undue delay and no later than 48 hours, from a judicial authority or an administrative authority. \n\nFundamental rights impact assessment for high-risk AI systems \n\nPrior to deploying a high-risk AI system referred to in article 6(2) (i.e. high-risk AI systems detailed in Annex III of AI Act), deployers that are: I.  bodies governed by public law, or II.  private entities providing public services, and in each case are III.  deployers of high-risk AI systems intended \n\nto be used 31 1 2 3 4 5 6 7 8 9 10 \n\na.  to evaluate the creditworthiness of natural persons or establish their credit score (apart from AI systems used for the purpose of detecting financial fraud), and b.  for risk assessment and pricing in relation to natural persons in the case of life and health insurance must perform an assessment of the impact of the use of the system on fundamental rights (FRIA) . There is an exception for high-risk AI systems relating to critical infrastructure. The assessment consists of: \n\nâ€¢ a description of the deployerâ€™s processes in which the high-risk AI system will be used in line with its intended purpose; \n\nâ€¢ a description of the time period within which, and the frequency with which, each high-risk AI system is intended to be used; \n\nâ€¢ the categories of natural persons and groups likely to be affected by its use in the specific context; \n\nâ€¢ the specific risks of harm likely to have an impact on the categories of natural persons or groups of persons identified pursuant to point above, considering the information given by the provider pursuant to article 13; \n\nâ€¢ a description of the implementation of human oversight measures, according to the instructions for use; and \n\nâ€¢ the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal governance and complaint mechanisms. \n\n# Obligations for other parties in connection with high-risk AI systems \n\nMost obligations regarding high-risk systems in the AI Act are directed at providers and deployers. However, there are also a limited set of obligations for other parties: namely, importers and distributors of high-risk AI systems, and suppliers of any systems, tools, services, components or processes which are used or integrated in high-risk AI systems. Examples of services by suppliers include model (re)training, testing and evaluation and integration into software (recital 88). The obligations do not apply to suppliers that offer the relevant product or service under a free and open-source licence (article 25(4)). Additionally, \n\nit is possible for parties other than the original \n\nprovider of an AI system to be assigned the role of provider of a high-risk AI system by the AI Act. 32 1 2 3 4 5 6 7 8 9 10 \n\nImporters (article 23) Distributors (article 24) Suppliers (article 25) \n\nVerification: before placing the system on the market, verifying that the provider \n\nhas genuinely: \n\nâ€¢ carried out the conformity assessment procedure; \n\nâ€¢ drawn up the technical documentation; \n\nâ€¢ affixed the CE marking and has attached the EU declaration of conformity; \n\nand \n\nâ€¢ appointed an authorised \n\nrepresentative. \n\nVerification: before making the system available on the market, verifying that: \n\nâ€¢ it bears the CE marking; \n\nâ€¢ it is accompanied by a copy of the EU declaration of conformity and instructions for use; and \n\nâ€¢ the provider and the importer, as applicable, have complied with their respective obligations. \n\nProvide assistance: by written agreement, specifying the necessary information, capabilities, technical access and other assistance based on the generally acknowledged \n\nstate of the art, in order to \n\nenable the provider of the high-risk AI system to fully comply with their obligation. The AI Office/Commission may also develop and recommend voluntary model contractual terms between providers of high-risk AI systems and their third-party suppliers (article 25(4)) and recital 90). \n\nRisk flagging: inform the provider, the authorised representative and the market surveillance authority when the system presents a risk 1 to \n\nhealth, safety or fundamental rights of persons. \n\nRisk flagging : not make the system available when the distributor considers or has reason to consider that the system is not in conformity with the requirements set out in Section 2, until the system has been brought into conformity, and where the system presents a risk to health, safety or fundamental rights of persons, immediately inform the provider or the importer of the system and the competent authorities, giving details, in particular, of the non-compliance and of any corrective actions taken. \n\nCare: ensure that storage or \n\ntransport conditions do not jeopardise compliance with the requirements in Section 2. \n\nCare: ensure that storage or \n\ntransport conditions do not jeopardise compliance with the requirements in Section 2. 1.  Risk here means: â€œhaving the potential to affect adversely health and safety of persons in general, health and safety (...) to a  \n\n> degree which goes beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the product concerned, including the duration of use and, where applicable, its putting into service, installation and maintenance requirementsâ€ (article 79(1) AI Act in conjunction with Article 3(19) of Regulation (EU) 2019/1020 (Market surveillance regulation).\n\nObligations for importers, distributors and suppliers \n\nArticles 23, 24 and 25 set out the obligations for importers, distributors and suppliers: 33 1 2 3 4 5 6 7 8 9 10 \n\nImporters (article 23) Distributors (article 24) Suppliers (article 25) Cooperation with authorities: upon a \n\nreasoned request, provide competent authorities with all necessary information/ documentation, including technical documentation, to demonstrate conformity of the system and cooperate with these authorities in any action they take in relation to the system. \n\nCooperation with authorities: upon a reasoned \n\nrequest, provide competent authorities with all necessary information/documentation \n\nregarding their obligations \n\nin the rows above to demonstrate the conformity of that system, and cooperate with these authorities in any action they take in relation to the system. \n\nRecord keeping: keep, for a period of ten years after the system has been placed on the market/put into service, a copy of: the certificate issued by the notified body (in the event of third-party conformity assessment), the instructions for use and the EU declaration of conformity. \n\nContact details: indicate name, registered trade name or registered trademark and the address at which the importer can be contacted on the system and its packaging or accompanying documentation. \n\nCorrective actions: take the corrective actions necessary to bring the system into conformity, where the distributor considers or has reason to consider the system not to be in conformity with the requirements set out in Section 2, or withdraw or recall the system, or ensure that the provider, the importer or any relevant operator, as appropriate, takes those corrective actions. 34 1 2 3 4 5 6 7 8 9 10 \n\nBecoming a provider of someone elseâ€™s (high-risk) AI system \n\nArticle 25(1) provides that a person will be considered the provider of a high-risk AI system, even if that person was not originally the provider of the AI system, when that person: \n\nâ€¢ places their name or trademark on a high-risk AI system which is already placed on the market or put into service; \n\nâ€¢ makes a substantial modification 2 to an \n\nexisting high-risk AI system in such a way that it remains high-risk; and/or \n\nâ€¢ modifies the intended purpose of an AI system of an AI system which is not currently high-risk so that it becomes high-risk. If any of these three situations occur, the original provider will no longer be considered the provider of the (new or newly used) AI-system. One situation which often occurs in practice \n\nthat could lead to such switching of provider roles is the deployment of a general-purpose \n\nAI system by a deployer in a way that falls \n\nwithin the high-risk category as set out in article 6 (and Annexes I and III). As such, if a person deploys a general-purpose AI system in a high-risk way, that deployer assumes the responsibilities of a provider. The new provider will assume all the obligations of a provider of a high-risk AI system. The original provider is obliged to closely cooperate with the new provider and make available the necessary information and provide reasonably expected technical access and other assistance to the new provider to bring the system into conformity with the AI Act (article 25(2)). If, however, that original provider had â€clearly specifiedâ€ that the AI system was not to be changed into a high-risk AI system (article 25(2)) or â€œexpressly excluded the change of the AI system into a high-risk AI systemâ€ (recital 86), for example by prohibiting deployment for high-risk purposes in the applicable contract(s), then that original provider is not obligated to do 2.  A â€˜substantial modificationâ€™ is defined in article 3(23) as â€œa change to an AI system after its placing on the market or putting into service which is not foreseen or planned in the initial conformity assessment carried out by the provider and as a result of which \n\n> the compliance of the AI system with the requirements set out in Chapter III, Section 2 is affected or results in a modification to the\n> intended purpose for which the AI system has been assessedâ€ . The Commission will provide further guidelines on the practical implementation of the provisions related to substantial modification (Article 96(1)(c)). Recital 84 also provides that provisions established in certain Union harmonisation legislation based on the New Legislative Framework, such as the Medical Device Regulation, should continue to apply. For example, article 16(2) of the Medical Device Regulation provides that certain changes should not be modifications of a device that could affect its compliance with the applicable requirements, and these provisions should continue to apply to high-risk AI systems which are medical devices within the meaning of the Medical Device Regulation.\n\nthis. If high-risk deployment is not prohibited, then the co-operation obligation applies, but is without prejudice to the need to observe and protect intellectual property rights, confidential business information and trade secrets (article 25(5)). As such, the original provider does not have to help to the extent that it compromises their own intellectual property rights or trade secrets (recital 88). The Commission will provide guidelines on the application of the requirements and obligations referred to in this article 25 (article 96(1)(a)). 35 1 2 3 4 5 6 7 8 9 10 \n\nScope of high-risk systems article 6, Annexes I \n\nand III \n\nrecitals 46-63 \n\nRequirements for providers of high-risk AI systems articles 8-22, 43, \n\n47-49 \n\nrecitals 64-83, \n\n123-128, 147, 131 \n\nRequirements for deployers of high-risk AI systems article 26, 27 recitals 91-96 \n\nRequirements for importers of high-risk AI systems article 23 recitals 83 \n\nRequirements for distributors of high-risk AI systems article 24 recitals 83 \n\nRequirement for third-party suppliers to high-risk systems \n\narticle 25 recitals 83-90 \n\nStandards article 40, 41 recital 121 \n\nConformity assessment procedure article 28 recital 149 \n\n# Where can I find this? 36 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ General-purpose AI models are versatile AI components demonstrating immense generality in the tasks they can handle, particularly encompassing current generative AI models. \n\nâ€¢ Fine-tuning and modification of general-purpose AI models may result in new general-purpose AI models. \n\nâ€¢ Providers of general-purpose AI models are tasked with a number of transparency obligations both towards the AI Office and competent authorities as well as towards AI systems providers intending to integrate their AI systems with general-purpose AI models. \n\nâ€¢ General purpose AI models that pose systemic risks, i.e., the most versatile and powerful models to date, are under heightened evaluation, transparency, security, risk assessment and incident management obligations. The classification procedure for general-purpose AI models with systemic risk should be a key area of focus for general-purpose AI models providers. \n\nâ€¢ The development and publication of codes of practice will help general-purpose AI models providers identify specific technical and organisational measures to implement in order to comply with their obligations. \n\nâ€¢ Provisions regarding general-purpose AI models will apply from 2 August 2025. For providers of general-purpose AI models: undertake a thorough governance review and make necessary adjustments to ensure compliance â€“ the obligations for providers of general-purpose AI models are among the strictest in the AI Act. For providers of general-purpose AI models: conduct a comprehensive legal IP assessment â€“ regulations for general-purpose AI models are heavily intertwined with IP laws, particularly regarding the copyright policy and the various training data obligations. For providers of general-purpose AI models: continuously and closely \n\nmonitor the thresholds for â€œsystemic risk,â€ \n\nas these may be adjusted over time via delegated acts. For providers of general-purpose AI models: keep an eye out for the development and publication of codes of practice, which will include specific and technical details on how to comply with the obligations for general-purpose model providers in practice. Sign up to our Connected newsletter and keep up with the latest developments here! \n\nFamiliarise yourself with the concepts of general-purpose AI models, general-purpose AI systems, AI systems, and high-risk AI systems â€“ and their relation to each other. This understanding is crucial for assessing which systems your company uses or markets and for making informed legal evaluations. \n\n# To do list At a glance \n\nCHAPTER 5 \n\n# General-purpose AI models 37 1 2 3 4 5 6 7 8 9 10 \n\n# Background and relevance of general-purpose AI models \n\nOne of the most prominent debates in the legislative process of the AI Act revolved around the regulation of general-purpose AI. The first draft of the AI Act (the Commissionâ€™s proposal of April 2021) was based on the understanding that each AI system is created for a specific purpose, and that this purpose can be associated with a specific risk potential. This classification did not have in mind foundation models which are trained on broad data such that it can be applied across a wide range of use cases. These AI models did not fit into the risk-based scheme of the first draft of the AI Act. The categorisation had to be expanded to include a new category that took into account the specific capabilities and dangers of such models. In the summer of 2023, the â€œfoundation modelâ€ (later \n\nrenamed general-purpose AI) was added to the then-current draft of the AI Act. The AI Actâ€™s chapter on the regulation of general-purpose AI models holds significant importance for two main reasons: \n\nâ€¢ firstly, it addresses generative AI, a subset of AI that is currently opening up the most intriguing new opportunities in the business environment and encompasses the majority of corporate use cases; and \n\nâ€¢ secondly, the requirements for general-purpose AI under the AI Act, alongside those for high-risk AI systems, are the most demanding in the AI Act, necessitating the utmost diligence in corporate implementation. This significance is only somewhat diminished by the fact that all requirements are directed solely at providers, not deployers. \n\n# Terminology and general-purpose AI value chain \n\nGeneral-purpose AI models and general-purpose AI systems \n\nArticle 3(63) outlines the characteristics of a general-purpose AI model, emphasising its versatility and competence across various tasks. Recital 98 highlights two key indicators: 1.  having at least a billion parameters; and 2.  being trained with a large amount of data using self-supervision. These models are distinguished by their ability to integrate into and function within diverse downstream systems or applications. Typically, general-purpose AI models undergo extensive training with large datasets, often utilising methods like self-supervision at scale. Recital 99 further specifies that large generative AI Models, such as LLMs or Diffusion Models, are typical examples of general-purpose AI models. Recital 97 clarifies that while general-purpose \n\nAI models are crucial components of AI systems, they are not AI systems themselves. Additional elements, such as user interfaces, are needed to transform general-purpose AI models into fully operational AI systems. A general-purpose AI system is an AI system built upon a general-purpose AI model, maintaining its versatility across various tasks (article 3(66) and recital 100). To clarify with an example, a system that solely performs translations would likely not qualify as a general-purpose AI system. \n\nGeneral-purpose AI systems and high-risk AI systems \n\nRecital 85 emphasises that general-purpose AI systems, due to their versatility, may function as high-risk AI systems or as components within them. Providers of general-purpose AI systems must collaborate closely with providers of high-risk AI systems to ensure compliance with the AI Act and to distribute responsibilities fairly along the AI value chain (see Chapter 4 for more on high-risk systems). 38 1 2 3 4 5 6 7 8 9 10 \n\nModification and fine-tuning of general-\n\npurpose AI models \n\nModifying or fine-tuning a general-purpose AI model, where a new specialised training data set is fed into the model to achieve better performance for specific tasks, does not transform it into a general-purpose AI system; it remains an abstract model without an interface. Instead, such actions create a modified or fine-tuned general-purpose AI model. Recital 97 and recital 109 specify that a provider who modifies or fine-tunes a general-purpose AI model has limited obligations related only to the changes made, including providing technical documentation or a summary of the training data used. \n\n# Obligations for providers of general-purpose AI models \n\nA provider of a general-purpose AI model that places such a model on the market, or integrates it with its own AI system and places it on the market or puts it into service, is obliged to: a.  prepare and maintain up-to-date technical documentation containing i.a. a description of the model and information on its development process (including training, testing and validation) for the purpose of making it available to the AI Office and competent authorities (article 53(1)(a)) â€“a list of the minimum information required is provided in Annex XI; b.  prepare, maintain up-to-date and make certain information and documentation available to downstream AI systems providers (i.e. those who wish to integrate their AI systems with the general-purpose AI model) so that they can understand the modelâ€™s characteristics and comply with their own obligations (article 53(1)(b)) â€“ a list of the minimum information required is provided in Annex XII; providers are allowed to balance the information they share against their need to protect confidential business information and trade secrets; c.  establish a policy to comply with the EU regulations on copyright and related rights (article 53(1)(c)), taking into account, i.a., the right to opt-out of text and data mining as provided for in article 4(3) of Directive (EU) 2019/790 -on copyright and related rights in the Digital Single Market (the AI Act does not specify other matters that have to be addressed in the policy); d.  prepare and publicly share a comprehensive summary on the data used for training the model (article 53(1)(d)) â€“ the AI Office is tasked with providing a template for this purpose; as the recital 107 explains the summary should allow interested parties to exercise their rights by, for example, listing main data collections, databases or data archives used; e.  cooperate with the relevant authorities when they exercise the powers granted to them under AI Act (article 53(3)); and f.  if the provider is established outside the EU: appoint an authorised representative in the EU (article 54(1)). If a provider releases a general-purpose AI model under a free and open-source licence and makes relevant information publicly available, it is not obliged to fulfil the requirements listed in a-b and f above â€“ unless the general-purpose AI model is qualified as presenting a systemic risk (article 53(2) and article 54(6)). \n\n# General-purpose AI models with systemic risk \n\nQualification criteria \n\nThe AI Act introduces specific heightened obligations for general-purpose AI models \n\npresenting â€œsystemic risksâ€ , e.g. reasonably foreseeable negative effects relating to major accidents, disruption of critical sectors, serious consequences to public health and safety, public and economic security, democratic processes, the dissemination of false or discriminatory content, etc. (recital 110). According to article 51(1) of the AI Act, a general-purpose AI model is classified as a general-purpose AI model with systemic risk if it meets one of these two conditions: (a) it has â€œ high impact capabilities â€ evaluated on the basis of technical tools and methodologies, or (b) is designated by the Commission as having capabilities or impact equivalent to those set out in point (a) having regard to the criteria set out in Annex XIII of the AI Act. These criteria notably 39 1 2 3 4 5 6 7 8 9 10 \n\ninclude the number of parameters of the model, the quality or size of the data set, the amount of computation used for training, the modelâ€™s impact on the European market, the number of registered users the EU. In addition, a model is presumed to have â€œ high impact capabilities â€ if it is trained with more than 10^25 floating point operations, i.e., massive computing powers (article 51(2)). At the time of this Guide, only a handful of Large Language Models seem to meet this threshold. Article 52 of the AI Act sets out the classification procedure. Most notably, providers of general-purpose AI models which meet the systemic risk classification conditions must notify the Commission without delay, and at the latest within two weeks after that requirement is \n\nmet or it becomes known that it will be met. Providers may present arguments to demonstrate that their models do not pose systemic risks despite meeting the requirements. Should such arguments be rejected by the Commission, the concerned models will be considered as presenting systemic risks. Upon â€œreasoned requestâ€ of \n\na provider, the Commission may decide to reassess the classification (article 52(5)). A list of general-purpose AI models with systemic risk will be published and updated by the Commission (article 52(6). \n\nObligations for providers of general-purpose AI models with systemic risk \n\nIn addition to the general requirements applicable to all general-purpose AI models providers, the AI Act imposes additional heightened obligations on providers of general-purpose AI models with systemic risk (articles 53(1) and 55(1)). These obligations apply prior to the modelsâ€™ placing on the market and throughout their entire lifecycle, and relate to: \n\nâ€¢ models evaluation; \n\nâ€¢ assessment and mitigation of systemic risks; \n\nâ€¢ incident management and reporting; \n\nâ€¢ increased level of cybersecurity protection; \n\nand \n\nâ€¢ extended technical documentation. 40 1 2 3 4 5 6 7 8 9 10 \n\nThe AI Act classifies AI systems by risk level, with increased transparency demands for high-risk categories. Transparency is required for high-risk AI systems before they are placed on the market or put into service. See Chapter 4 of this guide for more details regarding the transparency requirements for high-risk AI systems. Additionally, the AI Act mandates transparency requirements under article 50 for specific \n\ntypes of products, requiring that adequate information be provided to individuals, by \n\neither providers or deployers. \n\nâ€¢ Disclaimers: providers of AI systems intended to interact directly with individualsâ€™ need to design and develop them, so that the individuals will be informed about the fact that they are interacting with an AI system. \n\nâ€¢ Marking requirement: providers of \n\nAI systems must mark AI-generated \n\ncontent (audio, images, videos, text) in \n\na way that distinguishes it from human-generated content. \n\nâ€¢ Deepfake marking: AI-generated content (images, audio, video) that resembles real entities and could mislead people into believing it is authentic must be labelled. \n\nâ€¢ Emotion recognition system/ biometric categorisation system: deployers of AI-systems should make individuals aware of the operation of these systems. The AI Actâ€™s transparency obligations collate \n\nwith the other regulatory framework in the EU. \n\nIn particular, there is some overlap between \n\nthe transparency requirements of the GDPR \n\nand the AI Act, although the latter is more technical in nature. Deepfakes: label as â€˜Deepfakeâ€™ in a clear and distinguishable manner to disclose their artificial creation or manipulation. Implement Marking: ensure AI-generated content is marked in a machine-readable format. \n\nFor providers For deployers \n\n# To do list At a glance \n\n> CHAPTER 6\n\n# Transparency \n\nImplement Disclaimers: ensure proper disclaimers are added to AI systems intended to interact directly with individuals. Emotion recognition system/ biometric categorisation system: make individuals aware that such a system is operating. 41 1 2 3 4 5 6 7 8 9 10 \n\n# General transparency obligations \n\nâ€¢ Form: in practice, providers can design a disclaimer in different forms (e.g. as an avatar, icon or interface), as long as it provides clear information that the individual is interacting with an AI system. \n\nExemptions \n\nâ€¢ Obvious cases: if, considering the circumstances and the context of the AI system use, it is obvious for an individual who is reasonably well-informed, observant and circumspect that they are interacting with an AI system, then the system is exempt from this transparency requirement. \n\nâ€¢ Legal use: AI systems that are permitted by law for use in detecting, preventing, investigating, or prosecuting criminal \n\nactivities, that are subject to appropriate safeguards for the rights and freedoms of third parties, are also exempt from these transparency requirements, unless those systems are available for the public to report \n\na criminal offence. \n\nMarking of AI-Generated Content (article 50(2) AI Act) \n\nArticle 50(2) of the AI Act mandates that providers of AI systems, including general-purpose AI systems, must appropriately mark synthetic content such as audio, images, videos, or text. Recital 133 explains the rationale: with AI technology advancing, AI-generated synthetic content is becoming increasingly indistinguishable from human-generated content, posing the risk of misinformation, manipulation, fraud, impersonation, and consumer deception. \n\nThe Marking Obligation \n\nâ€¢ Marking : only providers of AI systems are required to mark AI-generated content. This requirement does not extend to deployers or other users of the content. \n\nâ€¢ Format : the output must be marked in a machine-readable format to indicate that it is artificially generated or manipulated. The AI Act acknowledges the importance of transparency in the use of AI systems. Individuals \n\nshould be enabled to understand the AI \n\nsystemâ€™s design and use, and there should be accountability for decisions made by companies and public authorities. Transparency is also essential for creating public trust in AI systems and ensuring their responsible deployment. Transparency also enhances the broader concept of â€˜AI literacyâ€™, developing awareness about the opportunities and risks of AI and the possible harm it can cause. Such awareness should especially be developed amongst: \n\nâ€¢ individuals concerned, giving them a better understanding of their rights in the context \n\nof AI, and \n\nâ€¢ deployers, allowing them to deploy AI systems in an informed way. Providers, and in certain circumstances deployers as well, have their own transparency requirements. The AI Act classifies AI systems by risk level, with increasing transparency demands for higher risk categories. The transparency requirements for specific types of products are described below. \n\nProvider Obligations: \n\nChatbots (article 50(1) AI Act) \n\nArticle 50(1) of the AI Act mandates that providers of AI systems need to ensure that such systems intended to interact directly with individuals are designed and developed such that the individuals concerned are informed that they are interacting with an AI system. \n\nâ€¢ Target audience: when implementing that obligation, the provider should identify not only \n\nthe intended but also the broader potential \n\ntarget audience to whom the disclaimer may be displayed. The characteristics of individuals belonging to vulnerable groups due to their age or disability should be taken into account, to the extent the AI system is also intended to interact with those groups. The intended or potential target audience has a significant impact on accessibility considerations. 42 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ Technical standards : the markings should be effective, interoperable, robust, and reliable. Providers need to consider the type of content, implementation costs, and current technical standards. \n\nMarking methods :\n\nâ€” Watermarks: visible watermarks can be easily implemented â€“ but also removed with basic editing tools, whereas invisible watermarks require specialised software for detection and removal. \n\nâ€” Metadata: this provides information \n\nabout the fileâ€™s creation and origin but \n\ncan be easily altered or removed with file editing tools. \n\nâ€” Algorithmic fingerprints: AI models leave unique traces or anomalies in the content they generate. For instance, AI-generated images might have minor distortions in textures or patterns, and AI-created audio files could display unnatural pauses or \n\ntonal shifts. \n\nâ€” Cryptographic signatures: digital \n\nsignatures embedded using cryptographic methods, such as a cryptographic hash \n\nthat verifies content authenticity. Even minor changes in the data result in a different hash, ensuring easy verification \n\nof alterations. Numerous tools and initiatives exist to manage and detect AI-generated content. Certain platforms use deepfake detection software that analyses algorithmic patterns and embedded metadata, while others rely on metadata and cryptographic hashes to authenticate the source of the content. For example, platforms might use voice analysis tools to detect synthetic audio, or employ blockchain technology to track the origin of and modifications to digital art. \n\nExemptions \n\nâ€¢ Editorial assistance: AI systems that mainly provide support for routine editing tasks or do not significantly change the original input data are exempt from the marking obligation. \n\nâ€¢ Legal use: AI systems that are authorised for use in detecting, preventing, investigating, or prosecuting criminal activities are also exempt from the marking requirement. \n\nDeployer obligations: \n\nEmotion recognition/ biometric categorisation systems (article 50(3) AI Act) \n\nArticle 50(3) of the AI Act sets forth specific transparency requirements for deployers of: \n\nâ€¢ Emotion recognition systems: AI systems used for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data, e.g., non-verbal signs such as facial expression. \n\nor \n\nâ€¢ Biometric categorisation systems: AI \n\nsystems used for the purpose of assigning natural persons to specific categories on the basis of their biometric data. Such specific categories can relate, e.g., to aspects such as sex, age, hair colour, eye colour, tattoos, personal traits, ethnic origin, personal preferences and interests. See Chapter 4 of this guide for more details on when the use of emotion recognition systems or biometric categorisation systems is prohibited. When these systems are allowed, deployers must inform the natural persons exposed to them about the use of the system. In particular, individuals should be notified when they are exposed to AI systems that, by processing their biometric data, can identify or infer their emotions or intentions or assign them to specific categories. \n\nExemptions \n\nâ€¢ Legal use: AI systems that are permitted for use in detecting, preventing or investigating criminal activities that are subject to \n\nappropriate safeguards for the rights and \n\nfreedoms of third parties and in accordance with the Union law, are exempt from these requirements. \n\nâ€¢ Biometric categorisation systems of ancillary use: AI systems whose use is ancillary to another commercial service and strictly necessary for objective technical reasons are exempt from these requirements. At present, there are no definitive guidelines on the scope of information that should be provided. Deployers, when using these systems, process personal data in accordance with GDPR and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable, apart from the requirements on 43 1 2 3 4 5 6 7 8 9 10 \n\nthe legal basis of the processing. This means that these regulations also constitute separate transparency obligations for deployers acting as controllers. In such cases, individuals should nevertheless be informed about the processing of their data as required under Article 13 and 14 GDPR. In relation to any automated processing, controllers are expected to additionally explain the logic behind their decision-making. In the case of an AI system, this might be provided as part of an explainability statement â€“ a document providing a non-technical explanation of i.a. why the organisation uses AI, how AI was developed, and how it operates and is used. \n\nDeepfakes (article 50(4) AI Act) \n\nArticle 50(4) of the AI Act sets forth specific labelling requirements for content known as \n\nâ€œDeepfakesâ€ . These obligations are crucial for ensuring transparency when AI systems are used to generate or manipulate content. \n\nDefinition of Deepfakes (article 3(60) AI Act) \n\nDeployers using AI to create content that: \n\nâ€¢ generates or manipulates images, audio, \n\nor video; \n\nâ€¢ significantly resembles real people, objects, places, entities, or events; and \n\nâ€¢ could mislead a person into believing the content is authentic or truthful. Examples of Deepfakes: \n\nâ€¢ Deepfake video calls mimicking company executives to trick employees into transferring large sums of money. \n\nâ€¢ AI-generated audio of politicians misleading voters about election dates via robocalls. \n\nâ€¢ Deepfake video ads impersonating political figures to manipulate public opinion on social media. \n\nâ€¢ Fake Zoom interviews using deepfake technology to impersonate high-profile individuals. \n\nâ€¢ Digital avatars delivering fabricated news reports to deceive viewers. \n\nLabelling requirements \n\nThe AI Act mandates that any content generated or manipulated by AI systems must be clearly and distinguishably labelled to disclose its artificial creation or manipulation. This requirement aims to ensure transparency and prevent the public from being misled by such content. At present, there are no definitive guidelines on how content should be labelled. This issue is likely to be addressed in future Codes of Conduct. Techniques such as watermarks, metadata identifications, fingerprints or other methods should be employed to indicate the contentâ€™s artificial nature (see recital 133). It is crucial that these labels are easily, instantly and constantly visible to the audience. For instance, in the case of videos, pre-roll labels or persistent watermarks may be used to meet these requirements effectively. \n\nExemptions \n\nThere are certain reliefs and exceptions to the labelling requirements under article 50(4) AI Act: \n\nâ€¢ The transparency requirements are more relaxed for artistic, creative, satirical, fictional, or similar works. Examples of such works include AI-generated movies or parodies, digital art exhibits, and AI-generated music videos. In these instances, the obligation is to disclose the AI involvement in a manner that does not disrupt the viewerâ€™s experience. This can be achieved through subtle watermarks, brief audio disclaimers, or notes in the description texts on digital platforms. \n\nâ€¢ The obligation to label AI-generated content does not apply if the AI systemâ€™s use is legally authorised for the purposes of detecting, preventing, investigating or prosecuting criminal offences. \n\nâ€¢ The labelling obligation may not apply if the AI-generated content has undergone human review or editorial control, with a natural or legal person holding editorial responsibility for the publication. This means that, if a human has reviewed and approved the AI-generated content, ensuring its accuracy and integrity, the stringent labelling requirements may be relaxed. This exception recognises the role of human oversight in maintaining the quality and reliability of AI-generated content. 44 1 2 3 4 5 6 7 8 9 10 \n\n# Transparency obligations for high-risk AI systems \n\nArticle 50(6) explains that the transparency \n\nobligations outlined here operate alongside \n\nother regulatory requirements. They neither replace nor reduce the obligations specified in Chapter III or other transparency requirements under EU or national legislation. See Chapter 4 of this guide for more details. \n\n# Timing and format \n\nAll the information required to meet the transparency obligations under article 50 must be provided to the individuals concerned: \n\nâ€¢ in a clear and distinguishable manner; \n\nâ€¢ by no later than the time of their first interaction or exposure to the AI system; and \n\nâ€¢ in conformity with the applicable accessibility requirements. The accessibility requirement means that the information should be accessible to diverse audiences, including individuals with disabilities. In practice, this may imply that, depending \n\non the circumstances, disclaimers or other marking methods will have to be displayed not only in written form but also in aural and (audio) visual form. Another aspect to be taken into account is that the individual should be provided with an amount of information that is clear and adequate but not overwhelming. \n\n# Transparency obligations at the national level and codes of practice \n\nThe transparency obligations outlined in article 50(1)-(4) AI Act are designed to coexist with \n\nother regulatory requirements, according to article 50(6) AI Act. They neither replace nor diminish the requirements set forth in \n\nChapter III or other transparency mandates under Union or national law. The AI Office is responsible for promoting and facilitating the development of codes of practice to support the effective implementation of the transparency obligations under article 50(1)-(4) AI Act at the EU level, under article 50(7) AI Act. These codes are intended to clarify the methods for detecting and labelling AI-generated content, to enhance cooperation throughout the \n\nvalue chain, and to ensure that the public \n\ncan clearly distinguish between content \n\ncreated by humans, and content generated \n\nby AI (recital 135). \n\n# Relationship with other regulatory frameworks \n\nâ€¢ The AI Actâ€™s marking obligations under article 50(2)-(4) support the Digital Services Actâ€™s (DSA) requirements for very large online platforms (VLOP) and search engines (VLOS) \n\nto identify and mitigate the risks associated with the dissemination of deepfakes (article 33 et seq. DSA). If the AI provider is separate from the VLOP or VLOS, these markings enable the platforms to recognise AI-generated content more efficiently. Conversely, if a VLOP or VLOS is also the AI provider, their DSA obligations are further detailed and enhanced by the AI Act. \n\nâ€¢ The transparency regulations for deepfakes will correlate with the European guidelines on misleading advertising (see Unfair Commercial Practices Directive) as well as national criminal provisions on deepfakes. \n\nâ€¢ The AI Actâ€™s transparency obligations also support and supplement the transparency requirements under Regulation (EU) 2016/679. However, the GDPR transparency requirements apply if personal data is processed when using AI technologies at all different stages of the AI lifecycle (e.g. when developing, testing or deploying AI technologies), and apply to controllers. Developers and providers of AI tools will not always be acting in such a role. In such case they may still be obliged to provide specific information to controllers to enable the latter to meet their obligations. 45 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ The AI Act enables the establishment of â€œAI regulatory sandboxesâ€ to provide a controlled environment in which to test innovative AI systems for a limited period before they are placed on the market. \n\nâ€¢ This regime is intended to encourage AI providers (or potential providers) to experiment with new and innovative products under supervision by regulators. There are specific incentives aimed at encouraging participation by SMEs and start-ups. \n\nâ€¢ Each Member State must establish at least one AI regulatory sandbox by 2 August 2026, although this can be done in co-operation with other Member States. \n\nâ€¢ The Commission is expected to adopt implementing acts to set out detailed arrangements for the establishment, operation and supervision of AI \n\nregulatory sandboxes. \n\nâ€¢ The AI Act also provides for â€œreal-worldâ€ \n\ntesting of AI systems, both inside and outside of regulatory sandboxes, subject to certain conditions to protect participants. \n\nâ€¢ The regimes relating to AI regulatory sandboxes and real-world testing are intended to be harmonise across the Union. However, there is the potential for divergent approaches at a national level, leading to a possibility of \n\nâ€œforum shoppingâ€ by providers. You should think about the countries in which you would like to test your AI services/products. Although the AI Act intends to establish a harmonised regime, there may be national differences which make some Member States more appropriate for you than others. Once you decide to participate in an AI regulatory sandbox, you will need to prepare a sandbox plan and follow the guidelines and supervision provided by the relevant national competent authority. If you decide to conduct real-world tests, you will also need to prepare a testing plan and seek approval from the relevant market surveillance authority. When you successfully complete an \n\nAI regulatory sandbox process, you \n\nshould obtain an exit report from the relevant national competent authority. This may be useful to accelerate the conformity assessment process for \n\nyour AI product/service. Participation in AI regulatory sandboxes and real-world testing is voluntary. AI providers should familiarise themselves with the relevant provisions of the AI Act if they intend to participate in a sandbox or real-world tests and should look out for further announcements and guidance on these topics, including detailed arrangements for AI regulatory \n\nsandboxes to be specified by the Commission in due course. \n\n# To do list At a glance \n\n> CHAPTER 7\n\n# AI regulatory sandboxes 46 1 2 3 4 5 6 7 8 9 10 \n\n# AI regulatory sandboxes \n\nThe AI Act enables the creation of â€œregulatory sandboxesâ€ to provide a controlled environment in which to test innovative AI systems for a limited period before they are placed on the market or otherwise put into service. The objectives of the AI regulatory sandbox regime include: \n\nâ€¢ fostering AI innovation while ensuring innovative AI systems comply with the AI Act; \n\nâ€¢ enhancing legal certainty for innovators; \n\nâ€¢ enhancing national competent authority understanding of the opportunities, risks and the impacts of AI use; \n\nâ€¢ supporting cooperation and the sharing of best practices; and \n\nâ€¢ accelerating access to markets, including by removing barriers for SMEs and start-ups. \n\nWhat is a regulatory sandbox under the AI Act? \n\nThe AI Act defines an â€œAI regulatory sandboxâ€ as: \n\nâ€œa controlled framework set up by a \n\ncompetent authority which offers providers \n\nor prospective providers of AI systems the possibility to develop, train, validate and test, where appropriate in real-world conditions, an innovative AI system, pursuant to a sandbox plan for a limited time under regulatory supervision. â€\n\nAI regulatory sandboxes can be established in physical, digital or hybrid form and may accommodate physical as well as digital products. \n\nObligation on Member States to establish AI regulatory sandboxes \n\nThe obligation to establish AI regulatory sandboxes rests with the Member States \n\nand their national competent authorities (see Chapter 8 for more on these). Each Member \n\nState must establish at least one AI regulatory sandbox by 2 August 2026. However, Member States can choose to either (i) establish one \n\nor more AI regulatory sandboxes at national level; (ii) jointly establish a sandbox with the national competent authorities of one or more other Member States or (iii) participate in an existing sandbox. National competent authorities establishing AI regulatory sandboxes should cooperate with other relevant national competent authorities where appropriate and may also involve other actors within the AI ecosystem. The EU Data Protection Supervisor may also establish an AI regulatory sandbox for European Union institutions, bodies, offices and agencies. A list of planned and existing sandboxes will \n\nbe made publicly available by the AI Office. \n\nThe Commission also intends to develop a \n\nsingle interface containing relevant information relating to AI regulatory sandboxes to allow stakeholders to: \n\nâ€¢ interact with AI regulatory sandboxes; \n\nâ€¢ raise enquiries with national competent authorities; and \n\nâ€¢ seek non-binding guidance on the \n\nconformity of innovative AI products, \n\nservices or business models. \n\nWho can participate in AI regulatory sandboxes? \n\nThe sandbox regime is aimed at providers (or prospective providers) of AI systems, although applications can be submitted in partnership with deployers and other relevant third parties. There are specific provisions which are designed to encourage participation by SMEs and start-ups, including: \n\nâ€¢ access to sandboxes should generally be free of charge for SMEs and start-ups; \n\nâ€¢ priority access for SMEs and start-ups with a registered office or branch in the EU; and \n\nâ€¢ SMEs and start-ups should have access to guidance on the implementation of the AI Act and other value-added services. 47 1 2 3 4 5 6 7 8 9 10 \n\nLiability \n\nProviders and prospective providers participating in an AI regulatory sandbox (including SMEs and start-ups) will remain liable for any harm inflicted on third parties as a result of the experimentation taking place in the sandbox. However, administrative fines will not be imposed on prospective providers if: \n\nâ€¢ they observe the relevant sandbox plan and the terms and conditions for their participation; and \n\nâ€¢ follow (in good faith) any guidance given by the national competent authority. \n\nImplementation of the sandbox regime \n\nIn order to avoid fragmentation across the EU, the Commission intends to adopt implementing acts specifying the detailed arrangements for the establishment, operation and supervision of AI regulatory sandboxes, including common principles on: \n\nâ€¢ eligibility and selection criteria for participation; \n\nâ€¢ procedures for the application, participation, monitoring, exiting from and termination \n\nof sandboxes; and \n\nâ€¢ the terms and conditions applicable \n\nto participants. These implementing acts are intended to ensure that AI regulatory sandboxes: \n\nâ€¢ are open to any provider who meets fair and transparent eligibility criteria; \n\nâ€¢ allow broad and equal access and keep up with demand for participation; \n\nâ€¢ facilitate the development of tools and infrastructure for testing and explaining dimensions of AI systems relevant for regulatory learning, such as accuracy, robustness and cybersecurity, as well as measures to mitigate risks to fundamental rights and society at large; \n\nâ€¢ facilitate the involvement of relevant actors within the AI ecosystem (e.g. notified bodies \n\nand standardisation organisations, testing \n\nand experimentation facilities, research and experimentation labs and European Digital Innovation Hubs), and also that participation in an AI regulatory sandbox is uniformly recognised (and carries the same legal effects) across the EU. \n\nNational competent authority obligations \n\nNational competent authorities must: \n\nâ€¢ allocate sufficient resources to ensure their sandbox regime complies with the requirements of the AI Act; \n\nâ€¢ provide guidance to sandbox participants on how to fulfil the requirements of the AI Act; \n\nâ€¢ provide participants with an exit report detailing the activities carried out in the sandbox, results and learning outcomes, which can later be used to demonstrate compliance with the AI Act through the conformity assessment process or relevant market surveillance activities; and \n\nâ€¢ provide annual reports to the AI Office and the Board (see Chapter 8 for more on these), identifying best practices, incidents and lessons learnt. National competent authorities will retain supervisory powers in relation to sandbox activities, including the ability to suspend or terminate activities carried out within a sandbox where it is necessary to address significant risks to fundamental rights or health and safety. \n\nProcessing of personal data within sandboxes \n\nPersonal data which has been lawfully collected for other purposes can be used in an AI regulatory sandbox subject to compliance with various conditions set out in the AI Act (all of which must be met for the relevant processing activities to be permitted). Some of the key conditions include: \n\nâ€¢ the relevant AI system being deployed in the sandbox must be aimed at safeguarding substantial public interest (e.g. public health, energy sustainability, safety of critical infrastructure); \n\nâ€¢ use of the personal data must be necessary and could not be substituted with anonymised or synthetic data; \n\nâ€¢ the personal data must be handled in a separate and protected environment and must be subject to appropriate technical and organisational measures; and 48 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ a detailed description of the process and \n\nrationale behind the training, testing and \n\nvalidation of the AI system is retained, together with the testing results. \n\n# Real-world testing of AI systems \n\nThe AI Act also enables the testing of AI systems in â€œreal-world conditionsâ€, subject to certain conditions. The AI Act defines â€œtesting in real-world conditionsâ€ as follows: \n\nâ€œthe temporary testing of an AI system for its intended purpose in real-world conditions outside a laboratory or otherwise simulated environment, with a view to gathering reliable and robust data and to assessing and verifying the conformity of the AI system with the requirements of [the AI Act] â€. Such real-world testing will not qualify as placing the relevant AI system on the market or putting it into service, provided that the relevant requirements of the AI Act are complied with. (See Chapter 2 for more on these concepts). The AI Act primarily focusses on real-world testing of high-risk AI systems outside of AI regulatory sandboxes. However, the AI Act also contemplates the possibility of AI systems (whether high-risk or not) being subject to real-world testing within the framework of an AI regulatory sandbox, under the supervision of a national competent authority. In both scenarios, the real-world testing must comply with various conditions set out in the AI Act (all of which must be met for the testing to be permitted, although there is greater flexibility where the testing is conducted within a sandbox). Some of the key conditions include: \n\nâ€¢ the proposed real-word tests have been approved by the relevant market surveillance authority and registered in the EU database for high-risk AI systems; \n\nâ€¢ the provider conducting the testing is \n\nestablished in the EU (or has appointed \n\na legal representative established in \n\nthe EU); \n\nâ€¢ testing is limited to a maximum of 6 months (which can be extended for an additional 6 months, although this requirement can be derogated from in relation to real-world testing within a sandbox environment); \n\nâ€¢ participants in the real-world testing are properly protected â€“ they must give informed consent, outcomes must be reversible (or capable of being disregarded) and they must be able to withdraw at any time; and \n\nâ€¢ market surveillance authorities can conduct unannounced inspections on the conduct of real-world tests. Providers and prospective providers will be liable for any damage caused in the course of their real-world testing. \n\nIs there a risk of â€œforum shoppingâ€ in relation to participation in sandboxes and real-world testing? \n\nAlthough the AI Act aims to harmonise the regimes relating to AI regulatory sandboxes and real-world testing across the EU, industry representatives and stakeholders will no doubt closely monitor their development and may elect participate in sandboxes and/or real-world testing in jurisdictions which are perceived to have the most industry-friendly approach (including in how liability relating to participation in sandboxes or real-world testing \n\nis determined). 49 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ The AI Act puts in place a post-market monitoring, reporting and information \n\nsharing process. \n\nâ€¢ Most obligations are on providers of high-risk AI systems who have to have post-market monitoring systems and procedures to report serious incidents. \n\nâ€¢ The serious incident reporting obligations can also sometimes apply to deployers. \n\nâ€¢ The timelines for reporting can be immediate. \n\nâ€¢ Reports need to be made to market surveillance authorities in Member States where the incident occurred; reporting to multiple authorities may therefore be needed. \n\nâ€¢ There is a multi-pronged approach to enforcement: \n\nâ€” The European Data Protection Supervisor handles EU institutions etc. \n\nâ€” The European Commission handles providers of general-purpose AI models. \n\nâ€” Competent authorities in each Member State are otherwise responsible. \n\nâ€¢ Sanctions are tiered, by reference to \n\nthe seriousness of the provision that has \n\nbeen infringed. \n\nâ€¢ Affected persons have a right to explanation of individual decision-making. Providers of high-risk systems should: \n\nâ€¢ Consider if they are already subject to other equivalent obligations; if so, check if you have double reporting obligations or not. \n\nâ€¢ Ensure quality management systems include serious incident reporting procedures. \n\nâ€¢ Ensure these procedures establish the nature of the serious incident (death, serious harm to health, violation of fundamental rights etc) and if they \n\nare widespread. \n\nâ€¢ Identify to whom you would have \n\nto report. Deployers of high-risk systems should: \n\nâ€¢ Develop stand-by procedures so they can report if needed. Providers of high-risk AI systems should: \n\nâ€¢ Watch for the European Commission template post-market monitoring plan, to be adopted by 2 February 2026. \n\nâ€¢ Prepare and implement a post-market monitoring plan. \n\nâ€¢ If already subject to existing post-market monitoring obligations, or a regulated financial services provider, consider if you can integrate your AI Act obligations into these systems. \n\n# To do list At a glance \n\nCHAPTER 8 \n\n# Enforcement and governance 50 1 2 3 4 5 6 7 8 9 10 \n\n> CHAPTER 8\n\nProviders and developers should: \n\nâ€¢ Check for the European Commission guidance due by 2 August 2025. \n\nâ€¢ Keep this under review as it will be \n\nre-assessed. Operators of non-high-risk AI systems \n\nshould: \n\nâ€¢ Ensure they comply with all existing product safety legislation. Providers of general-purpose AI models \n\nshould: \n\nâ€¢ Look out for, and consider responding to the consultation on, the European Commission implementing act relating to the arrangements for enforcement by the European Commission. All organisations in the AI value chain \n\nshould: \n\nâ€¢ Look out for, and consider responding to consultations on, rules relating to enforcement adopted at Member State level. \n\nâ€¢ Note the requirement to cooperate with market surveillance authorities where there is sufficient reason to consider that an AI system presents a risk. \n\nâ€¢ Note that disclosure of training, validation and testing data sets and source code might have to be disclosed. Deployers of high-risk systems should: \n\nâ€¢ Ensure they are able to provide clear and meaningful explanations as to the AIâ€™s decision-making procedure. \n\n# To do list 51 1 2 3 4 5 6 7 8 9 10 \n\n# Overview \n\nThe AI Act outlines a governance framework that provides for the implementation and supervision of both the ex ante requirements for AI systems and ex post surveillance and enforcement. The former is described in preceding chapters. The latter is the subject of this chapter, together with a description of the governance structure. The enforcement regime addresses two types of risk: risks to product safety, and risks to fundamental rights. In relation to the former, the AI Act builds upon existing product safety legislation and is mostly enforced by national market surveillance authorities. Where risks \n\nto fundamental rights are identified, the \n\nmarket surveillance authorities shall inform \n\nand fully cooperate with the relevant national public authorities or bodies protecting fundamental rights. Consistent with the risk-based approach in the AI Act, a multi-layered enforcement structure with different regimes applying to AI systems with varying risks is provided. For high-risk AI systems, the AI Act mandates, firstly, post-market monitoring obligations and, secondly, a requirement to report serious incidents. The serious incident reporting obligations can also sometimes apply to deployers, who should therefore also be aware of them. The marketing surveillance authorities can require operators to take all appropriate measures to ensure that AI systems do not present a risk and, where necessary, can demand the withdrawal of a product or AI system from the market. Very significant fines for non-compliance with the terms of the AI Act can also be levied. For general-purpose AI models, the European Commission has exclusive powers to supervise and enforce the obligations in the AI Act. The governance structure in the AI Act provides for the setting up of new institutional bodies at both the EU level (the AI Office, the European AI Board, the Advisory Forum and the Scientific Panel) and national level (notifying authorities and market surveillance authorities) and the roles and competencies of each of them are outlined. The coordination between these bodies will be key to the effective implementation and enforcement of the AI Act. Topics addressed in this chapter are as follows: \n\nâ€¢ Post-marketing obligations \n\nâ€¢ Market surveillance authorities \n\nâ€¢ Procedures for enforcement \n\nâ€¢ Authorities protecting fundamental rights \n\nâ€¢ General-purpose AI models \n\nâ€¢ Penalties \n\nâ€¢ Remedies for third parties \n\nâ€¢ Governance \n\n# Post-marketing obligations \n\nPost-market monitoring system for high-risk AI systems \n\nSince AI systems have the ability to adapt and continue to learn after they are placed on the market, it is important to monitor their performance once they are put on the market. Recital 155 explains that the aim of the post-market monitoring system is to ensure that providers of high-risk AI systems can consider experience from use of the system, so as to ensure ongoing compliance and improvement of the system. Providers of high-risk AI systems must include a post-market monitoring plan as part of the technical documentation that they draw up before they put the system on the market (articles 72(3) and 11(1)). This plan must be in line with the European Commission template, to be adopted by 2 February 2026. The post-marketing obligations will ensure that any need to immediately apply any necessary corrective or preventative actions are identified (article 3(25)). Article 72 provides that the post-market monitoring system (and the documentation of the system) must be proportionate to the nature of the AI technology and the risks of the systems. This system must actively and systematically collect, document, and analyse relevant data throughout the AI systemâ€™s lifetime, so as to allow the provider to evaluate continuous compliance. The data could be provided by deployers, or 52 1 2 3 4 5 6 7 8 9 10 \n\nby others (although sensitive operational data from law enforcement authority deployers is excluded). Where relevant, the system should also include analysis of interactions with other AI systems, including devices and software. Providers of certain types of high-risk AI systems, who already have post-market monitoring systems in place, can integrate their \n\nobligations under the AI Act into those \n\nexisting systems, provided this achieves an equivalent level of protection. This is the case for high-risk AI systems covered by Union harmonisation legislation listed in Section A \n\nof Annex I (i.e. including certain machinery, toys and medical devices). Itâ€™s also the case for financial institutions who are subject to requirements under Union financial services law regarding their internal governance, arrangements or processes, where these institutions place on the market high-risk AI systems listed in Annex III point 5 (in particular, evaluation of creditworthiness or for risk assessment and pricing in relation to life and health insurance) (article 72(4)). \n\nReporting of information on serious incidents for high-risk AI systems \n\nProviders of high-risk AI systems must report \n\nâ€œserious incidentsâ€ and the providerâ€™s quality management system must contain procedures relating to this (article 17(1)(i)). Ordinarily, deployers of high-risk AI systems must report serious incidents to the provider. However, if the deployer cannot reach the provider, then the serious incident reporting obligations of article 73 apply directly to the deployer (article 26(5)). Accordingly, deployers should also be aware of these provisions. The European Commission is to issue guidance for providers on incident reporting by 2 August 2025 and must keep this under regular review. Serious incidents are defined at article 3(49) and mean an incident or malfunctioning of an AI system that directly or indirectly causes: \n\nâ€¢ death, or serious harm to a personâ€™s health; \n\nâ€¢ serious and irreversible disruption to management or operation of critical infrastructure; \n\nâ€¢ violation of Union laws protecting fundamental rights; or \n\nâ€¢ serious harm to property or the environment. Serious incidents must be reported within set timelines, as set out below. If necessary, the provider or deployer may submit an initial report, which can be completed later (article 73(5)). \n\nSituation Period \n\nWidespread infringement \n\nOr \n\nSerious incident involving critical infrastructure Immediately â‰¤ than 2 days after awareness of the incident \n\nDeath of a person â‰¤ 10 days after awareness of the serious incident; or Immediately after establishing or suspecting a causal relationship between the serious incident and the AI system if earlier Other situations (i.e. serious harm to health, fundamental rights violations, serious harm \n\nto property or environment â€“ unless these \n\nare widespread) â‰¤ 15 days after awareness of the serious incident; or Immediately after the provider has established a causal link, or the reasonable likelihood \n\nof a link, between the AI system and the \n\nserious incident 53 1 2 3 4 5 6 7 8 9 10 \n\nAfter reporting, the provider must promptly conduct necessary investigations, including a risk assessment and corrective actions. The provider must not do anything that would alter the AI system in a way that may affect any subsequent evaluation of the cause of the incident before it has informed the competent authorities. Reports of serious incidents have to be made to the market surveillance authorities of the Member States where the incident occurred (article 73(1)). It follows that if a serious incident affects multiple Member States or affects multiple sectors so that there are multiple market surveillance authorities within a Member State, then multiple reports will need to be made. The market surveillance authority must take appropriate measures (which can include withdrawal or recall of the product) within seven days of receiving the notification and must also immediately notify the European Commission of any serious incident, whether or not they have taken action (article 73(8/11)). \n\nNon-high-risk AI systems \n\nAI systems relating to products that are not high-risk nevertheless must be safe when placed on the market or put into service. Regulation (EU) 2023/988 on general product safety and Regulation (EU) 2019/1020 on market surveillance and compliance of products apply to all AI systems governed by the AI Act, but these two Regulations provide the safety net for non-high-risk products (recital 166 and article 74(1)). Regulation (EU) 2019/1020 requires all operators to inform the relevant market surveillance authority when they have reason to believe that a product presents a risk under article 3(19) (see definition below). To the list of risks in article 3(19), the AI Act has added risks to fundamental rights of persons (article 79(1)). â€œProduct presenting a risk â€ means a product having the potential to affect adversely health and safety of persons in general, health and safety in the workplace, protection of consumers, the environment, public security and other public interests, protected by the applicable Union harmonisation legislation, to a degree which goes beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the product concerned, including the duration of use and, where applicable, its putting into service, installation and maintenance requirements. \n\n# Market surveillance authorities \n\nMember States play a key role as the enforcement of the AI Act will often require a local presence. Member States must each designate at least one market surveillance authority and one, if there is more than one, of these authorities must be set as a single point of contact vis-Ã -vis the public and other counterparts at Member State and Union level. The Member State shall notify the European Commission of the single point of contact and the European Commission will make a list of them available to the public (recital 153 and article 70(1/2)). The Member States have until 2 August 2025 to comply with these provisions (article 113(b)). \n\nWhich entities are to be designated market surveillance authorities? \n\nMember States have some flexibility in designating market surveillance authorities; they can either establish a new body dedicated to enforcing the AI Act or integrate the requirements of the AI Act into the framework of an existing body already responsible for market surveillance under the Union harmonisation laws listed in Section A of Annex I or the existing bodies regulating financial or credit institutions regulated by Union law (article 74(3/6/7)). However, for high-risk systems in the area of biometrics, law enforcement, migration, asylum and border control management and the administration of justice, Member States must designate either the national Data Protection Authority established by Regulation 54 1 2 3 4 5 6 7 8 9 10 \n\n(EU) 2016/679 or the supervisory authority designated under Directive (EU) 2016/680 \n\n(article 74(8)). Where AI systems relate to products already covered by the Union harmonisation legislation listed in Section A of Annex I and where such legal acts already provide for procedures ensuring an equivalent level of protection and having the same objective as the AI Act, the sectoral procedures shall apply instead of the national level enforcement procedures set out in articles 79 to 83 (see below under the heading â€˜Procedures for enforcementâ€™). In this instance, dual reporting of serious incidents is not required and providers report under those other laws (article 73(9) and 73(10)). These exceptions specifically apply to: \n\nâ€¢ Annex III-type high-risk AI systems, where the provider is subject to Union law that establishes reporting obligations equivalent to those set out in the AI Act. Such equivalence may - for example â€“ exist for critical infrastructure, which is covered by cybersecurity regulations that contain stand-alone incident reporting obligations that might be considered equivalent to those under the AI Act. However, it may not always be clear whether reporting obligations under other Union laws are considered equivalent to the reporting obligations under the AI Act; and \n\nâ€¢ high-risk AI systems that are safety components of devices, or are themselves devices, covered by Regulations (EU) 2017/745 on medical devices and (EU) 2017/746 on in vitro diagnostic medical devices. These both contain reporting obligations, according to which serious incidents must be reported to the competent authorities if they entail (a) the \n\ndeath of a patient, user or other person, (b) the \n\ntemporary or permanent serious deterioration \n\nof a patientâ€™s, userâ€™s or other personâ€™s state of \n\nhealth, or (c) a serious public health threat. However, in both instances, if the infringement relates to a violation of fundamental rights, it must still be notified under the AI Act and the relevant market surveillance authority must inform the national fundamental rights authority/ authorities. For AI systems used by Union institutions, agencies, offices, and bodies (with the exception of the Court of Justice of the European Union acting in its judicial capacity), the European Data Protection Supervisor will be the market surveillance authority (article 74(9)). \n\nPowers of the market surveillance authorities \n\nThe market surveillance authorities have all the broad enforcement powers set out in Regulation (EU) 2019/1020 in addition to further powers granted by the AI Act. For example, they have the power to: \n\nâ€¢ make operators disclose relevant documents, data and information on compliance. The AI Act adds that providers of high-risk AI systems may be compelled to disclose: \n\nâ€” training, validation and testing data sets used for the development of high-risk AI systems, including, where appropriate and subject to security safeguards, through application programming interfaces (API) or other relevant technical means and tools enabling remote access (article 74(12)); and \n\nâ€” where the testing or auditing procedures and verifications based on the data and documentation provided by the provider have been exhausted or proved insufficient, the source code if it is necessary to assess the conformity of a high-risk AI system with the requirements set out in chapter III, Section 2 (article 74(13)); \n\nâ€¢ make unannounced on-site inspections and make test purchases (article 74(5)); \n\nâ€¢ conduct investigations (engaging with the European Commission where high-risk AI systems are found to present a serious risk across two or more Member States) (article 74(11)); \n\nâ€¢ require operators to take appropriate actions to bring instances of non-compliance to an end, both formal non-compliance (article 83) and to eliminate a risk (articles 79-82); \n\nâ€¢ take appropriate measures where an operator fails to take corrective action or where the non-compliance persists, including withdrawal or recall (articles 73(8), 79-83); and \n\nâ€¢ impose penalties (articles 99-101). The market surveillance authorities shall also ensure that testing in real world conditions is in accordance with the AI Act (see Chapter 7). They have the power to require the provider or deployer to modify the testing or suspend or terminate it (article 76(3)). 55 1 2 3 4 5 6 7 8 9 10 \n\nHandling of confidential information \n\nAny information or documentation obtained by market surveillance authorities shall be treated in accordance with the confidentiality obligations set out in article 78. The provisions in article 78 also apply to the European Commission, the authorities protecting fundamental rights and natural and legal persons involved in the application of the AI Act. Such persons shall carry out their tasks in a manner which not only protects confidential information and trade secrets, but also protects intellectual property rights and the rights in source code, public and national security interests and classified information. These provisions shall apply from 2 August 2025. \n\n# Procedures for enforcement \n\nAs already noted, the following procedures do not apply where there exists already harmonising legislation providing an equivalent level of protection and having the same objective as the AI Act. \n\nAI systems presenting a risk (articles 79 and 81) \n\nWhere a market surveillance authority has sufficient reason to consider an AI system presents a risk (see definition above), it must carry out an evaluation as to whether the AI system is compliant with the AI Act. If it does not comply, the market surveillance authority shall without undue delay notify the relevant operator and require them to take all appropriate corrective actions to bring the AI system into compliance or to withdraw the AI system from the market, or to recall it. The market surveillance authority shall state how long the operator has to comply, but it will be no longer than 15 working days. If operator does not take adequate corrective action by the end of the specified period, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system being made available on its national market or put into service, to withdraw the product or the standalone AI system from that market or to recall it. The market surveillance authority must inform the operator of the grounds on which its decision is based. Where the non-compliance is not restricted to its national territory, the market surveillance authority shall inform the European Commission and the other Member States without undue delay of the results of the evaluation and of the actions which it has required the operator to take and the provisional measures which it has taken if the operator has not complied. The provisional measures shall be deemed justified if no objection has been raised by either a market surveillance authority of a Member State or by the European Commission within three months (reduced to 30 days in the event of non-compliance with the prohibitions referred to in article 5). However, if objections are raised, the European Commission shall consult with the market surveillance authority and the operator or operators and, within six months (or 60 days for an article 5 issue), decide whether the provisional measure is justified. If it is, all Member States shall ensure that they take appropriate restrictive measures in respect of the AI system concerned, such as requiring withdrawal from their market. If it is not, the provisional measure will be withdrawn. These provisions are without prejudice to the procedural rights of the operator set out in article 18 of Regulation (EU) 2019/1020, including the right to be heard. \n\nAI systems classified by the provider as non-\n\nhigh-risk (article 80) \n\nIf the market surveillance authorities have sufficient reason to consider an AI system classified by the provider as non-high-risk under article 6(3) is indeed high-risk, it must carry out an evaluation. The procedure to be followed is very much as described above, but article 80 specifically refers to the ability to fine the relevant provider. In exercising their power to monitor the application of article 80, market surveillance authorities may take into account the information stored in the EU database of \n\nhigh-risk AI systems (see below under the heading â€˜Governance at Union Level: Role of \n\nthe European Commissionâ€™). 56 1 2 3 4 5 6 7 8 9 10 \n\nCompliant AI systems which present a risk (article 82) \n\nIf a market surveillance authority finds that a high-risk AI system complies with the AI Act, but it nevertheless presents a risk to the health or safety of persons, to fundamental rights, or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that it no longer does so. \n\nFormal non-compliance (article 83) \n\nWhere a market surveillance authority finds that, for example, a CE marking has not been affixed where it should, no authorised representative has been appointed or technical documentation is not available, it shall require the relevant provider to correct the matter within a prescribed period. If the non-compliance persists, then the market surveillance authority shall take appropriate and proportionate measures to restrict or prohibit the high-risk AI system being made available on the market or to ensure that it is recalled or withdrawn from the market without delay. \n\n# Authorities protecting fundamental rights \n\nIn addition to identifying market surveillance authorities, by 2 November 2024, each Member State must identify the public authorities or bodies supervising and enforcing the obligations under Union law protecting fundamental rights, including the right to non-discrimination, in relation to the use of high-risk AI systems referred to in Annex III and shall notify them to the European Commission. Where market surveillance authorities identify risks to fundamental rights they must notify the relevant national public authority supervising their protection. These bodies have the power to request and access any documentation created or maintained under the AI Act when access to that documentation is necessary for effectively fulfilling their mandates. The relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request and, where the documentation proves insufficient may request the market surveillance authority to organise testing of the high-risk AI system through technical means (article 77). \n\n# General-purpose AI models \n\nThe European Commission is the sole authority responsible for supervising and enforcing obligations on providers of general-purpose AI models. The rationale behind this is to benefit from centralised expertise and synergies at Union level (article 88). In practice, however, the AI Office (see below under the heading â€˜Governanceâ€™) will carry out all necessary actions to monitor the effective implementation of the AI Act with regard to general-purpose AI models, provided that the organisational powers of the European Commission and the division of competences between Member States and the Union are not affected. The AI Office can investigate possible breaches of the rules by providers of general-purpose AI models on its own initiative, following the results of its monitoring activities, or at a request from market surveillance authorities. It has the powers of a market surveillance authority for AI systems which are based on a general-purpose AI model, where the model and system are developed by the same provider. Market surveillance authorities must cooperate with the AI Office to carry out compliance evaluations if a market surveillance authority considers that a general-purpose AI system (that can be used by deployers for at least one high-risk purpose) is non-compliant with the AI Act. Market surveillance authorities can request the AI Office to provide information related to general-purpose AI models, where the market surveillance authority is unable to access that information (and as a result is unable to conclude its investigation into a high-risk system) (article 75). \n\n# Penalties \n\nAny person, which fails to comply with the AI Act - whether a natural or legal person, a public authority or an EU or national institution - can be sanctioned for non-compliance. The provisions on penalties under the AI Act exceed even those provided for in the GDPR (which are up to EUR 57 1 2 3 4 5 6 7 8 9 10 \n\n20,000,000 or 4% of annual worldwide turnover). The maximum fine was revised throughout the legislative process but was ultimately set at EUR 35,000,000 or 7% of annual worldwide turnover. Fines can be imposed by national authorities, the European Data Protection Supervisor, or the European Commission. The European Data Protection Supervisor can impose fines on Union institutions, agencies and bodies. The European Commission can impose fines on providers of general-purpose AI models. National authorities can impose fines on other operators. The AI Act has a tiered approach to penalties, as shown below. \n\nGrounds of infringement EU bodies All other persons \n\nPenalties imposed \n\nby EDPS Penalties imposed by national authorities (unless GPAI models, in which case imposed by the European Commission). For sanctioned persons which are undertakings, the penalties are capped at the higher of the %-based amount or the figure below. If the undertaking is an SME, they are capped at the lower amount. For other sanctioned persons, the specified figure is the cap. Supplying incorrect, incomplete or misleading information to notified \n\nbodies or national \n\ncompetent authorities. â‰¤ â‚¬750,000 (article 100(3)) â‰¤ 1% total worldwide annual turnover in preceding year; or â‰¤ â‚¬7,500,000 (article 99(5)) \n\nObligations relating to \n\nhigh-risk AI systems. â‰¤ 3% of total worldwide annual turnover in preceding year; or â‰¤ â‚¬15,000,000 (article 99(4) for high-risk AI systems; article 101(1) for general-purpose AI models) \n\nObligations relating to \n\nproviders of general -purpose AI models. \n\nObligations relating to \n\nprohibited practices. â‰¤ â‚¬1,500,000 (article 100(2)) â‰¤ 7% of total worldwide annual turnover in preceding year; or â‰¤ â‚¬35,000,000 (article 99(3)) Curiously, there appear to be no penalties for failure to comply with the AI literacy obligations at article 4. \n\nPenalties and fines imposed by national \n\nauthorities \n\nIt is the responsibility of Member States to provide for effective, proportionate, and dissuasive sanctions. These measures may include both monetary and non-monetary measures or warnings. They must be notified to the European Commission by the date of entry into application (article 99(1/2)). Penalties are to be imposed on a case-by-case basis. The competent national authority should consider all relevant circumstances of the specific situation, with due regard to the nature, gravity, and duration of the infringement and its consequences, as well as the size of the provider (article 99(7)). 58 1 2 3 4 5 6 7 8 9 10 \n\nEnforcement at Member State level must be subject to appropriate procedural safeguards, including effective judicial remedies. \n\nFines on Union institutions, bodies, offices \n\nand agencies \n\nThe European Data Protection Supervisor has the power to impose fines on Union institutions, agencies and bodies. Before adopting a decision on a fine, the EDPS should communicate its preliminary findings to the Union institution and give it an opportunity to be heard. The fine is not to affect the effective operation of the institution and the funds collected by the imposition of fines are to be contributed to the general budget of the EU. \n\nFines on providers of general-purpose AI models \n\nThe European Commission may impose fines on providers of general-purpose AI models for infringements (article 101). Unlike the other provisions on penalties and fines in chapter XII, which apply from 2 August 2025, article 101 does not apply until 2 August 2026. The European Commission will publish an implementing act with details on arrangements and procedural safeguards for proceedings. When imposing a fixed amount or periodic penalty payment, the European Commission should take due account of the nature, gravity and duration of the infringement, and the principles of proportionality and appropriateness. Before adopting a decision on a fine, the European Commission should communicate its preliminary findings to the provider of the general-purpose AI model and give it an opportunity to be heard. The imposition of a fine must be subject to appropriate procedural safeguards, including judicial review before the Court of Justice of the European Union. The CJEU may cancel, reduce or increase the amount of a fine imposed. \n\n# Remedies for third parties \n\nComplaint to a market surveillance authority (article 85) \n\nUnion and Member State law already provide some effective remedies for natural and legal persons whose rights and freedoms are adversely affected by the use of AI systems. Notwithstanding, the AI Act introduces a new complaints mechanism. It mandates that any natural or legal person may submit a complaint to the competent market surveillance authority if it has grounds for believing there has been an infringement of the AI Act. Compare: Under the GDPR, a data subject has the right to lodge a complaint with a supervisory authority about an alleged infringement if the data subject believes that the processing of personal data relating to him or her violates rights under the GDPR. In contrast, a complaint lodged under the AI Act may concern not only an infringement of the rights of the complainant, but also compliance with the AI Act as a whole. In addition, under the GDPR a remedy can be filed only by the data subjects; under the AI Act, a complaint can also be filed by a legal person. \n\nRight to explanation of individual decision-making (article 86) \n\nUnder the AI Act, any affected person is entitled to receive â€œclear and meaningfulâ€ explanations \n\nfrom the deployer concerning decisions made by high-risk AI systems (except for critical infrastructure systems). These explanations must clarify the decision-making procedure used and the main elements of the decision made by the AI system (article 86). The right can be invoked if: \n\nâ€¢ a deployerâ€™s decision is mainly based on the output of high-risk AI systems; and \n\nâ€¢ that decision has legal effects or similarly significant effects on an affected person that adversely affect his or her health, safety or fundamental rights. Compare: The right to an explanation under the AI Act aligns with a controllerâ€™s obligation under the GDPR concerning automated decision-making processes (article 22 GDPR). Under the GDPR, the controller must provide the data 59 1 2 3 4 5 6 7 8 9 10 \n\nsubject with meaningful information on the logic and significance of the consequences of such processing. Article 86 of the AI Act complements the data subjectâ€™s right to an explanation under the GDPR; it is more specific to AI as it requires the deployer to explain the role of the AI system in the decision. In addition, the AI Act grants this right to all affected persons who can also be legal persons. National data protection \n\nauthorities under the GDPR are still the \n\ncompetent authorities to enforce the controllerâ€™s obligation to provide information when it comes to automated decision-making involving personal data processing, regardless of what authority is competent to enforce article 86 of the AI Act. \n\nProtection for whistleblowers (article 87) \n\nDirective (EU) 2019/1937 on the protection of persons who report breaches of Union law applies to the reporting of infringements of the AI Act. \n\nDownstream providersâ€™ complaint (article 89) \n\nThe AI Act enables complaints by downstream providers (deployers of general-purpose AI systems) about possible violations of the rules set out in the Act. Complaints can be made to the AI Office and must be well-substantiated. They should include \n\nat least: \n\nâ€¢ details of the provider of the general-purpose AI model that is the subject of the complaint, and its point of contact; \n\nâ€¢ a description of the relevant facts, together with the provisions that have been breached; \n\nâ€¢ the reasons why the complainant believes there has been an infringement; and \n\nâ€¢ any other information that the requesting downstream provider deems relevant, including, where appropriate, information gathered at its own initiative. The possibility for downstream providers to make such complaints enables the AI Office to effectively oversee the enforcement of the AI Act. \n\n# Governance \n\nThe governance structure has been established to coordinate and support the application of the AI Act. Its aim is to build capabilities at both Union and national levels, integrate stakeholders, and ensure trustworthy and constructive cooperation. \n\nGovernance at Union Level: role of the European Commission \n\nThe European Commission is tasked by the AI Act with many responsibilities including developing and implementing delegated acts, developing and publishing guidelines, setting standards and best practice and making binding decisions to implement the AI Act effectively. In practice, these tasks will be carried out by the AI Office (part of the administrative structure of the Directorate-General for Communication Networks, Content and Technology) in its role of supporting the European Commission. One of the tasks that the European Commission, in collaboration with the Member States, must perform is set out in chapter VIII of the AI Act. The European Commission must set up and maintain an EU database for high-risk AI systems referred to in article 6(2) and AI systems that are not considered as high-risk pursuant to article 6(3). The database will contain: \n\nâ€¢ the data listed in Sections A and B of Annex VIII entered into the EU database by the provider or the authorised representative; and \n\nâ€¢ the data listed in Section C of Annex VIII entered into the EU database by the deployer who is, or who acts on behalf of, a public authority, agency or body. The data will be available to the public (with the exception of data relating to AI systems in the areas of law enforcement, migration, asylum and boarder control management). 60 1 2 3 4 5 6 7 8 9 10 \n\nThe supranational bodies set up by the AI Act \n\nRole of the AI Office Actions \n\nThe AI Office was established by the European Commission by its decision of 24 January 2024 (C/2024/1459). The AI Officeâ€™s function is to oversee the advancements in AI models, including as regards general-purpose AI models, the interaction with the scientific community, and to play a key role in investigations and testing, enforcement and to have a global vocation (recital 5 of the decision). The AI Office may involve independent experts to carry out evaluations on its behalf. The AI Office must establish systems and procedures to manage and prevent potential conflicts of interest and must develop Union expertise and capabilities in the field of AI. The AI Office has a role in the surveillance \n\nand control of general-purpose AI systems (article 75). Monitoring and enforcement: Monitor compliance and implementation of obligations for providers of general-purpose AI models. Investigation: Investigate infringements by requesting documentation and information, conducting evaluations and requesting measures from providers of general-purpose \n\nAI models. Risk management: Request appropriate measures, including risk mitigation, in cases of identified systemic risks, as well as restricting market availability, withdrawing or recalling \n\nthe model. \n\nCoordination and support: Support national \n\nauthorities in creating AI regulatory sandboxes and facilitate cooperation and information-sharing and encourage and facilitate the creation of codes of conduct. Coordinate joint investigations by market surveillance authorities and the European Commission. Advice: Issue recommendations and written opinions to the European Commission and the Board regarding codes of conduct, codes of practice and guidelines. \n\nRole of the European Artificial Intelligence Role of the European Artificial Intelligence \n\nBoard (The Board) Board (The Board) Actions Actions \n\nThe Board comprises representatives from each The Board comprises representatives from each Member State and is tasked with advising and Member State and is tasked with advising and assisting the European Commission and the assisting the European Commission and the Member States on the consistent and effective Member States on the consistent and effective application of the AI Act. Additionally, the Board application of the AI Act. Additionally, the Board issues guidelines and recommendations (articles issues guidelines and recommendations (articles 65 and 66). 65 and 66). Representatives are appointed for a term of three Representatives are appointed for a term of three years, renewable once. They may be individuals years, renewable once. They may be individuals from public entities with expertise from public entities with expertise \n\nin AI and the authority to facilitate national-level in AI and the authority to facilitate national-level coordination. The Board is chaired by one of coordination. The Board is chaired by one of \n\nits representatives. its representatives. Coordination and cooperation: Among Coordination and cooperation: Among national competent authorities and Union national competent authorities and Union institutions, bodies, offices and agencies, as institutions, bodies, offices and agencies, as well as relevant Union expert groups and well as relevant Union expert groups and networks. networks. Expertise sharing: Collect and share technical Expertise sharing: Collect and share technical and regulatory expertise, best practices and and regulatory expertise, best practices and guidance documents. guidance documents. Advice and recommendations: Provide Advice and recommendations: Provide advice on the implementation of the AI Act, advice on the implementation of the AI Act, in particular as regards the enforcement of in particular as regards the enforcement of rules on general-purpose AI models, issue rules on general-purpose AI models, issue recommendations and written opinions (at recommendations and written opinions (at the request of the European Commission or the request of the European Commission or on its own initiative). on its own initiative). 61 1 2 3 4 5 6 7 8 9 10 \n\nRole of the European Artificial Intelligence Role of the European Artificial Intelligence \n\nBoard (The Board) Board (The Board) Actions Actions \n\nThe Board must establish two dedicated standing The Board must establish two dedicated standing \n\nsubgroups: subgroups: \n\nâ€¢ The standing subgroup for notifying authorities provides a  platform for cooperation and exchange on issues related to notified bodies \n\nâ€¢ The standing subgroup for market surveillance acts as the administrative cooperation group (ADCO) for the AI Act. The Board may establish other standing or The Board may establish other standing or temporary subgroups as appropriate for the temporary subgroups as appropriate for the purpose of examining specific issues. purpose of examining specific issues. The European Data Protection Supervisor and The European Data Protection Supervisor and the AI Office attend the Boardâ€™s meetings as the AI Office attend the Boardâ€™s meetings as observers. Other national and Union authorities, observers. Other national and Union authorities, bodies, or experts or representatives of the bodies, or experts or representatives of the advisory forum may be invited on a case-by-case advisory forum may be invited on a case-by-case basis. basis. Harmonisation: Standardise administrative Harmonisation: Standardise administrative practices and facilitate the development practices and facilitate the development of common criteria and a  shared of common criteria and a  shared understanding. understanding. Public awareness on AI: Work towards AI Public awareness on AI: Work towards AI literacy, public awareness and understanding literacy, public awareness and understanding of the benefits, risks, safeguards and rights of the benefits, risks, safeguards and rights \n\nand obligations in relation to the use of AI and obligations in relation to the use of AI \n\nsystems. systems. International Cooperation: Advise the International Cooperation: Advise the European Commission in relation to European Commission in relation to international matters on AI and cooperate international matters on AI and cooperate with competent authorities of third countries with competent authorities of third countries and with international organisations. and with international organisations. \n\nRole of the Advisory Forum Actions \n\nThe Advisory Forum has been created to ensure the involvement of stakeholders in the implementation and application of the AI Act (article 67). Members are appointed by the European Commission and represent a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society, and academia with recognised expertise in the field of AI. Members are appointed for a term of two years, which may be extended up to four years. They elect two co-chairs from among the members for a term of two years, renewable once. The Fundamental Rights Agency (FRA), the European Union Agency for Cybersecurity (ENISA), the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC), and the European Telecommunications Standards Institute (ETSI) shall be permanent members of the Advisory Forum. The Advisory Forum may establish standing or temporary sub-groups as appropriate for examining specific questions. The Advisory Forum meets at least twice a  year and may invite experts and other stakeholders to its meetings. Advice and technical expertise: Provide advice to the Board and the European Commission. Prepare opinions, recommendations, and written contributions upon request. Consultancy group: The European Commission has to consult the Forum when preparing a standardisation request or drafting common specifications as referred to in article 41. \n\nAnnual report: Prepare and publish an annual \n\nreport on its activities. 62 1 2 3 4 5 6 7 8 9 10 \n\nRole of the scientific panel of independent \n\nexpert Actions \n\nThe scientific panel is created to integrate the scientific community in supporting the European Commissionâ€™s enforcement activities (article 68). Experts are selected by the European Commission based on their current scientific or technical expertise in AI. The number of experts is determined by the European Commission, in consultation with \n\nthe Board, based on the required expertise \n\nneeds, ensuring fair gender and geographical representation. To provide the scientific panel with the necessary information for performing its tasks, a mechanism should be established allowing the panel to request the European Commission to obtain documentation or information from a provider. An implementing act will define how the scientific panel and its members can issue alerts and request assistance from the AI Office. Support the AI Office in the implementation and enforcement as regards general-purpose AI models and system: \n\nâ€¢ Alert the AI Office of possible systemic risks. \n\nâ€¢ Develop tools and methodologies for evaluating capabilities. \n\nâ€¢ Advise on the classification including systemic risk. \n\nâ€¢ Contribute to the development of tools and templates. \n\nâ€¢ Support market surveillance authorities: At their request including with regard to cross-border market surveillance activities. \n\nâ€¢ Assist in the Union safeguard procedure pursuant article 81. Support Member States with their enforcement activities upon demand: \n\nâ€¢ Member States may be required to pay fees for the advice and support provided by the scientific panel. \n\nâ€¢ The implementing act referred to in article 68(1) will define the fees and recoverable costs. 63 1 2 3 4 5 6 7 8 9 10 \n\nGovernance at national level: national competent authorities \n\nMember States play a crucial role in the application and enforcement of the AI Act. To ensure effective application, harmonisation, and coordination within the Union and among Member States, each Member State must designate at least one notifying authority and one market surveillance authority. Together, they constitute the national competent authorities. For AI systems used by Union institutions, agencies, offices, and bodies, the European Data Protection Supervisor will be the supervisory authority. \n\nRole of the notifying authority(ies) Actions \n\nThis authority is responsible for establishing and applying the framework for conformity assessment bodies (article 28). The authority must have an adequate number of competent personnel with the necessary expertise in fields such as information technology, AI, and law, including the supervision of fundamental rights. Notifying authorities must avoid any conflict of interest with conformity assessment bodies, ensuring the objectivity and impartiality of their activities. In particular, the decision to notify a conformity assessment body must not be made by the person who assessed the conformity assessment body. Setting up and carrying out procedures: Establish and execute necessary procedures for the assessment, designation, notification, and monitoring of conformity assessment bodies. Develop these procedures in cooperation with the notifying authorities of other Member States. Advice and guidance: Provide guidance and advice on the implementation of the AI Act, considering the input from the Board and the European Commission, and consulting national competent authorities under other Union laws, if applicable. Activity and service restrictions: \n\nâ€¢ Must not offer or provide any activities performed by conformity assessment bodies. \n\nâ€¢ Must not offer consultancy services on a commercial or competitive basis. 64 1 2 3 4 5 6 7 8 9 10 \n\n# Where can I find this? \n\nGovernance: Chapter VII  recitals 148-154, 163,179 \n\nEU Database: Chapter VIII  recital 131 \n\nEnforcement: Chapters IX and XII  recitals 162-164 and 168-172 \n\nRole of the market surveillance authority(ies) Actions \n\nResponsible for carrying out the activities and taking the measures pursuant to Regulation (EU) 2019/1020 (market surveillance and compliance of products) on market surveillance and compliance of products. One of the market surveillance authorities \n\nwill be designated by each Member State as \n\nthe single point of contact for the public and other counterparts at both Member State and Union levels. The European Data Protection Supervisor will act as the market surveillance authority for Union institutions, agencies, and bodies under the AI Act. Market surveillance authorities for high-risk AI systems in biometrics, used for law enforcement, migration, asylum, border control, justice, and democratic processes, should have strong investigative and corrective powers. This includes access to all personal data and necessary information for their task. Member States must facilitate coordination between market surveillance authorities and other relevant national authorities. Many of task and responsibilities of the market surveillance authorities are described above, but in addition they have the following tasks and responsibilities assigned to them: \n\nâ€¢ Authorisation for high-risk AI systems: Member States can temporarily authorise specific high-risk AI systems to be placed on the market or put into service in their territory for exceptional reasons of public security, health, environmental protection, or key infrastructure, pending conformity assessments (article 46). \n\nâ€¢ Annual reporting: to the European \n\nCommission and national competition authorities on surveillance activities and prohibited practices including: (i) any information identified that is of potential interest for the application of competition law; (ii) use of any prohibited practices; and (iii) measures taken in relation to those practices. \n\nâ€¢ Advice and guidance: Provide guidance and advice on the implementation of the AI Act, considering the input from the Board and the European Commission, and consulting national competent authorities under other Union laws, if applicable. 65 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ The AI Act entered into force on 1 August 2024. \n\nâ€¢ Most provisions are set to apply from 2 August 2026, and others are being phased in over a period of six to 36 months from the date of entry into force. \n\nâ€¢ The European Commission will develop delegated and implementing acts, guidelines, codes of conduct and standards. These initiatives are aimed at providing practical guidance, ethical principles and technical specifications related to the AI Act, with the goal of ensuring the effective implementation of the legislation. \n\nâ€¢ The Commission also sent, in July 2024, an updated version of its proposed AI Liability Directive to both the European Parliament and the Council for consideration. \n\nâ€¢ Bird & Birdâ€™s AI experts are equipped to \n\nmonitor the forthcoming initiatives expected under AI Act and help you navigate the different processes and requirements. All actors dealing with AI systems should actively monitor the development of the legislative and non-legislative initiatives outlined in this chapter. \n\n# To do list At a glance \n\nCHAPTER 9 \n\n# AI Act: Whatâ€™s Next 66 1 2 3 4 5 6 7 8 9 10 \n\n# AI Act: Whatâ€™s Next \n\nThis chapter provides an overview of the application deadlines of the AI Act and the forthcoming initiatives expected under the Regulation. The EU institutions regard the \n\nAI Act as a new form of â€œliving regulationâ€ that \n\nwill be supplemented on an ongoing basis via secondary legislation and other initiatives, in \n\nan effort to keep pace with technological advances. Over the coming months, the AI Act envisions the adoption of a range of delegated and implementing acts, guidance documents, codes of conduct, codes of practice and standardisation requests. These initiatives \n\nare designed to provide practical guidance, ethical principles and technical specifications regarding the Regulation, with the aim of ensuring effective implementation. The requirements laid down in such documents will greatly shape the effective implementation of 1.  Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) Text with EEA relevance, OJ L, 2024/1689, 12.7.2024. the AI Act and the ability of actors to comply with its obligations. All actors dealing with AI systems would therefore be advised to actively monitor the work of the Commission in developing the legislative and non-legislative initiatives mentioned in this chapter. Bird & Birdâ€™s Regulatory and Public Affairs team is equipped to monitor the forthcoming initiatives expected under AI Act and help you navigate the different processes and requirements. \n\n# AI Act application deadlines \n\nFollowing its publication in the EU Official Journal 1 on 12 July 2024, the AI Act entered into force on 1 August 2024. The relevant dates of application are set out below. \n\n12 July 2024 The AI Act was published in the Official Journal of the EU, triggering the dates for specific provisions in the Regulation becoming applicable. \n\n2 February 2025 Prohibited practices ban applies (Chapter II). AI literacy rules apply (article 4). \n\n2 May 2025 Codes of practice for general-purpose AI must be ready (article 56 (9)). \n\n2 August 2025 National authorities designated (Chapter III Section 4). Obligations for General-Purpose AI (GPAI) (Chapter V). Governance (at EU and national level) (Chapter VII). Confidentiality and penalties (other than in relation to gen-AI) \n\n(Chapter XII). \n\n2 August 2026 Start of application of all other provisions of the EU AI Act (unless a later date applies below). 67 1 2 3 4 5 6 7 8 9 10 \n\nBetween 1 August 2024 and 2 August 2027, the European Commission is expected to adopt various documents to implement the Regulation. These comprise delegated and implementing acts, guidance documents, codes of conduct, codes of practice and standardisation requests. Apart from a few exceptions, there are no specific set deadlines for the publication of these initiatives by the Commission. Nonetheless, it is assumed that the Commission will aim to adopt such documents ahead of the application deadlines of the respective provisions. \n\n# Delegated acts \n\nSeveral provisions will be the subject of delegated acts, to be adopted by the Commission to specify obligations and operational implementation. Article 97 grants the power to adopt delegated acts to the Commission for a five-year period that started on 1 August 2024. The Commission must report on this delegation nine months before the end of the period. This period is automatically extended for another five years unless the European Parliament or the Council opposes it three months before the end of each period. As mentioned above, there are no specific set deadlines for the adoption of such delegated acts. However, it must be presumed that their adoption will precede the application deadlines for the related provisions in the AI Act (see article 113). 2.  Inter-institutional Agreement between the European Parliament, the Council of the European Union and the European Commission on Better Law-Making, OJ L 123, 12.5.2016. Pursuant to article 97(4), before adopting a delegated act, the Commission will have to carry out public consultations during its preparatory work and will also consult with the relevant Expert Groups (composed of Member States experts). Once adopted, the Commission must notify the European Parliament and the Council simultaneously. A delegated act only enters into force if neither the European Parliament nor the Council objects within three months of notification, extendable by another three months if needed. The European Parliament or the Council can revoke this power at any time, but this will not affect the validity of existing delegated acts. In accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making 2, the \n\nCommission will have to ensure that the European Parliament and the Council receive all documents at the same time as Member Statesâ€™ experts, and the Parliament and Councilâ€™s experts should systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts. The AI Act foresees the adoption of the following delegated acts where the Commission considers this to be necessary: \n\nâ€¢ Article 6(6/7) : amend article 6(3) by adding new conditions to those laid down in paragraph 3, by modifying or by deleting them if there is concrete and reliable evidence of the existence of AI systems that should not fall \n\nunder Annex III or that should not fall under \n\nthe conditions of article 6(3); \n\n2 August 2027 High-risk categories listed in Annex I). General purpose AI models placed on the market before 2 August 2025 (article 111). \n\n2 August 2030 High-risk AI systems (other than those listed below), which have been placed on the market or put into service before 2 August 2026 and which are intended to be used by public authorities (article 111). \n\n31 December 2030 Components of large-scale IT systems listed in Annex X, which have \n\nbeen placed on the market or put into service before 2 August 2027 \n\n(article 111). 68 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ Article 7(1/3) : amend Annex III, by adding, modifying or removing use-cases of high-risk \n\nAI systems; \n\nâ€¢ Article 11(3) : amend Annex IV, where necessary, to ensure that, in light of technical progress, the technical documentation provides all the information necessary to assess the compliance of the system; \n\nâ€¢ Article 43(5) : amend Annexes VI and VII by updating them in light of technical progress; \n\nâ€¢ Article 45(6) : amend article 43(1/2) in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to third-party conformity assessments; \n\nâ€¢ Article 47(5) : amend Annex V by updating the content of the EU declaration of conformity set out in that Annex, in order to introduce elements that become necessary in light of technical progress; \n\nâ€¢ Article 51(3) : amend the thresholds for systemic general-purpose AI models listed in article 51(1/2) as well as to supplement benchmarks and indicators in light of evolving technological developments, such as algorithmic improvements or increased hardware efficiency, when necessary, for these thresholds to reflect the state of the art; \n\nâ€¢ Article 52(4) : amend Annex XIII by \n\nspecifying and updating the criteria for systemic general-purpose AI models; \n\nâ€¢ Article 53(5) : detail measurement and calculation methodologies with a view to allowing for comparable and verifiable documentation to facilitate compliance with Annex XI; and \n\nâ€¢ Article 53(6) : amend Annexes XI and XII in light of evolving technological development. \n\n# Implementing acts \n\nArticle 98(2) of the AI Act confers on the European Commission the power to adopt implementing acts in accordance with Regulation 182/2011 3. Implementing acts  aim to create 3.  Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 February 2011 laying down the rules and general principles concerning mechanisms for control by Member States of the Commissionâ€™s exercise of implementing powers, OJ L 55, 28.2.2011. uniform conditions for the implementation of a specific legislative act, if and when this is necessary. With respect to the drafting of the implementing acts, the Commission will be assisted by a â€œComitologyâ€ Committee comprising Member State experts. As is the case for delegated acts, the timeline for adoption of the expected implementing acts is not specified in the text, except for the foreseen implementing act referred to in article 72(3), which is due by 2 February 2026. Therefore, it should be presumed that the relevant implementing acts will be adopted ahead of the application deadlines for the related provisions in the AI Act (see above and article 113). The AI Act foresees the adoption of the following implementing acts, where the Commission deems it necessary to: \n\nâ€¢ Article 37(2) : suspend, restrict or withdraw the designation of notified bodies when the Member State fails to take the necessary corrective measures; \n\nâ€¢ Article 41(1/4/6) : establish, in consultation with the â€œAdvisory Forumâ€ referred to in \n\narticle 67, common specifications for the requirements for high-risk AI systems or \n\nfor the obligations for general-purpose AI \n\nmodels set out in Chapter V, Sections 2 and 3. When a reference to a harmonised standard is published in the Official Journal of the European Union, which covers the same requirements set out in Section 2 of this Chapter III, the Commission shall repeal the implementing act referred to in article 41(1). Where a Member State considers that a common specification does not entirely meet the requirements set out in Section 2 of this Chapter III, the Commission shall assess that information and, if appropriate, amend the implementing act referred to in article 41(1); \n\nâ€¢ Article 50(7) : approve codes of practice drawn up to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content, in accordance with the procedure laid down in article 56(6). If the code of practice is not adequate, the Commission may adopt an implementing act to lay down a set of common rules for the implementation of the transparency 69 1 2 3 4 5 6 7 8 9 10 \n\nobligations for providers and deployers of certain AI systems of article 50; \n\nâ€¢ Article 56(6) : approve a code of practice for general-purpose AI models and give it a general validity within the Union. If, by 2 August 2025, a code of practice cannot be finalised, or if the AI Office deems it is not adequate, the Commission may provide, by means of implementing acts, common rules for the implementation of the obligations provided for in articles 53 and 55, including the issues set out in article 56(2); \n\nâ€¢ Article 58(1) : specify the detailed arrangements for the establishment, development, implementation, operation and supervision of the AI regulatory sandboxes; \n\nâ€¢ Article 60(1) : specify the detailed elements of the real-world testing plan for providers of high-risk AI systems; \n\nâ€¢ Article 68(1) : make provisions on the establishment of a scientific panel of \n\nindependent experts (the â€œscientific panelâ€ )\n\nintended to support the enforcement activities of the AI Act; \n\nâ€¢ Article 72(3) : publish, by 2 February 2026, an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan for providers of high-risk AI systems and the list of elements to be included in the plan; \n\nâ€¢ Article 92(6) : set out the detailed \n\narrangements and the conditions for the AI Office of general-purpose AI models evaluations, including the detailed arrangements for involving independent experts, and the procedure for the selection thereof; and \n\nâ€¢ Article 101(6) : lay down detailed arrangements and procedural safeguards for proceedings in view of the possible fines on providers of general-purpose AI models. \n\n# Commission Guidelines \n\nâ€œCommission Guidelinesâ€ are explanatory documents produced by the Commission services to provide practical and informal guidance about how particular provisions of the AI Act should be applied. The AI Act foresees the adoption of the following Commission Guidelines: \n\nâ€¢ Article 6(5) : after consulting the European Artificial Intelligence Board, and no later than 2 February 2026, specifying the practical implementation of article 6, including a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk; \n\nâ€¢ Article 63(1) : on the elements of the quality management system which may be complied with in a simplified manner considering the needs of microenterprises, without affecting the level of protection or the need for compliance with the requirements in respect of high-risk AI systems (no set deadline for these guidelines); \n\nâ€¢ Article 73(7) : to facilitate compliance with \n\nthe reporting obligations of serious incident. The guidance has to be adopted by 2 August 2025, and will have to be assessed regularly \n\nby the Commission; \n\nâ€¢ Article 96 : on the practical implementation of this Regulation. There is no set deadline for the development of these guidelines. However, the related provisions apply from 2 August 2026. In particular, the Commission is to develop guidelines on: \n\nâ€” the application of the requirements and obligations referred to in articles 8 to 15 and in article 25; \n\nâ€” the prohibited practices referred to in \n\narticle 5; \n\nâ€” the practical implementation of the provisions related to substantial modification; \n\nâ€” the practical implementation of transparency obligations laid down in article 50; \n\nâ€” detailed information on the relationship of the AI Act with the EU harmonisation legislation listed in Annex I, as well as with other relevant EU laws, including as regards consistency in their enforcement; and \n\nâ€” the application of the definition of an AI system as set out in article 3, point (1). 70 1 2 3 4 5 6 7 8 9 10 \n\n# Codes of conduct and practice \n\nCodes of conduct \n\nCodes of conduct are documents of a voluntary nature that establish ethical guidelines and principles for the development and use of AI in certain conditions. They are also intended to foster the development of AI policies within organisations for the voluntary application of specific AI Act obligations. The AI Act calls for the adoption of the following codes of conduct: \n\nâ€¢ Recital 20 and article 4 : voluntary codes of conduct to advance AI literacy among persons dealing with the development, operation and use of AI. \n\nâ€” While there is no set deadline for the \n\ndevelopment of voluntary codes of practice to advance AI literacy, the related provisions on AI literacy in Article 4 will apply from 2 February 2025. \n\nâ€¢ Recital 165 and article 95 : codes of conduct intended to foster the voluntary application to AI systems of some or all the mandatory requirements applicable to high-risk AI systems. \n\nThese are adapted in light of the intended \n\npurpose of the systems and the lower risk involved, and take into account the available technical solutions and industry best practices such as model and data cards: \n\nâ€” to ensure that the voluntary codes of conduct are effective, they should be based on clear objectives and key performance indicators to measure the achievement of those objectives; \n\nâ€” they should also be developed in an inclusive way, as appropriate, with the involvement of relevant stakeholders such as business and civil society organisations, academia, research organisations, trade unions and consumer protection organisations; and \n\nâ€” while there is no set deadline for the development of voluntary codes of practice intended to foster the application to AI systems of some or all the mandatory requirements applicable to high-risk AI systems, the related provisions included in Article 95 will apply from 2 February 2026. By 2 August 2028 and every three years thereafter, the Commission is due to evaluate the impact and effectiveness of such voluntary codes of conduct. \n\nCodes of practice \n\nCodes of practice represent a central tool for proper compliance with specific obligations under the AI Act. In particular, one code of practice will detail the AI Act rules for providers of general-purpose AI models and general-purpose AI models with systemic risks. Another code of practice will focus on the detection and labelling of artificially generated or manipulated content. Organisations should be able to rely on codes of practice to demonstrate compliance with the relevant obligations, which is known as a \n\nâ€œpresumption of conformityâ€ .Specifically, the AI Act calls on the European Commissionâ€™s AI Office to facilitate the drawing up of the following codes of practice together with all interested stakeholders: \n\nâ€¢ Article 50(7): codes of practice at EU level to facilitate the effective implementation of the obligations in article 50(2/4), regarding the detection and labelling of artificially generated or manipulated content. The Commission may adopt implementing acts to approve those codes of practice. While there is no set deadline for the development of voluntary codes of practice to facilitate the effective implementation of the obligations in article 50(2/4), the related provisions included in Article 50 will apply from 2 February 2026. \n\nâ€¢ Article 56(1/3) : by 2 May 2025, codes of practice for general-purpose AI models. These will duly take into account international approaches as well as a diverse set of perspectives, by collaborating with relevant national competent authorities and, where appropriate, by consulting with civil society organisations and other relevant stakeholders and experts. These include the â€œScientific Panelâ€ of independent \n\nexperts established under the AI Act. By 2 August 2028 and every three years thereafter, the Commission will have to evaluate the impact and effectiveness of voluntary codes of practice. On 30 July 2024, the European AI Office opened \n\na call for expressions of interest to participate in the drawing-up of the first general-purpose AI Code of Practice. Interested parties could  express 71 1 2 3 4 5 6 7 8 9 10 \n\ntheir interest in participating by  25 August  2024. According to the Commission, this Code will be prepared by means of an iterative drafting process by April 2025, nine months from the AI Actâ€™s entry into force on 1 August 2024. The Code of Practice will facilitate the proper application of the rules of the AI Act for general-purpose AI models. The Commission may decide to approve the Code of Practice and give it a general validity within the European Union by means of an implementing act, pursuant to article 56(6).  If the Code of Practice is not deemed adequate, the Commission will provide common rules for the implementation of the relevant obligations. In addition, on 30 July 2024, the AI Office launched a consultation on trustworthy general-purpose AI models under the AI Act, specifically regarding the template for the summary of the content used for the training of the general-purpose AI models and the accompanying guidance. The deadline for responses was \n\n10 September 2024. \n\n# Standards \n\nInitial standardisation work \n\nThe process of drafting European standards in support of the AI Act started well before the adoption of the AI Act, with the Commissionâ€™s \n\nproposal on harmonised rules on artificial intelligence adopted as the Commission \n\nImplementing Decision C(2023)3215 on \n\n22 May 2023. This Implementing Decision requested the European Committee for Standardisation (CEN) and the European Committee for Electrotechnical \n\nStandardisation (CENELEC) to draft the \n\nfollowing new European standards or European standardisation deliverables on AI by 30 April 2025: \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on risk management systems for AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on governance and quality of datasets used to build \n\nAI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on record keeping through logging capabilities by AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on transparency and information provisions for users of AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on human oversight of AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on accuracy specifications for AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on robustness specifications for AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on cybersecurity specifications for AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on quality management systems for providers of AI systems, including post-market monitoring processes; and \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on conformity assessment for AI systems. \n\nThis standardisation request to CEN and \n\nCENELEC was made pursuant to action 63 of the European Commission 2022 â€œAnnual Union Work Programme for European standardisationâ€ with the aim of ensuring that AI systems are safe and trustworthy. \n\nFor the drafting of these standards, CEN \n\nand CENELEC have set up a specific joint technical committee named â€œCEN-CENELEC JTC 21 \n\nArtificial Intelligenceâ€ . CEN and CENELEC are \n\nalso collaborating on the drafting with the European Telecommunications Standards \n\nInstitute (ETSI) , an independent, not-for-profit, standardisation organisation in the field of information and communication. \n\nAI Act standardisation request \n\nArticle 40(2) of the AI Act calls on the European Commission to present, without undue delay \n\nafter the entry into force of the Regulation ,\n\nstandardisation requests for harmonised EU AI standards covering: \n\nâ€¢ all requirements set out in Section 2 of Chapter III of the AI Act; and 72 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ as applicable, standardisation requests covering obligations set out in Chapter V, Sections 2 and 3 of the AI Act. These requests revise the requests included \n\nin Commission Implementing Decision \n\nC(2023)3215 . This was also anticipated in the Commissionâ€™s Standardisation Work Programme \n\nfor 2024 published in February 2024. Indeed, Action 15 of the Work Programme calls for a \n\nâ€œrevision of the standardisation request in support \n\nof Union policy on artificial intelligenceâ€ , thereby calling for the revision of the Commission Decision in view of the final AI \n\nAct text. According to article 40(2) of the AI Act, the standardisation requests should also ask for deliverables on reporting and documentation processes to improve AI systemsâ€™ resource performance. Such requests could include reducing the consumption of energy and of other resources by high-risk AI systems during their lifecycle and the energy-efficient development of general-purpose AI models. The Commission should draft the requests after consulting with the European Artificial Intelligence Board and relevant stakeholders, including the Advisory Forum of stakeholders established under the AI Act. In addition, when issuing standardisation requests to the relevant European standardisation organisations, the Commission should specify that standards have to be clear and consistent. This prerequisite includes standards developed in the various sectors for products covered by the existing EU harmonisation legislation listed in Annex I. \n\nThey are aimed at ensuring that high-risk AI systems or general-purpose AI models placed on the market or put into service in the EU meet the relevant requirements or obligations laid down in the AI Act. By 2 August 2028 and every four years thereafter, the Commission will have to submit a report to review the progress made regarding the development of standardisation deliverables on the energy-efficient development of general-purpose AI models. In this context, the Commission will also be required to assess the need for further measures or actions, including binding measures or actions. The report will have to be submitted to the European Parliament and to the Council and made public. \n\n# Liability \n\nCommission amends proposal to align with AI Act \n\nFinally, it is worth noting that at the end of July 2024, the European Commission sent an updated version of its proposal adapting non-contractual civil liability rules to artificial intelligence (AI Liability Directive or AILD) to both the European Parliament and the Council. This proposal, which was first tabled by the Commission in September 2022, aims to address the risks generated by specific uses of AI through a set of rules focusing on respect of fundamental rights and safety. \n\nThe current changes are designed to align the \n\nAI Liability Directive proposal with the \n\ncompleted AI Act. It is notable that the new proposal amends article 4 regarding the increased potential responsibility of companies deploying AI systems. These deployers would now be presumed liable for damage caused if they â€œdid not monitor the operation of the AI system or, where appropriate, suspend [its] useâ€ or did not use â€œsufficiently representativeâ€ input data. The European Parliamentâ€™s lead draftsperson \n\n(â€œrapporteurâ€ ) for this file, the German Christian-democratic MEP Axel Voss, had previously requested the European Parliamentary Research Service to conduct an â€œalternative impact assessmentâ€ to evaluate whether the AILD is \n\nstill necessary in view of adoption of the AI Act. While the future of the proposed AI Liability Directive remains uncertain, it may proceed in \n\na reduced form. 73 1 2 3 4 5 6 7 8 9 10 \n\n# AI Guide Contributors \n\nAnne-Sophie Lampe \n\nPartner \n\n+33142686333 \n\nanne-sophie.lampe@twobirds.com \n\nBenoit Van Asbroeck \n\nOf Counsel \n\n+3222826067 \n\nbenoit.van.asbroeck@twobirds.com \n\nTobias BrÃ¤utigam \n\nPartner \n\n+358962266758 \n\ntobias.brautigam@twobirds.com As a market-leading law firm for technology, ranked Tier 1 for AI (first ranking of its kind within the European legal directory community) and TMT by Legal 500 in 12 jurisdictions and Band 1 for global multi-jurisdictional TMT by Chambers, we distinguish ourselves through our deep understanding of the technical intricacies involved in AI technology development and deployment. This expertise enables us to effectively collaborate with developers and commercial teams, speaking their language and asking the right questions from the outset. Our international AI group comprises over 120 experts , covering virtually every intersection where this transformative technology meets law and regulation. From handling ground-breaking IP litigation and guiding clients through complex regulatory changes to implementing effective governance frameworks and innovating commercial and contractual arrangements. If you have any questions about the content, please get in touch with any of the contributors below or your usual Bird & Bird contact. You can also find out more about the latest AI developments in our AI Hub \n\nPaolo Sasdelli \n\nRegulatory and Public Affairs Advisor \n\n+3222826076 \n\npaolo.sasdelli@twobirds.com \n\nBelgium Finland France Germany \n\nCen Zhang \n\nSenior Associate \n\n+33142686000 \n\ncen.zhang@twobirds.com \n\nDr. Miriam Ballhausen \n\nPartner \n\n+4940460636000 \n\nmiriam.ballhausen@twobirds.com \n\nFrancine Cunningham \n\nRegulatory and Public Affairs Director \n\n+3222826056 \n\nfrancine.cunningham@twobirds.com \n\nOliver Belitz \n\nCounsel \n\n+4969742226000 \n\noliver.belitz@twobirds.com \n\nDr. Nils LÃ¶lfing \n\nCounsel \n\n+4921120056000 \n\nnils.loelfing@twobirds.com 74 1 2 3 4 5 6 7 8 9 10 \n\nMarta Kwiatkowska-Cylke \n\nCounsel \n\n+48225837964 \n\nmarta.kwiatkowska-cylke@ twobirds.com \n\nGian Marco Rinaldi \n\nCounsel \n\n+390230356071 \n\ngianmarco.rinaldi@twobirds.com \n\nDr. Maria Jurek \n\nSenior Associate \n\n+48225837839 \n\nmaria.jurek@twobirds.com \n\nItaly Poland \n\nAleksandra Cywinska \n\nSenior Associate \n\n+48225837875 \n\naleksandra.cywinska@twobirds.com \n\nAleksandra Mizerska \n\nLawyer \n\n+48225837900 \n\naleksandra.mizerska@twobirds.com \n\nAndrzej Stelmachowski \n\nAssociate \n\n+48225837977 \n\nandrzej.stelmachowski \n\n@twobirds.com \n\nIzabela Kowalczuk-Pakula \n\nPartner \n\n+48225837932 \n\nizabela.kowalczuk-pakula@ twobirds.com \n\nPawel Lipski \n\nPartner \n\n+48225837991 \n\npawel.lipski@twobirds.com \n\nDr. Simon Hembt \n\nSenior Associate \n\n+4969742226000 \n\nsimon.hembt@twobirds.com \n\nThe Netherlands \n\nFeyo Sickinghe \n\nOf Counsel \n\n+31703538904 \n\nfeyo.sickinghe@twobirds.com \n\nSpain \n\nJoaquÃ­n MuÃ±oz \n\nPartner \n\n+34917906007 \n\njoaquin.munoz@twobirds.com \n\nUnited Kingdom \n\nAlex Jameson \n\nSenior Associate \n\n+442078507139 \n\nalex.jameson@twobirds.com \n\nGermany 75 1 2 3 4 5 6 7 8 9 10 \n\nLiz McAuliffe \n\nAssociate \n\n+442074156787 \n\nliz.mcauliffe@twobirds.com \n\nNora Santalu \n\nAssociate \n\n+442079826513 \n\nnora.santalu@twobirds.com \n\nUnited Kingdom \n\nIan Edwards \n\nPartner \n\n+442079056377 \n\nian.edwards@twobirds.com \n\nKaterina Tassi \n\nSenior Associate \n\n+442074156066 \n\nkaterina.tassi@twobirds.com \n\nKatharine Stephens \n\nPartner \n\n+442074156104 \n\nkatharine.stephens@ twobirds.com \n\nWill Bryson \n\nSenior Associate \n\n+442074156746 \n\nwill.bryson@twobirds.com \n\nToby Bond \n\nPartner \n\n+442074156718 \n\ntoby.bond@twobirds.com \n\nRuth Boardman \n\nPartner \n\n+442074156018 \n\nruth.boardman@twobirds.com 9 10 \n\n# twobirds.com \n\nThe information given in this document concerning technical legal or professional subject matter is for guidance only and does not constitute legal or professional advice. Always consult a suitably qualified lawyer on any specific legal problem or matter. Bird & Bird assumes no responsibility for such information contained in this document and disclaims all liability in respect of such information. This document is confidential. Bird & Bird is, unless otherwise stated, the owner of copyright of this document and its contents. No part of this document may be published, distributed, extracted, re-utilised, or reproduced in any material form. Bird & Bird is an international legal practice comprising Bird & Bird LLP and its affiliated and associated businesses. Bird & Bird LLP is a limited liability partnership, registered in England and Wales with registered number OC340318 and is authorised and regulated by the Solicitors Regulation Authority (SRA) with SRA ID497264. Its registered office and principal place of business is at 12 New Fetter Lane, London EC4A 1JP. A list of members of Bird & Bird LLP and of any non-members who are designated as partners, and of their respective professional qualifications, is open to inspection at that address.", "fetched_at_utc": "2026-02-08T18:50:35Z", "sha256": "d085ee7a46ea39634d2d38eef3a2aa413dcb9d798e771bed5c427fa8f6263dc3", "meta": {"file_name": "European Union Artificial Intelligence Act - Bird & Bird.pdf", "file_size": 1313072, "relative_path": "pdfs\\European Union Artificial Intelligence Act - Bird & Bird.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 46141}}}
{"doc_id": "pdf-pdfs-fbpml-organisationbp-v1-0-0-17-30-1a8bfb7dcffa", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_OrganisationBP_V1.0.0-17-30.pdf", "title": "FBPML_OrganisationBP_V1.0.0-17-30", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder. \n\n17 FBPML Organisation Best Practices v1.0.0. \n\n# Section 4. Data Governance \n\nObjective \n\nTo ensure the integrity, normalisation, fairness and non-discrimination of Projet and/or Model data. \n\n4. Data Governance \n\nControl:  Aim: \n\n4.1.  Data Governance \n\nPolicy \n\nA Policy and Guide, which promotes good \n\nData Governance in Product and Model \n\ndesign, development, and implementation \n\nought to be derived by Data Science \n\nManagers and approved by the Managerial \n\nCommittee. If a generic Data Governance \n\nPolicy already exists, the above should be \n\nintegrated accordingly. \n\nTo (a) ensure the integrity, \n\nnormalisation, fairness and non-\n\ndiscrimination of Product and/or \n\nModel data; and (b) provide clear \n\nOrganisation guidance to Products \n\non how to warrant data integrity, \n\nnormalisation, fairness and non-\n\ndiscrimination. \n\n4.2.  Data Governance \n\nProcedures \n\nA set of Procedures to operationalise \n\nthe Data Governance Policy should be \n\ndeveloped and implemented within \n\nProducts in light of Product Definitions, \n\nthe Product Risk Classification Portfolio, \n\nand Product Lifecycle and Workflow \n\nDescriptions. \n\nTo ensure the Data Governance of \n\nProducts and Models. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n18 FBPML Organisation Best Practices v1.0.0. \n\n# Section 5. Product and Model \n\n# Oversight & Management \n\nObjective \n\nTo (a) identify possible risks for protected classes of persons, animals and the natural environment; and (b) \n\nminimise the unequal distribution of Products and Models errors to prevent reinforcing and/or deriving social \n\ninequalities and/or ills. \n\n5.1 Fairness & Non-Discrimination \n\nControl:  Aim: \n\n5.1.1.  Fairness Policy  A Policy and Guide, which promotes \n\nProduct Fairness in - (a) Product \n\nDefinitions and Product design, \n\ndevelopment, and implementation; (b) \n\ndata processing; and (c) Model design, \n\ndevelopment, training, and output \n\nought to be derived by Data Science \n\nManagers and approved by the Managerial \n\nCommittee. \n\nTo ensure the Fairness of Machine \n\nLearning. \n\nTo provide clear Organisation \n\nguidance to Products on how \n\nto - (a) identify biases in Product \n\ndata and Models; (b) take \n\nremedial action against identified \n\nbiases; (c) identify and reduce \n\nasymmetric error rates between \n\nsubpopulations; and (d) implement \n\ndesign and processes to avoid and \n\ncircumvent risks that cannot be \n\nsolved by purely technical means. \n\n5.1.2.  Product Fairness \n\nProcedures \n\nA set of Procedures to operationalise the \n\nFairness Policy should be developed and \n\nimplemented within Products in light of \n\nProduct Definitions, the Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure the Fairness of Products \n\nand Models. \n\n5.1.3.  Fairness \n\nAssessments \n\nProducts should regularly complete \n\nFairness assessments according \n\nto Product Lifecycle and Workflow \n\nDescriptions to the extent that is \n\nreasonably practical. Assessment \n\nfindings ought to be documented by \n\nthe Product Team and reviewed by Data \n\nScience Managers and, when relevant, the \n\nManagement Committee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, Product \n\nFairness and non-discrimination \n\nat regular intervals within the \n\nProduct Lifecycle and Workflow \n\nDescription. \n\n5.1.4.  Review of the \n\nFairness Policy \n\nThe Fairness Policy should be reviewed \n\nperiodically, or if significant changes \n\noccur, by Data Science Managers to \n\nensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Fairness Policy \n\nis kept up-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n19 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\n5.1.5.  Review of \n\nProduct Fairness \n\nProcedures \n\nProduct Fairness Procedures should be \n\nreviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product Fairness \n\nProcedures are kept up-to-date. \n\nObjective \n\nTo ensure Data Quality and prevent unintentional effects, changes and/or deviations in Products and Models \n\noutputs associated with poor Product data. \n\n5.2 Data Quality \n\nControl:  Aim: \n\n5.2.1.  Data Quality Policy  A Policy and Guide, which describes \n\nthe assessment and remediation of an \n\nOrganisationâ€™s Data Quality. Chapters \n\nrelating to (a) Product Definitions and \n\nProduct design, development, and \n\nimplementation; (b) data processing; and \n\n(c) Model design, development, training, \n\nand output ought to be derived and \n\nincluded by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Data Quality of \n\nMachine Learning. \n\nTo provide Product Teams with \n\nreliable guidance on how to - (a) \n\nidentify Data Quality risks; (b) take \n\nremedial actions against identified \n\nData Quality risks; and (c) take \n\nsteps to account for Data Quality \n\nrisks and associated issues. \n\n5.2.2.  Product Data \n\nQuality Procedures \n\nA set of Procedures to operationalise the \n\nData Quality Policy should be developed \n\nand implemented within Products in light \n\nof Product Definitions, the Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions . \n\nTo ensure the Data Quality of \n\nProducts and Models. \n\n5.2.3.  Data Quality \n\nAssessments \n\nProducts should regularly complete Data \n\nQuality assessments according to Product \n\nLifecycle and Workflow Descriptions to \n\nthe extent that is reasonably practical. \n\nAssessment findings ought to be \n\ndocumented by the Product Team and \n\nreviewed by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo analyse, test, and report the \n\nrisks identified with, and measures \n\ntaken to ensure, Product Data \n\nQuality at regular intervals within \n\nthe Product Lifecycle and Workflow \n\nDescription. \n\n5.2.4.  Review of the Data \n\nQuality Policy \n\nThe Data Quality Policy should be reviewed \n\nperiodically, or if significant changes \n\noccur, by Data Science Managers to \n\nensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Data Quality \n\nPolicy is kept up-to-date. \n\n5.2.5.  Review of Product \n\nData Quality \n\nProcedures \n\nProduct Data Quality Procedures should \n\nbe reviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product Data Quality \n\nProcedures are kept up-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n20 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo (a) ensure that Product data and Models are representative of, and accurately specified for, target \n\nenvironments as far as is reasonably practical; and (b) guard against unintentional Products and Models \n\nbehaviours and outputs as far as is reasonably practical. \n\n5.3 Representativeness & Specification \n\nControl:  Aim: \n\n5.3.1.  Representativeness & \n\nSpecification Policy \n\nA Policy and Guide, which promotes \n\nRepresentativeness and Specification \n\nin - (a) Product Definitions and Product \n\ndesign, development, and implementation; \n\n(b) data processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the \n\nRepresentativeness and \n\nSpecification of Machine \n\nLearning. \n\nTo provide Product Teams with \n\nreliable guidance on how to -\n\n(a) identify Representativeness \n\nand Specification risks; (b) \n\nredress misspecification; and (c) \n\nremedy mis-, under- or over-\n\nrepresentation risks. \n\n5.3.2.  Product \n\nRepresentativeness \n\n& Specification \n\nProcedures \n\nA set of Procedures to operationalise the \n\nRepresentativeness & Specification Policy \n\nshould be developed and implemented \n\nwithin Products in light of Product \n\nDefinitions, the Product Risk Classification \n\nPortfolio, and the Product Lifecycle and \n\nWorkflow Descriptions. \n\nTo ensure the \n\nRepresentativeness and \n\nSpecification of Products and \n\nModels. \n\n5.3.3.  Representativeness \n\n& Specification \n\nAssessments \n\nProducts should regularly complete \n\nRepresentativeness & Specification \n\nassessments according to Product \n\nLifecycle and Workflow Descriptions to \n\nthe extent that is reasonably practical. \n\nAssessment findings ought to be \n\ndocumented by the Product Team and \n\nreviewed by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Representativeness and \n\nSpecification at regular intervals \n\nwithin the Product Lifecycle and \n\nWorkflow Description. \n\n5.3.4.  Review of the \n\nRepresentativeness & \n\nSpecification Policy \n\nThe Representativeness & Specification \n\nPolicy should be reviewed periodically, \n\nor if significant changes occur, by Data \n\nScience Managers to ensure its continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that the \n\nRepresentativeness & \n\nSpecification Policy is kept up-\n\nto-date. \n\n5.3.5.  Review of Product \n\nRepresentativeness \n\n& Specification \n\nProcedures \n\nProduct Representativeness & \n\nSpecification Procedures should be \n\nreviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that \n\nRepresentativeness & \n\nSpecification Procedures are \n\nkept up-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n21 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo warrant Model outcomes and prevent unintentional Model behaviour a priori under operational conditions as \n\nfar as is reasonably practical. \n\n5.4 Performance Robustness \n\nControl:  Aim: \n\n5.4.1.  Performance \n\nRobustness Policy \n\nA Policy and Guide, which promotes \n\nProduct Performance Robustness in - (a) \n\nProduct Definitions and Product design, \n\ndevelopment, and implementation; (b) \n\ndata processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Performance and \n\nRobustness of Machine Learning. \n\nTo provide Product Teams with \n\nreliable guidance on how to -\n\n(a) test, control, and improve \n\nPerformance Robustness under \n\noperational conditions before \n\ngoing live; and (b) assess and \n\ncontrol Performance Robustness \n\nrisks concerning unexpected \n\nconditions. \n\n5.4.2.  Product \n\nPerformance \n\nRobustness \n\nProcedures \n\nA set of Procedures to operationalise the \n\nPerformance Robustness Policy should \n\nbe developed and implemented within \n\nProducts in light of Product Definitions, \n\nthe Product Risk Classification Portfolio, \n\nand the Product Lifecycle and Workflow \n\nDescriptions . \n\nTo ensure the Performance \n\nRobustness of Products and \n\nModels. \n\n5.4.3.  Performance \n\nRobustness \n\nAssessments \n\nProducts should regularly complete \n\nPerformance Robustness Assessments \n\naccording to Product Lifecycle and \n\nWorkflow Descriptions to the extent \n\nthat is reasonably practical. Assessment \n\nfindings ought to be documented by \n\nthe Product Team and reviewed by Data \n\nScience Managers and, when relevant, the \n\nManagement Committee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Performance Robustness \n\nat regular intervals within the \n\nProduct Lifecycle and Workflow \n\nDescription. \n\n5.4.4.  Review of the \n\nPerformance \n\nRobustness Policy \n\nThe Performance Robustness Policy should \n\nbe reviewed periodically, or if significant \n\nchanges occur, by Data Science Managers \n\nto ensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Performance \n\nRobustness Policy is kept up-to-\n\ndate. \n\n5.4.5.  Review of Product \n\nPerformance \n\nRobustness \n\nProcedures \n\nProduct Performance Robustness \n\nProcedures should be reviewed \n\nperiodically, or if significant changes \n\noccur, by the Product Team to ensure their \n\ncontinued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that Performance \n\nRobustness Procedures are kept \n\nup-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n22 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo ensure that Products and Models remain within acceptable operational bounds. \n\n5.5 Monitoring & Maintenance \n\nControl:  Aim: \n\n5.5.1.  Monitoring & \n\nMaintenance Policy \n\nA Policy and Guide, which promotes \n\nProduct monitoring and maintenance \n\nin - (a) Product Definitions and Product \n\ndesign, development, and implementation; \n\n(b) data processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the monitoring and \n\nmaintenance of Machine Learning. \n\nTo provide Product Teams with \n\nreliable guidance on how to -\n\n(a) define, monitor and maintain \n\nacceptable operating bounds, \n\nincluding, inter alia, guarding \n\nagainst model drift; (b) define \n\nand review alert conditions \n\nand severity; and (c) create \n\nscenario playbooks regarding \n\nresponsibility, escalation, roll-\n\nback and resolution. \n\n5.5.2.  Product Monitoring \n\n& Maintenance \n\nProcedures \n\nA set of Procedures to operationalise the \n\nMonitoring & Maintenance Policy should \n\nbe developed and implemented within \n\nProducts in light of Product Definitions, \n\nthe Product Risk Classification Portfolio, \n\nand the Product Lifecycle and Workflow \n\nDescriptions. \n\nTo ensure the monitoring and \n\nmaintenance of Products and \n\nModels. \n\n5.5.3.  Monitoring & \n\nMaintenance \n\nAssessments \n\nProducts should regularly complete \n\nMonitoring & Maintenance assessments \n\naccording to Product Lifecycle and \n\nWorkflow Descriptions to the extent \n\nthat is reasonably practical. Assessment \n\nfindings ought to be documented by \n\nthe Product Team and reviewed by Data \n\nScience Managers and, when relevant, the \n\nManagement Committee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct monitoring and \n\nmaintenance at regular intervals \n\nwithin the Product Lifecycle and \n\nWorkflow Description. \n\n5.5.4.  Review of the \n\nMonitoring & \n\nMaintenance Policy \n\nThe Monitoring & Maintenance Policy \n\nshould be reviewed periodically, or \n\nif significant changes occur, by Data \n\nScience Managers to ensure its continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that the Monitoring & \n\nMaintenance Policy is kept up-to-\n\ndate. \n\n5.5.5.  Review of Product \n\nMonitoring & \n\nMaintenance \n\nProcedures \n\nProduct Monitoring & Maintenance \n\nProcedures should be reviewed \n\nperiodically, or if significant changes \n\noccur, by the Product Team to ensure their \n\ncontinued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that Monitoring & \n\nMaintenance Procedures are kept \n\nup-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n23 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo ensure Products and Models functions and outputs are explainable and justifiable as far as is practically \n\nreasonable. \n\n5.6 Explainability \n\nControl:  Aim: \n\n5.6.1.  Explainability \n\nPolicy \n\nA Policy and Guide, which promotes \n\nProduct Explainability in - (a) Product \n\nDefinitions and Product design, \n\ndevelopment, and implementation; (b) \n\ndata processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Explainability of \n\nMachine Learning. \n\nTo provide clear Organisation \n\nguidance to Products on how \n\nto - (a) ensure the justifiability \n\nof individual Model predictions \n\nand decisions; (b) maintain and \n\npromote Product and Model \n\ninner workings amongst Product \n\nTeams, Business Stakeholders, \n\nOrganisation Stakeholders \n\nand end-consumers; and (c) \n\nprovide Model transparency for \n\nauthorities, Special Interest \n\nGroups and/or the Public. \n\n5.6.2.  Product \n\nExplainability \n\nProcedures \n\nA set of Procedures to operationalise the \n\nExplainability Policy should be developed \n\nand implemented within Products in light \n\nof Product Definitions, the Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure the Explainability of \n\nProducts and Models. \n\n5.6.3.  Explainability \n\nAssessments \n\nProducts should regularly complete \n\nExplainability assessments according \n\nto Product Lifecycle and Workflow \n\nDescriptions to the extent that is \n\nreasonably practical. Assessment \n\nfindings ought to be documented by \n\nthe Product Team and reviewed by Data \n\nScience Managers and, when relevant, the \n\nManagement Committee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Explainability at \n\nregular intervals within the \n\nProduct Lifecycle and Workflow \n\nDescription. \n\n5.6.4.  Review of the \n\nExplainability \n\nPolicy \n\nThe Explainability Policy should be \n\nreviewed periodically, or if significant \n\nchanges occur, by Data Science Managers \n\nto ensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Explainability \n\nPolicy is kept up-to-date. \n\n5.6.5.  Review of Product \n\nExplainability \n\nProcedures \n\nProduct Explainability Procedures should \n\nbe reviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product \n\nExplainability Procedures are kept \n\nup-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n24 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo (a) prevent adversarial actions against, and encourage graceful failures for, Products and/or Models; (b) avert \n\nmalicious extraction of Models, data and/or intellectual property; (c) prevent Model based physical or irreparable \n\nharms; and (d) prevent erosion of trust in outputs or methods. \n\n5.7 Safety & Security \n\nControl:  Aim: \n\n5.7.1.  Safety & Security \n\nPolicy \n\nA Policy and Guide, which promotes \n\nProduct Safety & Security in - (a) \n\nProduct Definitions and Product design, \n\ndevelopment, and implementation; (b) \n\ndata processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Safety of Machine \n\nLearning. \n\nTo provide clear Organisation \n\nguidance to Products on how \n\nto identify and guard against \n\nProduct vulnerabilities from \n\nAdversarial Actions. \n\n5.7.2.  Product Safety \n\nProcedures \n\nA set of Procedures to operationalise the \n\nSafety Policy should be developed and \n\nimplemented within Products in light of \n\nProduct Definitions, the Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions . \n\nTo ensure the Safety of Products \n\nand Models. \n\n5.7.3.  Safety \n\nAssessments \n\nProducts should regularly complete \n\nSafety assessments according to Product \n\nLifecycle and Workflow Descriptions to \n\nthe extent that is reasonably practical. \n\nAssessment findings ought to be \n\ndocumented by the Product Team and \n\nreviewed by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Safety at regular intervals \n\nwithin the Product Lifecycle and \n\nWorkflow Description. \n\n5.7.4.  Review of the \n\nSafety Policy \n\nThe Safety Policy should be reviewed \n\nperiodically, or if significant changes \n\noccur, by Data Science Managers to ensure \n\nits continued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that the Safety Policy is \n\nkept up-to-date. \n\n5.7.5.  Review of Product \n\nSafety Procedures \n\nProduct Safety Procedures should be \n\nreviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product Safety \n\nProcedures are kept up-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n25 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo ensure (a) building desirable solutions; (b) human control over Products and Models; and (c) that individuals \n\naffected by Products and Models outputs can obtain redress. \n\nControl:  Aim: \n\n5.8.1.  Human-centric \n\nDesign & Redress \n\nPolicy \n\nA Policy and Guide, which promotes \n\nProduct Human-Centric Design & Redress \n\nin - (a) Product Definitions and Product \n\ndesign, development, and implementation; \n\n(b) data processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Human-Centric \n\nDesign & Redress of Machine \n\nLearning. \n\nTo provide clear Organisation \n\nguidance to Products on how to -\n\n(a) adds value and desirability of \n\nyour product for end users; \n\n(b) assess and implement human-\n\nin-control requirements; and \n\n(c) assess and implement human-\n\ncentric remediation and redress. \n\n5.8.2.  Product Human-\n\nCentric Design \n\n& Redress \n\nProcedures \n\nA set of Procedures to operationalise the \n\nHuman-Centric Design & Redress Policy \n\nshould be developed and implemented \n\nwithin Products in light of Product \n\nDefinitions, the Product Risk Classification \n\nPortfolio, and the Product Lifecycle and \n\nWorkflow Descriptions. \n\nTo ensure the Human-Centric \n\nDesign & Redress of Products and \n\nModels. \n\n5.8.3.  Human-Centric \n\nDesign & Redress \n\nAssessments \n\nProducts should regularly complete \n\nHuman-Centric Design & Redress \n\nassessments according to Product \n\nLifecycle and Workflow Descriptions to \n\nthe extent that is reasonably practical. \n\nAssessment findings ought to be \n\ndocumented by the Product Team and \n\nreviewed by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Human-Centric Design \n\n& Redress at regular intervals \n\nwithin the Product Lifecycle and \n\nWorkflow Description. \n\n5.8.4.  Review of the \n\nHuman-Centric \n\nDesign & Redress \n\nPolicy \n\nThe Human-Centric Design Policy & \n\nRedress should be reviewed periodically, \n\nor if significant changes occur, by Data \n\nScience Managers to ensure its continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that the Human-Centric \n\nDesign & Redress Policy is kept \n\nup-to-date. \n\n5.8.5.  Review of Product \n\nHuman-Centric \n\nDesign & Redress \n\nProcedures \n\nProduct Human-centric Design & \n\nRedress Procedures should be reviewed \n\nperiodically, or if significant changes \n\noccur, by the Product Team to ensure their \n\ncontinued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that Product Human-\n\nCentric Design & Redress \n\nProcedures are kept up-to-date. \n\n5.8 Human-Centric Design & Redress Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n26 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo prevent (in)direct adverse social and environmental effects as a consequence of interactions amongst \n\nProducts, Models, the Organisation, and the Public. \n\nControl:  Aim: \n\n5.9.1.  Systemic Stability \n\nPolicy \n\nA Policy and Guide, which promotes \n\nawareness and control of systemic effects \n\nand interactions in - (a) Product Definitions \n\nand Product design, development, and \n\nimplementation; (b) data processing; and \n\n(c) Model design, development, training, \n\nand output ought to be derived by Data \n\nScience Managers and approved by the \n\nManagerial Committee. \n\nTo ensure the Systemic Stability \n\nof Machine Learning. \n\nTo provide clear Organisation \n\nguidance to Products on how to \n\nidentify, analyse and prevent risks \n\nderived from (higher-order and/or \n\nhighly complex) relations between \n\nProducts, Models, Product design, \n\nOrganisation processes and \n\nsociety at large. \n\n5.9.2.  Product Systemic \n\nStability \n\nProcedures \n\nA set of Procedures to operationalise \n\nthe Systemic Stability Policy should be \n\ndeveloped and implemented within \n\nProducts in light of Product Definitions, \n\nthe Product Risk Classification Portfolio, \n\nand the Product Lifecycle and Workflow \n\nDescriptions. \n\nTo ensure the Systemic Stability \n\nof Products. \n\n5.9.3.  Systemic Stability \n\nAssessments \n\nProducts should regularly complete \n\nSystemic Stability according to Product \n\nLifecycle and Workflow Descriptions to \n\nthe extent that is reasonably practical. \n\nAssessment findings ought to be \n\ndocumented by the Product Team and \n\nreviewed by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Systemic Stability at \n\nregular intervals within the \n\nProduct Lifecycle and Workflow \n\nDescription. \n\n5.9.4.  Review of the \n\nSystemic Stability \n\nPolicy \n\nSystemic Stability should be reviewed \n\nperiodically, or if significant changes \n\noccur, by Data Science Managers to ensure \n\nits continued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that the Systemic \n\nStability Policy is kept up-to-date. \n\n5.9.5.  Review of \n\nSystemic Stability \n\nProcedures \n\nProduct Systemic Stability should be \n\nreviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product Systemic \n\nStability Procedures are kept up-\n\nto-date. \n\n5.9 Systemic Stability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n27 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo ensure the clear and complete Traceability of Products, Models and their assets (inclusive of, inter alia, data, \n\ncode, artifacts, output, and documentation) for as long as is reasonably practical. \n\nObjective \n\nTo ensure that Product decision-making is done in a clear, informed, unbiased and collaborative manner with a \n\ndiversity of Organisation opinions, when relevant. \n\nControl:  Aim: \n\n5.10.1.  Product \n\nTraceability Policy \n\nA Policy and Guide, which promotes \n\nProduct Traceability during and after - (a) \n\nProduct Definitions and Product design, \n\ndevelopment, and implementation; (b) \n\ndata processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Product Traceability \n\nof Machine Learning. \n\nTo provide clear Organisation \n\nguidance to Products on how to \n\nmanage Product Traceability. \n\n5.10.2.  Product \n\nTraceability \n\nProcedures \n\nA set of Procedures to operationalise the \n\nTraceability Policy should be developed \n\nand implemented within Products in light \n\nof Product Definitions, the Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure the Product Traceability \n\nof Products. \n\n5.10.3.  Review of \n\nthe Product \n\nTraceability Policy \n\nThe Product Traceability should be \n\nreviewed periodically, or if significant \n\nchanges occur, by Data Science Managers \n\nto ensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Product \n\nTraceability Policy is kept up-to-\n\ndate. \n\n5.10.4.  Review of Product \n\nTraceability \n\nProcedures \n\nProduct Product Traceability should be \n\nreviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product Product \n\nTraceability Procedures are kept \n\nup-to-date. \n\nControl:  Aim: \n\n5.11.1.  Product \n\nDecision-Making \n\nPolicy \n\nA Policy which promotes Product decision-\n\nmaking clarity in - (a) Product Definitions \n\nand Product design, development, and \n\nimplementation; (b) data processing; \n\n(c) Model design, development, training, \n\nand output; and (d) consideration for the \n\nProduct Risk Classification Portfolio ought \n\nto be derived by Data Science Managers \n\nand approved by the Managerial Committee. \n\nTo ensure Product decisions - (a) are \n\nbased on all available information, \n\ninclusive of those derived from \n\nPolicies and Procedures of this \n\ndocument; (b) follow from and are \n\naligned with the Product Lifecycle \n\nand Workflow Description; and (c) \n\nare made with reasonable care to \n\navoid cognitive bias. \n\n5.10 Product Traceability \n\n5.11 Product Decision-Making Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n28 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\n5.11.2.  Product \n\nDecision-Making \n\nProcedures \n\nA set of Procedures to operationalise the \n\nDecision-Making Policy within Products \n\nshould be developed and implemented, \n\ninclusive of consultations with Business \n\nStakeholders. \n\nTo ensure clear, informed, unbiased \n\nand collaborative decision-making \n\nin Products. \n\n5.11.3.  Product \n\nDecision-Making \n\nDiversity \n\nA diversity of Organisation stakeholder and \n\nBusiness Stakeholder opinions and input \n\nshould be obtained, considered and, when \n\nrelevant, weighed when making material \n\nProduct decisions \n\nTo ensure a diversity of opinions \n\nwhen making material Product \n\ndecisions. \n\n5.11.4.  Product \n\nDecision-Making \n\nDocumentation \n\nProduct decisions should be documented, \n\nindexed, stored and, when relevant, \n\nreviewed. \n\nTo ensure that Product decisions \n\nare documented and indexed to \n\nwarrant their effective management \n\nand oversight. \n\nObjective \n\nTo ensure that Products have sufficient capacity and capabilities to meet Product Definitions. \n\nControl:  Aim: \n\n5.12.1.  Financial \n\nResource \n\nAllocation \n\nSufficient financial resources ought \n\nto be allocated to Products based on \n\ntheir Product Definitions, Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure that Products have \n\nsufficient financial resources to \n\nallow for their success. \n\n5.12.2.  Human Resource \n\nAllocation \n\nSufficient human resources ought \n\nto be allocated to Products based on \n\ntheir Product Definitions, Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure that Products have \n\nsufficient human resources to allow \n\nfor their success. \n\n5.12.3.  Assets Allocation  Sufficient Assets ought to be allocated \n\nto Products based on their Product \n\nDefinitions, Product Risk Classification \n\nPortfolio, and the Product Lifecycle and \n\nWorkflow Descriptions. \n\nTo ensure that Products have \n\nsufficient Assets to allow for their \n\nsuccess. \n\n5.12.4.  Software \n\nAllocation \n\nSufficient Software ought to be allocated \n\nto Products based on their Product \n\nDefinitions, Product Risk Classification \n\nPortfolio, and the Product Lifecycle and \n\nWorkflow Descriptions. \n\nTo ensure that Products have \n\nsufficient Software to allow for their \n\nsuccess. \n\n5.12.5.  Knowledge & \n\nDevelopment \n\nProduct Teams should receive sufficient \n\ntraining and development based on \n\nthe Product Definitions, Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure that Product Teams have \n\nsufficient training and development \n\nto allow for their success. \n\n5.12 Product Capabilities Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n29 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\n5.12.6.  Review of \n\nProduct \n\nCapabilities \n\nProduct resource allocation ought to be \n\nperiodically reviewed, or if significant \n\nchanges occur, by Data Science Managers, \n\nin consultation with Product Owners, to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Products resources \n\nare sufficiently maintained. \n\nObjective \n\nTo promote the documentation and recording of Product, Product Team and employees tasks, deliverables and \n\nprogress. \n\nControl:  Aim: \n\n5.13.1.  Product Record  A clear and detailed Product record should \n\nbe continually kept of Product design, \n\ndevelopment, implementation, deliverables, \n\nprogress and employee tasks. \n\nTo ensure that a clear Product \n\nrecord is kept to warrant effective \n\noversight, management and \n\naccountability. \n\n5.13.2.  Employee and \n\nProduct Team \n\nDocumentation \n\nEmployee and Product Team processes \n\nought to promote the documentation of \n\nProduct tasks, discussions and deliverables \n\nwhen relevant and as much as is reasonably \n\npractical. \n\nTo ensure that sufficient \n\ndocumentation is kept to warrant \n\neffective oversight, management \n\nand accountability. \n\n5.13.3.  Log of User \n\nAccess \n\nA formal log of user access rights to \n\nProducts should be maintained and \n\nreviewed at regular intervals by Product \n\nOwners. \n\nTo ensure the management, \n\nintegrity and review of user access. \n\n5.13.4.  Event Logs  Event logs of Product user activities, \n\nexceptions, and faults should be produced, \n\nkept and regularly reviewed by Product \n\nOwners. \n\nTo formally index and manage \n\nuser activities to maintain Product \n\noversight and integrity. \n\n5.13 Product Record Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n30 FBPML Organisation Best Practices v1.0.0. \n\n# Section 6. Product Validation \n\nTo autonomously and impartially validate Products, Models and their outputs. \n\nObjective \n\nControl:  Aim: \n\n6.1.  Validation \n\nDepartment \n\nThe Organisation ought to have a \n\ndepartment that is independent of Data \n\nScience Managers and Product Teams \n\nwho validate Products. \n\nTo validate Product compliance, \n\nethics, and performance. \n\n6.2.  Validation Policy  A Policy detailing the responsibilities \n\nof the Validation Department and the \n\nrequirements for Products to be Validated \n\nought to be derived by the Managerial \n\nCommittee. \n\nTo ensure the validity of \n\nOrganisation Machine Learning. \n\n6.3.  Validation \n\nProcedures \n\nThe Validation Department should develop \n\nand implement a set of Procedures to \n\noperationalise the Validation Policy within \n\nthe Organisation. \n\nTo validate Product compliance, \n\nEthics, and performance within the \n\nOrganisation. \n\n6.4.  Review of the \n\nValidation Policy \n\nThe Validation Policy should be reviewed \n\nperiodically, or if significant changes \n\noccur, by the Managerial Committee \n\nto ensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Validation Policy \n\nis kept up-to-date. \n\n6.5.  Review of Product \n\nValidation \n\nProcedures \n\nProduct Validation Procedures should \n\nbe reviewed periodically, or if significant \n\nchanges occur, by the Validation \n\nDepartment to ensure their continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that Product Validation \n\nProcedures are kept up-to-date.", "fetched_at_utc": "2026-02-08T18:50:37Z", "sha256": "1a8bfb7dcffa71d485b7784adf9aed85d38640996ac030621c115ef35f820c90", "meta": {"file_name": "FBPML_OrganisationBP_V1.0.0-17-30.pdf", "file_size": 378097, "relative_path": "pdfs\\FBPML_OrganisationBP_V1.0.0-17-30.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 8566}}}
{"doc_id": "pdf-pdfs-fbpml-organisationbp-v1-0-0-38-40-030bf251474f", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_OrganisationBP_V1.0.0-38-40.pdf", "title": "FBPML_OrganisationBP_V1.0.0-38-40", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder. \n\n38 FBPML Organisation Best Practices v1.0.0. \n\n# Section 11. Third Party Contracts \n\n# Management \n\nTo ensure the integrity and implementation of Policies and Procedures in third-party contracts. \n\nObjective \n\nControl:  Aim: \n\n11.1.  Policies and \n\nProcedures \n\nfor Third-Party \n\nContracts \n\nProcedures ought to be designed and \n\nimplemented to ensure that Policies and \n\nProcedures are legally enforceable in \n\nrelevant third-party contracts. \n\nTo ensure that Policies and \n\nProcedures are implemented by \n\nthird-party contractors. \n\n11.2.  Compliance \n\nin Third-Party \n\nContracts \n\nProcedures should be implemented to \n\nensure that Policies and Procedures are \n\ncomplied with to agreed-upon standards \n\nin relevant third-party contracts. \n\nTo ensure that Policies and \n\nProcedures are complied with by \n\nthird-party contractors. \n\n11.3.  Monitoring, Review \n\nand Auditing \n\nof Third-Party \n\nContracts \n\nThe Organisation should regularly monitor, \n\nreview and audit third party contracts \n\nto warrant third-party compliance with \n\nPolicies and Procedures. \n\nTo ensure effective oversight of \n\nthird-party contractors. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n39 FBPML Organisation Best Practices v1.0.0. \n\n# Section 12. Ethics & \n\n# Transparency Management \n\nTo ensure that Products are transparent and ethical. \n\nObjective \n\nControl:  Aim: \n\n12.1.  Ethics Policy  An ethics policy, which promotes Ethical Practices \n\nin Machine Learning, ought to be derived by Data \n\nScience Managers and approved by the Management \n\nCommittee and Ethics Committee. The Ethics Policy \n\nmust be communicated to the Public. \n\nTo ensure that Machine \n\nLearning is designed, \n\ndeveloped and implemented \n\nin accordance with the \n\nEthical Practices. \n\n12.2.  Review of the \n\nEthics Policy \n\nThe Ethics Policy should be reviewed periodically, \n\nor if significant changes occur, by Data Science \n\nManagers to ensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Ethics \n\nPolicy is kept up-to-date. \n\n12.3.  Transparency \n\nPolicy \n\nA Public Interest and Transparency Policy, which \n\npromotes Public engagement, regulator engagement, \n\nand Transparency in Machine Learning, ought to be \n\nderived by Data Science Managers and approved by \n\nthe Management Committee and Ethics Committee. \n\nThe Transparency Policy must be communicated to \n\nthe Public. \n\nTo ensure that Machine \n\nLearning is made \n\ntransparent to the Public and \n\nis designed, developed and \n\nimplemented in accordance \n\nwith the Public Interest. \n\n12.4.  Review of the \n\nTransparency \n\nPolicy \n\nThe Transparency Policy should be reviewed \n\nperiodically, or if significant changes occur, by \n\nData Science Managers to ensure its continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that the \n\nTransparency Policy is kept \n\nup-to-date. \n\n12.5.  Speaking-Out \n\nPolicy \n\nA policy, which promotes Product Teams and/ \n\nor Product Teams members to speak-out against \n\nunethical practices, ought to be derived by Data \n\nScience Managers and approved by the Management \n\nCommittee and Ethics Committee. \n\nTo ensure that Product \n\nTeams and/or Product Teams \n\nmembers have a safe space \n\nto voice their concerns about \n\nMachine Learning and/or \n\nProducts practices and/or \n\ndecisions. \n\n12.6.  Review of the \n\nSpeaking-Out \n\nPolicy \n\nThe Speaking-Out Policy should be reviewed \n\nperiodically, or if significant changes occur, by \n\nData Science Managers to ensure its continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that the Speaking-\n\nOut Policy is kept up-to-date. \n\n12.7.  Contact with \n\nAuthorities \n\nAppropriate contact with relevant sector authorities \n\nregarding Products, and their implementation, should \n\nbe maintained. Products and Product Teams ought \n\nto work in close collaboration with relevant sector \n\nauthorities in a collaborative and bona fide manner. \n\nTo ensure awareness and \n\noversight of Products by \n\nauthorities. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder. \n\n40 FBPML Organisation Best Practices v1.0.0. \n\n# Section 13. Compliance, Auditing & \n\n# Legal Management and Oversight \n\nTo ensure that Policies and Procedures are designed, developed and implemented in accordance with the law, \n\nindustry best practices, contractual obligations, and/or regulatory Guidelines. \n\nObjective \n\nControl:  Aim: \n\n13.1.  Identification of \n\nLaws, Regulations and \n\nContracts \n\nRelevant laws, regulations and contracts \n\nconcerning Machine Learning ought to be \n\nidentified and documented. \n\nTo ensure that relevant \n\nMachine Learning laws, \n\nregulations and contracts \n\nhave been identified and \n\ndocumented. \n\n13.2.  Response to Laws, \n\nRegulations and \n\nContracts \n\nProcedures should be implemented to \n\nincorporate relevant laws, regulations and \n\ncontracts into Policies and Procedures. \n\nTo ensure that relevant \n\nMachine Learning laws, \n\nregulations and contracts are \n\nimplemented and abided by. \n\n13.3.  Privacy and \n\nProtection of \n\nPersonally Identifiable \n\nInformation \n\nProcedures \n\nProcedures should be implemented to \n\nensure the privacy and protection of \n\npersonally identifiable information in \n\nProducts as required in law, regulations \n\nand/or contracts. \n\nTo ensure that relevant data \n\nprotection and privacy laws, \n\nregulations and contracts are \n\nimplemented and abided by. \n\n13.4.  Intellectual Property \n\nRights Procedures \n\nProcedures should be implemented \n\nto protect intellectual property rights \n\nand the use of proprietary products in \n\nProducts as required in law, regulations \n\nand/or contracts. \n\nTo ensure that relevant \n\nintellectual property rights \n\nlaws, regulations and contracts \n\nare implemented and abided \n\nby. \n\n13.5.  Internal Audit \n\nof Policies and \n\nProcedures \n\nDefined organisational employees should \n\naudit the implementation of Policies and \n\nProcedures within the Organisation. \n\nTo ensure the quality and \n\nintegrity of implemented \n\nPolicies and Procedures.", "fetched_at_utc": "2026-02-08T18:50:39Z", "sha256": "030bf251474f2fb439bcdc19ac0189618f6a9aa19003db9a3e6fbb1a949225ec", "meta": {"file_name": "FBPML_OrganisationBP_V1.0.0-38-40.pdf", "file_size": 143814, "relative_path": "pdfs\\FBPML_OrganisationBP_V1.0.0-38-40.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 1436}}}
{"doc_id": "pdf-pdfs-fbpml-organisationbp-v1-0-0-7-15-fcfbd902d81c", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_OrganisationBP_V1.0.0-7-15.pdf", "title": "FBPML_OrganisationBP_V1.0.0-7-15", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n7FBPML Organisation Best Practices v1.0.0. \n\n1.1.  Adversarial Action  means actions characterised by mala fide (malicious) intent and/or bad \n\nfaith. \n\n1.2.  Assessment  means the action or process of making a series of determinations and \n\njudgments after taking deliberate steps to test, measure and collectively \n\ndeliberate the objects of concern and their outcomes. \n\n1.3.  Assets  means information technology hardware that concerns Products Machine \n\nLearning. \n\n1.4.  Business Stakeholders  means the departments and/or teams within the Organisation who do \n\nnot conduct data science and/or technical Machine Learning, but have a \n\nmaterial interest in Products Machine Learning. \n\n1.5.  Corporate Governance \n\nPrinciples \n\nmean the structure of rules, practices and processes used to direct and \n\nmanage a company in terms of industry recognised and published legal \n\nguidelines. \n\n1.6.  Data Governance  means the systems of governance and/or management over data assets \n\nand/or processes within an Organisation. \n\n1.7.  Data Quality  means the calibre of qualitative or quantitative data. \n\n1.8.  Data Science  means an interdisciplinary field that uses scientific methods, processes, \n\nalgorithms and computational systems to extract knowledge and insights \n\nfrom structured and/or unstructured data. \n\n1.9.  Domain  means the societal and/or commercial environment within which the \n\nProduct will be and/or is operationalised. \n\n1.10.  Ethical Practices  means the ethical principles, values and/or practices that are \n\nencapsulated and promoted in an â€˜artificial intelligenceâ€™ ethics guideline \n\nand/or framework, such as (a) The Asilomar AI Principles (Asilomar AI \n\nPrinciples, 2017), (b) The Montreal Declaration for Responsible AI (Montreal \n\nDeclaration, 2017), (c) The Ethically Aligned Design: A Vision for Prioritizing \n\nHuman Well-being with Autonomous and Intelligent Systems (IEEE, 2017), \n\nand/or (d) any other analogous guideline and/or framework. \n\n1.11.  Ethics Committee  means the committee within the Organisation charged with managing and/ \n\nor directing organisation Ethical Practices. \n\n1.12.  Executive Management  means the managerial team at the highest level of management within the \n\nOrganisation. \n\n1.13.  Explainability  means the property of Models and Model outcomes to be interpreted and/ \n\nor explained by humans in a comprehensible manner. \n\nAs used in this Best Practice Guideline, the following terms shall have the following meanings where capitalised. \n\nAll references to the singular shall include references to the plural, where applicable, and vice versa. Any terms \n\nnot defined or capitalised in this Best Practice Guideline shall hold their plain text meaning as cited in English and \n\ndata science. \n\n# Section 1. Definitions Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n8FBPML Organisation Best Practices v1.0.0. \n\n1. Definitions - FBPML Organisation Best Practices v1.0.0. \n\n1.14.  Fairness & Non-\n\nDiscrimination \n\nmeans the property of Models and Model outcomes to be free from bias \n\nagainst protected classes. \n\n1.15.  Best Practice Guideline  means this document. \n\n1.16.  Guide  means an established and clearly documented series of actions or \n\nprocess(es) conducted in a certain order or manner to achieve particular \n\noutcomes. \n\n1.17.  Human-Centric Design \n\n& Redress \n\nmeans orienting Products and/or Models to focus on humans and their \n\nenvironments through promoting human and/or environment centric \n\nvalues and resources for redress. \n\n1.18.  Incident  means the occurrence of a technical event that affects the integrity of a \n\nProduct and/or Model. \n\n1.19.  Machine Learning  means the use and development of computer systems and Models that \n\nare able to learn and adapt with minimal explicit human instructions by \n\nusing algorithms and statistical modelling to analyse, draw inferences, and \n\nderive outputs from data. \n\n1.20.  Model  means Machine Learning algorithms and data processing designed, \n\ndeveloped, trained and implemented to achieve set outputs, inclusive of \n\ndatasets used for said purposes unless otherwise stated. \n\n1.21.  Organisation  means the concerned juristic entity designing, developing and/or \n\nimplementing Machine Learning. \n\n1.22.  Performance \n\nRobustness \n\nmeans the propensity of Products and/or Models to retain their desired \n\nperformance over diverse and wide operational conditions. \n\n1.23.  Policy  means a documented course of normative actions or set of principles \n\nadopted to achieve a particular outcome. \n\n1.24.  Procedure  means an established and defined series of actions or process(es) \n\nconducted in a certain order or manner to achieve a particular outcome. \n\n1.25.  Product  means the collective and broad process of design, development, \n\nimplementation and operationalisation of Models, and associated \n\nprocesses, to execute and achieve Product Definitions, inclusive of, inter \n\nalia, the integration of such operations and/or Models into organisation \n\nproducts, software and/or systems. \n\n1.26.  Product Team  means the collective group of Organisation employees directly charged \n\nwith designing, developing and/or implementing the Product. \n\n1.27.  Product Lifecycle  means the collective phases of Products from initiation to termination \n\n- such as design, exploration, experimentation, development, \n\nimplementation, operationalisation, and decommissioning - and their \n\nmutual iterations. \n\n1.28.  Product Owner  means the employee charged with (a) managing and maximising the \n\nvalue of the Product and its Product Team; and (b) engaging with various \n\nBusiness Stakeholders concerning the Product and its Product Definitions. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n9FBPML Organisation Best Practices v1.0.0. \n\n1. Definitions - FBPML Organisation Best Practices v1.0.0. \n\n1.29.  Public  means society at large. \n\n1.30.  Public Interest  means the welfare or well-being of the Public. \n\n1.31.  Representativeness  means the degree to which datasets and Models reflect the true \n\ndistribution and conditions of Subjects, Subject populations, and/or \n\nDomains. \n\n1.32.  Safety & Security  means (a) the resilience of Products and/or Models against malicious and/ \n\nor negligent activities that result in Organisational loss of control over \n\nconcerned Products and/or Models; and (b) real Product Domain based \n\nphysical harms that result through Products and/or Models applications \n\n1.33.  Social Corporate \n\nResponsibilities \n\nmeans the structure of rules, practices and processes used to direct and \n\nmanage a company in terms of industry recognised and published legal \n\nguidelines to positively contribute to economic, environmental and social \n\nprogress. \n\n1.34.  Software  means information technology software that concerns Products Machine \n\nLearning. \n\n1.35.  Special Interest Groups  means a specific body politic, or a particular collective of citizens, who can \n\nreasonably be determined to have a material interest in the Product. \n\n1.36.  Specification  means the accuracy, completeness and exactness of Products, Models \n\nand/or datasets in reflecting Product Definitions, Product Domains and/ \n\nor Product Subjects, either in their design and development and/or \n\noperationalisation. \n\n1.37.  Subjects  means the entities and/or objects that are represented as data points in \n\ndatasets and/or Models, and who may be the subject of Product and/or \n\nModel outcomes. \n\n1.38.  Systemic Stability  means the stability of Organisation, Domain, society and environments as \n\na collective ecosystem. \n\n1.39.  Traceability  means the ability to trace, recount, and reproduce Product outcomes, \n\nreports, intermediate products, and other artifacts, inclusive of Models, \n\ndatasets and codebases. \n\n1.40.  Transparency  means the provision of an informed target audiences understanding of \n\nOrganisation and/or Products Machine Learning, and their workings, based \n\non documented Organisation information. \n\n1.41.  Workflows  means the coordinated and standardised sequences of employee work \n\nactivities, processes, and tasks. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder. \n\n10 FBPML Organisation Best Practices v1.0.0. \n\n# Part A \n\n# Organisation Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n11 FBPML Organisation Best Practices v1.0.0. \n\n# Section 2. Managerial Oversight \n\n# & Management \n\nObjective \n\nTo ensure managerial direction and support for Products in accordance with Organisation strategies, business \n\nrequirements, Corporate Governance Principles, Social Corporate Responsibilities, legal regulations and Ethical \n\nPractices. \n\n2.1 Management Direction for Machine Learning \n\nControl:  Aim: \n\n2.1.1.  Management \n\nCommittee \n\nA managerial committee ought to be \n\nestablished to (a) oversee Organisation \n\nMachine Learning and Products; and \n\n(b) warrant their effective alignment in \n\naccordance with Organisation strategies, \n\nbusiness requirements, Corporate \n\nGovernance Principles, Social Corporate \n\nResponsibilities, legal regulations and \n\nEthical Practices. \n\nTo ensure clear managerial \n\nresponsibility, oversight and \n\ncustody of Organisation Machine \n\nLearning and Products. \n\n11.1.2.  Management \n\nCommittee \n\nDiversity \n\nThe Management Committee ought to \n\nhold a diversity of members from differing \n\nOrganisation departments, including \n\nExecutive Management, legal, finance, \n\noperations, public communications as \n\nwell as Data Science. \n\nTo (a) ensure the diversity of \n\nmanagerial opinions and oversight \n\nof Organisation Machine Learning \n\nand Products; and (b) foster \n\nOrganisation buy-in for Machine \n\nLearning and Products. \n\n11.1.3.  Managerial \n\nOversight \n\nProcedures \n\nThe Management Committee should \n\nestablish appropriate Procedures to \n\nwarrant managerial oversight and \n\ngovernance of Organisation Products, \n\ninclusive of the appointment of Data \n\nScience Managers. \n\nTo ensure the operationalisation \n\nof the oversight and management \n\nof Organisation Machine Learning \n\nand Products by the Management \n\nCommittee. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n12 FBPML Organisation Best Practices v1.0.0. \n\n# Section 3. Internal Organisation \n\n# Management & Oversight \n\nObjective \n\nTo establish managerial Procedures to control and oversee the design, development and implementation of \n\nProducts. \n\n3.1 Internal Organisation \n\nControl:  Aim: \n\n3.1.1.  Data Science \n\nManagers \n\nThe Management Committee should \n\nappoint Data Science Managers to oversee \n\nProducts and warrant their effective \n\nalignment in accordance with the \n\ndirectives of the Management Committee, \n\nPolicies, and, more broadly, Organisation \n\nstrategies, business requirements, \n\nCorporate Governance Principles, \n\nSocial Corporate Responsibilities, legal \n\nregulations and Ethical Practices. \n\nTo ensure the clear management, \n\noversight, ownership and custody \n\nof Products. \n\n3.1.2.  Data Science \n\nManagers Products \n\nOwnership and \n\nCustody \n\nThe Management Committee ought to \n\ndefine and allocate to Data Science \n\nManagers Products. \n\nTo ensure clear managerial \n\noversight, ownership and custody \n\nof Products. \n\n3.1.3.  Data Science \n\nManagers \n\nSegregation of \n\nDuties \n\nConflicting duties and areas of \n\nresponsibility of Data Science Managers \n\nshould be segregated to reduce \n\nopportunities for the unauthorised and/or \n\nunintentional modification and/or misuse \n\nof Products. \n\nTo reduce the threat of Product \n\nabuse, misuse and/or mala fide \n\nactions by Data Science Managers. \n\n3.1.4.  Product Owners  Data Science Managers ought to appoint \n\nProduct Owners to (a) oversee specific \n\nProducts and Product Teams; and (b) \n\nwarrant their effective management in \n\naccordance with the directives of Data \n\nScience Managers, the Management \n\nCommittee, and Organisation Policies. \n\nTo ensure the clear management, \n\noversight, ownership and custody \n\nof a Product and its Product Team. \n\n3.1.5.  Product Owners \n\nOwnership and \n\nCustody \n\nData Science Managers ought to define \n\nand allocate to designated Product \n\nOwners Products and Product Teams. \n\nTo ensure clear managerial \n\noversight, ownership and custody \n\nof a Product and its Product Team. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n13 FBPML Organisation Best Practices v1.0.0. \n\n3. Internal Organisation Management & Oversight - FBPML Organisation Best Practices v1.0.0. \n\n3.1.6.  Product Owners \n\nSegregation of \n\nDuties \n\nConflicting duties and areas of \n\nresponsibility of Product Owners should \n\nbe segregated to reduce opportunities \n\nfor the unauthorised and/or unintentional \n\nmodification and/or misuse of a Product. \n\nTo reduce the threat of Product \n\nabuse, misuse and/or mala fide \n\nactions by Product Owners. \n\n3.1.7.  Product Teams  Data Science Managers, in consultation \n\nwith Product Owners, should define and \n\nallocate Products to designated Product \n\nTeams. \n\nTo ensure clear Product ownership \n\nand custody. \n\n3.1.8.  Product Definitions  Data Science Managers, Product Owners, \n\nBusiness Stakeholders and, when relevant, \n\nProduct employees ought to collectively \n\ndocument and define clear Product \n\ndefinitions, aims, internal deliverables and \n\noutcomes. \n\nTo ensure Products have clear \n\nscopes to warrant (a) their \n\neffective oversight, management \n\nand execution, as well as (b) to \n\nallow for the accurate evaluation of \n\nProduct risks and controls. \n\n3.1.9.  Approval of \n\nProduct Definitions \n\nThe Management Committee should \n\nreview and approve Product Definitions. \n\nTo ensure managerial oversight of \n\nProducts scopes. \n\n3.1.10.  Product Definitions \n\nReview \n\nProduct Definitions ought to be reviewed \n\nperiodically, or if significant changes \n\noccur, by Data Science Managers, Product \n\nOwners, Business Stakeholders and, when \n\nrelevant, Product employees. \n\nTo ensure that Product Definitions \n\nare kept up-to-date to ensure \n\ntheir continued effectiveness, \n\nsuitability, and accuracy. \n\n3.1.11.  Product Risk \n\nClassification \n\nPolicy \n\nA Policy and Guide, which standarises \n\nthe approaches to assessing Product \n\nrisks, ought to be derived by Data Science \n\nManagers and approved by the Managerial \n\nCommittee. \n\nTo ensure that (a) clear guidelines \n\nexist on how to evaluate and \n\ndetermine Product based-risks for \n\nsubsequent evaluation in Product \n\nRisk Portfolios; and (b) Products \n\nare assigned risk-appropriate \n\nmandatory minimum capacity and \n\noversight. \n\n3.1.12.  Product Risk \n\nClassification \n\nPortfolio \n\nData Science Managers, Product Owners, \n\nBusiness Stakeholders and, when \n\nrelevant, Product employees ought to \n\ncollectively document and interrogate \n\n(a) Product Definitions and (b) Product \n\ndesign, development and implementation \n\nto identify Product based-risks and assign \n\nProduct risk values and classifications. \n\nTo ensure Products have clear \n\nrisk portfolios to warrant (a) their \n\neffective oversight, management \n\nand execution, as well as (b) to \n\nallow for the accurate evaluation of \n\nProduct risks and controls. \n\n3.1.13.  Approval of \n\nProduct Risk \n\nClassification \n\nPortfolio \n\nThe Management Committee should \n\nreview and approve Product Risk \n\nPortfolios. \n\nTo ensure managerial oversight of \n\nProducts risks. \n\n3.1.14.  Product Product \n\nRisk Classification \n\nPortfolio Review \n\nThe Product Risk Classification Portfolio \n\nought to be continuously reviewed and \n\ndeveloped by Data Science Managers, \n\nProduct Owners, Business Stakeholders \n\nand, when relevant, Product employees. \n\nTo ensure that Product Risk \n\nPortfolios are kept up-to-date \n\nto ensure their continued \n\neffectiveness, suitability, and \n\naccuracy. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n14 \n\n3. Internal Organisation Management & Oversight - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo establish Procedures to control the design, development and implementation of Products. \n\n3.2 Product Management \n\nControl:  Aim: \n\n3.2.1.  Product Lifecycle \n\nGuide \n\nData Science Managers and, when \n\nrelevant, Product Owners should derive \n\na clear Product Lifecycle Guide for the \n\nOrganisation. \n\nTo ensure a clear organisational \n\nProduct Lifecycle Guide to warrant \n\nthe effective management and \n\noversight of Machine Learning. \n\n3.2.2.  Product Lifecycle \n\nand Workflow \n\nDescriptions \n\nHaving consideration for the Product \n\nLifecycle Policy, Product Definitions, and \n\nthe Product Risk Classification Portfolio, \n\nProduct workflows ought to be derived, \n\ndeveloped, and documented by Data \n\nScience Managers, Product Owners and, \n\nwhen relevant, Product employees for \n\neach Product. \n\nTo ensure clear Lifecycle and \n\nWorkflows for Products to warrant \n\ntheir effective management and \n\noversight. \n\n3.2.3.  Reviewed of \n\nProduct Lifecycle \n\nGuide \n\nThe Product Lifecycle Guide should be \n\nreviewed and approved by Data Science \n\nManagers and, when relevant, the \n\nManagement Committee. \n\nTo ensure managerial oversight of \n\nthe Product Lifecycle Guide. \n\n3.2.4.  Reviewed of \n\nProduct Lifecycle \n\nand Workflow \n\nDescription \n\nProduct Lifecycle and Workflow \n\nDescriptions should be reviewed and \n\napproved by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo ensure managerial oversight of \n\nProduct Lifecycle and Workflow \n\nDescriptions. \n\n3.2.5.  Product Lifecycle \n\nand Workflow \n\nProcedures \n\nEach Product ought to derive, develop \n\nand implement a set of Procedures to \n\noperationalise Product Lifecycle and \n\nWorkflow Descriptions. \n\nTo ensure the operationalisation \n\nof Product Lifecycle and Workflow \n\nDescriptions. \n\n3.2.6.  Reviewed of \n\nProduct Lifecycle \n\nand Workflow \n\nProcedures \n\nThe Product Lifecycle and Workflow \n\nProcedures should be reviewed \n\nperiodically, or if significant changes \n\noccur, by the Product Team to ensure their \n\ncontinued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that Product Product \n\nLifecycle and Workflow Procedures \n\nare kept up-to-date. \n\n3.2.7.  Product Employee \n\nRoles and \n\nResponsibilities \n\nData Science Managers and Product \n\nOwners ought to define and allocate \n\nto Product employees defined \n\nresponsibilities and roles in terms \n\nof Product Lifecycle and Workflow \n\nDescriptions. \n\nTo establish clear employee \n\nresponsibilities and custodies in \n\nterms of Product Lifecycle and \n\nWorkflow Descriptions. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n15 \n\n3. Internal Organisation Management & Oversight - FBPML Organisation Best Practices v1.0.0. \n\n3.2.8.  Data Science \n\nManagers Reports \n\nFrequent reports detailing Product \n\nprogress, changes and risks ought to be \n\nmade to the Management Committee \n\nby Data Science Managers and, \n\nsubsequently, reviewed timeously. \n\nTo ensure the clear communication \n\nand management of Product \n\ndeliverables to the Management \n\nCommittee. \n\n3.2.9.  Product Owners \n\nReports \n\nFrequent reports detailing Product \n\nprogress, changes and risks ought to be \n\nmade to the Data Science Managers and \n\nBusiness Stakeholders by Product Owners \n\nand, subsequently, reviewed timeously. \n\nTo ensure the clear communication \n\nand management of Product \n\ndeliverables to Data Science \n\nManagers and Business \n\nStakeholders.", "fetched_at_utc": "2026-02-08T18:50:41Z", "sha256": "fcfbd902d81c60c193627bc07bacd52f3cc20de5f68f6d6794dc1679dbdb0dfc", "meta": {"file_name": "FBPML_OrganisationBP_V1.0.0-7-15.pdf", "file_size": 228824, "relative_path": "pdfs\\FBPML_OrganisationBP_V1.0.0-7-15.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4653}}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-13-30-166d9a6e0506", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-13-30.pdf", "title": "FBPML_TechnicalBP_V1.0.0-13-30", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n13 FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n2.1.  Product Team \n\nComposition \n\nDocument and define a clear diversity \n\nof Product Team roles and expertises \n\nneeded for the Product, inclusive of, \n\namongst other things, engineers, data \n\nscientists, Product Managers, and user \n\nexperience experts. Once established, \n\nrecruit accordingly. \n\nTo (a) assemble a robust team \n\nfor Product and/or Model design, \n\ndevelopment and deployment; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n2.2.  Product Team \n\nRoles \n\nDocument and allocate clear Product \n\nTeam roles and expectations for Product \n\nTeam members, including expectations \n\nfor, and the structure of, intra-Product \n\nTeam collaboration and overlapping \n\nresponsibilities. \n\nTo (a) ensure that Product Team \n\nroles are clearly defined; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n2.3.  Product Team \n\nStrengths and \n\nSkills Analysis \n\nDocument and assess the range of \n\nProduct Team member skills and \n\ninterests. Attempt to match member skills \n\nand interests to appropriate Product Team \n\nRoles as much as is practically possible. \n\nTo (a) ensure Product Team skill \n\nalignment and continued interest; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n2.4.  Product \n\nManagement \n\nDocument and allocate a clear Product \n\nManagement role and duties to Product \n\nManagers, inclusive of ensuring that \n\nProduct Managers have suitable Product \n\noversight, a clear understanding of \n\nProduct Team dynamics, and a contextual \n\nunderstanding of the Product and its \n\noperationalisation. \n\nPlease see Section 3 of the Organisation \n\nBest Practices Guideline for further \n\ncontext. \n\nTo (a) ensure that Product Manager \n\nroles are clearly defined; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nObjective: \n\nTo (a) ensure a balanced Product Team composition that fosters close collaboration and enhances a diversity of \n\nskills; and (b) to promote Product Team coordination and understanding through thorough team organization. \n\n# Section 2. Team Composition \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n14 FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n3.1.  Industry Context  Incorporate regulations, standards, and norms \n\nthat reflect industry values, boundaries, and \n\nconstraints during each phase of Product design and \n\ndeployment. Document and define clear qualitative \n\nmetrics and counter-metrics in Product & Outcome \n\nDefinitions, Data & Model Metrics and Acceptance \n\nCriteria Metrics, as relevant, as discussed in Section \n\n4 - Problem Mapping; Section 5 - Model Decision-\n\nMaking. \n\nTo (a) assemble a \n\nrobust team for Product \n\nand/or Model design, \n\ndevelopment and \n\ndeployment; and (b) \n\nhighlight associated risks \n\nthat might occur in the \n\nProduct Lifecycle. \n\n3.2.  Deployment \n\nContext \n\nIncorporate an understanding of the technical and \n\ninfrastructure aspects of the deployed Product \n\ninto the Product design process. Ensure that \n\ninfrastructure, integration, and scaling requirements \n\nand limitations are considered during the Problem \n\nMapping and Planning phases and document and \n\ndefine clear requirements for the Organisation \n\nCapacity Analysis, Product Scaling Analysis, Product \n\nIntegration Strategy, Product Risk Analysis, Testing \n\n- Automation Analysis, and POC-to-Production \n\nAnalysis, as discussed in Section 4 - Problem \n\nMapping; Section 6 - Management & Monitoring; \n\nSection 8 - Testing. \n\n3.3.  Societal Context  Research and consider the on and off platform \n\neffects of Product deployment on end users, their \n\ncommunities, and societies during each phase \n\nof Product design and deployment. Ensure that \n\nbehavioral shifts, power balance, and cultural \n\nconcerns are considered during the Problem Mapping \n\nand Planning phases, and that these provide input \n\nfor the Problem Statement & Solution Mapping, \n\nOutcome Definition, Product & Outcome Definitions \n\nData & Model Metrics, Product Risk Analysis, User \n\nExperience Mapping, Model Type - Best Fit Analysis, \n\nAcceptance Criteria, Privacy, Testing Participants, \n\nand Accuracy Perception, as discussed in Section \n\n4 - Problem Mapping; Section 7 - Privacy; Section 8 -\n\nTesting; Section 9 - Managing Expectations. \n\nObjective: \n\nTo ensure the Product Teamâ€™s continual access to a deep understanding of the various external contexts that \n\naffect the successful design and deployment of the Product. \n\n# Section 3. Context \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n15 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo determine and define an appropriate, feasible and solvable business problem through consideration of several \n\ninteracting analyses. \n\nControl:  Aim: \n\n4.1.  Problem Statement \n\n& Solution Mapping \n\nDocument and define clear problem \n\nstatements in terms of (i) User \n\nneeds, (ii) Organisation problem, \n\nand/or (iii) Organization opportunity. \n\nSubsequently, document and define \n\nclear solutions to the problem \n\nstatements, inclusive of the \n\ncontextual needs and/or variants of \n\nthe problem statements and/or their \n\nsolutions. \n\nTo ensure Products have clear scopes \n\nto warrant (a) their effective oversight, \n\nmanagement and execution, as well as \n\n(b) allow for the accurate evaluation of \n\nProduct risks and controls. \n\n4.2.  Data Capacity \n\nAnalysis \n\nMap and document the state of the \n\ndata delivery pipeline and available \n\ndatabases required to support the \n\nproblem statements and solutions. \n\nTo (a) ensure that the data pipeline is \n\nsufficient to support Product(s) and \n\nenable the desired Outcomes; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n4.3.  Product Definitions  Document and define clear Product \n\ndefinitions, aims, requirements and \n\ninternal deliverables having regard \n\nfor the above Problem Statement & \n\nSolution Mapping analysis, inclusive of \n\nsubsequent iterations thereof. \n\nTo ensure Product(s) have clear scope \n\nto warrant (a) their effective oversight, \n\nmanagement and execution, as well as \n\n(b) allow for the accurate evaluation of \n\nProduct risks and controls. \n\n4.4.  Outcomes \n\nDefinitions \n\nDocument, delineate, and define clear \n\nProduct Outcomes and Outcomes \n\ndeliveries based on the above \n\nProduct Definitions and the Problem \n\nStatement & Solution Mapping \n\nanalysis, inclusive of subsequent \n\niterations thereof. \n\nTo ensure Product(s) have clear scopes \n\nto warrant (a) their effective oversight, \n\nmanagement and execution, as well as \n\n(b) allow for the accurate evaluation of \n\nProduct risks and controls. \n\n4.5.  Product & Outcome \n\nDefinitions Data & \n\nModel Metrics \n\nDocument and define the above \n\nProduct and Outcome Definitions in \n\nterms of clear Model and data metrics. \n\nTo ensure Product(s) have clear scopes \n\nto warrant (a) their effective oversight, \n\nmanagement and execution, as well as \n\n(b) to allow for the accurate evaluation \n\nof Product risks and controls. \n\n# Section 4. Problem Mapping \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n16 FBPML Technical Best Practices v1.0.0. \n\n4. Problem Mapping - FBPML Technical Best Practices v1.0.0. \n\n4.6.  Organisation \n\nCapacity Analysis \n\nDocument and assess whether \n\nthe organisation has the requisite \n\ncapacity to achieve the above \n\nProduct Outcome and Product Metric \n\nDefinitions given the Product Team \n\nComposition and Product Team \n\nStrengths and Skills and Data Capacity \n\nAnalyses. If constraints detected, \n\nreiterate formulations of Product \n\nand/or Outcome Definitions to \n\naccommodate organisation capacity. \n\nTo (a) ensure that the Organization \n\nhas sufficient capacity to support \n\nProduct(s) and enable desired \n\nOutcomes; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n4.7.  Product Scaling \n\nAnalysis \n\nDocument and assess the estimated \n\ndegree to which the Product can \n\nbe feasibly scaled within Product \n\nDomains and the Organisation, having \n\nconsideration for the Organisation \n\nCapacity Analysis. \n\nTo (a) ensure that the Organization \n\nhas sufficient capacity to support \n\nthe Product(s) and enable the desired \n\nOutcomes as the Product scales; and \n\n(b) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n4.8.  Product Integration \n\nStrategy \n\nDocument and assess the processes \n\nneeded to integrate and scale \n\nthe Product into organisational \n\nstructures based on the Organisation \n\nCapacity and Product Scaling \n\nAnalyses. If constraints detected and/ \n\nor integration appears unfeasible, \n\nreiterate formulations of Product \n\nand/or Outcome Definitions and/ \n\nor review the Organisation Capacity \n\nand/or Product Scaling Analyses to \n\naccommodate a practical Product \n\nIntegration Strategy. \n\nTo (a) ensure the Product and Outcome \n\nDefinitions can be achieved within the \n\nbounds of the Organisation Capacity \n\nand Product Scaling Analyses; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n4.9.  Product Risk \n\nAnalysis \n\nDocument and assess the estimated \n\nrisks associated with Product design, \n\ndevelopment, implementation, and \n\noperation, inclusive of considerations \n\nfrom the Product Scaling Analysis, and \n\nthe Product Integration Strategy. \n\nTo (a) ensure Products have clear risk \n\nportfolios to warrant (i) their effective \n\noversight, management and execution, \n\nas well as (ii) to allow for the accurate \n\nevaluation of Product risks and \n\ncontrols; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n17 FBPML Technical Best Practices v1.0.0. \n\n4. Problem Mapping - FBPML Technical Best Practices v1.0.0. \n\n4.10.  Product Cost \n\nAnalysis \n\nCollaborate with Finance and \n\npurchasing to document and assess \n\nthe estimated costs associated \n\nwith Product design, development, \n\nimplementation, and operation, \n\ninclusive of considerations from the \n\nProduct Scaling Analysis, the Product \n\nIntegration Strategy, and the Product \n\nRisk Analysis. \n\nTo (a) ensure a realistic project \n\nbudget is provided; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n4.11.  User Experience \n\nMapping \n\nDocument and assess the user \n\nexperience and the desired \n\nexperience for various user groups, \n\nwhen interacting with the Product (e.g. \n\nusing Normanâ€™s Usability Heuristics). \n\nConsider mitigation strategies for \n\npossible negative impacts on and off \n\nplatform. If gaps in user experience \n\nare detected or a need for process \n\nredesign or behavioral changes are \n\nuncovered reiterate formulations of \n\nOutcome Definition, as discussed \n\nin Section 4 - Problem Mapping, \n\nOrganisation Capacity Analysis, \n\nand Product Integration Strategy \n\nto accommodate an effective user \n\nexperience. \n\nTo (a) ensure an effective user \n\nexperience; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n18 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo determine the most desirable and feasible model to achieve the desired Product Outcomes through \n\nconsideration of several interacting analyses. \n\nControl:  Aim: \n\n5.1.  Model Type - Metric \n\nFit Analysis \n\nDocument and assess the Model \n\nrequirements needed to meet \n\nthe Product Definitions, Outcome \n\nDefinitions, and Product & Outcome \n\nDefinitions Data & Model Metrics, \n\nas discussed in Section 4 - Problem \n\nMapping. \n\nTo ensure that chosen Model(s) meet \n\nthe requirements of the Product \n\nDefinitions, Outcome Definitions, and \n\nProduct & Outcome Definitions Data & \n\nModel Metrics. \n\n5.2.  Model Type - Risk \n\nAnalysis \n\nDocument and assess Model \n\nrequirements needed to meet the \n\nExplainability Requirements and \n\nProduct Risk Analysis, as discussed in \n\nSection 16 - Explainability; Section 4 -\n\nProblem Mapping. \n\nTo ensure that chosen Model(s) meet \n\nthe requirements of the Explainability \n\nRequirements and Product Risk \n\nAnalysis. \n\n5.3.  Model Type -\n\nOrganisation \n\nAnalysis \n\nDocument and assess the \n\ncompatibility of potential Models with \n\nthe Organisation Capacity Analysis, \n\nProduct Scaling Analysis, and Product \n\nIntegration Strategy, as discussed in \n\nSection 4 - Problem Mapping, given \n\ntechnical considerations. \n\nTo ensure that chosen Model(s) meet \n\nthe requirements of the Organisation \n\nCapacity Analysis, Product Scaling \n\nAnalysis, and Product Integration \n\nStrategy. \n\n5.4.  Model Type - Best \n\nFit Analysis \n\nDocument and assess the most \n\nappropriate Models that best meet the \n\nrequirements of, and which produces \n\nthe most favorable outcome given the \n\ntrade-offs between, the Model Type \n\n- Metric Fit, Risk and Organization \n\nAnalyses. \n\nTo (a) ensure that the most appropriate \n\nModel(s) are chosen; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n5.5.  Acceptance \n\nCriteria - Metrics \n\nDocument and define the desired \n\nperformance for an acceptable Model \n\nin terms of clear Model and data \n\nmetrics that are written from the end \n\nuser's perspective. \n\nTo (a) determine the metrics and \n\ndesired performance for an acceptable \n\nModel; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n# Section 5. Model Decision-Making \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n19 FBPML Technical Best Practices v1.0.0. \n\n5. Model Decision-Making - FBPML Technical Best Practices v1.0.0. \n\n5.6.  Acceptance \n\nCriteria -\n\nAccuracy, Bias, and \n\nFairness \n\nDocument and define clear, narrow \n\naccuracy goals and metrics that \n\nmanage the tradeoff of accuracy \n\nand explainability. Document and \n\ndefine the Model requirements \n\nneeded to meet the Fairness & Non-\n\nDiscrimination goals, as discussed \n\nmore thoroughly and technically \n\nin Section 11 - Fairness & Non-\n\nDiscrimination. \n\nTo (a) ensure appropriate accuracy, \n\nbias and fairness metrics for Model(s); \n\nand (b) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n5.7.  Acceptance \n\nCriteria -\n\nError Rate Analysis \n\nConsider the Societal and Industry \n\nContexts in determining the \n\nacceptable method for error \n\nmeasurement, as discussed in Section \n\n4 - Problem Mapping. Document and \n\ndefine the acceptable error types and \n\nrates for the Product as required by \n\nRepresentativeness & Specification, \n\nas discussed more thoroughly \n\nand technically in Section 13 -\n\nRepresentativeness & Specification. \n\nAnalyze any potential tension between \n\nachievable and acceptable error rates \n\nand determine whether that tension \n\ncan be resolved. \n\nTo (a) ensure appropriate error type \n\nand rate metrics for Model(s); and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n5.8.  Acceptance \n\nCriteria -\n\nKey Business \n\nMetrics / Targeted \n\nMetrics \n\nDocument and define the key business \n\nmetrics (KPIs) as determined in \n\nProblem Statement & Solution \n\nMapping, as discussed in Section \n\n4 - Problem Mapping, and translate \n\nthem into metrics that can be tracked \n\nwithin the framework of chosen \n\nModel(s), or into proxy metrics if direct \n\ntracking is not feasible. \n\nTo (a) ensure appropriate business \n\nmetrics for Model(s); and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n5.9.  Technical \n\nConsiderations \n\nDocument and assess technical issues \n\nthat should be considered during the \n\nModel selection process. \n\nTo (a) ensure that technical issues are \n\nconsidered when selecting Models; \n\nand (b) highlight associated risks that \n\nmight occur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n20 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure an effective and auditable Product Lifecycle. \n\nControl:  Aim: \n\n6.1.  Product \n\nRequirements \n\nDraft and document clear Product requirements. \n\nReview Model Type - Metrics and Acceptance \n\nCriteria, as discussed in Section 4 - Problem \n\nMapping, to ensure alignment. Regularly review \n\nProduct & Outcome Definitions Data & Model \n\nMetrics and User Experience Mapping, as \n\ndiscussed in Section 4 - Problem Mapping, and \n\nupdate as necessary \n\nTo (a) ensure current, clear, \n\nand actionable Product \n\nrequirements; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n6.2.  Product \n\nRoadmap and \n\nPipeline \n\nDevelop and document a Product roadmap \n\nand pipeline that enable the experience \n\nenvisioned in the User Experience Mapping, as \n\ndiscussed in Section 4 - Problem Mapping, and \n\ninclude the following sections: Schedule and \n\nmilestones, tasks and deliverables, limitations \n\nand exclusions (scope), initial prioritization, and \n\nmethods for determining future priority. \n\nTo (a) ensure a clear, \n\nactionable, and prioritized \n\nProduct roadmap and pipeline; \n\nand (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n6.3.  Experimentation \n\nConstraints \n\nDevelop and document a method for evaluating \n\nthe quality of predictions. Develop and \n\ndocument criteria for determining when to stop \n\nthe experimentation process. \n\nTo (a) develop processes to \n\nensure a balance between \n\neffectiveness and efficiency in \n\nthe experimentation cycle; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n6.4.  Behavioral \n\nChange Analysis \n\n- Process \n\nChanges \n\nResearch and assess the business processes \n\nthat will be affected by the new Product and/ \n\nor the infrastructure changes that enable the \n\nProduct. Review the User Experience Mapping, \n\nData Capacity Analysis, and Organisation \n\nCapacity Analysis, as discussed in Section 4 -\n\nProblem Mapping, and reformulate as necessary. \n\nDevelop and document a plan to retrain affected \n\nparties as necessary and mitigate business \n\ndisruptions as much as feasible. \n\nTo (a) determine business \n\nprocesses that may be affected \n\nby the project and create a plan \n\nto retrain or mitigate impacts \n\nas necessary; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n# Section 6. Management & \n\n# Monitoring \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n21 FBPML Technical Best Practices v1.0.0. \n\n6. Management & Monitoring - FBPML Technical Best Practices v1.0.0. \n\n6.5.  Behavioral \n\nChange Analysis \n\n- Social (Off-\n\nPlatform) \n\nResearch and document ways in which the \n\nProduct can be abused or negatively impact \n\ncustomers, end users, or the broader society. \n\nDevelop and document a plan to mitigate \n\nnegative impacts as much as feasible. Develop \n\nand document counter metrics to assess \n\nwhether users or the model are â€˜gamingâ€™ the \n\nsystem. \n\nTo (a) determine (i) negative \n\nproduct uses, (ii) negative \n\nproduct impacts (iii) negative \n\nuser or model behaviors \n\nand create a plan to counter \n\nbehaviors or mitigate impacts \n\nas necessary; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n6.6.  Resource \n\nAssessment \n\nDocument the processes, tools, and staffing \n\nthat are required for every phase of the \n\nproject, including the Data Capacity Analysis, \n\nOrganisation Capacity Analysis, Product \n\nScaling Analysis, and Product Cost Analysis, \n\nas discussed in Section 4 - Problem Mapping, \n\nbefore starting each phase of the project and \n\nupdate as necessary. \n\nTo (a) ensure adequate \n\nresources and funding during \n\nevery phase of the Product \n\nLifecycle; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n6.7.  POC-to-\n\nProduction \n\nChecklist \n\nDocument and define a POC-to-Production \n\nChecklist that details the existing system \n\nmodifications, and new system builds, required \n\nfor integrating the Product into Organisation \n\ninfrastructure and incorporating additional data \n\nsources. If gaps in organisational capacity are \n\ndetected, reiterate formulations of Organisation \n\nCapacity Analysis, as discussed in Section 4 -\n\nProblem Mapping, as necessary. \n\nTo (a) ensure sufficient planning \n\nfor Product development and \n\nproduction; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n6.8.  Update Schedule  Document and define a POC-to-Production \n\nChecklist that details the existing system \n\nmodifications, and new system builds, required \n\nfor integrating the Product into Organisation \n\ninfrastructure and incorporating additional data \n\nsources. If gaps in organisational capacity are \n\ndetected, reiterate formulations of Organisation \n\nCapacity Analysis, as discussed in Section 4 -\n\nProblem Mapping, as necessary. \n\nTo (a) ensure the Product \n\nand its related software are \n\nupdated and upgraded regularly \n\nand that the schedule for \n\nsaid updates are coordinated \n\nwith information technology \n\ndepartment(s); and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n22 FBPML Technical Best Practices v1.0.0. \n\n6. Management & Monitoring - FBPML Technical Best Practices v1.0.0. \n\n6.9.  Project Records  Develop and document a process for preserving \n\ndata of the information considered when making \n\nsignificant product decisions. Include any \n\nmethods for standardized experiment tracking \n\nand artifact capturing that are developed \n\nby Data Science and Engineering. Develop \n\na continuously maintained and consistently \n\navailable repository for Product Requirements \n\nand any data related to their updates. \n\nTo (a) maintain a historical \n\nrecord of Product and data \n\nand ensure that all iterations \n\nof Product Requirements \n\nare continuously available to \n\nStakeholders; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n6.10.  Project Records \n\n- Stakeholder \n\nSign-offs \n\nDevelop a standard Stakeholder Sign-off \n\nDocument to be utilized (i) after the finalization \n\nof the following documents and analyses: \n\nProblem Statement & Solution Mapping, \n\nOutcomes Definition, Product & Outcome \n\nDefinitions, Product Integration Strategy, Model \n\nType - Best Fit Analysis, Acceptance Criteria -\n\nKey Business Metrics/Targeted Metrics, Testing \n\nDesign and Scheduling Framework, Resource \n\nAssessment, as discussed in Section 4 - Problem \n\nMapping, Section 5 - Model Decision-Making; \n\nand (ii) at Project Checkpoints, as discussed in \n\nSection 10 - Project Checkpoints. \n\nTo (a) ensure stakeholder \n\nbuy-in; and (b) provide an \n\nauditable record of project and \n\nstakeholder expectations at \n\nevery major project decision-\n\npoint. \n\n6.11.  Custody  Develop a system for documenting the chain \n\nof custody for Product(s) and the data, \n\nmicroservices, and applications that it is built on \n\nand with, that indicates: i) provenance ii) control \n\niii) transfer, iv) analysis, and v) transformation. \n\nTo (a) ensure that the building \n\nblocks of the Product can be \n\ntraced back to their origins; (b) \n\nallow for undesirable changes \n\nto be reverted; and (c) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n23 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo determine the most appropriate and feasible privacy-preserving techniques for the Product. \n\nControl:  Aim: \n\n7.1.  Decentralization \n\nMethod Analysis \n\nConsider the appropriateness of utilizing \n\nmethods for distributing data or training across \n\ndecentralized devices, services, or storage. \n\nWhen analyzing federated learning methods, \n\nconsider Data Capacity Analysis, Product \n\nIntegration Strategy, Product Traceability, and \n\nFairness & Non-Discrimination, as discussed \n\nmore thoroughly in Section 4 - Problem Mapping; \n\nSection 21 - Product Traceability; and Section 11 \n\n- Fairness & Non-Discrimination. When analyzing \n\ndifferential privacy methods, consider Data \n\nQuality - Noise, as discussed more thoroughly in \n\nSection 12 - Data Quality. \n\nTo (a) ensure appropriate \n\nprivacy-preserving techniques \n\nthat are aligned with chosen \n\nModels; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n7.2.  Cryptographic \n\nMethods Analysis \n\nConsider the appropriateness of utilizing \n\nmethods for encrypting all or various parts of \n\nthe data and/or Model pipeline. When analyzing \n\nhomomorphic encryption methods, consider \n\nProduct Integration Strategy and Product \n\nScaling Analysis, as discussed more thoroughly \n\nin Section 4 - Problem Mapping. Additionally, \n\nconsider -\n\n(a) whether the types of operations and \n\ncalculations that can be performed meet the \n\nrequirements of Model Type - Best Fit Analysis, \n\nas discussed more thoroughly in Section 5 -\n\nModel Decision-Making; and/or \n\n(b) whether the encrypted Model processing \n\nspeed is acceptable with consideration for real \n\nworld robustness and direct user interaction, \n\nas discussed more thoroughly in Section 14 -\n\nPerformance Robustness. \n\nTo (a) ensure appropriate \n\nprivacy-preserving techniques \n\nthat are aligned with chosen \n\nModels; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n# Section 7. Privacy \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n24 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure (a) that robust, effective, and efficient strategies, methodologies and schedules are developed for \n\ntesting the Product; and (b) clear maintenance metrics and/or phases to warrant continued Product alignment \n\nwith chosen metrics and performance goals. \n\nControl:  Aim: \n\n8.1.  Testing Design \n\nand Scheduling \n\nFramework \n\nDocument and define a testing design and \n\nschedule that does not artificially constrain \n\nthe testing process. Incorporate the Feedback \n\nLoop Analysis in the testing design. Review the \n\nAutomation Analysis in determining what level \n\nof automation is appropriate for each stage of \n\ntesting. Ensure the individuals chosen through \n\nthe Testing Participant Identification process \n\nare involved at the earliest stages of the testing \n\nschedule as practical. Post Product deployment, \n\ndocument and define a framework and process \n\nfor testing and selecting variations of the \n\nproduction Model. \n\nTo (a) ensure a robust and \n\nfeasible testing design and \n\nscheduling framework that \n\nallows for effective Product \n\noptimization; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n8.2.  Testing \n\nParticipant \n\nIdentification \n\nDocument and define a process for identifying \n\ntest participants as required by User Experience \n\nMapping and Societal Context, as discussed in \n\nSection 4 - Problem Mapping; and Section 3 -\n\nContext. Determine a framework for ensuring \n\nthat testing participants are intentionally \n\ndiverse across use cases, user types and roles, \n\nand internal and external Stakeholders. \n\nTo (a) ensure testing for user \n\nimpact and participant pool \n\ndiversity; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n# Section 8. Testing \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n25 FBPML Technical Best Practices v1.0.0. \n\n8. Testing - FBPML Technical Best Practices v1.0.0. \n\n8.3.  Automation \n\nAnalysis \n\nDetermine and define data, Model, and \n\ncomponent integration validations that can be \n\nreasonably automated. Assess and define any \n\nprocesses during the development, deployment, \n\nor maintenance phases that could benefit \n\nfrom integrating automation into the testing \n\ninfrastructure. Be sure to review -\n\n(a) the Organisation Capacity Analysis, as \n\ndiscussed in Section 4 - Problem Mapping, while \n\ndetermining the feasibility of automating the \n\nidentified processes; and/or \n\n(b) the Industry, Deployment, and Societal \n\nContexts, as discussed in Section 3 - Context, to \n\nuncover any gaps or misalignment raised by the \n\nautomation of any identified process. \n\nTo (a) identify suitable and \n\neffective areas for incorporating \n\ntesting automation within \n\nthe Product development, \n\ndeployment, and maintenance \n\nphases; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n8.4.  Feedback Loop  Document and define a feedback loop that \n\nenables monitoring of stability, performance, \n\nand operations metrics, and counter-metrics, \n\nas required by Performance Robustness, \n\nMonitoring and Maintenance, and Systemic \n\nStability, as discussed more thoroughly in \n\nSection 14 - Performance Robustness; Section \n\n15 - Monitoring & Maintenance; and Section 20 \n\n- Systemic Stability. Develop and incorporate a \n\nmethod for flagging bias and for issue reporting. \n\nDocument and define a process for real-time \n\nsharing of testing participant feedback with \n\nthe development and maintenance teams. \n\nIncorporate the Feedback Loop in the Testing \n\nDesign and Scheduling Framework to ensure \n\nthat the features the Model is utilizing are \n\nacceptable for the application during the \n\ndevelopment, deployment, and maintenance \n\nphases. \n\nTo (a) ensure robust and \n\nresponsive feedback loop \n\nmeasures that enable monitoring \n\nof necessary metrics and \n\neffectively integrate into the \n\nTesting Design and Scheduling \n\nFramework; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n26 FBPML Technical Best Practices v1.0.0. \n\nFBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo effectively set and communicate realistic Product expectations to Stakeholders and obtain their buy-in. \n\nControl:  Aim: \n\n9.1.  Performance  Product management should attempt to set \n\nrealistic Product performance expectations \n\nfor Stakeholders through periodic stakeholder \n\ndiscussions on the following issues: (i) limited \n\nindustry understanding of what tasks are \n\ndifficult for the Product; (ii) difficulty of \n\ndetermining what type of modifications -\n\nnetwork design, input features, or training data \n\n- will create the greatest Product improvement; \n\nand (iii) Model improvement can stall \n\nsignificantly while experimenting with different \n\nvariable modifications. \n\nTo (a) effectively communicate \n\nrealistic Product performance \n\nexpectations throughout the \n\ndevelopment process; and (b) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. \n\n9.2.  Timeframe  Product management should set expectations \n\nfor long-term investment in the Product for \n\nStakeholders, specifically focusing on: (i) the \n\nunpredictability of Product improvement; (ii) \n\nProduct difficulties are traditionally hard to \n\ndiagnose as they are often caused by subtle \n\nissues of intersecting inputs; and (iii) it is \n\npossible for the Product to completely stall with \n\nabsolutely no discernible improvement in spite \n\nof significant time and effort. \n\nTo (a) effectively set Stakeholder \n\nexpectations regarding the \n\ndifficulty of locking down \n\nProduct timelines; and (b) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. \n\n9.3.  Accuracy \n\nperception \n\nThe Product Team should work to ensure that \n\nthe solution will be accurate enough to meet a \n\nvariety of different Stakeholdersâ€™ expectations, \n\nrecognizing that each group of Stakeholders will \n\nhave different views on what is â€˜accurateâ€™ based \n\non their interaction with the Product. Product \n\nmanagement should set and communicate \n\nexpectations in-line with the achievable level of \n\naccuracy for each user group. \n\nTo (a) effectively communicate \n\nachievable accuracy levels, \n\nconsidering individual \n\nStakeholder accuracy \n\npreferences; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n# Section 9. Managing Expectations Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n27 FBPML Technical Best Practices v1.0.0. \n\n9. Managing Expectations - FBPML Technical Best Practices v1.0.0. \n\n9.4.  POC-to-\n\nProduction \n\nThe Product Team should effectively \n\ncommunicate that infrastructure is often the \n\ndetermining factor for the success of the POC-\n\nto-Production transition and rely heavily on \n\nthe POC-to-Production Checklist, as discussed \n\nin Section 6 - Management & Monitoring, to \n\nset and align Stakeholder expectations of the \n\ntransition process. The Product Team should set \n\nthe expectation, before beginning the transition \n\nprocess, that novel problems will likely arise \n\nduring the transition that may significantly \n\naffect the timeline and costs. The Product Team \n\nshould be on alert for integration issues arising \n\nclose to the final release of the solution, which \n\nthe Product Manager should communicate to \n\nrelevant Stakeholders, along with progress \n\nupdates, at a progressively more frequent \n\ncadence. \n\nTo (a) uncover and communicate \n\nissues that may delay the \n\ntransition of the solution from \n\nPOC-to-Production or make that \n\ntransition less feasible; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n9.5.  Production \n\nCosts \n\nReview and analyze the finalized POC budget to \n\ndetermine a realistic Product implementation \n\nbudget. Review the Product Cost Analysis as \n\ndiscussed in Section 4 - Problem Mapping to \n\nensure its continued accuracy and reformulate \n\nas necessary. The Product Team should \n\neffectively communicate to Stakeholders that \n\nthe budget for implementation will likely be \n\nin-line or more expensive than the cost to get \n\nthrough POC. \n\nTo (a) ensure realistic \n\nexpectations for a sufficient \n\nProduct implementation budget \n\nare communicated; and (b) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n28 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure that necessary factors are considered at key decision points. \n\nControl:  Aim: \n\n10.1.  Machine \n\nLearning \n\nAppropriate \n\nTool Analysis \n\nThe Product Team should work cross-functionally with \n\nStakeholders to define and document a Machine Learning \n\nchecklist that considers the following areas, amongst \n\nother things: \n\na.  Is there a different approach that will generate a \n\ngreater return more quickly; \n\nb.  Given the results of the Data Capacity Analysis, \n\ndoes the Organisation have enough secure, non-\n\ndiscriminatory, representative, high quality data for \n\nevery stage of the process; \n\nc.  Can the problem be solved by simple rules; \n\nd.  Does the Product solution require emotional \n\nintelligence or empathy; \n\ne.  Does the Product solution need to be fully \n\ninterpretable or explainable; \n\nf.  Given the results of the Organisation Capacity \n\nAnalysis, does the Organization have the people, \n\nprocesses, and tools necessary to productize the end \n\nproduct; \n\ng.  Can the consequences of Product failure be easily \n\nfixed or mitigated; and/or \n\nh.  What other non-technical solutions can be used \n\nto augment the Product and its offering and/or, \n\nmore directly, whether Machine Learning is the best \n\nsolution for the Product at hand. \n\nTo (a) ensure that \n\nMachine Learning is the \n\nappropriate method \n\nfor solving the chosen \n\nproblem; and (b) highlight \n\nassociated risks that \n\nmight occur in the \n\nProduct Lifecycle. \n\n# Section 10. Project Checkpoints Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n29 FBPML Technical Best Practices v1.0.0. \n\n10. Project Checkpoints- FBPML Technical Best Practices v1.0.0. \n\n10.2  Data Buy v. \n\nBuild Analysis \n\nThe Product Team should work cross-functionally with \n\nrelevant Stakeholders to define and document a Buy v. \n\nBuild checklist that considers the following areas: \n\na.  Does the Organisation have enough data for every \n\nstage of the process (training, POC, production) \n\nand for every purpose (replacing stale/flawed data, \n\nmeasuring success); \n\nb.  Does the Organisation have the right type of data for \n\nevery stage of the process (training, POC, production) \n\nand for every purpose (replacing stale/flawed data, \n\nmeasuring success); \n\nc.  Is bought data secure and free of privacy concerns; \n\nd.  Is the bias in the bought data limited, mitigatable, or \n\nremovable; \n\ne.  Given the results of the Data Quality Analysis, does \n\nthe Organisation have quality data and are datasets \n\ncomplete; \n\nf.  Given the Product Team Composition, does the \n\nOrganisation have the staffing and expertise to clean, \n\nprepare, and maintain internal data; and/or \n\ng.  Given the Data Capacity Analysis, is the necessary \n\ndata easily and readily available internally. \n\nTo (a) ensure that the \n\nOrganisationâ€™s decision \n\nto either purchase data \n\nor utilize in-house data \n\nis appropriate based on \n\nOrganisation capacity \n\nand/or constraints; and \n\n(b) highlight associated \n\nrisks that might occur in \n\nthe Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n30 FBPML Technical Best Practices v1.0.0. \n\n10. Project Checkpoints- FBPML Technical Best Practices v1.0.0. \n\n10.3  Model Buy v. \n\nBuild Analysis \n\nThe Product Team should work cross-functionally with \n\nrelevant Stakeholders to define and document a Buy v. \n\nBuild checklist that considers the following areas: \n\na.  Is the scope of the Product manageable, given the \n\nresults of the Organisation Capacity Analysis; \n\nb.  Can bought Models be used for other Products (eg. \n\ntransfer learning); \n\nc.  Does the Organisation have the in-house expertise \n\nrequired to acquire and label the training data, given \n\nthe Product Team Composition; \n\nd.  How much would it cost to acquire a properly labeled \n\ntraining dataset; \n\ne.  Given the Product Team Composition, does the \n\nOrganisation have the in-house expertise required to \n\nretrain Models, if necessary; \n\nf.  How important is Model customization and, if so, can \n\nbought Models be customised; \n\ng.  Are the Acceptance Criteria - Accuracy, Bias, and \n\nFairness requirements for bought Models feasible \n\ngiven the timeline, Product Team Composition, and \n\nOrganisation Capacity Analysis; and/or \n\nh.  What are the usage limits and costs for pre-trained \n\nModels. \n\nTo (a) ensure that the \n\nOrganisationâ€™s decision \n\nto either purchase or \n\nbuild the Models is \n\nappropriate based on \n\nOrganisation capacity \n\nand/or constraints; and \n\n(b) highlight associated \n\nrisks that might occur in \n\nthe Product Lifecycle. \n\n10.4  POC-to-\n\nProduction \n\nGo/No-Go \n\nAnalysis \n\nThe Product Team should work cross-functionally with \n\nrelevant Stakeholders to define and document a Go/No-\n\nGo checklist that considers qualitative and quantitative \n\nfactors in the following areas: \n\na.  Can POC-to-Production Checklist be adequately \n\naddressed; \n\nb.  Is the Product Cost Analysis still feasible; \n\nc.  Does the Product Team have approval for a Product \n\nmaintenance budget; \n\nd.  Are the updates, upgrades, and add-ons to the data \n\ninfrastructure near completion; \n\ne.  What is the state of customer process reconstruction \n\nand end-user training; \n\nf.  Has the failsafe, rollback, or emergency shutdown \n\nplan been completed and approved; and/or \n\ng.  Have the communication and mitigation plans in case \n\nof failsafe, rollback, or emergency shutdown been \n\ncompleted and approved. \n\nTo (a) ensure that the \n\nsolution should be \n\ndeployed in production \n\nand/or Product Domains; \n\nand (b) highlight \n\nassociated risks that \n\nmight occur in the \n\nProduct Lifecycle.", "fetched_at_utc": "2026-02-08T18:50:43Z", "sha256": "166d9a6e0506c0fa81e351c96dd63f920d7bd6700137808dad36fb7be7041508", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-13-30.pdf", "file_size": 368603, "relative_path": "pdfs\\FBPML_TechnicalBP_V1.0.0-13-30.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 9618}}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-32-62-12585dab1747", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-32-62.pdf", "title": "FBPML_TechnicalBP_V1.0.0-32-62", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n32 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) identify and mitigate risk of disproportionately unfavorable Outcomes for protected (Sub)populations; and \n\n(b) minimise the unequal distribution of Product and Model errors to prevent reinforcing and/or deriving social \n\ninequalities and/or ills, and (c) promote compliance with existing anti-discrimination laws and statutes. \n\nWhat do we mean when we refer to Fairness? \n\nFairness is a complex socio-technical challenge for which there is no single generic definition. Broadly speaking -\n\nFairness is about identifying bias in a machine learning Model or Product and mitigating discrimination with \n\nrespect to sensitive, and (usually) legally protected attributes such as ethnicity, gender, age, religion, disability, \n\nor sexual orientation. \n\nAlgorithmic discrimination can take many forms and may occur unintentionally. Machine learning Products might \n\nunfairly allocate opportunities, resources, or information, and they might fail to provide the same quality of \n\nservice to some people as they do to others. \n\nThe conversation about fairness distinguishes between group fairness and individual fairness measures. Group \n\nfairness ensures some form of statistical parity (e.g. equal calibration, equal false positive/negative rate) across \n\nprotected groups. Individual fairness requires that individuals who are similar with respect to the predictive task \n\nbe assigned similar outcomes regardless of the sensitive attribute. \n\nWhy is Fairness relevant? \n\nMachine learning Products are increasingly used to inform high-stakes decisions that impact peopleâ€™s lives.It is \n\ntherefore important that ML-driven decisions do not reflect discriminatory behavior toward certain populations. \n\nIt is the responsibility of data science practitioners and business leaders to design machine learning Products \n\nthat minimizes bias and promotes inclusive representation. \n\nSome business leaders express concerns about a potential increase in the risk of reputational damage and legal \n\nallegations in case of discriminatory â€˜black boxâ€™ Models. AI fairness can substantially reduce these concerns. \n\nAnother reason for taking AI fairness seriously is the development of regulatory frameworks for AI. For example, \n\nthe European Commission published a white paper on AI in 2020, which was followed in 2021 by a regulatory \n\nframework proposal for AI in the European Union. \n\nHow to apply Fairness? \n\nFairness should be considered throughout the product lifecycle. Given that AI systems are usually designed \n\nto evolve with experience, fairness should be closely monitored during deployment as well as during product \n\ndevelopment. The Technical Best Practices Guidelines provide detailed guidance into implementing fairness in \n\nyour AI products. \n\n# Section 11. Fairness & Non-\n\n# Discrimination Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n33 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) identify and mitigate risk of disproportionately unfavorable Outcomes for protected (Sub)populations; and \n\n(b) minimise the unequal distribution of Product and Model errors to prevent reinforcing and/or deriving social \n\ninequalities and/or ills, and (c) promote compliance with existing anti-discrimination laws and statutes. \n\n11.1 Product Definitions \n\nControl:  Aim: \n\n11.1.1.  (Sub)populations \n\nDefinition \n\nDefine (Sub)populations that are subject \n\nto Fairness concern, with input from \n\nDomain and/or legal experts when \n\nrelevant. \n\nTo (a) ensure that vulnerable \n\nand affected populations are \n\nappropriately identified in all \n\nsubsequent Fairness testing and \n\nModel build; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.1.2.  (Sub)population \n\nData \n\nGather data on (Sub)population \n\nmembership. If a proxy approach is used, \n\nensure the performance of the proxy is \n\nadequate in this context. \n\nTo (a) facilitate Fairness testing \n\npre- and post-Model deployment; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.3.  (Sub)population \n\nOutcome \n\nPerceptions \n\nDocument and assess whether scored \n\n(Sub)populations would view Model \n\nOutcomes as favorable or not, using \n\ninput from subject matter experts and \n\nstakeholders in affected (Sub)populations. \n\nDocument and assess any divergent views \n\namongst (Sub)populations. \n\nTo (a) ensure uniformity in (Sub) \n\npopulation outcome perception, \n\nif applicable; (b) highlight \n\nOutcome effects for different \n\n(Sub)populations; and (c) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.1.4.  Erroneous \n\nOutcome \n\nConsequence \n\nEstimation \n\nDivergence \n\nDocument and assess the results \n\nof erroneous (false positive & false \n\nnegative) outcome consequences, both \n\nreal and perceived, specifically in terms \n\nof divergence between relevant (Sub) \n\npopulations. If material divergence \n\npresent, take measures to harmonise \n\nOutcome perceptions and/or mitigate \n\nerroneous Outcome consequences in \n\nModel design, exploration, development, \n\nand production. \n\nTo (a) ensure uniformity in \n\nerroneous Outcomes for (Sub) \n\npopulations; (b) highlight outcome \n\neffects for different (Sub) \n\npopulations; and (c) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.1.5.  Positive Outcome \n\nSpread \n\nDocument and assess the degree to \n\nwhich Model positive outcomes can be \n\ndistributed to non-scored (Sub)population, \n\nwhen contextually appropriate. If present, \n\ntake measures to promote Model Outcome \n\ndistribution in Model design, exploration, \n\ndevelopment, and production. \n\nTo (a) ensure the non-prejudicial \n\nspread of positive Model Outcomes; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n34 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\n11.1.6.  Enduring Bias \n\nEstimation \n\nDocument and assess whether exclusions \n\nfrom Product usage might perpetuate \n\npre-existing societal inequalities between \n\n(Sub)populations. If present, take \n\nmeasures to mitigate societal inequalities \n\nperpetuation in Model design, exploration, \n\ndevelopment, and production. \n\nTo (a) ensure the non-prejudicial \n\nspread of Model Outcomes; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.7.  Appropriate \n\nFairness Metrics \n\nConsult Domain experts to inform \n\nwhich Fairness metrics are contextually \n\nmost appropriate for the Model when \n\nconducting Fairness testing. \n\nTo (a) ensure that fairness testing \n\nand subsequent Model changes (i) \n\nresult in outcome changes which \n\nare relevant for (Sub)populations; \n\nand/or (ii) are consistent with \n\nregulatory guidance and context-\n\nspecific best practices; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.8.  Model Implications  Document and assess the downside risks \n\nof Model misclassification/inaccuracy \n\nfor modeled populations. Use the relative \n\nseverity of these risks to inform the \n\nchoice of Fairness metrics. \n\nTo (a) ensure that improving in the \n\nchosen Fairness metrics achieves \n\nthe greatest Fairness in Model \n\ndecisioning after deployed; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.9.  Fairness Testing \n\nApproach \n\nDocument and assess the Fairness testing \n\nmethodologies that will be applied to \n\nModel and/or candidate Models, along with \n\nany applicable thresholds for statistical/ \n\npractical significance, acceptable \n\nperformance loss tolerance, amongst \n\nother metrics. \n\nTo (a) prevent Fairness testing \n\nmethodology and associated \n\nthresholds change during \n\nModel review; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\nObjective \n\nTo identify and control for Fairness and Non-Discrimination risks based on the available datasets. \n\n11.2 Exploraition \n\nControl:  Aim: \n\n11.2.1.  (Sub)population Data \n\nAccess \n\nKeep separate Model development \n\ndata and (Sub)population membership \n\ndata (if applicable Regulations allow \n\nthe possession and processing of such \n\nin the first place), especially if the use \n\nof (Sub)population data in the Model is \n\nprohibited or would introduce fairness \n\nconcerns. \n\nTo (a) guarantee that (Sub) \n\npopulation membership data does \n\nnot inadvertently leak into a Model \n\nduring development. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n35 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\n11.2.2.  Univariate \n\nAssessments \n\nDocument and perform univariate \n\nassessments of relationship between \n\n(Sub)populations and Model input \n\nFeatures, including appropriate \n\ncorrelation statistics. \n\nTo (a) identify input Feature trends \n\nassociated with (Sub)populations; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.2.3.  Prohibited Data \n\nSources \n\nDevelop and maintain an index of data \n\nsources or features that should not be \n\nmade available or utilized because of \n\nthe risks of harming (Sub)populations, \n\nspecifically Protected Classes. \n\nTo (a) prohibit the actioning of data \n\nsources that will disproportionately \n\nprejudice (Sub)populations; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.2.4.  Data \n\nRepresentativeness \n\nEnsure the membership rates of (Sub) \n\npopulations in Model development data \n\nalign with expectations and that data is \n\nrepresentative of Domain populations. \n\nTo (a) guarantee that Model \n\nperformance and Fairness testing \n\nduring model development will \n\nprovide a consistent picture \n\nof Model performance after \n\ndeployment; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.2.5.  (Sub)population \n\nProxies and \n\nRelationships \n\nDocument and assess the relationship \n\nbetween potential input Features and \n\n(membership of) (Sub)populations \n\nof interest based on, amongst other \n\nthings, (i) reviews with diverse Domain \n\nexperts, (ii) explicit encoding of (Sub) \n\npopulation membership, (iii) correlation \n\nanalyses, (iv) visualization methods. If \n\nrelationships exist, the concerned input \n\nFeatures should be excluded from Model \n\ndatasets, unless a convincing case can \n\nbe made that an (adapted version of) the \n\ninput Feature will not adversely affect \n\nany (Sub)populations, and document this. \n\nTo (a) prevent Model decisions \n\nbased directly or indirectly on \n\nprotected attributes or protected \n\nclass membership; (b) reduce \n\nthe risk of Model bias against \n\nrelevant (Sub)populations; (c) \n\nunderstand any differences in \n\ndata distributions across (Sub) \n\npopulations before development \n\nbegins; and (c) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\nAssociated Controls  Review the following controls with particular attention in the context of bias \n\nand fairness with respect to protected (Sub)populations: \n\nSection 12.2.2. - Missing and Bad Data Assessment. \n\nSection 13.2.4. - Selection Function; which is concerned with accurate \n\nrepresentation of (Sub)populations. \n\nSection 13.3.1. - 13.3.4.; which are concerned with the choice and definition of \n\nthe Target Feature. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n36 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n11.3.1.  Explainability (xAI) \n\n(Sub)population \n\nOutcomes \n\nKeep separate Model development data \n\nand (Sub)population membership data \n\n(if applicable Regulations allow the \n\npossession and processing of such in the \n\nfirst place), especially if the use of (Sub) \n\npopulation data in the Model is prohibited \n\nor would introduce fairness concerns. \n\nTo (a) guarantee that (Sub) \n\npopulation membership data does \n\nnot inadvertently leak into a Model \n\nduring development. \n\n11.3.2.  Model Architecture \n\nand Interpretability \n\nChoose Model architecture that maximizes \n\ninterpretability and identification \n\nof causes of unfairness. Consider \n\ndifferent methodologies within the \n\nsame Model architecture (ex. monotonic \n\nXGBoost, explainable neural networks). \n\nEvaluate whether Product Aims can be \n\naccomplished with a more interpretable \n\nModel. \n\nTo (a) provide information that can \n\nguide Model-builders; (b) ensure \n\nthat Model decisions are made in \n\nline with expectations; (c) allow \n\nProduct Subjects and/or End Users \n\nto understand why they received \n\ncorresponding Outcomes; (d) help \n\ninform the causes of Fairness \n\nissues if issues are detected; \n\nand (e) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.3.  Fairness Testing of \n\nOutcomes \n\nFocus fairness testing initially on \n\noutcomes that are immediately \n\nexperienced by (Sub)populations. For \n\nexample, if a model uses a series of \n\nsub-Models to generate a score and \n\na threshold is applied to that score to \n\ndetermine an Outcome, focus on Fairness \n\nissues related to that Outcome. If issues \n\nare identified, then diagnose the issue \n\nby moving â€œup-the-chainâ€ and testing the \n\nModel score and sub-Models. \n\nTo (a) ensure that the testing \n\nperformed best reflects what will \n\nhappen when Models are deployed \n\nin the real world; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.4.  Disparate Impact \n\nTesting \n\nIf applicable, test Model(s) for disparate \n\nimpact. Evaluate whether Model(s) predict \n\na Positive Outcome at the same rate \n\nacross (Sub)populations. \n\nTo (a) ensure that (Sub)population \n\nmembers are receiving the Positive \n\nOutcome as often as their peers; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.5.  Equalized \n\nOpportunity \n\nTesting \n\nIf applicable, test Model(s) for equalized \n\nopportunity. Evaluate whether Model(s) \n\npredict a Positive Outcome for (Sub) \n\npopulation members that are actually in \n\nthe positive class at the same rates as \n\nacross (Sub)populations. \n\nTo (a) ensure that (Sub)population \n\nmembers who should receive the \n\nPositive Outcome are receiving the \n\nPositive Outcome as often as their \n\npeers; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\nObjective \n\nTo minimise the unequal distribution of Product and Model errors for (Sub)populations during Model development \n\nin the most appropriate manner. \n\n11.3. Development Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n37 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\n11.3.6.  Equalized Odds \n\nTesting \n\nIf applicable, test Model(s) for equalized \n\nodds. Evaluate whether Model(s) predict \n\na Positive & Negative Outcome for (Sub) \n\npopulation members that are actually in \n\nthe positive & negative class respectively \n\nat the same rates across (Sub)populations. \n\nTo (a) ensure that (i) protected \n\n(Sub)populations who should \n\nreceive the Positive Outcome are \n\nreceiving the Positive Outcome as \n\noften as other (Sub)populations, \n\nand (ii) protected (Sub)populations \n\nwho should not receive the Positive \n\nOutcome are not receiving the \n\nPositive Outcome as often as other \n\n(Sub)populations; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.7.  Conditional \n\nStatistical Parity \n\nTesting \n\nIf applicable, test Model(s) for conditional \n\nstatistical parity. Evaluate whether \n\nModel(s) predict a Positive Outcome at \n\nthe same rate across (Sub)populations \n\ngiven some predefined set of â€œlegitimate \n\nexplanatory factorsâ€. \n\nTo (a) ensure that (Sub)populations \n\nmembers are receiving the Positive \n\nOutcome just as often as (Sub) \n\npopulations with similar underlying \n\ncharacteristics; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.8.  Calibration Testing \n\nAcross (Sub) \n\npopulations \n\nIf applicable, test Model(s) for calibration. \n\nEvaluate whether (Sub)populations \n\nmembers with the same predicted \n\nOutcome have an equal probability of \n\nactually being in the positive class. \n\nTo (a) ensure that Subpopulations \n\neach have the same likelihood of \n\ndeserving the Positive Outcome \n\nfor a given Model prediction; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.9.  Differential Validity \n\nTesting \n\nIf applicable, test Model(s) for differential \n\nvalidity. Evaluate whether Model \n\nperformance varies meaningfully by (Sub) \n\npopulation, with a special focus on any \n\ngroups that are underrepresented in \n\nmodelling data. \n\nTo (a) ensure that the Modelâ€™s \n\npredictive abilities arenâ€™t isolated in \n\nor concentrated to (Sub)population \n\nmembers; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.10.  Feature Selection \n\nFairness Review \n\nEvaluate the impact of removing or \n\nmodifying potentially problematic input \n\nFeatures on Fairness metrics and Model \n\nquality. \n\nTo (a) assess whether more fair \n\nalternative Models can be made \n\nthat fulfill Model objectives; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.11.  Modeling \n\nMethodology \n\nFairness Review \n\nEvaluate the impact of changing Modelling \n\nmethodology choices (f.e. algorithm, \n\nsegmentation, hyperparameters, etc.) on \n\nFairness metrics and Model quality. \n\nTo (a) assess whether more fair \n\nalternative Models can be made \n\nthat fulfill the Model objectives; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n38 FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo maintain operationalised Fairness at the level established during Model Development. \n\n11.4 Production \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n11.4.1.  Domain Population \n\nStability \n\nContinually assess the stability of the \n\nDomain population being scored, both in \n\nterms of its composition relative to the \n\nModel development population, and the \n\nquality of the Model by class. \n\nTo (a) ensure the continued \n\naccuracy of Fairness tests \n\nand metrics; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.4.2.  Fairness Testing \n\nSchedule \n\nDefine a policy for timing of re-\n\nassessment of Model fairness that \n\nincludes re-testing at regular intervals \n\nand/or established trigger events (e.g. \n\nany modifications to Model inputs or \n\nstructure, changes to the composition of \n\nthe modeled population, impactful policy \n\nchanges). \n\nTo (a) detect issues with Model \n\nFairness that may not have existed \n\nduring pre-deployment of the \n\nModel; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n11.4.3.  Input Data \n\nTransparency \n\nEnsure that Product Subjects have the \n\nability to observe attributes relied on \n\nin the modeling decision and correct \n\ninaccuracy. Collect data around this \n\nprocess and use it to identify issues in the \n\ndata sourcing/aggregation pipeline. \n\nTo (a) ensure that the Model is \n\nmaking decisions on accurate \n\ndata; (b) learn whether there are \n\nproblems with Modelâ€™s data assets; \n\nand (c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.4.4.  Feature Attribution  Ensure that Product Subjects can \n\nunderstand why the Model made the \n\ndecision it did, or how the Model output \n\ncontributed to the decision. Ideally, an \n\nunderstanding would include which \n\nfeatures were most important in the \n\ndecision and give some guidance as \n\nto how the subject could improve in \n\nthe eyes of the Model. (See Section 13 -\n\nRepresentativeness & Specification for \n\nfurther information.) \n\nTo (a) ensure that Product Subjects \n\n(i) have some level of trust/ \n\nunderstanding in the Model that \n\naffect them and (ii) feel that they \n\nhave agency over the process \n\nand that Model Outcomes are not \n\narbitrary. \n\n11.4.5.  Product Subject \n\nAppeal Process \n\nIncorporate a â€œright of appealâ€ procedure \n\ninto the Modelâ€™s deployment, where \n\nProduct Subjects can request a human \n\nreview of the modeling decision. Collect \n\ndata around this process and use it to \n\ninform Model design choices. \n\nTo (a) ensure that Product Subjects \n\nare, at a minimum, made aware of \n\nthe results of Model decisions; and \n\n(b) allow inaccurate predictions to \n\nbe corrected. \n\n11.4.6.  Feature attribution \n\nMonitoring \n\nAs part of regularly scheduled review, or \n\nmore frequently, monitor any changes in \n\nfeature attribution or other explainable \n\nmetric by sub-population. (See Section \n\n15 - Monitoring & Maintenance for further \n\ninformation. \n\nTo (a) detect reasons for changes \n\nin Model performance, as well \n\nas any changes earlier in the \n\ndata pipeline; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n39 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure Data Quality and prevent unintentional effects, changes and/or deviations in Product and Model \n\noutputs associated with poor Product data. \n\nWhat is Data Quality? \n\nLike many other concepts in Machine learning and data science, Data quality is something without a single and \n\nwidely accepted definition. Nonetheless, we think of -\n\nData Quality as data which is fit for use for its intended purpose and satisfies business, system and technical \n\nrequirements. \n\nIn technical terms, data quality can be a measure of its completeness, accuracy, consistency, reliability and \n\nwhether it is up-to-date. \n\nData integrity is sometimes used interchangeably with data quality. However, data integrity is a broader concept \n\nthan data quality and can encompass data quality, data governance and data protection. \n\nWhy is Data quality important? \n\nIt is not difficult for all stakeholders involved in a Project to agree that good data quality is of prime importance. \n\nBad data quality means a business or an organization may not have a good grasp on whether they are successful \n\nin meeting prior set objectives or not. Bad data quality results in poor analytical solutions, wrong insights and \n\nconclusions. This translates into inadequate response to market opportunities, an inability to timely react to \n\ncustomersâ€™ requests, increased costs, and last, but not least, potential shortcomings in meeting compliance \n\nrequirements. In short, poor data results in poor products and poor decisions. This is undesirable. \n\nThe How of Data quality \n\nData quality is something that needs to be addressed throughout the product lifecycle, not only in the early \n\nstages of it, and not in any stage in isolation. \n\n# Section 12. Data Quality Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n40 FBPML Technical Best Practices v1.0.0. \n\n12. Data Quality - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo determine if the quality of the data shall be sufficient, or can be made sufficient, to achieve the Product \n\nDefinitions. \n\n12.1 Exploration \n\nControl:  Aim: \n\n12.1.1.  Data Definitions  Document and ensure all subtleties \n\nof definitions of all data dimensions \n\nare clear, inclusive of but not limited \n\nto gathering methods, allowed values, \n\ncollection frequency, etc. If not, acquire \n\nsuch knowledge, or discard the dimension. \n\nTo (a) assess and prevent \n\nunjustified assumptions about the \n\nmeaning of a data dimension or its \n\nvalues; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n12.1.2.  Data Modeling  Document and ensure all relationships \n\nbetween (the fields of) different datasets \n\nare clear, in the light of their Data \n\nDefinitions. (See Section 12.1.1 - Data \n\nDefinitions for further information.) If \n\nthis â€œData Modelâ€ is not clear or available, \n\ncreate it, or discard the datasets. \n\nTo (a) prevent the creation and/or \n\ncombination of invalid datasets; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n12.1.3.  Missing and Bad \n\nData Assessment \n\nDocument and assess (a) the occurrence \n\nrates and (b) co-variances of missing \n\nvalues and nonsensical values throughout \n\nthe Model data. If either is significant, \n\ninvestigate causes and consider \n\ndiscarding affected data dimension(s) \n\nor commit dedicated research and \n\ndevelopment to mitigating measures for \n\naffected data dimension(s). (See Section \n\n12.3.1. - Live Data Quality for further \n\ninformation.) \n\nTo assess (a) the risk of low quality \n\ndata introducing bias to Model \n\ndata and/or Outcomes; and (b) \n\nwhether Model dataset(s) quality is \n\nsufficient for Product Definitions; \n\nand (c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n12.1.4.  Data Veracity \n\nUncertainty & \n\nPrecision \n\nDocument and assess the veracity and \n\nprecision of data. If compromised, \n\nuncertain and/or unknown, document and \n\nassess (i) the causes and sources hereof \n\nand (ii) statistical accuracy .Incorporate \n\nappropriate statistical handling \n\nprocedures, such as calibration, and \n\nappropriate control mechanisms in Model, \n\nor discard the data dimension. \n\nTo assess (a) the risk of low quality \n\ndata introducing bias to Model data \n\nand/or outcomes; (b) a priori the \n\nplausibly achievable performance; \n\n(c) whether the Model dataset(s) \n\nquality is sufficient for Product \n\nDefinitions; and (d) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n41 FBPML Technical Best Practices v1.0.0. \n\n12. Data Quality - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo determine if Model performance is affected or biased due to data quality issues. \n\n12.2 Development \n\nControl:  Aim: \n\n12.2.1.  Missing and Bad \n\nData Handling \n\nDocument and assess how missing and \n\nnonsensical data (a) are handled in the \n\nModel, through datapoint exclusion or \n\ndata imputation; (b) affect the Selection \n\nFunction through datapoint removal; (c) \n\naffect Model performance and Fairness for \n\nsubpopulations through data imputation. \n\nIf (Sub)populations are unequally affected, \n\ntake additional measures to increase \n\ndata quality and/or improve Model \n\nresilience. Consult Domain experts during \n\nassessment and mitigation. \n\nTo (a) prevent introducing bias to \n\nModel Outcomes due to low quality \n\ndata; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n12.2.2.  Error - Quality \n\nCorrelation \n\nDocument and assess whether low-quality \n\ndatapoints (those with low-confidence, \n\nuncertain, nonsensical, missing and/or \n\nimputed attributes) correlate with high \n\n(rates of) error, and how this affects \n\n(Sub)populations. If so, take additional \n\nmeasures to increase data quality and/or \n\nimprove Model performance for specific \n\n(Sub)populations. \n\nTo (a) prevent introducing bias \n\nto Model Outcomes due to low \n\nquality data; (b) whether the Model \n\ndataset(s) quality is sufficient \n\nfor Product Definition(s); and \n\n(c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nObjective \n\nTo ensure the quality of incoming data to the Product during operations. \n\n12.3 Production \n\nControl:  Aim: \n\n12.3.1.  Live Data Quality  Document and assess whether live \n\nincoming data with low quality (low-\n\nconfidence, uncertain, nonsensical, \n\nmissing and/or imputed attributes) can be \n\nhandled appropriately by the Model on the \n\nper-Data Subject level. If not, implement \n\nadditional measures, and/or re-assess \n\nvalidity of Product Definition(s) in view \n\nof non-applicability to low quality live \n\nsubsets. \n\nTo (a) assess and control that all \n\nProduct Subjects can be supported \n\nappropriately by the live Product; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n42 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) ensure that Product data and Model(s) are representative of, and accurately specified for, Product Domain \n\nas far as is reasonably practical; and (b) guard against unintentional Product and Model behaviour and Outcomes \n\nas far as is reasonably practical. \n\nWhat is Representativeness and Specification? \n\nRepresentativeness is a concept that is often used in statistics and machine learning with regards to the data \n\nwe use to train a Model. A representative data sample is a set from a larger statistical population that adequately \n\nreplicates the larger group according to a characteristic or quality under study. Put less metaphorically -\n\nRepresentativeness means the ability of the Model and its data to adequately replicate and represent that \n\ncharacteristics of its operational environment. \n\nIt should not be confused with representation learning (also known as feature learning) in machine learning. \n\nThe latter refers to a set of techniques for automatically detecting feature patterns and in fact replaces manual \n\nfeature engineering. \n\nSpecification is a less known term. In our context -\n\nSpecification refers to ensuring the appropriate degrees of freedom in the Model. \n\nFor example, we have selected the appropriate cost function for the problem at hand, the target variable is \n\nappropriate and not a proxy for what we are really interested in measuring, etc. It is like representativeness \n\nbut for the Model, and not the data. Unlike the performance robustness section, many of the controls here will \n\nbe difficult to precisely measure quantitatively. However, we should still try to consider as many scenarios as \n\npossible and minimize all risks stemming from not addressing them rigorously. \n\nWhy is Representativeness and Specification important? \n\nIf the data is not representative with relation to the goal of the Product, it will not serve us well. It will result \n\nin poor performing Models when deployed, and it will inherently contain bias (not in the fairness and non-\n\ndiscrimination sense but in relation to sampling). This can lead to misleading conclusions and unrealistic \n\nassumptions and expectations. Correct specifications on the other hand relates to selecting appropriate and \n\nrigorous features, selection function, and target, etc. This ensures that the Model we develop is rigorous, robust \n\nand has a properly specified number of parameters. \n\nThe How of Representativeness and Specification \n\nRepresentativeness and Specification is something that needs to be addressed throughout the product lifecycle, \n\nnot only in the early stages of it, and not in any stage in isolation. \n\n# Section 13. Representativeness \n\n# & Specification Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n43 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) ensure the pragmatic formulation and accurate specification of Product Definition(s); (b) minimise Model \n\nsimplifications, assumptions and ambiguities; and (c) ensure adequate vigil of the non-reducible ones throughout \n\nthe Product Lifecycle. \n\n13.1 Product Definitions \n\nControl:  Aim: \n\n13.1.1.  R&S Product \n\nDefinition(s) \n\nAssessment \n\nDocument and assess whether recorded \n\nProduct Definition(s) are complete, \n\nunambiguous and representative of \n\nintended Product Outcomes. If they are \n\nnot, refine them as much as is reasonably \n\npractical. \n\nTo (a) enable reliable execution of \n\nall further research, development \n\nand assessments; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.2.  Product \n\nAssumptions \n\nDocument and assess Product \n\nassumptions, the likelihood of their \n\nappropriateness, their continued validity, \n\nand inherent risks. \n\nTo (a) detect, mitigate and review \n\nProduct assumptions and their \n\ninherent risks; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.3.  Product \n\nSimplifications \n\nDocument and assess Product \n\nsimplifications, the likelihood of their \n\nappropriateness, and their inherent risks. \n\nTo (a) detect, mitigate and review \n\nProduct simplifications and their \n\ninherent risks; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.4.  Product Limits  Document and assess the limitations \n\nof the Productâ€™s application and the \n\napplicability of Product Definitions. \n\nTo (a) detect and review Model \n\nlimitations in light of (i) Model \n\nassumptions and (ii) Model \n\nsimplifications; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.5.  R&S Problem \n\nDefinition Review \n\nR&S Product Definition(s) ought to be \n\nreviewed continually, specifically when \n\nsignificant Model changes occur. \n\nTo ensure that R&S Product \n\nDefinition(s) are kept up-to-\n\ndate to ensure their continued \n\neffectiveness, suitability, and \n\naccuracy; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n44 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n13.2.1.  Data Subjectivity  Document and assess whether the \n\nModel dataset(s) contain subjective \n\ncomponents. If subjective components \n\nare present, take measures to handle or \n\navoid subjectivity risks in Product and/ \n\nor Model design as much as is reasonably \n\npractical. \n\nTo (a) assess and control for the \n\naccuracy of the specification \n\nof Model inputs, manipulations, \n\nOutcomes, and interpretations \n\nto ensure the unambiguous \n\napplicability of Model(s) in Product \n\nDomain(s); and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.2.2.  Heterogeneous \n\nVariable \n\nSimplification \n\nDocument and assess whether Model \n\ndatasets contain, or Model components \n\nproduce, simplified input Features that \n\nrepresent inherently heterogeneous \n\nconcepts in Product Domains. If \n\nsimplified, take measures to reflect the \n\nheterogeneity of Product Domains as \n\nmuch as is reasonably practical. \n\nTo (a) detect, review and \n\ncontrol for the simplification of \n\nheterogeneous input Variables; \n\n(b) prevent generalization and \n\nspurious correlation; and (c) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. \n\n13.2.3.  Hidden Variables  Document and assess whether \n\nModel datasets are missing, or Model \n\ncomponents hide relevant attributes of \n\nProduct Subjects or systemic Variables \n\nwith respect to Product Domains. If \n\nhidden, obtain additional data and/ \n\nor account for the hidden Variables in \n\nmodelling as much as is reasonably \n\npractical. \n\nTo (a) assess and control for hidden \n\ncorrelations and causal relations in \n\nModel datasets and Variables and/ \n\nor risks of relations being spurious, \n\nambiguous and/or confounding; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n13.2.4.  Selection Function  Document and assess the propensity \n\nof subpopulations and subpopulation \n\nmembers to be (accurately) recorded in \n\nModel datasets, with particular care for \n\n(i) unrecorded individuals, (ii) Protected \n\nClasses, and (iii) survivorship effects. \n\nIncorporate the Selection Function \n\nin Model development and evaluation \n\nin particular during Fairness & Non-\n\nDiscrimination, Performance Robustness \n\ncontrols. \n\nTo (a) assess and control for the \n\naccuracy of Model and Model \n\ndatasets in representing (Sub) \n\npopulations; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\nObjective \n\nTo (a) ensure that Model dataset(s) correspond to the Product Definition in sufficient detail, completeness and \n\nwithout material unambiguity; and (b) to identify associated risks in order to ensure an adequate vigil throughout \n\nthe Product Lifecycle. \n\n13.2 Exploration Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n45 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\n13.2.5.  Feature \n\nConstraints \n\nEvaluate whether any constraints should \n\nbe applied to input Features, such as \n\nmonotonicity or constraints on input \n\nFeature interactions in consultation with \n\nDomain experts. If determined, utilise \n\nidentified constraints. \n\nTo (a) ensure that (i) Model \n\nOutcomes are maximally \n\ninterpretable and (ii) Model \n\nbehavior for individual Model \n\nSubjects is consistent with Domain \n\nexperience; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\nControl:  Aim: \n\n13.3.1.  Target \n\nSubjectivity \n\nDocument and assess whether the Target \n\nFeature(s) objectively represent Product \n\nDomain(s). If subjective, consider refining \n\nProduct Definition(s), choosing a different \n\nTarget Feature, or taking measures \n\nto promote the objectivity of Product \n\nOutcomes. \n\nTo (a) ensure that Product Outcomes \n\nare representative of subpopulations \n\nand applications, and are not \n\nmisinterpreted; (b) ensure that Models \n\nare optimized only and precisely \n\naccording to Product Definitions; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n13.3.2.  Target Proxies  Document and assess whether the \n\nTarget Feature(s) are proxies for the true \n\nTarget(s) of Interest in Product Domain(s). \n\nIf Target Features are proxies, take \n\nmeasures to ensure and review non-\n\ndivergence of Product Outcomes with \n\nregard to Product Definitions. \n\nTo (a) ensure that Product Outcomes \n\nare representative of subpopulations \n\nand applications, and are not \n\nmisinterpreted; (b) ensure that Models \n\nare optimized only and precisely \n\naccording to Product Definitions; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n13.3.3.  Target Proxy \n\nvs. True Target \n\nof Interest \n\nContrasting \n\nIf the Target Feature is a proxy (i) \n\ndocument and assess whether the \n\ntrue Target(s) of Interest correlate with \n\nprotected attributes and classes, including \n\nthrough hidden systemic Variables as \n\nmuch as is reasonably practical; and (ii) \n\ndocument and assess whether the true \n\nTarget(s) of Interest and the proxy Target \n\nFeature(s) correlate differently with the \n\nModel datasets. If true, take measures to \n\nmitigate this as much as is reasonably \n\npractical. \n\nTo (a) ensure that the Model design \n\nis oriented to the true Target(s) of \n\nInterest; and (b) highlight associated \n\nrisks that might occur in the lack \n\nthereof in the Product Lifecycle. \n\nObjective \n\nTo (a) ensure that Model design is sufficiently specified to represent Product Domain(s) and the Product \n\nDefinition(s) as much as is reasonably practical; and (b) minimise the risks of (i) adverse effects from the Modelâ€™s \n\noptimisation leading to unintended loopholes and local optima, and (ii) mis-balancing competing optimisation \n\nrequirements in Model design and development. \n\n13.3 Development Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n46 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\n13.3.4.  Heterogeneous \n\nTarget Variable \n\nSimplification \n\nDocument and assess whether the \n\nTarget Feature is a simplification of, or \n\ncontains a subset of, true Target(s) of \n\nInterest. If true, consider refining Product \n\nDefinitions, recovering the heterogeneity, \n\nor failing that, take measures to mitigate \n\nand review this as much as is reasonably \n\npractical. \n\nIdem Section 11.3.1-2; and to (a) detect \n\nand control for risks of generalization \n\nand spurious correlation creation. \n\n13.3.5.  Cost Function \n\nSpecification & \n\nOptimisation \n\nDocument and assess the risk propensity \n\nfor - (i) optimizing for subset of \n\nobjectives to the detriment of other \n\nProduct objectives, (ii) optimizing for \n\nOutcomes that are unintended and/or \n\nnot aligned with any Product objectives, \n\n(iii) feedback loops (when containing \n\nnested optimization loops), and (iv) \n\nModel confinement in adverse or less-\n\nthan-optimal parameter or solution \n\nspace - through Model cost function \n\nand optimisation procedures during the \n\nProduct Lifecycle. If risks occur, take \n\nmeasures to mitigate them as much as is \n\nreasonably practical. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n13.3.6.  Importance \n\nWeighting \n\nDocument and assess whether Model \n\ndata points are weighted by design or as \n\ncollateral effect. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle \n\n13.3.7.  Asymmetric \n\nError Weights \n\nDocument and assess whether Model \n\nerrors, and error rates, are weighted \n\nasymmetrically in the Model. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle \n\n13.3.8.  Feature \n\nWeighting \n\nDocument and assess whether Model \n\nfeatures are weighted by design or as \n\ncollateral effect. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n47 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n13.4.1.  Asymmetric \n\nError Costs \n\nDocument and assess whether Product \n\nDomain(s) costs produced by different \n\nModel errors types are accounted for in \n\nProduct implementation and application \n\nin software and processes. If not, take \n\nmeasures to ensure that they are. \n\nTo (a) ensure that Product \n\nDomain(s) and Product Subjects \n\nconsequences are accurately \n\nconsidered when implementing \n\nProduct outcomes; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n13.4.2.  Output \n\nInterpretation(s) \n\nDocument and assess whether Product \n\nOutcomes can be clearly, completely \n\nand unambiguously interpreted by the \n\nnon-technical parties and whether these \n\ninterpretations remain representative \n\nof Product Definition(s) and Model inner \n\nworkings. If not, take measures to ensure \n\nthat they are as much as is reasonably \n\npractical. \n\nTo (a) prevent (i) misinterpretation \n\nof Product Outcomes, (ii) the \n\napplication of Products in contexts \n\nand/or to Subjects for which \n\ntheir appropriateness and/or \n\nquality is unconfirmed, unknown, \n\nand/or unsatisfactory, (iii) the \n\nintentional and/or unintentional \n\nmisuse of Product components \n\nand Outcomes; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nObjective \n\nTo ensure that the Implementation of the Product and Model(s) align with and represent Product Definition(s) and \n\nProduct Domain(s). \n\n13.4 Production \n\n13.3.9.  Output \n\nInterpretation(s) \n\nDocument and assess whether the \n\ninterpretation of the Model Outcomes are \n\nclearly, completely and unambiguously \n\ndefined. If not, take measures to promote \n\nOutcome interpretation(s) clarity and \n\ncompleteness as much as is reasonably \n\npractical. \n\nTo (a) guard against the \n\nmisinterpretation and/or \n\nmisapplication of Model Outcomes; \n\nand (b) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n13.3.10.  Time-dependent \n\nData Modeling \n\nDocument and assess whether all time-\n\ndependent aspects of data generation \n\n(including but not limited to gathering, \n\ncalibration, cleaning, and annotation), data \n\nmodeling and data usage are specified \n\nand incorporated in Model design and \n\nProduct Definition(s). \n\nTo (a) prevent data leakage and other \n\nforms of â€œtime travelingâ€ information \n\nleading to inaccurate representations \n\nof the data and/or Data Subjects; and \n\n(b) highlight associated risks that \n\nmight occur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n48 FBPML Technical Best Practices v1.0.0. \n\n# Section 14. Performance \n\n# Robustness \n\nObjective: \n\nTo warrant Model Outcomes and prevent unintentional Model behaviour a priori under operational conditions as \n\nfar as is reasonably practical. \n\nWhat is Performance Robustness? \n\nModel robustness is the property of an algorithm that, when tested on a training sample and on a similar testing \n\nsample, the performance is similar. In other words, a robust model is one for which the testing error is similar to \n\nthe training error. Performance robustness takes into account prospective scenarios where (one of more) inputs \n\nor assumptions are (drastically) changed due to unforeseen circumstances and, in light of these, the ability of the \n\nModel to to still consistently generate accurate output. Put more holistically -\n\nPerformance Robustness means the ability of the Model to generate consistent, accurate results across different \n\nsampling tests and in light of changes in operational circumstances. \n\nWhy do we need Performance Robustness? \n\nA Model that is not robust will hopefully not end up being used and deployed. Good performance in training, \n\nbut significantly worse performance when tested on real data, is one of the reasons many proof-of-concepts \n\ndo not end up being utilized. A Model which is not robust will inevitably deteriorate over time. Its predictions \n\nand recommendations will deviate from the ground truth and the end users will lose trust in the Model and may \n\nstop utilizing it altogether. This is the optimistic case. More worrisome is when users of the Model continue to \n\nuse a poor performing Model and are unaware of its poor accuracy or precision, but still take it into account \n\nwhen making (important) judgement calls. In scenarios where there is no human-in-the-loop, detecting poor \n\nperformance robustness can be even more difficult and time costly. This will result in more unknown harm, which \n\nis naturally hard to detect and determine. So, it is clear that it is in everyoneâ€™s interest to ensure the Modelâ€™s \n\nperformance robustness. \n\nHow to ensure Performance Robustness? \n\nThough performance robustness needs to be of a certain level to even consider deploying the Model, it is \n\nsomething that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not \n\nin any stage in isolation. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n49 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo prevent performance loss due to Product Definition changes. \n\nObjective \n\nTo prevent performance loss due to (a) data and/or data definition instability; (b) volatile data elements; and/or (c) \n\nprospective increases in scale. \n\n14.1 Product Definitions \n\n14.2 Exploration \n\nControl:  Aim: \n\n14.1.1.  Product \n\nDefinition(s) \n\nStability \n\nDocument and assess the stability of historic \n\nand prospective Product Definition(s) and \n\nProduct Aim(s). If unstable, take measures \n\nto redefine or, failing that, to correct for or \n\nmitigate as much as is reasonably practical. \n\nTo (a) ensure that Product \n\nDefinition(s) and Models remain \n\nstable and up-to-date in light of \n\nProduct Domain Stability; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n14.1.2.  Product Domain \n\nStability \n\nDocument and assess the stability of \n\nhistoric and prospective Product Domain(s). \n\nIf unstable, revise Product Definition(s) \n\naccordingly to ensure Product consistency \n\nand stability. \n\nTo (a) ensure that Product \n\nDefinition(s) and Models remain \n\nstable and up-to-date in light of \n\nProduct Domain Stability; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nControl:  Aim: \n\n14.2.1.  Data Drift \n\nAssessment \n\nDocument and assess historic and \n\nprospective changes in data distribution, \n\ninclusive of missing and nonsensical data. If \n\ndata drift is apparent and/or expected in the \n\nfuture, implement mitigating measures as \n\nmuch as is reasonably practical. \n\nTo (a) assess and promote the \n\nstability of data distributions (data \n\ndrift); (b) determine the need for \n\ndata distributions monitoring, \n\nrisk-based mitigation strategies \n\nand responses, drift resistance \n\nand adaptation simulations and \n\noptimization, and data distribution \n\ncalibration; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.2.2.  Data Definition \n\nTemporal \n\nStability \n\nDocument and assess - both technically \n\nand conceptually - historic and prospective \n\nchanges of each data dimension definition. \n\nIf unstable, consider refining Product \n\nDefinitions and/or limiting usage of unstable \n\ndata dimensions. \n\nTo (a) assess and control for the \n\nneed for Model design adaptation \n\nbased on data definition stability; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n50 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\n14.2.3.  Outlier \n\nOccurrence \n\nRates \n\nDocument and assess outliers, their causes, \n\nand occurrence rates as a function of their \n\nlocation in data space. If numerous and \n\npersistent, include mitigating measures in \n\nModel design accordingly. \n\nTo (a) identify outliers and \n\nassess the need for Model design \n\nadaptation; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.2.4.  Selection \n\nFunction \n\nTemporal \n\nStability \n\nDocument and assess the historic and \n\nprospective behaviour of Selection \n\nFunction(s) of Model data. (See Section \n\n13.2.4. - Selection Function for more \n\ninformation.) If unstable, take measures \n\nto account for past and future changes, \n\nand/or promote the consistency and \n\nrepresentativeness of Model datasets and \n\ndata gathering as much as is reasonably \n\npractical. \n\nTo (a) assess and control for \n\nhard-to-measure changes to the \n\nrelation between Model datasets \n\nand Product Domain(s); (b) identify \n\nthe risk of hard-to-diagnose Model \n\nperformance degradation and \n\nbias throughout Product Lifecycle \n\n(to be controlled by 14.3.6); and \n\n(c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n14.2.5.  Data Generating \n\nProcess \n\nTemporal \n\nStability \n\nDocument and assess the historic and \n\nprospective behaviour of data generating \n\nprocesses, and their influence on the \n\nSelection Function. If unstable, take \n\nmeasures to account for past and future \n\nchanges and/or promote the stability and \n\nconsistency of data generation processes as \n\nmuch as is reasonably practical. \n\nTo (a) assess and control for \n\nhard-to-measure changes to the \n\nrelation between Model datasets \n\nand Product Domain(s); (b) identify \n\nthe risk of hard-to-diagnose Model \n\nperformance degradation and \n\nbias throughout Product Lifecycle \n\n(to be controlled by 14.3.6); and \n\n(c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle \n\nObjective \n\nTo characterize, determine and control for Model performance variation, risks and robustness under live \n\nconditions a priori and throughout the Product Lifecycle. \n\n14.3 Development \n\nControl:  Aim: \n\n14.3.1.  Target Feature \n\nDefinition \n\nStability \n\nDocument and assess - both technically \n\nand conceptually - the historic and \n\nprospective stability of the Target Feature \n\ndefinition. If unstable, consider refining \n\nProduct Definitions and/or choosing a \n\ndifferent Target Feature. \n\nTo (a) assess the need for Model design \n\nand Product Definition adaptation \n\nbased on Target Feature definition \n\nstability; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n51 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\n14.3.2.  Blind \n\nPerformance \n\nValidation \n\nDocument and validate that Model \n\nPerformance can always be reproduced \n\non never-before-seen hold-out data-\n\nsubsets and prove that these hold-out \n\ndata-subsets are never used to guide \n\nModel and Product design choices by \n\ncomparing Model performance on the \n\nhold-out dataset. If performance cannot \n\nbe reproduced on never-before-seen \n\nhold-out data-subset, take measures to \n\nimprove robustness and Model fitting as \n\nmuch as is reasonably practical. \n\nTo (a) ensure Model performance \n\nrobustness against insufficient \n\ngeneralization capabilities on live data \n\n(such as overfitting); and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.3.3.  Error \n\nDistributions \n\nDocument and assess error and/or \n\nresidual distributions along as many \n\ndimensions and/or subsets as is \n\npractically feasible. If distributions are \n\ntoo broad and/or too unequal between \n\nsubsets, improve Model(s). \n\nTo (a) assess and control for \n\nperformance influence of data \n\npoints and/or groups; (b) assess \n\nand control for the distribution of \n\nerrors to influence - (i) performance \n\nrobustness as a function of data drift, \n\n(ii) the systematic performance of \n\nminority data-subsets, and (iii) the \n\nrisks of unacceptable errors and/or \n\ncatastrophic failure; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.3.4.  Output Edge \n\nCases \n\nDocument and assess the causes, \n\noccurrence probabilities, overall \n\nperformance impact of Edge Cases \n\noutput by Model(s), inclusive of on Model \n\ntraining and design. If their influence \n\nis significant, improve model design. If \n\noccurrence is high, increase Model, code \n\nand data quality control. \n\nTo (a) assess and control for the \n\nimpact of Output Edge Cases on Model \n\ndesign, bugs and performance; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n14.3.5.  Performance \n\nRoot Cause \n\nAnalysis \n\nDocument and assess Model performance \n\nRoot Cause Analysis as well as its \n\ntesting method. If Root Cause Analysis \n\nis ineffective, simplify Model and/or \n\nincrease diagnostics like logging and \n\ntracking. \n\nTo (a) assess and control for Model \n\nperformance changes and assist \n\nin Model design, development, and \n\ndebugging; (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n14.3.6.  Model Drift \n\n& Model \n\nRobustness \n\nSimulations \n\nDocument and perform simulations of \n\nModel training and retraining cycles, \n\nusing historic and synthetic data. \n\nDocument and assess the effects of \n\ntemporal changes to, amongst other \n\nthings, the Selection Function, Data \n\nGenerating Process and Data Drift \n\non the drift in performance and error \n\ndistributions of said simulations. If Model \n\ndrift is apparent, document and perform \n\nfurther simulations for Model drift \n\nresponse optimization, and/or consider \n\nrefining Product Definitions. \n\nTo (a) assess and control for Model \n\npropensity for Model drift; (b) \n\ndetermine the robustness of Model \n\nperformance as a function of data \n\nchanges; (c) determine appropriate \n\nProduct response to drift; and (d) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n52 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\n14.3.7.  Catastrophic \n\nFailures \n\nDocument and assess the prevalence of \n\npredictions with High Confidence Values, \n\nbut large Evaluation Errors. If apparent, \n\nimprove Model to avoid these, and/or \n\nimplement processes to mitigate these as \n\nmuch as is reasonably practical. \n\nTo (a) assess the propensity of the \n\nModel for catastrophic failures; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n14.3.8.  Performance \n\nUncertainty \n\nand Sensitivity \n\nAnalysis \n\nDocument and assess the probability \n\ndistribution of the model performance \n\nusing cross-validation, statistical \n\nand simulation techniques under - (a) \n\nthe assumption that the distribution \n\nof training and validation data is \n\nrepresentative of the distribution of \n\nlive data; and (b) multiple realistic \n\nvariations to the Model data due to both \n\nstatistical and contextual causes. If Model \n\nperformance variation is high, improve \n\nModel and/or take measures to mitigate \n\nperformance variation impact. \n\nTo (a) assess and control for the \n\nrange of expected values of Model \n\nperformance under both constant \n\nand changing conditions; (b) assess \n\nand control for whether trained model \n\nperformance is consistent with these \n\nranges; (c) identify main sources of \n\nuncertainty and variation for further \n\ncontrol; and (d) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n14.3.9.  Outlier Handling  Document and assess the effect of \n\nvarious outlier handling procedures on \n\n(a) Performance Robustness and (b) \n\nRepresentativeness & Specification. \n\nEnsure that only procedures are \n\nimplemented that positively affect both. \n\nTo (a) ensure that outlier removal \n\nis not used to heedlessly improve \n\ntest-time performance only and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\nObjective \n\nTo ensure the future satisfaction of Product Definition(s) through the technical and functional implementation of \n\nthe Product Model(s) and systems. \n\n14.4 Production \n\nControl:  Aim: \n\n14.4.1.  Real World \n\nRobustness \n\nDocument and assess potential future \n\nchange in the applied effects of the Product, \n\nsuch as through diminishing returns and/ \n\nor psychological effects. If significant \n\nchange or decrease is expected, consider \n\nrefining Product Definitions and/or develop \n\nprocedures for mitigation. \n\nTo (a) assess and control for the \n\nvariation in applied effects of the \n\nProduct on Product Definition(s) \n\nand performance; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.4.2.  Performance \n\nStress Testing \n\nPerform and document experiments \n\ndesigned to attempt to induce failures in the \n\nProduct and/or Model, for example, but not \n\nlimited to, by supplying large quantities of or \n\nunusual data to the training or inferencing \n\nphases. \n\nTo (a) identify and control for \n\nrisks associated with operational \n\nscenarioâ€™s outside of regimes \n\nencountered during Model \n\ndevelopment. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n53 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure that Products and Models remain within acceptable operational bounds. \n\nWhat is Monitoring and maintenance? \n\nMachine learning -\n\nMonitoring refers to the processes of tracking and analysing the performance of a model over time and once it is \n\ndeployed in production \n\nIt provides early warning signals for performance issues. Maintenance is closely related to monitoring but is a \n\nmore actionable concept. \n\nMaintenance relates to the activities we need to perform upon detecting or suspecting possible deterioration in \n\nthe performance of the model. \n\nThough itâ€™s a process closely related to Models in production, note that maintenance and monitoring steps need \n\nto be designed and addressed in early stages of the Product Lifecycle too. \n\nWhy is Monitoring and maintenance important? \n\nMonitoring and maintenance is not only important but it is a â€˜must haveâ€™ for any Product that is deployed in a \n\nproduction environment. Over time, the â€˜liveâ€™ data will differ in small or significant ways from the historical data \n\nused to train the Model. Trends and preferences will change too. The way certain data sources are measured and \n\ncoded will also change over time: new data sources are added, while others become unavailable. Therefore, we \n\nneed to continuously, real-time monitor the Models that are deployed. A Model that is not maintained or updated \n\nover time eventually deteriorates, makes errors and could lead to a loss of trust and varying degrees of harm (if \n\nthe domain in question is a high-stakes decision domain). \n\nThe How of Monitoring and maintenance \n\nModel monitoring and maintenance though most commonly discussed in the deployment phase, is something \n\nthat needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage \n\nin isolation. \n\n# Section 15. Monitoring & \n\n# Maintenance Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n54 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) track Model performance in production; and (b) ensure desired Model performance. \n\nObjective \n\nTo (a) define robust Product and/or Model monitoring requirements, inclusive of concerns related to Features and \n\nskews of the data; and (b) ensure the continued monitoring of Products and/or Models throughout their lifecycles. \n\n15.1 Product Definitions \n\n15.2 Exploration \n\nControl:  Aim: \n\n15.1.1.  Monitoring \n\nObjectives \n\nBased on Product Definition(s), document \n\nand assess Product and Model monitoring \n\nobjectives, inclusive of which Product \n\nand/or Model elements need close \n\nmonitoring attention, such as Model \n\ndata and code. Document and assess \n\nthe associated risks of failing to achieve \n\nModel and/or Product Monitoring \n\nObjectives. \n\nTo (a) define Product and Model \n\nmonitoring objectives; and (b) \n\nhighlight associated risks for failed \n\nmonitoring. \n\n15.1.2.  Monitoring Risks  Document and assess the associated risks \n\nof failing to achieve Monitoring Objectives. \n\nTo (a) define Product and Model \n\nmonitoring risks. \n\nControl:  Aim: \n\n15.2.1.  Data Source \n\nMismatch: \n\nTraining & \n\nProduction Data \n\nDefine and deploy methods to detect \n\nthe degree to which data sources \n\nand Features, in Model training and \n\nproduction data, match one another. If \n\nmismatch is detected, take measures to \n\nensure that data sources and Features \n\nare adequately matched in both Model \n\ntraining and production data. \n\nTo (a) reduce nonsensical predictions \n\nof the Model due to (i) missing data, (ii) \n\nlack of data incorporated, or (iii) data \n\nmeasurement scaling, encoding and/or \n\nmeaning; (b) to reduce the discrepancy \n\nbetween training and production data; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n15.2.2.  Data \n\nDefinitions and \n\nMeasurements: \n\nTraining & \n\nProduction Data \n\nDefine and deploy methods by which to \n\ndetect the degree to which data sources \n\nin Model training and production have \n\nthe same definitions and measurement \n\nscales . \n\nTo (a) reduce nonsensical predictions \n\nof the Model due to (i) missing data, (ii) \n\nlack of data incorporated, or (iii) data \n\nmeasurement scaling, encoding and/or \n\nmeaning; (b) to reduce the discrepancy \n\nbetween training and production data; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n15.2.3.  Data \n\nDependencies \n\nand Upstream \n\nChanges \n\nDerive and implement change \n\nassessments for changes in data due \n\nto - (i) one or multiple internal or external \n\nsources (partial) updates, (ii) substantial \n\nsource change, and/or (iii) changes in \n\ndata production and/or delivery. \n\nTo (a) reduce nonsensical predictions \n\nof the Model due to (i) missing data, (ii) \n\nlack of data incorporated, or (iii) data \n\nmeasurement scaling, encoding and/or \n\nmeaning; (b) to reduce the discrepancy \n\nbetween training and production data; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n55 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\n15.2.4.  Data Drift \n\nDetection \n\nDefine and deploy monitoring metrics \n\nand thresholds for detecting sudden \n\nand/or gradual, short term and/or long \n\nterm changes in data distributions, \n\ngiving priority to those that can detect \n\npast observed changes. (See Section \n\n12.2.1- Missing and Bad Data Handling \n\nfor further information). Document and \n\nassess distribution families, statistical \n\nmoments, similarity measures, trends \n\nand seasonalities. \n\nTo (a) prevent predictions from \n\ndiverging from training data and/ \n\nor Product Definitions by assessing \n\nwhether production data is \n\nrepresentative of older data; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n15.2.5.  Product and/or \n\nProduct Domain \n\nChanges: Trends \n\nand Preferences \n\nDefine and deploy (a) monitoring \n\nmethods for detecting changes in \n\nProduct Domain(s) and/or Product \n\nDefinition(s); and (b) timeframes and/ \n\nor contextual triggers for reassessment \n\nof Product Domain(s) and Product \n\nDefinition(s) continued stability. \n\nTo (a) ensure Models capture accurate, \n\nrelevant, and current trends and \n\npreferences in Product Domain(s); (b) \n\nreduce Model â€˜blind spotsâ€™ and better \n\ncapture malicious events/attempts; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\nObjective \n\nTo (a) create metrics for (i) Model performance and (ii) Model performance deterioration; and (b) ensure the \n\ncontinued monitoring of Products and/or Models throughout their lifecycles. \n\n15.3 Development \n\nControl:  Aim: \n\n15.3.1.  Model \n\nPerformance \n\nDeterioration \n\nThresholds \n\nDocument, assess, and set thresholds \n\nfor Model performance deterioration in \n\nconsultation with Stakeholders. \n\nTo (a) ensure clear guidelines \n\nand indices of Model failure and \n\nperformance deterioration; (b) \n\nreduce the risk of unacknowledged \n\nModel failure and performance \n\ndeterioration; (c) reduce the \n\nlikelihood of Model decay, ensure \n\nrobustness and good performance \n\nin terms of selected metrics \n\nand scenarios; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n15.3.2.  Product \n\nContextual \n\nIndicators: \n\nModel \n\nPerformance \n\nDeterioration \n\nDocument, assess, and set Product and \n\nProduct Domain specific indicators of Model \n\nperformance deterioration, inclusive of \n\ntechnical and non-technical indicators. \n\nTo (a) ensure clear guidelines \n\nand indices of Model failure and \n\nperformance deterioration; (b) \n\nreduce the risk of unacknowledged \n\nModel failure and performance \n\ndeterioration; (c) reduce the \n\nlikelihood of Model decay, ensure \n\nrobustness and good performance \n\nin terms of selected metrics \n\nand scenarios; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n56 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\n15.3.3.  Reactive Model \n\nMaintenance \n\nIndicators \n\nDocument, assess, and set thresholds for \n\nModel failure and reactive maintenance \n\nTo (a) ensure clear guidelines \n\nand indices of Model failure and \n\nperformance deterioration; (b) \n\nreduce the risk of unacknowledged \n\nModel failure and performance \n\ndeterioration; (c) reduce the \n\nlikelihood of Model decay, ensure \n\nrobustness and good performance \n\nin terms of selected metrics \n\nand scenarios; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n15.3.4.  Awareness of \n\nfeedback loops \n\nDefine and deploy as far as is reasonably \n\npractical (a) methods to detect whether \n\nfeedback loops are occuring, and/or (b) \n\ntechnical and non-technical warning \n\nindicators for increased risk of the same. \n\nAs per Section 17 - Security: to \n\nprevent (in)direct adverse social \n\nand environmental effects as a \n\nconsequence of self-reinforcing \n\ninteractions with the Model(s); \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nObjective \n\nTo (a) identify operational maintenance metrics; and (b) ensure timely update, re-train and re-deployment of \n\nModel(s). \n\n15.4 Production \n\nControl:  Aim: \n\n15.4.1.  Operational \n\nPerformance \n\nThresholds \n\nDefine and set metrics and tolerance \n\nintervals for operational performance of \n\nModels and Products, such as, amongst \n\nother things, latencies, memory size, CPU \n\nand GPU usage. \n\nTo (a) prevent unavailable and \n\nunreliable service; (b) enable quick \n\ndetection of bugs in the code; (c) \n\nensure smooth integration of the \n\nModel with the rest of the systems; \n\nand (d) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n15.4.2.  Continuous \n\nDelivery of \n\nMetrics: Model \n\nPerformance \n\nContinuously report on and record \n\nmetrics about Model performance, \n\npredictions, errors, Features, and \n\nassociated performance metrics to \n\nrelevant Stakeholders (as decided upon \n\nin Section 13.2 _- Representativeness & \n\nSpecification: Exploration and Section \n\n13.3 - Representativeness & Specification: \n\nDevelopment). \n\nTo (a) enable rapid identification of \n\nModel decay, and/or red flags and \n\nbugs in Model and/or data pipelines; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n57 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\n15.4.3.  Model Decay & \n\nData Updates \n\nOperationalise procedures to mitigate Data \n\nDrift and/or Model decay (as described in \n\nSection 14.2 _- Performance Robustness: \n\nExploration and Section 14.3 - Performance \n\nRobustness: Development). \n\nTo (a) ensure timely implementation \n\nof any changes required in data \n\nand/or Modelling pipelines; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n15.4.4.  Model Re-\n\ntraining \n\nOperationalise procedures on how Model \n\nre-training ought to be conducted as well \n\nas approached, inclusive of, amongst other \n\nthings, -\n\n(1) when will (i) a new Model be deployed, \n\nand/or (ii) a Model with the same \n\nhyperparameters but trained on new data; \n\nand/or \n\n(2) when operationalizing re-trained \n\nModels ought they be run in parallel with \n\nolder Models and/or do to gracefully \n\ndecommission older Models. \n\nTo (a) ensure timely implementation \n\nof any changes required in data \n\nand/or Modelling pipelines; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n15.4.5.  Create \n\nContingency \n\nPlans \n\nDevelop and put in place contingency plans \n\nin case of technical failures and out-of-\n\nbounds behaviour based on (a) bounds and \n\nthreshold set in other controls; and (b) risk \n\nassessment of failure modes. \n\nTo (a) prevent adverse effects \n\nfrom failures and unexpected \n\nbehaviour by providing clear \n\ninstructions on roll-back, mitigation \n\nand remediation; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n58 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure Model functions and outputs are explainable and justifiable as far as is practically reasonable in \n\norder to (a) foster explainability for Stakeholders, (b) promote Model trust, (c) facilitate Model debugging and \n\nunderstanding, and (d) promote compliance with existing laws and statutes. \n\nWhat do we mean when we refer to Explainability? \n\nThere is not one agreed-upon definition of explainability but the working definition we have adopted is that \n\nExplainability refers to making the behavior and decisions of a complex machine learning model understandable \n\nto humans. \n\nClosely related to the concept of explainability is interpretability. Interpretability refers to the degree to which a \n\nhuman can inherently understand the cause of a Modelâ€™s decision. In other words, interpretability relates to using \n\nModels that are transparent and can be inherently understood by humans; while explainability concerns making \n\ncomplex, non-transparent models understandable to humans. Many researchers and practitioners use the terms \n\ninterchangeably. \n\nTransparency is another closely related concept to explainability and interpretability. It is the broadest of the \n\nthree. Transparency refers to the openness of the workings and/or processes and/or features of data, Models \n\nand the overall project (the Product). Transparency can be both comprehensible or incomprehensible depending \n\non its content. Transparency does not necessarily mean comprehension: this is important and why it differs \n\nfrom explainability and interpretability. Again, transparency just refers to the openness of the workings and/or \n\nprocesses and/or features of data, Models and the overall project - whether technical or not. \n\nWhy is Explainability relevant? \n\nWhen we talk about machine learning used for high-stakes decisions, there is a strong agreement that it is \n\nextremely important for the public and for machine learning practitioners to understand the inner workings and \n\ndecision-making of Models. This is because through such understandings, we can ensure that machine learning \n\nis done fairly or, rather, that it does not generate unfair or harmful consequences. To put it more simply, we can \n\nensure human oversight and correction over machine learning operations. Explainability also is very important \n\nfor promoting trust and social acceptance of machine learning. People do not often trust and accept things they \n\ndo not understand. Through explainability, we can help people understand machine learning and, in turn, trust it. \n\nHow to apply Explainability? \n\nIn order to generate thorough and thoughtful explainability, it must be considered continuously throughout all \n\nstages of the product lifecycle. This means that explainability must be addressed at the (a) Product Definition(s), \n\n(b) Exploration, (c) Development and (d) Production stages of machine learning operations. \n\n# Section 16. Explainability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n59 FBPML Technical Best Practices v1.0.0. \n\n16. Explainability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) ensure the transparency of Product Definitions; (b) foster multi-stakeholder buy-in through explanations; \n\nand (c) reduce ethical risks in Product Definition(s) decision-making and Model Runs. \n\n16.1 Product Definitions \n\nControl:  Aim: \n\n16.1.1.  Explainability \n\nAims \n\nHaving consideration for (a) Product \n\nDefinition(s), (b) the explanations and/or \n\ntransparency sought, (c) the Model adopted, \n\nand (d) datasets used, document and assess \n\nthe explainability aims of the Model. \n\nTo (a) clearly document the \n\nexplainability and transparency \n\naims of the Model; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.1.2.  Explainability \n\nStakeholder \n\nDocument and assess the internal and \n\nexternal Stakeholders affected by the Model. \n\nTo identify the Model explainability \n\nStakeholders; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.1.3.  Explainability \n\nRisks \n\nAssessment \n\nDocument and assess the individual risks \n\nof failing to provide model explainability, \n\ninclusive of a legal liability and Explainability \n\nStakeholders mistrust. \n\nTo identify the risks of failing \n\nto provide Model explainability; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n16.1.4.  Legal \n\nRequirements \n\nfor \n\nInterpretability \n\nDocument and assess any specific \n\nlegal requirements for Explainability in \n\nconsultation with legal experts. \n\nTo (a) ensure that minimum \n\nstandards for explainability are \n\nmet and legal risk is addressed; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n16.1.5.  Explainability \n\nRequirements \n\nDocument and assess the explainability \n\nand transparency requirements and \n\nlevels in light of (a) Explainability Aims, \n\n(b) Explainability Stakeholders, and (c) \n\nExplainability Risks, taking care that the \n\nelicitation of said requirements involves \n\nappropriate guidance, education and \n\nunderstanding of Stakeholders. \n\nTo (a) clearly document the \n\nexplainability requirements of the \n\nModel; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n60 FBPML Technical Best Practices v1.0.0. \n\n16. Explainability - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n16.2.1.  Stakeholder \n\nAppraisal \n\nDocument and conduct (a) ad-hoc \n\ninterviews, (b) structured surveys and/ \n\nor (c) workshops with Explainability \n\nStakeholders about their Model and \n\nProduct concerns and literacy. \n\nTo (a) generate Explainability \n\nStakeholders analytics in order to map \n\nModel explainability requirements and \n\ndemands; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n16.2.2.  Stakeholder \n\nAppraisal \n\nAnalysis \n\nDocument, analyse and map the \n\noutcomes of the Stakeholder Appraisal \n\nagainst the Explainability Aims and \n\nExplainability Risks. \n\nTo (a) map and analyse Model \n\nexplainability requirements and \n\ndemands in light of the needs of \n\nExplainability Stakeholders; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n16.2.3.  Explainability \n\nMatrix \n\nDocument, assess, and derive a matrix \n\nevaluating and ranking the metrics and/ \n\nor criteria of explanations needed for \n\nbased on the (a) Stakeholder Appraisal \n\nAnalyse, (b) Explainability Aims, (c) \n\nExplainability Risks, and (d) Explainability \n\nRequirements, inclusive of explanations \n\naccuracy, fidelity, consistency, stability, \n\ncomprehensibility, certainty, and \n\nrepresentativeness. \n\nTo (a) derive a clear matrix from \n\nwhich to assess Model explainability \n\nrequirements; and (b) highlight \n\nassociated risks that might occur in the \n\nProduct Lifecycle. \n\n16.2.4.  Explainability \n\nFeature \n\nSelection \n\nDocument and analyse the degree of \n\nFeature explainability needed in light of \n\nthe Explainability Matrix. \n\nTo (a) identify the requisite degree of \n\nFeature explainability needed; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle, such \n\nas later stage Model retraining due to \n\nfeature ambiguity. \n\n16.2.5.  Explainability \n\nModelling \n\nMapping & \n\nAnalysis \n\nDocument and analyse the technical \n\nneeds of Model explainability in light \n\nof the Explainability Matrix, inclusive \n\nof considerations of global vs. local \n\nexplainability and/or pre-modelling \n\nexplainability, modelling explainability, \n\nand post-hoc modelling explainability \n\nTo (a) identify the technical needs \n\nof the Explainability Matrix; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n16.2.6.  Explanation \n\nFrequency \n\n& Delivery \n\nAssessment \n\nDocument and assess the frequency, \n\nmost suitable and practically reasonable \n\nmethods of communicating Model \n\nexplainability in light of the Explainability \n\nMatrix and Stakeholder Appraisal \n\nAnalysis. \n\nTo (a) identify the most appropriate \n\nmethod of communicating Model \n\nexplainability in order to promote \n\nexplainability comprehension; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\nObjective \n\nTo identify and document Model explainability and transparency requirements, inclusive of Stakeholder needs. \n\n16.2 Exploration Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n61 FBPML Technical Best Practices v1.0.0. \n\n16. Explainability - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n16.3.1.  Explainability \n\nFeature \n\nSelection \n\nAssessment \n\nConduct a Feature analysis of the \n\nExplainability Feature Selection in order to \n\nremove correlated and dependent Features. \n\nTo (a) interrogate the assumption \n\nof zero Feature dependency in \n\nexplainability modelling; (b) prevent \n\nmisleading Model explainability \n\nand transparency; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.3.2.  Global \n\nExplainability \n\nModel Run \n\nDocument and run as many types of global \n\nexplainability Models as is reasonably \n\npractical, such as Feature importances, \n\nFeature interactions, global surrogate \n\nModels, perturbation-based techniques \n\nor gradient-based techniques. When \n\nthere is doubt about the stability of the \n\ntechniques being used, test their quality \n\nthrough alternative parameterizations or by \n\ncomparing across techniques. \n\nTo (a) generate global explainability \n\nof the model; (b) help promote \n\nmodel debugging; (c) ensure \n\nexplainability fidelity and stability \n\nthrough numerous explainability \n\nmodel runs; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.3.3.  Local \n\nExplainability \n\nModel Run \n\nDocument and run as many types of local \n\nexplainability Models as is reasonably \n\npractical, such as perturbation-based \n\ntechniques or gradient-based techniques \n\nor, for more specific examples, Local \n\nInterpretable Model-Agnostic Explanations \n\n(LIME), SHAP values, Anchor explanations \n\namongst others. When there is doubt \n\nabout the stability of the techniques being \n\nused, test their quality through alternative \n\nparameterizations or by comparing across \n\ntechniques. \n\nTo (a) generate global explainability \n\nof the model; (b) help promote \n\nmodel debugging; (c) ensure \n\nexplainability fidelity and stability \n\nthrough numerous explainability \n\nmodel runs; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.3.4.  Visual \n\nExplanations \n\nAssessment \n\nDevelop visual aids to present and represent \n\nModel explainability and transparency \n\ninsights, such as Tabular Graphics, Partial \n\nDependency Plots, Individual Conditional \n\nExpectations, and/or Accumulated Local \n\nEffects plot. \n\nTo promote explainability \n\ncomprehension. \n\n16.3.5.  Example-based \n\nand Contrastive \n\nExplanations \n\nAssessment \n\nDevelop example-based and contrastive \n\nexplanations to present and represent \n\nModel explainability insights, such as the \n\nunderlying distribution of the data or select \n\nparticular instances. \n\nTo promote explainability \n\ncomprehension, such as of \n\ncomplex data distributions and/ \n\nor datasets for Explainability \n\naudiences. \n\nObjective \n\nTo ensure that Model design represents the explainability requirements and demands of transparency aims as \n\nmuch as is reasonably practical. \n\n16.3 Development Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n62 FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo monitor and track the performance of the explanations and trigger when any of the explainability approaches \n\nneed to be re-trained. \n\n16.4 Production \n\nControl:  Aim: \n\n16.4.1.  Explainability \n\nModel \n\nThresholds \n\nSet clear performance thresholds and \n\nlimitations for explainability Model(s). \n\nTo (a) define parameters for \n\nthe continued suitability and \n\nperformance of explainability \n\nModel(s); and (b) highlight \n\nassociated risks. \n\n16.4.2.  Explainability \n\nModel Review & \n\nMonitoring \n\nPeriodically, or when significant Model \n\nchanges occur, review implemented \n\nexplainability Model(s) in light of \n\nExplainability Model Thresholds. \n\nTo (a) ensure the continued \n\nsuitability and performance of \n\nexplainability Model(s) and their \n\nexplanations; and (b) highlight \n\nassociated risks. \n\n16.4.3.  Explanation \n\nTracking & \n\nMonitoring \n\nDocument and conduct (a) ad-hoc \n\ninterviews, (b) structured surveys, and/or (c) \n\nworkshops with Explainability Stakeholders \n\non explanations provided and adjust \n\noutcomes in Section 14 - Performance \n\nRobustness accordingly. \n\nTo ensure the continued \n\neffectiveness and suitability of \n\nprovided Model explanations.", "fetched_at_utc": "2026-02-08T18:50:45Z", "sha256": "12585dab1747c66e4581210aea88c74799e2f45c82c2f1f703f702da76123171", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-32-62.pdf", "file_size": 945946, "relative_path": "pdfs\\FBPML_TechnicalBP_V1.0.0-32-62.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 20487}}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-63-87-d95e143273c4", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-63-87.pdf", "title": "FBPML_TechnicalBP_V1.0.0-63-87", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n63 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) prevent adversarial actions against, and encourage graceful failures for, Products and/or Models; (b) \n\navert malicious extraction of Models, data and/or intellectual property; (c) prevent Model based physical and/or \n\nirreparable harms; and (d) prevent erosion of trust in Outputs or methods. \n\nWhat do we mean when we refer to Security? \n\nSecurity is broadly defined as the state of being free from danger or threat. Building on this definition, within the \n\ncontext of machine learning -\n\nSecurity refers to the state of ensuring that machine learning Products and/or Models are free from adversarial \n\ndanger, threat or attacks. \n\nAdversarial danger, threat or attacks are understood as the malicious intent to negatively impact machine \n\nlearning Productsâ€™ and/or Modelsâ€™ functionality and/or metrics without organisation consent, whether threatened \n\nor actualised. If an organisation does consent to any such activity, this is - rather - a form of penetration testing \n\nand/or security analysis, as opposed to an adversarial danger, threat or attack. \n\nWhy is Security relevant? \n\nMachine learning Product and/or Model security is imperative to ensure operational robust performance. Without \n\nthe ability to secure the Productâ€™s and/or Modelâ€™s integrity from adversarial danger, threat or attack, malicious \n\nthird parties can use an organisationâ€™s Products and Models to either unlawfully enrich themselves or, more \n\nseriously, cause operational environment harms, including death and/or destruction. These are intolerable risks \n\nas they undermine organisation, societal and machine learning trust and confidence. \n\nHow to apply Security? \n\nIn order to generate thorough and thoughtful security, it must be considered continuously throughout all \n\nstages of the product lifecycle. This means that security must be addressed at the (a) Product Definition(s), (b) \n\nExploration & Development, (c) Production and (d) Confidence & Trust stages of machine learning operations. \n\n# Section 17. Security Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n64 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo identify and control for Adversarial risks and motives based on Product Definition, characterized by adversary \n\ngoals. \n\n17.1 Product Definitions \n\nControl:  Aim: \n\n17.1.1.  Exfiltration \n\nAttacks \n\nDocument and assess whether the data \n\nemployed and gathered by the Product, and \n\nthe intellectual property generated possess \n\nvalue for potential adversarial actors. \n\nTo (a) identify the risks associated \n\nwith (i) Product Subject physical, \n\nfinancial, social and psychological \n\nwellbeing, and (ii) Organization \n\nfinancial wellbeing; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n17.1.2.  Evasion Attacks  Document and assess whether Product \n\nSubjects gain advantage from evading \n\nand/or manipulating the Product Outputs. \n\nDocument and assess whether adversarial \n\nactors stand to gain advantage in \n\nmanipulating Product Subject by evading \n\nand/or manipulating Product Output. \n\nTo (a) identify the risks associated \n\nwith Product Output manipulation \n\nin regard to malicious and \n\nnefarious motives; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n17.1.3.  Targeted \n\nSabotage \n\nDocument and assess whether adversarial \n\nactors can cause harm to specific targeted \n\nProduct Subjects by manipulating Product \n\nOutputs. \n\nDocument and assess whether \n\nadversarial actors can cause \n\nharm to specific targeted Product \n\nSubjects by manipulating Product \n\nOutputs. \n\n17.1.4.  Performance \n\nDegradation \n\nAttack \n\nDocument and assess whether a malicious \n\nperformance degradation for a specific (Sub) \n\npopulation can cause harm to that (Sub) \n\npopulation. Document and assess whether \n\ngeneral performance degradation can cause \n\nharm to society, Product Subjects, the \n\nOrganization, the Domain and/or the field of \n\nMachine Learning. \n\nTo (a) identify the risks in \n\nregard to (i) Product Subjectsâ€™ \n\nphysical, financial, social and \n\npsychological wellbeing, (ii) the \n\nOrganizationâ€™s financial and \n\nreputational wellbeing, (iii) society-\n\nwide environmental, social and \n\neconomic wellbeing, and (iv) the \n\nDomainsâ€™ reputational wellbeing; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n65 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\n17.2.1.  Data Poisoning \n\nAssessment \n\nDocument and assess the ease and extent with \n\nwhich adversarial actors may influence training \n\ndata through manipulating and/or introducing \n\n- (i) raw data; (ii) annotation processes; (iii) \n\nnew data points; (iv) data gathering systems \n\n(like sensors); (v) metadata; and/or (vi) multiple \n\ncomponents thereof simultaneously. If this \n\nconstitutes an elevated risk, document, assess \n\nand implement measurements that can be taken \n\nto detect and/or prevent the above manipulation \n\nof training data. \n\nTo (a) prevent adversarial \n\nactors from seeding \n\nsusceptibility to Evasion \n\nAttacks, Targeted Sabotage \n\nand Performance Degradation \n\nAttacks by way of (i) \n\nintroducing hard to detect \n\ntriggers, (ii) increasing \n\nnoise, and/or (iii) occluding \n\nor otherwise degrading \n\ninformation content; and (b) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. \n\n17.2.2.  Public Datasets  Employ public datasets whose characteristics \n\nand Error Rates are well known as a benchmark \n\nand/or make the Product evaluation results \n\npublic. \n\nTo (a) increase the probability \n\nof detection adversarial \n\nattacks, such as Data \n\nPoisoning, by enabling \n\ncomparison with and by public \n\nresources; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.3.  Data Exfiltration \n\nSusceptibility \n\nDocument and assess the susceptibility of the \n\nModel to data Exfiltration Attacks through - (i) \n\nthe leakage of (parts of) input data through \n\nModel Output; (ii) Model memorization of training \n\ndata that may be exposed through Model output; \n\n(iii) the inclusion by design of (some) training \n\ndata in stored Model artifacts; and/or (iv) \n\nrepeated querying of the Model. \n\nTo (a) warrant and control the \n\nrisk of Model data theft; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.2.4.  Model \n\nExfiltration \n\nSusceptibility \n\nDocument and assess the susceptibility of \n\nModels to Exfiltration Attacks with the aim of \n\nobtaining a copy, or approximation of, the Model \n\nor other Organization intellectual property, \n\nthrough repeated querying of the Model and \n\nanalysing the obtained results and confidence \n\nscores. \n\nTo (a) warrant and control the \n\nrisk of Model and intellectual \n\nproperty theft; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\nObjective \n\nTo identify and control for Adversarial Risks based on and originating in Model properties and/or Model data \n\nproperties. \n\n17.2. Exploration & Development \n\nControl:  Aim: Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n66 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\n17.2.5.  Exfiltration \n\nDefence \n\nTo reduce susceptibility of Exfiltration Attacks, \n\n(a) make Exfiltration Attacks computationally \n\nexpensive; (b) remove as much as possible \n\ninformation from Model Output; (c) add noise \n\nto Model Outputs through techniques such \n\nas differential privacy; (d) limit querying \n\npossibilities in volume and/or scope; and/or (e) \n\nchange Model architecture. \n\nTo (a) warrant and control the \n\nrisk of Exfiltration Attacks; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.2.6.  Adversarial \n\nInput \n\nSusceptibility \n\nDocument and assess the susceptibility \n\nof Models to be effectively influenced by \n\nmanipulated (inferencing) input. Reduce \n\nthis susceptibility by (a) increasing the \n\nrepresentational robustness (f.e. through \n\nmore complete embeddings or latent space \n\nrepresentation); and/or (b) applying robust \n\ntransformations (possibly cryptographic) and \n\ncleaning. \n\nTo (a) warrant the control of the \n\nrisk of Evasion and Sabotage \n\nAttacks, including Adversarial \n\nExamples; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.7.  Filtering \n\nSusceptibility \n\nIf sufficient potential motive has been \n\ndetermined for adversarial attack, document \n\nand assess the specific susceptibility of the \n\npre-processing filtering procedures of Models \n\nbeing evaded by tailored inputs, based on the \n\ninformation available to an adversarial attacker \n\nabout these procedures; in addition to the \n\ngeneral Susceptibility Assessment. Increase the \n\nrobustness of this filtering as far as practically \n\nfeasible. \n\nTo (a) warrant the control of the \n\nrisk of Evasion and Sabotage \n\nAttacks, including Adversarial \n\nExamples; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.8.  Training \n\nSusceptibility \n\nIf sufficient potential motives have been \n\ndetermined for adversarial attack, document \n\nand assess the specific susceptibility of Model \n\ntraining to attack through the manipulation of (a) \n\nthe partitioning of train, validation and test sets, \n\nand/or (b) Modelsâ€™ hyperparameters; in addition \n\nto the general Susceptibility Assessment. \n\nImplement more strict access control on \n\nproduction-grade training and hyperparameter \n\noptimization procedures. \n\nTo (a) warrant the control of \n\nthe risk of Evasion, Sabotage \n\nand Performance Degradation \n\nAttacks; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.9.  Adversarial \n\nExample \n\nSusceptibility \n\nIf sufficient potential motives have been \n\ndetermined for adversarial attack, document and \n\nassess the specific susceptibility of Models to \n\nAdversarial Examples by considering - (a) sparse \n\nor empty regions of the input space, and/or (b) \n\nModel architectures; in addition to the general \n\nSusceptibility Assessment. Document and \n\nimplement specific protective measures, such \n\nas but not limited to adversarial training. \n\nTo (a) warrant the control of \n\nthe risk of Evasion Attacks, \n\nspecifically Adversarial \n\nExamples; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n67 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\n17.2.10.  Adversarial \n\nDefence \n\nIf sufficient potential motive and susceptibility \n\nto adversarial attacks have been determined, \n\nimplement as far as reasonably practical \n\n- (a) data testing methods for detection of \n\noutside influence on input and Output Data; \n\n(b) reproducibility; (c) increase redundancy \n\nby incorporating multimodal input; and/or (d) \n\nperiodic resets or cleaning of Models and data. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.11.  General \n\nSusceptibility -\n\nInformation \n\nDocument, assess and control the general \n\nsusceptibility to attack due to information \n\nobtainable by attackers, by considering (a) \n\nsensitivity to input noise and/or noise as \n\na protective measure; (b) the amount of \n\ninformation an adversarial actor may obtain \n\nfrom over-extensive logging; and/or (c) whether \n\nproviding confidence scores as Output is \n\nbeneficial to adversarial actors. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.12.  General \n\nSusceptibility -\n\nExploitability \n\nDocument, assess and control the general \n\nModel susceptibility to attack due to exploitable \n\nproperties of Models, considering (a) overfit \n\nor highly sensitivity Models and Model \n\nhyperparameters are easier to attack; (b) an \n\nover-reliance on gradient methods that make \n\nModels more predictable and inspectable; (c) \n\nModels may be pushed past their applicability \n\nboundaries if input is not validated; and (d) non-\n\nrandom random number generators might be \n\nreplaced by cryptographically secure random \n\nnumber generators. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.13.  General \n\nSusceptibility -\n\nDetection \n\nDocument, assess and control the capability to \n\ndetect attacks through the ability to understand \n\nwhen Model behaviour is anomalous by (a) \n\ndecreasing Model opaqueness, and/or (b) \n\nincreasing Model robustness. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.14.  Open Source \n\nand Transfer \n\nLearning \n\nVulnerability \n\nDocument the correspondence between \n\npotential attack motives and attack \n\nsusceptibility posed by using, re-using or \n\nemploying for transfer learning open source \n\nModels, Model weights, and/or Model parameters \n\nthrough - (a) maliciously inserted behaviour and/ \n\nor code (â€œtrojansâ€), (b) the ability of an adversarial \n\nactor to investigate and attack open source \n\nModels unhindered; and (c) improper (re-)use. \n\nConsider using non-open source Models or \n\nmaking significant changes aimed at reducing \n\nsusceptibility. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n68 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo identify and control for Adversarial Risks based on and/or originating in Models production. \n\n17.3 Production \n\nControl:  Aim: \n\n17.3.1.  IT Security  Traditional IT security practices are referred \n\nto. Areas of particular importance to ML-\n\nbased systems include - (a) backdoor access \n\nto the Product, in particular the components \n\nvulnerable to attack risk as identified in \n\nother controls; (b) remote host servers \n\nvulnerability; (c) hardened and isolated \n\nsystems; (d) malicious insiders (e)man-in-\n\nthe-middle attacks; and/or (f) denial-of-\n\nservice. \n\nTraditional IT security practices \n\nare referred to. Areas of particular \n\nimportance to ML-based systems \n\ninclude - (a) backdoor access \n\nto the Product, in particular \n\nthe components vulnerable to \n\nattack risk as identified in other \n\ncontrols; (b) remote host servers \n\nvulnerability; (c) hardened and \n\nisolated systems; (d) malicious \n\ninsiders (e)man-in-the-middle \n\nattacks; and/or (f) denial-of-\n\nservice. \n\n17.3.2.  Periodic Review \n\nand Validation \n\nIf motive and risk for Adversarial Attack is \n\nhigh, perform more stringent and frequent \n\nreview and validation activities. \n\nTo (a) warrant and control the risk \n\nof Adversarial Attacks in general \n\nby increasing detection probability \n\nand fixing vulnerabilities quickly; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.3.3.  input and Output \n\nVulnerability \n\nDocument and assess the vulnerability of \n\nthe Product and related systems to direct \n\nmanipulation of inputs and Outputs. \n\nDirect Output manipulation if possible is the \n\nmost straightforward, simplest, cheapest \n\nand hardest to detect attack \n\nTo (a) create redundancy with input \n\nand inferencing hyperparameter \n\nsusceptibility; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n17.3.4.  Defense \n\nStrength \n\nAssessment \n\nDocument and assess the strength and \n\nweaknesses of all layers of defense against \n\nattacks and identify the weakest links. \n\nTo (a) build defense in depth; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n69 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n17.4.1.  Trust Erosion  Document and assess the potential impact \n\non trust from adversarial and defacement \n\nattacks, and establish a strategy to mitigate \n\ntrust erosion in case of successful attacks. \n\nTo (a) prevent erosion of trust in \n\nProduct Outputs, the Product, the \n\nOrganization, and/or Domains from \n\npreventing beneficial Products \n\nand technologies to be employed; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.4.2.  Confidence  Document and assess the degree of over-\n\nand under-confidence in the Product output \n\nby Product Team, Stakeholder(s) and End \n\nUsers. Encourage an appropriate level of \n\nconfidence through education and self-\n\nreflection. \n\nNote: Underconfidence will lead to users \n\nover-ruling the Product in unexpected ways. \n\nOverconfidence leads to lower scrutiny and \n\ntherefore lowers the chance of detection \n\nand prevention of attacks. \n\nTo (a) balance the risk of \n\ncompromising Product effects \n\nagainst reduced vigilance; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.4.3.  Warning Fatigue  Document and assess the frequency of \n\nwarnings and alerts provided to Product \n\noperators, maintainers, and Product \n\nSubjects, and refine the thresholds and \n\nprocesses such that no over-exposure to \n\nalerts can lead to systematic ignoring of \n\nalerts. \n\nTo (a) prevent an overexposure \n\nto alerts that can lead to ignoring \n\nserious defects and incidents, \n\ncausing harm; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nObjective \n\nTo identify and control for Adversarial Risks based on and/or originating in Product trust and confidence. \n\n17.4 Confidence & Trust Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n70 \n\nFBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) prevent Model-based physical and/or irreparable harms; and (b) identify and mitigate risks due to Product \n\nfailures, including Model failures, IT failures, and process failures. \n\nWhat do we mean when we refer to Safety? \n\nWhen referring to safety in the context of machine learning we mean-\n\nSafety means the protection of the operational environment - and its subjects - from negative physical and/or \n\nother harms that might result from machine learning Products and/or Models, either directly or indirectly. \n\nPut slightly differently, when we discuss safety we are not talking about the safety of machine learning Products \n\nand Models (called, rather, security), but, instead, the operational environment within which machine learning \n\nProducts and Models operate. Specifically, the harms and risks that machine learning Products and Models might \n\ncause for these environments and their subjects. For example, an autonomous vehicle crashing and causing \n\ninjury, death, or destruction. \n\nWhy is Safety important? \n\nMachine learning Product and Model safety is imperative to ensure the integrity of the operational environment. \n\nWithout such safety, machine learning Products and Models can cause grave operational environment harms, \n\nsuch as physical injury or, at worst, death. These are intolerable risks as they undermine organisation, societal \n\nand machine learning trust and confidence. Moreover, they cause irreparable real damage in the real world. \n\nHow to apply Safety? \n\nIn order to generate thorough and thoughtful safety, it must be considered continuously throughout all stages of \n\nthe product lifecycle. This means that safety must be addressed at the (a) Product Definition(s), (b) Exploration, \n\n(c) Development and (d) Production stages of machine learning operations. \n\n# Section 18. Safety Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n71 \n\n18. Safety - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo establish the appropriate safety-oriented attitudes based on first principles and Product Definitions. \n\nObjective \n\nTo start the process of identifying,specifying and controlling (potential) risks and failures modes of the Model(s) \n\nand Product based on research and exploration, and sustain this process throughout the Product Lifecycle. \n\n18.1 Product Definitions \n\n18.2 Exploration \n\nControl:  Aim: \n\n18.1.1.  Physical and \n\nIrreparable \n\nHarm Risk \n\nDocument and assess whether any \n\nlikely failure modes can cause physical \n\nand/or irreparable harm, based on the \n\nProduct Definitions. If such is the case, \n\nwarrant increased oversight and attention \n\nthroughout the Product Lifecycle to risks \n\nand controls in general and from this section \n\nin particular. \n\nTo warrant the necessary amount of \n\ncontrol and resources throughout \n\nthe Product Lifecycle with regard \n\nto preventing and mitigating \n\nsignificant threats to individualsâ€™ \n\nphysical, financial, social, and \n\npsychological well being. \n\n18.1.2.  Domain-first \n\nHumble Culture \n\nDocument and establish tenents for Product \n\nTeam culture to promote risk awareness \n\nand prevent blind spots, inclusive of (a) put \n\nDomain expertise central; (b) never assume \n\nonly positive effects; (c) admit uncertainty \n\nwhen assessing impacts. \n\nTo promote risk awareness and \n\nprevent blindspots in analysing \n\nfailure modes and other safety \n\nrelated controls and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nControl:  Aim: \n\n18.2.1.  Forecast Failure \n\nModes \n\n(a) Document and assess continuously \n\nthroughout the Product Lifecycle all \n\npotential failure modes that can be \n\nidentified through - (i) researching past \n\nfailures; and/or (ii) interrogating all \n\ncomponents or product and context with \n\nan open mind; (b) rank identified failure \n\nmodes according to likelihood and severity; \n\n(c) prepare for and mitigate these risks as \n\nfar as is reasonably practical in order of risk \n\nthroughout the Product Lifecycle. \n\nTo (a) reduce harmful \n\nconsequences of failures through \n\nanticipation and preparation; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n18.2.2.  Prediction \n\nLimits \n\nDocument and assess with a diversity of \n\nStakeholders the real limitations on the \n\nProduct and Model Outcomes that ought be \n\nstrictly enforced in order to prevent physical \n\nand/or irreparable harm and/or other Failure \n\nModes. \n\nTo prevent Model and Product \n\nOutcomes from violating clear, \n\nfixed and safe operating bounds. \n\n18.2.3.  Surprise Diary  Document continuously throughout the \n\nProduct Lifecycle all surprising findings and \n\noccurrences. \n\nTo discover and subsequently \n\ncontrol for previously unknown or \n\nunanticipated failures modes. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n72 \n\n18. Safety - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo control Safety risks and failure modes based on testing and controlling Model and Product technical \n\ncomponents. \n\nObjective \n\nTo control for Safety risks and failure modes and prevent physical and/or irreparable harm by performing \n\nassessments and implementing measures at the systemic and organisational level. \n\n18.3 Development \n\n18.4 Production \n\nControl:  Aim: \n\n18.3.1.  General \n\nIT Testing \n\nPractices \n\nAdhere to all traditional IT/Software Testing \n\nbest practices. \n\nTo warrant and control the risk of \n\nfailures due to code, software and \n\nother IT mistakes in general. \n\n18.3.2.  Testing by \n\nDomain Experts \n\nDocument and perform testing of the \n\nModel(s) and Product by Domain experts. \n\nTo warrant that Product and \n\nProduct Safety are tested against \n\nthe most relevant requirements and \n\nprevent blind spots caused by lack \n\nof multidisciplinarity. \n\n18.3.3.  Algorithm \n\nBenchmarking \n\nDocument and perform benchmark testing \n\nof Models and Model code against well-\n\nknown, trusted and/or simpler Models/code. \n\nTo warrant the correct \n\nimplementation of Models and \n\ncode, and safeguard reproducibility \n\nin general. \n\nControl:  Aim: \n\n18.4.1.  System Failure \n\nPropagation \n\nDocument and assess how failures in \n\nModels and Product components propagate \n\nto other components and other systems, \n\nand what damage they may cause there. \n\nIncorporate such information in Failure \n\nMode risk assessments and implementation \n\nof Graceful Failures and Kill Switches. \n\nTo (a) prevent blind spots and \n\ncascading failures and (b) provide \n\nessential input for creating \n\nmitigation measures with a \n\nminimum of uncontrolled side-\n\neffects. \n\n18.4.2.  Graceful Failure  Document and assess whether (i) Model \n\nerrors, (ii) Model failures, (iii) Product \n\nfailures, (iv) IT failures, and/or (v) process \n\nand implementation failures - whether \n\ncaused by attack or not - can result in \n\nphysical or irreparable harm to humans, \n\nsociety and/or the environment. If present, \n\nmitigate these risks by implementing \n\ntechnological and/or process measures that \n\nmake these failures graceful. \n\nTo identify risks and mitigating \n\nmeasures throughout the Product \n\nLifecycle with regard to significant \n\nthreats to individualsâ€™ physical, \n\nfinancial, social and psychological \n\nwellbeing. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n73 \n\n18. Safety - FBPML Technical Best Practices v1.0.0. \n\n18.4.3.  Kill Switch  Document and implement Kill Switches \n\naccording to the findings of all previous \n\ncontrols, taking into account (a) instructions \n\nand procedures for engaging the Kill Switch; \n\n(b) who is/are responsible for engaging the \n\nKill Switch; (c) what impacts the engagement \n\nof the Kill Switch has on users, other parts of \n\nthe Product and other systems. \n\nDocument and implement Kill \n\nSwitches according to the findings \n\nof all previous controls, taking \n\ninto account (a) instructions and \n\nprocedures for engaging the Kill \n\nSwitch; (b) who is/are responsible \n\nfor engaging the Kill Switch; (c) \n\nwhat impacts the engagement of \n\nthe Kill Switch has on users, other \n\nparts of the Product and other \n\nsystems. \n\n18.4.4.  Safety Stress \n\nTesting \n\nDocument and perform scenario-based \n\nstress testing of Product in Domain, Society \n\nand Environmental contexts, for realistic but \n\nrare high-impact scenarios, recording the \n\nProductâ€™s reaction to and influence on the \n\nDomain, Society and Environment. \n\nTo control and prepare for worst-\n\ncase scenarios in the context \n\nof rapid and/or large changes \n\nin Domain, in Society or in \n\nEnvironment. \n\n18.4.5.  Product Incident \n\nResponse \n\nDocument and prepare Product-specific \n\nimplementation of the Organisation Incident \n\nResponse Plan insofar as that does not cover \n\nthe Productâ€™s specific risks, if appropriate. \n\nTo (a) control for and contain \n\nProduct Incidents; (b) minimize \n\nharms stemming from Product \n\nIncidents; (c) repair harms caused \n\nby Product Incidents; and (d) \n\nincorporate lessons learned. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n74 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure (a) building desirable solutions; (b) human control over Products and Models; and (c) that individuals \n\naffected by Product and Model outputs can obtain redress. \n\nWhat is Human-centric Design? \n\nHuman-centric (or also called human-centered) Design is a creative approach to problem-solving, by involving \n\nthe human perspective in all steps of problem solving. \n\nIn the context of machine learning and the Model framework, Human-centric Design makes you stay focused on \n\nthe user when designing with ML, therefore building desirable solutions for your target users. Moreover, it also \n\nensures that the right stakeholders are involved throughout the whole design and development process and helps \n\nto properly identify the right opportunity areas. Lastly, human-centric design encompasses the extent to which \n\nhumans have control over the Model and its output as well as the degree to which humans can obtain redress, if \n\nthey are affected by the Model. \n\nWhy Human-centric Design? \n\nIncorporating Human-centric Design in the Product is vital. It ensures the Model is not built in isolation but \n\nis integrated with other problems and, most of all, that it helps solve the right questions. Having the right \n\nStakeholders (beyond the technical teams) involved in the whole Model lifecycle translates to higher trust levels \n\nin the Model, increases the rate of adoption, as well as results in more human-friendly and creative solutions. Not \n\nhaving the human-centric part of the Model will inevitably result in an inferior Model - and one which very likely \n\nend up on a â€˜shelfâ€™ and, therefore, not be applied in practice. \n\nHow to ensure Human-centric Design? \n\nHuman-centric Design is something that needs to be addressed throughout the product lifecycle, not only in the \n\nearly stages of it, and not in any stage in isolation. \n\n# Section 19. Human-Centred Design Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n75 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo discover and gain insight so that the Product and Model(s) will solve the right problems, designed for human \n\nneeds and values, before building it. \n\nObjective \n\nTo (a) cluster, (b) find insights and (c) define the right opportunity area, ensuring to focus on the right questions to \n\nsolve in preparation for the development and production phase. \n\n19.1 Product Definitions \n\n19.2 Exploration \n\nControl:  Aim: \n\n19.1.1.  Human Centered \n\nMachine \n\nLearning \n\nIncorporate the human (non-technical) \n\nperspective in your (technical) process of \n\nexploration, development and production \n\nby applying user research, design thinking, \n\nprototyping and rapid feedback, and human \n\nfactors when defining a usable product or \n\nmodel. \n\nTo (a) ensure that Product(s) and \n\nModel(s) are not only feasible and \n\nviable, but also align with a human \n\nneeds; and (b) highlight associated \n\nrisks failing such. \n\n19.1.2.  UX (or user) \n\nresearch \n\nFocus on understanding user behaviours, \n\nneeds, and motivations through observation \n\ntechniques, task analysis, user interviews, \n\nand other research methodologies. \n\nTo prevent (a) a focus on technology \n\nfrom overshadowing a focus on \n\nproblem solving; and (b) cognitive \n\nbiases from adverse influence \n\nProduct and Model design. \n\n19.1.3.  Design for \n\nHuman values \n\nInclude activities for (a) the identification \n\nof societal values, (b) deciding on a moral \n\ndeliberation approach (e.g. through \n\nalgorithms, user control or regulation), and \n\n(c) methods to link values to formal system \n\nrequirements (e.g. value sensitive design \n\n(VSD) mapping). \n\nTo reflect societal concerns about \n\nthe ethics of AI, and ensure that AI \n\nsystems are developed responsibly, \n\nincorporating social, ethical values \n\nand ensuring that systems will \n\nuphold human values. The moral \n\nquality of a technology depends on \n\nits consequences. \n\nControl:  Aim: \n\n19.2.1.  Design Thinking  Ensure an iterative development process by \n\n(a) empathize: research your usersâ€™ needs, \n\n(b) define: state your usersâ€™ most important \n\nneeds and problems to solve, (c) ideate: \n\nchallenge assumptions and create ideas, (d) \n\nprototype: start to create solutions and (e) \n\ntest: gather user feedback early and often. \n\nTo let data scientists organise and \n\nstrategise their next steps in the \n\nexploratory phase. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n76 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\n19.2.2.  Ethical \n\nassessment \n\nDiscuss with your team to what extend \n\n(a) the AI product actively or passively \n\ndiscriminates against groups of people \n\nin a harmful way; (b) everyone involved in \n\nthe development and use of the AI product \n\nunderstands, accepts and is able to \n\nexercise their rights and responsibilities; \n\n(c) the intended users of an AI product \n\ncan meaningfully understand the purpose \n\nof the product, how it works, and (where \n\napplicable) how specific decisions were \n\nmade. \n\nDiscuss with your team to what \n\nextend (a) the AI product actively \n\nor passively discriminates against \n\ngroups of people in a harmful \n\nway; (b) everyone involved in the \n\ndevelopment and use of the AI \n\nproduct understands, accepts \n\nand is able to exercise their \n\nrights and responsibilities; (c) the \n\nintended users of an AI product \n\ncan meaningfully understand the \n\npurpose of the product, how it \n\nworks, and (where applicable) how \n\nspecific decisions were made. \n\n19.2.3.  Estimating the \n\nvalue vs effort \n\nof possible \n\nopportunity \n\nareas \n\nExplore the details of what mental Models \n\nand expectations people might bring when \n\ninteracting with an ML system as well as \n\nwhat data would be needed for that system. \n\nE.g. an Impact Matrix. \n\nTo reveal the automatic \n\nassumptions people will bring to an \n\nML-powered product, to be used \n\nas prompts for a product team \n\ndiscussion or as stimuli in user \n\nresearch. (See also Section 4.11 -\n\nUser Experience Mapping.) \n\nObjective \n\nTo (a) ensure rapid iteration and targeted feedback from relevant Stakeholders, allowing a larger range of \n\npossible solutions to be considered in the selection process. (b) Increase the creativity and options considered, \n\nwhile avoiding avoiding personal biases and/or pigeon-holing a solution. \n\n19.3 Development \n\nControl:  Aim: \n\n19.3.1.  Prototyping  1: Focus on quick and minimum viable \n\nprototypes that offer enough tangibility \n\nto find out whether they solve the initial \n\nproblem or answer the initial question. \n\nDocument how test participants react and \n\nwhat assumptions they make when they \n\nâ€œuseâ€ your mockup. \n\n2: Design a so-called â€˜Wizard of Ozâ€™ test; have \n\nparticipants interact with what they believe \n\nto be an autonomous system, but which is \n\nactually being controlled by a human (usually \n\na team member) \n\nTo gain early feedback (without \n\nhaving to actually have build an \n\nML product) needed to (a) adjust \n\nor pivot your Products(s) and/or \n\nModel(s) thus ensuring business \n\nviability; and/or (b) assess the cost \n\nand benefits of potential features \n\nwith more validity than using \n\ndummy examples or conceptual \n\ndescriptions. \n\n19.3.2.  Cost weighing of \n\nfalse positives \n\nand false \n\nnegatives \n\nWhile all errors are equal to an ML system, \n\nnot all errors are equal to all people. Discuss \n\nwith your team how mistakes of your ML \n\nmodel might affect the userâ€™s experience of \n\nthe product. \n\nto avoid sensitive decisions being \n\ntaken (a) autonomously; or (b) \n\nwithout human consideration. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n77 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\n19.3.3.  Visual \n\nStorytelling \n\nFocus on explanatory analysis over \n\nexploratory analysis, taking the mental \n\nmodels of your target audience in account. \n\nTo avoid uninformed decisions \n\nabout your product or model by \n\nnon-technical stakeholders, when \n\npresenting complex analysis, \n\nmodels, and findings. \n\n19.3.4.  Preventative \n\nProcess Design \n\nDocument and assess whether high-risk \n\nand/or high-impact Model (sub)problems \n\nor dilemmas that are present in the Product \n\n(as determined from following the Best \n\nPractices Framework) can be mitigated or \n\navoided by applying non-Model process \n\nand implementation solutions. If non-Model \n\nsolutions are not applied, document the \n\nreasons for this, document the sustained \n\npresence of these risks and implement \n\nappropriate incident response measures. \n\nTo (a) prevent high-risk and/or \n\nhigh-impact Model (sub)problems \n\nor dilemmas through non-Model \n\nprocess and implementation \n\nsolutions; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nObjective \n\nTo ensure (a) delivering a user-friendly product, (b) increasing the adoption rate of your product or model, \n\nfocussing on (dis-)trust as main fundamental risk of ML models with (non-technical) end users \n\n19.4 Production \n\nControl:  Aim: \n\n19.4.1.  Trust; increased \n\nby design \n\nAllow for users to develop systems \n\nheuristics (ease of use) via design patterns \n\nwhile at the same time facilitate a detailed \n\nunderstanding to those who value the \n\nâ€˜intelligentâ€™ technology used. (See Section \n\n19.4.2 -Design for Human Error; Section \n\n19.4.3 - Algorithmic transparency; and \n\nSection 19.4.4 - Progressive disclosure for \n\nfurther information.) \n\nTo avoid (a) that the user does not \n\ntrust the outcome, and will act \n\ncounter to the design, causing at \n\nbest inefficiencies and at worst \n\nserious harms, or (b) that -trusting \n\nan application will do what we think \n\nit will do- an user can confirm their \n\ntrust is justified. \n\n19.4.2.  Design for \n\nHuman Error \n\n(a) Understand the causes of error and \n\ndesign to minimise those causes; (b) \n\nDo sensibility checks. Does the action \n\npass the â€œcommon senseâ€ test (e.g. is the \n\nnumber is correct? - 10.000g or 10.000kg) \n\n(c) Make it possible to reverse actions - to \n\nâ€œundoâ€ them - or make it harder to do what \n\ncannot be reversed (eg. add constraints \n\nto block errors - either change the color \n\nto red or mention â€œDo you want to delete \n\nthis file? Are you sure?â€). (d) make it easier \n\nfor people to discover the errors that do \n\noccur, and make them easier to correct \n\nTo (a) increase trust between the \n\nend user and the model; (b) minimize \n\nthe opportunities for errors while \n\nalso mitigating the consequences. \n\nIncrease the trust users have with \n\nyour product by design for deliberate \n\nmis-use of your model (making your \n\nmodel or product â€œidiot-proofâ€) so \n\nusers are (a) able to insert data to \n\ncompare the model outcome with \n\ntheir own expected outcome which \n\nwill increase their trust, or (b) users \n\nable to test the limitations of your \n\nproduct or model -via fake or highly \n\nunlikely data- without breaking your \n\nproduct or model. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n78 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\n19.4.3.  Algorithmic \n\ntransparency \n\nAssess the appropriate system heuristics \n\n(eg. ease of use), document all factors that \n\ninfluence the algorithmic decisions, and \n\nuse them as a design tool to make them \n\nvisible, or transparent, to users who use or \n\nare affected by the ML systems. \n\nTo (a) increase trust between the end \n\nuser and the model; (b) increase end-\n\nuser control; (c) improve acceptance \n\nrate of tool; (d) promote user \n\nlearning with complex data; and (e) \n\nenable oversight by developers. \n\n19.4.4.  Progressive \n\ndisclosure \n\nAt the point where the end-user interacts \n\nwith the Product outcomes, show them \n\nonly the initial features and/or information \n\nnecessary at that point in the interaction \n\n(thus initially hiding more advanced \n\ninterface controls). Show the secondary \n\nfeatures and/or information only when the \n\nuser requests it (show less, provide more-\n\nprinciple). \n\nTo greatly reduce unwanted \n\ncomplexity for the end-user and thus \n\npreventing (a) end-user non-adoption \n\nor misunderstanding and (b) ensuring \n\nan increased feeling of trust by the \n\nusers. \n\n19.4.5.  Human in the \n\nloop (HITL) \n\nEmbed human interaction with machine \n\nlearning systems to be able to label \n\nor correct inaccuracies in machine \n\npredictions. \n\nTo avoid the risk of the Product \n\napplying a materially detrimental or \n\ncatastrophic Product Outcome to \n\na Product Subject without human \n\nintervention. \n\n19.4.6.  Remediation  Document, assess and implement in \n\nthe Model(s), Product and Organization \n\nprocesses, requirements for enabling \n\nProduct Subjects to challenge and obtain \n\nredress for Product Outcomes applied to \n\nthem. \n\nTo ensure detrimental Product \n\nOutcomes are easily reverted when \n\nappropriate. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n79 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo prevent (in)direct adverse social and environmental effects as a consequence of interactions amongst \n\nProducts, Models, the Organisation, and the Public. \n\nWhat is Systemic Stability? \n\nModel stability is a relatively popular notion. It is usually centered at putting a bound at the Modelâ€™s generalization \n\nerror. \n\nSystemic Stability refers to the robustness of the Model (or lack thereof) stemming from the interaction between \n\nthe Model, Organization, environment and the broader public (society at large). \n\nThere are numerous potential risks that can emerge in this interaction. Many of them can impact the stability of \n\nthe Model - beyond the context of traditional performance robustness or deterioration of the Model over time. \n\nAnother way to think of it is as the extent to which the Model and/or its building blocks are susceptible to chain \n\neffects and self-reinforcing interactions between the Model, Organization, environment and society. \n\nWhy Systemic stability? \n\nSystemic stability forces one to think beyond the traditional definitions of Model stability and its potential \n\ncauses and consequences. Systemic stability ensures that we consider the effect on the Model and society \n\ndue to the interaction between the Model, the Organization, the environment and society. This means thinking \n\nabout susceptibility to feedback loops, self-fulfilling prophecies and how either of them may impact the data \n\nor the Model and its output. It, therefore, reduces risks related to deteriorated performance and minimises the \n\npropagation of undesirable biases. \n\nHow to ensure Systemic stability? \n\nIn order to ensure systemic stability, it must be considered continuously throughout all stages of the product \n\nlifecycle. This means that systemic stability must be addressed at the (a) Product Definition(s), (b) Exploration, (c) \n\nDevelopment and (d) Production stages of machine learning operations. \n\n# Section 20. System Stability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n80 \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct Definition(s). \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct exploration. \n\n20.1 Product Definitions \n\n20.2 Exploration \n\nControl:  Aim: \n\n20.1.1.  Product \n\nAssumption \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in invalidating \n\nProduct Assumptions. If so, attempt to \n\nredefine Product Assumptions to warrant \n\ntheir longevity. \n\nTo (a) prevent unpredictable social \n\nand/or environmental behaviour \n\nthrough Product Outcomes; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. \n\nControl:  Aim: \n\n20.2.1.  Selection \n\nFunction \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in invalidating \n\nProduct Assumptions. If so, attempt to \n\nredefine Product Assumptions to warrant \n\ntheir longevity. \n\nTo (a) prevent unpredictable social \n\nand/or environmental behaviour \n\nthrough Product Outcomes; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.2.2.  Data Definition \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes \n\nto the Selection Function, and whether \n\nthis is a self-reinforcing interaction. If \n\ntrue, attempt to mitigate or stabilize \n\nassociated effects through refining \n\nProduct Definition(s) and/or improving \n\nModel design and/or Product and process \n\nimplementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n81 \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. \n\n20.2.3.  Data Generating \n\nProcess \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes to \n\nthe Product data definitions, and whether \n\nthis is a self-reinforcing interaction. If \n\ntrue, attempt to mitigate or stabilize \n\nassociated effects through refining \n\nProduct Definition(s) and/or improving \n\nModel design and/or Product and process \n\nimplementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.2.4.  Data \n\nDistributions \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes to \n\nthe data generating process, and whether \n\nthis is a self-reinforcing interaction. If \n\ntrue, attempt to mitigate or stabilize \n\nassociated effects through refining \n\nProduct Definition(s) and/or improving \n\nModel design and/or Product and process \n\nimplementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.2.5.  Hidden Variable \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in the creation \n\nof new hidden Variables in the system. If \n\ntrue, record the new Variable during data \n\ngathering, or prevent the creation of the \n\nnew Variable through improved Product \n\nDefinition(s) and implementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n82 \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct development. \n\n20.3 Development \n\nControl:  Aim: \n\n20.3.1.  Target Feature \n\nDefinition \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes to the \n\nTarget Feature definition. If true, attempt to \n\nmitigate associated effects through refining \n\nProduct Output and/or Model design and/or \n\ndevelopment. \n\nTo (a) determine and prevent risk of \n\nunpredictable behaviour once the \n\nProduct outcomes are applied; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.3.2.  Optimization \n\nFeedback Loop \n\nSusceptibility \n\nDocument and assess whether the cost \n\nfunction and/or optimization algorithm \n\nexhibits a feedback loop behaviour that \n\nincludes the gathering of data that has been \n\ninfluenced by previous Model iterations, and \n\nwhether this behaviour is self-reinforcing \n\nor self-limiting. If true, attempt to mitigate \n\nassociated effects through refining \n\nProduct Output and/or Model design and/or \n\ndevelopment. \n\nIdem Section 20.2.1- Selection \n\nFunction Susceptibility \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct application. \n\n20.4 Production \n\nControl:  Aim: \n\n20.4.1.  Self-fulfilling \n\nProphecies \n\nDocument and assess whether applying \n\nProduct Outputs will result in change to \n\nProduct inputs, dependencies and/or \n\nDomain(s) (other than those mentioned in \n\ncontrols elsewhere) and whether this is a \n\nself-reinforcing interaction. If true, attempt \n\nto mitigate associated effects through \n\nrefining Product Output and/or Model design \n\nand/or development. \n\nIdem Section 20.2.1- Selection \n\nFunction Susceptibility \n\n20.4.2.  Hidden Variable \n\nDependencies \n\nDocument and assess whether the effect \n\nof applying Product Outputs depends \n\non Hidden Variables. If true, control for \n\nHidden Variables, for example through \n\nmarginalization and/or by deriving indicators \n\nfor Hidden Variables influence. \n\nTo (a) prevent diverging effects on \n\nseemingly similar individuals or \n\ndatapoints; (b) prevent or detect \n\nhigh-risk interactions; and (c) \n\nhighlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n83 \n\n20.4.3.  Society \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs results in potentially \n\nharmful societal or environmental changes, \n\nand research the possible knock-on effects \n\nas far as reasonably practical. \n\nTo (a) identify and prevent both \n\ndirect and indirect adverse effects \n\non society and the environment; \n\n(b) determine if there is a risk of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.4.4.  Domain \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs results in changes to \n\napplication Domain(s), and research \n\nthe possible knock-on effects as far as \n\nreasonably practical. \n\nTo (a) identify and prevent both \n\ndirect and indirect adverse \n\neffects on Product Domain(s); \n\n(b) determine if there is a risk of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.4.5.  Other \n\nOrganisation \n\nProducts \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs result in changes to inputs, \n\ndependencies and/or context for other \n\nOrganisation Products. If true, attempt to \n\nmitigate associated effects through refining \n\nProduct Output and/or Model design and/or \n\ndevelopment. \n\nTo (a) identify and prevent both \n\ndirect and indirect adverse \n\neffects on the Organisation or \n\nother Organisation Products; (b) \n\ndetermine if there is a risk of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n84 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure the clear and complete Traceability of Products, Models and their assets (inclusive of, amongst other \n\nthings, data, code, artifacts, output, and documentation) for as long as is reasonably practical. \n\nWhat do we mean when we refer to Product Traceability? \n\nProduct Traceability refers to the ability to identify, track and trace elements of the Product as it is designed, \n\ndeveloped, and implemented. \n\nAlternatively put, Product Traceability, is the ability to trace and track all Product elements and decisions \n\nthroughout the Product Lifecyle. It is the identification, indexing, storage, and management of each unique \n\nProduct element. \n\nWhy is Product Traceability relevant? \n\nThrough Product Traceability, each element of the Product can be easily identified and, thereafter, re-examined, \n\nand amended. This allows for greater Product accountability and transparency as, through this process, each \n\nProduct element and its developers can be identified. \n\nHow to apply Product Traceability? \n\nIn order to generate thorough and thoughtful Product Traceability, it must be considered continuously throughout \n\nall stages of the Product Lifecycle. This means that Product Traceability must be addressed at the (a) Product \n\nDefinition(s), (b) Exploration, (c) Development and (d) Production stages of Machine Learning Operations. \n\n# Section 21. Product Traceability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n85 \n\n21. Product Traceability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo document and maintain an overview of the requirements necessary to complete the Product and the \n\ninterdependencies in the Product design phase. \n\nObjective \n\nTo document the impact analysis of each requirement. \n\n21.1 Product Definitions \n\n21.2 Exploration \n\nControl:  Aim: \n\n21.3.1.  Document \n\nStorage \n\nDefine a single fixed storage solution for all \n\nreports, documents, and other traceability \n\nfiles. \n\nTo (a) prevent the usage and \n\ndissemination of outdated and/ \n\nor incorrect files; (b) prevent the \n\nhaphazards storage of Product \n\nreports, documents and/or files; \n\nand (c) highlight associated risks in \n\nthe Product Lifecycle. \n\n21.3.2.  Version Control \n\nof Documents \n\nEnsure that document changes are tracked \n\nwhen changes are made. Subsequent \n\nversions ought to list version number, \n\nauthor, date of change, and short \n\ndescription of the changes made. \n\nTo (a) track changes to any and all \n\ndocuments; (b) ensure everyone \n\nis using the same and latest \n\ndocument version; and (c) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.3.  Architectural \n\nRequirements \n\nDocument \n\nDocument which information technology \n\nresources are necessary for each element \n\nof the Product to provide a necessary \n\noverview of system requirements and \n\ncost distribution. Document the reasons \n\neach resource was chosen along with \n\njustifications. \n\nTo (a) provide clear documentation \n\nof which system resources are \n\nused, where they are used, why \n\nthey are used, and costs; and (b) \n\nhighlight associated risks in the \n\nProduct Lifecycle. \n\nControl:  Aim: \n\n21.2.1.  Document \n\nImpact Analysis \n\nof Requirements \n\nDocument and complete an impact analysis \n\non the resources and design of the Product \n\nthat can result in technical debt. \n\nTo (a) avoid Product failures due \n\nto unresolved technical debt by \n\ndocumenting potential sources of \n\nfriction and the solutions; and (b) \n\nhighlight associated risks in the \n\nProduct Lifecycle. \n\n21.2.2.  Resource \n\nTraceability \n\nMatrix \n\nProvide and keep up to date a clear view of \n\nthe relationships and interdependencies \n\nbetween resources in a documented matrix. \n\nTo (a) document and show resource \n\ncoverage for each use case; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n86 \n\n21. Product Traceability - FBPML Technical Best Practices v1.0.0. \n\n21.2.3.  Design \n\nTraceability \n\nMatrix \n\nProvide and keep up to date a clear view of \n\nthe relationships and interdependencies \n\nbetween designs and interactions thereof in \n\na documented matrix. \n\nTo (a) document design and \n\nexecution status; (b) clearly trace \n\ncurrent work and what can be \n\npursued next; and (c) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.2.4.  Results \n\nReproducibility \n\nLogs \n\nThroughout the entire Product Lifecycle, \n\nwhenever a Product component - inclusive \n\nof Models, experiments, analyses, \n\ntransformation, and evaluations - are run, all \n\nparameters, hyperparameters and results \n\nought to be logged and/or tracked, including \n\nunique identifier(s) for runs, artifacts, code \n\nand environments. \n\nTo (a) enable Absolute \n\nReproducibility; (b) validate Models \n\nand Outcomes through enablement \n\nof analysis of logs, run comparisons \n\nand reproducibility. \n\nObjective \n\nTo document and maintain the status of each product and the testing results. Ensure 100% test coverage. \n\nPrevent inconsistencies between project elements and prevent feature creep. \n\n21.3 Development \n\nControl:  Aim: \n\n21.3.1.  Backlog  Ensure that an effective backlog is \n\nmaintained to track work items and serve as \n\na historical representation and timeline of \n\ncompleted features and velocity. \n\nTo (a) ensure a comprehensive \n\nbreakdown of Features and \n\ntasks necessary to achieve full \n\nproduct functionality; (b) provide \n\nhighly readable coarse-grained \n\nversioning; and (c) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.2.  Documentation \n\nfor Technical \n\nContributors \n\nMaintain technical documentation that \n\nenables all current and future contributors \n\nto efficaciously and safely develop and \n\nmaintain the Product, including such \n\ninformation as description of each file, the \n\nworkflow, author, environments, accrued \n\ntechnical debt. \n\nTo (a) maintain Product technical \n\nintegrity by ensuring safe \n\ncontribution and maintenance \n\npractices; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.3.  Version Control \n\nof Code \n\nMaintain uninterrupted version control \n\nsystems and practices of all code used by, in \n\nand during the Product and its Lifecycle. \n\nTo (a) maintain Product technical \n\nintegrity by ensuring safe \n\ncontribution and maintenance \n\npractices; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n87 \n\n21.3.4.  Docstrings and \n\nCode Comments \n\nDocument in each function the author of \n\ncode, purpose of code, input, Output, and \n\nimprovements to be made. Document the \n\nsource of inputs and potentially a short \n\nbusiness description of data used. \n\nTo (a) ensure Model clarity as to \n\ntechnical progress; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.5.  Project Status \n\nReports \n\nEnsure that all status reports and similar \n\ncommunications to Management and \n\nStakeholders are stored and maintained, \n\ninclusive of team updates, reports to the \n\nProduct Manager, and Stakeholder reports \n\nby request. \n\nTo (a) maintain a formal written \n\nrecord of decisions, progress and \n\ncontext evolution; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21. Product Traceability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo document the observed impact of updates to the product. Document product runs and their input for \n\nreproducibility. \n\n21.4 Production \n\nControl:  Aim: \n\n21.4.1.  Version control \n\nthrough CI/CD \n\nMaintain distinct production versions \n\nto easily revert or roll back to a working \n\nprevious Product, if production issues arise. \n\nProperly set up CI/CD enables easy redeploy \n\nof any artifact and version. \n\nTo (a) provide functional Product \n\nto users at all times; (b) seamlessly \n\nredeploy Product versions if \n\nneeded; and (c) highlight associated \n\nrisks in the Product Lifecycle. \n\n21.4.2.  Data Lineage \n\nManifest \n\nUtilise a data lake for production data, \n\nintermediate results, and end results. Each \n\nstep should be documented in a manifest \n\nthat is passed from one step of the process \n\nto the next and always accompanies stored \n\ndata and results. \n\nTo (a) create a structured way for \n\ntracing where data has been, what \n\nwas done to it, and results; and (b) \n\nhighlight associated risks in the \n\nProduct Lifecycle.", "fetched_at_utc": "2026-02-08T18:50:47Z", "sha256": "d95e143273c46729361d8e462f63055d85030182d8f6e0d3a1fc96c16703e2b4", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-63-87.pdf", "file_size": 1007448, "relative_path": "pdfs\\FBPML_TechnicalBP_V1.0.0-63-87.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 14691}}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-7-9-a2d2073e84c2", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-7-9.pdf", "title": "FBPML_TechnicalBP_V1.0.0-7-9", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n7FBPML Technical Best Practices v1.0.0. \n\n1.1.  Absolute Reproducibility  means a guarantee that any and all results, outputs, outcomes, artifacts, \n\netc can be exactly reproduced under any circumstances. \n\n1.2.  Best Practice Guideline  means this document. \n\n1.3.  Confidence Value  means a measure of a Modelâ€™s self-reported certainty that the given Output \n\nis correct. \n\n1.4.  Data Generating \n\nProcess \n\nmeans the process, through physical and digital means, by which Records \n\nof data are created (usually representing events, objects or persons). \n\n1.5.  Data Science  means an interdisciplinary field that uses scientific methods, processes, \n\nalgorithms and computational systems to extract knowledge and insights \n\nfrom structural and/or unstructured data. \n\n1.6.  Domain  means the societal and/or commercial environment within which the \n\nProduct will be and/or is operationalised. \n\n1.7.  Edge Case  means an outlier in the space of both input Features and Model Outputs. \n\n1.8.  Error Rate  means the frequency of occurrence of errors in the (Sub)population \n\nrelative to the size of the (Sub)population \n\n1.9.  Evaluation Error  means the difference between the ground truth and a Modelâ€™s prediction or \n\noutput. \n\n1.10.  Fairness & Non-\n\nDiscrimination \n\nmeans the property of Models and Model outcomes to be free from bias \n\nagainst Protected Classes. \n\n1.11.  Features  mean the different attributes of datapoints as recorded in the data. \n\n1.12.  Hidden Variable  means an attribute of a datapoint or an attribute of a system that \n\nhas a causal relation to other attributes, but is itself not measured or \n\nunmeasurable. \n\n1.13.  Human-Centric Design \n\n& Redress \n\nmeans orienting Products and/or Models to focus on humans and their \n\nenvironments through promoting human and/or environment centric \n\nvalues and allowing for redress. \n\n1.14.  Implementation  means every aspect of the Product and Model(s) insertion of and/or \n\napplication to Organisation systems, infrastructure, processes and culture \n\nand Domains and Society. \n\n1.15.  Incident  means the occurrence of a technical event that affects the integrity of a \n\nProduct and/or Model. \n\n1.16.  Label  means the Feature that represents the (supposed) ground-truth values \n\ncorresponding to the Target Variable. \n\nAs used in this Best Practice Guideline, the following terms shall have the following meanings where capitalised. \n\nAll references to the singular shall include references to the plural, where applicable, and vice versa. Any terms \n\nnot defined or capitalised in this Best Practice Guideline shall hold their plain text meaning as cited in English and \n\ndata science. \n\n# Section 1. Definitions Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n8FBPML Technical Best Practices v1.0.0. \n\n1. Definitions - FBPML Technical Best Practices v1.0.0. \n\n1.17.  Machine Learning  means the use and development of computer systems and Models that \n\nare able to learn and adapt with minimal explicit human instructions by \n\nusing algorithms and statistical modelling to analyse, draw inferences, and \n\nderive outputs from data. \n\n1.18.  Model  means Machine Learning algorithms and data processing designed, \n\ndeveloped, trained and implemented to achieve set outputs, inclusive of \n\ndatasets used for said purposes unless otherwise stated. \n\n1.19.  Organisation  means the concerned juristic entity designing, developing and/or \n\nimplementing Machine Learning. \n\n1.20.  Outcome  means the resultant effect of applying Models and/or Products. \n\n1.21.  Output  means that which Models produce, typically (but not exclusively) \n\npredictions or decisions. \n\n1.22.  Performance \n\nRobustness \n\nmeans the propensity of Products and/or Models to retain their desired \n\nperformance over diverse and wide operational conditions. \n\n1.23.  Product  means the collective and broad process of design, development, \n\nimplementation and operationalisation of Models, and associated \n\nprocesses, to execute and achieve Product Definition(s), inclusive of, \n\namongst other things, the integration of such operations and/or Models \n\ninto organisation products, software and/or systems. \n\n1.24.  Product Manager  means either a Design Owner and/or Run Owner as identified in the \n\nOrganisation Best Practice Guideline in Sections 3.1.4. & 3.1.7. respectively. \n\n1.25.  Product Team  means the collective group of Organisation employees directly charged \n\nwith designing, developing and/or implementing the Product. \n\n1.26.  Product Subjects  means the entities and/or objects that are represented as data points in \n\ndatasets and/or Models, and who may be the subject of Product and/or \n\nModel outcomes. \n\n1.27.  Project Lifecycle  means the collective phases of Products from initiation to termination \n\n- such as design, exploration, experimentation, development, \n\nimplementation, operationalisation, and decommissioning - and their \n\nmutual iterations. \n\n1.28.  Protected Classes  mean (Sub)populations of Product Subjects, typically persons, that are \n\nprotected by law, regulation, policy or based on Product Definition(s) \n\n1.29.  Root Cause Analysis  means the activity and/or report of the investigation into the primary \n\ncausal reasons for the existence of some behaviour (usually an error or \n\ndeviation). \n\n1.30.  Safety  means real Product Domain based physical harms that result through \n\nProducts and/or Models applications. \n\n1.31.  Security  means the resilience of Products and/or Models against malicious and/ \n\nor negligent activities that result in Organisational loss of control over \n\nconcerned Products and/or Models. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n9FBPML Technical Best Practices v1.0.0. \n\n1. Definitions - FBPML Technical Best Practices v1.0.0. \n\n1.32.  Selection Function  means a (where possible mathematical) description of the probability or \n\nproportion of all real Subjects that might potentially be recorded in the \n\ndataset that are actually recorded in a dataset. \n\n1.33.  Stakeholders  mean the department(s) and/or team(s) within the Organisation who do \n\nnot conduct data science and/or technical Machine Learning, but have a \n\nmaterial interest in Product Machine Learning. \n\n1.34.  (Sub)population  means any group of persons, animals, or any other entities represented \n\nby a piece of data , that is part of a larger (potential) dataset and \n\ncharacterized by any (combination of) attributes. The importance of (Sub) \n\npopulations is particularly high when some (Sub)populations are vulnerable \n\nor protected (Protected Classes). \n\n1.35.  Systemic Stability  means the stability of Organisation, Domain, society and environment as a \n\ncollective ecosystem. \n\n1.36.  Target of Interest  means the fundamental concept that the Product is truly interested in \n\nwhen all is said and done, even if it is something that is not (objectively) \n\nmeasureable. \n\n1.37.  Target Variable  means the Variable which a Model is made to predict and/or output. \n\n1.38.  Traceability  means the ability to trace, recount, and reproduce Product outcomes, \n\nreports, intermediate products, and other artifacts, inclusive of Models, \n\ndatasets and codebases. \n\n1.39.  Variables  mean the different attributes of subjects or systems which may or may not \n\nbe measured.", "fetched_at_utc": "2026-02-08T18:50:48Z", "sha256": "a2d2073e84c287ca7fc7f721b93d8bf9a5d57a19d9762573a085ef82c292fe7a", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-7-9.pdf", "file_size": 115002, "relative_path": "pdfs\\FBPML_TechnicalBP_V1.0.0-7-9.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 1809}}}
{"doc_id": "pdf-pdfs-general-purpose-ai-model-compliance-guide-part-1-oliver-patel-8739a1b95698", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel.pdf", "title": "General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1) \n\nhttps://oliverpatel.substack.com/p/general-purpose-ai-model-compliance  2/22 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! \n\nFollow me on LinkedIn for more frequent updates. It has been a consequential few weeks for the EU AI Act's provisions on general-purpose AI (GPAI) models. Now that we've all had some time to (hopefully) take a deep breath, relax, and take stock, it is the ideal moment for a deep-dive on GPAI model compliance. The next three editions of Enterprise AI Governance will provide a comprehensive, practical, and actionable GPAI Model Compliance Guide. It is targeted at AI governance, legal, and compliance practitioners working for firms that develop, fine-tune, modify, deploy, or otherwise use GPAI models. That is quickly encompassing all of us.   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 3/22\n\nMy goal is to simplify these complex obligations and highlight what they mean for your business. \n\nNote: this series assumes familiarity with the EU AI Act, its core concepts, and the topic of general-purpose AI and foundation models more broadly. \n\nPart 1's focus (this edition) is on Obligations for Providers of GPAI Models . It covers: \n\nâœ… What is a GPAI model? \n\nâœ… What do providers of GPAI models need to do? \n\nâœ… What about open-source GPAI models? \n\nâœ… What about legacy GPAI models (released before 2 August 2025)? \n\nâœ… How does the GPAI Code of Practice fit in? \n\nâœ… How and when will the GPAI model provisions be enforced? And here is a glimpse of what's coming in parts 2 and 3. Part 2 will focus on GPAI Models with Systemic Risk , covering: \n\nâœ… What is a GPAI model with systemic risk? \n\nâœ… How are such models classified and what are the exemptions? \n\nâœ… What do providers of GPAI models with systemic risk need to do?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 4/22\n\nâœ… How do the compliance obligations differ? \n\nâœ… Deep dive on the GPAI Code of Practice: Safety and Security Chapter And Part 3 wraps up the series with the question on everyone's lips: am I a GPAI model provider!? The focus here will be on 'Downstream Actors': Modification, Deployment, and Use of GPAI Models :\n\nâœ… Who exactly is a provider of a GPAI model? \n\nâœ… How do you become the provider of a GPAI model you are modifying? \n\nâœ… What is a 'downstream provider' and how do you become one? \n\nâœ… What is a general-purpose AI system? \n\nâœ… What if you integrate a GPAI model into a high-risk AI system? \n\nâœ… How should you assess GPAI model providers and choose which models to use? By the end of the series, you will have a thorough understanding of precisely what your organisation needs to do to achieve compliance. My analysis represents a simplified breakdown of the following official legal and regulatory documents, which are referenced throughout. \n\nEU AI Act full text European Commission Guidelines for Providers of GPAI Models   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 5/22\n\nGPAI Code of Practice \n\nTransparency Chapter Model Documentation Form Copyright Chapter Safety and Security Chapter \n\nTemplate for publishing GPAI Model Training Data Summary \n\nThe aforementioned Code of Practice, Guidelines, and Training Data Template were all published in July 2025. And the Code of Practice was formally approved by the EU on 1 August 2025, one day before the GPAI model provisions became applicable on 2 August 2025. Whether or not you are a provider of a GPAI model, and thus subject to the below obligations, depends on whether or not the model you are developing and placing on the market is in fact a GPAI model. The AI Act defines a GPAI model as: \n\nWhat is a GPAI model?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 6/22\n\n\"An AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale , that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications , except AI models that are used for research, development or prototyping activities before they are placed on the market\" .The corresponding AI Act recital says that models with at least 1 billion parameters, trained in this way, should be considered to be GPAI models. In its recent guidelines, the European Commission acknowledged that the above definition is fairly general and lacks specific, objective criteria that organisations can use to determine whether or not their models constitute GPAI. Plugging this gap, the Commission has now provided specific criteria for GPAI model classification. This criteria is based on the amount of computational resource used to train the model. Specifically, if a model's training compute is greater than 10^23 floating-point operations (FLOP) and it can generate language (either text or audio), or generate image or video based on text inputs, then it should be considered a GPAI model.   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 7/22\n\nThe Commission's position is that there is a direct correlation between how much computational resource is used to train the modelâ€”which in itself is linked to the size of the model and the volume of training and pre-training dataâ€”and the general-purpose capabilities of the model. However, it is acknowledged that there could be exceptional instances where a model trained with this amount of computational resource that generates text would not meet the legal definition of a GPAI model (e.g., if it did not display significant generality in its capabilities). Also, a model which does not meet this indicative criteria may also meet the legal definition of a GPAI model. The key message is that organisations developing and fine-tuning large AI models at scale need to track the computational resource they use in a standardised and automated way. Providers of GPAI models are organisations or entities that develop such AI models and place them on the market or otherwise make them available. \n\nWhat do providers of GPAI models need to do?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 8/22\n\nPart 3 of this series will cover exactly who is a provider of a GPAI model and how to know if you are one. For now, here are some examples the European Commission provides that constitute a provider placing a GPAI model on the market. In all instances, company A is the GPAI model provider. Company A develops a GPAI model and makes it available via a software library, repository, API, or cloud computing service. Company A commissions company B to develop a GPAI model on its behalf, and company A makes it available via a software library, repository, API, or cloud computing service. Company A develops a GPAI model and uploads it to an online repository hosted and managed by company C. There are a suite of obligations that apply to providers of all GPAI models, as well as additional obligations that also apply to providers of GPAI models with systemic risk. Part 2 of this series will cover, in detail, the obligations for providers of GPAI models with systemic risk. They are left out here. Below is a summary of the four main sets of obligations that apply to providers of all \n\nGPAI models: \n\n1. Providers of GPAI models must develop, maintain, and keep up-to-date   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 9/22\n\ncomprehensive technical documentation which provides information about the GPAI model, how it was developed, and how it should be used. The Transparency Chapter of the GPAI Code of Practice provides extensive detail about how this can be done. It is accompanied by a Model Documentation Form consisting of 42 metadata attributes across 8 categories. This builds on the elements set out in Annex XI and XII of the AI Act. The attributes include training time and computation, model size, energy consumption, data collection and curation methods, and input and output modalities. Some of this information must be shared with the European Commission's AI Office and/or national regulators (on request), and some must be proactively shared with 'downstream providers', which are organisations that integrate GPAI models into an AI system. Some documentation is required for all GPAI models, whereas certain additional documentation is only required for GPAI models with systemic risk. \n\n2. Providers of GPAI models must produce and make publicly available a detailed summary of the content and data used to train the GPAI model. \n\nThe AI Act requires this training data summary to be \"sufficiently detailed\" and structured in line with a template provided by the AI Office. That template was   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 10/22\n\npublished by the European Commission in July 2025, alongside explanatory guidance. The template carefully attempts to balance protecting trade secrets and confidentiality with promoting transparency, data protection, and copyright protection. In a nutshell, the training data summary should cover the amount and type of training data used, as well as the use of: publicly available datasets; private non-publicly available datasets; data crawled and scraped from online sources; user data; and synthetic data. Importantly, the template does not require providers to publish or disclose details about the specific data and works used to train GPAI models, as this would \"go beyond the requirement to provide a 'summary'\" .\n\n3. Providers of GPAI models must implement a policy to comply with EU copyright and intellectual property law.   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 11/22\n\nGiven the large volume of copyright protected material typically used to train GPAI models, this obligation is significant. It compels providers to consider and document how they will comply with EU copyright law in such contexts. The AI Act provides minimal detail about how to do this. However, it does stipulate that \"state-of-the-art technologies\" must be used to promote copyright compliance in the context of developing GPAI models. The Copyright Chapter of the GPAI Code of Practice provides more detail, outlining six practical measures for organisations to implement. \n\n4. Providers of GPAI models must cooperate with the European Commission and regulatory authorities and appoint an EU-based authorised representative if they are based outside of the EU. \n\nThe AI Act has extraterritorial effect. In this context, it means that if a provider places their GPAI model on the market or makes it available in the EU, then that provider must comply with the AI Act, irrespective of where the provider is established or operating from. In such extraterritorial scenarios, the appointment of an authorised representative is important, to enable effective cooperation and correspondence with the AI Office and any other relevant authorities.   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 12/22\n\nThe AI Act exempts providers of open-source GPAI models from the first and fourth set of obligations. Namely, they do not need to develop, maintain, and keep up-to-date comprehensive technical documentation referenced in point 1 above. And they do not need to appoint an authorised representative (if based outside the EU), as referenced in point 4. However, providers of open-source GPAI models are obliged to comply with points 2 and 3. This means they must implement a copyright compliance policy and publish the training data summary. Furthermore, providers of open-source GPAI models with systemic risk are not exempt from any of these obligations. This means that they must comply with the obligations for both all GPAI models and the additional obligations for GPAI models with systemic risk. The European Commission's guidelines provide clarity about what exactly constitutes an open-source GPAI modelâ€”a hotly contested topic. Here is a summary of the recent guidance: \n\nWhat about open-source GPAI models?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 13/22\n\nTo qualify as open-source, a GPAI model must be released under a free and open-source licence that permits unrestricted access, use, modification, and distribution without payment or significant limitations. All model parameters, architecture, and usage information must be publicly available to ensure usability. A licence is not considered open-source if it includes restrictions like non-commercial use only, prohibitions on distribution, usage limits based on scale, or requirements for separate commercial licences. Essentially, most usage restrictions, aside from those justifiably designed to protect users or other groups, or ensure appropriate attribution, would likely disqualify a GPAI model from open-source classification. Monetisation generally also disqualifies a model from open-source exemptions if payment is required for core access, use, or essential, indistinguishably linked support, or if it's exclusively hosted on paid platforms. However, optional paid services that don't hinder free use are permissible. \n\nWhat about 'legacy' GPAI models (released before 2 August 2025)?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 14/22\n\nIrrespective of when a GPAI model was developed and placed on the market, providers of that model must eventually comply with the AI Act's obligations. This means that the full suite of obligations, including publishing training data summaries, implementing a copyright compliance policy, and maintaining technical documentation, applies to both 'legacy' and 'new' GPAI models. The only difference is the applicable date of those obligations. For the 'legacy' GPAI models, placed on the market before 2 August 2025, providers have until 2 August 2027 to comply with the aforementioned provisions. For GPAI models released after 2 August 2025, there is no longer a grace period. The practical implication of this is that virtually all GPAI models that have already been released and made available in the EU will have to retrospectively become compliant. This represents a practical and operational headache for AI developers, especially given how recently the Code of Practice and guidelines about how to comply were released. The European Commission acknowledges that this is going to be challenging, noting that \"the AI Office is dedicated to supporting providers in taking the necessary steps to comply with their obligations by 2 August 2027\" .However, the Commission confirmed that providers of 'legacy' GPAI models will not have to conduct \"retraining or unlearning of models\" where this is not possible or   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 15/22\n\nwhere it would cause undue burden. The Commission accepts that information about historical training data may not always be available and encourages providers to be open about this. The GPAI Code of Practice was published by the European Commission on 10 July 2025 and was approved by the EU on 1 August 2025. The AI Act mandated the Commission to work with external experts to develop the Code of Practice, to assist GPAI model providers in their compliance journey. It is a voluntary tool which helps providers comply with the full suite of obligations for GPAI models. Although organisations are not obliged to sign and implement the Code of Practice, doing so provides a standardised and endorsed approach for regulatory compliance. This offers more legal certainty regarding the validity of an organisation's approach. However, you can still be compliant, via adequate alternative means, even if you do not sign up to and follow the Code of Practice. \n\nHow does the GPAI Code of Practice fit in?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 16/22\n\nThe Code of Practice consists of three chapters: 1) Transparency, 2) Copyright, and 3) Safety and Security. Each chapter contains several 'measures'. These are practical steps and actions that signatory organisations commit to taking and implementing to achieve GPAI model compliance. Organisations are able to sign up to specific chapters, or the entire Code. Thus far, 26 organisations have signed up in full and xAI (Grok's creator) has signed up to the Safety and Security Chapter only. Meta announced that it will not sign up. The Transparency Chapter focuses on the technical documentation which GPAI model providers are obliged to produce, maintain, and share with key stakeholders. It includes the 'Model Documentation Form' template, which providers can use to ensure they gather and document all the information which they are required to maintain and potentially share with the AI Office, national regulators, and downstream providers. The Copyright Chapter focuses on how organisations can comply with the obligation to implement an EU copyright compliance policy. This includes measures for compliant web crawling and scraping and mitigating the risk of copyright-infringing outputs via technical guardrails. Finally, the Safety and Security Chapter focuses exclusively on the obligations for providers of GPAI models with systemic risk. It consists of ten commitments, all relating to GPAI model risk identification, management, mitigation, treatment, monitoring,   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 17/22\n\nownership, and accountability. The European Commission strongly encourages providers to sign the Code of Practice, noting its various benefits and calling it a \"straightforward way of demonstrating compliance\" . The Commission even indicates that it will be more trusting of signatory organisations, due to their transparent approach. It noted that providers who do not sign up may be \"subject to a larger number of requests for information and requests for access to conduct model evaluations throughout the entire model lifecycle because the AI Office will have less of an understanding of how they are ensuring compliance with their obligations\" .In future, the GPAI Code of Practice may be superseded by harmonised technical standards. These remain under development. The most important thing to understand is that the obligations relating to providers of GPAI models are enforced by the European Commission's AI Office, not member state regulators. This is in contrast to the rest of the AI Act's provisions. With respect to enforcement actions, the AI Office has the following powers: \n\nHow and when will the GPAI model provisions be enforced?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 18/22\n\nRequest information from providers. Conduct evaluations of GPAI models. Request the implementation of measures and mitigations. Recall GPAI models from the market. Impose financial penalties of up to 3% of global annual turnover or â‚¬15m (whichever is higher). Because the AI Office is charged with enforcement, the European Commission's guidelines on GPAI models carry significant weight, even though they are not legally binding. In those guidelines, the Commission explained that it expects to forge close working relationships with GPAI model providers. It encourages \"close informal cooperation with providers during the training of their GPAI models\" , as well as \"proactive reporting by providers of GPAI models with systemic risk\" . Given the various notification, reporting, and transparency obligations, there will be a lot of back and forth between providers and the AI Office. For those that signed the Code of Practice, the AI Office's focus will be on monitoring their adherence to its measures. Expect slightly more leniency and goodwill. For those that opted not to, the focus will be on in-depth assessments and investigations as to how they are nonetheless compliant.   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 19/22\n\nIn terms of timelines for compliance and enforcement, the obligations for providers of GPAI models became applicable on 2 August 2025. This means that organisations are legally obliged to be compliant today. However, this is only for GPAI models that were placed on the market from 2 August 2025 onwards. As discussed above, for â€˜legacyâ€™ GPAI models placed on the market before then (i.e., the vast majority of GPAI models available today), providers have until 2 August 2027 to comply. Although the obligations have been applicable since 2 August 2025, the European Commission, including its AI Office, does not have any enforcement powers until 2August 2026 . This means there will be no enforcement proceedings or fines for at least one year from now. However, this does not change the fact that organisations are obliged to work on compliance from today. Any critical gaps in the first year may be taken into account in future enforcement actions or rulings. Irrespective of the provision, and whether or not enforcement is managed by the AI Office or member state regulators, the Court of Justice of the European Union (CJEU) always has the final say on AI Act interpretation. Over the years, it will be interesting to monitor and learn from the inevitable GPAI model case law that will pile up. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.", "fetched_at_utc": "2026-02-08T18:50:50Z", "sha256": "8739a1b95698e309a5954196a355fe3350ff51e3731d281c6d3173b04e6e889c", "meta": {"file_name": "General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel.pdf", "file_size": 755365, "relative_path": "pdfs\\General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4887}}}
{"doc_id": "pdf-pdfs-general-purpose-ai-model-compliance-guide-part-2-oliver-patel-4fda32651a33", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel.pdf", "title": "General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel, author and creator of Enterprise AI Governance .\n\n1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2) \n\nhttps://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46  2/24 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! \n\nFollow me on LinkedIn for more frequent updates. Welcome to Part 2 of the General-Purpose AI (GPAI) Model Compliance Guide. This 3-part series is posted exclusively on Enterprise AI Governance. To all subscribers and new readers, thanks for supporting the newsletter! This weekâ€™s edition provides a comprehensive yet accessible overview of the EU AI Actâ€™s provisions for GPAI Models with Systemic Risk .You will learn: \n\nâœ… What is a GPAI model with systemic risk? \n\nâœ… What are the notification and exception procedures for providers of GPAI models   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 3/24\n\nwith systemic risk? \n\nâœ… What are the compliance obligations for providers of GPAI models with systemic risk? \n\nâœ… What exactly is a â€œsystemic riskâ€? \n\nâœ… Deep dive on the GPAI Code of Practice: Safety and Security Chapter \n\nIf you havenâ€™t read Part 1, you should check it out here and read it first. It provides a detailed breakdown of the Obligations for Providers of GPAI Models , including what the core obligations for all GPAI models are, how and when these obligations will be enforced by the AI Office, as well as specific considerations for open-source models and legacy GPAI models (released before 2 August 2025). All this is essential background information that is necessary to fully understand the compliance implications for GPAI models with systemic risk. Part 3, coming next week, will cover the knotty issue of 'Downstream Actors': Modification, Deployment, and Use of GPAI Models . This will address the important question of who exactly is a provider of a GPAI modelâ€”it could be you! \n\nNote: this series assumes familiarity with the EU AI Act, its core concepts, and the topic of general-purpose AI and foundation models more broadly. Also, I was not involved in the multi-stakeholder process of drafting and developing the EUâ€™s GPAI Code of Practice. Finally, none of this should be taken as legal advice. Always consult a legal professional.   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 4/24\n\nThe following official sources have been used to create this guide: \n\nEU AI Act full text European Commission Guidelines for Providers of GPAI Models GPAI Code of Practice \n\nTransparency Chapter Model Documentation Form Copyright Chapter Safety and Security Chapter \n\nTemplate for publishing GPAI Model Training Data Summary \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nThere are two broad categories of GPAI models that the AI Act regulates. These are: \n\nWhat is a GPAI model with systemic risk?   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 5/24\n\n1. GPAI models \n\n2. GPAI models with systemic risk As explained in Part 1 of this series, GPAI models â€œare trained with a large amount of data using self-supervision at scale [â€¦ ] display significant generality and are capable of performing a wide range of distinct tasksâ€. The European Commissionâ€™s recent guidelines also clarify that if an AI modelâ€™s training compute exceeds 10^23 floating-point operations, and it can generate language (either text or audio), or generate image or video based on text inputs, then it should be considered a GPAI model. An AI model is classified as a GPAI model with systemic risk if it meets the above definition and criteria and also has â€œ high impact capabilities â€. The AI Act defines this as capabilities that â€œ match or exceed the capabilities recorded in the most advanced GPAI models â€. If the cumulative amount of computation used to train a GPAI model exceeds 10^25 floating-point operations, then it is, by default, presumed to have â€œhigh impact capabilitiesâ€ and thus classified as a GPAI model with systemic risk. By relying on this compute threshold, the EUâ€™s position is that there is a direct correlation between how much computational resource is used to train an AI model and both the general-purpose capabilities of the model and the level of risk that it poses.   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 6/24\n\nThis means that the larger an AI model is (e.g., in terms of number of parameters) and the more data that is used to train it (e.g., in terms of different examples), the more likely it is to be classified as a GPAI model with systemic risk. Whilst the European Commission acknowledges that â€œ training compute is an imperfect proxy for generality and capabilities â€, it argues that it is â€œ the most suitable approach at presentâ€ .However, the European Commission is empowered to amend this compute threshold, or introduce an entirely new indicator, via a delegated act. This means it can do so independently, without reopening and amending the AI Act itself. However, the European Parliament and the Council (i.e., the member states) have a right to object to any such changes. Interestingly, it is possible for a GPAI model to be classified as a GPAI model with systemic risk even if it does not meet the 10^25 floating-point operations compute threshold. This would require the European Commission to determine that it nonetheless has high impact capabilities, despite the lower cumulative amount of compute used to train it. In making such a decisionâ€”that would prove controversial due to the impact on the impacted providerâ€”the European Commission would consider factors like the size of the model, input and output modalities, benchmark and evaluation results, model   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 7/24\n\nautonomy level, and the number of end-users. Ultimately, it would have to prove that its capabilities match or exceed those of the most advanced GPAI models, despite the fact that less compute was used to train it. The key point for enterprises is that they must carefully forecast, measure, track, and record their estimates of the amount of computational resource used to develop, train, modify, and fine-tune GPAI models, in order to determine what compliance obligations they may have to adhere to. When estimating and measuring compute levels, the European Commissionâ€™s guidance is that providers should â€œ as a general rule, account for all compute that contributed or will contribute to the modelâ€™s capabilities â€. This even includes the compute expended to generate synthetic data for training, even if not all the synthetic data was eventually used to train the GPAI model. Once an organisation knows that a GPAI model it has developed (or is in the process of developing) meets the threshold for training compute (which means it is classified \n\nWhat are the notification and exception procedures for providers of GPAI models with systemic risk?   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 8/24\n\nas a GPAI model with systemic risk), it must notify the European Commission of this as soon as possible, and within two weeks at the latest. In some cases, this notification will be required before the overall training process is completed (e.g., if the threshold is exceeded mid-training run). This notification should include both the precise computation amount as well as a detailed explanation of how this has been estimated. In its guidelines, the European Commission recommends that â€œ providers should estimate the cumulative amount of training compute that they will use â€ before the training process begins. If their pre-training estimate surpasses the systemic risk threshold, they should inform the Commission of this. Zooming out, this notification procedure enables the European Commissionâ€™s AI Office to fulfill its role as the regulator overseeing and enforcing the AI Actâ€™s provisions on GPAI models. It will also promote transparency, as the European Commission will publish a list of all GPAI models with systemic risk that are in scope of the AI Act. Finally, it is possible for a provider of a GPAI model that is by default classified as a GPAI model with systemic risk to secure an exception. To do this, the provider must demonstrate that its GPAI model does not have â€œhigh impact capabilitiesâ€ and therefore does not pose systemic risks and should not be classified as such, despite surpassing the cumulative compute for training threshold of 10^25 floating-point operations.   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 9/24\n\nProviders can do this by pointing to evidence like benchmark and evaluation results, especially if these demonstrate a capability gap between their model and the most advanced AI models. It is important to note that providers cannot get out of the GPAI model with systemic risk classification merely by implementing robust controls and safeguards which mitigate the systemic risk. To secure an exception, they must convince the European Commission, with cold, hard evidence, that the model genuinely does not have high impact capabilities. The AI Act describes this as an â€œexceptionalâ€ scenario, requiring European Commission approval. In such instances, the burden of proof will be on the provider. Given the extensive additional compliance obligations for providers of GPAI models with systemic risk (as compared to GPAI models), the question of which GPAI models are and are not classified as posing systemic risk is significant. Precisely what these additional compliance obligations are is explained below. \n\nWhat are the compliance obligations for providers of GPAI models with systemic risk?   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 10/24\n\nPart 1 of this series provides a detailed breakdown of the compliance obligations for providers of GPAI models. In summary, the four core obligations for GPAI models are: \n\n1. Develop, maintain, and keep up-to-date comprehensive technical documentation. \n\n2. Produce and make publicly available a detailed summary of the content and data used to train the GPAI model. \n\n3. Implement a policy to comply with EU copyright and intellectual property law. \n\n4. Cooperate with the European Commission and regulatory authorities and appoint an EU-based authorised representative (if based outside of the EU). Providers of open-source GPAI models are exempt from obligations 1 and 4. This means they still need to publish a training data summary and implement a copyright compliance policy. Providers of GPAI models with systemic risk must comply with all of the above obligations. Also, providers of open-source GPAI models with systemic risk are not exempt from any of the above obligations. This means that sufficiently advanced and capable open-source AI models are treated the same, from an AI Act compliance perspective, as proprietary models. In other words, providers of any GPAI model with systemic risk, that is placed on the market or made available in the EU, irrespective of whether it is open-source, must   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 11/24\n\ncomply with all the above obligations (for providers of GPAI models), as well as the additional obligations for providers of GPAI models with systemic risk. There are four core additional obligations that only apply to providers of GPAI models with systemic risk. These are: \n\n1. Perform model evaluation using state of the art tools and protocols. This includes conducting adversarial testing to enable the identification and mitigation of â€œsystemic risks â€. \n\n2. Assess and mitigate potential systemic risks that may stem from the development, deployment, or use of the GPAI model with systemic risk. \n\n3. Track, document, and report information about serious incidents and any corrective measures to address them. \n\n4. Ensure an â€œadequate level of cybersecurity protectionâ€ for both the GPAI model with systemic risk and the physical infrastructure of the model. These obligations reflect the fact that EU lawmakers deem it both appropriate and necessary for the most advanced and capable foundation models to be subject to rigorous governanceâ€”including stringent safety and security testing and evaluation procedures, the implementation of technical guardrails and safeguards to mitigate risk, continuous monitoring and oversight, and documented accountability and risk ownershipâ€”due to the widespread use of these models and the distinct possibility   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 12/24\n\nthat this use could lead to significant negative impact. However, the text of the AI Act itself does not provide much detail about how to approach and implement the above four obligations. That is why there is a GPAI Code of Practice. The Code provides a detailed, standardised, and step-by-step compliance framework for GPAI model providers. The GPAI Code of Practice: Safety and Security Chapter , which is most relevant for these obligations, is analysed below. But first, we explore the definition of â€œsystemic riskâ€, which is at the heart of these obligations. The overarching purpose of the above obligations is for providers to uncover and mitigate the systemic risks which their most capable and advanced GPAI models pose. This includes reducing the likelihood of these risks materialising and reducing their impact if they do materialise. This raises an important question for GPAI model providers: what exactly is a â€œsystemic riskâ€? \n\nWhat exactly is a â€œsystemic riskâ€?   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 13/24\n\nThe GPAI Code of Practic e builds on the AI Act by providing additional detail regarding precisely how providers should define, identify, and evaluate the systemic risks that their GPAI models could pose. It classifies the following four risks as systemic risks. This means that if any providers that are also signatories identify any of these risks, they must be classified and treated as systemic risks: \n\nChemical, biological, radiological, and nuclear (CBRN) , e.g., a GPAI model that makes it easier or otherwise enables the design, development, and use of CBRN-related weapons or materials. \n\nLoss of control , e.g., a GPAI model that autonomously self-replicates and creates new, more advanced AI models, without human awareness or control. \n\nCyber offence , e.g., a GPAI model that can be used to significantly lower barriers to entry for scaling cyber attacks. \n\nHarmful manipulation , e.g., a GPAI model that targets large populations of people and uses deceptive techniques to promote harmful or destructive behaviour. Recital 110 of the AI Act complements this, by providing an illustrative list of examples of systemic risks:   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 14/24\n\nMajor accidents. Disruptions of critical sectors. Serious consequences to public health and safety. Negative effects of democratic processes. Negative effects on public or economic security. The dissemination of illegal, false, or discriminatory content. More broadly the GPAI Code of Practice clarifies the essential characteristics of a systemic risk, to further enable their identification. This clarification is based on the formal definition of systemic risk provided in the AI Act. The three essential characteristics of a systemic risk are: \n\n1. The risk is directly related to the GPAI modelâ€™s high-impact capabilities. \n\n2. The risk has a significant impact on the EU due to its reach or due to the actual or potential negative impact on public health, safety, public security, fundamental rights, or society as a whole. \n\n3. The impact can spread widely, at scale, through connected AI systems and the AI and industry ecosystem more broadly. The EUâ€™s view is that as model capabilities and model reach increase, so do the potential systemic risks. Recital 110 of the AI Act also highlights that such systemic   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 15/24\n\nrisks can arise due to various factors and causes, including (but not limited to): Model misuse Model reliability Model fairness Model security Model autonomy level Tool access Model modalities Release and distribution mechanisms Potential to remove model guardrails The detailed information provided in the AI Act and the Code of Practice is sufficient to enable providers of GPAI models with systemic risk to fulfil their obligation of identifying, evaluating, and mitigating the specific systemic risks their GPAI models may pose. \n\nDeep dive on the \n\nGPAI Code of Practice: Safety and Security Chapter   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 16/24\n\nThe final section of this article summarises and analyses the key elements of the GPAI Code of Practice: Safety and Security Chapter. \n\nThis Chapter focuses exclusively on the specific obligations for providers of GPAI models with systemic risk. It provides a comprehensive and standardised set of commitments and measures that signatory organisations will implement, in order to adhere to the four compliance obligations for GPAI models with systemic risk (detailed above). For a dedicated explainer on the GPAI Code of Practice, check out this previous post \n\non Enterprise AI Governance. The most important things to know about the GPAI Code of Practice are that i) it was approved by the EU on 1 August 2025, ii) it is a voluntary resource which helps providers comply with the full suite of obligations for GPAI models, and iii) it consists of three chapters: 1) Transparency, 2) Copyright, and 3) Safety and Security. The European Commission strongly encourages providers to sign the Code of Practice and even indicated that it will be more trusting of signatory organisations. However, the Code itself is not law. The Safety and Security Chapter, which is by far the most detailed of the three chapters, consists of ten commitments. All commitments directly relate to GPAI model with systemic risk risk identification, management, mitigation, treatment, monitoring,   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 17/24\n\nownership, and accountability. Below is a detailed breakdown of each of the ten commitments and a summary of the most important measures that signatory organisations have committed to implementing.   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 18/24\n\n1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2) \n\nhttps://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46  19/24 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2) \n\nhttps://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46  20/24 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2) \n\nhttps://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46  21/24", "fetched_at_utc": "2026-02-08T18:50:53Z", "sha256": "4fda32651a331c4eec7607653ab4e15672769ef3bbf9f1079f5c3060cf180226", "meta": {"file_name": "General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel.pdf", "file_size": 1723762, "relative_path": "pdfs\\General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4371}}}
{"doc_id": "pdf-pdfs-how-could-the-eu-ai-act-change-oliver-patel-0ecedae03efa", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\How could the EU AI Act change - Oliver Patel.pdf", "title": "How could the EU AI Act change - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel \n\nhttps://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change  2/23 On Wednesday 19 November 2025, the European Commission unveiled its Digital Omnibus Package, proposing targeted yet impactful amendments to the EU AI Act. This article distils what could change, why it matters for enterprise AI governance practitioners, and what to watch as trilogue negotiations begin. \n\nIf you value my work and want to learn more about the EU AI Act and AI governance implementation, sign up to secure a 25% discount for my forthcoming book, \n\nFundamentals of AI Governance (2026). On Wednesday 19 November 2025, the European Commission (henceforth the Commission) announced proposed changes to the AI Act. These changes are presented as â€œinnovation-friendly AI rulesâ€ that will â€œreduce compliance costs for businessesâ€. It did so by publishing a proposal for a new regulation. The purpose of this proposed regulation is to â€œsimplifyâ€ the AI Act with targeted yet meaningful amendments. This is part of the Commissionâ€™s broader â€œDigital Packageâ€, which is a major programme of work aiming to â€œsimplify EU digital rules and boost innovationâ€. The \n\nWhat are the most important proposed EU AI Act changes?   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 3/23\n\nDigital Packageâ€”which encompasses the â€œDigital Omnibus Regulationâ€â€”includes proposals to amend flagship digital laws like the AI Act, the GDPR, the ePrivacy Directive, the Data Act, and the NIS 2 Directive. Specifically, the Commission simultaneously published proposals for two regulations (so itâ€™s not really an â€œomnibusâ€ anymore): \n\nProposal for Regulation on simplification of AI rules (which covers the AI Act amendments); and \n\nProposal for Regulation on simplification of the digital legislation (which covers the amendments to the other EU digital laws mentioned above). This article explains and analyses the six most important AI Act amendments that enterprise AI governance professionals need to understand. These are: \n\n1. Timeline changes for high-risk AI system compliance. \n\n2. Timeline changes for transparency-requiring AI system compliance. \n\n3. Limiting registration in the public EU database for high-risk AI systems. \n\n4. Softening of the AI literacy obligation. \n\n5. Expanding the scope of the European AI Officeâ€™s regulatory powers. \n\n6. Proportionality for small mid-cap (SMC) enterprises.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 4/23\n\nFor each of these six proposed amendments, I explain what is in the law today, what changes are being proposed, and what the impact of these changes would be. Earlier this week, I published an article on Enterprise AI Governance that explains how we got to this point and why the EU is now doing this. It provides a detailed account of the background context to AI Act simplification, highlighting how the â€˜Draghi reportâ€™â€”which argued that digital regulatory burdens are impeding European growth and competitivenessâ€”has influenced the Commissionâ€™s proposals. It also outlines three important caveats on the EUâ€™s legislative process that are worth repeating: This merely represents the proposal of one EU institution (the Commission). Such amendments of EU law require formal approval from both the European Parliament and the EU member states via the Council of the EU (the Council). Therefore, this proposal will now be followed by lengthy and potentially fraught trilogue negotiations between the Commission, European Parliament, and Council. Finally, it is impossible to predict what the final legislative text will consist of, how long the negotiation and approval process will take, and whether approval to amend the AI Act will ultimately be agreed on and enacted.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 5/23\n\nScope of this article: this is not an exhaustive analysis of the entire AI Act simplification proposal and it does not cover every proposed amendment in the Commissionâ€™s 65-page document. Rather, it focuses on the six proposed changes that would be most consequential (if passed) for enterprises implementing AI governance. Also, it intentionally does not cover the proposed changes to the GDPR, nor the AI Act amendments that are directly related to the processing of personal data (e.g., use of sensitive personal data for bias mitigation), as this topic will be addressed in a future article on Enterprise AI Governance. \n\nDisclaimer: this article is not intended to be legal advice and must not be relied upon or used in that way. Always consult a qualified legal professional. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nWhat is in law today? \n\n1. Timeline changes for high-risk AI system compliance   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 6/23\n\nThe key provisions relating to high-risk AI systems apply from 2 August 2026. This means that from 2 August 2026, unless there is a change in the law, providers and deployers must adhere to the obligations and requirements for high-risk AI systems and can be subject to investigations and penalties for non-compliance. However, this applicable date only applies to high-risk AI systems listed in Annex III (e.g., education, employment, administration of justice etc.) that are placed on the market or put into service from 2 August 2026 onwards. For such Annex III high-risk AI systems that were placed on the market or put into service before 2 August 2026, providers and deployers are only subject to AI Act obligations and requirements if, from that date onwards, there is a significant change in design or intended purpose of the AI system. Furthermore, for high-risk AI systems that are products, or safety components of products, regulated by specific EU product safety laws listed in Annex I, the applicable date is 2 August 2027. \n\nWhat changes are being proposed? \n\nIf you are asked â€œwhen do the compliance obligations for high-risk AI systems apply?â€ ,your answer now has to be â€œit dependsâ€. Given the nature of the proposed amendments, there are various potential scenarios. The Commission is seeking to link the applicability of high-risk AI system provisions   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 7/23\n\nwith the availability of technical standards and associated support tools. However, if these artefacts are not approved and available within a certain timeframe, there is a backstop date, which represents the latest applicable date. Under this proposal there are, broadly speaking, three potential scenarios for when most of the provisions relating to high-risk AI systems may apply (covering high-risk AI system classification, development requirements, and obligations of providers, deployers, and other parties): \n\nScenario 1. If technical standards and associated support tools for high-risk AI system compliance are finalised and approved by the Commission, then the applicable compliance date will be six months after this approval (for high-risk AI systems listed in Annex III) and 12 months after this approval (for high-risk AI systems that are products, or safety components of products, regulated by an EU law listed in Annex I). \n\nScenario 2. However, if technical standards and associated support tools for high-risk AI system compliance are not finalised or approved by the Commission in time (i.e., before the dates below), then the applicable compliance dates will be 2 December 2027 (for high-risk AI systems listed in Annex III) and 2 August 2028 (for high-risk AI systems that are products, or safety components of products, regulated by an EU law listed in Annex I).   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 8/23\n\nScenario 3. Given that the applicable date in law today is 2 August 2026, if these amendments are not approved and enacted before this date, then the provisions relating to high-risk AI systems will, technically speaking, apply from then. This creates timeline pressure to get these changes approved within the next few months. \n\nWhat impact would these changes have? \n\nTo clarify, 2 December 2027 and 2 August 2028 are the backstop dates for high-risk AI system compliance. To reinforce this point, the Commission has explained that the grace period will be up to sixteen months (referring to the time between 2 August 2026 and 2 December 2027). This means that if technical standards come too late (i.e., after 2 June 2027, which is six months before 2 December 2027) these backstop dates will apply. These amendments shine the spotlight on the ongoing work being led by CEN/CENELEC to agree and publish technical standards. They also highlight the importance that the Commission places on these artefacts to support organisations and facilitate compliance. However, even if the applicable date may be delayedâ€”giving providers and deployers more time to prepareâ€”this extra time has been achieved at the expense of certainty, with organisations now not knowing what the applicable date will be.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 9/23\n\nWhat is in law today? \n\nArticle 50 of the AI Act outlines transparency obligations for providers and deployers of certain AI systems. Article 50 covers obligations relating to disclosure, informing end users about the use of AI, labelling certain deep fake content, and detectability of AI system outputs. Specifically, Article 50(2) stipulates that: \n\nâ€œproviders of AI systems, including general-purpose AI systems, that generate synthetic audio, image, video, or text content shall ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulatedâ€. \n\nCurrently, this specific obligation applies from 2 August 2026. This compliance date applies to all AI systems, irrespective of whether they are placed on the market or put into service before or after 2 August 2026. \n\nWhat changes are being proposed? \n\n2. Timeline changes for transparency-requiring AI system compliance   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 10/23\n\nThe Commission proposes to push back the applicable date for this specific transparency obligation to 2 February 2027 for providers of AI systems that have been placed on the market before 2 August 2026. This proposed six month delay to the applicable date only applies to obligation stipulated in Article 50(2) (on AI system output machine readability and detectability) and not the other transparency obligations outlined in Article 50. \n\nWhat impact would these changes have? \n\nThis change would give providers of AI systems that have already been placed on the market or put into service, or that will be before 2 August 2026, and that generate synthetic audio, image, video, or text content (i.e., most generative AI systems), an additional six months to ensure that these AI systems are developed in such a way that ensures the outputs they generate are detectable as AI-generated. Although this is a relatively short delay, it is nonetheless an acknowledgement by the Commission of the technical and engineering challenges providers face in developing or modifying their AI systems to adhere to this obligation. However, any AI systems placed on the market on or after 2 August 2026 will have to comply with this obligation from their release date. \n\n3. Limiting registration in the EU public database for high-risk AI   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 11/23\n\nWhat is in law today? \n\nAnnex III of the AI Act lists eight categories of high-risk AI system, including law enforcement (#6), education and vocational training (#3), and employment, workersâ€™ management and access to self-employment (#5). However, there are classification rules which mean that just because an AI system is intended for use or used in one of these domains does not necessarily mean it is a high-risk AI system. AI systems listed in Annex III are not considered high-risk if it is demonstrated that they do not pose significant risk of harm to health, safety, or fundamental rights. For example, if the AI system does not materially influence decisions or is only used for a narrow procedural task, the provider is entitled to demonstrate, based on a documented assessment, that it is not a high-risk AI system. This derogation, including the conditions to fulfil it, is outlined in Article 6(3) and only applies to AI systems listed in Annex III. Providers must register high-risk AI systems listed in Annex III in the EU public database for high-risk AI systems, before those AI systems are placed on the market or put into service. Interestingly, this registration obligation also includes AI systems that \n\nsystems   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 12/23\n\nthe provider has concluded are not high-risk via the derogation procedure outlined in Article 6(3). \n\nWhat changes are being proposed? \n\nThe Commission proposes to limit the scope of this registration obligation so that it no longer applies to AI systems that providers have concluded are not high-risk via the Article 6(3) derogation procedure. Simply put, where a provider has assessed and documented that an AI system used in an Annex III domain is not high-risk, the provider will not have to register that AI system in the EU public database for high-risk AI systems. However, although providers can make this assessment independently and do not require any external approval (e.g., from the AI Office or market surveillance authority), providers will still be obliged to share the documentation of the assessment, containing the justification and supporting evidence, upon request from a regulator. \n\nWhat impact would this have? \n\nThis may seem like a subtle change at first glance, but it would be consequential for organisations using AI at scale.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 13/23\n\nMost enterprise AI governance practitioners likely raised their eyebrows when they realised that every AI system used in a high-risk domain, including AI systems that are not high-risk due to their use for mere assistive, procedural, or preparatory tasks, would have to be registered. This will be difficult (or perhaps near impossible) to keep track of and implement, due to the increasingly ubiquitous use of AI to support and augment workflows across virtually all domains of enterprise activity. Indeed, the Commission describes this current registration obligation as a â€œdisproportionate compliance burdenâ€. Therefore, the most obvious impacts of this change would likely be far fewer AI systems registered in the EU public database for high-risk AI systems and reduced administrative overheads for organisations developing and deploying AI systems. However, it would also reduce public transparency regarding which AI systems providers deem not to be high-risk and how they have made such determinations. This could incentivise some providers to take a more expansive approach to interpreting Article 6(3) and determining what is not a high-risk AI system, as they may reasonably judge that the risk of doing so (and being penalised for getting it wrong) is lower with significantly less public scrutiny. \n\n4. Softening of the AI literacy obligation   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 14/23\n\nWhat is in law today? \n\nUnder Article 4 of the AI Act providers and deployers of AI systems are obliged to implement â€œAI literacyâ€. This is one of the most important aspects of the law, because it has contributed to many organisations in the EU and further afield rolling out AI training and upskilling initiatives for their workforce. Specifically, Article 4 requires organisations to ensure that â€œstaff and other persons dealing with the operation and use of AI systemsâ€ have a â€œsufficient level of AI literacyâ€. In practice, given that all staff in modern organisations can use AI systems (e.g., ChatGPT or Gemini), a reasonable interpretation of Article 4 is that all of these staff should receive some form of AI-focused training. The AI literacy obligation has been applicable since February 2025. However, there are no enforcement penalties for non-compliance with it. Although non-compliance could be taken into account during enforcement investigations or proceedings relating to other aspects of non-compliance. \n\nWhat changes are being proposed? \n\nThe Commission proposes to remove the obligation for providers and deployers to implement AI literacy. Rather than providers and deployers being legally required to ensure their staff operating and using AI systems have sufficient levels of AI literacy,   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 15/23\n\nthe Commission and Member States will be required to foster AI literacy and \n\nâ€œencourage providers and deployers of AI systems to take measures to ensure a sufficient level of AI literacyâ€ . The Commission has alluded to the fact that the ambiguity of the current â€œunspecified obligationâ€ has caused issues for businessesâ€”especially smaller firms. \n\nWhat impact would this have? \n\nThis change would be significant because it would remove the broad and expansive legal obligation for companies to implement AI literacy. However, human oversight of high-risk AI systems must still be assigned to staff with sufficient training and competence. Therefore, ensuring AI literacy is still required in that context. Moreover, it will be practically impossible for any organisation to comply with the AI Actâ€”or to manage AI risks, implement AI at scale, and maximise the value of AIâ€” without educational and training initiatives focused on AI. Therefore, forward-thinking enterprises are unlikely to abandon their AI literacy programmes because it is no longer a legal requirement. However, certain initiatives may be scaled back, deprioritised, or change in focus or scope.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 16/23\n\nWhat is in law today? \n\nHere is a simplified summary of the (rather complex) AI Act governance regime: There are governance and regulatory bodies at both the EU and member state level. At the EU level, the most important bodies are the European AI Office (which is part of the Commission) and the European AI Board. At the member state level, the most important bodies are the market surveillance authorities. They are responsible for monitoring, investigations, and enforcement of the AI Act. There will potentially be several in each EU member state. The AI Office is responsible for overseeing and enforcing the provisions on general-purpose AI models, whereas the market surveillance authorities are responsible for overseeing and enforcing the provisions on AI systems (e.g., high-risk and transparency-requiring AI systems), as well as most other AI Act provisions. \n\nWhat changes are being proposed? \n\n5. Expanding the scope of the European AI Officeâ€™s regulatory powers   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 17/23\n\nThe Commission proposes to â€œcentralise oversight over a large number of AI systems built on general-purpose AI modelsâ€ when the same provider develops both the general-purpose AI model and the AI system. The proposed amendments to Article 75 would render the AI Office as the body responsible for monitoring and supervising compliance of AI systems that leverage general-purpose AI models. However, this would only apply when the general-purpose AI model and the AI system are developed and placed on the market or put into service by the same provider. In such scenarios, the AI Office would be â€œexclusively competentâ€, which means that the market surveillance authorities in the respective EU member states would no longer have a supervisory role. The AI Office would also have â€œall the powers of a market surveillance authorityâ€. This expansion in scope of the AI Officeâ€™s responsibilities does not apply to high-risk AI systems covered by an Annex I EU product safety law. Therefore, this change primarily impacts high-risk AI systems listed in Annex III and transparency-requiring AI systems regulated by Article 50 (where such AI systems leverage general-purpose AI models). Finally, under this proposal, the AI Office would also have exclusive competence as the regulator of â€œAI systems that constitute or that are integrated into a designated very large online platform or very large online search engineâ€ (as defined and regulated by the Digital Services Act).   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 18/23\n\nWhat impact would this have? \n\nThe implied rationale behind this change is that the Commission does not think it makes sense for the AI Office to be responsible for overseeing providers of general-purpose AI models but not the AI systems developed, made available, and put into service by those same providers. These changes would make the AI Office the supervisory authority for many of the most widely used AI systems worldwide. This is because most mainstream generative AI platforms, such as ChatGPT, Gemini, Claude, Grok, and Microsoft Copilot, are AI systems built on general-purpose AI models, with the same organisation being the provider of both the AI model and the AI system. Therefore, this would represent a meaningful increase in the relevance and prominence of the AI Office for AI Act oversight and enforcement, and it would also enable the AI Office to pursue investigations and enforcement action relevant for both general-purpose AI model and AI system compliance in a coordinated manner. This would also mean that certain AI system providers would not be subject to regulatory investigations and enforcement action across multiple EU member states, which is possible if the law isnâ€™t amended.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 19/23\n\nThe AI Act provides an element of flexibility and proportionality for micro, small, and medium-size enterprises (SMEs), including start-ups. For example, the compliance penalties which SMEs can face are capped in the following way: 35 million EUR or 7% of total worldwide annual turnover (whichever is lower). 15 million EUR or 3% of total worldwide annual turnover (whichever is lower). 7.5 million EUR or 1% of total worldwide annual turnover (whichever is lower). This contrasts with the â€œwhichever is higherâ€ penalty logic that applies for all other businesses (i.e., those which are not SMEs). In practice, this means that many non-SME businesses could face potential penalties into the billions of euros, whereas penalties for SMEs will always be capped as per the above. Other ways in which the AI Act seeks to ease compliance burdens for SMEs include allowing SME providers of high-risk AI systems to provide the required technical documentation in a simplified way and providing SMEs with free access to AI regulatory sandboxes. \n\n6. Proportionality for small mid-cap (SMC) enterprises   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 20/23\n\nWhat changes are being proposed? \n\nThe first proposed change is to add legal definitions of SME and small mid-cap enterprise (SMC) to the AI Act. These are: SME: an enterprise which employs fewer than 250 people and which has an annual turnover not exceeding 50 million EUR, and/or an annual balance sheet total not exceeding 43 million EUR. SMC: an enterprise which employs fewer than 750 people and which has an annual turnover not exceeding 150m EUR or an annual balance sheet total not exceeding 129m EUR. The second and more significant proposed change is to extend the flexibility and proportionality penalties afforded to SMEs to SMCs also. This means that SMCs would benefit from the same capped enforcement penalty regime as SMEs, significantly reducing their total potential penalty exposure in certain circumstances. SMCs that are providers of high-risk AI systems would also be able to provide the required technical documentation in a simplified manner. \n\nWhat impact would this have?   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 21/23\n\nCore threads from the Draghi report are woven throughout this proposal. The Commission will be hoping that easing regulatory compliance burdens and softening the enforcement environment for a larger pool of companies will make it easier for EU digital start-ups and scale-ups to grow, innovate, and compete internationally. Indeed, the Draghi report argued that â€œregulatory burdensâ€ are particularly damaging for digital sector SMEs trying to rapidly scale up. \n\nThanks for reading! Subscribe below for weekly updates from Enterprise AI Governance. \n\n> 12 Likes âˆ™ 2 Restacks\n\nDiscussion about this post \n\nWrite a comment...", "fetched_at_utc": "2026-02-08T18:50:56Z", "sha256": "0ecedae03efafe388dc2480f5790ec48727b1ad86b24f1ab8e3fe20381d4f18c", "meta": {"file_name": "How could the EU AI Act change - Oliver Patel.pdf", "file_size": 802855, "relative_path": "pdfs\\How could the EU AI Act change - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 5622}}}
{"doc_id": "pdf-pdfs-initial-reflections-on-agentic-ai-governance-oliver-patel-67545ac8c4be", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Initial reflections on agentic AI governance - Oliver Patel.pdf", "title": "Initial reflections on agentic AI governance - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 4:48 PM Initial reflections on agentic AI governance \n\nhttps://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai  2/29 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! ICYMI: visit this page to download my free 20-page AI Usage Policy Playbook and register interest for my upcoming AI Usage Policy Bootcamp .This weekâ€™s edition is an essay on agentic AI governance . It covers: \n\nâœ… What is agentic AI? \n\nâœ… How agentic AI is used today and how it could be used in future? \n\nâœ… What novel risks does agentic AI pose? \n\nâœ… How do these risks challenge existing AI governance frameworks? \n\nâœ… What new policies, standards, and guardrails are required to address these challenges and mitigate risk? The key message is that urgent work is required, within the AI governance community, to refine and update our approach to AI governance and risk management in the   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 3/29\n\nagentic AI era. I am very keen for comments and feedback, to guide my thinking and work in this emerging field. However, I also donâ€™t want to waste anyoneâ€™s time. Therefore, if you donâ€™t want to read a 4,000-word essay on agentic AI governance, then this may not be for you. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week. \n\nThis essay outlines my initial reflections on agentic AI governance. The phrase â€˜initial reflectionsâ€™ is designed to serve as a health warning for all readers. Large language model (LLM) based AI agents are a relatively new technology. There are not many enterprise use cases in production, at least compared to generative AI and traditional machine learning. \n\n## Initial reflections on agentic AI governance   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 4/29\n\nFurthermore, there are not yet any laws, standards, frameworks, or guidelines which directly address or stipulate how the novel risks of agentic AI should be mitigated. Finally, not much has been researched or published on the topic of agentic AI ethics and governance. However, I will reference some of what has been written throughout this essay. Considering the above, it is reasonable for you to ask why I am writing and publishing this today. Well, the main reason is that agentic AI is comingâ€”whether the AI governance community is ready or not. As we should all appreciate by now, technology is not going to wait for us to figure things out. As a community, we had several years to discuss, align on, and codify the key ethical principles, policies, and standards for AI, before enterprise adoption of machine learning became mainstream. And, although our response and adaptation timelines were accelerated for generative AI, the launch of ChatGPT was a landmark moment that made it obvious there was pressing work for us to do.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 5/29\n\nWith agentic AI, the challenge is twofold. Not only is time in short supply, I also do not anticipate that there will be a watershed moment that highlights the urgency of agentic AI governance. It is, and will continue to, creep up on us and permeate our organisations. For this reason, there is a serious possibility that we may fail to identify and grasp the unique challenges and risks which agentic AI poses in time for the inevitable proliferation of use cases, adoption at scale, and democratisation. This could leave our organisations and stakeholders exposed, with an AI governance and risk management framework that is no longer fit for purpose in the agentic AI era. I hope that my â€˜initial reflectionsâ€™ essay can help to address this challenge. However, please take everything I say with a pinch of salt, as this is not a peer-reviewed paper, and there is a lot of work for me to do to fully wrap my head around this complex topic. You will perhaps be unsurprised to read that there is not yet an internationally agreed definition of agentic AI. Here are some industry definitions, to get us started: \n\nWhat is agentic AI?   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 6/29\n\nâ€œAI agents are software systems that use AI to pursue goals and complete tasks on behalf of usersâ€ Google \n\nâ€œAgentic AI systems can accomplish a specific goal with limited supervisionâ€ IBM \n\nâ€œAgentic AI systems act autonomously, make decisions, and adapt dynamically to complex environmentsâ€ Kieran Gilmurray \n\nCentral to all definitions of agentic AI is the concept of proactive and autonomous completion of tasks. In his new book Kieran Gilmurray outlines the three waves of AI: predictive AI, generative AI, and agentic AI. With traditional machine learning, models generate predictive outputs, like scores, classifications, and recommendations. With generative AI, models generate content, like text, video, or code. These predictive or generative AI outputs are typically provided to humans via a user interface and are then used by those humans to assist, augment, or optimise their work. With agentic AI systems, the work itself may no longer be done by the humans.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 7/29\n\nAgentic AI systems can autonomously develop plans, solve problems, retrieve data, leverage memory, use tools, and execute tasks in a range of other applications which they are integrated with. They do so by constantly processing inputs, learning from, and adapting to their environment, and proactively determining the best course of action to take. Hence the notion of â€˜agencyâ€™. AI agents are powered by LLMs and APIs. The LLM enables the system to process the initial instructions and mimic reasoning to break down complex problems into a series of smaller steps to be executed. The APIs enable integration with a range of other applications, tools, and databases, to retrieve data and make things happen. Because agents are LLM-powered, they are unpredictable and non-deterministic, in contrast to more traditional forms of software automation and workflows, like RPA; more on that below. As you can imagine, the potential spectrum of agentic AI use cases is limitless. However, there are not a huge number of AI agents in production today. This is still a \n\nWhat are the use cases for agentic AI?   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 8/29\n\nnovel (and by extension relatively unreliable and untested) technology. \n\nIBM has declared 2025 as the year of agentic exploration. Each organisation will likely witness a proliferation of PoCs and pilots this year, just like we have seen with generative AI over the past two and a half years. In this early wave of agentic AI use cases, there is an emphasis on assistive and productivity enhancing tasks, such as research, summarisation, and information retrieval. A survey by LangChain finds that the most popular agentic AI use cases today are research and summarisation, personal assistance and productivity, customer service, and code generation. In many cases, AI agents are already working behind the scenes, without our awareness, to improve the performance and effectiveness of the most widely used generative AI applications, like Perplexity and ChatGPT. For example, the current crop of â€˜deep researchâ€™ tools function by leveraging teams of AI agents which collaborate with each other to scour the internet and other data sources to retrieve, collate, assess, merge, and summarise relevant information, to augment and enhance the final AI-generated â€˜reportâ€™ or response which the user receives.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 9/29\n\nThe architecture of such applications has become much more sophisticated than one model receiving a prompt, performing inference, and generating a predictive output. Looking ahead, the thinking is that agentic and multi-agentic systems will be able to take on increasingly complex tasks and projects, such as managing customer service interactions, planning and booking holidays, planning, creating, and posting social media content, and managing investment portfolios. Before we get there, we need a bulletproof approach to agentic AI governance. Although there is currently ample hype (some of which is inevitably overblown), this is not an excuse to ignore or disregard this technological trend. Agentic AI poses unique risks, which the AI governance community cannot afford to overlook. The risks of AI stem primarily from the way the technology is used and the real-world impact this use can have. \n\nThe novel risks and challenges of agentic AI   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 10/29\n\nTherefore, even if agentic AI is based upon the same underlying technology as generative AI (i.e., LLMs), this does not mean it will be used in the same way. The deployment and use of agentic AI, and thus its impact on people, organisations, and society, will be markedly different to what has come before. Increasingly autonomous capability enables novel AI use cases, such as control of computers and automation of dynamic, data-intensive processes in sensitive areas like supply chain management and logistics planning. Furthermore, it is conceivable that, in future, knowledge workers will have access to their own personalised AI agent, to assist with all aspects of their work. Taken together, these examples represent a meaningful shift from how AI is used today. To illustrate the novel risks, I will focus on four themes of the utmost importance for agentic AI: \n\n1. â€˜Human out of the loopâ€™ \n\n2. Autonomous task execution and performance   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 11/29\n\n3. Adaptiveness and unpredictability \n\n4. Data, privacy, and cyber security In the agentic AI era, all AI risks are amplified. Virtually all existing AI risk themes, such as bias, transparency, copyright, explainability, alignment, sustainability, and labour market disruption remain as relevantâ€”if not more soâ€”than ever. However, my goal here is to focus on the most novel challenges posed by agentic AI. \n\n1. Human out of the loop \n\nIt is not an exaggeration to state that the purpose of agentic AI is to take the human out of the loop. Why plan and book your own holiday when an AI agent can do it for you? Why respond to all of your fans and followers across multiple social media platforms when an AI agent can take care of the correspondence? Why employ hundreds of call centre workers when an army of autonomous agents can do the job? However, this is in direct tension with the concept of human in the loop, which is a foundational pillar of AI governance. By delegating and outsourcing tasks to AI agents, humans may be freed to focus their   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 12/29\n\ntime and energy elsewhere. However, AI agents will also be trusted to take on increasingly important tasks, with diminishing human oversight. The key risk is that we become overly trusting of agentic AI systems and take the human out of the loop to a degree which becomes dangerous. In the quest for efficiency gains, we may underestimate the level of human oversight required for safe agentic deployment. This risk is especially pertinent at first, as we do not truly understand the limitations and capabilities of agentic AI systems. An associated challenge, discussed below, will be the complexity of refining and updating our approach to human oversight. This will require defining exactly when human review and approval is required before an action can be taken. Moreover, if the AI agent executes the action, it may become even harder to determine which human, or entity, should be held accountable for it. This will need to be codified at the outset of agentic development. \n\n2. Autonomous task execution and performance   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 13/29\n\nIf you thought AI hallucinations were bad, wait till you learn about â€˜cascading hallucinationsâ€™. OWASP describes this as when an â€œAI agent generates inaccurate information, which is then reinforced through its memory, tool use, or multi-agent interactions, amplifying misinformation across multiple decision-making stepsâ€. This can lead to self-reinforcing destructive behaviours and systemic failures in agentic AI performance. Agentic AI raises the stakes for the hallucination problem. An LLM hallucination is primarily a problem if the user naively fails to verify the accuracy of the output before relying on or using it. However, an LLM hallucination which directly informs and shapes the course of action taken by an AI agent (or team of agents) could have severe consequences, if that agent is being trusted to execute tasks in a high-risk domain. The more autonomous AI agents become, and the more we trust those agents to take over tasks and projects in sensitive areas, the greater the risk and negative impact of malfunction, error, and performance degradation. Although AI performance concerns are nothing new, without appropriate controls, such as human oversight and observability, there is a risk that the agents we begin to trust let us down, without us even realising at first.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 14/29\n\nFor example, if the AI agent is autonomously handling and responding to customer complaints, how many inappropriate interactions and unnecessary follow ups could occur before this is flagged and addressed? It is also critical that the appropriate agent performs the appropriate task. In the modern enterprise there will be many agents, working together and supervising each other in complex hierarchies, based on their pre-defined roles, permissions, and guardrails. Therefore, the reliable performance of agents towards the top of the hierarchy is critical for the performance and effectiveness of all the other agents. By taking the human out of the loop and allowing AI agents to autonomously execute tasks, it is undeniable that there are immense potential efficiency and productivity gains. However, with increased autonomy comes increased risk. There may be some domains where we simply cannot afford agentic AI mistakes. A broader question is whether we are building these exciting new tools on the relatively shaky foundation of a technology which was ultimately designed to predict the next word, rather than perform important tasks.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 15/29\n\nResearchers from Hugging Face have sounded the alarm bell and argued that fully autonomous agents should not be developed, due to the unacceptable potential risks resulting from system inaccuracy, privacy and security breaches, spread of false information, and loss of human control. \n\n3. Adaptiveness and unpredictability \n\nAI agents are unpredictable precisely because they are proactive. If we knew or could consistently guess what they were going to do, they would not have agency in any meaningful sense. LLMs are non-deterministic, which means models can generate different outputs in response to the same inputs. This leads to unexpected, unpredictable, and unreliable outputs. This will inevitably be reflected in the behaviour and performance of AI agents, which will rely on the effectiveness of LLMs and their ability to accurately predict the next word. Given the proactive and dynamic way in which AI agents respond and adapt to their environment, it could become virtually impossible to predict and anticipate how they will behave, and therefore the risks that could emerge. This makes AI risk assessment, and therefore AI risk mitigation, much more challenging than it is today. It also requires much more continuous and comprehensive monitoring   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 16/29\n\nof AI performance. It is hard enough to predict the risks of downstream general-purpose AI usage, let alone the potential behaviour and risks of autonomous agents, which are integrated with a range of other applications, and empowered to solve complex and open-ended problems in whichever way they see fit. \n\n4. Data, privacy and cyber security \n\nData, privacy and cyber security risks are nothing new for AI. However, these risks are exacerbated by agentic AI. Agentic AI systems could easily mine and retrieve data from sources which they were not supposed to have access to, or sources which are not permitted to be used for AI processing or text and data mining. This could include copyrighted material without an appropriate license, or sensitive personal data originally collected for a different, more narrow purpose. Furthermore, there is also the risk of AI agents disclosing and revealing data to people who were not authorised to have access to it. Agents will be performing and automating increasingly personalised tasks, such as booking medical appointments, whilst having access to and being trained on huge amounts of personal data, such as medical records.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 17/29\n\nThis elevates the risk of data breaches and information leakage. Therefore, encoding privacy by design and data governance guardrails will be a challenging but necessary part of agentic AI governance. Agentic AI systems will also become attack surfaces. Nefarious actors will undoubtedly attempt to take control of, and manipulate, the autonomous systems which themselves may control important spheres of business activity. There is a lot you can do if you control an agentic system which itself can control computers with access to sensitive data and applications. Also, in situations where AI agents are trusted to both generate and execute code, the risk of autonomously executed (and non-vetted) malicious code creeping in to production applications increases. In a recent report , OWASP outlines 15 unique threats which agentic AI systems are vulnerable to. This includes cascading hallucination attacks, resource overload, tool misuse, and rogue agents in multi-agent systems. â€˜Traditional' AI governance frameworks, such as the EU AI Act and NIST AI Risk Management Framework, were developed during a time when traditional machine \n\nPolicies, guardrails and controls to manage agentic AI risks   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 18/29\n\nlearning and then generative AI was prevalent. They do not directly address many of the novel risks and challenges of agentic AI, discussed above. Indeed, if the EU AI Act was being drafted today, I am certain that some of my below points would be directly addressed in the law. However, we cannot rely on, or wait for, the regulators to come and save us with new guidance or updated standards. They will, rightly so, expect industry to figure out how to develop and implement agentic AI in a manner which is safe, secure, and respects existing laws, like the EU AI Act. None of the risks highlighted above represent insurmountable problems. I have full faith in the ingenuity of the AI governance community to solve them. Below, I will sketch out the six most important considerations for AI governance professionals seeking to refine, update, and implement policies, guardrails, and controls, to meet the challenge of managing risk in the agentic AI era. This includes: \n\n1. Action permissions and thresholds \n\n2. Integrations and data access \n\n3. Hierarchy and approval matrixes \n\n4. Monitoring, observability, and orchestration \n\n5. Human oversight, accountability, and control   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 19/29\n\n6. Use cases and risk assessments \n\n1. Action permissions and thresholds \n\nConfiguring action permissions appropriately is an essential part of agentic AI governance. Autonomy is a spectrum. Just because an agent can do something does not mean we should let it. We can determine exactly which actions an agent can and cannot perform. The potential behaviour and permissions of agents can be restricted at the system and API level. If there are certain tasks which an agent could in theory perform in a given system or environment, or certain tools the agent could use, and we do not want the agent to do so, we can specify and encode these restrictions. This sounds simple enough at first. For example, in a financial context, we may not want the agent to execute or process any transaction, or make any decision, which carries a financial value over a certain amount. Similarly, we may want to restrict the ability of an agent to execute AI-generated code in certain applications.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 20/29\n\nWhat is more challenging is to define generally applicable policy principles, which can be used to determine, for any use case in any domain, what type of action permissions and restrictions we should impose, and what thresholds we should use to guide this. Some potential action permission threshold categories could be: Financial value Direct impact on people Number of people impacted Impact on patients Importance of the decision or action on the business Potential ethical risk (e.g., EU AI Act high-risk AI system categories) \n\n2. Integrations and data access \n\nOn a similar note, we can also determine and restrict which applications an agent is integrated with, as well as which data it has access to. As well as enabling privacy by design and data governance, this also supports the points raised above relating to access restrictions.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 21/29\n\nIf an agent is unable to access certain data and/or is not integrated with the application where a particular task is performed or tool is used, then it will be unable to use that data or tool to do something which we do not want it to do. Again, we will need to formulate generally applicable policy principles which can steer us in our assessment of which applications agents should and should not be integrated with, as well as which datasets should and should not augment their knowledge base. \n\n3. Hierarchy and approval matrixes \n\nIn the modern enterprise there will be countless agents working together, in complex hierarchies of agentic collaboration, supervision, and oversight. There will need to be a clearly defined RACI or matrix for AI agents, which outlines the roles, responsibilities, and segregation of duties. It is crucial that agent Y only performs tasks within its permitted duties and that agent X does the same. Agents towards the top of the hierarchy will be empowered to review, approve, authorise, and restrict the work of other agents. And agents lower down in the hierarchy should not be allowed to operate in a way which circumvents the authority of their superiors.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 22/29\n\nThis will require both complex engineering and architectural design, as well as a new conceptual framework for AI governance professionals to lean on. \n\n4. Monitoring, observability, and orchestration \n\nWe are moving from MLOps and LLMOps to AgentOps. In the â€˜old worldâ€™, MLOps is used to validate, test, and monitor model performance, including robustness and accuracy. This primarily focuses on the predictive outputs that are generated and how they perform across a range of key metrics. With AgentOps, the goal is to automate the monitoring, oversight, and orchestration of agentic and multi-agentic AI systems, so we can keep tabs on what actions they are performing, how they are behaving, which tools they are using, the impact this is having, and ultimately, whether we can trust them to keep working on our behalf. There should also be visibility as to whether any agents are operating contrary to their guardrails and permissions. Assessing and evaluating agentic AI performance also entails additional complexity, at least compared with traditional machine learning performance evaluation. This is because the actual tasks that agents perform are much more wide-ranging, varied, and hard to anticipate, given the proactive nature of agentic AI. Therefore, we   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 23/29\n\nwill need updated and rigorous performance and accuracy metrics, which can account for the variety of possible tasks and agent could perform. \n\n5. Human oversight, accountability and control \n\nWhat we mean by human oversight will also require a refresh. It will no longer make sense to mandate human review and approval of each AI-generated output, when the purpose of agentic AI is to take the human out of the loop, to automate processes and drive efficiencies. If the goal is AI autonomy, humans cannot review everything. However, this does not mean human oversight is no longer relevant. For example, human oversight could mean reviewing the ultimate output of an agentic AI system, such as a â€˜deep research report or generated code (as opposed to all the outputs generated and decisions taken to reach its conclusion and generate that final output). Human oversight could also mean having a human in the loop to review actions, tasks, and decisions which meet a certain threshold or risk level, which could be aligned to the action permissions and thresholds detailed above. We will need clearly defined   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 24/29\n\ntouch points for human oversight and review, and it will be more nuanced than what we have today. Finally, humans must always have the ability to override or shut down an agentic AI system, no matter how much autonomy we have empowered it with. According to a LangChain survey on agentic AI usage in 2025, very few companies are allowing agents to freely read, write, and delete data and information from the applications they are operating in and the databases they have access to. Rather, agents are given read-only permissions, with human approval required for significant actions. \n\n6. Use cases and risk assessments \n\nFinally, it is important to determine which agentic AI use cases should be off limits, at least for now. The EU AI Act serves as a useful starting point. AI agents should obviously not be used to for anything which constitutes a prohibited AI practice. Furthermore, I would also advise extreme caution in using agents to autonomously perform tasks, which can have a material impact on decision making or a process, in a domain relating to high-risk AI systems, such as recruitment, critical infrastructure safety management, or determining eligibility for welfare payments.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 25/29\n\nFor one, there is no evidence that agentic AI systems can yet be trusted to perform to a high enough standard required for these sensitive domains. Furthermore, it will be challenging to comply with the AI Actâ€™s obligation for deployers to assign human oversight and the GDPRâ€™s restrictions on solely automated decision-making, whilst also leveraging autonomous agentic AI systems to automate decision-making in sensitive and high-risk domains. However, you will need to look beyond EU law in your work to determine what use cases are appropriate, inappropriate, and off limits for your organisation. Consider the fundamentals of what agents can and cannot do, as well as their strengths and weaknesses. Google, for example, highlights that agents struggle with and should not be used for tasks requiring empathy and emotional intelligence, complex human interactions, high-stakes ethical decision-making, and the navigation of unpredictable physical environments. Once you have figured this all out, you will also need to update your approach to risk assessments, as well as your supporting guidance and training. The key question which needs to be answered throughout is when is it safe to use agentic AI and when is it not?   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 26/29\n\n*The purpose of this essay is to highlight some of the novel risks and governance challenges of agentic AI. Whilst I am not proposing a complete overhaul of AI governance frameworks and policies, the considerations I have outlined above should serve as a starting point for refining and updating your organisationâ€™s approach to AI governance in the agentic AI era. If you have made it this far, I would greatly appreciate comments and feedback. Thank you! \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.", "fetched_at_utc": "2026-02-08T18:51:00Z", "sha256": "67545ac8c4be65b85732e7ce73aa33544f3f1e9643db47caba4ba952cdca6db0", "meta": {"file_name": "Initial reflections on agentic AI governance - Oliver Patel.pdf", "file_size": 814142, "relative_path": "pdfs\\Initial reflections on agentic AI governance - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 6402}}}
{"doc_id": "pdf-pdfs-netherlands-ai-act-guide-3015a2b513d5", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Netherlands AI Act Guide.pdf", "title": "Netherlands AI Act Guide", "text": "AI Act Guide \n\n# Version 1.1 â€“ September 2025 AI Act Guide  | 2\n\nCover image: Wes Cockx & Google DeepMind / BetterImages of AI / AI large language models / CC-BY 4.0 AI Act Guide  | Guide to reading and disclaimer  3\n\nGuide to reading this document and disclaimer \n\nYou develop AI systems or are considering using them in your organisation. In that case, you may \n\nwell come into contact with the AI Act. This guide has been prepared as a tool to provide you with \n\neasy insight into the key aspects of the AI Act.  No rights may be derived from the content of this \n\nguide.  The legal text of the AI Act always takes precedence. \n\nPlease submit any feedback on this guide  to  ai-verordening@minezk.nl . Your feedback will be \n\nused to improve future editions of the guide. \n\nIf you are reading a paper version of the guide , visit  ondernemersplein.nl  for the latest version. \n\nThe website also provides references to the latest guidelines from the European Commission. At the \n\ntime of publication, guidelines have been issued regarding the definition of AI, prohibited AI and \n\nGeneral Purpose AI models. \n\n# The AI Act \n\nThe AI Act is an extensive legal document governing Artificial Intelligence (AI) for the entire European Union \n\n(EU). The AI Act contains rules for the responsible development and use of AI by businesses, government \n\nand other organisations. The aim of the regulation is to protect the safety, health and fundamental rights \n\nof natural persons. Application of the regulation means that organisations can be certain that the AI they \n\nuse is responsible and that they can enjoy the benefits and opportunities offered by AI. \n\nThe regulation will be introduced in phases and the majority will apply as from mid-2026 onwards. \n\nA number of AI systems have already been prohibited since February 2025. Given this situation, it is \n\nimportant that you make the necessary preparations. To help you in that process, this guide lists the \n\nmost important provisions from the AI Act. However, no rights may be derived from the information \n\ncontained in this document. Its sole purpose is to provide support. Click  here 1 for the complete text of \n\nthe regulation. \n\n## What does the AI Act mean for your organisation? \n\nDepending on the type of AI system and the use to which the organisation puts that system, requirements \n\nwill be imposed on its development and use. Whether requirements are imposed will among others depend \n\non the risk the AI system represents to safety, health and fundamental rights. Different requirements will \n\nbe imposed on organisations that develops an AI-system or has it developed than on organisations that \n\nmake use of AI. To find out what the AI Act means for your organisation, it is important to work through \n\nthe four steps listed below. These steps are explained further in this guide: \n\nStep 1 (Risk):  Is our (AI) system covered by one of the risk categories? \n\nStep 2 (AI):  Is our system â€˜AIâ€™ classified according to the AI Act? \n\nStep 3 (Role):  Are we the provider or deployer of the AI system? \n\nStep 4 (Obligations):  What obligations must we comply with? \n\nNote:  Many other guides and step-by-step plans start at step 2 (is our system â€˜AIâ€™) rather than step 1 \n\n(risk categories). After all, if an AI system does not qualify as â€˜AIâ€™ there are no requirements subject to the \n\nAI Act. However, even for systems which are not categorised as AI according to the AI Act, it is important \n\nto have a clear idea of the risks of the purposes for which they are used. That is why in this AI Act Guide, \n\nwe have chosen to start with the risk categories.  \n\n> 1https://eur-lex.europa.eu/legal-content/NL/TXT/?uri=CELEX:32024R1689\n\nAI Act Guide  | Step 1  4\n\n# Step 1. (Risk) Is our (AI) system covered by one \n\n# of the risk categories? \n\nAll AI systems are subject to the AI Act, but depending on the risk, different requirements are imposed \n\non different categories of system. The risk is determined by the intended application or the product for \n\nwhich the AI system is being developed, sold and used: \n\nâ€¢ Prohibited AI practices:  these AI systems may not be placed on the market, put into service for used \n\nfor certain practices. 2\n\nâ€¢ High-risk AI systems:  these AI systems must satisfy a number of requirements to mitigate the risks \n\nbefore they may be placed on the market or used. 3\n\nOther requirements will also apply to AI models and AI systems capable of performing certain tasks: \n\nâ€¢ General purpose AI models and systems : these models and systems will be subject to specific \n\ninformation requirements. In certain cases, other requirements must be complied with in order to \n\nmitigate risks. 4\n\nâ€¢ Generative AI and chatbots : these applications will be subject to specific transparency requirements \n\ndepending on whether the system is or is not a high-risk system. 5\n\nThe same AI system can sometimes be covered by multiple categories. A chatbot, for example, can be \n\ndeployed for a high-risk application and/or based on a general purpose AI model. AI systems not covered \n\nby any of the categories described above are not required to comply with the requirements from the \n\nAI  Act. Nevertheless, you must remember that AI systems and the development of AI systems may also \n\nbe required to comply with requirements from other regulations such as the General Data Protection \n\nRegulation (GDPR). \n\nTo determine whether you are required to comply with requirements from the AI Act, it is important to \n\nfirst identify the category that covers your AI system. Below we discuss the different risk categories in \n\nmore detail. \n\n## 1.1.  Prohibited AI systems \n\nCertain AI practices bring about an unacceptable risk for people and society. These practices have \n\ntherefore been prohibited since February 2025. This means that these systems may not be placed on \n\nthe market, put into service or used for these practices. These prohibitions apply both to providers and \n\ndeployers since 2 February 2025 (further explanation is provided under  Step 3. Are we the provider or \n\ndeployer of the AI system? on page 12 ).  \n\n> 2\n\nChapter II, Article 5 AI Act.  \n\n> 3\n\nChapter III, Article 6 through to 49 AI Act.  \n\n> 4\n\nChapter V, Article 51 through to 56 AI Act.  \n\n> 5\n\nChapter IV, Article 50 AI Act. Specific transparency requirements will also apply for emotion recognition and biometric \n\ncategorisation systems. As these systems are also high-risk AI systems, they will also have to adhere to the requirement for \n\nthese systems, as described in step 4.2. AI Act Guide  | Step 1  5\n\nProhibited AI systems 6\n\n1.  Systems intended to  manipulate human behaviour  with a view to restricting the free choice of \n\nindividuals and which can result in significant harm to those persons. \n\n2.  Systems which  exploit the vulnerabilities  of persons due to their age, disability or a specific social \n\nor economic situation and which are likely to cause significant harm to those persons. \n\n3.  Systems that draw up a point system of rewards and punishments based on social behaviour \n\nor personality traits, known as  social scoring , which could lead to detrimental or unfavourable \n\ntreatment. \n\n4.  Prohibition on systems for making  risk assessments to predict the risk of a person committing \n\na criminal offence , based solely on profiling or personality or other traits. \n\n5.  Systems which create or expand  facial recognition databases  through the untargeted  scraping \n\nof facial images from the internet or CCTV footage. \n\n6.  Systems for  emotion recognition  in the workplace and in education institutions, except where \n\nintended for medical or safety reasons. \n\n7.  Systems used for categorising  individual persons using biometric categorisation systems  in \n\ncertain sensitive categories such as race and sexual orientation. \n\n8.  The use of real-time remote biometric identification systems in publicly accessible spaces \n\nfor the purposes of law enforcement.  There are a number of exceptions in cases in which use \n\nis strictly necessary, for example when searching for specific victims of obduction, trafficking in \n\nhuman beings or missing persons. These applications are subject to additional guarantees. \n\n## 1.2.  High-risk AI systems \n\nHigh-risk AI systems may result in risks to health, safety or fundamental rights of natural persons, such \n\nas the right to privacy and the right not to be discriminated. At the same time, these systems can also \n\nhave positive impact on natural persons and organisations, if they are reliable and the risks are mitigated. \n\nAgainst that background, from August 2026 onwards, high-risk AI systems must comply with a variety \n\nof requirements before being placed on the market, used or put into service. This means that during the \n\ndevelopment of the system,  providers  must ensure that the system satisfies these requirements before \n\nit is first placed on the market or used. A professional party that uses the AI system subject to its personal \n\nresponsibility is considered a  deployer  (explained in more detail under  Step 3. Are we the provider or \n\ndeployer of the AI system? on page 12 ). \n\nDeployers are also subject to obligations with the aim of mitigating risks resulting from the specific use of \n\nthe system. There are two types of high-risk AI systems: \n\nâ€¢ High-risk products : AI systems that are directly or indirectly also subject to a selection of  existing \n\nproduct regulations  (see below). For example an AI system as a safety component of a lift or an AI \n\nsystem that in and of itself is a medical device. \n\nâ€¢ High-risk applications : AI systems developed and deployed for specific applications in â€˜high-risk \n\napplication areasâ€™. These are eight application areas for AI that range from AI for law enforcement to AI \n\nin education. Within those eight areas, around 30 different specific applications have been identified \n\nthat result in high risks, such as AI systems that support the deployment of emergency first response \n\nservices. \n\nThe product groups and application areas in which AI systems are categorised as high-risk appear in the \n\nfigures below. \n\nThe obligations for this category also apply to high-risk products as from 2 August 2027 and to high-risk \n\napplication areas as from 2 August 2026. The obligations are described under  4.2. High-risk AI on page 13 . \n\n> 6Article 5 AI Act, see also Commission Guidelines on prohibited artificial intelligence practices.\n\nAI Act Guide  | Step 1  6\n\nHigh-risk AI as (safety element of) existing products \n\nThese are products already regulated within the EU. A product is considered as representing a risk if \n\nin accordance with existing product regulations, third-party approval is required before the product \n\ncan be placed on the market (conformity assessment). If AI is a safety-related component of the risk \n\nproduct or if the risk product itself is an AI system, it is considered as high-risk AI. This applies to \n\nproducts covered by the following product legislation: 7\n\nâ€¢ Machines  (Directive 2006/42/EC) \n\nâ€¢ Toys  (Directive 2009/48/EC) \n\nâ€¢ Recreational craft  (Directive 2013/53/EU) \n\nâ€¢ Lifts  (Directive 2014/33/EU) \n\nâ€¢ Equipment and protective systems intended for use in potentially explosive atmospheres \n\n(Directive 2014/34/EU) \n\nâ€¢ Radio equipment  (Directive 2014/53/EU) \n\nâ€¢ Pressure equipment  (Directive 2014/68/EU) \n\nâ€¢ Cableway installations  (Regulation (EU) 2016/424) \n\nâ€¢ Personal protective equipment  (Regulation (EU) 2016/425) \n\nâ€¢ Appliances burning gaseous fuels  (Regulation (EU) 2016/425) \n\nâ€¢ Medical devices  (Regulation (EU) 2017/745) \n\nâ€¢ In-vitro diagnostic medical devices  (Regulation (EU) 2017/746) \n\nIn addition, the AI Act contains a further list of products also considered as high-risk AI, but which \n\nare not subject to any direct requirements under the AI Act. Nevertheless, at a later moment, the \n\nrequirements from the AI Act will be used to clarify the specific product legislation applicable to \n\nthese products. It is not yet known when this will take place, and it will differ from product to \n\nproduct. The products in question are subject to the following product legislation: 8\n\nâ€¢ Civil aviation security  (Regulation (EC) 300/2008 and Regulation (EU) 2018/1139) \n\nâ€¢ Two or three-wheeled vehicles and quadricycles  (Regulation (EU) 168/2013) \n\nâ€¢ Agricultural and forestry vehicles  (Regulation (EU) 167/2013) \n\nâ€¢ Marine equipment  (Directive 2014/90/EU) \n\nâ€¢ Interoperability of the railway system in the EU  (Directive (EU) 2016/797) \n\nâ€¢ Motor vehicles and trailers  (Regulation (EU) 2018/858 and Regulation (EU) 2019/2144) \n\nHigh-risk application areas \n\nAn AI system is within the scope of one of the high-risk application areas if the provider intended the \n\nuse of the AI system in one of these areas. In the documentation of the AI system, the provider must \n\nexplicitly state the purpose, including the instructions for use, advertising materials and any other \n\ntechnical documentation.  Note:  Even if the provider did not intend the AI system as being high-risk \n\nwhen it was placed on the market, it may still be that in practice, a deployer does use the system for \n\none of the high-risk application areas. In that case, the deployer is seen as the provider, and as such \n\nbecomes responsible for the requirements imposed on high-risk AI systems. See also  chapter 4.2 .\n\nThere are eight high-risk application areas. This does not mean that all AI systems covered by \n\nthe often abstractly described application areas are necessarily high-risk. A number of specific \n\napplications are listed for each area. 9\n\nTip:  First check whether your AI system is covered by one of the eight application areas and then \n\ndetermine whether your AI system is one of the AI systems described in that category. Only in that \n\ncase are you dealing with a high-risk AI system that must comply with the requirements.    \n\n> 7Article 6(1) and Annex I, Section A AI Act.\n> 8Article 2(2) and Annex I, Section B AI Act.\n> 9Annex III AI Act.\n\nAI Act Guide  | Step 1  7\n\n1.  Biometrics \n\nâ€¢ Remote biometric identification systems, unless the system is only used for verification. \n\nâ€¢ Systems used for biometric categorisation according to sensitive or protected attributes. \n\nâ€¢ Systems for emotion recognition. \n\n2.  Critical infrastructure \n\nâ€¢ Systems intended to be used as safety components for the management and operation of critical \n\ndigital infrastructure, road traffic or in the supply of water, gas, heating or electricity. \n\n3.  Education and vocational training \n\nâ€¢ Systems for admission to or allocation of (vocational) education. \n\nâ€¢ Systems for evaluating learning outcomes. \n\nâ€¢ Systems for assessing the level of (vocational) education. \n\nâ€¢ Systems for monitoring students during tests. \n\n4.  Employment, workersâ€™ management and access to self employment. \n\nâ€¢ Systems for the recruitment or selection of candidates. \n\nâ€¢ Systems to be used to make decisions affecting terms of work-related relationships, the allocation \n\nof tasks or the monitoring and evaluation of workers. \n\n5.  Essential private services and public services and benefits \n\nâ€¢ Systems for evaluating the eligibility to essential public assistance benefits and services. \n\nâ€¢ Systems for evaluating the creditworthiness or credit score of natural persons unless used for the \n\npurposes of detecting financial fraud. \n\nâ€¢ Systems for risk assessment and pricing in relation to life and health insurance. \n\nâ€¢ Systems for evaluating emergency calls and prioritising the dispatch of emergency first response \n\nservices and emergency health care patient triage systems. \n\n6.  Law enforcement \n\nâ€¢ Systems for law enforcement to assess the risk of a natural person becoming the victim of \n\ncriminal offences. \n\nâ€¢ Systems for law enforcement to be deployed as polygraphs or similar tools. \n\nâ€¢ Systems for law enforcement for evaluating the reliability of evidence. \n\nâ€¢ Systems for law enforcement for assessing or predicting the risk of a natural personal offending or \n\nto assess past criminal behaviour of natural persons or groups. \n\nâ€¢ Systems for law enforcement for the profiling of natural persons in the course of detection, \n\ninvestigation or prosecution of criminal offences. \n\n7.  Migration, asylum and border control \n\nâ€¢ Systems for public authorities to be used as polygraphs or similar tools. \n\nâ€¢ Systems for public authorities for assessing a security risk, the risk of irregular migration or a \n\nhealth risk upon entry into a country. \n\nâ€¢ Systems for public authorities to assist in the examination of applications for asylum, visa or \n\nresidence permit, including associated complaints. \n\nâ€¢ Systems for public authorities for detecting, recognising or identifying natural persons, with the \n\nexception of the verification of travel documents. \n\n8.  Administration of justice and democratic processes \n\nâ€¢ Systems to be used by a judicial authority to assist in applying the law and resolving disputes and \n\nresearching and interpreting facts and applying the law to a concrete set of facts. \n\nâ€¢ Systems for influencing the outcome of an election or referendum or the voting behaviour \n\nof natural persons, with the exception of tools used to support political campaigns from an \n\nadministrative or logistic point of view. AI Act Guide  | Step 1  8\n\nExceptions to high-risk application areas \n\nThere are a number of specific exceptions in which AI systems are covered by one of the application \n\nareas but which are not seen as high-risk AI. This applies where there is no significant risk to health, \n\nsafety or fundamental human rights. This for example applies if an AI system has  no significant \n\nimpact on the outcome of a decision,  for example because the system is intended for :10 \n\nâ€¢ Performing a narrow procedural task; \n\nâ€¢ Improving the result of a previously completed human activity; \n\nâ€¢ Detecting decision-making patterns or deviations from prior decision-making patterns and not \n\nmeant to replace or influence the previously completed human assessment; \n\nâ€¢ Performing a preparatory task to an assessment relevant to one of the high-risk application areas. \n\nIt should also be noted that an AI system used for profiling natural persons cannot make use of \n\nthis exception. If you have determined that your (non-profiling) AI system is subject to one of the \n\nexceptions, you must record this fact and register the AI system in the EU database for high-risk AI \n\nsystems. 11  At a later moment, the European Commission will draw up a list of examples to clarify \n\nwhat is and what is not covered by the exceptions. \n\n## 1.3.  General purpose AI models and AI systems \n\nAn AI model is an essential component of an AI system, but is not an AI system in and of itself. This \n\nrequires more elements, for example a user interface. 12 \n\nA general purpose AI model  (General Purpose AI) can successfully perform a wide range of different \n\ntasks and as such can be integrated in a variety of AI systems. These models are often trained on large \n\nvolumes of data using self-supervision techniques. 13 \n\nThe broad deployability of these models via specific AI systems means that they are used for a wide \n\nrange of applications. These can include high-risk applications. Due to the potential large impact of these \n\nmodels, from August 2025 onwards, they must comply with various requirements. \n\nIf an AI system is based on a general purpose AI model which itself can actually serve multiple purposes, \n\nthen it is a  general purpose AI system. 14 \n\nThe obligations applicable to this category apply from 2 August 2025 and are described under \n\n4.3. General  purpose AI models and systems on page 18 .     \n\n> 10 Article 6(3) AI Act.\n> 11 Article 6(4) AI Act.\n> 12 Consideration 97 AI Act.\n> 13 Article 3(63) AI Act.\n> 14 Article 3(66) AI Act.\n\nAI Act Guide  | Step 1  9\n\n## 1.4.  Generative AI and Chatbots \n\nCertain AI systems are subject to transparency obligations. 15  These are systems with which natural \n\npersons often interact directly. It must therefore be clear to these natural persons that they are \n\ninteracting with AI or that the content has been manipulated or generated. \n\nâ€¢ Systems used for generating audio, images, video or text ( generative AI ); \n\nâ€¢ Systems made for interaction ( chatbots ). \n\nThe obligations applicable to this category apply from 2 August 2026 and are described under \n\n4.4. Generative AI  and Chatbots on page 19 .\n\n## 1.5.  Other AI \n\nSee  4.5. Other AI on page 20  for more information on AI systems not covered by one of the risk \n\ncategories described above.  \n\n> 15 Article 50 AI Act.\n\nAI Act Guide  | Step 2  10 \n\n# Step 2. Is our system â€˜AIâ€™ classified according to \n\n# the AI Act? \n\nThe AI Act imposes regulations on AI systems. There are different ideas about what AI is and what is not \n\nAI. The AI Act offers the following definition, which is intended to demarcate the nature of AI as a product \n\non the market: \n\nâ€œAn AI system means a  machine-based system  that is designed to operate  with varying levels of autonomy  and \n\nthat may exhibit  adaptiveness  after deployment and that, for  explicit or implicit objectives, infers,  from the  input  it \n\nreceives, how to generate  outputs such as predictions, content, recommendations, or decisions  that can influence \n\nphysical or virtual environments .â€ 16 \n\nThese different elements can be present both in the development phase and in the use phase. What is \n\nthe meaning of the terms used in this definition? 17 \n\nâ€¢ Autonomy : This element is satisfied if autonomy is present in a system to a certain extent, even if very \n\nlimited. Systems without any form of autonomy are systems that only operate if human actions or \n\ninterventions are required for all actions by that system. \n\nâ€¢ Adaptiveness:  It is stated that a system â€˜mayâ€™ exhibit adaptiveness after deployment. Although \n\nadaptiveness is therefore not identified as a decisive element, its presence is an indication that the \n\nsystem is an AI system. \n\nâ€¢ It infers how to generate output on the basis of input (capacity to infer):  This relates not only to \n\ngenerating output during the â€˜use phaseâ€™ but also the capacity of an AI system to infer models and/or \n\nalgorithms from data during the development phase. \n\nWhat does this cover? 18 \n\nâ€¢ Systems that make use of  machine learning  in which the system learns how certain objectives can be \n\nachieved on the basis of data. Examples of machine learning are (un)supervised learning, self-super -\n\nvised learning, reinforcement learning and deep learning. \n\nâ€¢ Systems that make use of  knowledge and logic-based approaches  which make learning, reasoning or \n\nmodelling possible on the basis of determined knowledge or a symbolic representation of a task to be \n\nsolved. Examples of these approaches are knowledge representation, inductive (logic) programming, \n\nknowledge bases, inference and deductive engines, (symbolic) reasoning, expert systems and search \n\nand optimisation methods. \n\nWhat is  not  covered? \n\nâ€¢ Systems based on rules laid down exclusively by natural persons to conduct automatic actions. Some \n\nof these systems can to a certain extent derive how to generate output from input received, but are still \n\nbeyond the definition because they are only able to analyse patterns to a limited extent or are unable to \n\nautonomously adapt their output. Examples can be systems for improved mathematical optimisation, \n\nstandard data processing, systems based on classic heuristics and simple prediction systems. \n\nâ€¢ Systems designed to be used with full human intervention. These systems do not operate with a \n\nminimum level of autonomy.  \n\n> 16\n\nArticle 3(1) AI Act.  \n\n> 17\n\nCommission Guidelines on the definition of an artificial intelligence system.  \n\n> 18\n\nConsideration 12 AI Act. AI Act Guide  | Step 2  11 \n\nThe European Commission has issued a guideline to further clarify the definition of AI. You can find the \n\nlatest version on this guideline at  ondernemersplein.nl .\n\nIf your system is not considered AI under the AI Act but is covered by one of the risk categories, it is \n\nimportant to hold a discussion within your organisation about the extent to which the system still \n\nrepresents risks and to mitigate these risks by complying with (specific) requirements from the AI Act. \n\nSystems beyond the scope of the AI Act may nevertheless still have to comply with requirements from \n\nother legislation and regulations. AI Act Guide  | Step 3  12 \n\n# Step 3. Are we the provider or deployer of the \n\n# AI system? \n\nOnce you have determined the risk category that covers you AI system and whether your AI system is in \n\nfact subject to the AI Act, you must then determine whether you are the provider or deployer. \n\nâ€¢ Provider : a person or organisation that develops or commissions the development of an AI system or \n\nmodel and places it on the market or puts the AI system into service. 19 \n\nâ€¢ Deployer : a person or organisation using an AI system under its personal authority. This does not \n\ninclude non-professional use. 20 \n\nThe description of the requirements in step 4 describes for each risk category which obligations apply \n\nto providers and deployers. Each is required to comply with other obligations. The strictest obligations \n\napply to providers. \n\nNote:  As deployer, in certain cases you can also become provider of a high-risk AI system such that you \n\nare required to comply with the high-risk obligations for providers. 21  This is further explained in steps \n\n4.2. High-risk  AI on page 13  and  4.3. General purpose AI models and systems on page 18 .\n\nNote : the AI Act also includes other roles such as authorised representative, importer and distributor. 22 \n\nThe obligations governing these actors are not discussed in this guide.  \n\n> 19\n\nArticle 3(3) AI Act.  \n\n> 20\n\nArticle 3(4) AI Act.  \n\n> 21\n\nArticle 25 AI Act.  \n\n> 22\n\nArticle 3(5), (6) and (7), Articles 22, 23 and 24. AI Act Guide  | Step 4  13 \n\n# Step 4. What obligations must we comply with? \n\n## 4.1.  Prohibited AI practices \n\nThese AI practices result in unacceptable risk and have therefore been prohibited since 2 February  2025. \n\nThis means that AI systems cannot be placed on the market or used for these practices. These prohibitions \n\napply to both  providers  and  deployers. 23 \n\nThere are sharply demarcated exceptions to the prohibition on the use of real-time remote biometric \n\nidentification systems in publicly accessible spaces for the purposes of law enforcement, and the use of \n\nthose systems must be provided with a basis in national legislation. There are also additional guarantees \n\nrelating to the deployment of these systems. \n\n## 4.2.  High-risk AI \n\nThe majority of requirements from the AI Act will apply to high-risk AI systems.  Providers  must comply \n\nwith various obligations such as: 24 \n\nâ€¢ System for risk management; \n\nâ€¢ Data and data governance; \n\nâ€¢ Technical documentation; \n\nâ€¢ Record-keeping (logs); \n\nâ€¢ Transparency and information; \n\nâ€¢ Human oversight; \n\nâ€¢ Accuracy, robustness and cybersecurity; \n\nâ€¢ Quality management system; \n\nâ€¢ Monitoring. \n\nIf you as provider comply or believe you comply with all these obligations, you will be required to \n\nconduct a  conformity assessment . In certain cases you can conduct this assessment yourself, while \n\nin other cases it must be conducted by a third party on your behalf. 25  A future version of this guide will \n\nexplain in more detail when you are required to conduct which procedure. \n\nDeployers  must also comply with various obligations. Additional obligations apply to government \n\norganisations using AI systems. 26 \n\nEach obligation is explained in the figure below. These obligations will be further elaborated over the \n\ncoming years in European standards. Participation will be organised via the standardisation institutes of \n\nthe European Member States. In the Netherlands this is the  NEN 27 .\n\nNote:  In two cases, as deployer of a high-risk AI system, you yourself can become the provider of \n\nthat system: 28 \n\nâ€¢ When you as deployer place your own name or trademark on the high-risk system; \n\nâ€¢ When you as deployer make a substantial modification to the high-risk AI system that was not \n\nintended by the provider as a consequence of which the system no longer complies with the require -\n\nments or as a consequence of which the purpose of the system intended by the provider changes. This \n\nlatter situation arises if you make use of an AI system that was not intended for high-risk applications, \n\nfor any such applications.  \n\n> 23\n\nArticle 5 AI Act.  \n\n> 24\n\nArticle 16 and Section 2 of Chapter III (Articles 8-15) AI Act.  \n\n> 25\n\nArticle 43 AI Act.  \n\n> 26\n\nArticles 26 and 27 AI Act.  \n\n> 27\n\nhttps://www.nen.nl/ict/digitale-ethiek-en-veiligheid/ai . \n\n> 28\n\nArticle 25 AI Act. AI Act Guide  | Step 4  14 \n\n## Requirements for high-risk AI systems \n\n1.  System for risk management 29 \n\nVarious steps must be taken for this system: \n\nâ€¢ Identification and analysis of foreseeable risks the system can pose to health, safety or \n\nfundamental rights. \n\nâ€¢ Taking suitable risk management measures to ensure that the risks that remain following \n\nimplementation of these measures are acceptable. \n\nThe following points must be taken into account: \n\nâ€¢ The risks must be identified and dealt with before the AI system is placed on the market or used \n\nand subsequently continuously during the use of the AI system. \n\nâ€¢ Foreseeable abuse of the system must be taken into account. \n\nâ€¢ The context of the use, including the knowledge and experience of the deployer of such AI \n\nsystems or the fact that children and vulnerable groups may experience consequences of the \n\nAI system, must be taken into account. It may for example be necessary to offer training to the \n\npeople working with the AI system. \n\nâ€¢ The risk management measures must be tested to check that they are actually effective. \n\nThis must be carried out on the basis of benchmarks appropriate to the purpose for which the \n\nAI system is deployed. \n\nâ€¢ If a risk management system must also be established pursuant to existing product legislation, \n\nthis may be combined to form a single risk management system. \n\n2.  Data and data governance 30 \n\nDifferent requirements are imposed on the datasets used for training, validating and testing \n\nhigh-risk AI systems. \n\nâ€¢ Data management appropriate to the purpose of the AI system, including: \n\nâ€¢ The registration of the processes, including processes for data gathering and data processing; \n\nâ€¢ The recording of assumptions about the datasets; \n\nâ€¢ An assessment of the availability, quantity and suitability of the datasets including possible \n\nbiases that could have consequences for natural persons; \n\nâ€¢ Measures for tracing, preventing and mitigating biases; \n\nâ€¢ Tackling shortcomings in the datasets that may prevent compliance with the AI Act (for example \n\nmitigating risks according to the risk management system). \n\nâ€¢ For the purpose for which they are used, datasets must be sufficiently representative and as far \n\nas possible error-free. The context in which the AI system is to be used must also be taken into \n\naccount; for example the geographical or social context. \n\nâ€¢ Subject to a number of strict conditions, special categories of personal data (a term from the \n\nGeneral Data Protection Regulation) may be processed as a means of tackling bias.   \n\n> 29 Article 9 AI Act.\n> 30 Article 10 AI Act.\n\nAI Act Guide  | Step 4  15 \n\n3.  Technical documentation 31 \n\nThe technical documentation must demonstrate that the high-risk AI system complies with the \n\nrequirements laid down in the AI Act. The technical documentation must among others include: \n\nâ€¢ A general description of the AI system including the intended purpose of the system, the name of \n\nthe provider and instructions for use; \n\nâ€¢ A detailed description of the elements of the AI system and of the development process for that \n\nsystem, including the steps in development, the design choices, the expected output from the \n\nsystem, the risk management system and the datasets used; \n\nâ€¢ Detailed information about the monitoring, operation and control of the AI system, including the \n\ndegree of accuracy at individual and general level, risks, the system for evaluation during use and \n\nmeasures for monitoring and human oversight; \n\nâ€¢ An overview of the applicable standards; \n\nâ€¢ The EU conformity declaration (the CE mark). \n\nSME enterprises can record their technical documentation in a simplified manner. At a future \n\nmoment, the European Commission will issue a relevant form. \n\n4.  Record-keeping (logs) 32 \n\nAutomatic logs must be retained during the lifecycle of the AI system so that risks can be traced in a \n\ntimely manner and the operation of the system can be monitored. \n\nThese logs must be stored for at least six months. At least the following events must be recorded: \n\nâ€¢ The duration of each use of the AI system; \n\nâ€¢ The input data and the control of that data by the AI system (and the reference database); \n\nâ€¢ The identification of the natural persons involved in the verification of the results. \n\n5.  Transparency and information 33 \n\nThe provider of the AI system knows how the system operates and how it should be used. For that \n\nreason, the provider must ensure that the AI system is sufficiently transparent so that deployers \n\nunderstand how they can correctly make use of the output from the system. \n\nWith this in mind,  instructions for use  must be drawn up, that include at least the following points: \n\nâ€¢ Contact details; \n\nâ€¢ The purpose, characteristics, capacities and limitations of the performance of the AI system; \n\nâ€¢ The measures for human oversight. \n\n6.  Human oversight 34 \n\nHigh-risk AI systems must be designed by the provider in such a way that during use they can be \n\neffectively overseen by natural persons in order to mitigate the risks for natural persons. Human \n\noversight shall be context dependent â€“ the greater the risks, the stricter the oversight must be. \n\nThe measures for oversight may be technical in nature (for example a clear human-machine \n\ninterface), or measures that must be undertaken by the deployer (for example a compulsory course \n\nfor their personnel).     \n\n> 31 Article 11 AI Act.\n> 32 Article 12 AI Act.\n> 33 Article 13 AI Act.\n> 34 Article 14 AI Act.\n\nAI Act Guide  | Step 4  16 \n\nThe eventual objective of these measures is to ensure that the natural persons who use the \n\nAI system are capable of the following: \n\nâ€¢ They understand the capacities of the system and can monitor its functioning; \n\nâ€¢ They are aware of automation bias; \n\nâ€¢ They can correctly interpret and if necessary ignore or replace the output; \n\nâ€¢ They can halt the system. \n\n7.  Accuracy, robustness and cybersecurity 35 \n\nHigh-risk AI systems must offer an appropriate level of accuracy, robustness and cybersecurity. \n\nTo achieve this, benchmarks and measuring methods are developed by the European Commission. \n\nAt least the following measures must be mentioned: \n\nâ€¢ Technical and organisational measures to prevent errors that occur in interaction between the AI \n\nsystem and natural persons; \n\nâ€¢ Solutions for robustness such as backups or security measures in the event of defects; \n\nâ€¢ Removing or mitigating negative influencing of the system by limiting feedback loops; \n\nâ€¢ Cybersecurity that prevents unauthorised third-party access by tracing, responding to and dealing \n\nwith attacks. These are attacks aimed at data poisoning, model poisoning, adapting input or \n\nobtaining confidential data. \n\n8.  Quality management system 36 \n\nThe quality management system must ensure that the requirements from the AI Act are complied \n\nwith. How extensive the quality management system must be will depend on the size of the \n\norganisation. For example by documenting the following: \n\nâ€¢ A strategy for compliance; \n\nâ€¢ Techniques, procedures and measures for the design, development and quality assurance of the \n\nAI system; \n\nâ€¢ Whether standardisation is used; \n\nâ€¢ Systems and procedures for data management, risk management, monitoring, incident reporting \n\nand documentation. \n\n9.  Monitoring 37 \n\nAs soon as an AI system has been placed on the market or is in use, providers must monitor the \n\nsystem on the basis of use data, thereby determining whether the system continues to comply with \n\nthe requirements from the AI Act. For this purpose, providers must draw up a monitoring plan. \n\nIf the provider of a high-risk AI system discovers that the system no longer functions in compliance \n\nwith the AI Act, corrective measures must be taken immediately to correct the situation. This may \n\neven include recalling the system if necessary. The provider must also work alongside the deployer \n\nand duly inform the surveillance authorities. \n\nSerious incidents involving the AI system must be reported to the surveillance authorities. 38     \n\n> 35 Article 15 AI Act.\n> 36 Article 17 AI Act.\n> 37 Article 72 AI Act.\n> 38 Article 73 AI Act.\n\nAI Act Guide  | Step 4  17 \n\nOther requirements \n\nâ€¢ The registration of the high-risk AI system in the EU database. 39 \n\nâ€¢ The contact details of the provider must be registered with the AI system. 40 \n\nâ€¢ The technical documentation, documentation concerning the quality management system and \n\ndocumentation concerning the conformity assessment must be kept for 10 years. 41 \n\nObligations for deployers of high-risk AI systems \n\nNot only providers but also the deployers of high-risk AI systems are subject to obligations. After \n\nall they are the parties who control how the AI system is used in practice and as such have a major \n\nimpact on the risks that can occur. \n\nDeployers must: 42 \n\nâ€¢ Take appropriate technical and organisational measures to ensure that the high-risk AI system is \n\nused in accordance with the instructions for use; \n\nâ€¢ Assign human oversight to natural persons who have the necessary competence, training and \n\nauthority; \n\nâ€¢ Ensure that the input data is relevant and sufficiently representative, wherever possible; \n\nâ€¢ Monitor the operation of the AI system on the basis of the instructions for use; \n\nâ€¢ If the deployer has reason to believe that the system no longer complies with the requirements \n\nfrom the AI Act, the deployer must duly inform the provider and cease use of the system; \n\nâ€¢ Inform the provider and surveillance authorities of possible risks and serious incidents that have \n\noccurred; \n\nâ€¢ Keep the logbook under their control for at least six months; \n\nâ€¢ Inform worker representation if the AI system is to be deployed on the shop floor; \n\nâ€¢ Duly inform people if decisions are taken about natural persons using the high-risk AI system; \n\nâ€¢ If use is made of AI for emotion recognition or biometric categorisation, the natural persons in \n\nrespect of whom the system is used must be duly informed. \n\nSpecific obligations for government organisations as deployers  In addition to the obligations \n\ndescribed above, government organisations must comply with a number of additional obligations: \n\nâ€¢ Register use of a high-risk system in the EU database; 43 \n\nâ€¢ Assess the potential consequences for fundamental rights if the high-risk AI system is used \n\nwith a view to the specific context within which use takes place (a  fundamental rights impact \n\nassessment ). They will for example consider the duration of use, the processes within which \n\nthe system is used and the potential impact of use on the fundamental rights of natural persons \n\nand groups. Following identification of the risks, deployers must take measures for human \n\noversight and deal with possible risks. A report must also be submitted to the market surveillance \n\nauthorities unless an appeal can be made to an exception based on public safety or protection of \n\nhuman health. 44 \n\nNote:  This obligation also applies to private entities providing public services, the use of AI systems \n\nfor assessing the creditworthiness of natural persons and AI systems for risk assessments for life and \n\nhealth insurance.       \n\n> 39 Article 49 AI Act.\n> 40 Article 16(b) AI Act.\n> 41 Article 18 AI Act.\n> 42 Article 26 AI Act.\n> 43 Article 49(3) AI Act.\n> 44 Article 27 AI Act.\n\nAI Act Guide  | Step 4  18 \n\n## 4.3.  General purpose AI models and systems \n\nObligations for providers of general purpose AI models 45 \n\nGeneral purpose AI models can be integrated in all kinds of different AI systems. It is essential \n\nthat the providers of these AI systems know what the AI model is and is not capable of. Specific \n\nrequirements are also imposed on the training of these models because training often makes use of \n\nlarge datasets. The providers of these models must: \n\nâ€¢ Draw up technical documentation of the model including the training and testing process and the \n\nresults and evaluation; \n\nâ€¢ Draw up and keep up to date information and documentation for providers of AI systems \n\nwho intend to integrate the model in their AI system. The information must provide a good \n\nunderstanding of the capacities and limitations of the AI model and must enable the provider of \n\nthe AI system to comply with the obligations from the AI Act. \n\nâ€¢ Draw up a policy to ensure that they train the model without infringing the copyrights of natural \n\npersons and organisations; \n\nâ€¢ Draw up and publish a sufficiently detailed summary about the content used for training the AI model. \n\nProviders of open source models are not required to comply with the first two obligations (technical \n\ndocumentation and drawing up information for downstream providers). \n\nObligations for providers of general purpose AI models with systemic risks 46 \n\nIn certain cases, general purpose AI models can generate systemic risks. This applies if the model \n\nhas high impact capacity, for example due to the scope of the model or due to (potential) negative \n\nimpact on public health, safety, fundamental rights or society as a whole. This is at least assumed \n\nif at least 10 25  floating point operations (FLOPs) are used to train the model. On the basis of specific \n\ncriteria, the European Commission can also determine that the model has a similar major impact in \n\nsome other way. These models must: \n\nâ€¢ Comply with the obligations for general purpose AI models; \n\nâ€¢ Implement model evaluations to map out the systemic risks; \n\nâ€¢ Mitigate systemic risks; \n\nâ€¢ Record information about serious incidents and report those incidents to the AI Office; \n\nâ€¢ Ensure appropriate cybersecurity. \n\nNote : these obligations only apply to the largest AI models. \n\nProviders of these models with systemic risks cannot appeal to an exception for open source.   \n\n> 45 Article 53 AI Act.\n> 46 Article 55 AI Act.\n\nAI Act Guide  | Step 4  19 \n\nWhat rights do you have if you integrate a general purpose AI model in your (high-risk) AI system? \n\nAs indicated above, you must at least receive information and documentation to enable you to determine \n\nfor yourself how you can make use of the model in your AI system for the chosen purpose. If you include \n\nthe model in a high-risk AI system, as a provider you must then still comply with the obligations from the \n\nAI Act. \n\nHow should you deal with general purpose AI systems?  As indicated in  1.3. General purpose AI models \n\nand AI systems on page 8 , there are also AI systems that can serve multiple purposes. Take for example \n\nthe widely known AI chatbots.  Note:  If you deploy these systems for high-risk purposes, according to the \n\nAI Act you yourself become a provider of a high-risk AI system. 47  It is then up to you to comply with the \n\napplicable obligations. In this situation it is very difficult to comply with the obligations for a high-risk AI \n\nsystem, which means that you may run the risk of receiving a penalty. \n\n## 4.4.  Generative AI and Chatbots \n\nTo ensure that natural persons know whether they are talking to an AI system or are seeing content \n\ngenerated by AI, transparency obligations are imposed on generative AI and chatbots. \n\nRules for providers of chatbots 48 \n\nProviders of systems designed for direct interaction with natural persons must ensure that these \n\nnatural persons are informed that they are interacting with an AI system. \n\nRules for providers of generative AI 49 \n\nProviders of systems that generate audio, image, video or text content must ensure that the output \n\nis marked in a machine readable format so that the output can be detected as artificially generated \n\nor manipulated. \n\nRules for deployers of generative AI 50 \n\nDeployers of systems that generate audio, image or video content must ensure that it is clear that \n\nthe content is artificially generated or manipulated. This can for example be achieved by applying a \n\nwatermark. For creative, satirical, fictional or analogue work, this may be carried out in a way that \n\ndoes not ruin the work. \n\nA special regime applies to artificially generated text. Only in cases where texts are used with the \n\npurpose of â€˜informing the public on matters of public interestâ€™, the fact must be disclosed that the \n\ntext has been artificially generated or manipulated. If there is editorial review and responsibility, this \n\nneed not be carried out. \n\nRules for deployers of emotion recognition systems or systems for biometric categorisation 51 \n\nThe deployers of these AI systems must inform the natural persons exposed to these systems about \n\nhow the system works.      \n\n> 47 Article 25(1)(c) AI Act.\n> 48 Article 50(1) AI Act.\n> 49 Article 50(2) AI Act.\n> 50 Article 50(4) AI Act.\n> 51 Article 50(3) AI Act.\n\nAI Act Guide  | Step 4  20 \n\n## 4.5.  Other AI \n\nAI systems beyond the categories referred to above are not required to comply with requirements \n\naccording to the AI Act. \n\nBut note : if you as  deployer  make use of â€˜other categoryâ€™ AI systems for a high-risk application as \n\nreferred to in the AI Act (see  1.2. High-risk AI systems on page 5 ), it automatically becomes a high-risk AI \n\nsystem and you must comply with the requirements from the AI Act as a system  provider .52  \n\n> 52 Article 25(1)(c) AI Act.\n\nColofon \n\nDit is een uitgave van; \n\nMinisterie van Economische Zaken \n\nPostbus 20901  | 2500 EX Den Haag \n\nwww.rijksoverheid.nl", "fetched_at_utc": "2026-02-08T18:51:37Z", "sha256": "3015a2b513d55c3a2595ac227ceb79dfccae8b256d0a076b69168d4b000aad77", "meta": {"file_name": "Netherlands AI Act Guide.pdf", "file_size": 293902, "relative_path": "pdfs\\Netherlands AI Act Guide.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 9985}}}
{"doc_id": "pdf-pdfs-prohibited-ai-practices-oliver-patel-0436fda42825", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Prohibited AI practices - Oliver Patel.pdf", "title": "Prohibited AI practices - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:04 PM Prohibited AI practices: A compliance guide \n\nhttps://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance  2/18 This free newsletter delivers practical, actionable and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! For more frequent updates, be sure to follow me on LinkedIn .This weekâ€™s edition features: \n\nâœ… Top 5 practical tips for prohibited AI compliance \n\nâœ… What does the EU AI Act say on prohibited AI? \n\nâœ… The European Commissionâ€™s new Guidelines on prohibited AI (140 pages distilled) \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week. \n\nTop 5 practical tips for prohibited AI compliance   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 3/18\n\nMany readers of Enterprise AI Governance are experienced lawyers, compliance executives, and governance leaders seeking practical advice and actionable insights. Therefore, before providing an overview of the prohibited AI practices and the European Commissionâ€™s new Guidelines on the topic, I will frontload this weekâ€™s edition with my practical tips for meeting Article 5 of the AI Act head on. \n\n1. Deliver company-wide communications, awareness, and training campaigns on prohibited AI. AI governance leaders need to constantly remind themselves that although AI is everywhere, the AI Act is still a relatively niche topic. Do not assume that everyone already knows about prohibited AI, just because it is top of mind for us. Indeed, the legal concept of prohibited AI practices is new for us all. Although there are many uses of AI that would of course be illegal, the AI Act is the first EU law which explicitly prohibits specific AI use cases and systems. Therefore, invest ample time in rolling out dedicated training and communications campaigns, so that your organisation is fully aware of these new provisions, their scope, and practical impact. \n\n2. Determine whether prohibited in the EU means prohibited worldwide. And be prepared to justify your decision. A realistic operational scenario that could arise is a team or department in your organisation, based outside of the EU, wanting to use an AI system, which is, or could potentially be, prohibited under   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 4/18\n\nthe AI Act. The use of this AI system is confined to one specific non-EU location. No AI outputs will be used in the EU and no EU employees, customers and stakeholders will be involved in any way. It is feasible that this use case could be perfectly legal in the relevant jurisdiction. Furthermore, perhaps it is not even particularly problematic, from an ethical or cultural standpoint. This raises a key question for global organisations: should prohibited in the EU mean prohibited worldwide? And, if so, what is the justification for this? On the one hand, the EU has prohibited certain AI practices due to their morally egregious nature and/or the potential for harm. However, on the other hand, no jurisdiction has a monopoly on morality. \n\n3. Do not assume that there are no activities in your organisation that breach these provisions. Be wary of the unknown unknowns. You may have a well-established and embedded AI governance process, which captures all new AI projects, initiatives, and vendor applications. It may be that nothing has ever come through which relates to, or closely resembles, any of the prohibited AI practices. Unfortunately, this does not necessarily mean that no such development, deployment, or use of prohibited AI is occurring. Assumptions are dangerous and you donâ€™t know what you donâ€™t know. That is why dedicated training, compliance, and audit exercises are crucial.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 5/18\n\n4. Conduct dedicated compliance initiatives, such as assurance evaluations and audits. In large organisations, where it is difficult to keep track of and monitor everything, there is a need for bespoke compliance initiatives aimed at deterring, preventing, identifying, and halting AI activities which risk falling into the prohibited category. These initiatives are in addition to your BAU AI governance risk assessment processes. Ad hoc mechanisms, such as senior leadership attestation, assurance monitoring and evaluation, and internal and external audits, should all form part of your arsenal. Such initiatives will focus minds, hold leaders to account, and steer behavioural and cultural change in the right direction. \n\n5. Pay close attention to tools which may have emotional recognition capabilities, as there will be plenty of grey areas for companies. Every prohibited AI category has grey areas and exceptions, but perhaps none more so than emotional recognition. For most responsible organisations, it is reasonable to suggest that they are unlikely to develop, deploy, or use AI systems in most of the prohibited AI categories. Emotional recognition could be an outlier, as it is something that could be inadvertently or unintentionlly used, or turned on, in applications designed for recruitment, coaching, training, productivity, and employee and customer engagement or support. Furthermore, it may be that similar tools can lawfully be used to predict and monitor performance, sentiment, or behaviour, but close attention will need to be paid to ensure this does not cross over into emotional recognition. It is also not necessarily a capability which is   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 6/18\n\ndeemed socially, culturally, or morally unacceptable in every jurisdiction worldwide. \n\nBonus tip : without a robust AI governance framework and risk assessment process, which is based upon a solid policy foundation, sponsored by senior leadership, and embedded across the organisation, you will be much less likely to identify and prevent any prohibited AI practices. Always prioritise establishing the fundamental building blocks of AI governance, before turning your attention elsewhere. Article 5 of the EU AI Act may only take up three out of 144 pages, but it is probably the most consequential section. It outlines the AI practices that are now prohibited under EU law. The provisions on prohibited AI practices became applicable on 2nd February 2025, exactly 6 months after the AI Act entered into force on 1st August 2024. The prohibited AI practices are deemed to be particularly harmful, abusive, and contrary to EU values. Breaching these rules carries the largest potential enforcement \n\nWhat does the EU AI Act say on prohibited AI?   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 7/18\n\nfines of up to 7% of global annual turnover. This is a highly dissuasive figure which should focus minds in the boardroom. Below, I will provide a digestible summary of the eight categories of prohibited AI practice. For some of these, there are specific and limited exceptions, which must be carefully considered and reviewed on a case-by-case basis. For example, the practices of predictive policing and the use of emotion recognition systems are not outright prohibited in every scenario. Also, some of these categories proved highly contentious during the AI Actâ€™s legislative process and trilogue negotiations. In particular, the question of whether law enforcement authorities should be permitted to use AI systems for real-time remote biometric identification in public was totemic. The European Parliament argued for an outright prohibition, whereas member states wanted their authorities to retain such capabilities. A compromise between these two positions was eventually struck. Now the dust has settled, it is prohibited, under EU law, to sell, make available, or use AI systems for any of the practices listed below. \n\nCausing harm by deploying subliminal or deceptive techniques   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 8/18\n\nAI systems which deploy subliminal, manipulative, or deceptive techniques in order to distort the behaviour of an individual or group, in a way which causes, or is likely to cause, significant harm. This can include persuading or manipulating people to engage in unwanted behaviours. This practice is prohibited even if there was no intention to cause harm. \n\nCausing harm by exploiting vulnerabilities \n\nAI systems which exploit the vulnerabilities of an individual or group (e.g., age, disability, or socioeconomic status) to distort that individual or groupâ€™s behaviour in a way which causes, or is likely to cause, significant harm. This practice is also prohibited even if there was no intention to cause harm. \n\nSocial credit scoring systems \n\nAI systems which evaluate and score people based on social behaviour and personal characteristics, with the score leading to detrimental or unfavourable treatment which is disproportionate, unjustified, and/or unrelated to the context of the original data. Such AI systems can lead to discrimination, marginalisation, and social exclusion.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 9/18\n\nPredictive policing \n\nAI systems which are used to assess or predict the risk of an individual committing a criminal offence, where the assessment is based solely on automated profiling of their traits and characteristics. It is unlawful to use any of the following traits as the sole basis for assessing and predicting whether someone will commit a crime: nationality, birth location, residence location, number of children, level of debt etc. However, predictive policing AI systems are classified as high-risk (i.e., not prohibited), when they are not based solely on the type of automated profiling described above. \n\nEmotion recognition in the workplace and education \n\nAI systems which infer or detect emotions, based on biometric data (e.g., facial images, fingerprints, or physiological data) in the context of the workplace and educational institutions. This includes emotions like happiness, excitement, sadness, and anger. However, it does not include physical states like pain or fatigue. There are exemptions for emotional recognition systems used for medical or safety purposes. In such permissible scenarios, an emotion recognition system would be classified as high-risk.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 10/18\n\nCreating facial recognition databases via untargeted scraping \n\nAI systems which create or expand databases for facial recognition, via untargeted scraping of facial images from the internet or CCTV footage. \n\nBiometric categorisation to infer specific protected characteristics \n\nAI systems which categorise individuals based on biometric data, to infer protected characteristics like race, political opinions, trade union membership, religious beliefs, sexual orientation, or sex life. If biometric categorisation systems are used to infer characteristics or traits which are not protected, then those AI systems are classified as high-risk. \n\nLaw enforcement use of real-time biometric identification in public \n\nLaw enforcement use of â€™real-timeâ€™ remote biometric identification systems in public (e.g., facial recognition used to identify and stop potential suspects in public) is prohibited, apart from in very specific and narrowly defined scenarios. This prohibition is intended to safeguard privacy and prevent discrimination. Acceptable scenarios for leveraging AI in this way include searching for victims of serious crime, preventing imminent threats to life (e.g., terrorist attacks), or locating suspects or perpetrators of serious crimes (e.g., murder).   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 11/18\n\nHowever, independent judicial or administrative authorisation must be granted, before such AI systems are used. The use must only occur for a limited time period, with safeguards. National regulators and data protection authorities must be notified each time an AI system is used in this way. The European Commission will publish an annual report tracking and documenting these use cases. The development and placing on the market of AI systems intended for this purpose is, however, not prohibited. But there are prohibitions on the use of such systems. Last week, the European Commission published Draft Guidelines on prohibited AI practices. These shed light on how organisations can interpret, practically implement, and comply with these important provisions. The guidelines are also designed to assist regulatory authorities reviewing such cases, although those authorities are not legally obliged to take these guidelines into account. \n\nThe European Commissionâ€™s new Guidelines on prohibited AI   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 12/18\n\nAt 140 pagesâ€”which is not too dissimilar to the overall length of the EU AI Actâ€”we get a glimpse of the task ahead. To be facetious, if the EU keeps up its rate of 140 pages of guidance for every 3 pages of legal text, then AI governance professionals may have over 6700 pages of overall text which they will need to understand, in order to fully grasp this new law. On a more serious note, this highlights that over the coming months and years, there will be a huge amount of additional documentation and analysis, such as official guidelines, recommendations, opinions, standards, codes of practice, enforcement decisions, and court judgements, which we will all need to keep up with. Luckily, youâ€™ve come to the right place ðŸ˜‰ \n\nSummary of key points \n\nRegulatory authorities should use these guidelines in relevant cases and investigations. However, despite the various examples provided, there will always need to be a detailed, case-by-case assessment of the AI system in question. Although much enforcement of Article 5 will be actioned by regulators at the member state level, they must keep the Commission and other regulators informed regarding any cases with cross-border impact. Furthermore, regulatory authorities should strive for harmonisation on prohibited AI enforcement, via collaboration at the European AI Board.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 13/18\n\nThe prohibitions apply to any AI system, whether it has a narrow â€˜intended purposeâ€™ which is prohibitedâ€™, or whether it is a general-purpose AI system used in a manner which is prohibited. Deployers must therefore ensure that they do not use or customise general-purpose AI systems in a prohibited way. There are other scenarios, under EU law, where the use of an AI system could be prohibited, even if this is not explicitly stipulated in the AI Act. For example, there are other conceivable uses of AI which could breach the EUâ€™s Charter of Fundamental Rights. \n\nExample of potential prohibited scenarios for each category \n\nIn the guidelines, the European Commission provided some illustrative examples of what could be prohibited, across each category. I have provided a selection below. \n\nCausing harm by deploying subliminal or deceptive techniques \n\nAn AI companionship app imitates how humans talk, act, and respond. It uses human-like traits and emotional signals to affect how users feel and think. This can make people emotionally attached to the app, leading to addiction-like behavior. In some cases, this might cause serious problems, like suicidal thoughts or even harm to others.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 14/18\n\nCausing harm by exploiting vulnerabilities \n\nAn AI chatbot targets and radicalises socio-economically disadvantaged people, encouraging them to harm or injure other people, by tapping into their fears, vulnerabilities, and sense of social exclusion. \n\nSocial credit scoring systems \n\nA national labour agency uses AI to determine whether unemployed people should receive state employment benefits. As part of the scoring process, data is used and relied upon which is unrelated to the purpose of the evaluation, such as marital status, health problems, or addictions. \n\nPredictive policing \n\nA law enforcement authority uses AI to predict and determine that an individual is more likely to commit a terrorism offence, based solely on personal characteristics like their age, nationality, address, type of car, and marital status. \n\nEmotion recognition in the workplace and education \n\nInferring emotions from written text (e.g., sentiment analysis) is not emotion recognition. However, inferring emotions from keystrokes, facial expressions, body postures, and movement is emotional recognition, as it is based on biometric data.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 15/18\n\nCreating facial recognition databases via untargeted scraping \n\nA facial recognition company collects photos of people's faces using an automated tool that searches social media platforms. It gathers these images along with other data, such as the image's URL, location data, and the person's name. The company then processes the photos to extract facial features and converts them into mathematical data for easy storage and comparison. When someone uploads a new photo to their facial recognition system, it is checked for any matches in the facial database. \n\nBiometric categorisation to infer specific protected characteristics \n\nAn AI system which categorises individualsâ€™ social media profiles based on their assumed sexual orientation, which is predicted by analysing biometric data from their photos, is prohibited. \n\n## ICYMI: get the full 17-page report which compares AI laws in the EU, China and U.S.A ðŸ‘‡ðŸ¼", "fetched_at_utc": "2026-02-08T18:51:39Z", "sha256": "0436fda428252583733cac243d6f4faed26dd1fea581aff5727ec713479bdc02", "meta": {"file_name": "Prohibited AI practices - Oliver Patel.pdf", "file_size": 916744, "relative_path": "pdfs\\Prohibited AI practices - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 3789}}}
{"doc_id": "pdf-pdfs-proposal-digital-omnibus-c10497b01d38", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Proposal Digital Omnibus.pdf", "title": "Proposal Digital Omnibus", "text": "EN EN \n\nEUROPEAN \n\nCOMMISSION \n\nBrussels, 19.11.2025 \n\nCOM(2025) 836 final \n\n2025/0359 (COD) \n\nProposal for a \n\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL \n\namending Regulations (EU) 2024/1689 and (EU) 2018/1139 as regards the simplification \n\nof the implementation of harmonised rules on artificial intelligence (Digital Omnibus on \n\nAI) \n\n{SWD(2025) 836 final} \n\n(Text with EEA relevance) EN 1 EN \n\nEXPLANATORY MEMORANDUM \n\n1. CONTEXT OF THE PROPOSAL \n\nâ€¢ Reasons for and objectives of the proposal \n\nIn its Communication on a Simpler and Faster Europe ( 1), the Commission announced its commitment to an ambitious programme to promote forward-looking, innovative policies that strengthen the European Unionâ€™s (EU) competitiveness and lighten the regulatory burdens on people, businesses and administrations, while maintaining the highest standard in promoting its values. \n\nRegulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence (â€˜AI Actâ€™), which entered into force on 1 August 2024, establishes a single market for trustworthy and human-centric artificial intelligence (â€˜AIâ€™) across the EU. Its purpose is to promote innovation and the uptake of AI while ensuring a high level of protection for health, safety, and fundamental rights, including democracy and the rule of law. \n\nThe AI Actâ€™s entry into application occurs in stages, with all rules entering into application by 2 August 2027. The prohibitions on AI practices with unacceptable risks and the obligations for general-purpose AI models are already applicable. However, most provisions â€“ in particular those governing high-risk AI systems â€“ will only start to apply from 2 August 2026 or 2 August 2027. These provisions include detailed requirements for data governance, transparency, documentation, human oversight, and robustness, so as to ensure that AI systems placed on the EU market are safe, transparent, and reliable. \n\nThe Commission is committed to a clear, simple, and innovation-friendly implementation of the AI Act, as set out in the AI Continent Action Plan (2) and the Apply AI Strategy (3). Initiatives such as the General-Purpose AI Code of Practice, Commission guidelines and templates, the AI Pact and the launch of the AI Act Service Desk build clarity regarding the applicable rules and support for their application. In particular, the website through which the AI Act Service Desk is provided offers a single information platform ( 4) on all resources available to stakeholders to navigate the AI Act, including guidelines, national authorities and support initiatives, webinars, and harmonised standards. These efforts will continue, with further guidance and digital tools under preparation. \n\nBuilding on experience gained from the implementation of already applicable provisions, the Commission held a series of consultations, including a public consultation to identify potential challenges with implementing the AI Actâ€™s provisions, a call for evidence in preparation of the Digital Omnibus, a reality check allowing stakeholders to directly share their implementation experiences and an SME panel to identify their particular needs in the implementation of the AI Act.     \n\n> 1COM(2025) 47 final.\n> 2COM(2025)165 final.\n> 3COM(2025) 723 final.\n> 4https://ai-act-service-desk.ec.europa.eu/\n\n# EN 2 EN \n\nThese consultations reveal implementation challenges that could jeopardise the effective entry into application of key provisions of the AI Act. These include delays in designating national competent authorities and conformity assessment bodies, as well as a lack of harmonised standards for the AI Actâ€™s high-risk requirements, guidance, and compliance tools. Such delays risk significantly increasing the compliance costs for businesses and public authorities and slowing down innovation. \n\nTo address these challenges, the Commission is proposing targeted simplification measures \n\nto ensure timely, smooth, and proportionate implementation of certain of the AI Actâ€™s provisions. These include: \n\nâ€¢ linking the implementation timeline of high-risk rules to the availability of standards or other support tools; \n\nâ€¢ extending regulatory simplifications granted to small and medium-sized enterprises (SMEs) to small mid-caps (SMCs), including simplified technical documentation requirements and special consideration in the application of penalties; \n\nâ€¢ requiring the Commission and the Member States to foster AI literacy instead enforcing unspecified obligation on providers and deployers of AI systems in this respect, while training obligations for high-risk deployers remain; \n\nâ€¢ offering more flexibility in the post-market monitoring by removing aprescription of a harmonised post-market monitoring plan; \n\nâ€¢ reducing the registration burden for providers of AI systems that are used in high-risk areas but for which the provider has concluded that they are not high-risk as they are only used for narrow or procedural tasks; \n\nâ€¢ Centralising oversight over a large number of AI systems built on general-purpose AI models or embedded in very large online platforms and very large search engines with the AI Office; \n\nâ€¢ facilitating compliance with the data protection laws by allowing providers and deployers of all AI systems and models to process special categories of personal data for ensuring bias detection and correction, with the appropriate safeguards; \n\nâ€¢ a broader use of AI regulatory sandboxes and real-world testing , that will benefit European key industries such as the automotive industry, and facilitating an EU-level AI regulatory sandbox which the AI Office will set up as from 2028; \n\nâ€¢ targeted changes clarifying the interplay between the AI Act and other EU legislation and adjusting the AI Actâ€™s procedures to improve its overall implementation and operation. \n\nBeyond the legislative measures, the Commission is taking further measures to facilitate compliance with the AI Act and address the concerns raised by stakeholders. Further guidance is under preparation, focusing on offering clear and practical instructions to apply the AI Act in parallel with other EU legislation. This includes: \n\nâ€¢ Guidelines on the practical application of the high-risk classification; \n\nâ€¢ Guidelines on the practical application of the transparency requirements under Article 50 AI Act; \n\nâ€¢ Guidance on the reporting of serious incidents by providers of high-risk AI systems; \n\nâ€¢ Guidelines on the practical application of the high-risk requirements; EN 3 EN \n\nâ€¢ Guidelines on the practical application of the obligations for providers and deployers of high-risk AI systems; \n\nâ€¢ Guidelines with a template for the fundamental rights impact assessment; \n\nâ€¢ Guidelines on the practical application of rules for responsibilities along the AI value chain; \n\nâ€¢ Guidelines on the practical application of the provisions related to substantial modification; \n\nâ€¢ Guidelines on the post-market monitoring of high-risk AI systems; \n\nâ€¢ Gudelines on the elements of the quality management system which SMEs and SMCs may comply with in a simplified manner; \n\nâ€¢ Guidelines on the AI Actâ€™s interplay with other Union legislation, for example joint guidelines of the Commission and European Data Protection Board on the interplay of the AI Act and EU data protection law, guidelines on the interplay between the AI Act and the Cyber Resilience Act, and guidelines on the interplay between the AI Act and the Machinery Regulation; \n\nâ€¢ Guidelines on the competences and designation procedure for conformity assessment bodies to be designated under the AI Act. \n\nIn particular, stakeholder consultations reveal the need to offer guidance on the practical application of the AI Actâ€™s research exemptions under Article 2(6) and (8), including how they apply in sectoral contexts like in the pre-clinical research and product development in the field of medicinal products or medical devices, which the Commission will work on with priority. \n\nThese simplification efforts will help to ensure that the implementation of the AI Act is smooth, predictable, and innovation-friendly, enabling Europe to strengthen its position as the AI continent and to pursue an AI-first approach safely. \n\nâ€¢ Consistency with existing policy provisions in the policy area \n\nThe proposal is part of a broader Digital Package on Simplification composed of measures to reduce the administrative costs of compliance for businesses and administrations in the EU, which applies to several regulations of the EUâ€™s digital acquis without compromising the objectives of the underlying rules. The proposal builds on Regulation (EU) 2024/1689 and is aligned with existing policies to make the EU a global leader in AI, to make the EU an AI continent and to promote the uptake of human-centric and trustworthy a AI. \n\nâ€¢ Consistency with other Union policies \n\nThe proposal is part of a series of simplification packages. \n\n2. LEGAL BASIS, SUBSIDIARITY AND PROPORTIONALITY \n\nâ€¢ Legal basis \n\nThe legal basis for this proposal is Article 114 of the Treaty on the Functioning of the European Union (TFEU) in line with the original legal basis for the adoption of the legal acts which this proposal aims to amend. EN 4 EN \n\nâ€¢ Subsidiarity (for non-exclusive competence) \n\nRegulation (EU) 2024/1689 was adopted at EU level. Accordingly, amendments to that Regulation need to be made at EU level. \n\nâ€¢ Proportionality \n\nThe initiative does not go beyond what is necessary to achieve the objectives of simplification and burden reduction without lowering the protection of health, safety and fundamental rights. \n\nâ€¢ Choice of the instrument \n\nThe proposal amends Regulation (EU) 2024/1689 adopted by ordinary legislative procedure. Therefore, the amendments to that Regulation also must be adopted by regulation in accordance with the ordinary legislative procedure. \n\n3. RESULTS OF EX-POST EVALUATIONS, STAKEHOLDER CONSULTATIONS AND IMPACT ASSESSMENTS \n\nâ€¢ Ex-post evaluations/fitness checks of existing legislation \n\nThe proposal is accompanied by a Commission staff working document that provides a detailed overview of the impact of the proposed amendments to certain provisions of Regulation (EU) 2024/1689. It also provides an analysis of the positive impacts of the proposed measures. The analysis is based on existing data, information gathered through consultations and during a reality check and through written stakeholder feedback through a call for evidence. \n\nâ€¢ Stakeholder consultations \n\nSeveral consultations were carried out in the context of the proposal. They all complemented one another, addressing either different topical issues or stakeholder groups concerned by the initiative. \n\nIn the initial scoping phase of the Digital Package on Simplification, three public consultations and calls for evidence were published on the key strands of the proposal in the spring of 2025. A consultation was held on the Apply AI Strategy from 9 April to 4 June 2025 (5), another on the revision of the Cybersecurity Act from 11 April to 20 June 2025 ( 6), and finally another on the European Data Union Strategy from 23 May to 20 July 2025 ( 7). Each consultation included a questionnaire with a section (or at times multiple) on implementation and simplification concerns, directly related to the reflections on the Digital Package on Simplification. Taken together, 718 responses were received as part of this first consultation exercise.         \n\n> 5European Commission (2025) Call for evidence on the Apply AI Strategy . Available at: Apply AI\n> Strategy â€“ strengthening the AI continent\n> 6European Commission (2025) Call for evidence on the revision of the Cybersecurity Act. Available at:\n> The EU Cybersecurity Act\n> 7European Commission (2025) Call for evidence on the European Data Union Strategy . Available at:\n> European Data Union Strategy\n\n# EN 5 EN \n\nFrom 16 September to 14 October 2025, a call for evidence on the Digital Package on Simplification was further published ( 8). Its aim was to cover the whole scope of the initiative and give an opportunity to stakeholders to comment on a more targeted set of proposals in one go. A total of 513 responses were received, by a wide range of stakeholders. \n\nWith a view to raising awareness on the Digital Package on Simplification among small and medium-sized enterprises (SMEs), and collecting their feedback, a dedicated SME Panel was organised through the Enterprise Europe Network (EEN) between 4 September and 16 October 2025. The EEN is the worldâ€™s largest support network for SMEs and is implemented by the Commissionâ€™s European Innovation Council and SMEs Executive Agency (EISMEA). SME Panels are a way to consult stakeholders falling under this framework. SMEs have the opportunity to contribute their views to upcoming policy initiatives. In addition to the online written consultation (where 106 SMEsâ€™ responses were received), the Commission also presented the Digital Package on Simplification to SME associations part of the EEN, in a meeting that took place on 1 October 2025. \n\nA large number of bilateral meetings were organised by the Commission services with stakeholders in 2025, to address specific concerns. Discussions were also held with Member States. In addition to bilateral exchanges, specific agenda points on the Digital Simplification Package were discussed at Council Working Parties in June and September 2025, where the Commission presented the current situation and asked Member Statesâ€™ to express their views. \n\nOverall, stakeholder feedback converged on the need for a simplified application of some of the digital rules. Better coherence, and a focus on optimisation of compliance costs, was largely supported by a cross-section of stakeholders. Some differences in opinion were expressed regarding some of the more tailored measures. A more detailed overview of these stakeholder consultations, and how they were reflected in the proposal can be found in the staff working document accompanying the Digital Package on Simplification. \n\nâ€¢ Collection and use of expertise \n\nIn addition to the consultation outlined above, the Commission mainly relied on its own internal analysis for the purpose of this proposal. \n\nâ€¢ Impact assessment \n\nThe amendments put forward in the proposal are technical in nature. They are designed to ensure a more efficient implementation of rules that were already agreed at political level. There are no policy options that could meaningfully be tested and compared in an impact assessment report. \n\nThe accompanying staff working document examines the reasoning behind the amendments and outlines the views of stakeholders on the different measures. It also presents the costs savings and other types of impacts the proposal may entail. In many cases, it builds on the impact assessments that was originally carried out for the Regulation (EU) 2024/1689.   \n\n> 8European Commission (2025) Call for evidence on the digital package and omnibus . Available at:\n> Simplification â€“ digital package and omnibus\n\n# EN 6 EN \n\nThe staff working document therefore serves as a reference point to inform the European Parliament and the Councilâ€™s debate on the proposal, as well as the public, in a clear and engaged way. \n\nâ€¢ Regulatory fitness and simplification \n\nThe proposal aims to produce a significant reduction in administrative burden for businesses, national administrations, and the public at large. Initial estimates project possible savings of â‰ˆ\n\nEUR 297.2 to 433.2 million . Non-quantifiable benefits are also expected, notably due to a streamlined set of rules which will ease compliance and enforcement thereof. \n\nSMEs already benefit from regulatory privileges under Regulation (EU) 2024/1689. Some regulatory privileges already afforded to SMEs are extended to small mid-caps (SMCs). Since SMEs and SMCs are disproportionality more impacted by the compliance burden, it is expected that they will particularly benefit from these simplification measures. \n\nThe proposal is consistent with the Commissionâ€™s â€˜Digital Fitness Check for the digital rulebookâ€™, which aims to ensure properly aligned policy proposals with real-world digital environments (see Chapter 4 on Legislative and Financial Digital Statement). \n\nâ€¢ Fundamental rights \n\nRegulation (EU) 2024/1689 is expected to promote the protection of a number of fundamental rights and freedoms set out in the EU Charter of Fundamental Rights ( 9), as well as positively impacting the rights of a number of special groups ( 10 ). At the same time, the Regulation (EU) 2024/1689 imposes some restrictions on certain rights and freedoms (11 ), which are proportionate and limited to the minimum necessary. The proposal is not expected to modify the impact of the Regulation (EU) 2024/1689 on fundamental rights since the targeted nature of envisaged amendments do not affect the scope of the regulated AI systems or on the substantive requirements applicable to those systems. \n\n4. BUDGETARY IMPLICATIONS \n\nThe proposal amends the supervision and enforcement system of Regulation (EU) 2024/1689, whereby oversight over certain AI systems will be transferred to the Commissionâ€™s AI Office. In addition, to facilitate compliance by operators, the AI Office should set up an EU-level AI regulatory sandbox. To implement these new tasks, to the Commission will need the appropriate resources, which is estimated to stand at 53 FTE, of which 15 FTE can be covered    \n\n> 9In detail: the right to human dignity (Article 1), respect for private life and protection of personal data (Articles 7 and 8), non-discrimination (Article 21) and equality between women and men (Article 23), freedom of expression (Article 11) and freedom of assembly (Article 12), right to an effective remedy and to a fair trial, the rights of defence, and the presumption of innocence (Articles 47 and 48), right to a high level of environmental protection and the improvement of the quality of the environment (Article 37).\n> 10 In detail: workersâ€™ rights to fair and just working conditions (Article 31), a high level of consumer protection (Article 28), the rights of the child (Article 24) and the integration of persons with disabilities (Article 26).\n> 11 In detail: the freedom to conduct business (Article 16) and the freedom of art and science (Article 13).\n\n# EN 7 EN \n\nby internal redeployment. These implications have to be considered against the backdrop of reduced budgetary implications for the Member States which no longer have to ensure the oversight for those certain AI systems. A detailed overview of the costs involved in this transfer of competences is provided in the â€˜Legislative and Financial Digital Statementâ€™ accompanying this proposal. \n\n5. OTHER ELEMENTS \n\nâ€¢ Implementation plans and monitoring, evaluation and reporting arrangements \n\nThe Commission will monitor the implementation, application, and compliance with the new provisions. Furthermore, the Regulation that is amended by this proposal is regularly evaluated for its efficiency, effectiveness in reaching its objectives, relevance, coherence and value added in line with the EUâ€™s better regulation principles. This proposal does not require an implementation plan. \n\nâ€¢ Explanatory documents (for directives) \n\nNot applicable. \n\nâ€¢ Detailed explanation of the specific provisions of the proposal \n\nArticle 1 amends Regulation (EU) 2024/1689 (â€˜AI Actâ€™).  In particular, \n\nâ€¢ Paragraph 1 adds a reference to SMCs in the subject matter of the AI Act. \n\nâ€¢ Paragraph 2 is a technical change that is necessary to enable extending the real-world testing to high-risk AI systems embedded in products covered under Section B of Annex I AI Act. \n\nâ€¢ Paragraph 3 adds legal definitions for SME and SMC to the definitions in Article 3 of the AI Act. \n\nâ€¢ Paragraph 4 transforms the obligation for providers and deployers of AI systems with regards to AI literacy in Article 4 AI Act to an obligation on the Commission and the Member States to foster AI literacy. \n\nâ€¢ Paragraph 5 introduces a new Article 4a, replacing Article 10(5) AI Act, which provides a legal basis for providers and deployers of AI systems and AI models to exceptionally process special categories of personal data for the purpose of ensuring bias detection and correction under certain conditions. \n\nâ€¢ Paragraphs 6, 14 and 32 refer to the deletion of the obligation for providers to register AI systems in the EU database for high-risk systems under Annex III, where they have been exempted from classification as high-risk under Article 6(3) AI Act, because they are for instance only used for preparatory tasks. \n\nâ€¢ Paragraph 7 contains editorial follow-up changes to amendments made by paragraph 4. \n\nâ€¢ Paragraphs 8 and 9 extend existing regulatory privileges of the AI Act for SMEs to SMCs on technical documentation and putting in place a quality management system that takes into account their size. \n\nâ€¢ Paragraph 10 introduces a new procedure in Article 28 AI Act, whereby Member States are required to ensure that a conformity assessment body that applies for designation both under this Regulation and Union harmonization legislation listed in EN 8 EN \n\nSection A of Annex I AI Act shall be provided with the possibility to submit a single application and undergo a single assessment procedure to be designated. \n\nâ€¢ Paragraph 11 proposes to replace paragraph 4 of Article 29 AI Act which requires conformity assessment bodies to submit a single application in the cases to which reference is made in that paragraph. \n\nâ€¢ Paragraph 12 amends Article 30 AI Act by requiring conformity assessment bodies which apply to be designated as notified bodies to make that application in accordance with the codes, categories, and corresponding types of AI systems referred to in a new Annex XIV for the Commissionâ€™s New Approach Notified and Designated Organisations (â€˜NANDOâ€™) information system, and empowers the Commission to amend these codes, categories, and corresponding types in light of technological developments. \n\nâ€¢ Paragraph 13 clarifies the conformity assessment procedure laid down in Article 43 AI Act where a high-risk AI system is covered by Union harmonisation legislation listed under Section A of Annex I to the AI Act and where an AI system is classified as high-risk both under Annex I and Annex III to the AI Act. \n\nâ€¢ Paragraphs 15 and 16 remove the Commission empowerments in Articles 50 and 56 AI Act to adopt implementing acts to give codes of practice for general purpose AI models and transparency obligations for certain AI systems general validity in the Union. \n\nâ€¢ Paragraph 17 introduces amendments to the rules on AI regulatory sandboxes in Article 57 AI Act, inter alia, by providing the legal basis for the AI Office to introduce an AI regulatory sandbox on EU level for certain AI systems within its exclusive competence of supervision and require Member States to strengthen cross-border cooperation of their sandboxes. \n\nâ€¢ Paragraph 18 specifies the empowerment of the Commission to adopt implementing acts specifying the detailed arrangements for the establishment, development, implementation, operation, governance and supervision of AI regulatory sandboxes. \n\nâ€¢ Paragraph 19 introduces changes to the testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes as governed by Article 60 AI Act, inter alia extending this opportunity to high-risk AI systems covered by Section A of Annex I. \n\nâ€¢ Paragraph 20 creates an additional legal basis for interested Member States and the Commission, on voluntary basis, to enter into written agreements to test high-risk AI systems referred to in Section B of Annex I in real world-conditions. \n\nâ€¢ Paragraph 21 extends the derogation from micro-enterprises to SMEs to comply with certain elements of the quality management system required by Article 17 AI Act in \n\na simplified manner .\n\nâ€¢ Paragraph 22 removes an empowerment of the Commission in Article 69 AI Act to adopt an implementing act in relation to the reimbursement of experts of the scientific panel when called upon by Member States, to simplify the procedure. \n\nâ€¢ Paragraph 23 extends the focus of guidance which national authorities may provide from SMEs to SMCs. \n\nâ€¢ Paragraph 24 replaces the empowerment of the Commission in Article 72 AI Act to adopt an implementing act with regard to the post-market monitoring plan. EN 9 EN \n\nâ€¢ Paragraph 25 makes amendments to the supervision and enforcement of certain AI systems in Article 75 AI Act: \n\nâ€¢ Point (a) changes the heading. \n\nâ€¢ Point (b) reinforces the competence of the AI Office for the supervision and enforcement of certain AI systems, that are based on a general-purpose AI model, where the model and the system are provided by the same provider. At the same time, the provision clarifies that AI systems related to products covered under Annex I are not included in that supervision. Moreover, it is clarified that the supervision and enforcement of the compliance of AI systems embedded in designated very large online platforms or very large online search engines should fall under the competence of the AI Office. \n\nâ€¢ Point (c) introduces several new paragraphs, empowering the Commission to adopt implementing acts to define the enforcement powers and the procedures for the exercise of those powers of the AI Office, introducing a reference to Regulation (EU) 2019/1020 ensuring certain procedural safeguards apply to providers covered and empowering the Commission to carry out conformity assessments of AI systems within the scope of Article 75. \n\nâ€¢ Paragraph 26 amends Article 77 AI Act as regards the powers of authorities or bodies protecting fundamental rights and cooperation with market surveillance authorities. \n\nâ€¢ Paragraphs 27 and 28 extends provisions in Articles 95 and 96 that require that voluntary support tools should take into account the needs of SMEs to SMCs. \n\nâ€¢ Paragraph 29 extends existing regulatory privileges in Article 99 AI Act on penalties for SMEs to SMCs. \n\nâ€¢ Paragraph 30 contains amendments to Article 111 AI Act which result from amendments made in paragraph 30 and introduces a transitional period of 6 months for providers who need to retroactively include technical solutions in their generative AI systems, to make them machine readable and detectable as artificially generated or manipulated. \n\nâ€¢ Paragraph 31 introduces changes to the entry into application of certain provisions of the AI Act: \n\nâ€¢ For the obligations for high-risk AI systems in Chapter III, a mechanism is introduced that links the entry into application to the availability of measures in support of compliance with the AI Actâ€™s high-risk rules, such as harmonised standards, common specifications, and Commission guidelines. This availability will be confirmed by the Commission by decision, following which the rules for high-risk AI systems start to apply after an appropriate transition period. However, this flexibility should apply only for a limited time and a definite date by which the rules apply in any case should be set. Moreover, it is appropriate to distinguish between the two types of AI systems that classify as high-risk and extend a longer transition period to AI systems that classify as high-risk pursuant to Article 6(1) and Annex I to the AI Act. \n\nâ€¢ It is clarified that the amendments necessary to integrate the high-risk requirements into sectoral law listed in Section B of Annex I apply with the Digital Omnibusâ€™ entry into force. EN 10 EN \n\nâ€¢ Paragraph 33 is related to the change in paragraph 11 and introduces a new Annex XIV setting out codes, categories, and corresponding types of AI systems referred to in a new Annex XIV for the Commissionâ€™s New Approach Notified and Designated Organisations (â€˜NANDOâ€™) information system. \n\nArticle 2 makes amendments with regards to Regulation (EU) 2018/1139, to allow a smooth integration of the AI Actâ€™s high-risk requirements into that Regulation. \n\nArticle 3 provides the rule of entry into force and the binding nature of this Regulation. EN 11 EN \n\n2025/0359 (COD) \n\nProposal for a \n\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL \n\namending Regulations (EU) 2024/1689 and (EU) 2018/1139 as regards the simplification of the implementation of harmonised rules on artificial intelligence (Digital Omnibus on AI) \n\n(Text with EEA relevance) \n\nTHE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION, \n\nHaving regard to the Treaty on the Functioning of the European Union, and in particular Article 114 thereof, \n\nHaving regard to the proposal from the European Commission, \n\nAfter transmission of the draft legislative act to the national parliaments, \n\nHaving regard to the opinion of the European Economic and Social Committee 1,\n\nHaving regard to the opinion of the Committee of the Regions 2,\n\nActing in accordance with the ordinary legislative procedure, \n\nWhereas: \n\n(1) Regulation (EU) 2024/1689 of the European Parliament and of the Council 3 lays down harmonised rules on artificial intelligence (AI) and aims to improve the functioning of the internal market, to promote the uptake of human-centric and trustworthy artificial intelligence, while ensuring a high level of protection of health, safety and fundamental rights, and supporting innovation. Regulation (EU) 2024/1689 entered into force on 1 August 2024. Its provisions enter into application in a staggered manner, with all rules entering into application by 2 August 2027. \n\n(2) The experience gathered in implementing the parts of Regulation (EU) 2024/1689 that have already entered into application can inform the implementation of those parts that are yet to apply. In this context, the delayed preparation of standards, which should provide technical solutions for providers of high-risk AI systems to ensure compliance with their obligations under that regulation, and the delayed establishment of    \n\n> 1OJ C , , p. .\n> 2OJ C , , p. .\n> 3Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).\n\n# EN 12 EN \n\nthe governance and the conformity assessment frameworks at national level result in a compliance burden that is heavier than expected. In addition, consultations of stakeholders have revealed the need for additional measures that facilitate and provide clarification on the implementation and compliance, without reducing the level of protection for health, safety and fundamental rights from AI-related risks that the rules of Regulation (EU) 2024/1689 seek to achieve. \n\n(3) Consequently, targeted amendments to Regulation (EU) 2024/1689 are necessary to address certain implementation challenges, with a view to the effective application of the relevant rules. \n\n(4) Enterprises outgrowing the micro, small and medium-sized enterprises (â€˜SMEâ€™) definition â€“ the â€˜small mid-cap enterprisesâ€™ (â€˜SMCsâ€™) â€“ play a vital role in the Unionâ€™s economy. Compared to SMEs, SMCs tend to demonstrate a higher pace of growth, and level of innovation and digitisation. Nevertheless, they face challenges similar to SMEs in relation to administrative burden, leading to a need for proportionality in the implementation of Regulation (EU) 2024/1689 and for targeted support. To enable the smooth transition of enterprises from SMEs into SMCs, it is important to address in a coherent manner the effect that regulation may have on their activity once those enterprises outgrow the segment of SMEs and are faced with rules that apply to large enterprises. Regulation (EU) 2024/1689 provides for several measures for small-scale providers, which should be extended to SMCs. In order to clarify the treatment of SMEs and SMCs in Regulation (EU) 2024/1689, it is necessary to introduce definitions for SMEs and SMCs, which should correspond to the definition set out in the Annex to Commission Recommendation 2003/361/EC 4 and Annex to Commission Recommendation 2025/3500/EC 5.\n\n(5) Article 4 of Regulation (EU) 2024/1689 currently imposes an obligation on all providers and deployers of AI systems to ensure AI literacy of their staff. AI literacy development starting from education and training and continuing in a lifelong learning manner is crucial to equip providers, deployers and other affected persons with the necessary notions to make informed decisions regarding AI systems deployment. However, experience shared by stakeholders reveals that a one-size-fits-all solution is not suitable for all types of providers and deployers in relation to the promotion of AI literacy, rendering such a horizontal obligation ineffective in achieving the objective pursued by this provision. Moreover, data indicate that imposing such an obligation creates an additional compliance burden, particularly for smaller enterprises, whereas AI literacy should be a strategic priority, regardless of regulatory obligations and potential sanctions. In light of that, Article 4 of Regulation (EU) 2024/1689 should be amended to require the Member States and the Commission, without prejudice to their respective competences, to individually, collectively and in cooperation with relevant stakeholders encourage providers and deployers to provide a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI   \n\n> 4Commission Recommendation of 6 May 2003 concerning the definition of micro, small and medium-sized enterprises (OJ L 124, 20.5.2003, pp. 36â€“41, ELI: http://data.europa.eu/eli/reco/2003/361/oj).\n> 5Commission Recommendation (EU) 2025/1099 of 21 May 2025 on the definition of small mid-cap enterprises (OJ L, 2025/1099, 28.5.2025, ELI: http://data.europa.eu/eli/reco/2025/1099/oj).\n\n# EN 13 EN \n\nsystems on their behalf, including through offering training opportunities, providing informational resources, and allowing exchange of good practices and other non-legally binding initiatives. The European Artificial Intelligence Board (â€˜Boardâ€™) will ensure recurrent exchange between the Commission and Member States on the topic, while the Apply AI Alliance will allow discussion with the wider community. This amendment is without prejudice to the broader measures taken by the Commission and the Member States to promote AI literacy and competences for the wider population, including learners, students, and citizens at different ages and in particular through education and training systems. \n\n(6) Bias detection and correction constitute a substantial public interest because they protect natural persons from biasesâ€™ adverse effects, including discrimination. Discrimination might result from the bias in AI models and AI systems other than high-risk AI systems for which of Regulation (EU) 2024/1689 already provides a legal basis authorising the processing of special categories of personal data under Article 9(2), point (g), of Regulation (EU) 2016/679 of the European Parliament and of the Council 6. Given that discrimination might result also from those other AI systems and models, it is therefore appropriate that Regulation (EU) 2024/1689 should provide for a legal basis for the processing of special categories of personal data also by providers and deployers of other AI systems and AI models as well as deployers of high-risk AI systems. The legal basis is established in compliance with Article 9(2), point (g) of Regulation (EU) 2016/679 Article 10(2), point (g) of Regulation (EU) 2018/1725 of the European Parliament and of the Council 7 and Article 10, point (a) of Directive (EU) 2016/680 of the European Parliament and of the Council 8 provides a legal basis allowing, where necessary for the detection and removal of bias, the processing of special categories of personal data by providers and deployers of all AI systems and models, subject to appropriate safeguards that complement Regulations (EU) 2016/679, Regulation (EU) 2018/1725 and Directive (EU) 2016/680, as applicable. \n\n(7) In order to ensure consistency, avoid duplication and minimise administrative burdens in relation to the procedure for designating notified bodies under Regulation (EU) 2024/1689, while maintaining the same level of scrutiny, a single application and a single assessment procedure should be available for new conformity assessment                        \n\n> 6Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1, ELI: http://data.europa.eu/eli/reg/2016/679/oj).\n> 7Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of natural persons with regard to the processing of personal data by the Union institutions, bodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No 45/2001 and Decision No 1247/2002/EC (OJ L295, 21.11.2018, p. 39, ELI: http://data.europa.eu/eli/reg/2018/1725/oj).\n> 8Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA (OJ L119, 4.5.2016, pp. 89â€“131, ELI: http://data.europa.eu/eli/dir/2016/680/oj).\n\n# EN 14 EN \n\nbodies and notified bodies which are designated under the Union harmonisation legislation listed in Section A of Annex I to Regulation (EU) 2024/1689, such as under Regulations (EU) 2017/745 9 and (EU) 2017/746 10  of the European Parliament and of the Council, where such a procedure is established under that Union harmonisation legislation. The single application and assessment procedure aims at facilitating, supporting and expediting the designation procedure under Regulation (EU) 2024/1689, while ensuring compliance with the requirements applicable to notified bodies under that Regulation and the Union harmonisation legislation listed in Section A of Annex I thereto. \n\n(8) With a view to ensuring the smooth application and consistency of Regulation (EU) 2024/1689, amendments should be made to it. A technical correction to Article 43(3), first subparagraph, of Regulation (EU) 2024/1689 should be added to align the conformity assessment requirements with the requirements of providers of high-risk AI systems in Article 16 of that Regulation. Moreover, it should be clarified that where a provider of a high-risk AI system is subject to the conformity assessment procedure under Union harmonisation legislation listed in Section A of Annex I to Regulation (EU) 2024/1689, and the conformity assessment extends to compliance of the quality management system of that Regulation and of such Union harmonisation legislation, the provider should be able to include aspects related to quality management systems under that Regulation as part of the quality management systems under such Union harmonisation legislation, in line with Article 17(3) of Regulation (EU) 2024/1689. Article 43(3), second subparagraph, should be amended to clarify that notified bodies which have been notified under the Union harmonisation legislation listed in Section A of Annex I to Regulation (EU) 2024/1689 and which aim to assess high-risk AI systems covered by the Union harmonisation legislation listed in Section A of Annex I to that Regulation, should apply for the designation as a notified body under that Regulation within 18 months from [the entry into application of this Regulation]. This amendment is without prejudice to Article 28 of Regulation (EU) 2024/1689. Moreover, Regulation (EU) 2024/1689 should be amended to clarify that where a high-risk AI system is both covered by the Union harmonisation legislation listed in Section A of Annex I to Regulation (EU) 2024/1689 and falls within one of the use-cases listed in Annex III to that Regulation, the provider should follow the relevant conformity assessment procedure as required under that relevant harmonisation legislation. \n\n(9) To streamline compliance and reduce the associated costs, providers of AI systems should not be required to register AI systems referred to in Article 6(3) of Regulation (EU) 2024/1689 in the EU database pursuant to Article 49(2) of that Regulation. Given that such systems are not considered high-risk under certain conditions where   \n\n> 9Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1, ELI: http://data.europa.eu/eli/reg/2017/745/oj).\n> 10 Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro diagnostic medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176, ELI: http://data.europa.eu/eli/reg/2017/746/oj).\n\n# EN 15 EN \n\nthey do not pose significant risk of harm to the health, safety or fundamental rights of persons, imposing registration requirements would constitute a disproportionate compliance burden. Nevertheless, a provider who considers that an AI system falls under Article 6(3) remains obligated to document its assessment before that system is placed on the market or put into service. This assessment may be requested by national competent authorities. \n\n(10) Articles 57, 58 and 60 of Regulation (EU) 2024/1689 should be amended to strengthen further cooperation at Union level of AI regulatory sandboxes, foster clarity and consistency in the governance of AI regulatory sandboxes, and to extend the scope of real-world testing outside AI regulatory sandboxes to high-risk AI systems covered by the Union harmonisation legislation listed in Annex I to that Regulation. In particular, to allow procedural simplification, where applicable, in the projects supervised in the AI regulatory sandboxes that include also real-world testing, the real-world testing plan should be integrated in the sandbox plan agreed by the providers or prospective providers and the competent authority in a single document. In addition, it is appropriate to provide for the possibility of the AI Office to establish an AI regulatory sandbox at Union level for AI systems that are covered by Article 75(1) of Regulation (EU) 2024/1689. By leveraging these infrastructures and facilitating cross-border collaboration, coordination would be better streamlined and resources optimally utilised. \n\n(11) To foster innovation, it is also appropriate to extend the scope of real-world testing outside AI regulatory sandboxes in Article 60 of Regulation (EU) 2024/1689, currently applicable to high-risk AI systems listed in Annex III to that Regulation, and allow providers and prospective providers of high-risk AI systems covered by the Union harmonisation legislation listed in Annex I to that Regulation to also test such systems in real-world conditions. This is without prejudice to other Union or national law on the testing in real-world conditions of high-risk AI systems related to products covered by that Union harmonisation legislation. To address the specific situation of high-risk AI systems covered the Union harmonisation legislation listed in Section B of Annex I to that Regulation, it is necessary to allow the conclusion of voluntary agreements between the Commission and Member States to enable testing of such high-risk AI systems in real-world conditions. \n\n(12) Article 63 of Regulation (EU) 2024/1689 offers microenterprises who are providers of high-risk AI systems the possibility to benefit from a simplified way to comply with the obligation to establish a quality management system. With a view to facilitating compliance for more innovators, that possibility should be extended to all SMEs, including start-ups. \n\n(13) Article 69 of Regulation (EU) 2024/1689 should be amended to simplify the fee structure of the scientific panel. If Member States call upon the panelâ€™s expertise, the fees they may be required to pay the experts should be equivalent to the remuneration the Commission is obliged to pay in similar circumstances. Furthermore, to reduce the procedural complexity, Member States should be able to consult the experts of the scientific panel directly, without involvement of the Commission. \n\n(14) In order to strengthen the governance system for AI systems based on general-purpose AI models, it is necessary to clarify the role of the AI Office in monitoring and supervising compliance of such AI systems with Regulation (EU) 2024/1689, while excluding AI systems related to products covered by the Union harmonisation legislation listed in Annex I to that Regulation. While sectoral authorities continue to EN 16 EN \n\nremain responsible for the supervision of AI systems related to products covered by that Union harmonisation legislation, Article 75(1) Regulation (EU) 2024/1689 should be modified to bring all AI systems based on general-purpose AI models developed by the same provider within the scope of the AI Office's supervision. This does not include AI systems placed on the market, put into service or used by Union institutions, bodies, offices or agencies, which are under the supervision of the European Data Protection Supervisor pursuant to Article 74(9) of Regulation (EU) 2024/1689. To ensure effective supervision for those AI systems in accordance with the tasks and responsibilities assigned to market surveillance authorities under Regulation (EU) 2024/1689, the AI Office should be empowered to take the appropriate measures and decisions to adequately exercise its powers provided for in that Section and Regulation (EU) 2019/1020 of the European Parliament and of the Council 11 . Article 14 of Regulation (EU) 2019/1020 should apply mutatis mutandis. Furthermore, to ensure effective enforcement, the authorities involved in the application of Regulation (EU) 2024/1689 should cooperate actively in the exercise of those powers, in particular where enforcement actions need to be taken in the territory of a Member State. \n\n(15) Considering the existing supervisory and enforcement system under Regulation (EU) 2022/2065 of the European Parliament and of the Council 12 , it is appropriate to grant the Commission the powers of a competent market surveillance authority under Regulation (EU) 2024/1689 where an AI system qualifies as a very large online platform or a very large online search engine within the meaning of Regulation (EU) 2022/2065, or where it is embedded in such a platform or search engine. This should contribute to ensuring that the exercise of the Commissionâ€™s supervision and enforcement powers under Regulation (EU) 2024/1689 and Regulation (EU) 2022/2065, as well as those applicable to general-purpose AI models integrated into such platforms or search engines, are carried out in a coherent manner. In the case of AI systems embedded in or qualifying as a very large online platform or search engine, the first point of entry for the assessment of the AI systems are the risk assessment, mitigating measures and audit obligations prescribed by Articles 34, 35 and 37 of Regulation (EU) 2022/2065, without prejudice to the AI Officeâ€™s powers to investigate and enforce ex post non-compliance with the rules of this Regulation. In the context of the analysis of this risk assessment, mitigating measures and audits, the Commission services responsible for the enforcement of Regulation (EU) 2022/2065 may seek the opinion of the AI Office on the outcome of a potential earlier or parallel risk assessment carried out under this Regulation and the applicability of prohibitions under this Regulation. In addition, the AI Office and the competent national authorities under (EU) 2024/1689 should coordinate their enforcement efforts with the authorities              \n\n> 11 Regulation (EU) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market surveillance and compliance of products and amending Directive 2004/42/EC and Regulations (EC) No 765/2008 and (EU) No 305/2011 (OJ L169, 25.6.2019, p. 1, ELI: http://data.europa.eu/eli/reg/2019/1020/oj).\n> 12 Regulation (EU) 2022/2065 of the European Parliament and of the Council of 19 October 2022 on a Single Market For Digital Services and amending Directive 2000/31/EC (Digital Services Act) (OJ L 277, 27.10.2022, p. 1, ELI: http://data.europa.eu/eli/reg/2022/2065/oj).\n\n# EN 17 EN \n\ncompetent for the supervision and enforcement of Regulation (EU) 2022/2065, including the Commission, in order to ensure that the principles of loyal cooperation, proportionality and non bis in idem are respected, while information obtained under the respective other Regulation would be used for the purposes of supervision and enforcement of the other only provided the undertaking agrees. In particular, those authorities should exchange views regularly and take into account, in their respective areas of competence, any fines and penalties imposed on the same provider for the same conduct through a final decision in proceedings relating to an infringement of other Union or national rules, so as to ensure that the overall fines and penalties imposed are proportionate and correspond to the seriousness of the infringements committed. \n\n(16) To further operationalise the AI Officeâ€™s supervision and enforcement set out in Article 75(1) of Regulation (EU) 2024/1689, it is necessary to further define the which of the powers listed in Article 14 of Regulation (EU) 2019/1020 should be conferred upon the AI Office. The Commission should therefore be empowered to adopt implementing acts to specify those powers, including the ability to impose penalties, such as fines or other administrative sanctions, in accordance with the conditions and ceilings referred to in Article 99, and applicable procedures. This should ensure that the AI Office has the necessary tools to effectively monitor and supervise compliance with Regulation (EU) 2024/1689. \n\n(17) Additionally, it is essential to ensure that effective procedural safeguards apply to providers of AI systems subject to monitoring and supervision by the AI Office. To that end, the procedural rights provided for in Article 18 of Regulation (EU) 2019/1020 should apply mutatis mutandis to providers of AI systems, without prejudice to more specific procedural rights provided for in Regulation (EU) 2024/1689. \n\n(18) To enable access to Union market for AI systems which are under the supervision by the AI Office pursuant to Article 75 of Regulation (EU) 2024/1689 and subject to third party conformity assessment, the Commission should be enabled to carry out pre-market conformity assessments of those systems. \n\n(19) Article 77 and related provisions of Regulation (EU) 2024/1689 constitute an important governance mechanism, as they aim to enable authorities or bodies responsible for enforcing or supervising Union law intended to protect fundamental rights to fulfil their mandate under specific conditions and to foster cooperation with market surveillance authorities responsible for the supervision and enforcement of that Regulation. It is necessary to clarify the scope of such cooperation, as well as to clarify which public authorities or bodies benefit from it. With a view to reinforcing the cooperation, it should be clarified that requests to access information and documentation should be made to the competent market surveillance authority, which should respond to such requests, and that the involved authorities or bodies should have a mutual obligation to cooperate. \n\n(20) To allow sufficient time for providers of generative AI systems subject to the marking obligations laid down in Article 50(2) of Regulation (EU) 2024/1689 to adapt their practices within a reasonable time without disrupting the market, it is appropriate to introduce a transitional period of 6 months for providers who have already placed their systems on the market before the 2 August 2026. \n\n(21) To provide sufficient time for providers of high-risk AI systems and to clarify applicable rules to the AI systems already placed on the market or put into service EN 18 EN \n\nbefore the entry into application of relevant provisions of the Regulation (EU) 2024/1689, it is appropriate to clarify the application of a grace period provided in Article 111(2) of that Regulation. The grace period, for the purpose of Article 111(2), should apply to a type and model of AI systems already placed in the market. This means that if at least one individual unit of the high-risk AI system has been lawfully placed on the market or put into service before the date specified in Article 111(2), other individual units of the same type and model of high-risk AI system are subject to the grace period provided in Article 111(2) and thus may continue to be placed on the market, made available or put into service on the Union market without any additional obligations, requirements or the need for additional certification, as long as the design of that high-risk AI system remains unchanged. For the purposes of application of the grace period provided in Article 111(2), the decisive factor is the date on which the first unit of that type and model of high-risk AI system was placed on the market or put into service on the Union market for the first time. Any significant change to the design of that AI system after the date specified in Article 111(2) should trigger the obligation of the provider to comply fully with all relevant provisions of this Regulation applicable to high-risk AI systems, including the conformity assessment requirements. \n\n(22) Article 113 of Regulation (EU) 2024/1689 establishes the dates of entry into force and application of that Regulation, notably that the general date of application is 2 August 2026. For the obligations related to high-risk AI systems laid down in Sections 1, 2 and 3 of Chapter III of Regulation (EU) 2024/1689, the delayed availability of standards, common specifications, and alternative guidance and the delayed establishment of national competent authorities lead to challenges that jeopardise those obligationâ€™s effective entry into application and that risk to significantly increase implementation costs in a way that does not justify maintaining their initial date of application, namely 2 August 2026. Building on experience, it is appropriate to put in place a mechanism that links the entry into application to the availability of measures in support of compliance with Chapter III, which may include harmonised standards, common specifications, and Commission guidelines. This should be confirmed by the Commission by decision, following which the rules obligations for high-risk AI systems should apply after 6 months as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III and after 12 months as regards AI systems classified as high-risk pursuant to Article 6(1) and Annex I to Regulation (EU) 2024/1689. However, this flexibility should only be extended until 2 December 2027 as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III and until 2 August 2028 as regards AI systems classified as high-risk pursuant to Article 6(1) and Annex I to that Regulation, by which dates those rules should enter into application in any case. The distinction between the entry into application of the rules as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III and Article 6(1) and Annex I to that Regulation is consistent with the difference between the initial dates of application envisaged in Regulation (EU) 2024/1689 and aims to provide the necessary time for adaptation and implementation of the corresponding obligations. \n\n(23) In light of the objective to reduce implementation challenges for citizens, businesses and public administrations, it is essential that harmonised conditions for the implementation of certain rules are adopted only where strictly necessary. For that purpose, it is appropriate to remove certain empowerments bestowed on the Commission to adopt such harmonised conditions by means of implementing acts in cases where those conditions are not met. Regulation (EU) 2024/1689 should therefore EN 19 EN \n\nbe amended to remove the empowerments conferred on the Commission in Article 50(7), Article 56(6), and Article 72(3) thereof to adopt implementing acts. The removal of the empowerment to adopt a harmonised template for a post-market monitoring plan in Article 72(3) of Regulation (EU) 2024/1689 has as an additional benefit that it will offer more flexibility for providers of high-risk AI systems to put in place a system for post-market monitoring that is tailored to their organisation. At the same time, recognising the need to offer clarity how providers of high-risk AI systems are required to comply, the Commission should be required to publish guidance. \n\n(24) Conformity assessment of high-risk AI systems under Regulation (EU) 2024/1689 may require involvement of conformity assessment bodies. Only conformity assessment bodies that have been designated under that Regulation may carry out conformity assessments and only for the activities related to the categories and types of AI systems concerned. To enable the specification of the scope of the designation of conformity assessment bodies notified under Article 30 of Regulation (EU) 2024/1689, it is necessary to draw up a list of codes, categories, and corresponding types of AI systems. The list of codes should take into account whether the AI system is a component of a product or itself a product covered by the Union harmonisation legislation listed in Annex I (referred to as â€˜AIP codesâ€™, for AI systems covered by product legislation) or a system referred in Annex III of Regulation (EU) 2024/1689, which currently concerns only biometric AI systems referred to in point (1) of Annex III (referred to as â€˜AIB codesâ€™, for biometric AI systems). Both AIP codes and AIB codes are vertical codes. The AIP codes are reference codes to provide a link to the Union harmonisation legislation listed in Section A of Annex I of Regulation (EU) 2024/1689. The AIB codes are new codes specific to Regulation (EU) 2024/1689 to identify biometric AI systems referred in paragraph 1 of Annex III of that Regulation. The list of codes should also take into account specific types and underlying technologies of AI systems (referred to as â€˜AIH codesâ€™, for horizontal AI system codes). The AIH codes are new AI technology-specific codes and can be applied in conjunction with AIP or AIB vertical codes. The AIH codes cover AI systemsâ€™ underlying types and technologies. The list of codes, including three categories, should provide for a multi-dimensional typology of AI systems which ensures that conformity assessment bodies designated as notified bodies are fully competent for the AI systems they are required to assess. \n\n(25) Regulation (EU) 2018/1139 of the European Parliament and the Council 13  lays down common rules in the field of civil aviation. Article 108 of Regulation (EU) 2024/1689 sets out amendments to Regulation (EU) 2018/1139 to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of the civil aviation sector, and without interfering with existing governance, conformity              \n\n> 13 Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in the field of civil aviation and establishing a European Union Aviation Safety Agency, and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the European Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the Council and Council Regulation (EEC) No 3922/91(OJ L212, 22.8.2018, pp. 1â€“122, ELI: http://data.europa.eu/eli/reg/2018/1139/oj).\n\n# EN 20 EN \n\nassessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in Regulation (EU) 2024/1689 when adopting any relevant delegated or implementing acts on the basis of that act. A technical correction extending specific articles of Regulation (EU) 2018/1139 is necessary to ensure that those mandatory requirements for high-risk AI systems laid down in Regulation (EU) 2024/1689 are fully covered when adopting relevant delegated or implementing acts on the basis of Regulation (EU) 2018/1139. \n\n(26) In order to ensure legal certainty as soon as possible, with a view to the imminent general application of Regulation (EU) 2024/1689, this Regulation should enter into force as a matter of urgency, \n\nHAVE ADOPTED THIS REGULATION: \n\nArticle 1 \n\nAmendments to Regulation (EU) 2024/1689 \n\nRegulation (EU) 2024/1689 is amended as follows: \n\n(1) in Article 1(2), point (g) is replaced by the following: \n\nâ€™(g) measures to support innovation, with a particular focus on small mid-cap enterprises (SMCs) and small and medium-sized enterprises (SMEs), including start-ups.â€™; \n\n(2) in Article 2, paragraph 2 is replaced by the following: \n\nâ€˜2. For AI systems classified as high-risk AI systems in accordance with Article 6(1) related to products covered by the Union harmonisation legislation listed in Section B of Annex I, only Article 6(1), Article 60a, Articles 102 to 109 and Articles 111 and 112 shall apply. Article 57 shall apply only in so far as the requirements for high-risk AI systems under this Regulation have been integrated in that Union harmonisation legislation.; \n\n(3) in Article 3, the following points (14a) and (14b) are inserted: \n\nâ€˜(14a) micro, small and medium-sized enterprise (â€˜SMEâ€™) means a micro, small or medium-sized enterprise as defined in Article 2 of the Annex to Commission Recommendation 2003/361/EC; \n\n(14b) small mid-cap enterprise (â€˜SMCâ€™) means a small mid-cap enterprise as defined in point (2) of the Annex to Commission Recommendation (EU) 2025/1099â€™; \n\n(4) Article 4 is replaced by the following: \n\nâ€˜ Article 4 \n\nAI literacy \n\nâ€˜The Commission and Member States shall encourage providers and deployers of AI systems to take measures to ensure a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, level of education and training and the context the AI EN 21 EN \n\nsystems are to be used in, and considering the persons or groups of persons on whom the AI systems are to be used.â€™; \n\n(5) the following Article 4a is inserted in Chapter I: \n\nâ€˜Article 4a \n\nProcessing of special categories of personal data for bias detection and mitigation \n\n1. To the extent necessary to ensure bias detection and correction in relation to high-risk AI systems in accordance with Article 10 (2), points (f) and (g), of this Regulation, providers of such systems may exceptionally process special categories of personal data, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons. In addition to the safeguards set out in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable, all the following conditions shall be met in order for such processing to occur: \n\n(a) the bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic or anonymised data; \n\n(b) the special categories of personal data are subject to technical limitations on the re-use of the personal data, and state-of-the-art security and privacy-preserving measures, including pseudonymisation; \n\n(c) the special categories of personal data are subject to measures to ensure that the personal data processed are secured, protected, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse and ensure that only authorised persons have access to those personal data with appropriate confidentiality obligations; \n\n(d) the special categories of personal data are not transmitted, transferred or otherwise accessed by other parties; \n\n(e) the special categories of personal data are deleted once the bias has been corrected or the personal data has reached the end of its retention period, whichever comes first; \n\n(f) the records of processing activities pursuant to Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 include the reasons why the processing of special categories of personal data was necessary to detect and correct biases, and why that objective could not be achieved by processing other data. \n\n2. Paragraph 1 may apply to providers and deployers of other AI systems and models and deployers of high-risk AI systems where necessary and proportionate if the processing occurs for the purposes set out therein and provided that the conditions set out under the safeguards set out in this paragraph.; \n\n(6) in Article 6(4), paragraph 4 is replaced by the following: \n\nâ€˜4. A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessment before that system is placed on the market or put into service. Upon request of national competent authorities, the provider shall provide the documentation of the assessment.â€™; EN 22 EN \n\n(7) Article 10 is amended as follows: \n\n(a) paragraph 1 is replaced by the following: \n\nâ€˜1. High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2, 3 and 4 of this Article and in Article 4a(1) whenever such data sets are used.â€™; \n\n(b) paragraph 5 is deleted; \n\n(c) paragraph 6 is replaced by the following: \n\nâ€˜6. For the development of high-risk AI systems not using techniques involving the training of AI models, paragraphs 2, 3 and 4 of this Article and Article 4a(1) shall apply only to the testing data sets.â€™; \n\n(8) in Article 11(1), the second subparagraph is replaced by the following: \n\nâ€˜That technical documentation shall be drawn up in such a way as to demonstrate that the high-risk AI system complies with the requirements set out in this Section and to provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive form to assess the compliance of the AI system with those requirements. It shall contain, at a minimum, the elements set out in Annex IV. SMCs and SMEs, including start-ups, may provide the elements of the technical documentation specified in Annex IV in a simplified manner. To that end, the Commission shall establish asimplified technical documentation form targeted at the needs of SMCs and SMEs, including start-ups. Where an SMC or SME, including a start-up, opts to provide the information required in Annex IV in a simplified manner, it shall use the form referred to in this paragraph. Notified bodies shall accept the form for the purposes of the conformity assessment.â€™; \n\n(9) in Article 17, paragraph 2 is replaced by the following: \n\nâ€˜2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the size of the providerâ€™s organisation, in particular, if the provider is an SMC or an SME, including a start-up. Providers shall, in any event, respect the degree of rigour and the level of protection required to ensure the compliance of their high-risk AI systems with this Regulation.â€™; \n\n(10) in Article 28, the following paragraph 8 is added: \n\nâ€˜8. Notifying authorities designated under this Regulation responsible for AI systems covered by the Union harmonisation legislation listed in Section A of Annex I shall be established, organised and operated in such a way that ensures that the conformity assessment body that applies for designation both under this Regulation and the Union harmonisation legislation listed in Section A of Annex I shall be provided with the possibility to submit a single application and undergo a single assessment procedure to be designated under this Regulation and Union harmonisation legislation listed in Section A of Annex I, where the EN 23 EN \n\nrelevant Union harmonisation legislation provides for such single application and single assessment procedure. \n\nThe single application and single assessment procedure referred to in this paragraph shall also be made available to notified bodies already designated under the Union harmonisation legislation listed in Section A of Annex I, when those notified bodies apply for designation under this Regulation, provided that the relevant Union harmonisation legislation provides for such a procedure. \n\nThe single application and single assessment procedure shall avoid any unnecessary duplications, build on the existing procedures for designation under the Union harmonisation legislation listed in Section A of Annex I and ensure compliance with the requirements both relating to notified bodies under this Regulation and the relevant Union harmonisation legislation.â€™; \n\n(11) in Article 29, paragraph 4 is replaced by the following: \n\nâ€˜4. For notified bodies which are designated under any other Union harmonisation legislation, all documents and certificates linked to those designations may be used to support and expedite their designation procedure under this Regulation, as appropriate. \n\nNotified bodies, which are designated under any of the Union harmonisation legislation listed in Section A of Annex I and which apply for the single assessment referred to in Article 28(8), shall submit the single application for assessment to the notifying authority designated in accordance with that Union harmonisation legislation. \n\nThe notified body shall update the documentation referred to in paragraphs 2 \n\nand 3 of this Article whenever relevant changes occur, in order to enable the authority responsible for notified bodies to monitor and verify continuous compliance with all the requirements laid down in \n\nArticle 31. â€™; \n\n(12) in Article 30, paragraph 2 is replaced by the following: \n\nâ€˜2. Notifying authorities shall notify the Commission and the other Member States, based on the list of codes, categories, and corresponding types of AI systems referred to in Annex XIV, and using the electronic notification tool developed and managed by the Commission, of each conformity assessment body referred to in paragraph 1. \n\nThe Commission is empowered to adopt delegated acts in accordance with Article 97 to amend Annex XIV, in the light of technical progress, advances in knowledge or new scientific evidence by adding to the list of codes, categories, and corresponding types of AI systems a new code, a category or a type of AI system, withdrawing an existing code, category or a type of AI system from that list or moving a code or type of AI system from one category to another.â€™; \n\n(13) in Article 43, paragraph 3 is replaced by the following: \n\nâ€˜For high-risk AI systems covered by the Union harmonisation legislation listed in Section A of Annex I, the provider of the system shall follow the relevant conformity assessment procedure as required under the relevant EN 24 EN \n\nUnion harmonisation legislation. The requirements set out in Section 2 of this Chapter shall apply to those high-risk AI systems and shall be part of that assessment. Assessment of the quality management system set out in Article 17 and Annex VII shall also apply. \n\nFor the purposes of that conformity assessment, notified bodies which have been notified under the Union harmonisation legislation listed in Section A of Annex I shall have the power to assess the conformity of high-risk AI systems with the requirements set out in Section 2, provided that the compliance of those notified bodies with the requirements laid down in Article 31(4), (5), (10) and (11) has been assessed in the context of the notification procedure under the relevant Union harmonisation legislation. Without prejudice to Article 28, such notified bodies which have been notified under the Union harmonisation legislation in Section A of Annex I, shall apply for designation in accordance with Section 4 at the latest [18 months from the entry into application of this Regulation]. \n\nWhere Union harmonisation legislation listed in Section A of Annex I provides the product manufacturer with an option to opt out from a third-party conformity assessment, provided that that manufacturer has applied harmonised standards covering all the relevant requirements, that manufacturer may use that option only if it has also applied harmonised standards or, where applicable, common specifications referred to in Article 41, covering all requirements set out in Section 2 of this Chapter. \n\nWhere a high-risk AI system is both covered by the Union harmonisation legislation listed in Section A of Annex I and it falls within one of the categories listed in Annex III, the provider of the system shall follow the relevant conformity assessment procedure as required under the relevant Union harmonisation legislation listed in Section A of Annex I.â€™; \n\n(14) in Article 49, paragraph 2 is deleted; \n\n(15) in Article 50, paragraph 7 is replaced by the following: \n\nâ€˜7. The AI Office shall encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective implementation of the obligations regarding the detection, marking and labelling of artificially generated or manipulated content. The Commission may assess whether adherence to those codes of practice is adequate to ensure compliance with the obligation laid down in paragraph 2, in accordance with the procedure laid down in Article 56(6), first subparagraph. If it deems the code is not adequate, the Commission may adopt an implementing act specifying common rules for the implementation of those obligations in accordance with the examination procedure laid down in Article 98(2).â€™; \n\n(16) in Article 56(6), the first subparagraph is replaced by the following: \n\nâ€˜6. The Commission and the Board shall regularly monitor and evaluate the achievement of the objectives of the codes of practice by the participants and their contribution to the proper application of this Regulation. The Commission, taking utmost account of the opinion of the Board, shall assess whether the codes of practice cover the obligations provided for in Articles 53 and 55, and shall regularly monitor and evaluate the EN 25 EN \n\nachievement of their objectives. The Commission shall publish its assessment of the adequacy of the codes of practice.â€™; \n\n(17) Article 57 is amended as follows: \n\n(a) the following paragraph 3a is inserted: \n\nâ€˜The AI Office may also establish an AI regulatory sandbox at Union level for AI systems covered by Article 75(1). Such an AI regulatory sandbox shall be implemented in close cooperation with relevant competent authorities, in particular when Union legislation other than this Regulation is supervised in the AI regulatory sandbox, and shall provide priority access to SMEs.â€™; \n\n(b) paragraph 5 is replaced by the following: \n\nâ€˜5. AI regulatory sandboxes established under this Article shall provide for a controlled environment that fosters innovation and facilitates the development, training, testing and validation of innovative AI systems for a limited time before their being placed on the market or put into service pursuant to a specific sandbox plan agreed between the providers or prospective providers and the competent authority, ensuring that appropriate safeguards are in place. Such sandboxes may include testing in real world conditions supervised therein. When applicable, the sandbox plan shall incorporate in a single document the real-world testing plan.â€™; \n\n(c) paragraph 9, point (e) is replaced by the following: \n\nâ€˜(e) facilitating and accelerating access to the Union market for AI systems, in particular when provided by SMCs and SMEs, including start-ups.â€™; \n\n(d) paragraph 13 is replaced by the following: \n\nâ€™13. The AI regulatory sandboxes shall be designed and implemented in such a way that they facilitate cross-border cooperation between national competent authorities.â€™; \n\n(e) paragraph 14 is replaced by the following: \n\nâ€™14. National competent authorities shall coordinate their activities and cooperate within the framework of the Board. They shall support the joint establishment and operation of AI regulatory sandboxes, including in different sectors.â€™; \n\n(18) Article 58, paragraph 1, is replaced by the following: \n\nâ€˜1. In order to avoid fragmentation across the Union, the Commission shall adopt implementing acts specifying the detailed arrangements for the establishment, development, implementation, operation, governance, and supervision of the AI regulatory sandboxes. The implementing acts shall include common principles on the following issues: \n\n(a) eligibility and selection criteria for participation in the AI regulatory sandbox; \n\n(b) procedures for the application, participation, monitoring, exiting from and termination of the AI regulatory sandbox, including the sandbox plan and the exit report; EN 26 EN \n\n(c) the terms and conditions applicable to the participants; \n\n(d) the detailed rules applicable to the governance of AI regulatory sandboxes covered under Article 57, including as regards the exercise of the tasks of the competent authorities and the coordination and cooperation at national and EU level.â€™; \n\n(19) Article 60 is amended as follows: \n\n(a) in paragraph 1, the first subparagraph is replaced by the following: \n\nâ€˜Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers of high-risk AI systems listed in Annex III or covered by Union harmonisation legislation listed in Section A of Annex I, in accordance with this Article and the real-world testing plan referred to in this Article, without prejudice to the prohibitions under Article 5.â€™; \n\n(b) paragraph 2 is replaced by the following: \n\nâ€˜2. Providers or prospective providers may conduct testing of high-risk AI systems referred to in Annex III or covered by Union harmonisation legislation listed in Section A of Annex I in real world conditions at any time before the placing on the market or the putting into service of the AI system on their own or in partnership with one or more deployers or prospective deployers.â€™; \n\n(20) the following Article 60a is inserted: \n\nâ€˜Article 60a \n\nTesting of high-risk AI systems covered by Union harmonisation legislation listed in Section B of Annex I in real-world conditions outside AI regulatory sandboxes \n\n1. Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers of AI enabled products covered by Union harmonisation legislation listed in Section B of Annex I, in accordance with this Article and a voluntary real-world testing agreement, without prejudice to the prohibitions under Article 5. \n\n2. The voluntary real-world testing agreement referred to in paragraph 1 shall be concluded in writing between interested Member States and the Commission. It shall set the requirements for the testing of those AI-enabled products covered by Union harmonisation legislation listed in Section B of Annex I in real-world conditions. \n\n3. Member States, the Commission, market surveillance authorities and public authorities responsible for the management and operation of infrastructure and products covered by Union harmonisation legislation listed in Section B of Annex I shall cooperate closely with each other and in good faith, and shall remove any practical obstacles, including on procedural rules providing access to physical public infrastructure, where this is necessary, to successfully implement the voluntary real-world testing agreement and test AI-enabled products covered by Union harmonisation legislation listed in Section B of Annex. EN 27 EN \n\n4. The signatories of the voluntary real-world testing agreement, shall specify conditions of the testing in real world conditions and establish detailed elements of the real-world testing plan for AI systems covered by Union harmonisation legislation listed in Section B of Annex I. \n\n5. Article 60(2), (5) and (9) shall apply.â€™; \n\n(21) Article 63(1) is replaced by the following: \n\nâ€˜1. SMEs, including start-ups, may comply with certain elements of the quality management system required by Article 17 in a simplified manner. For that purpose, the Commission shall develop guidelines on the elements of the quality management system which may be complied with in a simplified manner considering the needs of SMEs, without affecting the level of protection or the need for compliance with the requirements in respect of high-risk AI systems.â€™; \n\n(22) Article 69 is amended as follows: \n\n(a) paragraph 2 is replaced by the following: \n\nâ€˜2. The Member States may be required to pay fees for the advice and support provided by the experts at a rate equivalent to the remuneration fees applicable to the Commission pursuant to the implementing act referred to in Article 68(1).â€™; \n\n(b) paragraph 3 is deleted. \n\n(23) in Article 70, paragraph 8 is replaced by the following: \n\nâ€˜8. National competent authorities may provide guidance and advice on the implementation of this Regulation, in particular to SMCs and SMEs, including start-ups, taking into account the guidance and advice of the Board and the Commission, as appropriate. Whenever national competent authorities intend to provide guidance and advice with regard to an AI system in areas covered by other Union law, the national competent authorities under that Union law shall be consulted, as appropriate.â€™; \n\n(24) in Article 72, paragraph 3 is replaced by the following: \n\nâ€˜3. The post-market monitoring system shall be based on a post-market monitoring plan. The post-market monitoring plan shall be part of the technical documentation referred to in Annex IV. The Commission shall adopt guidance on the post-market monitoring plan.â€™; \n\n(25) Article 75 is amended as follows: \n\n(a) the heading of Article 75 is replaced by the following: \n\nâ€˜Market surveillance and control of AI systems and mutual assistance â€™; \n\n(b) paragraph 1 is replaced by the following: \n\nâ€˜1. Where an AI system is based on a general-purpose AI model, with the exclusion of AI systems related to products covered by the Union harmonisation legislation listed in Annex I, and that model and that system are developed by the same provider, the AI Office shall be exclusively competent for the supervision and enforcement of that system with the obligations of this Regulation in accordance with the tasks and EN 28 EN \n\nresponsibilities assigned by it to market surveillance authorities. The AI Office shall also be exclusively competent for the supervision and enforcement of the obligations under this Regulation in relation to AI system that constitute or that are integrated into a designated very large online platform or very large online search engine within the meaning of Regulation (EU) 2022/2065. \n\nWhen exercising its tasks of supervision and enforcement under the first subparagraph, the AI Office shall have all the powers of a market surveillance authority provided for in this Section and in Regulation (EU) 2019/1020. The AI Office shall be empowered to take appropriate measures and decisions to adequately exercise its supervisory and enforcement powers. Article 14 of Regulation (EU) 2019/1020 shall apply mutatis mutandis. \n\nThe authorities involved in the application of this Regulation shall cooperate actively in the exercise of these powers, in particular where enforcement actions need to be taken in the territory of a Member State.â€™; \n\n(c) the following paragraphs 1a to 1c are inserted: \n\nâ€˜1a. The Commission shall adopt an implementing act to define the enforcement powers and the procedures for the exercise of those powers of the AI Office, including its ability to impose penalties, such as fines or other administrative sanctions, in accordance with the conditions and ceilings identified in Article 99, in relation to AI systems referenced to in paragraphs 1 and 1a of this Article that are found to be non-compliant with this Regulation, in the context of its monitoring and supervision tasks under this Article.â€™ \n\nâ€˜1b. Article 18 of Regulation (EU) 2019/1020 shall apply mutatis mutandis to providers of AI systems referred to in paragraph 1, without prejudice to more specific procedural rights provided for in this Regulation.â€™ \n\nâ€˜1c. The Commission shall organise and carry out pre-market conformity assessments and tests of AI systems referred to in paragraph 1 that are classified as high-risk and subject to third-party conformity assessment under Article 43 before such AI systems are placed on the market or put into service. These tests and assessments shall verify that the systems comply with the relevant requirements of this Regulation and may be placed on the market or put into service in the Union in accordance with this Regulation. The Commission may entrust the performance of these tests or assessments to notified bodies designated under this Regulation, in which case the notified body shall act on behalf of the Commission. Article 34(1) and (2) shall apply mutatis mutandis to the Commission when exercising its powers under this paragraph. \n\nThe fees for testing and assessment activities shall be levied on the provider of a high-risk AI system who has applied for third-party conformity assessment to the Commission. The costs related to the services entrusted by the Commission to the notified bodies in accordance with this Article shall be directly paid by the provider to the notified body.â€™; \n\n(26) Article 77 is amended as follows: \n\n(a) the heading is replaced by the following: EN 29 EN \n\nâ€˜Powers of authorities protecting fundamental rights and cooperation with market surveillance authorities â€™\n\n(b) paragraph 1 is replaced by the following: \n\nâ€˜1. National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights, including the right to non-discrimination, shall have the power to make a request and access any information or documentation created or maintained from the relevant market surveillance authority under this Regulation in accessible language and format where access to that information or documentation is necessary for effectively fulfilling their mandates within the limits of their jurisdiction.â€™; \n\n(c) the following paragraph 1a and 1b are inserted: \n\nâ€˜1a. Subject to the conditions specified in this Article, the market surveillance authority shall grant the relevant public authority or body referred to in paragraph 1 access to such information or documentation, including by requesting such information or documentation from the provider or the deployer, where necessary.â€™ \n\nâ€˜1b. Market surveillance authorities and public authorities or bodies referred to in paragraph 1 shall cooperate closely and provide each other with mutual assistance necessary for fulfilling their respective mandates, with a view to ensuring coherent application of this Regulation and Union law protecting fundamental rights and streamlining procedures. This shall include, in particular, exchange of information where necessary for the effective supervision or enforcement of this Regulation and the respective other Union legislation.â€™; \n\n(27) Article 95, paragraph 4 is replaced by the following: \n\nâ€˜4. The AI Office and the Member States shall take into account the specific interests and needs of SMCs and SMEs, including start-ups, when encouraging and facilitating the drawing up of codes of conduct.â€™; \n\n(28) in Article 96(1), the second subparagraph is replaced by the following: \n\nâ€˜When issuing such guidelines, the Commission shall pay particular attention to the needs of SMCs and SMEs including start-ups, of local public authorities and of the sectors most likely to be affected by this Regulation.â€™; \n\n(29) Article 99 is amended as follows: \n\n(a) paragraph 1 is replaced by the following: \n\nâ€˜1. In accordance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on penalties and other enforcement measures, which may also include warnings and non-monetary measures, applicable to infringements of this Regulation by operators, and shall take all measures necessary to ensure that they are properly and effectively implemented, thereby taking into account the guidelines issued by the Commission pursuant to Article 96. The penalties provided for shall be effective, proportionate and dissuasive. The Member States shall take into account the interests of SMCs and EN 30 EN \n\nSMEs, including start-ups, and their economic viability when imposing penalties.â€™; \n\n(b) paragraph 6 is replaced by the following: \n\nâ€˜6. In the case of SMCs and SMEs, including start-ups, each fine referred to in this Article shall be up to the percentages or amount referred to in paragraphs 3, 4 and 5, whichever thereof is lower.â€™; \n\n(30) Article 111 is amended as follows: \n\n(a) paragraph 2 is replaced by the following: \n\nâ€˜2. Without prejudice to the application of Article 5 as referred to in Article 113(3), third paragraph, point (a), this Regulation shall apply to operators of high-risk AI systems, other than the systems referred to in paragraph 1 of this Article, that have been placed on the market or put into service before the date of application of Chapter III and corresponding obligations referred to in Article 113, only if, as from that date, those systems are subject to significant changes in their designs. In any case, the providers and deployers of high-risk AI systems intended to be used by public authorities shall take the necessary steps to comply with the requirements and obligations laid down in this Regulation by 2 August 2030.â€™; \n\n(b) the following paragraph 4 is added: \n\nâ€˜4. Providers of AI systems, including general-purpose AI systems, generating synthetic audio, image, video or text content, that have been placed on the market before 2 August 2026 shall take the necessary steps in order to comply with Article 50(2) by 2 February 2027.â€™; \n\n(31) Article 113 is amended as follows: \n\n(a) in the third paragraph, point (d) is added: \n\nâ€˜(d) Chapter III, Sections 1, 2, and 3, shall apply following the adoption of a decision of the Commission confirming that adequate measures in support of compliance with Chapter III are available, from the following dates: \n\n(i) 6 months after the adoption of that decision as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III, and \n\n(ii) 12 months after the adoption of the decision as regards AI systems classified as high-risk pursuant to Article 6(1) and Annex I. \n\nIn the absence of the adoption of the decision within the meaning of subparagraph 1, or where the dates below are earlier than those that follow the adoption of that decision, Chapter III, Sections 1, 2, and 3, shall apply: \n\n(i) on 2 December 2027 as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III, and \n\n(ii) on 2 August 2028 as regards AI systems classified as high-risk pursuant to Article 6(1) and Annex I.â€™; \n\n(b) in the third paragraph, point (e) is added: EN 31 EN \n\nâ€˜ 3. Articles 102 to 110 shall apply from [the date of entry into application of this Regulation].â€™; \n\n(32) in Annex VIII, section B is deleted; \n\n(33) the following Annex XIV is added: \n\nâ€˜Annex XIV \n\nThe list of codes, categories and corresponding types of AI systems for the purpose of the notification procedure referred to in Article 30 specifying the scope of the designation as notified bodies \n\n1. Introduction \n\nConformity assessment of high-risk AI systems under this Regulation may require involvement of conformity assessment bodies. Only conformity assessment bodies that have been designated in accordance with this Regulation may carry out conformity assessments and only for the activities related to the types of AI systems concerned. The list of codes, categories, and corresponding types of AI systems sets the scope of the designation of conformity assessment bodies notified under Article 30 of this Regulation. \n\n2. List of Codes, categories, and corresponding AI systems \n\n1. AI systems subject to Annex I of the AI Act \n\nAIA Code \n\nAIP 0101  AI systems subject to Annex I.A.1. of the AI Act. \n\nAIP 0102  AI systems subject to Annex I.A.2. of the AI Act. \n\nAIP 0103  AI systems subject to Annex I.A.3. of the AI Act. \n\nAIP 0104  AI systems subject to Annex I.A.4. of the AI Act. \n\nAIP 0105  AI systems subject to Annex I.A.5. of the AI Act. \n\nAIP 0106  AI systems subject to Annex I.A.6. of the AI Act. \n\nAIP 0107  AI systems subject to Annex I.A.7. of the AI Act. \n\nAIP 0108  AI systems subject to Annex I.A.8. of the AI Act. \n\nAIP 0109  AI systems subject to Annex I.A.9. of the AI Act. \n\nAIP 0110  AI systems subject to Annex I.A.10. of the AI Act. \n\nAIP 0111  AI systems subject to Annex I.A.11. of the AI Act. \n\nAIP 0112  AI systems subject to Annex I.A.12. of the AI Act. \n\n2. AI systems subject to Annex III.1 of the AI Act EN 32 EN \n\nAIA Code \n\nAIB 0201  Remote biometric identification systems under Annex III.1.a. of the \n\nAI Act intended to be put into service by Union institutions, bodies, offices or agencies. \n\nAIB 0202  Biometric categorisation AI systems under Annex III.1.b. of the AI \n\nAct intended to be put into service by Union institutions, bodies, \n\noffices or agencies. \n\nAIB 0203  Emotion recognition AI systems under Annex III.1.c. of the AI Act \n\nintended to be put into service by Union institutions, bodies, offices or agencies. \n\nAIB 0204  Remote biometric identification systems under Annex III.1.a. of the \n\nAI Act intended to be put into service by law enforcement, \n\nimmigration or asylum authorities. \n\nAIB 0205  Biometric categorisation AI systems under Annex III.1.b. of the AI \n\nAct intended to be put into service by law enforcement, immigration or asylum authorities. \n\nAIB 0206  Emotion recognition AI systems under Annex III.1.c. of the AI Act \n\nintended to be put into service by law enforcement, immigration or \n\nasylum authorities. \n\nAIB 0207  Remote biometric identification systems under Annex III.1.a. of the \n\nAI Act (general). \n\nAIB 0208  Biometric categorisation AI systems under Annex III.1.b. of the AI \n\nAct (general). \n\nAIB 0209  Emotion recognition AI systems under Annex III.1.c. of the AI Act \n\n(general). \n\n3. AI technology-specific codes \n\na) Symbolic AI, expert systems and mathematical optimization \n\nAIA Code \n\nAIH 0101  Logic - and knowledge -based AI systems that infer from encoded \n\nknowledge or symbolic representation, expert systems \n\nAIH 0102  Logic-based AI systems, excluding basic data processing \n\nb) Machine learning, excluding GPAI and single modality generative AI \n\nAIA Code \n\nAIH 0201  AI systems that process structured data \n\nAIH 0202  AI systems that process signal and audio data \n\nAIH 0203  AI systems that process text data EN 33 EN \n\nAIH 0204  AI systems that process image and video \n\nAIH 0205  AI systems that learn from their environment, excluding agentic AI \n\nc) AI systems based on GPAI or single modality generative AI \n\nAIA Code \n\nAIH 0301  Single modality generative AI systems \n\nAIH 0302  Multimodal generative AI systems, including AI systems based on \n\nGPAI models \n\nd) Agentic AI \n\nAIA Code \n\nAIH 0401  Agentic AI \n\n3. Application for designation \n\nConformity assessment bodies shall use the lists of codes, categories and corresponding types of AI systems set out in this Annex when specifying the types of AI systems in the application for designation referred to in Article 29 of this Regulation.â€™. \n\nArticle 2 \n\nAmendments to Regulation (EU) 2018/1139 \n\nRegulation (EU) 2018/1139 is amended as follows: \n\n(1) in Article 27, the following paragraph is added: \n\nâ€˜3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council 14 , the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(2) in Article 31, the following paragraph is added:  \n\n> 14 Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).\n\n# EN 34 EN \n\nâ€˜3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(3) in Article 32, the following paragraph is added: \n\nâ€˜3. When adopting delegated acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(4) in Article 36, the following paragraph is added: \n\nâ€˜3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(5) in Article 39 the following paragraph is added: \n\nâ€˜3. When adopting delegated acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(6) in Article 50, the following paragraph is added: \n\nâ€˜3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(7) in Article 53, the following paragraph is added: \n\nâ€˜3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™. \n\nArticle 3 \n\nEntry into force and application \n\nThis Regulation shall enter into force on the third day following that of its publication in the \n\nOfficial Journal of the European Union .EN 35 EN \n\nThis Regulation shall be binding in its entirety and directly applicable in all Member States. \n\nDone at Brussels, \n\nFor the European Parliament For the Council \n\nThe President The President EN 1 EN \n\nLEGISLATIVE FINANCIAL AND DIGITAL STATEMENT \n\n1. FRAMEWORK OF THE PROPOSAL/INITIATIVE ................................................. 3 \n\n1.1. Title of the proposal/initiative ...................................................................................... 3 \n\n1.2. Policy area(s) concerned .............................................................................................. 3 \n\n1.3. Objective(s) .................................................................................................................. 3 \n\n1.3.1. General objective(s) ..................................................................................................... 3 \n\n1.3.2. Specific objective(s) ..................................................................................................... 3 \n\n1.3.3. Expected result(s) and impact ...................................................................................... 3 \n\n1.3.4. Indicators of performance ............................................................................................ 3 \n\n1.4. The proposal/initiative relates to: ................................................................................. 4 \n\n1.5. Grounds for the proposal/initiative .............................................................................. 4 \n\n1.5.1. Requirement(s) to be met in the short or long term including a detailed timeline for roll-out of the implementation of the initiative ............................................................ 4 \n\n1.5.2. Added value of EU involvement (it may result from different factors, e.g. coordination gains, legal certainty, greater effectiveness or complementarities). For the purposes of this section â€˜added value of EU involvementâ€™ is the value resulting from EU action, that is additional to the value that would have been otherwise created by Member States alone. ................................................................................. 4 \n\n1.5.3. Lessons learned from similar experiences in the past .................................................. 4 \n\n1.5.4. Compatibility with the multiannual financial framework and possible synergies with other appropriate instruments....................................................................................... 5 \n\n1.5.5. Assessment of the different available financing options, including scope for redeployment ................................................................................................................ 5 \n\n1.6. Duration of the proposal/initiative and of its financial impact .................................... 6 \n\n1.7. Method(s) of budget implementation planned ............................................................. 6 \n\n2. MANAGEMENT MEASURES .................................................................................. 8 \n\n2.1. Monitoring and reporting rules .................................................................................... 8 \n\n2.2. Management and control system(s) ............................................................................. 8 \n\n2.2.1. Justification of the budget implementation method(s), the funding implementation mechanism(s), the payment modalities and the control strategy proposed .................. 8 \n\n2.2.2. Information concerning the risks identified and the internal control system(s) set up to mitigate them............................................................................................................ 8 \n\n2.2.3. Estimation and justification of the cost-effectiveness of the controls (ratio between the control costs and the value of the related funds managed), and assessment of the expected levels of risk of error (at payment & at closure) ........................................... 8 \n\n2.3. Measures to prevent fraud and irregularities................................................................ 9 \n\n3. ESTIMATED FINANCIAL IMPACT OF THE PROPOSAL/INITIATIVE............ 10 \n\n3.1. Heading(s) of the multiannual financial framework and expenditure budget line(s) affected ....................................................................................................................... 10 EN 2 EN \n\n3.2. Estimated financial impact of the proposal on appropriations ................................... 12 \n\n3.2.1. Summary of estimated impact on operational appropriations.................................... 12 \n\n3.2.1.1. Appropriations from voted budget ............................................................................. 12 \n\n3.2.1.2. Appropriations from external assigned revenues ....................................................... 17 \n\n3.2.2. Estimated output funded from operational appropriations......................................... 22 \n\n3.2.3. Summary of estimated impact on administrative appropriations ............................... 24 \n\n3.2.3.1. Appropriations from voted budget .............................................................................. 24 \n\n3.2.3.2. Appropriations from external assigned revenues ....................................................... 24 \n\n3.2.3.3. Total appropriations ................................................................................................... 24 \n\n3.2.4. Estimated requirements of human resources.............................................................. 25 \n\n3.2.4.1. Financed from voted budget....................................................................................... 25 \n\n3.2.4.2. Financed from external assigned revenues ................................................................ 26 \n\n3.2.4.3. Total requirements of human resources ..................................................................... 26 \n\n3.2.5. Overview of estimated impact on digital technology-related investments ................ 28 \n\n3.2.6. Compatibility with the current multiannual financial framework.............................. 28 \n\n3.2.7. Third-party contributions ........................................................................................... 28 \n\n3.3. Estimated impact on revenue ..................................................................................... 29 \n\n4. DIGITAL DIMENSIONS .......................................................................................... 29 \n\n4.1. Requirements of digital relevance.............................................................................. 30 \n\n4.2. Data ............................................................................................................................ 30 \n\n4.3. Digital solutions ......................................................................................................... 31 \n\n4.4. Interoperability assessment ........................................................................................ 31 \n\n4.5. Measures to support digital implementation .............................................................. 32 EN 3 EN \n\n1. FRAMEWORK OF THE PROPOSAL/INITIATIVE \n\n1.1. Title of the proposal/initiative \n\nProposal for a Regulation of the European Parliament and of the Council amending \n\nRegulations (EU) 2024/1689 and (EU) 2018/1139 as regards the simplification of the \n\nimplementation of harmonised rules on artificial intelligence (Digital Omnibus on \n\nAI) \n\n1.2. Policy area(s) concerned \n\nCommunications Networks, Content and Technology; \n\nInternal Market, Industry, Entrepreneurship and SMEs \n\nThe budgetary impact concerns the new tasks entrusted with the AI Office. \n\n1.3. Objective(s) \n\n1.3.1. General objective(s) \n\n1. To strengthen the monitoring and supervision of certain categories of AI systems \n\nby the AI Office. \n\n2. To facilitate the development and testing at EU level of innovative AI systems \n\nunder strict regulatory oversight before these systems are placed on the market or \n\notherwise put into service. \n\n1.3.2. Specific objective(s) \n\nSpecific objective No 1 \n\nTo enhance governance and effective enforcement of the AI Act rules related to AI \n\nsystems by reinforcing the powers and procedures applicable as well as by providing \n\nfor new resources for the AI Office in charge of the enforcement. \n\nSpecific objective No 2 \n\nTo provide for the establishment of a sandbox at EU level, enabling cross border \n\nactivities and testing. \n\n1.3.3. Expected result(s) and impact \n\n> Specify the effects which the proposal/initiative should have on the beneficiaries/groups targeted.\n\nAI providers should benefit from a centralised level of governance and the access to \n\nan EU-level sandbox for certain categories of AI systems, avoiding duplication of \n\nprocedures and costs. \n\n1.3.4. Indicators of performance \n\n> Specify the indicators for monitoring progress and achievements.\n\nIndicator 1 \n\nNumber of AI systems falling under the scope of the monitoring and supervision \n\ntasks to be carried out by the AI Office. \n\nIndicator 2 \n\nNumber of providers and prospective providers requesting access to the sandbox at \n\nEU level. EN 4 EN \n\n1.4. The proposal/initiative relates to:  \n\n> ï‚¨\n\na new action  \n\n> ï‚¨\n\na new action following a pilot project / preparatory action 26  \n\n> ï¸\n\nthe extension of an existing action  \n\n> ï‚¨\n\na merger or redirection of one or more actions towards another/a new action \n\n1.5. Grounds for the proposal/initiative \n\n1.5.1. Requirement(s) to be met in the short or long term including a detailed timeline for roll-out of the implementation of the initiative \n\nThe additional elements relevant for the enhancement of the governance structure of \n\nthe AI Office should be in place before the entry into application of the provisions \n\napplicable to AI systems. \n\nThe first EU sandbox is expected to be operational in 2028, although some key \n\nsetting details should be established beforehand. \n\n1.5.2. Added value of EU involvement (it may result from different factors, e.g. coordination gains, legal certainty, greater effectiveness or complementarities). For the purposes of this section 'added value of EU involvement' is the value resulting from EU action, that is additional to the value that would have been otherwise created by Member States alone. \n\nThe AI Office will have the power to monitor and supervise the compliance of all AI \n\nsystems based on general-purpose AI (GPAI) models, where the model and the \n\nsystem are developed by the same provider, as well as all AI systems embedded in or \n\nconstituting very large online platforms or search engines, even if the system and \n\nGPAI model provider are different. The tasks that the AI Office would need to carry \n\nout for this vast range of AI systems include requesting full access to documentation, \n\ntraining/validation/testing datasets, and, when necessary, the source code of high-risk \n\nAI systems, supervising real-world testing, identifying and evaluating risks, dealing \n\nwith serious incidents, taking preventive and corrective measures while ensuring \n\ncooperation with national market surveillance authorities, dealing with AI systems \n\nclassified as not high-risk by the provider, dealing with complaints of non-\n\ncompliance, and imposing penalties. Moreover, to allow market access for AI \n\nsystems in the scope of this provision which are also subject to pre-market third-\n\nparty conformity assessment under the AI Act, the AI Office will be the responsible \n\nbody to carry out conformity assessments. All these actions require resources and a \n\nset of enforcement procedures to be developed and implemented, as well as the \n\nappropriate technical support to assess and evaluate systems. \n\nThe AI Officeâ€™s role in ensuring compliance would also involve ensuring synergies \n\nwith the evaluation of the GPAI models, which would strengthen the overall \n\nevaluations of models and systems provided by the same provider. This would enable \n\na more comprehensive understanding of the AI systems and their associated risks,  \n\n> 26 As referred to in Article 58(2), point (a) or (b) of the Financial Regulation.\n\n# EN 5 EN \n\nallowing for more effective monitoring and enforcement. The AI Office will also \n\nneed to consider the unique challenges posed by agentic AI, which can operate \n\nautonomously and make decisions that may have significant consequences, and \n\ndevelop strategies to address these risks in line with Commission policies. \n\nThe enhancement of the AI Officeâ€™s governance would bring numerous benefits to \n\nthe regulation of AI systems in the EU. One of the primary advantages is the \n\nconsistency and coherence it would ensure in the application of the AI Act across the \n\nEU. By having a single authority overseeing the implementation of the AI Act in \n\nrelation to certain categories of AI systems, the risk of conflicting interpretations and \n\ndecisions would be significantly reduced, providing clarity and certainty for \n\ncompanies operating in the EU. \n\nFurthermore, this would simplify the regulatory landscape for companies, as they \n\nwould only need to deal with one regulator, rather than multiple national authorities. \n\nThis would reduce the complexity and administrative burden associated with \n\nnavigating different regulatory frameworks, allowing companies to focus on \n\ninnovation and growth. The centralised approach would also enable the development \n\nwithin the Commission of specialised expertise in AI systems and GPAI models, \n\nenabling more effective monitoring and enforcement of the AI Act. \n\nThis approach would avoid diverging national enforcement actions on the AI systems \n\nconcerned that may lead to the fragmentation of the internal market and decrease \n\nlegal certainty for operators. This would also address the challenges faced by \n\nMember States in securing specialised resources to staff their authorities responsible \n\nfor implementing the AI Act and overseeing AI systems within their territories. By \n\ncentralizing market surveillance authoritiesâ€™ powers within the AI Office, this \n\nscenario would enable the AI Office to assume responsibility for evaluating and \n\nmonitoring complex AI systems provided by the same model provider, as well as AI \n\nsystems constituting or embedded into platforms, thereby alleviating the burden on \n\nnational authorities. This would leverage the AI Office's existing expertise in \n\nevaluating GPAI models and monitoring their compliance, creating a unique \n\nconcentration of specialized knowledge and capabilities. As a result, the AI Office \n\nwould be well-positioned to provide consistent and effective oversight, while also \n\nsupporting Member States in their efforts to implement the AI Act and ensure a \n\nharmonized regulatory environment across the EU. With the AI Office handling \n\nthese additional tasks, national authorities could focus more on their enforcement \n\nactions under the AI Act, allowing for a more efficient allocation of resources and a \n\nmore effective implementation of the AI Act across the EU. \n\n1.5.3. Lessons learned from similar experiences in the past \n\nThe European Commission's experience in enforcing the Digital Services Act (DSA) \n\nprovides valuable lessons that can be applied to the enforcement of the AI Act. In \n\nparticular, the establishment of a robust and transparent enforcement framework, \n\nwhich sets out clear procedures for investigating and addressing breaches of the DSA \n\nand the close cooperation with national authorities, to ensure that enforcement \n\nactions are coordinated and effective, represent relevant elements in this context. \n\nThe Commissionâ€™s experience with DSA enforcement has shown that this approach \n\ncan be effective in promoting compliance and protecting users' rights. For example, \n\nthe Commission has already taken action against several online platforms for \n\nbreaches of the DSA, and has worked with national authorities to develop guidance \n\nand best practices for compliance. EN 6 EN \n\nBy building on the lessons learned from DSA enforcement, the Commission can \n\ndevelop an effective enforcement framework for the AI Act that promotes \n\ncompliance, and supports the development of a trustworthy and innovative AI \n\necosystem in the EU. This will involve enhancing the AI Office enforcement role to \n\nduly monitor and supervision certain categories of AI systems, and working closely \n\nwith national authorities to ensure that the AI Act is enforced in a consistent and \n\neffective manner. \n\nThe possibility to provide for an EU-level sandbox should be seen as complementing \n\nthe sandboxes established at national level and should be implemented in a way to \n\nfacilitate cross-border cooperation between national competent authorities. \n\n1.5.4. Compatibility with the multiannual financial framework and possible synergies with other appropriate instruments \n\nThe amendments proposed to the AI Act within this initiative would result in a \n\nsignificant increase in the number of AI systems subject to the monitoring and \n\nsupervision of the AI Office, with a corresponding rise in the number of systems \n\npotentially eligible to participate in an EU-level sandbox. To effectively manage this \n\nexpansion, it is essential to strengthen the European regulatory and coordination \n\nfunction, as proposed in this initiative. This reinforcement would enable the AI \n\nOffice to efficiently oversee the growing number of AI systems, ensure compliance \n\nwith the regulatory framework, and provide a supportive environment for innovation \n\nand testing through the EU-level sandbox. \n\n1.5.5. Assessment of the different available financing options, including scope for redeployment \n\nThe AI Office will make an effort in order to redeploy part of the staff allocated but \n\ncould do it only partially (15 FTEs) as the staff is currently fully allocated to tasks \n\ndirectly linked to ensuring a timely and correct implementation of the AI Act. New \n\nresources will be needed (estimated in 38 additional FTEs) to efficiently exercise the \n\nnew enforcement tasks. \n\nIn particular, the AI Office plans to identify colleagues with legal and procedural \n\nexpertise who can take on part of the upcoming new enforcement tasks. At this stage, \n\nwe estimate that around 5 CAs with relevant profiles can be redeployed for this \n\npurpose. \n\nIn addition, the AI Office will make an effort to redeploy 5 officials. \n\nThe AI Office envisages to make fully operational the EU-level sandbox for AI \n\nsystems falling under its monitoring in 2028, which will make possible a\n\nredeployment of 3 CAs needed to set up and run the sandbox. This phased approach \n\nwould enable to ensure the full operational capacity of the sandbox by 2028, and in \n\nparticular will also give the AI Office the time to identify the most suitable staff to \n\ncover this task and ensure proper project management for facilitating the \n\ndevelopment, training, testing, and validation of innovative AI systems. \n\nIn addition, the AI Office will explore opportunities to expand the scope of IT tools \n\n(currently mostly in development or pre-launch phase) supporting the AI Act to also \n\ncover relevant new enforcement activities (i.e. case handling, AI system registry, \n\nmonitoring and reporting, exchange of information with authorities). 2 FTEs with IT \n\nand administrative profiles will be redeployed to manage these IT tools. This would \n\nhelp to partially cover the management needs related to the new tasks. EN 7 EN \n\nOverall, these redeployment efforts and synergies will help to address some of the \n\nstaffing needs for the new enforcement tasks, while additional resources will be \n\nnecessary to ensure the effective implementation of the AI Act. \n\nAdditional staff will be funded under DEP support, given that the objectives of the \n\nproposed amendments contribute directly to one key objective of Digital Europe â€“ \n\naccelerating AI development and deployment in Europe. EN 8 EN \n\n1.6. Duration of the proposal/initiative and of its financial impact \n\nï‚¨ limited duration \n\nâ€“ ï‚¨ in effect from [DD/MM]YYYY to [DD/MM]YYYY \n\nâ€“ ï‚¨ financial impact from YYYY to YYYY for commitment appropriations and from YYYY to YYYY for payment appropriations. \n\nï¸ unlimited duration \n\nâ€“ Implementation with a start-up period from 2026 to 2027, \n\nâ€“ followed by full-scale operation. \n\n1.7. Method(s) of budget implementation planned \n\nï¸ Direct management by the Commission \n\nâ€“ ï‚¨ by its departments, including by its staff in the Union delegations; \n\nâ€“ ï‚¨ by the executive agencies \n\nï‚¨ Shared management with the Member States \n\nï‚¨ Indirect management by entrusting budget implementation tasks to: \n\nâ€“ ï‚¨ third countries or the bodies they have designated \n\nâ€“ ï‚¨ international organisations and their agencies (to be specified) \n\nâ€“ ï‚¨ the European Investment Bank and the European Investment Fund \n\nâ€“ ï‚¨ bodies referred to in Articles 70 and 71 of the Financial Regulation \n\nâ€“ ï‚¨ public law bodies \n\nâ€“ ï‚¨ bodies governed by private law with a public service mission to the extent that they are provided with adequate financial guarantees \n\nâ€“ ï‚¨ bodies governed by the private law of a Member State that are entrusted with the implementation of a public-private partnership and that are provided with adequate financial guarantees \n\nâ€“ ï‚¨ bodies or persons entrusted with the implementation of specific actions in the common foreign and security policy pursuant to Title V of the Treaty on European Union, and identified in the relevant basic act \n\nâ€“ ï‚¨ï‚  bodies established in a Member State, governed by the private law of a Member State or Union law and eligible to be entrusted, in accordance with sector-specific rules, with the implementation of Union funds or budgetary guarantees, to the extent that such bodies are controlled by public law bodies or by bodies governed by private law with a public service mission, and are provided with adequate financial guarantees in the form of joint and several liability by the controlling bodies or equivalent financial guarantees and which may be, for each action, limited to the maximum amount of the Union support. EN 9 EN \n\n2. MANAGEMENT MEASURES \n\n2.1. Monitoring and reporting rules \n\nSpecify frequency and conditions. \n\nThe strengthened dispositions will be reviewed and evaluated with the entire AI Act \n\nin August 2029. The Commission will report on the findings of the evaluation to the \n\nEuropean Parliament, the Council and the European Economic and Social \n\nCommittee. \n\n2.2. Management and control system(s) \n\n2.2.1. Justification of the budget implementation method(s), the funding implementation mechanism(s), the payment modalities and the control strategy proposed \n\nThe regulation reinforces the European policy with regard to harmonised rules for \n\nthe provision of artificial intelligence systems in the internal market while ensuring \n\nthe respect of safety and fundamental rights. The simplified single supervision \n\nensures consistency for the cross-border application of the obligations under this \n\nRegulation. \n\nIn order to face these new tasks, it is necessary to appropriately resource the \n\nCommissionâ€™s services. The enforcement of the new regulation is estimated to \n\nrequire 53 FTE. \n\n2.2.2. Information concerning the risks identified and the internal control system(s) set up to mitigate them \n\nThe risks correspond to the standard risks of Commission operations and are \n\nadequately covered by existing standard risk minimising procedures. \n\n2.2.3. Estimation and justification of the cost-effectiveness of the controls (ratio between the control costs and the value of the related funds managed), and assessment of the expected levels of risk of error (at payment & at closure) \n\nFor the meeting expenditure, given the low value per transaction (e.g. refunding \n\ntravel costs for a delegate for a meeting), standard control procedures seem \n\nsufficient. \n\n2.3. Measures to prevent fraud and irregularities \n\nSpecify existing or envisaged prevention and protection measures, e.g. from the anti-fraud strategy. \n\nThe existing fraud prevention measures applicable to the Commission will cover the \n\nadditional appropriations necessary for this Regulation. EN 10 EN \n\n3. ESTIMATED FINANCIAL IMPACT OF THE PROPOSAL/INITIATIVE \n\n3.1. Heading(s) of the multiannual financial framework and expenditure budget line(s) affected \n\nâ€¢ Existing budget lines \n\nIn order of multiannual financial framework headings and budget lines. \n\n> Heading of multiannual financial framework\n\nBudget line  Type of expenditure  Contribution \n\nNumber  Diff./Non-diff. 27 \n\n> from EFTA countries\n> 28\n> from candidate countries and potential candidates\n> 29\n> From other third countries\n> other assigned revenue\n\n7 20 02 06 Administrative expenditure  Nondiff  No \n\n1 02 04 03 DEP Artificial Intelligence  Diff.  YES  NO  yes  NO \n\n1 02 01 30 01 Support expenditure for the Digital Europe programme  Nondiff  yes  yes  \n\n> 27\n\nDiff. = Differentiated appropriations / Non-diff. = Non-differentiated appropriations.  \n\n> 28\n\nEFTA: European Free Trade Association.  \n\n> 29\n\nCandidate countries and, where applicable, potential candidates from the Western Balkans. EN 11 EN \n\n3.2. Estimated financial impact of the proposal on appropriations \n\n3.2.1. Summary of estimated impact on operational appropriations \n\nâ€“ ï‚¨ The proposal/initiative does not require the use of operational appropriations \n\nâ€“ ï¸ The proposal/initiative requires the use of operational appropriations, as explained below \n\n3.2.1.1. Appropriations from voted budget \n\n[\n\n> EUR million (to three decimal places)\n\nHeading of multiannual financial framework  1\n\nDG: CNECT \n\nYear  Year  Year  Year  After 2027  TOTAL MFF 2021-2027 \n\n2024  2025  2026  2027  After 2027 \n\nBudget line 02 04 03   \n\n> Commitments (1a)\n\n0,500 30  0,500 31  1,000   \n\n> Payments (2a)\n\n0,500  0,500  1,000 \n\nAppropriations of an administrative nature financed from the envelope of specific programmes  \n\n> 30\n\nThis budget is already eamarked in the DEP WP 26-27 for the AI office  \n\n> 31\n\nThis budget is already eamarked in the DEP WP 26-27 for the AI office EN 12 EN \n\nBudget line 02 01 30 01  (3)  2,642 32  6,283  33  7,283  8,925 \n\nTOTAL appropriations \n\nfor DG CNECT \n\nCommitments  =1a+1b+3  3,142  6,783  7,283  9,925 \n\nPayments  =2a+2b+3  2,642  6,783  7,783  9,925 \n\nTOTAL \n\nYear  Year  Year  Year  After 2027  TOTAL MFF 2021-2027 \n\n2024  2025  2026  2027  After 2027 \n\nBudget line 02 04 03 \n\nCommitments  (1a)  0,500 34  0,500 35  1,000 \n\nPayments  (2a)  0,500  0,500  1,000 \n\nAppropriations of an administrative nature financed from the envelope of specific programmes  \n\n> 32\n\nThis budget corresponds to [48] additional FTEs for 6 months [(43 CAs and 5 SNEs)], the baseline being the staffing level agreed in the context of the 2026 budgetary procedure. The budget will be redeployed in the DEP admin envelope to cover the additional costs.  \n\n> 33\n\nThe amount will be redeployed from 02.0403 (SO2 artificial intelligence) in 2027, the request will be introduced in the 2027 budgetary procedure.  \n\n> 34\n\nThis budget is already earmarked in the DEP WP 26-27 for the AI Office.  \n\n> 35\n\nThis budget is already earmarked in the DEP WP 26-27 for the AI Office. EN 13 EN \n\nBudget line 02 01 30 01  (3)  2,642 36  6,283  37  7,283  8,925 \n\nTOTAL appropriations \n\nfor DG CNECT \n\nCommitments  =1a+1b+3  3,142  6,783  7,283  9,925 \n\nPayments  =2a+2b+3  2,642  6,783  7,783  9,925 \n\n]\n\n[\n\nHeading of multiannual financial framework  7 â€˜Administrative expenditureâ€™ \n\nDG: CNECT  Year  Year  Year  Year  TOTAL \n\nMFF 2021-2027 2024  2025  2026  2027 \n\nï‚Ÿ Human resources  0,940  0,940  1,880 \n\nï‚Ÿ Other administrative expenditure  0,025  0,025  0,050 \n\nTOTAL DG CNECT  Appropriations  0,965  0,965  1,930  \n\n> 36\n\nThis budget corresponds to 48 additional FTEs for 6 months (43 CAs and 5 SNEs), the baseline being the staffing level agreed in the context of the 2026 budgetary procedure. The budget will be redeployed in the DEP admin envelope to cover the additional costs.  \n\n> 37\n\nThe amount will be redeployed from 02.0403 (SO2 artificial intelligence) in 2027, the request will be introduced in the 2027 budgetary procedure. EN 14 EN \n\nTOTAL appropriations under HEADING 7 of the multiannual financial framework \n\n(Total commitments = Total payments) \n\n0,965  0,965  1,930 \n\nEUR million (to three decimal places) \n\nYear  Year  Year  Year  After 2027  TOTAL MFF 2021-2027 2024  2025  2026  2027 \n\nTOTAL appropriations under HEADINGS 1 to 7  Commitments  4,107  7,748  8,248  11,855 \n\nof the multiannual financial framework  Payments  3,607  7,748  8,748  11,855 \n\n]\n\n3.2.2. Estimated output funded from operational appropriations (not to be completed for decentralised agencies) \n\nCommitment appropriations in EUR million (to three decimal places) \n\nIndicate objectives and outputs \n\nïƒ²\n\nYear \n\n2024 \n\nYear \n\n2025 \n\nYear \n\n2026 \n\nYear \n\n2027 \n\nEnter as many years as necessary to show the duration of the impact (see Section1.6)  TOTAL \n\nOUTPUTS \n\nType 38  Avera ge cost \n\n> No\n\nCost \n\n> No\n\nCost \n\n> No\n\nCost \n\n> No\n\nCost \n\n> No\n\nCost \n\n> No\n\nCost \n\n> No\n\nCost  Total No \n\nTotal cost \n\nSPECIFIC OBJECTIVE No 1 39 â€¦ \n\n> 38\n\nOutputs are products and services to be supplied (e.g. number of student exchanges financed, number of km of roads built, etc.). EN 15 EN \n\n- Output \n\n- Output \n\n- Output \n\nSubtotal for specific objective No 1 \n\nSPECIFIC OBJECTIVE No 2 ... \n\n- Output \n\nSubtotal for specific objective No 2 \n\nTOTALS  \n\n> 39\n\nAs described in Section 1.3.2. â€˜Specific objective(s)â€™ EN 16 EN \n\n3.2.3. Summary of estimated impact on administrative appropriations \n\nâ€“ ï‚¨ The proposal/initiative does not require the use of appropriations of an administrative nature \n\nâ€“ ï¸ The proposal/initiative requires the use of appropriations of an administrative nature, as explained below \n\n3.2.3.1. Appropriations from voted budget \n\n[\n\nVOTED APPROPRIATIONS  Year  Year  Year  Year  TOTAL 2021 - 2027 \n\n2024  2025  2026  2027                   \n\n> HEADING 7\n> Human resources 0,940 0,940 1,880\n> Other administrative expenditure 0,025 0,025 0,050\n> Subtotal HEADING 7 0,965 0,965 1,930\n> Outside HEADING 7\n> Human resources 2,429 4,858 7,287\n> Other expenditure of an administrative nature 0,213 1,425 1,638\n> Subtotal outside HEADING 7 2,642 6,283 8,925\n\n]\n\nThe appropriations required for human resources and other expenditure of an administrative nature will be met by appropriations from the DG that are already assigned to management of the action and/or have been redeployed within the DG, together, if necessary, with any additional allocation which may be granted to the managing DG under the annual allocation procedure and in the light of budgetary constraints. \n\n3.2.4. Estimated requirements of human resources \n\nâ€“ ï‚¨ The proposal/initiative does not require the use of human resources \n\nâ€“ ï¸ The proposal/initiative requires the use of human resources, as explained below EN 17 EN \n\n3.2.4.1. Financed from voted budget \n\nEstimate to be expressed in full-time equivalent units (FTEs) \n\n[\n\nVOTED APPROPRIATIONS  Year  Year  Year  Year \n\n2024  2025  2026  2027 \n\nï‚Ÿ Establishment plan posts (officials and temporary staff) \n\n20 01 02 01 (Headquarters and Commissionâ€™s Representation Offices)  0 0 5 5\n\n20 01 02 03 (EU Delegations)  0 0 0 0\n\n01 01 01 01 (Indirect research)  0 0 0 0\n\n01 01 01 11 (Direct research)  0 0 0 0\n\nOther budget lines (specify)  0 0 0 0\n\nâ€¢ External staff (in FTEs) \n\n20 02 01 (AC, END from the â€˜global envelopeâ€™)  0 0 0 0\n\n20 02 03 (AC, AL, END and JPD in the EU Delegations)  0 0 0 0\n\nAdmin. Support line [XX.01.YY.YY] \n\n- at Headquarters  0 0 0 0\n\n- in EU Delegations  0 0 0 0\n\n01 01 01 02 (AC, END - Indirect research)  0 0 0 0\n\n01 01 01 12 (AC, END - Direct research)  0 0 0 0\n\nOther budget lines (specify) - Heading 7  0 0 0 0\n\nOther budget lines (02 01 30 01) - Outside Heading 7  0 0 48  48 \n\nTOTAL  0 0 53  53 \n\n]\n\nThe staff required to implement the proposal (in FTEs): \n\nTo be covered by current staff \n\nExceptional additional staff* EN 18 EN \n\navailable in the \n\nCommission services \n\nTo be financed under Heading 7 or Research \n\nTo be financed from BA line \n\nTo be financed from fees \n\nEstablishment plan posts \n\n5 N/A \n\nExternal staff (CA, SNEs, INT) \n\n10  38 \n\nDescription of tasks to be carried out by:  \n\n> Officials and temporary staff The strengthening of the central supervision by the AI Office will lead to a significant increase in the number of AI systems. These task cannot be carried out by the current staff levels, which are only sufficient for the current scope of supervision. External staff\n\n3.2.5. Overview of estimated impact on digital technology-related investments \n\nCompulsory: the best estimate of the digital technology-related investments entailed by the proposal/initiative should be included in the table below. \n\nExceptionally, when required for the implementation of the proposal/initiative, the appropriations under Heading 7 should be presented in the designated line. \n\nThe appropriations under Headings 1-6 should be reflected as â€˜Policy IT expenditure on operational programmesâ€™. This expenditure refers to the operational budget to be used to re-use/ buy/ develop IT platforms/ tools directly linked to the implementation of the initiative and their associated investments (e.g. licences, studies, data storage etc). The information provided in this table should be consistent with details presented under Section 4 â€˜Digital dimensionsâ€™. EN 19 EN \n\nTOTAL Digital and IT appropriations \n\nYear  Year  Year  Year  TOTAL MFF 2021 -2027 2024  2025  2026  2027 \n\n> HEADING 7\n\nIT expenditure (corporate)  0.000  0.000  0.000  0.000  0.000      \n\n> Subtotal HEADING 7 0.000 0.000 0.000 0.000 0.000\n> Outside HEADING 7\n\nPolicy IT expenditure on operational \n\nprogrammes  0.000  0.000  0.000  0.000  0.000           \n\n> Subtotal outside HEADING 7 0.000 0.000 0.000 0.000 0.000\n> TOTAL 0.000 0.000 0.000 0.000 0.000\n\n3.2.6. Compatibility with the current multiannual financial framework \n\nThe proposal/initiative: \n\nâ€“ ï¸ can be fully financed through redeployment within the relevant heading of the multiannual financial framework (MFF) \n\nThe amounts will be redeployed from 02.013001 support expenditure for the Digital Europe Programme for 2026 and from 02.0403 (SO2 artificial intelligence) for 2027. \n\nâ€“ ï‚¨ requires use of the unallocated margin under the relevant heading of the MFF and/or use of the special instruments as defined in the MFF Regulation \n\nâ€“ ï‚¨ requires a revision of the MFF \n\n3.2.7. Third-party contributions \n\nThe proposal/initiative: \n\nâ€“ ï¸ does not provide for co-financing by third parties \n\nâ€“ ï‚¨ provides for the co-financing by third parties estimated below: \n\nAppropriations in EUR million (to three decimal places) EN 20 EN \n\nYear \n\n2024 \n\nYear \n\n2025 \n\nYear \n\n2026 \n\nYear \n\n2027  Total \n\nSpecify the co-financing body \n\nTOTAL appropriations co-financed \n\n3.3. Estimated impact on revenue \n\nâ€“ ï¸ The proposal/initiative has no financial impact on revenue. \n\nâ€“ ï‚¨ The proposal/initiative has the following financial impact: \n\nâ€“ ï‚¨ on own resources \n\nâ€“ ï‚¨ on other revenue \n\nâ€“ ï‚¨ please indicate, if the revenue is assigned to expenditure lines \n\nEUR million (to three decimal places)         \n\n> Budget revenue line: Appropriations available for the current financial year\n> Impact of the proposal/initiative 40\n> Year 2024 Year 2025 Year 2026 Year 2027\n> Article â€¦â€¦â€¦â€¦.\n\nFor assigned revenue, specify the budget expenditure line(s) affected.  \n\n> 40\n\nAs regards traditional own resources (customs duties, sugar levies), the amounts indicated must be net amounts, i.e. gross amounts after deduction of 20% for collection costs. EN 21 EN \n\nOther remarks (e.g. method/formula used for calculating the impact on revenue or any other information). \n\n4. DIGITAL DIMENSIONS \n\n4.1. Requirements of digital relevance \n\nReference to the requirement  Requirement description  Actors affected or concerned by the requirement \n\nHigh-level Processes  Categories \n\nArticle 1(5)  Inserting Article 4a : Allowing providers and deployers of AI systems and AI models to exceptionally process special categories of personal data to the extent necessary for the purpose of ensuring bias detection and correction, subject to certain conditions. \n\nProviders and deployers of AI systems and AI models \n\nConcerned data subjects \n\nData processing  Data \n\nArticle 1(8)  Amending Article 11(1), second subparagraph : Relating to the technical documentation of high-risk AI systems that needs to be drawn up before that system is placed on the market or put into service. SMEs and SMCs are given certain regulatory privileges as concerns this provision of information. \n\nProviders of high-risk AI systems (including SMCs and SMEs) \n\nNational competent authorities \n\nNotified bodies \n\nEuropean Commission \n\nTechnical documentation \n\nData EN 22 EN \n\nArticle 1(10)  Amending Article 28, inserting paragraph (1a) : Conformity assessment bodies that apply for a designation may be offered the possibility to submit a single application and undergo a single assessment procedure. \n\nConformity assessment bodies \n\nNotifying authorities \n\nApplication submission \n\nData \n\nArticle 1(11)  Amending Article 29(4 ): Notified bodies which apply for a single assessment shall submit the single application to the notifying authority. The notified body shall update the documentation if relevant changes occur. \n\nNotified bodies \n\nNotifying authority \n\nApplication submission \n\nData \n\nArticle 1(16)  Amending Article 56(6) : The Commission shall publish its assessments on the adequacy of the codes of practice. \n\nEuropean Commission  Assessment publication \n\nData \n\nArticle 1(26)  Amending Article 77 :\n\nâ€¢ Paragraph 1 : National public authorities/bodies which supervise/enforce EU law obligations protecting fundamental rights may make a reasoned request and access any information/documentation from the relevant market surveillance authority \n\nâ€¢ Paragraph 1a : market surveillance authority shall grant access and, where needed, request the information from the provider/deployer \n\nâ€¢ Paragraph 1b : where necessary, the aforementioned market surveillance \n\nNational public authorities/bodies which supervise/enforce EU law obligations protecting fundamental rights \n\nMarket surveillance authorities \n\nProviders/deployers of AI systems \n\nInformation exchange \n\nData EN 23 EN \n\nauthorities and public \n\nauthorities/bodies shall exchange information. \n\n4.2. Data \n\nHigh-level description of the data in scope \n\nType of data  Reference to the requirement(s)  Standard and/or specification (if applicable) \n\nSpecial categories of personal data (where the processing is needed for bias detection/correction) \n\nArticle 1(5)  // \n\nTechnical documentation for high-risk AI systems  Article 1(8)  Technical documentation shall contain, at a minimum, the elements set out in Annex IV of the AI Act. The Commission shall establish a simplified technical documentation form targeted at SMCs and SMEs. \n\nApplications of conformity assessment bodies for designation \n\nArticle 1(10)  // \n\nApplications of a conformity assessment bodies for notification \n\nArticle 1(11)  The notified body shall update the relevant documentation whenever relevant changes occur. \n\nCommission assessment of the adequacy of the codes of practice \n\nArticle 1(16)  // EN 24 EN \n\nRequest for access to information on AI systems  Article 1(26)  // \n\nInformation or documentation requested by national public authorities/bodies which supervise/enforce obligations relating to fundamental rights \n\nArticle 1(26)  To be provided in accessible language and format. \n\nAlignment with the European Data Strategy \n\nExplanation of how the requirement(s) are aligned with the European Data Strategy \n\nArticle 1(4) establishes that the processing of special categories of personal data shall be subject to appropriate safeguards for fundamental rights and freedoms of natural persons. This is in alignment with Regulations (EU) 2016/679 (GDPR) and (EU) 2018/1725 (EUDPR). \n\nAlignment with the once-only principle \n\nExplanation of how the once-only principle has been considered and how the possibility to reuse existing data has been explored \n\nArticle 1(10) states that conformity assessment bodies may be provided the possibility to submit a single application and undergo a single assessment procedure. \n\nExplanation of how newly created data is findable, accessible, interoperable and reusable, and meets high-quality standards \n\nData flows \n\nHigh-level description of the data flows \n\nType of data  Reference(s) to the \n\nActors who provide the data \n\nActors who receive the data \n\nTrigger for the data exchange \n\nFrequency (if applicable) EN 25 EN \n\nrequirement(s) \n\nApplications of a conformity assessment bodies for notification \n\nArticle 1(11)  Notified bodies which are designated under Union harmonisation legislation listed in Section A of Annex I \n\nNotifying authority designated in accordance with Union harmonisation legislation listed in Section A of Annex I\n\nApplication being made for single assessment \n\n// \n\nCommission assessment of the adequacy of the codes of practice \n\nArticle 1(16)  European Commission \n\nGeneral Public  Performance of an assessment as regards the codes of practice \n\nRegularly \n\nRequest for access to information on AI systems \n\nArticle 1(26)  National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights \n\nMarket surveillance authority \n\nNational public authorities/bodies require the information in order to fulfil their mandates \n\n// \n\nInformation or documentation requested by national public \n\nArticle 1(26)  Market surveillance \n\nNational public authorities or \n\nSubmission of a reasoned request to \n\n// EN 26 EN \n\nauthorities/bodies which \n\nsupervise/enforce obligations relating to fundamental rights \n\nauthority  bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights \n\naccess information \n\nInformation or documentation requested by market surveillance authorities \n\nArticle 1(26)  Market surveillance authorities \n\nProviders/ deployers of AI systems \n\nMarket surveillance authority is in need of the information so as to answer to a request from national public authorities/bodies which supervise/enforce obligations relating to fundamental rights) \n\n// \n\nInformation exchanges as part of the cooperation of market surveillance authorities and public authorities/bodies which supervise/enforce obligations relating to fundamental rights \n\nArticle 1(26)  Market surveillance authorities \n\n/ Public authorities/bodies \n\nMarket surveillance authorities \n\n/ Public authorities/bodies \n\nInformation exchange need identified in the course of cooperation and mutual assistance \n\n// \n\n4.3. Digital solutions \n\nHigh-level description of digital solutions EN 27 EN \n\nDigital solution \n\nReference(s) to the requirement(s) \n\nMain mandated functionalities  Responsible body \n\nHow is accessibility catered for? \n\nHow is reusability considered? \n\nUse of AI technologies (if applicable) \n\nN.A. (the proposed amendments to the AI Act do not foresee the adoption of new digital solutions) \n\nFor each digital solution, explanation of how the digital solution complies with applicable digital policies and legislative enactments \n\nDigital Solution #1 \n\nDigital and/or sectorial policy (when these are applicable) \n\nExplanation on how it aligns \n\nAI Act \n\nEU Cybersecurity framework \n\neIDAS \n\nSingle Digital Gateway and IMI \n\nOthers EN 28 EN \n\n4.4. Interoperability assessment \n\nHigh-level description of the digital public service(s) affected by the requirements \n\nDigital public \n\nservice or category of digital public services \n\nDescription  Reference(s) to the requirement(s) \n\nInteroperable Europe \n\nSolution(s) \n\n(NOT APPLICABLE) \n\nOther interoperability solution(s) \n\nN.A. (the proposed amendments to the AI Act do not affect digital public services) \n\nImpact of the requirement(s) as per digital public service on cross-border interoperability \n\nDigital Public Service #1 \n\nAssessment  Measure(s)  Potential remaining barriers (if applicable) \n\nAlignment with existing digital and sectorial policies \n\nPlease list the applicable digital and sectorial policies identified EN 29 EN \n\nOrganisational measures for a smooth cross-border digital public services delivery \n\nPlease list the governance measures foreseen \n\nMeasures taken to ensure a shared understanding of the data \n\nPlease list such measures \n\nUse of commonly agreed open technical specifications and standards \n\nPlease list such measures \n\n4.5. Measures to support digital implementation \n\nHigh-level description of measures supporting digital implementation \n\nDescription of the measure  Reference(s) to the requirement(s) \n\nCommission role \n\n(if applicable) \n\nActors to be involved \n\n(if applicable) \n\nExpected timeline \n\n(if applicable) \n\nN.A.", "fetched_at_utc": "2026-02-08T18:51:42Z", "sha256": "c10497b01d3883e105e26b1e9471322ed5d0af402a71a3f5e4c3948ffb6a0fc8", "meta": {"file_name": "Proposal Digital Omnibus.pdf", "file_size": 731943, "relative_path": "pdfs\\Proposal Digital Omnibus.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 30507}}}
{"doc_id": "pdf-pdfs-singapore-governance-for-agentic-ai-d729cf2c58b9", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Singapore - Governance for Agentic AI.pdf", "title": "Singapore - Governance for Agentic AI", "text": "MODEL AI GOVERNANCE \n\n# FRAMEWORK FOR AGENTIC AI \n\nVersion 1.0 | Published 22 January 2026 \n\nPublished on 19 January 2026 by: Contents \n\nExecutive Summary ................................ ................................ ................................ ........... 1\n\n1 Introduction to Agentic AI ................................ ................................ ........................... 3\n\n1.1 What is Agentic AI? ................................ ................................ ............................. 3\n\n1.1.1 Core components of an agent ................................ ................................ ................... 3\n\n1.1.2 Multi -agent setups ................................ ................................ ................................ .... 4\n\n1.1.3 How agent design affects the limits and capabilities of each agent ............................. 4\n\n1.2 Risks of Agentic AI ................................ ................................ ............................... 6\n\n1.2.1 Sources of risk ................................ ................................ ................................ .......... 6\n\n1.2.2 Types of risk ................................ ................................ ................................ ............. 7\n\n2 Model AI Governance Framework for Agentic AI ................................ ........................... 8\n\n2.1 Assess and bound the risks upfront ................................ ................................ ...... 9\n\n2.1.1 Determine suitable use cases for agent deployment ................................ .................. 9\n\n2.1.2 Bound risks through design by defining agents limits and permissions ...................... 11 \n\n2.2 Make humans meaningfully accountable ................................ ............................ 13 \n\n2.2.1 Clear allocation of responsibilities within and outside the organisation .................... 13 \n\n2.2.2 Design for meaningful human oversight ................................ ................................ ... 16 \n\n2.3 Implement technical controls and processes ................................ ...................... 18 \n\n2.3.1 During design and development, use technical controls ................................ ........... 18 \n\n2.3.2 Before deploying, test agents ................................ ................................ .................. 19 \n\n2.3.3 When deploying, continuously monitor and test ................................ ....................... 20 \n\n2.4 Enable end -user responsibility ................................ ................................ ........... 22 \n\n2.4.1 Different users, different needs ................................ ................................ ............... 22 \n\n2.4.2 Users who interact with agents ................................ ................................ ................ 23 \n\n2.4.3 Users who integrate agents into their work processes ................................ .............. 23 \n\nAnnex A: Further resources ................................ ................................ ............................... 25 \n\nAnnex B: Call for feedback and case studies ................................ ................................ ...... 27 1\n\n# Executive Summary \n\nAgentic AI is the next evolution of AI , holding transformative potential for users and businesses. \n\nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \n\nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \n\nevolving, agents are already transforming the workplace through coding assistants, customer \n\nservice agents, and automating enterprise productivity workflows. \n\nThese greater capabilities also bring forth new risks . Agentsâ€™ access to sensitive data and ability \n\nto make changes to their environment, such as updating a customer database or making a payment, \n\nare double -edged swords. As we move towards deploying multiple agents with complex interactions, \n\noutcomes also become more unpredictable. \n\nHumans must remain accountable and properly manage these risks. While existing governance \n\nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \n\nneed to be translated in practice for agents. Meaningful human control and oversight need to be \n\nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \n\nhuman oversight over all agent workflows becomes impractical at scale .\n\nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \n\noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \n\nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \n\ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in -house or \n\nusing third -party agentic solutions. Building on our previous model governance frameworks, we have \n\noutlined key considerations for or ganisations in four areas when it comes to agents: \n\n1.  Assess and bound the risks upfront \n\nOrganisations should adapt their internal structures and processes to account for new risks \n\nfrom agents. Key to this is first understanding the risks posed by the agentâ€™s actions, which \n\ndepend on factors such as the scope of actions the agent can take, the reversibility of those \n\nactions, and the agentâ€™s level of autonomy. \n\nTo manage these risks early, organisations could limit the scope of impact of their agents by \n\ndesigning appropriate boundaries at the planning stage, such as limiting the agentâ€™s access \n\nto tools and external systems. They could also ensure that the agentâ€™s actions are traceable \n\nand controllable through establishing robust identity management and access controls for \n\nagents. \n\n2.  Make humans meaningfully accountable \n\nOnce the â€œgreen lightâ€ is given for agentic AI deployment, an organisation should take steps \n\nto ensure human accountability. However, the autonomy of agents may complicate \n\ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \n\nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \n\ntherefore important to clearly define the responsibilities of different stakeholders, both 2\n\nwithin the organisation and with external vendors, while emphasising adaptive governance, \n\nso that the organisation is set up to quickly understand new developments and update its \n\napproach as the technology evolves. \n\nSpecifically, â€œhuman -in -the -loopâ€ has to be adapted to address automation bias, which has \n\nbecome a bigger concern with increasingly capable agents. This includes defining significant \n\ncheckpoints in the agentic workflow that require human approval, such as high -stakes or \n\nirreversible actions, and regularly auditing human oversight to check that it remains effective \n\nover time. \n\n3.  Implement technical controls and processes \n\nOrganisations should ensure the safe and reliable operationalisation of AI agents by \n\nimplementing technical measures across the agent lifecycle. During development, \n\norganisations should incorporate technical controls for new agentic components such as \n\nplanning, tools and still -maturing protocols , to address increased risks from these new \n\nattack surfaces. \n\nBefore deployment, organisations should test agents for baseline safety and reliability, \n\nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \n\nuse. New testing approaches will be needed to evaluate agents. \n\nDuring and after deployment , as agents interact dynamically with their environment and not \n\nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \n\ncontinuous monitoring after deployment. \n\n4.  Enable end -user responsibility \n\nTrustworthy deployment of agents does not rely solely on developers, but also on end -users \n\nusing them responsibly. To enable responsible use, as a baseline, users should be informed \n\nof the agentâ€™s range of actions, access to data, and the userâ€™s own responsibilities. \n\nOrganisations should consider laye ring on training to equip employees with the knowledge \n\nrequired to manage human -agent interactions and exercise effective oversight, while \n\nmaintaining their tradecraft and foundational skills. \n\nThis is a living document. We have worked with government agencies and leading companies to \n\ncollate current best practices, but this is a fast -developing space, and best practices will evolve. This \n\nframework will need to be continuously updated to keep pace with new developments. W e invite \n\nfeedback to refine the framework, and case studies demonstrating how the framework can be \n\napplied for responsible agentic deployment. 3\n\n# 1 Introduction to Agentic AI \n\n# 1.1 What is Agentic AI ?\n\nAgentic AI systems are systems that can plan across multiple steps to achieve specified \n\nobjectives, using AI agents .1 There is no consensus on what defines an agent, but there are certain \n\ncommon features â€“ agents usually possess some degree of independent planning and action taking \n\n(e.g. searching the web or creating files) over multiple steps to achieve a user -defined goal. 2\n\nIn this framework, we focus on agents built on language models, which are increasingly being \n\nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \n\nas its brain to make decisions and complete tasks. However, it is w orth noting that software agents \n\nare not a new concept and other types of agents exist, such as those which use deterministic rules, \n\nor other neural networks, to make decisions. 3\n\n# 1.1.1 Core components of an agent \n\nCore components of a simple agent 4\n\nAs agents are built on top of language models, it is helpful to start with the core components of \n\na simple LLM -based app. \n\n1.  Model : an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \n\nthe â€œbrainâ€ of the agent. It processes instructions, interprets user inputs, and generates \n\ncontextually appropriate responses.  \n\n> 1\n\nAdapted from C yber Security Agency of Singapore (CSA) , Draft Addendum on Securing Agentic AI . \n\n> 2\n\nSee  International AI Safety Report . \n\n> 3\n\nSee World Economic Forum (WEF) , AI Agents in Action: Foundations for Evaluation and Governance . \n\n> 4\n\nAdapted from GovTech Singapor e,  Agentic Risk & Capability Framework , CSA Singapore , Draft \n\nAddendum on Securing Agentic AI  and Anthropic , Building Effective Agents ). \n\nGuide decisions \n\nand actions \n\nRetrieve and store \n\ninfo for long -term \n\nInteract with \n\nexternal systems \n\nModel \n\nPlanning & \n\nReasoning \n\nInstructions  Tools Memory \n\n1\n\nv\n\n2\n\nv\n\n3\n\nv\n\n5\n\nv\n\n4\n\nv4\n\n2.  Instructions : Natural language commands that define an agent's role, capabilities, and \n\nbehavioural constraints e.g. a system prompt for an LLM. \n\n3.  Memory : Information that is stored and accessible to the LLM, either in short or long -term \n\nstorage. Sometimes added to allow the model to obtain information from previous user \n\ninteractions or external knowledge sources. \n\nAn agent uses the model, instructions and memory in similar ways as an LLM -based app. In \n\naddition, it has other components that enable it to complete more complex tasks: \n\n4.  Planning and reasoning : The model is usually trained to reason and plan, meaning that it \n\ncan output a series of steps needed for a task. \n\n5.  Tools: Tools enable the agent to take actions and interact with other systems, such as writing \n\nto files and databases, controlling devices, or performing transactions. The model calls tools \n\nto complete a task. \n\n6.  Protocols: This is a standardised way for agents to communicate with tools and other agents. \n\nFor example, the Model Context Protocol (MCP) has been developed for agents to \n\ncommunicate with tools, 5 whereas the Agent2Agent Protocol (A2A) defines a standard for \n\nagents to communicate with each other. 6\n\n# 1.1.2 Multi -agent setups \n\nIn an agentic system, it is common for multiple agents to be set up to work together. This can \n\nsometimes improve performance, by allowing each agent to specialise in a certain function or task \n\nand work in parallel.  7\n\nThree common design patterns for multi -agent systems are :8\n\nâ€¢ Sequential : Agents work one after another in a linear workflow. Each agentâ€™s output \n\nbecomes the next agentâ€™s input. \n\nâ€¢ Supervisor : One supervising agent coordinates specialised agents under it. \n\nâ€¢ Swarm : Agents work at the same time, handing off to another agent when needed \n\n# 1.1.3 How agent design affects the limits and capabilities of each agent \n\nWhile each agent may have the same core components, the design of each component can \n\nsignificantly affect what the agent can do . It is generally helpful to distinguish between two \n\nconcepts when considering what an agent can do: 9\n\nâ€¢ Action -space (or authority, capabilities): Range of actions the agent is permitted to take, \n\ndetermined by the tools it is allowed to use, transactions it can execute, etc.              \n\n> 5See Anthropic, Model Context Protocol .\n> 6See Google, Agent2Agent Protocol .\n> 7See LangChain ,Benchmarking Multi -Agent Architectures .\n> 8Adapted from AWS ,Multi -Agent Collaboration Patterns with Strands Agents and Amazon Nova .\n> 9See WEF, AI Agents in Action: Foundations for Evaluation and Governance .\n\n5\n\nâ€¢ Autonomy (or decision -making): Degree to which an agent can decide when and how to act \n\ntowards a goal, such as by defining the steps to be taken in a workflow. This can be \n\ndetermined by its instructions and level of human involvement. \n\n## Action -space \n\nAn agentâ€™s action -space mainly depends on the tools it has access to, which can affect: \n\nâ€¢ Systems it can access :\n\no Sandboxes only: Sandboxed tools (e.g. for code execution, data analysis) that cannot \n\naffect any other system \n\no Internal systems: Tools internal to the organisation, such as being able to search and \n\nupdate the organisationâ€™s databases \n\no External systems: Tools that enable the agent to access external services, such as \n\nretrieving and updating data through third -party pre -defined APIs. \n\nâ€¢ Actions it can take in relation to the system it can access :\n\no Read vs write: An agent may only be able to read and retrieve information from a \n\nsystem, rather than write to and modify data within the system. \n\nAn emerging modality of agentic AI is a computer use agent, whose primary tool is access to a \n\ncomputer and browser. This means that it can take any action that a human can take with a computer \n\nand browser without having to rely on specifically defined tool s and APIs. This significantly increases \n\nwhat the agent can access and do. \n\n## Autonomy \n\nAn agentâ€™s autonomy mainly depends on its instructions component and the level of human \n\ninvolvement in the agentic system. \n\nIn terms of instructions, an agent can be given differing level of instructions: \n\nâ€¢ Detailed instructions and SOP: An agent instructed to follow a detailed SOP to complete a \n\ntask would be limited in the decisions it can make at each stage. \n\nâ€¢ Using its own judgment: An agent instructed to use its own judgment to complete a task \n\nwould have more freedom to define its plan and workflow. \n\nAnother relevant factor is the level of human involvement. When interacting with an agent, a human \n\ncan be involved to different levels: 10 \n\nâ€¢ Agent proposes, human operates: The human directs and approves every step taken by an \n\nagent. \n\nâ€¢ Agent and human collaborate: The human and agent work together. The agent requires \n\nhuman approval at significant steps, such as before writing to a database or making a \n\npayment. However, the human can intervene anytime by taking over the agentâ€™s work or \n\npausing the agent and requesti ng a change.   \n\n> 10 See Knight First Amendment Institute at Columbia University, Levels of Autonomy for AI Agents .\n\n6\n\nâ€¢ Agent operates, human approves: The agent requires human approval only at critical steps \n\nor failures, such as deleting a database or making a payment above a predefined amount. \n\nâ€¢ Agent operat es, human observes: The agent does not require human approval as it \n\ncompletes its task, though its actions may be audited after the fact. \n\n# 1.2 Risks of Agentic AI \n\n# 1.2.1 Sources of risk \n\nThe new components of an agent constitute new sources of risks .11  The risks themselves are \n\nfamiliar â€“ fundamentally, agents are software systems built on LLMs. They inherit traditional \n\nsoftware vulnerabilities (such as SQL injection) and LLM -specific risks (such as hallucination, bias, \n\ndata leakage and adversarial prompt injections). 12 \n\nHowever, the risks can manifest differently through the different components. For example: \n\nâ€¢ Planning and reasoning: An agent can hallucinate and make a wrong plan to complete a \n\ntask. \n\nâ€¢ Tools: An agent can hallucinate by calling non -existent tools or calling tools with the wrong \n\ninput, or call ing tools in a biased manner. As tools connect the agent to external systems, \n\nprompt or code injections can also manipulate the agent to exfiltrate or otherwise \n\nmanipulate the data it has access to. \n\nâ€¢ Protocols: Finally, as new protocols emerge to handle agent communication, they can also \n\nbe poorly deployed or compromised e.g. an untrusted MCP server deployed with code to \n\nexfiltrate the userâ€™s data. \n\nAs components within an agent or multiple agents interact, risks can also arise at the system \n\nlevel. 13  For example: \n\nâ€¢ Cascading effect: A mistake by one agent can quickly escalate as its outputs are passed \n\nonto other agents. For example, in supply chain management, a hallucinated inventory figure \n\nfrom one agent could potentially cause downstream agents to reorder excessive or \n\ninsufficient stock. \n\nâ€¢ Unpredictable outcomes: Agents working together can also compete or coordinate in \n\nunintended ways. For example, in manufacturing, different agents may be involved in \n\nmanaging machines and inventory. While coordinating to meet production goals, the agents \n\nmight interact unpredict ably due to complex optimi sation algorithms and over or under -\n\nprioritise one resource or machine, leading to unexpected bottlenecks.        \n\n> 11 BCG highlighted examples of new risks from agents e.g. agents that optimize their own goals locally\n> may create instability across the system, flawed behaviour by one agent may spread to other agents\n> (see What Happens When AI Stops Asking Permission? )\n> 12 Adapted from CSA ,Draft Addendum on Securing Agentic AI .\n> 13 See WEF, AI Agents in Action: Foundations for Evaluation and Governance , which highlighted a new\n> class of failure modes, linked to potentially misaligned interactions in multi -agent systems e.g.\n> orchestration drift, semantic misalignment, interconnectedness and cascading effects.\n\n7\n\n# 1.2.2 Types of risk \n\nBecause agents take actions in the real world, when they malfunction , it can lead to harmful \n\nreal -world impact . Organisations should be aware of these negative outcomes :\n\nâ€¢ Erroneous actions : Incorrect actions such as an agent fixing appointments on the wrong \n\ndate or producing flawed code. The exact harmful outcome depends on the action in \n\nquestion, e.g. flawed code can lead to exploited security vulnerabilities, and wrong medical \n\nappointment s may affect a patientâ€™s health outcomes. \n\nâ€¢ Unauthorised actions : Actions taken by the agent outside its permitted scope or authority, \n\nsuch as taking an action without escalating it for human approval based on a company policy \n\nor standard operating procedure. \n\nâ€¢ Biased or unfair actions : Actions that lead to unfair outcomes, especially when dealing with \n\ngroups of different profiles and demographics, such as biased vendor selection in \n\nprocurement, disbursements of grants, and/or hiring decisions. \n\nâ€¢ Data breaches : Actions that lead to the exposure or manipulation of sensitive data. Such \n\ndata may be personally identifiable information or confidential information e.g. customer \n\ndetails, trade secrets, and/or internal communications. This can be due to a security breach, \n\nwhere attackers exploit agents to reveal private information, or an agent disclosing sensitive \n\ndata due to a failure to recognise it as sensitive. \n\nâ€¢ Disruption to connected systems : As agents interact with other systems, they can cause \n\ndisruption to connected systems when they are compromised or malfunction e.g. deleting a\n\nproduction codebase, or overwhelming external system s with requests. \n\nErroneous actions  Unauthorised \n\nactions \n\nBiased or unfair \n\nactions \n\nData breaches \n\nDisruption to \n\nconnected \n\nsystems 8\n\n# 2 Model AI Governance Framework for Agentic AI \n\nFour dimensions of the MGF for Agentic AI \n\nThe MGF for Agentic AI builds on the responsible AI practices for organisations set out in MGF \n\n(2020) 14  by highlighting emerging best practices to address new concerns from agentic AI. This is so \n\nthat organisations can develop and use agentic AI with the requisite knowledge and judgment .\n\nThe framework begins with helping organisations to assess and bound the risks upfront . It \n\nhighlights new risks that should be considered during risk assessment, and design considerations at \n\nthe planning stage to limit the potential scope of impact of the agents, as well as ensure that agents \n\nare traceable and controllable. \n\nWhile agents may act autonomously, human responsibility continues to apply. Once the â€œgreen lightâ€ \n\nis given to deploy agentic AI, an organisation should take immediate steps to mak e humans \n\nmeaningfully accountabl e. This includes clearly defining responsibility across multiple actors \n\nwithin and outside the organisation involved in the agent lifecycle; and taking measures to ensure \n\nthat human -in -the -loop remains effective over time notwithstanding automation bias. \n\nTo ensure safe and reliable operationalisation of agents, an organisation should adopt technical \n\ncontrols and processes across the AI lifecycle. During development, guardrails for new \n\ncomponents in AI agents such as planning and tools should be implemented. Before deployment, \n\nagents should be tested for baseline safety and reliability. After deployment, agents should be \n\ncon tinuously monitored as they interact dynamically with their environment. \n\nFinally, trustworthy deployment of agents does not rest solely on developers, but also on end -users. \n\nOrganisations are responsible for enabling end -user responsibility by equipping them with \n\nessential information to use agents appropriately and exercise effective oversight, while maintaining \n\ntheir tradecraft and foundational skills.    \n\n> 14 See Model AI Governance Framework (2 nd Ed) .\n\n1. Assess and bound \n\nthe risks upfront \n\n2. Make humans \n\nmeaningfully accountable \n\n3. Implement technical \n\ncontrols and processes \n\n4. Enable end -user \n\nresponsibility 9\n\n# 2.1 Assess and bound the risks upfront \n\nAgents bring new risks, especially in their access to sensitive data and ability to change their \n\nenvironment through action -taking. Their adaptive, autonomous and multi -step nature also \n\nincreases the potential for unexpected actions, emergent risks and cas cading impacts. \n\nOrganisations should consider these new dimensions as part of risk assessment, and limit the scope \n\nof impact of their agents by designing appropriate boundaries at an early stage. \n\nWhen planning for the use of agentic AI, organisations should consider: \n\nâ€¢ Determining suitable use cases for agent deployment by considering agent -specific \n\nfactors that can affect the likelihood and impact of the risk. \n\nâ€¢ Design choices to bound the risks upfront by applying limits on agentâ€™s access to tools and \n\nsystems and defining a robust identity and permissions framework. \n\n# 2.1.1 Determine suitable use cases for agent deployment \n\nRisk identification and assessment is the first step when considering if an agentic use case is \n\nsuitable for development or deployment . Risk is a function of likelihood (probability of the risk \n\nmanifesting) and impact (severity of impact if the risk manifests). \n\nThe following non -exhaustive factors affect the level of risk of an agentic use case: \n\nFactors affecting impact \n\nFactor  Description  Illustration \n\nDomain and use \n\ncase in which \n\nagent is being \n\ndeployed \n\nLevel of tolerance of error in the \n\ndomain and use case in which the \n\nagent is being deployed to \n\nAgent executing financial \n\ntransactions which require a high \n\ndegree of accuracy, vs agent that \n\nsummarises internal meetings \n\nAgentâ€™s access to \n\nsensitive data \n\nWhether the agent can access \n\nsensitive data, such as personal \n\ninformation or confidential data \n\nAgent that requires access to \n\npersonal customer data gives rise to \n\nthe risk of leaking such data, vs \n\nagent who only has access to \n\npublicly available information \n\nAgentâ€™s access to \n\nexternal systems \n\nWhether the agent can access \n\nexternal systems \n\nAgent that sends data to third -party \n\nAPIs can leak data to these third \n\nparties, or disrupt these systems by \n\nmaking too many requests, vs agent \n\nthat only has access to sandboxed \n\nor internal tools \n\nScope of agentâ€™s \n\nactions \n\nWhether an agent can only read \n\nfrom or modify the data and systems \n\nit has access to \n\nRead vs write: Agent that can only \n\nread from a database vs being able \n\nto write to it \n\nMany tools vs a few: Agent that can \n\nonly choose from a few pre -defined \n\ntools, vs an agent who has unlimited \n\naccess to a browser tool 10 \n\nReversibility of \n\nagentâ€™s actions \n\nIf the agent can modify data and \n\nsystems, whether such \n\nmodifications are easily reversed \n\nAgent that schedules meetings vs \n\nagent that sends email \n\ncommunications to external parties \n\nFactors affecting likelihood \n\nFactor  Description  Illustration \n\nAgentâ€™s level of \n\nautonomy \n\nWhether the agent can define the \n\nentire workflow or must follow a \n\nwell -defined procedure. \n\nA higher level of autonomy can \n\nresult in higher unpredictability, \n\nincreasing likelihood of error. \n\nAgent is provided with a SOP and \n\ninstructed to follow it when carrying \n\nout a task, vs agent is instructed to \n\nuse its best judgment to select and \n\nexecute every step \n\nTask complexity  How complex the task is, in relation \n\nto the number of steps required to \n\ncomplete it and the level of analysis \n\nrequired at each step. \n\nA higher level of complexity similarly \n\nincreases unpredictability and the \n\nlikelihood of error. \n\nAgent is required to extract key \n\naction points from a meeting \n\ntranscript, vs agent is tasked to \n\nfollow a nuanced data sharing policy \n\nwhen handling external requests for \n\ninformation \n\nAgentâ€™s access to \n\nexternal systems \n\nWhether the agent is exposed to \n\nexternal systems, and who \n\nmaintains these systems. \n\nA higher level of exposure makes the \n\nagent more vulnerable to prompt \n\ninjections and cyberattacks. \n\nAgent can only access an internal \n\nknowledge base which is \n\nmaintained by trusted internal \n\nteams, vs an agent who can access \n\nthe web containing untrusted data \n\nThreat modelling also makes risk assessment more \n\nrigorous by systematically identifying specific ways \n\nin which an attacker may take to compromise the \n\nsystem. Common security threats to agentic systems \n\ninclude memory poisoning, tool misuse, and privilege \n\ncompromise. 15  As agentic systems (especially multi -\n\nagent systems) can become very complex, it is often \n\nuseful to use a method called taint tracing to map out \n\nall the workflows and interactions to track how \n\nuntrusted data can move through the system. For more \n\ninformatio n on how to perform threat modelling and \n\ntaint tracing for agentic systems, organisations may \n\nrefer to  CSAâ€™s Draft Addendum on Securing Agentic AI .                \n\n> 15 For a more comprehensive coverage of potential security threats to agentic AI systems , see OWASP,\n> Agentic AI â€“Threats and Mitigations .\n> The relationship between threat modelling\n> and risk assessment\n> Threat modelling augments the risk assessment\n> process by generating contextualised threat\n> events with well -described sequence of actions,\n> activities and scenarios that the attacker may\n> take to compromise the system. With more\n> relevant threat events, risk a ssessments will be\n> more rigorous and robust, resulting in more\n> targeted controls and effective layered defence.\n> Since risk assessment is continuous, the threat\n> model should be regularly updated.\n> Adapted from CSA, Guide to Cyber Threat\n> Modelling\n\n11 \n\n# 2.1.2 Bound risks through design by defining agents limits and permissions \n\nHaving selected an appropriate agent use case, organisations can further bound the risks by \n\ndefining appropriate limits and permission policies for each agent. \n\n## Agent limits \n\nOrganisations should consider defining limits on: \n\nâ€¢ Agentâ€™s access to tools and systems: Define policies that give agents only the minimum \n\ntools and data access needed for it to complete its task. 16  For example, a coding assistant \n\nmay not require access to a web search tool, especially if it already has curated access to \n\nthe latest software documentation. \n\nâ€¢ Agentâ€™s autonomy: For process -driven tasks, SOPs and protocols are frequently used to \n\nimprove consistency and reduce unpredictability.  17  Define similar SOPs for agentic \n\nworkflows that an agent is constrained to follow, rather than giving the agent the freedom to \n\ndefine every step of the workflow. \n\nâ€¢ Agentâ€™s area of impact: Design mechanisms and procedures to take agents offline and limit \n\ntheir potential scope of impact when they malfunction. This can include running agents in \n\nself -contained environments with limited network and data access, particularly when they \n\nare carrying out high -risk tasks such as code execution. 18 \n\n## Agent identity \n\nIdentity management and access control is one of the key means in which organisations enable \n\ntraceability and accountability today for humans. As agents become more autonomous, identity \n\nmanagement has to be extended to agents as well to track individual agent behaviour and establish \n\nwho holds accountability for each agent. \n\nThis is an evolving space, and gaps exist today in terms of handling agent identity robustly . For \n\nexample, current authorisation systems typically have pre -defined, static scopes. However, to \n\noperate safely in more complex scenarios, agents require fine -grained permissions that may change \n\ndynamically depending on the context, risk levels, and tas k objectives. Current authentication \n\nsystems are also typically based on a single, unique individual. Such systems face difficulty in \n\nhandling complex agent setups, such as when agents act for multiple human users with different \n\npermissions, or recursive delegation scenarios where agents spin up multiple sub -agents. 19                    \n\n> 16 See PwC ,The rise â€“and risks â€“of agentic AI .\n> 17 Grab introduced anLLM agent framework leverag ing on Standard Operating Procedures (SOPs) to\n> guide AI -driven execution (see Introducing the SOP -driven LLM agent frameworks ).\n> 18 See McKinsey, Deploying agentic AI with safety and security: A playbook for technology leaders .\n> 19 For a more comprehensive treatment of how current identity systems may face challenges when\n> catering to agentic AI, see OpenID, Identity Management for Agentic AI .\n\n12 \n\nSolutions are being developed to address these issues, such as integrating well -established \n\nstandards like OAuth 2.0 into MCP. 20  The industry is also developing new standards and solutions for \n\nagents, such as decentralised identity management and dynamic access control. 21 \n\nIn the interim, organisations should consider these best practices to enable agent control and \n\ntraceability: \n\nâ€¢ Identification: An agent should have its own unique identity, such that it can identify itself to \n\nthe organisation, its human user, or other agents. However, an agentâ€™s identity may need to \n\nbe tied to a supervising agent, a human user, or an organisational department for \n\naccountability and tracking. Additionally, the different capacities in which an agent acts (e.g. \n\nindependently or on behalf of a specified human user) should also be recorded. \n\nâ€¢ Authorisation: An agent can have pre -defined permissions based on its role or the task at \n\nhand, or its permissions may be dynamically set by its authorising human user, or a \n\ncombination of both. As a rule of thumb, the human user should not be able to set \n\npermissions fo r the agent greater than what the human user is himself authorised to do. Such \n\ndelegations of authority should be clearly recorded.         \n\n> 20 See MCP specifications for Authentication support ,Authorisation support .\n> 21 See proposed framework for agentic identity by Cloud Security Alliance, Agentic AI Identity & Access\n> Management: A New Approach .\n> Evaluating the residual risks\n> Residual risk is the risk that remains after mitigation measures have been applied. It is important to note\n> that there will always be a level of risk remaining, even after efforts are taken to identify appropriate agentic\n> use cases and define limits on any agents, especially given how quickly agentic AI is evolving. Ultimately,\n> organisations should evaluate and determine if the residual risk for their agentic deployment is of a\n> tolerable level and can be accepted.\n\n13 \n\n# 2.2 Make humans meaningfully accountable \n\nThe organisations that deploy agents and the humans who oversee them remain accountable for the \n\nagentsâ€™ behaviours and actions. But it can be challenging to fulfil this accountability when agent \n\nactions emerge dynamically and adaptively from interactions instead of fixed logic. Multiple \n\nstakehol ders may also be involved in different parts of the agent lifecycle, diffusing accountability. \n\nFinally, automation bias, or the tendency to over -trust a n automated system , especially when it has \n\nperformed reliably in the past, becomes a bigger concern as humans supervise increasingly capable \n\nagents. \n\nTo address these challenges to human accountability, organisations should consider: \n\nâ€¢ Clear allocation of responsibilities within and outside the organisation , by establishing \n\nchains of accountability across the agent value chain and lifecycle, while emphasising \n\nadaptive governance, so that the organisation is set up to quickly understand new \n\ndevelopments and update their approach as the technology evolves. \n\nâ€¢ Measures to enable meaningful human oversight of agents , such as requiring human \n\napproval at significant checkpoints, auditing the effectiveness of human approvals, and \n\ncomplementing these measures with automated monitoring .\n\n# 2.2.1 Clear allocation of responsibilities within and outside the organisation \n\nAs deployers, organisations and humans remain accountable for the decisions and actions of \n\nagents. However, as with AI, the value chain for agentic AI involves multiple actors. Organisations \n\nshould consider the allocation of responsibility both within their organisation, and vis -Ã -vis other \n\norganisations along the value chain. \n\nSimplified agentic AI value chain 22    \n\n> 22 For a more comprehensive list of potential stakeholders involved in the agentic AI ecosystem, see CSA\n> and FAR.AI, Securing Agentic AI: A Discussion Paper .\n\nModel developers \n\nModels that agents \n\ncan be built on  Agentic AI system \n\nproviders \n\nProviding platforms to \n\nbuild agents on or full \n\nSaaS solutions \n\nDeploying \n\norganisation \n\nMay also develop \n\nagents in -house \n\nEnd users \n\nInteracts with \n\nand uses \n\nagents \n\nTooling providers \n\ne.g. MCP, APIs \n\nAllow agents to \n\nconnect to external \n\nsystems 14 \n\n## Within the organisation \n\nWithin the organisation, organisations should allocate responsibilities for different teams \n\nacross the agent lifecycle. While each organisation is structured differently, this is an illustration \n\nof how such responsibilities may be allocated across different teams: \n\nKey decision \n\nmakers \n\nProduct teams \n\nCybersecurity \n\nteams \n\nWho: Leaders who define strategic decisions and high -level policies \n\nfor the organisation e.g. board members, C -suite executives, \n\nmanaging directors, or department leaders. \n\nKey responsibilities can include: \n\nâ€¢ Setting high -level goals for use of agents \n\nâ€¢ Defining permitted operational use cases for agents, \n\nincluding limits on agentâ€™s data access \n\nâ€¢ Setting the overall governance approach, including risk \n\nmanagement frameworks and escalation processes \n\nWho: These roles oversee the translation of stakeholder needs or \n\nbusiness goals into a technical agentic solution e.g. Product \n\nManagers, UI / UX Designers, AI Engineers, Software Engineers \n\nKey responsibilities can include: \n\nâ€¢ Defining the design and requirements for agents, as well as \n\nany feature controls or phased rollouts \n\nâ€¢ Reliable implementation of agents i.e. development, pre -\n\ndeployment testing and post -deployment monitoring \n\nacross the agent lifecycle \n\nâ€¢ Educating users on responsible use of agentic product \n\nWho: These roles oversee the protection of agentic systems from \n\ncyber threats, by implementing and managing security measures, \n\nidentifying vulnerabilities, and responding to incidents e.g. Chief \n\nSecurity Officer, Cyber Security Specialist, Penetration Tester \n\nKey responsibilities can include: \n\nâ€¢ Defining baseline security guardrails and secure -by -design \n\ntemplates that technical teams should implement or adapt \n\nto the agentic system being deployed \n\nâ€¢ Conducting regular red teaming and threat modelling 15 \n\nUsers \n\n## Outside the organisation \n\nOrganisations may also need to work with external parties when deploying agents e.g. model \n\ndevelopers, agentic AI providers, or hosts of external MCP servers or tools. \n\nIn these cases, organisations should similarly ensure that there are measures in place to fulfil its \n\nown accountability. Some agent -specific considerations are: \n\nâ€¢ Clarify distribution of obligations in any terms and conditions or contracts between the \n\norganisation and the external party. In particular, organisations should consider provisions \n\nto address any security arrangements, performance guarantees, or data protection and \n\nconfidentiality. Where th ere are gaps, the organisation should reassess if the agentic \n\ndeployment meets its risk tolerance. \n\nâ€¢ Features to maintain security and control. Organisations should consider if the external \n\npartyâ€™s product offers features for the organisation to maintain a sufficient level of security \n\nor control. This includes strong authentication measures such as scoped API keys, per -agent \n\nidentity tokens, and robust observability such as the logging of tool calls and access history. \n\nWhere such features are lacking, organisations should consider alternative or in -house \n\nsolutions, or scoping down the agentic use case, suc h as restricting access to sensitive data. \n\n## End users \n\nOrganisations may deploy agents to users within or outside their organisation. In doing so, \n\norganisations should ensure that users are provided sufficient information to hold the organisation \n\naccountable, as well as any information relating to the userâ€™s own responsibilities. More information \n\ncan be found in  Enabling end -user responsibility  below. \n\nWho: Any individual who utilises the output of the agents to contribute to \n\nan organisational goal e.g. company employees making decisions or \n\nautomating workflows and practices. \n\nKey responsibilities can include: \n\nâ€¢ Ethical and responsible usage of agents \n\nâ€¢ Attending required training, complying with usage policies, \n\ntimely reporting of bugs or issues with agents \n\n> Developing internal capabilities for adaptive governance\n> All teams involved in the agentic AI lifecycle should also develop internal capabilities to\n> understand agentic AI. As the technology is quickly evolving, being aware of the improvements\n> and limitations of new agentic developments, such as new modalities li ke computer use agents,\n> or new evaluation frameworks for agents, allow organisations to quickly adapt their governance\n> approach to new developments.\n\n16 \n\n# 2.2.2 Design for meaningful human oversight \n\nSetting up a system for effective human supervision \n\nOrganisations should define significant checkpoints or action boundaries that require human \n\napproval , especially before sensitive actions are executed. This can include: 23 \n\nâ€¢ High -stakes actions and decisions e.g. editing of sensitive data, final decisions in high -risk \n\ndomains (such as healthcare or legal), actions that may trigger liability \n\nâ€¢ Irreversible actions e.g. permanently deleting data, sending communications, making \n\npayments \n\nâ€¢ Outlier or atypical behaviour e.g. when agent accesses a system or database outside of its \n\nwork scope, when agent selects a delivery route that is twice as long as the median distance \n\nâ€¢ User -defined . Agents may act on behalf of users who have different risk appetites. Beyond \n\norganisation -defined boundaries, users may be given the option to define their own \n\nboundaries e.g. requiring approval for purchases above a certain amount \n\nApart from considering when approvals are required, organisations should also consider what \n\nform approvals should take. These considerations include: \n\nâ€¢ Keep approval requests contextual and digestible. When asking humans for approval, \n\nkeep the request short and clear, instead of providing long logs or raw data that may be \n\nchallenging to decipher and understand. \n\nâ€¢ Consider the form of human input required. For straightforward actions such as accessing \n\na database, the human user can simply approve or reject. For more complex cases, such as \n\nreviewing an agentâ€™s plan before execution, it may be more productive for the human to edit \n\nthe plan before giving the ag ent the go -ahead. \n\nOrganisations should implement measures to ensure continued effectiveness of human \n\noversight , particularly as humans remain susceptible to alert fatigue and automation bias. These \n\nmeasures can include:  \n\n> 23 For further examples of where human involvement may be considered, see Partnership on AI,\n> Prioritising real -time failure detection in AI agents ).\n\nDefine significant \n\ncheckpoints or action \n\nboundaries that require \n\nhuman approval \n\nTrain humans to evaluate \n\nthese requests for approval \n\neffectively, and audit these \n\napprovals \n\nComplement this with \n\nautomated monitoring \n\nmechanisms and \n\npredefined alert thresholds 17 \n\nâ€¢ Training humans to identify common failure modes e.g. inconsistent agent reasoning, \n\nagents referring to outdated policies \n\nâ€¢ Regularly auditing the effectiveness of human oversight \n\nFinally, human oversight should be complemented with automated real -time monitoring to \n\nescalate any unexpected or anomalous behaviour . This can be done by implementing alerts for \n\ncertain logged events (e.g. attempted unauthorised access or multiple failed attempts to call a tool), \n\nusing data science techniques to identify anomalous agent trajectories, or using agents to monitor \n\nother ag ents. For more information, see  Continuous testing and monitoring  below. 18 \n\n# 2.3 Implement technical controls and processes \n\nThe agentic components that differentiate agents from simple LLM -based applications necessitate \n\nadditional controls during the key stages of the implementation lifecycle. \n\nOrganisations should consider: \n\nâ€¢ During design and development, design and implement technical controls . The new \n\ncomponents and capabilities of agents also necessitate new and tailored controls. \n\nDepending on the agent design, implement controls such as tool guardrails and plan \n\nreflections. Further, limit the agentâ€™s impact on the external environment by enf orcing least -\n\nprivilege access to tools and data. \n\nâ€¢ Pre -deployment, test agents for safety and security. As with all software, testing before \n\ndeployment ensures that the system behaves as expected. Specifically for agents, test for \n\nnew dimensions such as overall task execution, policy adherence and tool use accuracy, \n\nand test at different levels and across v aried datasets to capture the full spectrum of agent \n\nbehaviour. \n\nâ€¢ When deploying, gradually roll out agents and continuously monitor them in production. \n\nThe autonomous nature of agents and the changing environment makes it challenging to \n\naccount for and test all possible outcomes before deployment. Hence it is recommended to \n\nroll out agents gradually, supported with real -time monitoring post -deployment to ensure \n\nthat agents function safely. \n\n# 2.3.1 During design and development, use technical controls \n\nOrganisations should design and implement technical controls in the agentic AI system to \n\nmitigate identified risks. For agents specifically, in addition to baseline software and LLM controls, \n\nconsider adding controls for: \n\nâ€¢ New agentic components, such as planning and reasoning and tools \n\nâ€¢ Increased security concerns from the larger attack surface and new protocols \n\nFor illustration, these are some sample controls for agents. For a more comprehensive list, \n\norganisations can refer to CSAâ€™s  Draft Addendum on Securing Agentic AI  and GovTechâ€™s  Agentic Risk \n\nand Capability Framework .\n\nPlanning  â€¢ Prompt agent to reflect on whether its plan adheres to user instructions \n\nâ€¢ Prompt the agent to summarise its understanding and request clarification \n\nfrom the user before proceeding \n\nâ€¢ Log the agentâ€™s plan and reasoning for the user to evaluate and verify \n\nTools  â€¢ Configure tools to require strict input formats \n\nâ€¢ Apply the principle of least privilege to limit tools available to each agent, \n\nenforced through robust authentication and authorisation \n\nâ€¢ For data -related tools: \n\no Do not grant agent write access to tables in sensitive databases unless \n\nstrictly required 19 \n\no Configure agent to let user take over control when keying in sensitive \n\ndata (e.g. passwords, API keys) \n\nProtocols  â€¢ Use standardised protocols where applicable (e.g. agentic commerce \n\nprotocols when agent is handling a financial transaction) \n\nâ€¢ For MCP servers: \n\no Whitelist trusted servers and only allow agent to interact with servers \n\non that whitelist \n\no Sandbox any code execution \n\n# 2.3.2 Before deploying, test agents \n\nOrganisations should test agents for safety and security before deployment. This provides \n\nconfidence that the agents work as expected and controls are effective. Best practices on software \n\nand LLM testing are still relevant, such as unit and integration testing for software systems, as well \n\nas selecting representative datasets, an d useful metrics and evaluators for LLM testing. \n\nOrganisations can refer to previous guidance, such as the Starter Kit for testing of LLM -based apps \n\nfor safety and reliability. \n\nHowever, organisations should adapt their testing approaches for agents. Some considerations \n\ninclude: \n\nâ€¢ Testing for new risks: Beyond producing incorrect outputs, agents can take unsafe or \n\nunintended actions through tools. Organisations can consider testing for: 24 \n\no Overall task execution : Whether agent can complete task accurately \n\no Policy compliance : Whether an agent follows defined SOPs and routes for human \n\napproval when required \n\no Tool calling : Whether an agent calls the right tools, with the right permissions, with \n\nthe right inputs and in the right order \n\no Robustness : As agents are expected to react and adapt to real -world situations, test \n\nfor their response to errors and edge cases \n\nâ€¢ Testing entire agent workflows: Agents can take multiple steps in sequence without human \n\ninvolvement. Thus, beyond testing an agentâ€™s final output, agents should be tested across \n\ntheir entire workflow, including reasoning and tool calling. \n\nâ€¢ Testing agents individually and together: Beyond individual agents, testing should be \n\ncarried out at the multi -agent system level, to understand any emergent risks and behaviours \n\nwhen agents collaborate, such as competitive behaviours or the impact on other agents \n\nwhen one agent has been compromis ed. \n\nâ€¢ Testing in real or realistic environments: As agents may be expected to navigate real -world \n\nsituations, testing should occur in a properly configured execution environment that mirrors \n\nproduction as closely as possible, such as using tool integrations, external APIs, and \n\nsandboxes that behave as th ey would in deployment. However, organisations should    \n\n> 24 For an example of new agentic aspects to test for, see Microsoft Foundry, Agent evaluators .\n\n20 \n\ncalibrate the need for realism against the risk of prematurely allowing agents to access tools \n\nthat affect the real world. \n\nâ€¢ Testing repeatedly and across varied datasets: Agent behaviour is inherently stochastic \n\nand context -dependent. Testing should thus be done at scale and across varied datasets to \n\nobserve any unexpected low -probability behaviours, especially if they are high -impact. This \n\nrequires generating test dataset s that cover different conditions that agents may encounter \n\nand running these tests multiple times, including minor perturbations where needed. \n\nâ€¢ Evaluating test results at scale: Reliably evaluating test results at scale is a known \n\nchallenge for LLM testing. Agents add a further layer of complexity as their workflows can be \n\nlong and contain unstructured information that cannot be easily processed by humans or \n\nautomated scripts. Org anisations may consider using different evaluation methods for \n\ndifferent parts of the agentic workflow (e.g. deterministic tests for structured tool calls vs \n\nLLM or human evaluation for unstructured agent reasoning). Howev er, there is still a need to \n\nevaluate agents holistically, so that agent patterns across steps can be evaluated. Current \n\nindustry solutions thus include defining LLMs or agents to evaluate other agents. 25 \n\n# 2.3.3 When deploy ing , continuous ly monitor and test \n\nAs agents are adaptive and autonomous , organisations should consider mechanisms to respond to \n\nunexpected or emergent risks when deploying agents .\n\n## Gradual deployment of agents \n\nOrganisations should consider gradually rolling out agents into production to control the \n\namount of risk exposure . Such rollouts can be controlled based on :\n\nâ€¢ User s of agents e.g. rolling out to trained or experienced users first \n\nâ€¢ Tools and protocols available to agent e.g. restricting agents to more secure, whitelisted \n\nMCP servers first \n\nâ€¢ Systems exposed to agent e.g. using agents in lower -risk internal systems first \n\n## Continuous test ing and monitor ing \n\nOrganisations should continuously m onitor and log agent behaviour post -deployment , and \n\nes tablish reporting and failsafe mechanisms for agent failures or unexpected behaviours . This \n\nallows the organisation to :\n\nâ€¢ Intervene in real -time : When potential failures are detected, s top agent workflow and \n\nescalate to a human supervisor e.g. if agent attempts unauthorised access \n\nâ€¢ Debug when incidents happen : Logging and tracing each step of an agent workflow and \n\nagent -to -agent interactions help to identify points of failure \n\nâ€¢ Audit at regular intervals: This ensure s that the system is performing as expected .    \n\n> 25 For an example of agent evaluation solutions, see AWS Labs ,Agent Evaluation .\n\n21 \n\nMonitoring and observability are not new concept s, but agents introduce some challenges . As \n\nagents execute multiple actions at machine speed, organisations face the issue of extracting \n\nmeaningful insights from the voluminous logs generated by monitoring systems. This becomes more \n\ndifficult when high -risk anomalies are expected to be detected in real -time and surfaced as early as \n\npossible .\n\nKey considerations when setting up a monitoring system include: \n\nâ€¢ What to log : Organisations should determine their objectives for monitoring (e.g. real -time \n\nintervention, debugging , integration between components ) to identify what to log. In doing \n\nso, prioritise monitoring for high -risk activities such as updating database records or \n\nfinancial transactions. \n\nâ€¢ How to effectively monitor logs : Organisations can consider approaches such as: \n\no Defining alert thresholds: \n\nâ–ª Programmatic, threshold -based : Define alerts when agents trigger \n\nthresholds e.g. agent attempts unauthorised access or makes too many \n\nrepeated tool calls within a specified timeframe .\n\nâ–ª Outlier / anomaly detection : Use data science or deep learning technique s\n\nto process agent signals and identify anomalous behaviour that may indicate \n\nmalfunctions. \n\nâ–ª Agents monitoring other agents : Design agents to monitor other agents in \n\nreal -time, flagging any anomalies or inconsistencies. \n\no Defining specific interventions : For each alert type, consider what the level of \n\nintervention should be. Some degree of human review should be incorporated, \n\nproportionate to the risk level. For example, lower -priority alerts can be flagged for \n\nreview at a scheduled time, whereas higher -priority ones might require temporarily \n\nhalting agent execution until a human reviewer can assess. In the event of \n\ncatastrophic agent ic malfunction or compromis e, comm en surate measures such as \n\ntermination and fallback solutions should be considered .\n\nFinally, continuously test the agentic system even post -deployment to ensure that it works as \n\nexpected and is not affected by model drift or other changes in the environment. 22 \n\n# 2.4 Enabl e end -user responsibility \n\nUltimately, end users are the ones who use and rely on agents , and human accountability also \n\nextends to these users. Organisations should provide sufficient information to end users to \n\npromote trust and enable responsible use. \n\nOrganisations should consider: \n\nâ€¢ Transparency : Users should be informed of the agentsâ€™ capabilities (e.g. scope of agentâ€™s \n\naccess to userâ€™s data, actions the agent can take) and the contact points whom users can \n\nescalate to if the agent malfunctions .\n\nâ€¢ Education : Users should be educated on proper use and oversight of agents (e.g. training \n\nshould be provided on an agentâ€™s range of actions, common failure modes like hallucinations, \n\nusage policies for data), as well as the potential loss of trade craft i.e. as agents take over \n\nmore functions, basic operational knowledge could be eroded. Hence sufficient training \n\n(espe cially in areas where agents are prevalent) should be provided to ensure that humans \n\nretain core skills. \n\n# 2.4.1 Different users, different needs \n\nOrganisations should cater to different users with different information needs, to enable such \n\nusers to use AI responsibly. Broadly, there are two main archetypes of end -users â€“ those who \n\ninteract with agents, and those who integrate agents into their work processes or oversee them .\n\nUsers who interact with agents \n\ne.g. customer service, HR agents â€“\n\nmostly external -facing \n\nUsers who integrate agents into \n\ntheir work processes \n\ne.g. coding assistants, enterprise \n\nworkflows â€“ mostly internal -facing \n\nFocus on transparency  Layer on education and training 23 \n\n# 2.4.2 User s who interact with agents \n\nSuch users usually interact with agents that act on behalf of the organisation, e.g. customer \n\nservice or sales agents. These agents tend to be external facing , although they can also be deployed \n\nwithin the organisation e.g. a human resource agent that interacts with other users in the \n\norganisation. \n\nFor these users , focus on transparency . Organisations should share pertinent information to \n\nfoster trust and facilitate proper usage of agents. Such information can include: \n\nâ€¢ Userâ€™s responsibilities : Clearly define the userâ€™s responsibilities, such as asking the user to \n\ndouble -check all information provided by the agent. \n\nâ€¢ Interaction : Declare upfront that the user s are interacting with agent s.\n\nâ€¢ Agent sâ€™ range of actions an d decisions : Inform the user s on the range of actions and \n\ndecisions that the agent is authorised to perform and make .\n\nâ€¢ Data : Be clear on how user data is collected, stored, and used by the agents, in accordance \n\nwith the organi sation's data privacy policies. Where necessary, obtain explicit consent from \n\nusers before collecting or using their data for the agents. \n\nâ€¢ Human accountability and escalation : Provide user s with the respective human contact \n\npoint s who are responsible for the agent s, whom the user s can alert if the agent s malfunction \n\nor if they are dissatisfied with a decision. \n\n# 2.4.3 Users who integrate agents into their work processes \n\nSuch users typically utili se agents as part of their internal work flows e.g. coding assistants, \n\nautomation of enterprise processes . The agent acts for and on behalf of the user. \n\nFor these users , in addition to the information in the previous section , layer on education and \n\ntraining so that users can use the agents responsibly . Key aspects include education and training \n\non: \n\nâ€¢ Foundational knowledge on agents \n\no Relevant use cases , so that the user s understand how to best integrate the agents into \n\ntheir day -to -day work, and the scenarios under which the use of agent s should be \n\nrestricted (e.g. do not use an agent for confidential data) \n\no Instructing the agents e.g. general best practices in prompting, glossary of keywords to \n\nelicit specific responses \n\no Agentsâ€™ range of actions , so that the user is aware of their capabilities and potential \n\nimpact \n\nâ€¢ Effective oversight of agents \n\no Common agent failure modes , such as hallucinations, getting stuck in loops after errors ,\n\nso that the user can identify and flag out issues .\n\no Ongoing support , such as regular refreshers to update users on latest features and \n\ncommon user mistakes \n\nâ€¢ Potential impact on tradecraft 24 \n\no As agents take over entry level tasks, which typically serve as the training ground for new \n\nstaff, this could le ad to loss of basic operational knowledge for the users. \n\no Organisations should identify core capabilities of each job and provide s ufficient training \n\nand work exposure so that users retain foundational skills. 25 \n\n# Annex A: Further resources \n\n1.  Introduction to Agentic AI \n\nWhat is Agentic \n\nAI? \n\nâ€¢ AWS , Agentic AI Security Scoping Matrix: A framework for securing \n\nautonomous AI systems \n\nâ€¢ WEF , AI Agents in Action: Foundations for Evaluation and \n\nGovernance \n\nâ€¢ Anthropic,  Building effective agents \n\nâ€¢ IBM,  The 2026 Guide to AI Agents \n\nâ€¢ McKinsey , What is an AI agent? \n\nRisks of Agentic AI  â€¢ GovTech , Agentic Risk & Capability Framework \n\nâ€¢ CSA , Draft Addendum on Securing Agentic AI \n\nâ€¢ OWASP , Multi -Agentic System Threat Modelling Guide \n\nâ€¢ IBM,  AI agents: Opportunities, risks, and mitigations \n\nâ€¢ Infosys,  Agentic AI risks to the enterprise, and its mitigations \n\n2.  MGF for Agentic AI \n\nAssess and bound \n\nthe risks upfront \n\nAgentic governance in general \n\nâ€¢ EY , Building a risk framework for Agentic AI \n\nâ€¢ McKinsey , Deploying agentic AI with safety and security: A playbook \n\nfor technology leaders \n\nâ€¢ Bain,  Building the Foundation for Agentic AI \n\nâ€¢ OWASP,  State of Agentic AI Security and Governance 1.0 \n\nRisk assessment and threat modelling \n\nâ€¢ OWASP , Agentic AI â€“ Threats & Mitigations \n\nâ€¢ OWASP , Multi -Agentic System Threat Modelling Guide \n\nâ€¢ Cloud Security Alliance,  Agentic AI: Understanding Its Evolution, \n\nRisks, and Security Challenges \n\nâ€¢ EY,  Building a risk framework for Agentic AI \n\nAgent limits and agent identity \n\nâ€¢ Meta,  Agents Rule of Two: A Practical Approach to AI Agent Security \n\nâ€¢ OpenID , Identity Management for Agentic AI \n\nMake humans \n\nmeaningfully \n\naccountable \n\nAllocating responsibility within and outside an organisation \n\nâ€¢ Carnegie Mellon University,  The â€˜Whoâ€™, â€˜Whatâ€™, and â€˜Howâ€™ of \n\nResponsible AI Governance \n\nâ€¢ CSA and FAR.AI,  Securing Agentic AI: A Discussion Paper \n\nâ€¢ McKinsey,  Accountability by design in the agentic organization \n\nDesigning for meaningful human oversight \n\nâ€¢ Partnership on AI , Prioritizing real -time failure detection in AI agents 26 \n\nâ€¢ Permit.IO , Human -in -the -Loop for AI Agents: Best Practices, \n\nFrameworks, Use Cases, and Demo \n\nImplement \n\ntechnical controls \n\nand processes \n\nTechnical controls \n\nâ€¢ GovTech , Agentic Risk & Capability Framework \n\nâ€¢ CSA , Draft Addendum on Securing Agentic AI \n\nTesting and evaluation \n\nâ€¢ Microsoft , Microsoft Agent Evaluators \n\nâ€¢ AWS , AWS Agent Evaluation \n\nâ€¢ Anthropic,  Demystifying evals for AI agents \n\nâ€¢ IBM,  What is AI Agent Evaluation? \n\nMonitoring and observability \n\nâ€¢ Microsoft,  Top 5 agent observability best practices for reliable AI \n\nEnabling end -user \n\nresponsibility \n\nâ€¢ Zendesk , What is AI transparency? A comprehensive guide \n\nâ€¢ HR Brew , Salesforceâ€™s head of talent growth and development \n\nshares how the tech giant is training its 72,000 employees on \n\nagentic AI \n\nâ€¢ Harvard Business Review , The Perils of Using AI to Replace Entry -\n\nLevel Jobs 27 \n\n# Annex B: Call for feedback and case studies \n\nCall for feedback: This is a living document, and we invit e suggestions on how the framework can \n\nbe updated or refined. The following questions can be used as a guide: \n\nâ€¢ Introduction to Agentic AI : Are the descriptions of agentic AI systems accurate and \n\nsufficiently comprehensive for readers to obtain a clear overview of the governance \n\nchallenges of agentic AI ? Are there other risks that should be included? \n\nâ€¢ Proposed Model Governance Framework : Are the four dimensions of the framework \n\npractical and applicable? Are there any other dimensions that should be included? For \n\neach dimension, are there specific governance and technical challenges and best \n\npractices that should be included? \n\nCall for case studies: We also invite organisations to submit their own agentic governance \n\nexperiences as case studies on how specific aspects of the framework can be implemented, to serve \n\nas practical examples of responsible deploymen t that other organisations can refer to . Case studies \n\nshould ideally involve an organisationâ€™s deployment of an agentic use case that demonstrates one \n\nof the dimensions of the framework . While not exhaustive, we are specifically interested in case \n\nstudies that demonstra te good practices in :\n\nDimension  Example case studies \n\nAssess and bound \n\nthe risks upfront \n\nâ€¢ Defining use cases to reduce risk but maximise benefits of \n\nagents \n\nâ€¢ Defining limits on agentâ€™s autonomy through defined SOPs and \n\nworkflows \n\nâ€¢ Defining limits on agentâ€™s access to tools and systems \n\nâ€¢ How identity is implemented for agents, and how it interacts with \n\nhuman identities in an organisation \n\nMake humans \n\nmeaningfully \n\naccountable \n\nâ€¢ Allocating responsibility across the organisation for agentic \n\ndeployment \n\nâ€¢ Assessing when human approvals are required in an agentic use \n\ncase, and how requests for such approvals are implemented \n\nImplement technical \n\ncontrols and \n\nprocesses \n\nâ€¢ Designing and implementing technical controls for agents \n\nâ€¢ How agentic safety testing is carried out \n\nâ€¢ How monitoring and observability mechanisms are set up, \n\nincluding defining alert thresholds and processing large volumes \n\nof agent -related data \n\nEnable end -user \n\nresponsibility \n\nâ€¢ Making information available to internal and external \n\nstakeholders who interact with and use agents \n\nâ€¢ Training human overseers to exercise effective oversight \n\nFor an example of what a case study may look like , please refer to those in our previous  Model \n\nGovernance Framework for AI .\n\nPlease note that a ny feedback and case studies may be incorporated into an updated version of the \n\nframework, and contributors will be acknowledged accordingly . Please submit your feedback and \n\ncase studies at this link : https://go.gov.sg/mgfagentic -feedback .", "fetched_at_utc": "2026-02-08T18:51:47Z", "sha256": "d729cf2c58b90a9dcd0f9a35309e5fc837197030f026dd114a85ffac77296768", "meta": {"file_name": "Singapore - Governance for Agentic AI.pdf", "file_size": 1078313, "relative_path": "pdfs\\Singapore - Governance for Agentic AI.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 12742}}}
{"doc_id": "pdf-pdfs-the-language-of-trustworthy-ai-glossary-nist-ca7880ea8842", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The Language of Trustworthy AI Glossary - NIST.pdf", "title": "The Language of Trustworthy AI Glossary - NIST", "text": "Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\naccountability 1) relates to an allocated responsibility. The responsibility can be based on regulation or agreement or through assignment as part of delegation; 2) For systems, a property that ensures that actions of an entity can be traced uniquely to the entity; 3) In a governance context, the obligation of an individual or organization to account for its activities, for completion of a deliverable or task, accept the responsibility for those activities, deliverables or tasks, and to disclose the results in a transparent manner. ISO/IEC_TS_ 5723Ê¼2022(en) \"accountable\" (adjective vs. noun): answerable for actions, decisions, and performance ISO/IEC_TS_ 5723Ê¼2022(en) accuracy Closeness of computations or estimates to the exact or true values that the statistics were intended to measure. OECD A qualitative assessment of correctness or freedom from error. FDA_Glossary The measure of an instrument's capability to approach a true or absolute value. It is a function of precision and bias. FDA_Glossary The accuracy of a machine learning system is measured as the percentage of correct predictions or classifications made by the model over a specific data set. It is typically estimated using a test or \"hold out\" sample, other than the one(s) used to construct the model. Its complement, the error rate, is the proportion of incorrect predictions on the same data. Raynor measure of closeness of results of observations, computations, or estimates to the true values or the values accepted as being true ISO/IEC_TS_ 5723Ê¼2022(en) active learning A proposed method for modifying machine learning algorithms by allowing them to specify test regions to improve their accuracy. At any point, the algorithm can choose a new point x, observe the output and incorporate the new (x, y) pair into its training base. It has been applied to neural networks, prediction functions, and clustering functions. Raynor Active learning (also called â€œquery learning,â€ or sometimes â€œoptimal experimental designâ€ in the statistics literature) is a subfield of machine learning and, more generally, artificial intelligence. The key hypothesis is that, if the learning algorithm is allowed to choose the data from which it learnsâ€”to be â€œcurious,â€ if you willâ€”it will perform better with less training. settles_active _2009 the process of learning through activities and/or discussion in class, as opposed to passively listening to an expert. Freeman_et_a l_2014 active learning agent [a machine learning algorithm that can] decide what actions to take [with regards to its training data, in contrast to a passive learning agent, which is limited to a fixed policy]. Russell_and_N orvig passive learning agent activity Work that an organization performs using business processes; can be singular or compound. IEEE_Guide_I PA Set of cohesive tasks of a process. CSRC adaptive dynamic programming An adaptive dynamic programming (or ADP) agent takes advantage of the constraints among the utilities of states by learning the transition model that connects them and solving the corresponding Markov decision process using dynamic programming. Russell_and_N orvig A means of learning a model and a reward function from observations that then uses value or policy iteration to obtain the utilities or an optimal policy; makes optimal use of the local constraints on utilities of states imposed through the neighborhood structure of the environment. Russell_and_N orvig adaptive learning Updating predictive models online during their operation to react to concept drifts Gama,_Joao adversarial example Machine learning input sample formed by applying a small but intentionally worst-case perturbation ... to a clean example, such that the perturbed input causes a learned model to output an incorrect answer. NISTIR_8269_ Draft Samples generated from real samples with carefully designed imperceptible perturbations Zhang, _Yonggang adversarial perturbation adverse action notice A notification of i) a refusal to grant credit in substantially the amount or on substantially the terms requested in an application unless the creditor makes a counteroffer (to grant credit in a different amount or on other terms) and the applicant uses or expressly accepts the credit offered; ii) A termination of an account or an unfavorable change in the terms of an account that does not affect all or substantially all of a class of the creditor's accounts or iii) A refusal to increase the amount of credit available to an applicant who has made an application for an increase. ECOA adverse impact ratio privileged and unprivileged groups receiving different outcomes irrespective of the decision makerâ€™s intent and irrespective of the decision-making procedure. Quantified as the ratio: disparate impact ratio = ð‘ƒ ( ð‘¦Ì‚ (ð‘‹ ) = fav âˆ£âˆ£ ð‘ = unpr )/ ð‘ƒ ( ð‘¦Ì‚ (ð‘‹ )= fav âˆ£âˆ£ ð‘ = priv ) where ð‘ƒ (ð‘¦Ì‚ (ð‘‹ ) = fav) is the favorable label, ( ð‘ = priv) is the privileged group, and ( ð‘ = unpr) is the unprivileged group. Varshney, _Kush disparate impact ratio, relative risk ratio agile a development approach that delivers software in increments by following the principles of the Manifesto for Agile Software Development. Gartner A philosophy and methodology used to describe the continuous, iterative process to develop and deliver software and other digital technologies. User requirements and feedback inform incremental development and delivery by developers. NSCAI AI principles [An overarching concept, value, belief, or norm that guides AI development, testing, and deployment across the AI lifecycle. The OECD] identifies five complementary values-based principles for the responsible stewardship of trustworthy AI and calls on AI actors to promote and implement them: inclusive growth, sustainable development and well-being; human-centred values and fairness; transparency and explainability; robustness, security and safety; and accountability. OECD_CAI_re commendation algorithm A set of computational rules to be followed to solve a mathematical problem. More recently, the term has been adopted to refer to a process to be followed, often by a computer. Comptroller_O ffice precise rules for transforming specified inputs into specified outputs in a finite number of steps knuth_art_198 1algorithms are step-by-step procedures for solving problems. For concreteness, we can think of them simply as being computed programs, written in some precise computer languages garey_comput ers_1979 algorithmic aversion biased assessment of an algorithm which manifests in negative behaviours and attitudes towards the algorithm compared to a human agent. Ekaterina_et_ al_2020 alignment ensur[ing] that powerful AI is properly aligned with human values. ... The challenge of alignment has two parts. The first part is technical and focuses on how to formally encode values or principles in artificial agents so that they reliably do what they ought to do. ... The second part of the value alignment question is normative . It asks what values or principles, if any, we ought to encode in artificial agents. Gabriel_2020 amplification [an act of amplifying, which is] to make larger or greater (as in amount, importance, or intensity). Merriam-Webster_ampl ify Let [construct space] ð‘Œ â€² and [prediction space] ð‘Œ Ë† be categorical. Then, a model exhibits disparity amplification if \n\nð‘‘ tv ( ð‘Œ Ë† | ð‘ =0, ð‘Œ Ë† | ð‘ =1) > ð‘‘ tv ( ð‘Œ â€² | ð‘ =0, ð‘Œ â€² | ð‘ =1). dtv is the total variation distance defined as follows. Let ð‘Œ 0 and ð‘Œ 1 becategorical random variables with finite supports Y0 and Y1. Then,the total variation distance between ð‘Œ 0 and ð‘Œ 1 is ð‘‘ tv ( ð‘Œ 0, ð‘Œ 1) =12 Î£\n\nð‘¦ âˆˆY0 âˆªY1 Pr[ ð‘Œ 0= ð‘¦ ] âˆ’ Pr[ ð‘Œ 1= ð‘¦ ] .In the special case where ð‘Œ 0, ð‘Œ 1 âˆˆ {0, 1}, the total variation distancecan also be expressed as | Pr[ ð‘Œ 0=1] âˆ’ Pr[ ð‘Œ 1=1] |. yeom_avoiding _2021 analytics Analytics is the application of scientific & mathematical methods to the study & analysis of problems involving complex systems. There are three distinct types of analytics: * Descriptive Analytics gives insight into past events, using historical data. * Predictive Analytics provides insight on what will happen in the future. * Prescriptive Analytics helps with decision making by providing actionable advice. informs_analyt ics_2022 annotation Further documentation accompanying a requirement. IEEE_Soft_Vo cab [the act of] mak[ing] or furnish[ing] critical or explanatory notes or comment Merriam-Webster_anno tate anomaly Anything observed in the documentation or operation of a system that deviates from expectations based on previously verified system, software, or hardware products or reference documents. IEEE_Soft_Vo cab Condition that deviates from expectations, based on requirements specifications, design documents, user documents, or standards, or from someone's perceptions or experiences. SP800-160 anonymization The process in which individually identifiable data is altered in such a way that it no longer can be related back to a given individual. Among many techniques, there are three primary ways that data is anonymized. Suppression is the most basic version of anonymization and it simply removes some identifying values from data to reduce its identifiability. Generalization takes specific identifying values and makes them broader, such as changing a specific age (18) to an age range (18-24). Noise addition takes identifying values from a given data set and switches them with identifying values from another individual in that data set. Note that all of these processes will not guarantee that data is no longer identifiable and have to be performed in such a way that does not harm the usability of the data. IAPP_Privacy_ Glossary process that removes the association between the identifying dataset and the data subject CSRC anthropomorphism the attribution of distinctively human-like feelings, mental states, and behavioral characteristics to inanimate objects, animals, and in general to natural phenomena and supernatural entities Anthropomorp hism_in_AI_2 020 a particular human-like interpretation of existing physical features and behaviors that goes beyond what is directly observable Anthropomorp hism_in_AI_2 020 application A software program hosted by an information system. SP800-37 A hardware/software system implemented to satisfy a particular set of requirements. CSRC software or a program that is specific tothe solution of an application problem aime_measure ment_2022 citing ISO/IEC TR 24030 application programming interface (API) a software contract between the application and client, expressed as a collection of methods or functions. . . it defines the available functions you can execute; . . . the intermediary interface between the client and the application. Hands-On_Smart_Co ntract_Dev artificial intelligence (AI) system an engineered or machine-based system that can, for a given set of objectives, generate outputs such as predictions, recommendations, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy NIST AI RMF (Adapted from: OECD Recommendati on on AIÊ¼2019; ISO/IEC 22989Ê¼2022). artificial intelligence learning The ingestion of a corpus, application of semantic mapping, and relevant ontology of structured and/or unstructured data that yields inference and correlation leading to the creation of useful conclusive or predictive capabilities in a given knowledge domain. Strong AI learning also includes the capability of creating unique hypotheses, attributing data relevance, processing data relationships, and updating its own lines of inquiry to further the usefulness of its purpose. IEEE_Guide_I PA artificial narrow intelligence (ANI) [an AI system that] is designed to accomplish a specific problem-solving or reasoning task. OECD_Artifici al_Intelligence _in_Society weak intelligence; applied intelligence Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nartificial neural networks A computing system, made up of a number of simple, highly interconnected processing elements, which processes information by its dynamic state response to external inputs. Reznik,_Leon Definition 1. A directed graph is called an Artificial Neural Network (ANN) if it has x at least one start node (or Start Element; SE), x at least one end node (or End Element; EE), x at least one Processing Element (PE), x all the nodes used must be Processing Elements (PEs), except start nodes and end nodes, x a state variable ni associated with each node i, x a real valued weight wki associated with each link (ki) from node k to node i, x a real valued bias bi associated with each node i, x at least two of the multiple PEs connected in parallel, x a learning algorithm that helps to model the desired output for given input. x a flow on each link (ki) from node k to node i, that carries exactly the same flow which equals to nk caused by the output of node k , x each start node is connected to at least one end node, and each end node is connected to at least one start node, x no parallel edges (each link (ki) from node k to node i is unique). assessment Action of applying specific documented criteria to a specific software module, package or product for the purpose of determining acceptance or release of the software module, package or product. IEEE_Soft_Vo cab the action or an instance of making a judgment about something : the act of assessing something : APPRAISAL Merriam-Webster_asses sment attack Action targeting a learning system to cause malfunction. NISTIR_8269_ Draft Any kind of malicious activity that attempts to collect, disrupt, deny, degrade, or destroy information system resources or the information itself. CSRC attribute Property associated with a a set of real or abstract things that is some characteristic of interest. IEEE_Soft_Vo cab property or characteristic of an object that can be distinguished quantitatively or qualitatively by human or automated means aime_measure ment_2022, citing ISO/IEC TR 24029-1 audit Systematic, independent, documented process for obtaining records, statements of fact, or other relevant information and assessing them objectively, to determine the extent to which specified requirements are fulfilled. IEEE_Soft_Vo cab To conduct an independent review and examination of system records and activities in order to test the adequacy and effectiveness of data security and data integrity procedures, to ensure compliance with established policy and operational procedures, and to recommend any necessary changes. FDA_Glossary Independent examination of a software product, software process, or set of software processes to assess compliance with specifications, standards, contractual agreements, or other criteria NASA_Soft_St andards Independent review conducted to compare the various aspects of the laboratoryâ€™ s performance with a standard for that performance. Also defined as a systematic, independent and documented process for obtaining audit evidence and evaluating it objectively to determine the extent to which audit criteria are fulfilled. UNODC_Gloss ary_QA_GLP audit log A chronological record of system activities, including records of system accesses and operations performed in a given period. SP800-37 authenticity property that an entity is what it claims to be ISO/IEC_TS_ 5723Ê¼2022(en) automation Independent machine-managed choreography of the operation of one or more digital systems. IEEE_Guide_I PA conversion of processes or equipment to automatic operation, or the results of the conversion IEEE_Soft_Vo cab The system functions with no/little human operator involvement; however, the system performance is limited to the specific actions it has been designed to do. Typically these are well-defined tasks that have predetermined responses (i.e., simple rule-based responses). DOD_TEVV automation bias over-relying on the outputs of AI systems David_Leslie_ Morgan_Brigg sautonomic A monitor-analyze-plan-execute (MAPE) computer system capable of sensing environments, interpreting policy, accessing knowledge (data --- information ---knowledge), making decisions, and initiating dynamically assembled routines of choreographed activity to both complete a process and update the set of environmental variables that enables the autonomic system to self-manage its own operation and the processes it oversees. An autonomic system is identified by eight characteristics: a) Knows the resources to which it has access, what its capabilities and limitations are, and how and why it is connected to other systems. b) Is able to configure and reconfigure itself depending on the changing computing environment. c) Is able to optimize its performance to ensure the most efficient computing process. d) Is able to work around encountered problems either by repairing itself or routing functions away from the trouble. e) Is able to detect, identify, and protect itself against various types of attacks to maintain overall system security and integrity. f) Is able to adapt to its environment as it changes by interacting with neighboring systems and establishing communication protocols. g) Relies on open standards and requires access to proprietary environments to achieve full performance. h) Is able to anticipate the demand on its resources transparently to users. IEEE_Guide_I PA autonomous vehicle [an] automobile, bus, tractor, combine, boat, forklift, etc. . . . capable of sensing its environment and moving safely with little or no human input. Introduction_t o_Information _Systems autonomy A systemâ€™s level of independence from human involvement and ability to operate without human intervention. [Different AI systems have different levels of autonomy.] An autonomous system has a set of learning, adaptive and analytical capabilities to respond to situations that were not pre-programmed or anticipated (i.e., decision-based responses) prior to system deployment. Autonomous or semi-autonomous AI systems can be characterised as \"human-in-the-loop\", \"human-on-the-loop\", or \"human-out-of-the loop\" systems depending on their level of meaningful involvement of human beings. TTC6_Taxono my_Terminolo gy availability Ensuring timely and reliable access to and use of information. SP800-37 The property that data or information is accessible and usable upon demand by an authorized person. NIST_SP_800 property of being accessible and usable on demand by an authorized entity ISO/IEC_TS_ 5723Ê¼2022(en) back-testing A form of outcomes analysis that involves the comparison of actual outcomes with modeled forecasts during a development sample time period (in-sample back-testing) and during a sample period not used in model development (out-of-time back-testing), and at an observation frequency that matches the forecast horizon or performance window of the model. Comptroller_O ffice batched automation Process automation execution of intentionally segregated work processes that are able to be processed irrespective of their contextual placement within a service. IEEE_Guide_I PA benchmark Standard against which results can be measured or assessed; Procedure, problem, or test that can be used to compare systems or components to each other or to a standard. IEEE_Soft_Vo cab An alternative prediction or approach used to compare a modelâ€™s inputs and outputs to estimates from alternative internal or external data or models. Comptroller_O ffice The term benchmarking is used in machine learning (ML) to refer to the evaluationand comparison of ML methods regarding their ability to learn patterns in â€˜benchmarkâ€™datasets that have been applied as â€˜standardsâ€™. Benchmarking could be thought of simplyas a sanity check to confirm that a new method successfully runs as expected and canreliably find simple patterns that existing methods are known to identify. olson_pmlb_2 017 bias A systematic error. In the context of fairness, we are concerned with unwanted bias that places privileged groups at systematic advantage and unprivileged groups at systematic disadvantage. AI_Fairness_3 60 (computational bias) An effect which deprives a statistical result of representativeness by systematically distorting it, as distinct from a random error which may distort on any one occasion but balances out on the average. OECD (systemic bias) systematic difference in treatment of certain objects, people or groups in comparison to others measurement_ iso22989_2022 (mathematical) A point estimator \\theta_hat is said to be an unbiased estimator fo \\theta if E(\\theta_hat) = \\theta for every possible value of \\theta. If \\theta_hat is not unbiased, the difference E(\\theta_hat) - \\theta is called the bias of \\theta devore_probab ility_2004 bias mitigation algorithm A procedure for reducing unwanted bias in training data or models. AI_Fairness_3 60 bias testing As it relates to disparate impact, courts and regulators have utilized or considered as acceptable various statistical tests to evaluate evidence of disparate impact. Traditional methods of statistical bias testing look at differences in predictions across protected classes, such as race or sex. In particular, courts have looked to statistical significance testing to assess whether the challenged practice likely caused the disparity and was not the result of chance or a nondiscriminatory factor. SP1270 big data consists of extensive datasets primarily in the characteristics of volume, variety, velocity, and/or variability \u0000that require a scalable architecture for efficient storage, manipulation, and analysis NIST_1500 binning a technique of lumping small ranges of values together into categories, or \"bins,\" for the purpose of reducing the variability (removing some of the fine structure) in a data set. Pyle, _Dorian_Data _Preparation_ as_a_Process biometric data personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person, such as facial images or dactyloscopic data; GDPR an individualâ€™s physiological, biological, or behavioral characteristics, including information pertaining to an individualâ€™s deoxyribonucleic acid (DNA), that is used or is intended to be used singly or in combination with each other or with other identifying data, to establish individual identity. Biometric information includes, but is not limited to, imagery of the iris, retina, fingerprint, face, hand, palm, vein patterns, and voice recordings, from which an identifier template, such as a faceprint, a minutiae template, or a voiceprint, can be extracted, and keystroke patterns or rhythms, gait patterns or rhythms, and sleep, health, or exercise data that contain identifying information. CCPA A measurable physical characteristic or personal behavioral trait used to recognize the identity, or verify the claimed identity, of an applicant. Facial images, fingerprints, and iris scan samples are all examples of biometrics. SP800-12 personal data; processing boosting A machine learning technique that iteratively combines a set of simple and not very accurate classifiers (referred to as \"weak\" classifiers) into a classifier with high accuracy (a \"strong\" classifier) by upweighting the examples that the model is currently misclassifying aime_measure ment_2022, citing Machine Learning Glossary by Google breach The loss of control, compromise, unauthorized disclosure, unauthorized acquisition, or any similar occurrence where: a person other than an authorized user accesses or potentially accesses personally identifiable information; or an authorized user accesses personally identifiable information for another than authorized purpose. CSRC broad artificial intelligence (broad AI) Complex, computational, cognitive automation system capable of providing descriptive, predictive, prescriptive, and limited deductive analytics with relevance and accuracy exceeding human expertise in a broad, logically related set of knowledge domains. IEEE_Guide_I PA Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nbuilt-in test Equipment or software embedded in the operational components or systems, as opposed to external support units, which perform a test or sequence of tests to verify mechanical or electrical continuity of hardware, or the proper automatic sequencing, data processing, and readout of hardware or software systems. SP1011 bug-bounty Reward given to independent security researchers, penetrations testers, and white hat hackers for discovering exploitable software vulnerabilities and sharing this knowledge with the operator of a particular bug-bounty program (BBP). Kuehn, _Andreas business process A defined set of business activities that represent the steps or tasks required to achieve a business objective, including the flow and use of information, participants, and human or digital resources. IEEE_Guide_I PA business process management Discipline involving any combination of modeling, automation, execution, control, measurement and optimization of business activity flows, in support of enterprise goals, spanning systems, employees, customers, and partners within and beyond the enterprise boundaries. IEEE_Guide_I PA business rule Definition, constraint, dependency, or decision criteria that determine the method of execution of a task or tasks, or influences the order of execution of a task or tasks. Business rules assert control, or influence the behavior, of a business process within computing systems. IEEE_Guide_I PA calibration A comparison between a device under test and an established standard, such as UTC(NIST). When the calibration is finished, it should be possible to state the estimated time offset and/or frequency offset of the device under test with respect to the standard, as well as the measurement uncertainty. CSRC operation that, under specified conditions, in a first step, establishes a relation between the quantity values with measurement uncertainties provided by measurement standards and corresponding indications with associated measurement uncertainties and, in a second step, uses this information to establish a relation for obtaining a measurement result from an indication aime_measure ment_2022, citing ISO/IEC Guide 99 Set of operations that establish, under specified conditions, the relationship between values indicated by a measuring instrument or measuring system, or values represented by a material measure, and the corresponding known values of a measurand .UNODC_Gloss ary_QA_GLP capability measure of capacity and the ability of an entity, person or organization to achieve its objectives ISO/IEC_TS_ 5723Ê¼2022(en) case Single entry, single exit multiple way branch that defines a control expression, specifies the processing to be performed for each value of the control expression, and returns control in all instances to the statement immediately following the overall construct. IEEE_Soft_Vo cab chatbot Conversational agent that dialogues with its user (for example: empathic robots available to patients, or automated conversation services in customer relations). COE_AI_Gloss ary choreography An ordered sequence of system-to-system message exchanges between two or more participants. In choreography, there is no central controller, responsible entity, or observer of the process. IEEE_Guide_I PA classification When the output is one of a finite set of values (such as sunny, cloudy or rainy), the learning problem is called classification, and is called Boolean or binary classification if there are only two values. AIMA task of assigning collected data to target categories or classes. aime_measure ment_2022, citing ISO/IEC TR 24030 classifier A model that predicts categorical labels from features. AI_Fairness_3 60 clustering Detecting potentially useful clusters of input examples. AIMA The basic problem of clustering may be stated as follows: Given a set of data points, partition them into a set of groups which are as similar as possible. aggarwal_clust ering_2013 the tendency for items to be consistently grouped together in the course of recall. This grouping typically occurs for related items. It is readily apparent in memory tasks in which items from the same category, such as nonhuman animals, are recalled together. APA_clusterin gcognitive automation The identification, assessment, and application of available machine learning algorithms for the purpose of leveraging domain knowledge and reasoning to further automate the machine learning already present in a manner that may be thought of as cognitive. With cognitive automation, the system performs corrective actions driven by knowledge of the underlying analytics tool itself, iterates its own automation approaches and algorithms for more expansive or more thorough analysis, and is thereby able to fulfill its purpose. The automation of the cognitive process refines itself and dynamically generates novel hypotheses that it can likewise assess against its existing corpus and other information resources. IEEE_Guide_I PA cognitive computing Complex computational systems designed to â€” Sense (perceive the world and collect data); â€” Comprehend (analyze and understand the information collected); - Act (make informed decisions and provide guidance based on this analysis in an independent way); and â€” Adapt (adapt capabilities based on experience) in ways comparable to the human brain. IEEE_Guide_I PA column In the context of relational databases, a column is a set of data values, all of a single type, in a table. techopedia_co lumn_2022 computer vision The digital process of perceiving and learning visual tasks in order to interpret and understand the world through cameras and sensors. NSCAI An image understanding task that automatically builds a description not only of the image itself, but of the three dimensional scene that it depicts. NBSIR_82-2582 concept drift Use of a system outside the planned domain of application, and a common cause of performance gaps between laboratory settings and the real world. SP1270 an online supervised learning scenario when the relation between the input data and the target variable changes over time. Gama,_Joao Systems that classify or predict a concept (e.g., credit ratings or computer intrusion monitors) over time can suffer performance loss when the concept they are tracking changes. This is referred to as concept drift. This can either be a natural process that occurs without a reference to the system, or an active process, where others are reacting to the system (e.g., virus detection). Raynor confidentiality Data confidentiality is a property of data, usually resulting from legislative measures, which prevents it from unauthorized disclosure. OECD Preserving authorized restrictions on information access and disclosure, including means for protecting personal privacy and proprietary information. CSRC The property that data or information is not made available or disclosed to unauthorized persons or processes. NIST_SP_800 A property that information is not disclosed to users, processes, or devices unless they have been authorized to access the information. CISA confusion matrix A matrix showing the predicted and actual classifications. A confusion matrix is of size LxL, where L is the number of different label values Kohavi,_Ron consent â€˜Consentâ€™ of the data subject means any freely given, specific, informed and unambiguous indication of the data subject's wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her. GDPR â€œConsentâ€ means any freely given, specific, informed, and unambiguous indication of the consumerâ€™s wishes by which the consumer, or the consumerâ€™s legal guardian, a person who has power of attorney, or a person acting as a conservator for the consumer, including by a statement or by a clear affirmative action, signifies agreement to the processing of personal information relating to the consumer for a narrowly defined particular purpose. Acceptance of a general or broad terms of use, or similar document, that contains descriptions of personal information processing along with other, unrelated information, does not constitute consent. Hovering over, muting, pausing, or closing a given piece of content does not constitute consent. Likewise, agreement obtained through use of dark patterns does not constitute consent. CCPA personal data constituent system independent system that forms part of a system of systems (SoS) (note: Constituent systems can be part of one or more SoS. Each constituent system is a useful system by itself, having its own development, management, utilization, goals, and resources, but interacts within the SoS to provide the unique capability of the SoS). ISO/IEC_TS_ 5723Ê¼2022(en) constraint Specification of what may be contained in a data or metadata set in terms of the content or, for data only, in terms of the set of key combinations to which specific attributes (defined by the data structure) may be attached. OECD A limitation or implied requirement that constrains the design solution or implementation of the systems engineering process and is not changeable by the enterprise IEEE_Soft_Vo cab construct validity the degree to which the application of constructs to phenomena is warranted with respect to the research goals and questions. Wieringa, _Roel_J. Construct validation is involved whenever a test is to be interpreted as a measure of some attribute or quality which is not â€œoperationally defined.â€ The problem faced by the investigator is, â€œWhat constructs account for variance in test performance?â€ cronbach_con struct_1955 Established experimentally to demonstrate that a survey distinguishes between people who do and do not have certain characteristics. It is usually established experimentally. fink_survey_2 010 Establishing construct validity means demonstrating, in a variety of ways, that the measurements obtained from measurement model are both meaningful and useful. jacobs_measur ement_2023 content validity Refers to the extent to which a measure thoroughly and appropriately assesses the skills or characteristics it is intended to measure. fink_survey_2 010 the extent to which a test measures a representative sample of the subject matter or behavior under investigation. For example, if a test is designed to survey arithmetic skills at a third-grade level, content validity indicates how well it represents the range of arithmetic operations possible at that level. Modern approaches to determining content validity involve the use of exploratory factor analysis and other multivariate statistical procedures. APA_content_ validity context The context is the circumstances, purpose, and perspective under which an object is defined or used. OECD The immediate environment in which a function (or set of functions in a diagram) operates IEEE_Soft_Vo cab the interrelated conditions in which something exists or occurs. Merriam-Webster_cont ext contextual learning A computing system with sufficient knowledge regarding its purpose that it understands the source, relevance, and utility of data and inputs. IEEE_Guide_I PA context-of-use The Context of Use is the actual conditions under which a given artifact/software product is used, or will be used in a normal day to day working situation. interaction_co ntext_2023 comprises a combination of users, goals, tasks, resources, and the technical, physical and social, cultural and organizational environments in which a system, product or service is used[; ...] can include the interactions and interdependencies between the object of interest and other systems, products or services. ISO_9241-11Ê¼ 2018 controllability property of a system that allows a human or another external agent to intervene in the systemâ€™s functioning; such a system is heteronomous. ISO/IEC_TS_ 5723Ê¼2022(en) control class (control group) the set of observations in an experiment or prospective study that do not receive the experimental treatment(s). These observations serve (a) as a comparison point to evaluate the magnitude and significance of each experimental treatment, (b) as a reality check to compare the current observations with previous observation history, and (c) as a source of data for establishing the natural experimental error. nist_statistics _2012 controller â€˜Controllerâ€™ means the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law; GDPR personal data; processor Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\ncopilot An artificial intelligence powered software program designed to assist users with various tasks and automate features within compatible applications using advanced language models, machine-learning algorithms, and conversational interfaces to understand user requests and provide suggestions, summaries, and content generation in response. A product or service that provides assistance using, incorporating and/or based on artificial intelligence software and artificial intelligence software services corpus (corpora) A deliberately assembled collection of knowledge and data (structured and/or unstructured) believed to contain relevant information on a topic or topics to be used by software systems for which useful analysis, prediction, or outcome is being sought. IEEE_Guide_I PA correlation In its most general sense correlation denoted the interdependence between quantitative or qualitative data. In this sense it would include the association of dichotomised attributes and the contingency of multiply-classified attributes. OECD The correlation coefficient of two random variables y_1, and y_2, denoted \\rho (y_1,y_2) is: \\rho(y_1, y_2) = Cov(y_1, y_2)/\\sqrt{Var(y_1)*Var(y_2)} box_statistics _2005 counterfactual explanation Statements taking the form: Score p was returned because variables V had values (v1, v2,...) associated with them. If V instead had values (v1', v2',...) score p' would have been returned. wachter_coun terfactual_201 8counterfactual fairness A fairnessmetric that checks whether a classifier produces the same result for one individualas it does for another individual who is identical to the first, except withrespect to one or more sensitive attributes. Evaluating a classifier for counterfactualfairness is one method for surfacing potential sources of bias in a model aime_measure ment_2022, citing Machine Learning Glossary by Google Given a predictive problem with fairness considerations, where A, X and Y represent the protectedattributes, remaining attributes, and output of interest respectively, let us assume that we are given acausal model (U; V; F), where V = A \\cup X. We postulate the following criterion for predictors of Y .Definition 5 (Counterfactual fairness). Predictor ^Y is counterfactually fair if under any context X = x and A = a, P( ^Y_{A <- a} (U) = y | X = x; A = a) = P( ^Y_{A <- a')(U) = y | X = x;A = a); (1) for all y and for any value a' attainable by A. kusner_counte rfactual_2017 countermeasure Actions, devices, procedures, techniques, or other measures that reduce the vulnerability of a system. Synonymous with security controls and safeguards. SP800-37 Actions, devices, procedures, or techniques that meet or oppose (i.e., counters) a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken. GWUC safeguard; security control criterion validity compares responses to future performance or to those obtained from other, more well-established surveys. Criterion validity is made up two subcategories: predictive and concurrent. Predictive validity refers to the extent to which a survey measure forecasts future performance. A graduate school entry examination that predicts who will do well in graduate school has predictive validity. Concurrent validity is demonstrated when two assessments agree or a new measure is compared favorably with one that is already considered valid. fink_survey_2 010 an index of how well a test correlates with an established standard of comparison (i.e., a criterion). Criterion validity is divided into three types: predictive validity, concurrent validity, and retrospective validity. For example, if a measure of criminal behavior is valid, then it should be possible to use it to predict whether an individual (a) will be arrested in the future for a criminal violation, (b) is currently breaking the law, and (c) has a previous criminal record. APA_criterion _validity criterion-referenced validity; criterion-related validity crowdsource a type of participative online activity in which an individual, an institution, a non-profit organization, or company proposes to a group of individuals of varying knowledge, heterogeneity, and number, via a flexible open call, the voluntary undertaking of a task. Enrique customer The beneficiary of the execution of an automated task, process, or service. IEEE_Guide_I PA cybersecurity Prevention of damage to, protection of, and restoration of computers, electronic communications systems, electronic communications services, wire communication, and electronic communication, including information contained therein, to ensure its availability, integrity, authentication, confidentiality, and nonrepudiation. SP800-37 dark pattern â€œDark patternâ€ means a user interface designed or manipulated with the substantial effect of subverting or impairing user autonomy, decisionmaking, or choice, as further defined by regulation. CCPA data Characteristics or information, usually numerical, that are collected through observation. OECD re-interpretable representation of information ina formalized manner suitable for communication, interpretation or processing aime_measure ment_2022, citing ISO/IEC TR 24029-1 data analytics the process of applying graphical, statistical, or quantitative techniques to a set of observations or measurements in order to summarize it or to find general patterns. APA_data_ana lysis Data analysis is the process of transforming raw data into usable information, often presented in the form of a published analytical article, in order to add value to the statistical output. OECD data cleaning Data Cleaning is the process of identifying, correcting, or removing inaccurate or corrupt data records Ranschaert, _Erik data control management oversight of information policies for an organizationâ€™s information; observing and reporting on how processes are working and managing issues. Egnyte data dredging A statistical bias in which testing huge numbers of hypotheses of a dataset may appear to yield statistical significance even when the results are statistically nonsignificant. SP1270 statistical bias; p-hacking data drift The change in model input data that leads to model performance degradation. Microsoft_Azu re_documenta tion data-driven Data-driven decision making (DDD) refers to the practice of basing decisions on the analysis of data rather than purely on intuition. provost_data_ 2013 data fabric A data corpus, after the application of semantic mapping, relevant ontologies, and data seeding sufficient for artificial intelligence (AI) or machine learning algorithms to provide meaningful insight, prediction, and/or prescription. IEEE_Guide_I PA data fusion A process in which data, generated by multiple sensory sources, is integrated and/or correlated to create information, knowledge, and/or intelligence that may be displayed for user or be actionable to accomplish the tasks. SP1011 The process of combining data from multiple sources to produce more accurate, consistent, and concise information than that provided by any individual data source. Munir,_Arslan data governance A set of processes that ensures that data assets are formally managed throughout the enterprise. A data governance model establishes authority and management and decision making parameters related to the data produced or managed by the enterprise. CSRC refers to a system, including policies, people, practices, and technologies, necessary to ensure data management within an organization NIST_1500 data mining computational process that extracts patternsby analysing quantitative data from different perspectives and dimensions, categorizingthem, and summarizing potential relationships and impacts aime_measure ment_2022 citinig ISO/IEC 22989 the process of data analysis and information extraction from large amounts of datasets with machine learning, statistical approaches. and many others. Ranschaert, _Erik data point a discrete unit of information. TechTarget_da ta_point data preparation We define data preparation as the set of preprocessing operations performed in early stages of a data processing pipeline, i.e., data transformations at the structural and syntactical levels hameed_data_ 2020 data proxy Data that are closely related to and serve in place of data that are either unobservable or immeasurable. Comptroller_O ffice data quality degree to which the characteristics of data satisfy stated and implied needs when used under specified conditions IEEE_Soft_Vo cab The dimensions of the IMF definition of \"data quality\" are: - integrity; - methodological soundness; - accuracy and reliability; - serviceability; - accessibility. There are a number of prerequisites for quality. These comprise: - legal and institutional environment; - resources; - quality awareness. OECD data science Methodology for the synthesis of useful knowledge directly from data through a process of discovery or of hypothesis formulation and hypothesis testing. NIST_1500 Interdisciplinary science that uses statistics, algorithms, and other methods to extract meaningful and useful patterns from data setsâ€”sometimes known as â€œbig data.â€ Today, machine learning is often used in this field. Next to analysis of data, data science is also concerned with the capturing, preparation, and interpretation of data. AI_Ethics_Mar k_Coeckelberg hartificial intelligence (AI); machine learning (ML) data scientist A practitioner who has sufficient knowledge in the overlapping regimes of business needs, domain knowledge, analytical skills, and software and systems engineering to manage the end-to-end data processes in the analytics life cycle. NIST_1500 data seeding The intentional introduction of initial state conditions, influencing factors, and outcomes (both successful and unsuccessful) in a data fabric to create sufficient machine learning analysis signals to enable encouragement/discouragement to enrich deterministic relationships between data elements in a given information domain. IEEE_Guide_I PA data wrangling process by which the data required by an application is identified, extracted, cleaned and integrated, to yield a data set that is suitable for exploration and analysis. Furche,_Tim decision A conclusion reached after consideration of business rules and relevant data within a given process. IEEE_Guide_I PA Types of statements in which a choice between two or more possible outcomes controls which set of actions will result. IEEE_Soft_Vo cab decision point A point within a business process where the process flow can take one of several alternative paths, including recursive. IEEE_Guide_I PA decision tree Treeâ€structure resembling a flowchart, where every node represents a test to an attribute, each branch represents the possible outcomes of that test, and the leaves represent the class labels. Reznik,_Leon decision-making the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker. Every decision-making process produces a final choice, which may or may not prompt action. Wikipedia_Dec ision-making the cognitive process of choosing between two or more alternatives, ranging from the relatively clear cut (e.g., ordering a meal at a restaurant) to the complex (e.g., selecting a mate). Psychologists have adopted two converging strategies to understand decision making: (a) statistical analysis of multiple decisions involving complex tasks and (b) experimental manipulation of simple decisions, looking at the elements that recur within these decisions. APA_decision_ making Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\ndecision support system a computer program application used to improve a company's decision-making capabilities. It analyzes large amounts of data and presents an organization with the best possible options available[; they] bring together data and knowledge from different areas and sources to provide users with information beyond the usual reports and summaries. This is intended to help people make informed decisions. TechTarget_d ecision_suppo rt_system decommission the total or partial removal of existing components and their corresponding sub-components from Production and any relevant environment, minimizing risks and impacts, ensuring policy compliance, and maximizing the financial benefits (i. e., optimizing the cost reduction). IG1190M_AIOp s_Decommissi on_v1.0.0 deductive analytics Insights, reporting, and information answering the question, \"What would likely happen IFâ€¦?â€ Deductive analytics evaluates causes and outcomes of possible future events. IEEE_Guide_I PA deductive reasoning deep learning Deep learning is a broad family of techniques for machine learning in which hypotheses take the form of complex algebraic circuits with tunable connection strengths. The word â€œdeepâ€ refers to the fact that the circuits are typically organized into many layers, which means that computation paths from inputs to outputs have many steps. Deep learning is currently the most widely used approach for applications such as visual object recognition, machine translation, speech recognition, speech synthesis, and image synthesis; it also plays a significant role in reinforcement learning applications. Russell_and_N orvig A form of machine learning that uses neural networks with several layers of \"neurons\": simple interconnected processing units that interact. AI_Ethics_Mar k_Coeckelberg h[an approach to AI that allows] computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept defined through its relation to simpler concepts. By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all the knowledge that the computer needs. The hierarchy of concepts enables the computer to learn complicated concepts by building them out of simpler ones. If we draw a graph showing how these concepts are built on top of each other, the graph is deep, with many layers. deeplearningb ook_intro deepfake AI-generated or manipulated image, audio or video content that resembles existing persons, objects, places or other entities or events and would falsely appear to a person to be authentic or truthful. TTC6_Taxono my_Terminolo gy deletion Of an <X>, the action of destroying an instantiated <X>. IEEE_Soft_Vo cab denial-of-service The prevention of authorized access to resources or the delaying of time-critical operations. (Time-critical may be milliseconds or it maybe hours, depending upon the service provided). SP800-12 An attack that prevents or impairs the authorized use of information system resources or services. CISA when legitimate users are unable to access information systems, devices, or other network resources due to the actions of a malicious cyber threat actor. Services affected may include email, websites, online accounts (e.g., banking), or other services that rely on the affected computer or network. A denial-of-service condition is accomplished by flooding the targeted host or network with traffic until the target cannot respond or simply crashes, preventing access for legitimate users. DoS attacks can cost an organization both time and money while their resources and services are inaccessible. ST04-015 dependability <of an item> ability to perform as and when required (note 1Ê¼ includes availability, reliability, recoverability, maintainability, and maintenance support performance, and, in some cases, other characteristics such as durability, safety and security. Note 2Ê¼ used as a collective term for the time-related quality characteristics of an item). ISO/IEC_TS_ 5723Ê¼2022(en) deployment Phase of a project in which a system is put into operation and cutover issues are resolved IEEE_Soft_Vo cab descriptive analytics Insights, reporting, and information answering the question, â€œWhy did something happen?â€ Descriptive analytics determines information useful to understanding the cause(s) of an event(s). IEEE_Guide_I PA deterministic modelling [that] produces consistent outcomes for a given set of inputs, regardless of how many times the model is recalculated. The mathematical characteristics are known in this case. None of them is random, and each problem has just one set of specified values as well as one answer or solution. The unknown components in a deterministic model are external to the model. It deals with the definitive outcomes as opposed to random results and doesnâ€™t make allowances for error. Sourabh_Meht a_deterministi cdeterministic algorithm An algorithm that, given the same inputs, always produces the same outputs. CSRC developer A general term that includes developers or manufacturers of systems, system components, or system services; systems integrators; vendors; and product resellers. Development of systems, components, or services can occur internally within organizations or through external entities. SP800-37 Individual or organization that performs development activities (including requirements analysis, design, testing through acceptance) during the system or software lifeâ€cycle process. IEEE_Soft_Vo cab diagnostic analytics Insights, reporting, and information answering the question, â€œWhy did something happen?â€ Diagnostic analytics determines information useful to understanding the cause(s) of an event(s). IEEE_Guide_I PA diagnostics Pertaining to the detection and isolation of faults or failures IEEE_Software _Vocab differential privacy Differential privacy is a method for measuring how much information the output of a computation reveals about an individual. It is based on the randomised injection of \"noise\". Noise is a random alteration of data in a dataset so that values such as direct or indirect identifiers of individuals are harder to reveal. An important aspect of differential privacy is the concept of â€œepsilonâ€ or É›, which determines the level of added noise. Epsilon is also known as the â€œprivacy budgetâ€ or â€œprivacy parameterâ€. privacy-enhancing_tec hnologies For two datasets D and D' that differ in at most one element, a randomized algorithm $M$ guarantees \\emph{$(\\epsilon, \\delta)$-differential privacy} for any subset of the output $S$ if $M$ satisfies: \\begin{equation} Pr[M(D) \\in S] \\leq exp(\\epsilon)*Pr[M(D') \\in S] + \\delta \\end{equation} Furthermore, when $\\delta = 0$ an algorithm M is said to guarantee \\emph {$\\epsilon$-differential privacy} gong_different ial_2020 differential validity Differential validity states that the validities in two applicant populations are unequal, that is, pi != pa. hunter_differe ntial_1979 digital labor Digital automation of information technology systems and/or business processes that successfully delivers work output previously performed by human labor or new work output that would typically or alternatively have been performed by human labor. IEEE_Guide_I PA digital workforce The collective suite of automation technologies delivering existing or new work output as applied in a business; the manifestation of digital labor. IEEE_Guide_I PA dimension The dimension of an object is a topological measure of the size of its covering properties. Roughly speaking, it is the number of coordinates needed to specify a point on the object. wolfram_math _2022 Distinct components that a multidimensional construct encompasses IEEE_Soft_Vo cab dimension reduction Dimensionality reduction is the process of taking data in a high dimensional space and mapping it into a new space whose dimensionality is much smaller Shalev-Shwartz,_Shai disparate impact For Predictor Y and Sensitive Impact S. Definition 6.2 Disparate Impact (DI) = P[YË† = 1 | S != 1]/P[YË† = 1 | S = 1] friedler_comp arative_2019 disparate treatment Intentional discrimination, including (i) decisions explicitly based on protected characteristics; and (ii) intentional discrimination via proxy variables (e.g literacy tests for voting eligibility). Lipton, _Zachary distributional robustness Optimizing the predictive accuracy for a whole class of distributions instead of just a single target distribution. Meinshausen, _Nicolai diversity the practice of including the many communities, identities, races, ethnicities, backgrounds, abilities, cultures, and beliefs of the American people, including underserved communities. EO_DEIA_202 1inclusion documentation Collection of documents on a given subject; written or pictorial information describing, defining, specifying, reporting, or certifying activities, requirements, procedures, or results. IEEE_Soft_Vo cab domain Distinct scope, within which common characteristics are exhibited, common rules observed, and over which a distribution transparency is preserved. IEEE_Soft_Vo cab A set of elements, data, resources, and functions that share a commonality in combinations of: (1) roles supported, (2) rules governing their use, and (3) protection needs. SP800-160 <artificial intelligence> specific field of knowledgeor expertise aime_measure ment_2022, citing ISO/IEC 2382 domain expertise Domain expertise implies knowledge and understanding of the essential aspects of a specific field of inquiry. McCue_Collee ndomain shift Differences between the source and target domain data Stacke,_Karin distributional shift drinking your own champagne The practice in which tech workers use their own product consistently to see how well it works and where improvements can be made. kelley_dogfoo ding_2022 dogfooding, eating your own dogfood dynamic process The process in which one or more paths are defined and may be utilized based on the conditions present at the time of execution. IEEE_Guide_I PA edge case a problem or situation, especially in computer programming, that only happens at the highest or lowest end of a range of possible values or in extreme situations: cambridge_dic tionary_2022 embedding An embedding is a representation of a topological object, manifold, graph, field, etc. in a certain space in such a way that its connectivity or algebraic properties are preserved. For example, a field embedding preserves the algebraic structure of plus and times, an embedding of a topological space preserves open sets, and a graph embedding preserves connectivity. One space X is embedded in another space Y when the properties of Y restricted to X are the same as the properties of X. wolfram_math _2022 emulation The use of a data processing system to imitate another data processing system, so that the imitating system accepts the same data, executes the same programs, and achieves the same results as the imitated system. IEEE_Soft_Vo cab end event An activity, task, or output that describes or defines the conclusion of a process. IEEE_Guide_I PA engineer n. 3a: a designer or builder of engines; b: a person who is trained in or follows as a profession a branch of engineering; c: a person who carries through an enterprise by skillful or artful contrivance; 4Ê¼ a person who runs or supervises an engine or an apparatus. v. 1Ê¼ to lay out, construct, or manage as an engineer. \n\nMerriam-Webster_engi neer Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nensemble a machine learning paradigm where multiple models (often called â€œweak learnersâ€) are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models. Joseph_Rocca _Ensemble_m ethods environment Anything affecting a subject system or affected by a subject system through interactions with it, or anything sharing an interpretation of interactions with a subject system IEEE_Soft_Vo cab equality of odds (Equalized odds). We say that a predictor bY satisfies equalized odds with respect to protected attribute A and outcome Y, if bY and A are independent conditional on Y. hardt_equality _2016 The probability of a person in the positive class being correctly assigned a positive outcome and the probability of a person in a negative class being incorrectly assigned a positive outcome should both be the same for the protected and unprotected group members. In other words, the protected and unprotected groups should have equal rates for true positives and false positives. Mehrabi, _Ninareh equality of opportunity (Equal opportunity). We say that a binary predictor bY satisfies equal opportunity with respect to A and Y if Pr{bY = 1 | A = 0; Y = 1} = Pr{bY = 1 | A = 1; Y = 1}. hardt_equality _2016 The probability of a person in positive class being assigned to a positive outcome should be equal for both protected and unprotected group members. In other words, the protected and unprotected groups should have equal true positive rates. Mehrabi, _Ninareh error The difference between the observed value of an index and its â€œtrueâ€ value. Errors maybe random or systematic. Random errors are generally referred to as â€œerrorsâ€. Systematic errors are called â€œbiasesâ€. OECD Difference between a computed, observed, or measured value or condition and the true, specified, or theoretically correct value or condition. IEEE_Soft_Vo cab measured quantity value minus a reference quantity value aime_measure ment_2022, citing ISO/IEC Guide 99 error propagation the way in which uncertainties in the variables affect the uncertainty in the calculated results. Dorf_2018 propgation of uncertainty; proprgation of error ethics definition 1a: \"a set of moral principles : a theory or system of moral values\"; definition 1b: \"the principles of conduct governing an individual or a group\"; definition 1c: \"a consciousness of moral importance\"; definition 1d: \"a guiding philosophy\"; definition 2Ê¼ \"a set of moral issues or aspects (such as rightness)\"; definition 3Ê¼ \"the discipline dealing with what is good and bad and with moral duty and obligation\" Merriam-Webster_ethic n. 1. the branch of philosophy that investigates both the content of moral judgments (i.e., what is right and what is wrong) and their nature (i.e., whether such judgments should be considered objective or subjective). The study of the first type of question is sometimes termed normative ethics and that of the second metaethics. Also called moral philosophy. 2. the principles of morally right conduct accepted by a person or a group or considered appropriate to a specific field. In psychological research, for example, proper ethics requires that participants be treated fairly and without harm and that investigators report results and findings honestly. See code of ethics; professional ethics; research ethics. â€”ethical adj. APA_ethics ethics by design An approach to technology ethics and a key component of responsible innovation that aims to integrate ethics in the design and development stage of the technology. Sometimes formulated as \"embedding values in design.\" Similar terms are \"value-sensitive design\" and \"ethically aligned design.\" AI_Ethics_Mar k_Coeckelberg hevaluation (1) systematic determination of the extent to which an entity meets its specified criteria; (2) action that assesses the value of something aime_measrue ment_2022, citing ISO/IEC 24765 Test, Evaluation, Verification and Validation (TEVV) example definition 1Ê¼ \"one that serves as a pattern to be imitated or not to be imitated\"; definition 3Ê¼ \"one that is representative of all of a group or type\"; definition 4Ê¼ \"a parallel or closely similar case especially when serving as a precedent or model\"; definition 5Ê¼ \"an instance (such as a problem to be solved) serving to illustrate a rule or precept or to act as an exercise in the application of a rule\" Merriam-Webster_exam ple exception An event that occurs during the performance of the process that causes a diversion from the normal flow of the process. Exceptions are generated by an unanticipated event within a process due to an undefined or unknown input, undefined or unexpected outcome, or unforeseen sequencing of a task or event. IEEE_Guide_I PA execute To carry out a plan, a task command, or another instruction SP1011 To carry out an instruction, process, or computer program; directing, managing, performing, and accomplishing the project work, providing the deliverables, and providing work performance information. IEEE_Soft_Vo cab experiment a series of observations conducted under controlled conditions to study a relationship with the purpose of drawing causal inferences about that relationship. An experiment involves the manipulation of an independent variable, the measurement of a dependent variable, and the exposure of various participants to one or more of the conditions being studied. Random selection of participants and their random assignment to conditions also are necessary in experiments. apa_experime nt_2023 A study of a fundamental physical process by the use of one or more computer simulators. Like empirical experiments, input variables (factors) are systematically changed to assess their impact upon simulator outputs (responses). Unlike empirical experiments, the simulator responses are deterministic, and this has implications: Computer experiments can appropriately have their factors with intermediate levels and the scope, especially the number of runs, can be more ambitious. Further, modeling methods based on interpolators (especially kriging) emerge as a viable approach. Good practice is to use Latin hypercubes for computer experiments, and advanced nonparametric modeling methods such as kriging, neural networks, and multivariate adaptive regression splines (MARS) in the data analysis stage. Important applications of computer experimental methods are for determining process optima and for evaluating process tolerances. nist_statistics _2012 expert system A form of AI that attempts to replicate a human's expertise in an area, such as medical diagnosis. It combines a knowledge base with a set of hand-coded rules for applying that knowledge. Machine-learning techniques are increasingly replacing hand coding. Hutson, _Matthew Intelligent computer program that uses knowledge and inference procedures to solve problems that are difficult enough to require significant human expertise for their solution. Reznik,_Leon An expert system is an intelligent computer program that uses knowledge and inference procedures to solve problems that are difficult enough to require significant human expertise for their solution. OECD Computer system that provides for expertly solving problems in a given field or application area by drawing inferences from a knowledge base developed from human expertise. IEEE_Soft_Vo cab A computer system emulating the decision-making ability of a human expert through the use of reasoning, leveraging an encoding of domain-specific knowledge most commonly represented by sets of if-then rules rather than procedural code. The term â€œexpert systemâ€ was used largely during the 1970s and â€™80s amidst great enthusiasm about the power and promise of rule-based systems that relied on a â€œknowledge baseâ€ of domain-specific rules and rule-chaining procedures that map observations to conclusions or recommendations. NSCAI expertise The accumulation of specialized knowledge is often called expertise . Passive \n\nexpertise is a type of knowledge-based specialization that arises from experiences in life and one's position in a society or culture. Formal expertise is the result of a self-selection of a domain of knowledge that is mastered deliberately and for which there are clear benchmarks of success. Schneider_Mc Grew_in_Flan agan_McDono ugh_2018 explainability The ability to provide a human interpretable explanation for a machine learning prediction and produce insights about the causes of decisions, potentially to line up with human reasoning. NISTIR_8269_ Draft Within the context of AI, the extent to which AI decisioning processes and outcomes are reasonably understood. Comptroller_O ffice A characteristic of an AI system in which there is provision of accompanying evidence or reasons for system output in a manner that is meaningful or understandable to individual users (as well as to developers and auditors) and reflects the systemâ€™s process for generating the output (e.g., what alternatives were considered, but not proposed, and why not). NSCAI interpretability explainer Functionality for providing details on or causes for fairness metric results. AI_Fairness_3 60 explanation Systems deliver accompanying evidence or reason(s) for all outputs. NISTIR_8269_ Draft The explanation principle obligates AI systems to supply evidence, support, or reasoning for each output. NISTIR_8312 exploratory Exploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to 1. maximize insight into a data set; 2. uncover underlying structure; 3. extract important variables; 4. detect outliers and anomalies; 5. test underlying assumptions; 6. develop parsimonious models; and 7. determine optimal factor settings. nist_statistics _2012 external validity the extent to which the results of research or testing can be generalized beyond the sample that generated them. The more specialized the sample, the less likely will it be that the results are highly generalizable to other individuals, situations, and time periods. APA_external_ validity facial recognition (FR) Face recognition algorithms, however, have no built-in notion of a particular person. They are not built to identify particular people; instead they include a face detector followed by a feature extraction algorithm that converts one or more images of a person into a vector of values that relate to the identity of the person. The extractor typically consists of a neural network that has been trained on ID-labeled images available to the developer. In operations, they act as generic extractors of identity-related information from photos of persons they have usually never seen before. Recognition proceeds as a differential operator: Algorithms compare two feature vectors and emit a similarity score. This is a vendor-defined numeric value expressing how similar the parent faces are. It is compared to a threshold value to decide whether two samples are from, or represent, the same person or not. Thus, recognition is mediated by persistent identity information stored in a feature vector (or â€œtemplateâ€). NISTIR_8280 fairness metric A quantification of unwanted bias in training data or models. AI_Fairness_3 60 A mathematical definition of â€œfairnessâ€ that is measurable. Some commonly used fairness metrics include: equalized odds predictive parity counterfactual fairness demographic parity Many fairness metrics are mutually exclusive; see incompatibility of fairness metrics. google_glossar y_2023 false negative An example in which the predictive model mistakenly classifies an item as in the negative class. NSCAI an outcome where the model incorrectly predicts the negative class. google_dev_cl assification-true-false-positive-negative A false negative is denying an applicant who should be approved Varshney, _Kush 1. An instance in which a security tool intended to detect a particular threat fails to do so. 2. Incorrectly classifying malicious activity as benign. CSRC_false_n egative Type II error (in statistics) Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nfalse positive An example in which the model mistakenly classifies an item as in the positive class NSCAI an outcome where the model incorrectly predicts the positive class. google_dev_cl assification-true-false-positive-negative A false positive is approving an applicant who should be denied Varshney, _Kush 1. An alert that incorrectly indicates that a vulnerability is present. 2. An alert that incorrectly indicates that malicious activity is occurring. 3. An instance in which a security tool incorrectly classifies benign content as malicious. 4. Incorrectly classifying benign activity as malicious. 5. An erroneous acceptance of the hypothesis that a statistically significant event has been observed. This is also referred to as a type 1 error. This is also referred to as a type 1 error. When â€œhealth-testingâ€ the components of a device, it often refers to a declaration that a component has malfunctioned â€“ based on some statistical test(s) â€“ despite the fact that the component was actually working correctly. CSRC_false_p ositive Type I error (in statistics) fault tolerance The ability of a system or component to continue normal operation despite the presence of hardware or software faults SP1011 favorable label A label whose value corresponds to an outcome that provides an advantage to the recipient. The opposite is an unfavorable label. AI_Fairness_3 60 feature An attribute containing information for predicting the label. AI_Fairness_3 60 feature extraction a more general method in which one tries to develop a transformation of the input space onto the lowdimensional subspace that preserves most of the relevant information khalid_feature _2014 feature importance how important the feature was for the classification performance of the model; a measure of the individual contribution of the corresponding feature for a particular classifier, regardless of the shape (e.g., linear or nonlinear relationship) or direction of the feature effect saarela_featur e_2021 feature shift Unlike joint distribution shift detection, which cannot localize which features caused the shift, we define a new hypothesis test for each feature individually. NaÃ¯vely, the simplest test would be to check if the marginal distributions have changed for each feature (as explored by [25]); however, the marginal distribution would be easy for an adversary to simulate (e.g., by looping the sensor values from a previous day). Thus, marginal tests are not sufficient for our purpose. Therefore, we propose to use conditional distribution tests. More formally, our null and alternative hypothesis for the j-th feature is that its full conditional distribution (i.e., its distribution given all other features) has not shifted for all values of the other features. kulinski_featur e_2020 federated learning An approach to machine learning which addresses problems of data governance and privacy by training algorithms collaboratively without transferring the data to a central location. Each federated device trains on data locally and shares its local model parameters instead of sharing the training data. Different federated learning systems have different topologies that involve different ways of sharing parameters. TTC6_Taxono my_Terminolo gy feedback loop describes the process of leveraging the output of an AI system and corresponding end-user actions in order to retrain and improve models over time. The AI-generated output (predictions or recommendations) are compared against the final decision (for example, to perform work or not) and provides feedback to the model, allowing it to learn from its mistakes. C3. ai_feedback_l oop closed-loop learning fitting Fitting is the process of verifying whether the data item value is in the previously specified interval. OECD firmware Computer programs and data stored in hardware - typically in read-only memory (ROM) or programmable read-only memory (PROM) - such that the programs and data cannot be dynamically written or modified during execution of the programs. SP800-37 Combination of a hardware device and computer instructions or computer data that reside as read only software on the hardware device. IEEE_Soft_Vo cab Forecasting Estimate or prediction of conditions and events in the project's future based on information and knowledge available at the time of the forecast. The information is based on the project's past performance and expected future performance, and includes information that could impact the project in the future, such as estimate at completion and estimate to complete. IEEE_Soft_Vo cab fraud detection Monitoring the behavior of populations of users in order to estimate, detect, or avoid undesirable behavior. Kou,_Yufeng detecting and recognizing fraudulent activities as they enter systems and report them to a system manager. Behdad fully autonomous Accomplishes its assigned mission, within a defined scope, without human intervention while adapting to operational and environmental conditions SP1011 generative adversarial network (GAN) Generative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks. Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset. Brownlee, _Jason A pair of jointly trained neural networks that generates realistic new data and improves through competition. One net creates new examples (fake Picassos, say) as the other tries to detect the fakes. Hutson, _Matthew Generative adversarial networks (GANs) consist of two competing neural networksâ€”a generator network that tries to create fake outputs (such as pictures), and a discriminator network that tries to determine whether the outputs are real or fake. A major advantage of this structure is that GANs can learn from less data than other deep learning algorithms. CRS_AI An approach to training AI models useful for applications like data synthesis, augmentation, and compression where two neural networks are trained in tandem: one is designed to be a generative network (the forger) and the other a discriminative network (the forgery detector). The objective is for each network to train and better itself off the other, reducing the need for big labeled training data. NSCAI global A global explanation produces a model that approximates the non-interpretable model. NISTIR_8312_ Full governance The actions to ensure stakeholder needs, conditions, and options are evaluated to determine balanced, agreed-upon enterprise objectives; setting direction through prioritization and decision-making; and monitoring performance and ompliance against agreed-upon directions and objectives. AI governance may include policies on the nature of AI applications developed and deployed versus those limited or withheld. NSCAI A system of laws, policies, frameworks, practices and processes at international, national and organizational levels. AI governance helps various stakeholders implement, manage, oversee and regulate the development, deployment and use of AI technology. It also helps manage associated risks to ensure AI aligns with stakeholders' objectives, is developed and used responsibly and ethically, and complies with applicable legal and regulatory requirements. IAPP_Governa nce_Terms graph Diagram that represents the variation of a variable in comparison with that of one or more other variables. Diagram or other representation consisting of a finite set of nodes and internode connections called edges or arcs. IEEE_Soft_Vo cab A graph (sometimes called an undirected graph to distinguish it from a directed graph, or a simple graph to distinguish it from a multigraph) is a pair G = (V, E), where V is a set whose elements are called vertices (singular: vertex), and E is a set of paired vertices, whose elements are called edges (sometimes links or lines). wikipedia_gra ph_2023 graphical processing unit (GPU) A specialized chip capable of highly parallel processing. GPUs are well-suited for running machine learning and deep learning algorithms. GPUs were first developed for efficient parallel processing of arrays of values used in computer graphics. Modern-day GPUs are designed to be optimized for machine learning. NSCAI ground truth information provided by direct observation as opposed to information provided by inference Collins_Dictio nary_ground_ truth value of the target variable for a particular item of labelledinput data aime_measure ment_2022, citing ISO/IEC 22989 group fairness The goal of groups defined by protected attributes receiving similar treatments or outcomes. AI_Fairness_3 60 hacker Unauthorized user who attempts to or gains access to an information system. Reznik,_Leon Technically sophisticated computer enthusiast who uses his or her knowledge and means to gain unauthorized access to protected resources. IEEE_Soft_Vo cab hardware Physical equipment used to process, store, or transmit computer programs or data IEEE_Soft_Vo cab harm An undesired outcome [whose] cost exceeds some threshold[; ...] the key points in the definition of safety are that: costs have to be sufficiently high in some human sense for events to be harmful, and that safety involves reducing both the probability of expected harms and the possibility of unexpected harms. Engineering_s afety_in_mach ine_learning harmful bias Harmful bias can be either conscious or unconscious. Unconscious, also known as implicit bias, involves associations outside conscious awareness that lead to a negative evaluation of a person on the basis of characteristics such as race, gender, sexual orientation, or physical ability. Discrimination is behavior; discriminatory actions perpetrated by individuals or institutions refer to inequitable treatment of members of certain social groups that results in social advantages or disadvantages humphrey_add ressing_2020 human-assisted The type of human-robot-interaction that that refers to situations during which human interactions are needed at the level of detail of task plans, i.e., during the execution of a task SP1011 human-computer interaction (HCI) methods and approaches for designing and architecting user interfaces and the interactions between humans and computer (or information) technology. Poore_Lawren ce_ARLIS_202 3-01 human-cognitive bias Human-cognitive biases relate to how an individual or group perceives AI system information to make a decision or fill in missing information, or how humans think about purposes and functions of an AI system. Human biases are omnipresent in decision-making processes across the AI lifecycle and system use, including the design, implementation, operation, and maintenance of AI. NIST_AI_RMF _1.0 Systematic error in judgment and decision-making common to all human beings which can be due to cognitive limitations, motivational factors, and/or adaptations to natural environments. human-enabled machine learning Detection, correlation, and pattern recognition generated through machine-based observation of human operation of software systems capturing successful or unsuccessful operations to enable the creation of a useful predictive analytics capability. IEEE_Guide_I PA human-in-the-loop An AI system that requires human interaction. DOD_Modelin g_and_Simula tion_Glossary human-machine teaming (HMT) The ability of humans and AI systems to work together to undertake complex, evolving tasks in a variety of environments with seamless handoff both ways between human and AI team members. Areas of effort include developing effective policies for controlling human and machine initiatives, computing methods that ideally complement people, methods that optimize goals of teamwork, and designs that enhance human-AI interaction. NSCAI methods and approaches for coordinating the functions and actions of (semi) autonomous machine capabilities and human users, which are granted equal weighting. Poore_Lawren ce_ARLIS_202 3-01 human-AI teaming human-operator-intervention The need for human interaction in a normally fully autonomous behavior due to some extenuating circumstances. SP1011 Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nhuman subjects a living individual about whom an investigator (whether professional or student) conducting research: (i) Obtains information or biospecimens through intervention or interaction with the individual, and uses, studies, or analyzes the information or biospecimens; or (ii) Obtains, uses, studies, analyzes, or generates identifiable private information or identifiable biospecimens. 45_CFR_46_2 018_Requirem ents_ (2018_Commo n_Rule) participant human system integration (HSI) methods and approaches for testing and optimizing all human-related considerations from a â€œwhole-systemâ€ or â€œsystem-of-systemsâ€ level. Poore_Lawren ce_ARLIS_202 3-01 hyperparameters the parameters that are used to either configure a ML model (e.g., the penalty parameter C in a support vector machine, and the learning rate to train a neural network) or to specify the algorithm used to minimize the loss function (e.g., the activation function and optimizer types in a neural network, and the kernel type in a support vector machine). On_Hyperpara meter_Optimi zation hypothesis testing A term used generally to refer to testing significance when specific alternatives to the null hypothesis are considered. OECD impact assessment a risk management tool that seeks to ensure an organization has sufficiently considered a system's relative benefits and costs before implementation. In the context of AI, an impact assessment helps to answer a simple question: alongside this systemâ€™s intended use, for whom could it fail? Bipartisan_Poli cy_Center_im pact_assessme nts An evaluation process designed to identify, understand, document and mitigate the potential ethical, legal, economic and societal implications of an AI system in a specific use case. IAPP_Governa nce_Terms impersonation A malicious individual is able to impersonate a legitimate data subject to the data controller. The adversary forges a valid access request and goes through the identity verification enforced by the data controller. The data controller sends to the adversary the data of a legitimate data subject. Defeating impersonation is the primary objective of any authentication protocol. The result of this attack is a data breach (e.g. blaggers [sic] pretend to be someone they are not in order to wheedle out the information they are seeking obtaining information illegaly which they then sell for a specified price). Security_Analy sis_of_Subject _Access in-processing Techniques that modify the algorithms in order to mitigate bias during model training. Model training processes could incorporate changes to the objective (cost) function or impose a new optimization constraint. SP1270 Techniques that try to modify and change state-of-the-art learning algorithms to remove discrimination during the model training process. Mehrabi, _Ninareh in-processing algorithm A bias mitigation algorithm that is applied to a model during its training. AI_Fairness_3 60 incident a situation in which AI systems caused, or nearly caused, real-world harm. AI_Incident_D atabase the occurrence of a technical event that affects the integrity of a Product and/or Model. FBPML_Wiki an alleged harm or near harm event to people, property, or the environment where an AI system is implicated. AI_Incident_E ditors Adverse event(s) in a computer system or networks caused by a failure of a security mechanism, or an attempted or threatened breach of these mechanisms. Hasan,_Raza incident response a public official response to an incident ... from an entity (i.e. company, organization, individual) allegedly responsible for developing or deploying the AI or AI system involved in said incident. AIID_incident _response independence Of software quality assurance (SQA), situation in which SQA is free from technical, managerial, and financial influences, intentional or unintentional IEEE_Soft_Vo cab Two events are independent if the occurrence of one event does not affect the chances of the occurrence of the other event. The mathematical formulation of the independence of events A and B is the probability of the occurrence of both A and B being equal to the product of the probabilities of A and B (i.e., P(A and B) = P(A)P(B)) nist_800_2010 In simple terms, inclusion is getting the mix to work together. individual fairness The goal of similar individuals receiving similar treatments or outcomes. AI_Fairness_3 60 Give similar predictions to similar individuals Mehrabi, _Ninareh A fairness metricthat checks whether similar individuals are classified similarly aime_measure ment_2022 citing Machine Learning Glossary by Google inference The stage of ML in which a model is applied to a task. For example, a classifier model produces the classification of a test sample. NISTIR_8269_ Draft information input component One of the three components of a model. This component delivers assumptions and data to the model. Comptroller_O ffice information security preservation of confidentiality, integrity and availability of information; in addition, other properties, such as authenticity, accountability, non-repudiation, and reliability can also be involved. ISO/IEC_TS_ 5723Ê¼2022(en) input Data received from an external source IEEE_Soft_Vo cab insider attack Those who are within [an] organisation may have authorised access to vast amounts of sensitive company records that are essential for maintaining competitiveness and market position, and knowledge of information services and procedures that are crucial for daily operations. . . .[and] should an individual choose to act against the organisation, then with their privileged access and their extensive knowledge, they are well positioned to cause serious damage. IEEE_Caught_ in_the_Act in silico carrying out some experiment by means of a computer simulation World_Wide_ Words_In_sili co computer simulation testing instance Discrete, bounded thing with an intrinsic, immutable, and unique identity. Individual occurrence of a type IEEE_Soft_Vo cab A single object of the world from which a model will be learned, or on which a model will be used (e.g., for prediction). Kohavi,_Ron instance weight A numerical value that multiplies the contribution of a data point in a model. AI_Fairness_3 60 integrity Degree to which a system, product, or component prevents unauthorized access to, or modification of, computer programs or data. IEEE_Soft_Vo cab Guarding against improper information modification or destruction, and includes ensuring information non-repudiation and authenticity. CSRC The property whereby information, an information system, or a component of a system has not been modified or destroyed in an unauthorized manner. CISA <data> property whereby data have not been altered in an unauthorized manner since they were created, transmitted, or stored; <systems> property of accuracy and completeness ISO/IEC_TS_ 5723Ê¼2022(en) the quality of moral consistency, honesty, and truthfulness with oneself and others. APA_integrity intelligent process automation A preconfigured software instance that combines business rules, experience-based context determination logic, and decision criteria to initiate and execute multiple interrelated human and automated processes in a dynamic context. The goal is to complete the execution of a combination of processes, activities, and tasks in one or more unrelated software systems that deliver a result or service with minimal or no human intervention. IEEE_Guide_I PA interaction Action that takes place with the participation of the environment of the object. IEEE_Soft_Vo cab internal validity the degree to which a study or experiment is free from flaws in its internal structure and its results can therefore be taken to represent the true nature of the phenomenon. In other words, internal validity pertains to the soundness of results obtained within the controlled conditions of a particular study, specifically with respect to whether one can draw reasonable conclusions about cause-and-effect relationships among variables. APA_internal_ validity interoperability The ability of software or hardware systems or components to operate together successfully with minimal effort by end user SP1011 Degree to which two or more systems, products or components can exchange information and use the information that has been exchanged. IEEE_Soft_Vo cab The ability for tools to work together in execution, communication, and data exchange under specific conditions. NIST_1500 interpretability The ability to understand the value and accuracy of system output. Interpretability refers to the extent to which a cause and effect can be observed within a system or to which what is going to happen given a change in input or algorithmic parameters can be predicted. NSCAI The ability to explain or to present an ML modelâ€™s reasoning in understandable terms to a human aime_measure ment_2022, citing Machine Learning Glossary by Google explainability interpretable model An interpretable machine learning model obeys a domain-specific set of constraints to allow it (or its predictions, or the data) to be more easily understood by humans. These constraints can differ dramatically depending on the domain. rudin_interpre table_2022 intervenability the property that intervention is possible concerning all ongoing or planned privacy relevant data processing[; ...] the data subjects themselves should be able to intervene with regards to the processing of their own data ... [to ensure] that data subjects have the ability to control how their data is processed and by whom. Covert_et_al knowledge The sum of all information derived from diagnostic, descriptive, predictive, and prescriptive analytics embedded in or available to or from a cognitive computing system. IEEE_Guide_I PA <artificial intelligence> abstracted informationabout objects, events, concepts or rules, their relationships and properties, organizedfor goal-oriented systematic use aime_measure ment_2022, citinig ISO/IEC 22989 label A value corresponding to an outcome. AI_Fairness_3 60 target variable assigned to a sample aime_measure ment_2022, citing ISO/IEC 22989 label shift Under label shift, the label distribution p(y) might change but the class-conditional distributions p(x|y) do not. ... We work with the label shift assumption, i.e., ps(x|y) = pt(x|y) saurabh_label _2020 large language model (LLM) a class of language models that use deep-learning algorithms and are trained on extremely large textual datasets that can be multiple terabytes in size. LLMs can be classed into two types: generative or discriminatory. Generative LLMs are models that output text, such as the answer to a question or even writing an essay on a specific topic. They are typically unsupervised or semi-supervised learning models that predict what the response is for a given task. Discriminatory LLMs are supervised learning models that usually focus on classifying text, such as determining whether a text was made by a human or AI. AI_Assurance_ 2022 language model language model A language model is an approximative description that captures patterns and regularities present in natural language and is used for making assumptions on previously unseen language fragments. Gustavii,_Ebba large language model (LLM) learning A procedure in artificial intelligence by which an artificial intelligence program improves its performance by gaining knowledge. Dennis_Merca dal the acquisition of novel information, behaviors, or abilities after practice, observation, or other experiences, as evidenced by change in behavior, knowledge, or brain function. Learning involves consciously or nonconsciously attending to relevant aspects of incoming information, mentally organizing the information into a coherent cognitive representation, and integrating it with relevant existing knowledge activated from long-term memory. APA_learning Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nleast privilege The principle that a security architecture should be designed so that each entity is granted the minimum system resources and authorizations that the entity needs to perform its function. CSRC The security objective of granting users only those accesses they need to perform their official duties. SP-800-12 lemmatization the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Artasanchez_J oshi_AI_with_ Python in natural language processing[, ...] working with words according to their root lexical components Techopedia_le mmatization grouping together words with the same root or lemma but with different inflections or derivatives of meaning so they can be analyzed as one item. Techslang_lem matization linear model [a supervised learning algorithm that uses] a simple formula to find a best-fit line through a set of data points. dataiku_ML_a nd_linear_mo dels (linear) An operator L^~ is said to be linear if, for every pair of functions f and g and scalar t, L^~(f+g)=L^~f+L^~g and L^~(tf)=tL^~f. wolfram_math world_2022 local A local explanation explains a subset of decisions or is a per-decision explanation. NISTIR_8312_ Full localization Creation of a national or specific regional version of a product. IEEE_Soft_Vo cab logistic model (logistic equation) The continuous version of the logistic model is described by the differential equation (dN)/(dt)=(rN(K-N))/K, (1) where r is the Malthusian parameter (rate of maximum population growth) and K is the so-called carrying capacity (i.e., the maximum sustainable population). Dividing both sides by K and defining x=N/K then gives the differential equation (dx)/(dt)=rx(1-x), (2) which is known as the logistic equation and has solution x(t)=1/(1+(1/(x_0)-1)e^(-rt)). (3) The function x(t) is sometimes known as the sigmoid function. wolfram_math world_2022 machine learning A branch of Artificial Intelligence (AI) that focuses on the development of systems capable of learning from data to perform a task without being explicitly programmed to perform that task. Learning refers to the process of optimizing model parameters through computational techniques such that the model's behaviour is optimized for the training task. TTC6_Taxono my_Terminolo gy A subcategory of artificial intelligence; a method of designing a sequence of actions to solve a problem that optimizes automatically through experience and with limited or no human intervention. Comptroller_O ffice machine observation Machine detection and interpretation of relevant and meaningful events and conditions that impact operation of the computer system itself or other dependent mechanisms or processes essential to the purpose of the system. IEEE_Guide_I PA malware Hardware, firmware, or software that is intentionally included or inserted in a system for a harmful purpose. Reznik,_Leon Software that compromises the operation of a system by performing an unauthorized function or process. CISA trojan horse materiality Refers to the significance of a matter in relation to a set of financial or performance information. If a matter is material to the set of information, then it is likely to be of significance to a user of that information OECD McNamara fallacy presum[ing] that (A) quantitative models of reality are always more accurate than other models; (B) the quantitative measurements that can be made most easily must be the most relevant; and (C) factors other than those currently being used in quantitative metrics must either not exist or not have a significant influence on success. Also known as the quantitative fallacy. \n\nMcNamara_Fal lacy quantitative fallacy measurement (Quantitative) (1) act or process of assigning a number or category to an entity to describe an attribute of that entity; (2) assignment of numbers to objects in a systematic way to represent properties of the object; (3) use of a metric to assign a value (e.g., a number or category) from a scale to an attribute of an entity; (4) set of operations having the object of determining a value of a measure; (5) assignment of values and labels to aspects of software engineering work products, processes, and resources plus the models that are derived from them, whether these models are developed using statistical or other techniques; (6) figure, extent, or amount obtained by measuring aime_measure ment_2022, citing ISO/IEC 24765 (Qualitative) (1) a way of learning about social reality [...][that uses] approaches [...] to explore, describe, or explain social phenomen[a]; unpack the meaning people ascribe to activities, situations, events, or [artifacts]; build a depth of understanding about some aspect of social life; build \"thick descriptions\" (see Clifford Geertz, 1973) of people in naturalistic settings; explore new or underresearched areas; or make micro-macro links (illuminate connections between individuals-groups and institutional and/or cultural contexts). (2) [approaches that] can make visible and unpick the mechanisms which link particular variables, by looking at the explanations, or accounts, provided by those involved. Leavy_OHQR_ Intro Qualitative measurement engages research methods and techniques to provide information about the nature of phenomenon. Qualitative methods are designed for systematic collection, organization, description and interpretation of non-numeric (textual, verbal or visual) data (Hammarberg et. al, 2016). Qualitative measurement generally answers questions about why, for whom, when, and how something is (or is not) observed, whereas quantitative measurement answers questions about what is observed. Elements assessed using qualitative measurement may include contextual norms or meaning, socio-cultural dynamics, individual or collective beliefs, and complex multi-component interactions or interventions (Busetto et. al, 2020). Hammarberg_ 2016_Busetto_ 2020 Documentation of assumptions and methods used is a foundational element of qualitative measurement, as the choice of single or combined methods is made based on the phenomenon and its context (Russell & Gregory, 2003). When appropriately paired, qualitative and quantitative measurement can provide corroboration or elaboration, demonstrate use cases, and/or identify conditions for complementarity or contradiction (Brannen, 2005). Russell_2003_ Brannen_2005 measurement method generic description of a logical organization of operations used in a measurement aime_measure ment_2022, citing ISO/IEC Guide 99 logical sequence of operations, described generically, usedin quantifying an attribute with respect to a specified scale aime_measure ment_2022, citing ISO/IEC 24765 measurement model The initial confirmatory factory analysis (CFA) model that underlies the structural model [that] tests the adequacy (as indexed by model fit) of the specified relations whereby indicators are linked to their underlying construct. Little_2013 A statistical model that links unobservable theoretical constructs, operationalized as latent variables, and observable propertiesâ€”i.e., data about the world jackman_oxfor d_2008 measurability ability to assess an attribute of an entity against a metric (note 1Ê¼ \"measurable\" is the adjective form of \"measurability\") ISO/IEC_TS_ 5723Ê¼2022(en) membership inference given a machine learning model and a record, determining whether the record was used as part of the model's training dataset or not. metadata Metadata is data that defines and describes other data. OECD Data that describe other data. IEEE_Soft_Vo cab Data employed to annotate other data with descriptive information, possibly including their data descriptions, data about data ownership, access paths, access rights, and data volatility. metric defined measurement method and measurement scale ISO/IEC_TS_ 5723Ê¼2022(en) (1) quantitative measure of the degree to which a system, component, or process possesses a given attribute; (2) defined measurement method and the measurement scale; c.f., measure in this section above aime_measure ment_2022, citing ISO/IEC 24765 minimization (Part of the ICO framework for auditing AI) AI systems generally require large amounts of data. However, organisations must comply with the minimisation principle under data protection law if using personal data. This means ensuring that any personal data is adequate, relevant and limited to what is necessary for the purposes for which it is processed. [â€¦] The default approach of data scientists in designing and building AI systems will not necessarily take into account any data minimisation constraints. Organisations must therefore have in place risk management practices to ensure that data minimisation requirements, and all relevant minimisation techniques, are fully considered from the design phase, or, if AI systems are bought or operated by third parties, as part of the procurement process due diligence ICO_data_min imisation a data controller should limit the collection of personal information to what is directly relevant and necessary to accomplish a specified purpose. They should also retain the data only for as long as is necessary to fulfil that purpose. In other words, data controllers should collect only the personal data they really need, and should keep it only for as long as they need it. The data minimisation principle is expressed in Article 5(1)(c) of the GDPR and Article 4(1)(c) of Regulation (EU) 2018/1725, which provide that personal data must be \"adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed\". EDPS_data_mi nimization mixed methods In mixed methods, the researcher collects and analyzes both qualitative and quantitative data rigorously in response to research questions and hypotheses; integrates the two forms of data and their results; organizes these procedures into specific research designs that provide the logic and procedures for conducting the study; and frames these procedures within theory and philosophy. Creswell_Clark _mixed_meth ods research in which the inquirer or investigator collects and analyzes data, integrates the findings, and draws inferences using both qualitative and quantitative approaches or methods in a single study or a program of study. Lisa_M. _Given_SAGE MLOPS MLOps (machine learning operations) stands for the collection of techniques and tools for the deployment of ML models in production. symeonidis_M LOps_2022 model A function that takes features as input and predicts labels as output. AI_Fairness_3 60 A model is a formalised expression of a theory or the causal situation which is regarded as having generated observed data. In statistical analysis the model is generally expressed in symbols, that is to say in a mathematical form, but diagrammatic models are also found. The word has recently become very popular and possibly somewhat over-worked. OECD A core component of an AI system used to make inferences from inputs in order to produce outputs. A model characterizes an input-to-output transformation intended to perform a core computational task of the AI system (e.g., classifying an image, predicting the next word for a sequence, or selecting a robot's next action given its state and goals). TTC6_Taxono my_Terminolo gy A quantitative method, system, or approach that applies statistical, economic, financial, or mathematical theories, techniques, and assumptions to process input data into quantitative estimates. A model consists of three components: an information input component, which delivers assumptions and data to the model; a processing component, which transforms inputs into estimates; and a reporting component, which translates the estimates into useful business information. Comptroller_O ffice model assertion Model assertions are arbitrary functions over a modelâ€™s input and output that indicate when errors may be occurring Kang,_Daniel model card short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. [They] also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. Model_Cards_ for_Model_Re porting A brief document that discloses information about an AI model, like explanations about intended use, performance metrics and benchmarked evaluation in various conditions, such as across different cultures, demographics or race. IAPP_Governa nce_Terms model debugging Model debugging aims to diagnose a modelâ€™s failures. Jain_Saachi model decay Model decay depicts that the performance of the model is degrading over time Nayak,_Pragati model editing An area of research that aims to enable fast, data-efficient updates to a pre-trained base modelâ€™s behavior for only a small region of the domain, without damaging model performance on other inputs of interest Mitchell,_Eric model extraction Adversaries maliciously exploiting the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface. Chandrasekara n,_Varun model inversion; model stealing model governance Model Governance is the name for the overall internal framework of a firm or organization that controls the processes for Model Development, Model Validation and Model Usage, assign responsibilities and roles etc. open_risk_202 2model inventory in the context of Risk Management, [...] a database/[management information system] developed for the purpose of aggregating quantitative model related information that is in use by a firm or organization. ORM_model_i nventory model overlay Judgmental or qualitative adjustments to model inputs or outputs to compensate for model, data, or other known limitations. A model overlay is a type of override. Comptroller_O ffice model risk management model risk management encompasses governance and control mechanisms such as board and senior management oversight, policies and procedures, controls and compliance, and an appropriate incentive and organizational structure Fed_Reserve model suite A group of models that work together. Comptroller_O ffice Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nmodel training the phase in the data science development lifecycle where practitioners try to fit the best combination of weights and bias to a machine learning algorithm to minimize a loss function over the prediction range C3. ai_Model_Trai ning process to determine or to improve the parameters of a machine learning model, based on a machine learning algorithm, by using training data aime_measure ment_2022, citing ISO/IEC 22989 model validation the set of processes and activities intended to verify that models are performing as expected. yields. io_model_vali dation the set of principles, practices and organizational arrangements supporting a rigorous (audited) model development and validation cycle. Open_Risk_M anual_model_ validation monitoring Examination of the status of the activities of a supplier and of their results by the acquirer or a third party. IEEE_Soft_Vo cab Continual checking, supervising, critically observing or determining the status in order to identify change from the performance level required or expected. SP800-160 moral agency The capacity for moral action, reasoning, judgment, and decision making, as opposed to merely having moral consequences. AI_Ethics_Mar k_Coeckelberg hmoral patiency The moral standing of an entity in the sense of how that entity should be treated. AI_Ethics_Mar k_Coeckelberg hnaive Bayes The naive Bayes classifier is a Bayesian learning method that has been found to be useful in many practical applications. It is called \"naive\" because it incorporates the simplifying assumption that attribute values are conditionally independent, given the classification of the instance. The naive Bayes classifier applies to learning tasks where each instance x is described by a conjunction of attribute values and where the target function f(x) can take on any value from some finite set V. Mitchell,_Tom natural language processing The field concerned with machines capable of processing, analysing, and generating human language, either spoken, written or signed. TTC6_Taxono my_Terminolo gy neural network A model that, taking inspiration from the brain, is composed of layers (at least one of which is hidden) consisting of simple connected units or neurons followed by nonlinearities aime_measure ment_2022, citing Machine Learnign Glossary by Google nondiscrimination the practice of treating people, companies, countries, etc. in the same way as others in order to be fair: Cambridge Dictionary In the context of machine learning non-discrimination can be defined as follows: (1) people that are similar in terms non-protected characteristics should receive similar predictions, and (2) differences in predictions across groups of people can only be as large as justified by non-protected characteristics. Å½liobaitÄ—_Indr Ä—the practice of treating people, companies, countries, etc. in the same way as others in order to be fair Cambridge_Di ctionary_non-discrimination normal flow The intended flow of a process originating from a start event, continuing through all defined activities, and concluding successfully to its defined end event. IEEE_Guide_I PA normalization Conceptual procedure in database design that removes redundancy in a complex database by establishing dependencies and relationships between database entities. Normalization reduces storage requirements and avoids database inconsistencies. OECD The process of convertingan actual range of values into a standard range of values, typically âˆ’1to +1 or 0 to 1 aime_measure ment_2022, citing Machine Learning Glossary by Google objective evidence data supporting the existence or verity of something (note: can be obtained through observation, measurement, test, or other means). ISO/IEC_TS_ 5723Ê¼2022(en) observation a piece of information received online from users, sensors, or other knowledge sources poole_mackwo rth_observatio nthe careful, close examination of an object, process, or other phenomenon for the purpose of collecting data about it or drawing conclusions. APA_observati on offline learning implies ... a static dataset that [one] know[s] from the start and the parameters of [one's] machine learning algorithm are adjusted to the whole dataset at once often loading the whole dataset into memory or in batches. Ben_Auffarth_ 2021 online learning fitting [one's] model incrementally as the data flows in (streaming data). Ben_Auffarth_ 2021 ontology A set of concepts and categories in a subject area or knowledge domain that shows their properties and the relationships among them to enable interoperability among disparate elements and systems and specify interfaces to independent, knowledge-based services for the purpose of enabling certain kinds of automated reasoning. IEEE_Guide_I PA opacity The nature of some AI techniques whereby the inferential operations are complex, hidden, or otherwise opaque to their developers and end users in terms of providing an understanding of how classifications, recommendations, or actions are generated and what overall performance will be. NSCAI A description of some deep learning systems [that] take an input and provide an output, but the calculations that occur in between are not easy for humans to interpret. Hutson, _Matthew When one or more features of an AI system, such as processes, the provenance of datasets, functions, output or behaviour are unavailable or incomprehensible to all stakeholders â€“ usually an antonym for transparency. TTC6_Taxono my_Terminolo gy black box; unexplainable operationalization Putting AI systems or related concepts into use so they can be measured. operator A role assumed by the person performing remote control or teleoperation, semi-autonomous operations, or other human-in-the-loop types of operations SP1011 Individual or organization that performs the operations of a system. IEEE_Soft_Vo cab Individual or organization that performs the operations of a system. SP800-160 opt-in an individual makes an active affirmative indication of choice via a user interface signaling a desire to share their information with third parties. IAPP_Privacy_ Glossary privacy; consent; opt-out opt-out an individual makes an active affirmative indication of choice via a user interface signaling a desire not to share their information with third parties. IAPP_Privacy_ Glossary privacy; consent; opt-in outcome something that follows as a result or consequence merriam_webs ter_outcome outlier An outlier is a data point that is far from other points. Russell_and_N orvig An outlier is a data value that lies in the tail of the statistical distribution of a set of data values. OECD Values distant from mostother values. In machine learning, any of the following are outliers:â€¢ Weights with high absolute valuesâ€¢ Predicted values relatively far away from the actual valuesâ€¢ Input data whose values are more than roughly 3 standard deviations fromthe meanOutliers often cause problems in model training. Clipping is one way of managingoutliers aime_measure ment_2022 citing Machine Learning Glossary by Google output Data transmitted to an external destination IEEE_Soft_Vo cab Process by which an information processing system, or any of its parts, transfers data outside of that system or part IEEE_Soft_Vo cab overfitting Given a hypothesis space H, a hypothesis h element of H is said to overfit the training data if there exists some alternative hypothesis h' element of H, such that h has smaller error than h' over the training examples, but h' has a smaller error than h over the entire distribution of instance. Mitchell,_Tom package a folder with all the code and metadata needed to train and serve a machine learning model. about_ML_pa ckages parametric A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) Russell_and_N orvig parent process A process that may contain one or more sub-processes, activities, and tasks. IEEE_Guide_I PA parity Bit(s) used to determine whether a block of data has been altered. Rationale: Term has been replaced by the term â€œparity bitâ€. NIST_CSRC_p arity the quality or state of being equal or equivalent Merriam-Webster_parit yparticipation engag[ing] multiple stakeholders in deliberative processes in order to achieve consensus. Sloane_et_al_ 2020 participant A computer system, data, input, business rule, human intervention, and other contributor to the flow of a process. IEEE_Guide_I PA a living individual about whom an investigator (whether professional or student) conducting research: (i) Obtains information or biospecimens through intervention or interaction with the individual, and uses, studies, or analyzes the information or biospecimens; or (ii) Obtains, uses, studies, analyzes, or generates identifiable private information or identifiable biospecimens. 45_CFR_46_2 018_Requirem ents_ (2018_Commo n_Rule) human subject passive learning agent A passive learning agent has a fixed policy that determines its behavior. An active learning agent gets to decide what actions to take. Russell_and_N orvig active learning agent Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\npersonal data â€˜Personal dataâ€™ means any information relating to an identified or identifiable natural person (â€˜data subjectâ€™); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. GDPR (1) â€œPersonal informationâ€ means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular consumer or household. Personal information includes, but is not limited to, the following if it identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular consumer or household: (A) Identifiers such as a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, social security number, driverâ€™s license number, passport number, or other similar identifiers. (B) Any personal information described in subdivision (e) of Section 1798.80. (C) Characteristics of protected classifications under California or federal law. (D) Commercial information, including records of personal property, products or services purchased, obtained, or considered, or other purchasing or consuming histories or tendencies. (E) Biometric information. (F) Internet or other electronic network activity information, including, but not limited to, browsing history, search history, and information regarding a consumerâ€™s interaction with an internet website application, or advertisement. (G) Geolocation data. (H) Audio, electronic, visual, thermal, olfactory, or similar information. (I) Professional or employment-related information. (J) Education information, defined as information that is not publicly available personally identifiable information as defined in the Family Educational Rights and Privacy Act (20 U.S.C. Sec. 1232g; 34 C.F.R. Part 99). (K) Inferences drawn from any of the information identified in this subdivision to create a profile about a consumer reflecting the consumerâ€™s preferences, characteristics, psychological trends, predispositions, behavior, attitudes, intelligence, abilities, and aptitudes. (L) Sensitive personal information. (2) â€œPersonal informationâ€ does not include publicly available information or lawfully obtained, truthful information that is a matter of public concern. For purposes of this paragraph, â€œpublicly availableâ€ means: information that is lawfully made available from federal, state, or local government records, or information that a business has a reasonable basis to believe is lawfully made available to the general public by the consumer or from widely distributed media; or information made available by a person to whom the consumer has disclosed the information if the consumer has not restricted the information to a specific audience. â€œPublicly availableâ€ does not mean biometric information collected by a business about a consumer without the consumerâ€™s knowledge. (3) â€œPersonal informationâ€ does not include consumer information that is deidentified or aggregate consumer information. CCPA policy The general principles by which a government is guided in its management of public affairs, or the legislature in its measures. This term, as applied to a law, ordinance, or rule of law, denotes its general purpose or tendency considered as directed to the POLICY law_policy_20 23 A policy defines the learning agentâ€™s way of behaving at a given time sutton_reinfor cement_2018 positionality the researcher's starting points and standpoints before and during inquiry, as well as the conditions shaping the research situation, process, and product. Charmaz_Hen wood reflexivity post-hoc explanation Post-hoc explainability targets models that are not readily interpretable by design by resorting to diverse means to enhance their interpretability, such as text explanations, visual explanations, local explanations, explanations by example, explanations by simplification and feature relevance explanations techniques. Each of these techniques covers one of the most common ways humans explain systems and processes by themselves. NISTIR_8312_ Full Post-hoc explainability targets models that are not readily inter- pretable by design by resorting to diverse means to enhance their in- terpretability, such as text explanations, visual explanations, local expla- nations, explanations by example, explanations by simplification and feature relevance explanations techniques. Each of these techniques covers one of the most common ways humans explain systems and processes by themselves. barredo_explai nable_2020 post-processing Typically performed with the help of a holdout dataset (data not used in the training of the model). Here, the learned model is treated as a black box and its predictions are altered by a function during the post-processing phase. The function is deduced from the performance of the black box model on the holdout dataset. SP1270 Performed after training by accessing a holdout set that was not involved during the training of the model. If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-processing can be used in which the labels assigned by the black-box model initially get reassigned based on a function during the post-processing phase. Mehrabi, _Ninareh Steps performed after a machine learning model has been run to adjust its output. This can include adjusting a model's outputs or using a holdout dataset â€” data not used in the training of the model â€” to create a function run on the model's predictions to improve fairness or meet business requirements. IAPP_Governa nce_Terms post-processing algorithm A bias mitigation algorithm that is applied to predicted labels. AI_Fairness_3 60 practical significance a conceptual framework for evaluating discrimination cases developed primarily on statistical evidence that is the subject of increasing interest and discussion by some in the equal employment opportunity (EEO) field. DOL_Practical _Significance statistical significance (often paired in contrast to this); substantive significance (synonym) pre-processing algorithm A bias mitigation algorithm that is applied to training data. AI_Fairness_3 60 precision A metric for classification models. Precision identifies the frequency with which a model was correct when classifying the positive class. NSCAI closeness of agreement between indications or measuredquantity values obtained by replicate measurements on the same or similarobjects under specified conditions aime_measure ment_2022, citing ISO/IEC Guide 99 A metric for classification models.Precision identifies the frequency with which a model was correct when predictingthe positive class. That is:Precision = True Positive/(True Positive + False Positive) aime_measure ment_2022, citing Machine Learning Glossary by Google Closeness of agreement between independent test results obtained under prescribed conditions. It is generally dependent on analyte concentration, and this dependence should be determined and documented. The measure of precision is usually expressed in terms of imprecision and computed as a standard deviation of the test results. Higher imprecision is reflected by a larger standard deviation. Independent test results means results obtained in a manner not influenced by any previous results on the same or similar material. Precision covers repeatability and reproducibility [19]. Alternatively, precision is a measure for the reproducibility of measurements within a set, that is, of the scatter or dispersion of a set about its central value. Precision depends only on the distribution of random errors and does not relate to the true value or specified value. UNODC_Gloss ary_QA_GLP prediction Forecasting quantitative or qualitative outputs through function approximation, applied on input data or measurements. NSCAI primary output of an AI system when provided with input data or information aime_measure ment_2022, citing ISO/IEC 22989 predictive analysis The organization of analyses of structured and unstructured data for inference and correlation that provides a useful predictive capability to new circumstances or data. IEEE_Guide_I PA predictive analytics Insights, reporting, and information answering the question, \"What is likely to happen?\" Predictive analytics support high confidence foretelling of future event (s). IEEE_Guide_I PA preprocessing Transforming the data so that the underlying discrimination is mitigated. This method can be used if a modeling pipeline is allowed to modify the training data. SP1270 prescriptive analytics Insights, reporting, and information answering the question, â€œWhat should I do about it?\" Prescriptive analytics determines information that provides high confidence actions necessary to recover from an event or fulfill a need. IEEE_Guide_I PA privacy freedom from intrusion into the private life or affairs of an individual ISO/IEC_TS_ 5723Ê¼2022(en) freedom from intrusion into the private lifeor affairs of an individual when that intrusion results from undue or illegalgathering and use of data about that individual aime_measure ment_2022, citing ISO/IEC TR 24029-1 privacy-by-design Embedding privacy measures and privacy enhancing technologies directly into the design of information technologies and systems. ENISA data-protection-by-design (def: https:/ /eur-lex.europa. eu/legal-content/EN/T XT/? uri=CELEX% 3A02016R0679 -20160504&qid= 1532348683434 )privacy-enhancing technology A coherent system of ICT (Information and Communications Technology) measures that protects privacy by eliminating or reducing personal data or by preventing unnecessary and/or undesired processing of personal data, all without losing the functionality of the information system. PET_Handboo kprivileged protected attribute A value of a protected attribute indicating a group that has historically been at systematic advantage. AI_Fairness_3 60 procedure Information item that presents an ordered series of steps to perform a process, activity, or task. IEEE_Soft_Vo cab process A sequence or flow of activities in an organization with the objective of carrying out work, which may include a set of activities, events, tasks, and decisions in a sequenced flow that adhere to finite execution semantics. Process levels will generally follow structure at the capability maturity model integration (CMMI) level. IEEE_Guide_I PA Set of interrelated or interacting activities that transforms inputs into outputs IEEE_Soft_Vo cab process flow The defined representation of the overall progression of how a process is intended to be performed, including all exceptions. IEEE_Guide_I PA Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nprocessing â€˜Processingâ€™ means any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction. GDPR â€œProcessingâ€ means any operation or set of operations that are performed on personal information or on sets of personal information, whether or not by automated means. CCPA personal data; processing processing environment the combination of software and hardware on which the Application runs. Law_Insider_p rocessing_envi ronment processor â€˜Processorâ€™ means a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller. GDPR â€œProcessingâ€ means any operation or set of operations that are performed on personal information or on sets of personal information, whether or not by automated means. CCPA personal data; processing; controller product manager a specialized product management professional whose job is to manage the planning, development, launch, and success of products/solutions powered by AI, machine learning, and deep learning technologies. productmanag erHQ_Josh_Fe chter product owner [person who is] focused on providing direction and prioritization for the cross-functional AI team, ensuring everyone remains focused on the overall vision and road map. This role is responsible for unifying individuals with diverse skills and backgrounds toward a common goal. Forbes_Tracy _Kemp productization [turning the best performing model] into an actual \"data product,\" ready to be used in live services. Towards_Prod uctizing profiling â€˜Profilingâ€™ means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person's performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements. GDPR â€œProfilingâ€ means any form of automated processing of personal information, as further defined by regulations pursuant to paragraph (16) of subdivision (a) of Section 1798.185, to evaluate certain personal aspects relating to a natural person and in particular to analyze or predict aspects concerning that natural personâ€™s performance at work, economic situation, health, personal preferences, interests, reliability, behavior, location, or movements. CCPA Measuring the characteristics of expected activity so that changes to it can be more easily identified. CSRC personal data; processing protected attribute An attribute that partitions a population into groups whose outcomes should have parity. Examples include race, gender, caste, and religion. Protected attributes are not universal, but are application specific. AI_Fairness_3 60 protected class [a feature] that may not be used as the basis for decisions [and] could be chosen because of legal mandates or because of organizational values. Some common protected [classes] include race, religion, national origin, gender, marital status, age, and socioeconomic status. MIT_Protecte d_Attributes A group of people with a common characteristic who are legally protected from [...] discrimination on the basis of that characteristic. Protected classes are created by both federal and state law. Practical_Law _protected_cl ass prototype A prototype is an original model constructed to include all the technical characteristics and performances of the new product. OECD provisioning The granting of access rights and executional privilege to an agent (human or machine) within an application(s) or system(s). IEEE_Guide_I PA proxy A variable that can stand in for another, usually not directly observable or measurable, variable. SP1270 pseudo-anonymization (pseudonymization) â€˜Pseudonymisationâ€™ means the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person; GDPR â€œPseudonymizeâ€ or â€œPseudonymizationâ€ means the processing of personal information in a manner that renders the personal information no longer attributable to a specific consumer without the use of additional information, provided that the additional information is kept separately and is subject to technical and organizational measures to ensure that the personal information is not attributed to an identified or identifiable consumer. CCPA A data management technique to strip identifiers linking data to an individual. NSCAI personal data; processing quality The totality of features and characteristics of a product or service that bear on its ability to satisfy stated or implied needs. OECD <data> degree to which the characteristics of data satisfy stated and implied needs when used under specified conditions; <system> degree to which a set of inherent characteristics of an object fulfils requirements (an object can be a product, process or service) ISO/IEC_TS_ 5723Ê¼2022(en) racialized A socio-political process by which groups are ascribed a racial identity, whether or not members of the group self-identify as such AAAS_AI_and _Bias_2022-09 ranking a type of machine learning that sorts data in a relevant order[; often used by companies] to optimize search and recommendations. DEV_ranking position, order, or standing within a group : RANK Merriam-Webster_ranki ng recall A metric for classification models; identifies the frequency with which a model correctly classifies the true positive items. NSCAI A metric for classification modelsthat answers the following question: Out of all the possible positive labels,how many did the model correctly identify? That is: Recall = True Positive/(True Positive + false Negative) aime_measure ment_2022, citing Machine Learning Glossary by Google recognition the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. Pattern_Recog nition_and_M achine_Learni ng a sense of awareness and familiarity experienced when one encounters people, events, or objects that have been encountered before or when one comes upon material that has been learned in the past. APA_recogniti on to transfer prior learning or past experience to current consciousness: that is, to retrieve and reproduce information; to remember. APA_recall recommendation system A software tool and techniques that provide suggestion based on the customer's taste to discover new appropriate thing for them by filtering personalized information based on the user's preferences from a large volume of information Das,_Debashis A subclass of information filtering system that seek to predict â€˜ratingâ€™ or â€˜preferenceâ€™ that a user would give to an item (such as music, books or movies) or social element (e.g. people or group) they had not yet considered, using a model built from the characteristics of an item (content based approaches) or the userâ€™s social environment (collaborative filtering approaches) Sharma,_Lalita rectification An individualâ€™s right to have personal data about them corrected or amended by a business or other organization if it is inaccurate. IAPP_Privacy_ Glossary red-team A group of people authorized and organized to emulate a potential adversaryâ€™s attack or exploitation capabilities against an enterpriseâ€™s security posture. The Red Teamâ€™s objective is to improve enterprise cybersecurity by demonstrating the impacts of successful attacks and by demonstrating what works for the defenders (i.e., the Blue Team) in an operational environment. Also known as Cyber Red Team. CSRC reference class A class which is intended to describe structure and behavior of object identifiers. Its instances, called references, are passed by-value and indirectly represent objects by substituting for some primitive reference. IGI_Global_ref erence_class reflexivity A form of critical thinking that prompts us to consider the â€˜whysâ€™ and â€˜howsâ€™ of research, critically questioning the utility, ethics, and value of what, whom, and how we study Jamieson_Gov aart_Pownall in qualitative research, the self-referential quality of a study in which the researcher reflects on the assumptions behind the study and especially the influence of his or her own motives, history, and biases on its conduct. APA_reflexivity positionality regression Regression is a process of predicting the value to a yes or no label provided it falls on a continuous spectrum of input values, subcategory of supervised learning. Ranschaert, _Erik the prediction of an exact value using a given set of data Saleh_Alkhalifa _ML_in_Biote ch reinforcement learning A method of training algorithms to make suitable actions by maximizing rewarded behavior over the course of its actions. This type of learning can take place in simulated environments, such as game-playing, which reduces the need for real-world data. NSCAI Reinforcement learning (RL) is a subset of machine learning that allows an artificial system (sometimes referred to as an agent) in a given environment to optimize its behaviour. Agents learn from feedback signals received as a result of their actions, such as rewards or punishments, with the aim of maximizing the received reward. Such signals are computed based on a given reward function, which constitutes an abstract representation of the system's goal. The goal could be, for example, to earn a high video game score or to minimize idle worker time in a factory TTC6_Taxono my_Terminolo gy reliability Reliability refers to the closeness of the initial estimated value(s) to the subsequent estimated values. OECD ability of an item to perform as required, without failure, for a given time interval, under given conditions. Note 1 to <system> definition: The time interval duration can be expressed in units appropriate to the item concerned (e.g. calendar time, operating cycles, distance run, etc.) and the units should always be clearly stated. Note 2 to <system> definition: Given conditions include aspects that affect reliability, such as: mode of operation, stress levels, environmental conditions, and maintenance. ISO/IEC_TS_ 5723Ê¼2022(en) property of consistentintended behaviour and results aime_measure ment_2022, citing ISO/IEC 22989 remediation The process of treating data by cleaning, organizing, and migrating it to a safe and secure environment for optimized usage is called data remediation. Generally [understood] as a process involving deleting unnecessary or unused data. However, the actual process . . . is very detailed and includes several steps, including replacing, updating, or modifying data along with cleaning it, organizing it, and getting rid of unnecessary data. \n\nCPO_Magazin e_Amar_Kana garaj reproducibility Closeness of the agreement between the results of measurements of the same measurand carried out under changed conditions of measurement. IEEE_Soft_Vo cab requirement something essential to the existence or occurrence of something else : CONDITION Merriam-Webster_requi rement residual Residuals are differences between the one-step-predicted output from the model and the measured output from the validation data set. Thus, residuals represent the portion of the validation data not explained by the model. MathWorks_R esidual resilience The ability to prepare for and adapt to changing conditions and withstand and recover rapidly from disruptions. Resilience includes the ability to withstand and recover from deliberate attacks, accidents, or naturally occurring threats or incidents. The ability of a system to adapt to and recover from adverse conditions. NISTIR_8269_ Draft <governance> ability to anticipate and adapt to, resist, or quickly recover from a potentially disruptive event, whether natural or man-made; <system> capability of a system to maintain its functions and structure in the face of internal and external change, and to degrade gracefully when this is necessary ISO/IEC_TS_ 5723Ê¼2022(en) ability of a system to recover operational conditionquickly following an incident aime_measure ment_2022, citing ISO/IEC 22989 responsible AI An AI system that aligns development and behavior to goals and values. This includes developing and fielding AI technology in a manner that is consistent with democratic values. NSCAI result The consequential outcome of completing a process. IEEE_Guide_I PA retention limit refers to the amount of information that is stored long-term, and can be measured in volume (the size of the total collected logs in bytes) and time (the number of months or years that logs are stored for). Industrial_Net work_Security _2011 Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nrisk The composite measure of an eventâ€™s probability of occurring and the magnitude or degree of the consequences of the corresponding event. The impacts, or consequences, of AI systems can be positive, negative, or both and can result in opportunities or threats (Adapted from: iso 31000Ê¼2018 ) NIST_AI_RMF _1.0 A measure of the extent to which an entity is threatened by a potential circumstance or event, and typically a function of: (i) the adverse impacts that would arise if the circumstance or event occurs; and (ii) the likelihood of occurrence. SP800-12 An uncertain event or condition that, if it occurs, has a positive or negative effect on a project's objectives IEEE_Soft_Vo cab effect of uncertainty on objectives ISO_IEC_3850 7risk control mechanisms at the design, implementation, and evaluation stages [that can be taken] into consideration when developing responsible AI for organizations that includes security risks (cyber intrusion risks, privacy risks, and open source software risk), economic risks (e.g., job displacement risks), and performance risks (e.g., risk of errors and bias and risk of black box, and risk of explainability). Toward_an_u nderstanding_ of_responsible _artificial_inte lligence_practi ces risk tolerance Risk tolerance refers to the organizationâ€™s or AI actorâ€™s ... readiness to bear the risk in order to achieve its objectives. Risk tolerance can be influenced by legal or regulatoryrequirements. NIST_AI_RMF _1.0 robotic desktop automation (RDA) The computer application that makes available to a human operator a suite of predefined activity choreography to complete the execution of processes, activities, transactions, and tasks in one or more unrelated software systems to deliver a result or service in the course of human-initiated or -managed workflow. IEEE_Guide_I PA robotic process automation (RPA) A preconfigured software instance that uses business rules and predefined activity choreography to complete the autonomous execution of a combination of processes, activities, transactions, and tasks in one or more unrelated software systems to deliver a result or service with human exception management. IEEE_Guide_I PA Software to help in the automation of tasks, especially those that are tedious and repetitive. NSCAI robust AI An AI system that is resilient in real-world settings, such as an object-recognition application that is robust to significant changes in lighting. The phrase also refers to resilience when it comes to adversarial attacks on AI components. NSCAI robustness ability of a system to maintain its level of performance under a variety of circumstances ISO/IEC_TS_ 5723Ê¼2022(en) The ability of a machine learning model/algorithm to maintain correct and reliable performance under different conditions (e.g., unseen, noisy, or adversarially manipulated data). NISTIR_8269_ Draft root-mean-square deviation (RMSD) of an estimator of a parameter[; ...] the square-root of the mean squared error (MSE) of the estimator. In symbols, if X is an estimator of the parameter t, then RMSE(X) = ( E( (Xâˆ’t)2 ) )Â½. The RMSE of an estimator is a measure of the expected error of the estimator. The units of RMSE are the same as the units of the estimator. Glossary_of_S tatistical_Ter ms a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed Wikipedia_RM SD root-mean-square error (RMSE) row describes a single entity or observation and the columns describe properties about that entity or observation. The more rows you have, the more examples from the problem domain that you have. Machine_Lear ning_Mastery _Jason_Brown lee safety property of a system such that it does not, under defined conditions, lead to a state in which human life, health, property, or the environment is endangered; [safety involves reducing both the probability of expected harms and the possibility of unexpected harms]. ISO/IEC_TS_ 5723Ê¼2022(en) freedom from risk which is not tolerable aime_measure ment_2022, citinig ISO/IEC TR 24029-1 scalability The ability to increase or decrease the computational resources required to execute a varying volume of tasks, processes, or services. IEEE_Guide_I PA score A continuous value output from a classifier. Applying a threshold to a score results in a predicted label. AI_Fairness_3 60 screen out Screen-out discrimination occurs when â€œa disability prevents a job applicant or employee from meetingâ€”or lowers their performance onâ€”a selection criterion, and the applicant or employee loses a job opportunity as a result.â€ EEOC_ADA_AI security resistance to intentional, unauthorized act(s) designed to cause harm or damage to a system ISO/IEC_TS_ 5723Ê¼2022(en) degree to which a product or system (3.38)protects information (3.20) and data (3.11) so that persons or other productsor systems have the degree of data access appropriate to their types and levelsof authorization aime_measure ment_2022, citing ISO/IEC TR 24029-1 segmentation The process of identifying homogeneous subgroups within a data table. Raynor self-aware system A computing platform imbued with sufficient knowledge and analytic capability to make useful conclusions about its inputs, its own processing, and the use of its output so that it is capable of self- judgment and improvement consistent with its purpose. IEEE_Guide_I PA self-diagnosis Ability of a system to adequately take measurement information from sensors, validate the data, and communicate the processes and results to other devices SP1011 self-healing system A computing system able to perceive that it is not operating correctly and, without human intervention, make the necessary adjustments to restore itself to normalcy. IEEE_Guide_I PA semantic mapping A strategic schema or framework of metadata labels applied to all data, data groups, data fields, data types, or data content used to introduce new or raw data into a corpus or data fabric to give machine learning algorithms direction for investigating known or potential relationships between data. A semantic map provides a structure for the introduction of new data, information, or knowledge IEEE_Guide_I PA sensitivity analysis A â€œwhat-ifâ€ type of analysis to determine the sensitivity of the outcomes to changes in parameters. If a small change in a parameter results in relatively large changes in the outcomes, the outcomes are said to be sensitive to that parameter. OECD sensory digitization The conversion of typically analog or human sensory perception (e.g., vision, speech) to a digital format useful for machine-to-human interaction or machine processing of traditionally analog sensory information [e.g., optical character recognition (OCR)]. IEEE_Guide_I PA service A collection of coordinated processes that takes one or more kinds of input, performs a value-added transformation, and creates an output that fulfills the needs of a customer [or shareholder]. IEEE_Guide_I PA signal detection theory a framework for interpreting data from experiments in which accuracy is measured. Signal_Detecti on_Theory shallow learning Techniques that separate the process of feature extraction from learning itself. Reznik,_Leon situational awareness Perception of elements in the system and/or environment and a comprehension of their meaning, which could include a projection of the future status of perceived elements and the uncertainty associated with that status. SP800-160 socio-technical system how humans interact with technology within the broader societal context NIST SP1270 system that includes a combination of technical and human or natural elements ISO/IEC_TS_ 5723Ê¼2022(en) software testing Activity in which a system or component is executed under specified conditions, the results are observed or recorded, and an evaluation is made of some aspect of the system or component. IEEE_Soft_Vo cab sparsity refers to a matrix of numbers that includes many zeros or values that will not significantly impact a calculation. Dave_Salvator _sparsity specification A document that specifies, in a complete, precise, verifiable manner, the requirements, design, behavior, or other characteristics of a system or component and often the procedures for determining whether these provisions have been satisfied. SP800-37 stakeholder Individual or organization having a right, share, claim, or interest in a system or in its possession of characteristics that meet their needs and expectations. An individual, group, or organization who may affect, be affected by, or perceive itself to be affected by a decision, activity, or outcome of a project. IEEE_Soft_Vo cab any individual, group, or organization that can affect, be affected by, or perceive itself to be affected by a decision or activity ISO/IEC_TS_ 5723Ê¼2022(en) standard deviation The most widely used measure of dispersion of a frequency distribution introduced by K. Pearson (1893). It is equal to the positive square root of the variance. The standard deviation should not be confused with the root mean square deviation. OECD start event An activity, task, or input that describes or defines the beginning of a process. IEEE_Guide_I PA statistical bias A systematic tendency for estimates or measurements to be above or below their true values. Statistical biases arise from systematic as opposed to random error. Statistical bias can occur in the absence of prejudice, partiality, or discriminatory intent. SP1270 statistical parity The independence between the protected attribute and the outcome of the decision rule Besse, _Philippe statistical significance When the probability of obtaining a statistic of a given size due strictly to random sampling error, or chance, is less than the selected alpha level [or the probability of a type I error]; also represents a rejection of the null hypothesis. Statistics_in_P lain_English refers to whether a relationship between two or more variables exists beyond a probability expected by chance The_SAGE_En cyclopedia_of _Communicati on_Research_ Methods statistics Numerical data relating to an aggregate of individuals; the science of collecting, analysing and interpreting such data OECD stereotype a set of cognitive generalizations (e.g., beliefs, expectations) about the qualities and characteristics of the members of a group or social category. Stereotypes, like schemas, simplify and expedite perceptions and judgments, but they are often exaggerated, negative rather than positive, and resistant to revision even when perceivers encounter individuals with qualities that are not congruent with the stereotype. APA_stereotyp eContemporary social psychology typically defines stereotypes as mental representations of a group and its members, and stereotyping as the cognitive activity of treating individual elements in terms of higher level categorial properties Augoustinos_ Walker_1998 stochastic The adjective â€œstochasticâ€ implies the presence of a random variable; e.g. stochastic variation is variation in which at least one of the elements is a variate and a stochastic process is one wherein the system incorporates an element of randomness as opposed to a deterministic system. OECD straight-through processing (STP) The successful execution of a service, process, or transaction performed entirely through traditional application platforms with predefined interfaces (i.e., application programming interfaces [APIs]). IEEE_Guide_I PA Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nstrawperson a fallacious argument which irrelevantly attacks a position that appears similar to, but is actually different from, an opponent's position, and concludes that the opponent's real position has thereby been refuted. Hughes_Laver y_Critical_Thi nking stress test Type of performance efficiency testing conducted to evaluate a test item's behavior under conditions of loading above anticipated or specified capacity requirements, or of resource availability below minimum specified requirements IEEE_Soft_Vo cab structured data Data that has a predefined data model or is organized in a predefined way. NIST_1500 sub-process A subordinate process that can be included within a parent process. It can be present and/or repeated within other parent processes. IEEE_Guide_I PA supervised learning A type of machine learning in which the algorithm compares its outputs with the correct outputs during training. In unsupervised learning, the algorithm merely looks for patterns in a set of data. Hutson, _Matthew Algorithms, which develop a mathematical model from the input data and known desired outputs. Reznik,_Leon For a computer to process a set of data whose attributes have been divided into two groups and derive a relationship between the values of one and the values of the other. These two groups are sometimes called predictor and targets, respectively. In statistical terminology, they are called independent and dependent variables. Respectively. The learning Is \"supervised because the distinction between the predictors and the target variables is chosen by the investigator or some other outside agency. Raynor a general subset of machine learning in which data, like its associated labels, is used to train models that can learn or generalize from the data to make predictions, preferably with a high degree of certainty. Saleh_Alkhalifa _ML_in_Biote ch support vector machines A supervised machine learning model for data classification and regression analysis. One of the most used classifiers in machine learning. It optimizes the width of the gap between the points of separate categories in feature space. Ranschaert, _Erik system combination of interacting elements organized to achieve one or more stated purposes ISO/IEC_TS_ 5723Ê¼2022(en) systemic bias Systemic biases result from procedures and practices of particular institutions that operate in ways which result in certain social groups being advantaged or favored and others being disadvantaged or devalued. This need not be the result of any conscious prejudice or discrimination but rather of the majority following existing rules or norms. D. Chandler and R. Munday, A Dictionary of Media and Communicatio n. Oxford University Press, Jan. 2011, publication Title: A Dictionary of Media and Communicatio nsystem of systems set of systems and system elements that interact to provide a unique capability that none of the constituent systems can accomplish on its own (note: can be necessary to facilitate interaction of the constituent systems in the system of systems) ISO/IEC_TS_ 5723Ê¼2022(en) target a method for solving a problem that an AI algorithm parses its training data to find. Once an algorithm finds its target function, that function can be used to predict results (predictive analysis). The function can then be used to find output data related to inputs for real problems where, unlike training sets, outputs are not included. TechTarget_ta rget_function target variable, target value task The performance of a discrete activity with a defined start, stop, and outcome that cannot be broken down to a finer level of detail. IEEE_Guide_I PA Required, recommended, or permissible action, intended to contribute to the achievement of one or more outcomes of a process IEEE_Soft_Vo cab set of activities undertaken in order to achieve a specific goal aime_measure ment_2022, citing ISO/IEC TR 24030 taxonomy Taxonomy refers to classification according to presumed natural relationships among types and their subtypes. OECD technical control Security controls (i.e., safeguards or countermeasures) for an information system that are primarily implemented and executed by the information system through mechanisms contained in the hardware, software, or firmware components of the system. NIST_SP_800-30_Rev_1 technochauvinism The belief that technology is always the solution M. Broussard, Artificial Unintelligence: How Computers Misunderstand the World. MIT Press, 2018. techno-solutionism test Technical operation to determine one or more characteristics of or to evaluate the performance of a given product, material, equipment, organism, physical phenomenon, process or service according to a specified procedure .UNODC_Gloss ary_QA_GLP any activity aimed at evaluating an attribute or capability of a program or system and deteermining that it meets its required results. William_Hetze l(1) activity in which a system or component is executedunder specified conditions, the results are observed or recorded, and an evaluationis made of some aspect of the system or component; (2) to conduct anactivity as in (1); (3) set of one or more test cases and procedures. aime_measure ment_2022, citing ISO/IEC 24765 the process of executing a program with the intent of finding errors. The_Art_of_S oftware_Testi ng Test, Evaluation, Verification and Validation (TEVV) Test and Evaluation, Verification and Validation (TEVV) A framework for assessing, incorporating methods and metrics to determine that a technology or system satisfactorily meets its design specifications and requirements, and that it is sufficient for its intended use. NSCAI_Report third party an entity that is involved in some way in an interaction that is primarily between two other entities. [Please see note, especially regarding NIST CSRC terms that we might incorporate into this definition.] TechTarget_th ird_party three lines of defense Most financial institutions follow a three-lines-of-defense model, which separates front line groups, which are generally accountable for business risks (the First Line), from other risk oversight and independent challenge groups (the Second Line) and assurance (the Third Line) AIRS_Penn traceability Ability to trace the history, application or location of an entity by means of recorded identification. [\"Chain of custody\" is a related term.] Alternatively, traceability is a property of the result of a measurement or the value of a standard whereby it can be related with a stated uncertainty, to stated references, usually national or international standards, i.e. through an unbroken chain of comparisons. In this context, The standards referred to here are measurement standards rather than written standards. UNODC_Gloss ary_QA_GLP A characteristic of an AI system enabling a person to understand the technology, development processes, and operational capabilities (e.g., with transparent and auditable methodologies along with documented data sources and design procedures). NSCAI training data A dataset from which a model is learned. AI_Fairness_3 60 samples for training used to fit a machine learningmodel aime_measure ment_2022, citing ISO/IEC 22989 transaction Enactment of a process represented by a set of coordinated activities carried out by multiple systems and/or participants in accordance with defined relationships. This coordination leads to an intentional, consistent, and verifiable result across all participants. IEEE_Guide_I PA transfer learning A technique in machine learning in which an algorithm learns to perform one task, such as recognizing cars, and builds on that knowledge when learning a different but related task, such as recognizing cats. Hutson, _Matthew transformer A procedure that modifies a dataset. AI_Fairness_3 60 transparency <information> open, comprehensive, accessible, clear and understandable presentation of information; <systems> property of a system or process to imply openness and accountability ISO/IEC_TS_ 5723Ê¼2022(en) Understanding the working logic of the model. NISTIR_8269_ Draft <organization> property of an organization that appropriate activities and decisions are communicated to relevant stakeholders (3.5.13) in a comprehensive, accessible and understandable manner Note 1 to entry: Inappropriate communication of activities and decisions can violate security, privacy or confidentiality requirements. iso_22989_20 22 <system> property of a system that appropriate information about the system is made available to relevant stakeholders (3.5.13) Note 1 to entry: Appropriate information for system transparency can include aspects such as features, performance, limitations, components, procedures, measures, design goals, design choices and assumptions, data sources and labelling protocols. Note 2 to entry: Inappropriate disclosure of some aspects of a system can violate security, privacy or confidentiality requirements. iso_22989_20 22 true negative outcome where the model correctly predicts the negative class. google_dev_cl assification-true-false-positive-negative true positive an outcome where the model correctly predicts the positive class. google_dev_cl assification-true-false-positive-negative trust the system status in the mind of human beings based on their perception of and experience with the system; concerns the attitude that a person or technology will help achieve specific goals in a situation characterized by uncertainty and vulnerability. DOD_TEVV degree to which a user or other stakeholder has confidence that a product or system will behave as intended aime_measure ment_2022, citing ISO/IEC TR 24029-1 trustworthiness The degree to which an information system (including the information technology components that are used to build the system) can be expected to preserve the confidentiality, integrity, and availability of the information being processed, stored, or transmitted by the system across the full range of threats and individualsâ€™ privacy. SP800-37 Worthy of being trusted to fulfill whatever critical requirements may be needed for a particular component, subsystem, system, network, application, mission, enterprise, or other entity. SP800-160 ability to meet stakeholders' expectations in a verifiable way; an attribute that can be applied to services, products, technology, data and information as well as to organizations. ISO/IEC_TS_ 5723Ê¼2022(en) Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\ntrustworthy AI Characteristics of trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. NIST_AI_RMF _1.0 Trustworthy AI has three components: (1) it should be lawful, ensuring compliance with all applicable laws and regulations (2) it should be ethical, demonstrating respect for, and ensure adherence to, ethical principles and values and (3) it should be robust, both from a technical and social perspective, since, even with good intentions, AI systems can cause unintentional harm. Trustworthy AI concerns not only the trustworthiness of the AI system itself but also comprises the trustworthiness of all processes and actors that are part of the systemâ€™s life cycle. european_ethi cs_2019 Trustworthy AI has three components: (1) it should be lawful, ensuring compliance with all applicable laws and regulations (2) it should be ethical, demonstrating respect for, and ensure adherence to, ethical principles and values and (3) it should be robust, both from a technical and social perspective, since, even with good intentions, AI systems can cause unintentional harm. Characteristics of Trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. Trustworthy AI concerns not only the trustworthiness of the AI system itself but also comprises the trustworthiness of all processes and actors that are part of the AI systemâ€™s life cycle. Trustworthy AI is based on respect for human rights and democratic values. \n\n> TTC6_Taxonom y_Terminology\n\ntype I error The null hypothesis H0 is rejected, even though it is [true] berthold_guid e_2020 false positive rate james_statistic al_2014 type II error The null hypothesis H0 is accepted, even though it is [false] berthold_guid e_2020 true positive rate james_statistic al_2014 uncertainty Result of not having accurate or sufficient knowledge of a situation; state, even partial, of deficiency of information related to understanding or knowledge of an event, its consequence, or likelihood IEEE_Soft_Vo cab underfitting Underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data. Ranschaert, _Erik underrepresentation inadequately represented. (See note.) Merriam-Webster_unde rrepresented when members of discernible groups are not consistently present in representative bodies and among measures of well-being in numbers roughly proportionate to their numbers within the population. Encyclopedia. com_underrep resentation unexplainable impossibility of providing an explanation for certain decisions made by an intelligent system which is both 100% accurate and comprehensible. Roman_V. _Yampolskiy_ Unexplainabilit yblack box; opacity unstructured data Data that does not have a predefined data model or is not organized in a predefined way unsupervised learning A learning strategy that consists in observing and analyzing different entities and determining that some of their subsets can be grouped into certain classes, without any correctness test being performed on acquired knowledge through feedback from external knowledge sources. Note 1 to entry: Once a concept is formed, it is given a name that may be used in subsequent learning of other concepts. iso_2382_1997 usability extent to which a system product or service can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use (note 1Ê¼ The â€œspecifiedâ€ users, goals and context of use refer to the particular combination of users, goals and context of use for which usability is being considered; note 2Ê¼ used as a qualifier to refer to the design knowledge, competencies, activities and design attributes that contribute to usability, such as usability expertise, usability professional, usability engineering, usability method, usability evaluation, usability heuristic). [See also: ISO/IEC 9241-11 Ergonomic of Human-System Interaction â€” Part 11Ê¼ Usability: Definitions and Concepts. ISO, Geneva, Switzerland, 2018, https:/ /www.iso. org/standard/63500.html.] ISO/IEC_TS_ 5723Ê¼2022(en) usability testing refers to evaluating a product or service by testing it with representative users. Typically, during a test, participants will try to complete typical tasks while observers watch, listen and takes notes. The goal is to identify any usability problems, collect qualitative and quantitative data and determine the participant's satisfaction with the product. Usabilitygov user individual or group that interacts with a system or benefits from a system during its utilization IEEE_Soft_Vo cab A person, organization, or other entity which requests access to and uses the resources of a computer system or network. CSRC user-centered design the practice of the following principles, the active involvement of users for a clear understanding of user and task requirements, iterative design and evaluation, and a multi-disciplinary approach Vredenburg, _Karel Approach to system design and development that aims to make interactive systems more usable by focusing on the use of the system; applying human factors, ergonomics and usability knowledge and techniques. IEEE_Soft_Vo cab validation Confirmation by examination and provision of objective evidence that the particular requirements for a specific intended use are fulfilled. UNODC_Gloss ary_QA_GLP Confirmation, through the provision of objective evidence, that the requirements for a specific intended use or application have been fulfilled. IEEE_Soft_Vo cab provides objective evidence that the capability provided by the system complies with stakeholder performance requirements, achieving its use in its intended operational environment; answers the question, \"Is it the right solution to the problem?\" [C]onsists of evaluating the operational effectiveness, operational suitability, sustainability, and survivability of the system or system elements under operationally realistic conditions. DOD_TEVV A continuous monitoring of the process of compilation and of the results of this process. OECD Test and Evaluation, Verification, and Validation (TEVV) value sensitive design a theoretically grounded approach to the design of technology that accounts for human values in a principled and systematic manner throughout the design process. Friedman_et_ al_2017 variable A variable is a characteristic of a unit being observed that may assume more than one of a set of values to which a numerical measure or a category from a classification can be assigned. OECD Quantity or data item whose value can change IEEE_Soft_Vo cab variance The variance is the mean square deviation of the variable around the average value. It reflects the dispersion of the empirical values around its mean. OECD A quantifiable deviation, departure, or divergence away from a known baseline or expected value IEEE_Soft_Vo cab verifiable can be checked for correctness by a person or tool ISO/IEC_TS_ 5723Ê¼2022(en) provides evidence that the system or system element performs its intended functions and meets all performance requirements listed in the system performance specification and functional and allocated baselines; answers the question, \"Did you build the system correctly?\" DOD_TEVV Test and Evaluation, Verification and Validation (TEVV) word embedding a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. . . . A word embedding, trained on word co-occurrence in text corpora, represents each word (or common phrase) w as a d-dimensional word vector w~ 2 Rd. It serves as a dictionary of sorts for computer programs that would like to use word meaning. First, words with similar semantic meanings tend to have vectors that are close together. Second, the vector differences between words in embeddings have been shown to represent relationships between words. Bolukbasi_et_ al_Debiasing_ Word_Embedd ings ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nGDPR Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) https:/ /eur-lex.europa.eu/eli/reg/2016/679/oj CCPA California Consumer Privacy Act of 2018 https:/ /leginfo.legislature.ca.gov/faces/codes_displayText.xhtml?division=3.&part=4.&lawCode=CIV&title=1.81.5 AI_Incident_Database What is an AI incident? AI Incident Database AI Incident Database 2022 https:/ /incidentdatabase.ai/research/1-criteria Shubendhu_and_Vijay Applicability of Artificial Intelligence in Different Fields of Life Shubhendu, Shukla S. and Jaiswal Vijay International Journal of Scientific Engineering and Research (IJSER) 1 1 28-35 2013 https:/ /www.ijser.in/archives/v1i1/MDExMzA5MTU=.pdf Raynor Glossary of Computer System Software Development Terminology Raynor, William J., Jr. The International Dictionary of Artificial Intelligence 1999 https:/ /archive.org/details/internationaldic0000rayn AI_Fairness_360 Glossary AI Fairness 360 AI Fairness 360 https:/ /aif360.mybluemix.net/resources#glossary Mitchell,_Tom Machine Learning Mitchell, Tom Machine Learning 1997 http:/ /www.cs.cmu.edu/~tom/mlbook.html Brookings_Institution The Brookings glossary of AI and emerging technologies Allen, John R. and Darrell M. West Brookings Institution 2021 https:/ /www.brookings.edu/blog/techtank/2020/07/13/the-brookings-glossary-of-ai-and-emerging-technologies/ Brownlee,_Jason A Gentle Introduction to Generative Adversarial Networks (GANs) Brownlee, Jason Machine Learning Mastery 2019 https:/ /machinelearningmastery.com/what-are-generative-adversarial-networks-gans/ Pyle_and_San_JosÃ© An executiveâ€™s guide to machine learning Pyle, Dorian and Cristina San JosÃ© McKinsey Quarterly 2015 https:/ /www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/an-executives-guide-to-machine-learning Hutson,_Matthew AI Glossary: Artificial intelligence, in so many words Hutson, Matthew Science 357 6346 19 2017 https:/ /www.science.org/doi/10.1126/science.357.6346.19 FBPML_Wiki Definitions Foundation for Best Practices in Machine Learning FBPML Wiki https:/ /wiki.fbpml.org/wiki/Definitions IAPP_Privacy_Glossary Glossary of Privacy Terms https:/ /iapp.org/resources/glossary/ IAPP_Governance_Terms Glossary of Governance Terms Reznik,_Leon Introduction I.5 Glossary of Basic Terms Reznik, Leon Intelligent Security Systems: How Artificial Intelligence, Machine Learning and Data Science Work for and Against Computer Security xv-xxiv 2022 IEEE_Guide_IPA IEEE Guide for Terms and Concepts in Intelligent Process Automation IEEE Standards Association IEEE Guide for Terms and Concepts in Intelligent Process Automation Russell_and_Norvig Stuart Russell and Peter Norvig Artificial Intelligence: A Modern Approach (Fourth Edition) 2021 SP1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence Schwartz, Reva; Apostol Vassilev; Kristen Greene; Lori Perine; Andrew Burt; Patrick Hall NIST Special Publication 1270 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf SP1011 Autonomy Levels for Unmanned Systems (ALFUS) Framework Autonomy Levels for Unmanned Systems Working Group Participants NIST Special Publication 1011 2008 https:/ /www.nist.gov/system/files/documents/el/isd/ks/NISTSP_1011-I-2-0.pdf Gartner Gartner Glossary Gartner Group https:/ /www.gartner.com/en/glossary/all-terms Varshney,_Kush Trustworthy Machine Learning Varshney, Kush R. Munir,_Arslan Artificial Intelligence and Data Fusion at the Edge Munir, Arslan, Erik Blasch, Jisu Kwon, Joonho Kong, and Alexander Aved IEEE A&E SYSTEMS MAGAZINE 36 7 62-78 2021 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9475883 Wallace,_Brian Introduction to Artificial Intelligence for Security Professionals Wallace, Brian; Sepehr Akhavan-Masouleh; Andrew Davis; Mike Wojnowicz; John H. Brock 2017 http:/ /book.itep.ru/depository/AI/IntroductionToArtificialIntelligenceForSecurityProfessionals_Cylance.pdf NSCAI National Security Commission on Artificial Intelligence: The Final Report National Security Commission on Artificial Intelligence National Security Commission on Artificial Intelligence Final Report 2021 https:/ /www.nscai.gov/2021-final-report/ OECD Glossary of Statistical Terms Organisation for Economic Co-operation and Development 2007 https:/ /ec.europa.eu/eurostat/ramon/coded_files/OECD_glossary_stat_terms.pdf / https:/ /stats.oecd.org/glossary/ OECD_CAI_recommendati on Recommendation of the Council on Artificial Intelligence OECD OECD Legal Instruments 2019 https:/ /legalinstruments.oecd.org/en/instruments/oecd-legal-0449 NISTIR_8269_Draft A Taxonomy and Terminology of Adversarial Machine Learning Tabassi, Elham;Kevin J. Burns; Michael Hadjimichael; Andres D. Molina-Markham; Julian T. Sexton Draft NISTIR 8269 2019 https:/ /nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8269-draft.pdf SP800-37 Risk Management Framework for Information Systems and Organizations: A System Life Cycle Approach for Security and Privacy Joint Task Force Interagency Working Group NIST Special Publication 800-37 Revision 2 2 (revision 2) 2018 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-37r2.pdf IEEE_Soft_Vocab Systems and software engineering â€”Vocabulary ISO/IEC/IEEE 24765 2017 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8016712 Kohavi,_Ron Glossary of Terms: Special Issue on Applications of Machine Learning and the Knowledge Discovery Process Kohavi, Ron; Foster Provost Machine Learning 30 271-274 1998 http:/ /robotics.stanford.edu/~ronnyk/glossary.html Mitchell,_Tom Machine Learning Mitchell, Tom M. McGraw-Hill Science/Engineering/Math 1997 https:/ /www.cin.ufpe.br/~cavmj/Machine%20-%20Learning%20-%20Tom%20Mitchell.pdf Cyber_Guide Cyber Security Planning Guide Federal Communications Commision https:/ /www.fcc.gov/sites/default/files/cyberplanner.pdf CSRC Information Technology Laboratory Computer Security Resource Center Glossary NIST https:/ /csrc.nist.gov/glossary AIMA Artificial Inelligence: A Modern Approach Russell, Stuart; Peter Norvig Pearson 2010 https:/ /zoo.cs.yale.edu/classes/cs470/materials/aima2010.pdf Breiman_Leo Bagging Predictors Breiman, Leo Machine Learning 24 123-140 1996 https:/ /link.springer.com/content/pdf/10.1007/BF00058655.pdf NISTIR_8312 Four Principles of Explainable Artificial 3 Intelligence Phillips, P. Jonathon; Carina A. Hahn; Peter C. Fontana; David A. Broniatowski; Mark A. Przybocki Draft NISTIR 8312 2020 https:/ /nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8312-draft.pdf SP800-12 An Introductin to Information Security Nieles, Michael; Kelley Dempsey; Victoria Yan Pillitteri NIST SP 800-12 2017 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-12r1.pdf Steinhardt,_Jacob Certified Defenses for Data Poisoning Attacks Steinhardt_Jacob; Pang Wei Koh; Percy Liang 31st Conference on Neural Information Processing Systems 2017 https:/ /proceedings.neurips.cc/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf Ranschaert,_Erik Artificial Intelligence in Medical Imaging: Opportunities, Applications and Risks Ranschaert, Erik R.; Sergey Morozov; Paul R. Algra Springer 2019 https:/ /link.springer.com/content/pdf/10.1007/978-3-319-94878-2.pdf Blank,_Abagayle_Lee Computer Vision Machine Learning and Future-Oriented Ethics Blank, Abagayle Lee Seattle Pacific University https:/ /digitalcommons.spu.edu/cgi/viewcontent.cgi?article=1100&context=honorsprojects Crawford,_Kate The Trouble with Bias Crawford, Kate Neural Information Processing Systems, Long Beach 2017 https:/ /www.youtube.com/watch?v=fMym_BKWQzk COE_AI_Glossary Artificial Intelligence Glossary Council of Europe https:/ /www.coe.int/en/web/artificial-intelligence/glossary Kuehn,_Andreas Analyzing Bug Bounty Programs: An Institutional Perspective on the Economics of Software Vulnerabilities Kuehn, Andreas; Milton Mueller 2014 TPRC Conference Paper 2014 https:/ /papers.ssrn.com/sol3/papers.cfm?abstract_id=2418812 Kang,_Daniel Model Assertions for Monitoring and Improving ML Models Kang, Daniel; Deepti Raghavan; Peter Baili; Matei Zaharia 3rd MLSys Conference 2020 https:/ /arxiv.org/pdf/2003.01668.pdf MathWorks_Residual What Is Residual Analysis? MathWorks https:/ /www.mathworks.com/help/ident/ug/what-is-residual-analysis.html Nayak,_Pragati Concept Drift and Model Decay Detection using Machine Learning Algorithm Nayak, Pragati Aravind; Pavithra Sriganesh; Rakshitha K.M; Manoj Kumar M.V; Prashanth B S; Sneha H R 12th International Conference on Computing Communication and Networking Technologies (ICCCNT) 2021 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9580110 Hasan,_Raza Artificial Intelligence Based Model for Incident Response Hasan, Raza; Salman Mahmood; Akshyadeep Raghav; M. Asim Hasan 2011 International Conference on Information Management, Innovation Management and Industrial Engineering 2011 https:/ /ieeexplore.ieee.org/abstract/document/6114714 McCue_Colleen Data Mining and Predictive Analysis: Intelligence Gathering and Crime Analysis McCue, Colleen Butterworth-Heinemann 2007 https:/ /www.sciencedirect.com/topics/computer-science/domain-expertise#:~:text=2.1%20Domain%20Expertise,need%20to%20know%20your% 20stuff. Besse,_Philippe A Survey of Bias in Machine Learning Through the Prism of Statistical Parity Besse, Philippe; Eustasio del Barrio; Paula Gordaliza; Jean-Michel Loubes; Laurent Risser The American Statistician 76 2 188-198 2021 https:/ /www.tandfonline.com/doi/full/10.1080/00031305.2021.1952897 Muller,_Michael Designing Ground Truth and the Social Life of Labels Muller, Michael; Christine T. Wolf; Josh Andres; Michael Desmond; Narendra Nath Joshi; Zahra Ashktorab; Aabhas Sharma; Kristina Brimijoin; Qian Pan; Evelyn Duesterwald; Casey Dugan Proceedings of the 2021 CHI Conference on Human Factors in Computing System 1-16 2021 https:/ /doi.org/10.1145/3411764.3445402 Shalev-Shwartz,_Shai Understanding Machine Learning: From Theory to Algorithms Shalev-Shwartz, Shai; Shai Ben-David Cambridge Unversity Press 2014 https:/ /www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf SP800-160 Engineering Trustworthy Secure Systems Ross, Ron; Mark Winstead; Michael McEvilley NIST SP 800-160 2022 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-160v1r1.fpd.pdf Hard choices in artificial intelligence, Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz, https:/ /www.sciencedirect.com/science/article/pii/S0004370221001065?via% 3Dihub Khanna,_Anirudh A Study of Today's A.I. through Chatbots and Rediscovery of Machine Intelligence Khanna, Anirudh; Bishwajeet, Pandey; Kushagra, Vashishta; Kartik, Kalia; Bhale, Pradeepkumar; Teerath, Das International Journal of u-and e-Service, Science and Technology 8 7 277-284 2015 http:/ /article.nadiapub.com/IJUNESST/vol8_no7/28.pdf NBSIR_82-2582 An Overview of Computer Vision Gevarter, William B. NIST 82-2582 1982 https:/ /nvlpubs.nist.gov/nistpubs/Legacy/IR/nbsir82-2582.pdf Kou,_Yufeng Survey of fraud detection techniques Kou, Yufeng; Chang-Tien, Lu; Sirirat, Sirwongwattana; Yo-Ping, Huang IEEE International Conference on Networking, Sensing and Control 2 749-754 2004 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1297040 Merritt,_Linda Human capital management: More than HR with a new name. Merritt, Linda People and Strategy 30 2 14-16 2007 https:/ /www.proquest.com/docview/224596559?pq-origsite=gscholar&fromopenview=true CRS_AI Artificial Intelligence: Background, Selected Issues, and Policy Considerations Harris, Laurie A. Congressional Research Service 2021 https:/ /crsreports.congress.gov/product/pdf/R/R46795 IJMAE A Review of the Role of Marketing in Recruitment and Talent Acquisition. Alashmawy, Ahmad; Rashad, Yazdanifard International Journal of Management, Accounting and Economics 6 7 2019 https:/ /www.academia.edu/41261685/A_Review_of_the_Role_of_Marketing_in_Recruitment_and_Talent_Acquisition?from=cover_page Das,_Debashis A survey on recommendation system. Das, Debashis; Laxman, Sahoo; Sujoy, Datta. International Journal of Computer Applications 160 7 2017 https:/ /www.researchgate.net/profile/Debashis-Das-17/publication/313787463_A_Survey_on_Recommendation_System/links/5d7de0474585155f1e4de908/A-Survey-on-Recommendation-System.pdf Hyndman,_Rob Forecasting: principles and practice Hyndman, Rob J.; George, Athanasopoulos 14 2018 https:/ /books.google.com/books? hl=en&lr=&id=_bBhDwAAQBAJ&oi=fnd&pg=PA7&dq=Forecasting&ots=Tij_xmXIMJ&sig=c8LjAcmbLDC5QeH_xQno2l_gTr0#v=onepage&q=Forecasting&f=f alse Ustun,_Berk Actionable Recourse in Linear Classification Ustun, Berk; Spangher, Alexander; Liu, Yang Association of Computing Machinery Voight,_Paul The EU General Data Protection Regulation: A Practical Guide Voight,_Paul; von_dem_Bussche,_Axel Springer ECOA 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B) Consumer Financial Protection Bureau Gill,_Navdeep A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrimination Testing Gill, Navdeep; Hall, Patrick; Montgomery, Kim; Schmidt, Nicholas MDPI ISO_IEC_38507 Information Technology - Governance of IT - Governance implications of the use of artificial intelligence by organizations ISO/IEC ISO 1 2022 https:/ /www.iso.org/obp/ui/#iso:std:iso-iecÊ¼38507Ê¼ed-1Ê¼v1Ê¼en Darnell_Coss_Hall The Future of Analytics Dan Darnell, Rafael Coss, Patrick Hall O'Reilly Media Inc. Ch. 4 2020 https:/ /www.oreilly.com/library/view/the-future-of/9781492091769/ch04.html Fed_Reserve Guidance on Model Risk Management Patrick M. Parkinson The Federal Reserve 2011 https:/ /www.federalreserve.gov/supervisionreg/srletters/sr1107a1.pdf Jennifer,_Hill Causal Inference: Overview Jennifer Hill, Elizabeth A. Stuart International Encyclopedia of the Social & Behavioral Sciences (Second Edition) 255-260 2015 https:/ /www.sciencedirect.com/science/article/pii/B9780080970868420957 AIRS_Penn Artificial Intelligence Risk & Governance AIRS The Warton School, University of Pennslyvania 2022 https:/ /ai.wharton.upenn.edu/artificial-intelligence-risk-governance/ ENISA Privacy by Design European Union Agency for Cybersecurity https:/ /www.enisa.europa.eu/topics/data-protection/privacy-by-design Furche,_Tim Data wrangling for big data: Challenges and opportunities. Furche, Tim; George, Gottlob; Leonid, Libkin; Giorgio, Orsi; Norman, Paton Advances in Database Technologyâ€”EDBT 2016Ê¼ Proceedings of the 19th International Conference on Extending Database Technology 473-478 2016 https:/ /www.research.manchester.ac.uk/portal/files/50447231/paper_94_1_.pdf Enrique Towards an integrated crowdsourcing definition. EstellÃ©s-Arolas, Enrique; Fernando, GonzÃ¡lez-LadrÃ³n-de-Guevara Journal of Information science 38 2 189-200 2012 https:/ /journals.sagepub.com/doi/full/10.1177/0165551512437638 Behdad Nature-inspired techniques in the context of fraud detection. Behdad, Mohammad; Luigi, Barone; Mohammed, Bennamoun; Tim, French. EEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42 6 1273-1290 2012 https:/ /ieeexplore.ieee.org/abstract/document/6392447?casa_token=-mPJh2Do05MAAAAA: c5Eyg4__i64XlBowloZbwwwKNe0KnqAQvjg5romygO6ymRKDl70np3DiDL0mipMXmdmeboDj1hGwa9I Woodward Biometrics: A look at facial recognition. Woodward Jr, John D., Christopher Horn, Julius Gatune, and Aryn Thomas RAND CORP SANTA MONICA CA 2003 https:/ /apps.dtic.mil/sti/citations/ADA414520 Sharma,_Lalita A survey of recommendation system: Research challenges. Sharma, Lalita; Anju, Gera International Journal of Engineering Trends and Technology (IJETT) 4 5 1989-1992 2013 https:/ /d1wqtxts1xzle7.cloudfront.net/38584474/IJETT-V4I5P132_1_-with-cover-page-v2.pdf? Expires=1656295756&Signature=QAnv0QzAEvSK2TSOMFygnk1AGUQrGWtz7PD7~bal6kX9SbiWAwd18feH3nB0kH4EQDA0rEPGrm3V9E38s4eY2C5i52NHA-jY0h2zBYAnLETt5PB1cAMjSZf4qaqNWZCPvQtOpaxTkhV6YB1MLDTmaFCAjKcqQfqm6WPQoax5VLh0hIbF-hlc8p5wXiW7fE9z~OeYpmTJld4doW94bG3OrDUY75EN9cptH-IHlldQRs~jF1POAkJUO5PL91PKVJatYAjeWQn7eNchB2TszrfnDbs6pjCyPXIIvi1WdnQzkKVN0F6N-CA~YtZ6yGKIxTjDuxdcJh0AtfD2cjxLDvNFCA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA Å½liobaitÄ—_IndrÄ— A survey on measuring indirect discrimination in machine learning Å½liobaitÄ—, IndrÄ— CoRR 2018 https:/ /arxiv.org/abs/1511.00148 Vorobeychik Adversarial machine learning Vorobeychik, Yevgeniy; Murat, Kantarcioglu Synthesis Lectures on Artificial Intelligence and Machine Learning 12 3 2018 https:/ /www.morganclaypool.com/doi/abs/10.2200/S00861ED1V01Y201806AIM039?casa_token=bcr3UzYRz6AAAAAA:M-sh4ANwQuRFYcXk18O4x_x6zu7Qq3P5ZC12MrCgTckeNFm9sXOCAkAFvHtMce7t1A3lpUt4MFA Zhang,_Yonggang Principal component adversarial example Zhang,_Yonggang; Xinmei, Tian; Ya, Li; Xinchao, Wang; Dacheng, Tao IEEE Transactions on Image Processing 29 4804-4815 2020 https:/ /ieeexplore.ieee.org/abstract/document/9018372?casa_token=MFIULJJsFBEAAAAA:GVCQa1Ccivt9ETPlYlBmLvK74iIDNzODP-6mo2B4o4nkZG69ORI-HliXK0iXb3GG-Q8csl2vxtBorQ NIST_SP_800 Sp 800-66 rev. 1. an introductory resource guide for implementing the health insurance portability and accountability act (hipaa) security rule Scholl, Matthew A.; Kevin M. Stine; Joan Hash; Pauline Bowen; L. Arnold Johnson; Carla Dancy Smith; Daniel I. Steinberg National Institute of Standards & Technology 2008 https:/ /dl.acm.org/doi/pdf/10.5555/2206281 ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nJain_Saachi Missingness Bias in Model Debugging Jain, Saachi; Hadi Salman; Eric Wong; Pengchuan Zhang; Vibhav Vineet; Sai Vemprala; Aleksander Madry ICLR 2022 https:/ /arxiv.org/abs/2204.08945 Mehrabi,_Ninareh A Survey on Bias and Fairness in Machine Learning Mehrabi, Ninareh ; Fred Morstatter; Nripsuta Saxena; Kristina Lerman; Aram Galstyan ACM Computing Surveys 54 6 1-35 2022 https:/ /dl.acm.org/doi/abs/10.1145/3457607 Lipton,_Zachary Does mitigating MLâ€™s impact disparity require treatment disparity? Lipton, Zachary C.; Alexandra Chouldechova; Julian McAuley 32nd Conference on Neural Information Processing Systems 2018 https:/ /doi.org/10.48550/arXiv.1711.07076 Gustavii,_Ebba A Swedish Grammar for Word Prediction Gustavii, Ebba; Eva Pettersson Uppsala University - Department of Linguistics 2003 https:/ /www.researchgate.net/publication/2838153_A_Swedish_Grammar_for_Word_Prediction/link/00b4951a5f2645ca02000000/download Comptroller_Office Comptroller's Handbook: Model Risk Management, Version 1.0 Office of the Comptroller of the Currency Comptroller's Handbook: Model Risk Management, Version 1.0 2021 https:/ /www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html CISA A Glossary of Common Cybersecurity Words and Phrases National Initiative for Cybersecurity Careers and Studies 2022 https:/ /niccs.cisa.gov/cybersecurity-career-resources/glossary#D GWUC Cyber Glossary The George Washington University Natural Security Archive - The Cyber Vault Project 2018 https:/ /nsarchive.gwu.edu/cyber-glossary-b SP-800-12 Sp 800-12. an introduction to computer security: The nist handbook. Guttman, Barbara; Edward A. Roback 1995 https:/ /dl.acm.org/doi/pdf/10.5555/2206203 Thomas_Edgar Research Methods for Cyber Security Thomas W. Edgar; David O. Manz 367-392 2017 https:/ /www.sciencedirect.com/science/article/pii/B9780128053492000157 Vredenburg,_Karel A survey of user-centered design practice Vredenburg, Karel; Ji-Ye, Ma; Paul W., Smith; Tom, Carey Proceedings of the SIGCHI conference on Human factors in computing systems 471-478 2002 https:/ /dl.acm.org/doi/abs/10.1145/503376.503460 ST04-015 Understanding Denial-of-Service Attacks CISA https:/ /www.cisa.gov/uscert/ncas/tips/ST04-015 Chandrasekaran,_Varun Chandrasekaran,_Varun; Kamalika, Chaudhuri; Irene, Giacomelli; Somesh, Jha; Songbai, Yan 29th USENIX Security Symposium (USENIX Security 20) 1309-1326 2020 https:/ /www.usenix.org/conference/usenixsecurity20/presentation/chandrasekaran FDA_Glossary Glossary of Computer System Software Development Terminology FDA 1995 https:/ /www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/inspection-guides/glossary-computer-system-software-development-terminology-895#_top Meinshausen,_Nicolai Causality from a distributional robustness point of view Meinshausen, Nicolai 2018 IEEE Data Science Workshop (DSW) 6-10 2018 https:/ /ieeexplore.ieee.org/abstract/document/8439889?casa_token=8tga5hcskjQAAAAA: PlpvmJtJNGKSNXiBdghGmy670MxN91PXc3ekZRVEXwOcLoJkh0sxDuWseVzr0EowGRCf8WR-eYLWeU4 Stacke,_Karin Measuring domain shift for deep learning in histopathology. Stacke, Karin; Gabriel, Eilertsen; Jonas, Unger; Claes, LundstrÃ¶m IEEE journal of biomedical and health informatics 25 2 325-336 2020 https:/ /ieeexplore.ieee.org/abstract/document/9234592?casa_token=RIQcYxte8lIAAAAA: 4t6IwNomEw95f3c1ir73BRReG7OzKecUzpVQS_Bk5zIEWA5R75uG-66g9irlblzDDVwu7ut4jAo2i_8 NASA_Soft_Standards Software Assurance and Software Safety Standard NASA Standard 2020 https:/ /standards.nasa.gov/standard/nasa/nasa-std-87398 PET_Handbook Handbook of privacy and privacy-enhancing technologies. Van Blarkom, G. W.; John J. Borking; JG Eddy Olk Privacy Incorporated Software Agent (PISA) Consortium, The Hague 198 2003 https:/ /andrewpatrick.ca/pisa/handbook/Handbook_Privacy_and_PET_final.pdf NIST_1500 NIST Big Data Interoperability Framework Wo L. Chang; Nancy Grady NIST 1 2019 https:/ /www.nist.gov/publications/nist-big-data-interoperability-framework-volume-1-definitions?pub_id=918927 Microsoft_Azure_documen tation_Detect_data_drift Detect data drift (preview) on datasets Microsoft Azure documentation 2022 https:/ /docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-datasets?tabs=python Egnyte Egnyte Egnyte Data Control: Definition and Benefits 2022 https:/ /www.egnyte.com/guides/governance/data-control IG1190M_AIOps_Decommis sion_v1.0.0 IG1190M AIOps Decommission v1.0.0 AI Operations IG1190M AIOps Decommission v1.0.0 2022 https:/ /www.tmforum.org/resources/reference/ig1190m-aiops-decommission-v1-0-0/ peak.ai AI decision making: the future of business intelligence Peak.AI AI decision making: the future of business intelligence 2022 https:/ /peak.ai/hub/blog/ai-decision-making-the-future-of-business-intelligence/ Hochreiter,_Sepp Toward a Broad AI Hochreiter, Sepp Communications of the ACM 65 4 56-57 2022 https:/ /cacm.acm.org/magazines/2022/4/259402-toward-a-broad-ai/fulltext Mitchell,_Eric Memory-Based Model Editing at Scale. Mitchell, Eric; Charles, Lin; Antoine, Bosselut; Christopher D., Manning; Chelsea, Finn International Conference on Machine Learning, PMLR 15817-15831 2022 https:/ /proceedings.mlr.press/v162/mitchell22a.html ISO/IEC_TS_5723Ê¼2022(en) ISO/IEC TS 5723Ê¼2022(en) Trustworthiness â€” Vocabulary ISO/IEC ISO/IEC 2022 https:/ /www.iso.org/obp/ui/#iso:std:iso-iec:tsÊ¼5723Ê¼ed-1Ê¼v1Ê¼en H20.ai_glossary H20.ai Glossary H20.ai H20.ai 2022 https:/ /docs.h2o.ai/h2o/latest-stable/h2o-docs/glossary.html TechTarget_Ivy_Wigmore AI Washing Ivy Wigmore TechTarget 2017 https:/ /www.techtarget.com/searchenterpriseai/definition/AI-washing Roman_V. _Yampolskiy_Unexplainabil ity Unexplainability and Incomprehensibility of Artificial Intelligence Roman V. Yampokskiy PhilArchive 2019 https:/ /philarchive.org/archive/YAMUAI Forbes_Kayvan_Alikhani Remember 'Cloud Washing'? It's Happening In RegTech Kayvan Alikhani Forbes 2019 https:/ /www.forbes.com/sites/forbestechcouncil/2019/10/14/remember-cloud-washing-its-happening-in-regtech/?sh=2a52bd3a796c Ripley,_Brian Introduction and Examples Brian D. Ripley Pattern Recognition and Neural Networks 6 1996 https:/ /archive.org/details/patternrecogniti0000ripl/page/6/mode/2up?q=training&view=theater C3.ai_Model_Training Model Training C3.ai C3.ai Glossary https:/ /c3.ai/glossary/data-science/model-training/ Pyle, _Dorian_Data_Preparation _as_a_Process Data Preparation as a Process Dorian Pyle Data Preparation for Data Mining 89-124 1999 https:/ /www.google.com/books/edition/Data_Preparation_for_Data_Mining/hhdVr9F-JfAC?hl=en&gbpv=1&dq=Binning%20is%20a% 20technique&pg=PA110&printsec=frontcover Wieringa,_Roel_J. Conceptual Frameworks Roel J. Wieringa Design Science Methodology for Information Systems and Software Engineering 73-92 2014 https:/ /www.google.com/books/edition/Design_Science_Methodology_for_Informati/xLKLBQAAQBAJ?hl=en&gbpv=1&dq=% 22Construct+validity+is+defined+by+Shadish+et+al+24+p+506+as+the+degree+to+which+inferences+from+phenomena+to+constructs+are+warranted% 22&pg=PA87&printsec=frontcover York, _Dan_Internet_Society What Is the Splinternet? And Why You Should Be Paying Attention Dan York Internet Society 2022 https:/ /www.internetsociety.org/blog/2022/03/what-is-the-splinternet-and-why-you-should-be-paying-attention/ Splinternets Key Definitions European Parliament, Directorate-General for Parliamentary Research Services, Perarnaud, C., Rossi, J., Musiani, F., et al. 'Splinternets': Addressing the Renewed Debate on Internet Fragmentation 2022 https:/ /op.europa.eu/en/publication-detail/-/publication/5a5bfaed-0d52-11ed-b11c-01aa75ed71a1/language-en Regularization_for_Deep_ Learning Regularization for Deep Learning Ian Goodfellow, Yoshua Bengio, and Aaron Courville Deep Learning 221-266 2016 https:/ /www.google.com/books/edition/Deep_Learning/omivDQAAQBAJ?hl=en&gbpv=1&dq=% 22Sparsity+in+this+context+refers+to+the+fact+that+some+parameters+have+an+optimal+value+of+zero%22&pg=PA229&printsec=frontcover Statistics_in_Plain_English Statistical Significance and Effect Size Timothy C. Urdan Statistics in Plain English 43-56 2001 https:/ /www.google.com/books/edition/Statistics_in_Plain_English/MwCK52n_wBEC?hl=en&gbpv=1&dq=% 22Statistical+significance+When+the+probability+of+obtaining+a+statistic+of+a+given+size+due+strictly+to%22&pg=PA55&printsec=frontcover The_SAGE_Encyclopedia_ of_Communication_Resear ch_Methods Relationships Between Variables Mike Allen, ed. The SAGE Encyclopedia of Communication Research Methods, Volume 1 2017 https:/ /www.google.com/books/edition/The_SAGE_Encyclopedia_of_Communication_R/4GFCDgAAQBAJ?hl=en&gbpv=1&dq=% 22Statistical+significance+refers+to+whether+a+relationship+between+two+or+more+variables+exists+beyond+a+probability+expected+by+chance% 22&pg=PA1413&printsec=frontcover The_Science_of_Algorithm ic_Trading_and_Portfolio_ Management Evaluation Robert Kissell The Science of Algorithmic Trading and Portfolio Management 2013 https:/ /www.google.com/books/edition/The_Science_of_Algorithmic_Trading_and_P/FKPND2zz9OoC?hl=en&gbpv=1&dq=% 22Back+testing+is+the+quantitative+evaluation+of+a+model%E2%80%99s+performance+both+from+a+statistical+and+trading+perspective% 22&pg=PA446&printsec=frontcover Introduction_to_Informati on_Systems Artificial Intelligence R. Kelly Rainer, Reiner R. Kelly, and Brad Prince Introduction to Information Systems: International Adaptation 410-450 2022 https:/ /www.google.com/books/edition/Introduction_to_Information_Systems/Y75VEAAAQBAJ?hl=en&gbpv=1&dq=% 22An+autonomous+vehicle+automobile+bus+tractor+combine+boat+forklift+etc+is+a+vehicle+capable+of+sensing+its+environment+and+moving+safely+wi th+little+or+no+human+input%22&pg=PA437&printsec=frontcover OED_snake_oil Snake oil Oxford English Dictionary Oxford English Dictionary 2022 https:/ /www-oed-com.proxygw.wrlc.org/view/Entry/95490133?redirectedFrom=%22snake+oil%22 Nick_Higham_1 What Is Rounding? Nick Higham Nick Higham 2020 https:/ /nhigham.com/2020/04/28/what-is-rounding/ Nick_Higham_2 What Is Stochastic Rounding? Nick Higham Nick Higham 2020 https:/ /nhigham.com/2020/07/07/what-is-stochastic-rounding/ pwc_Model_Risk_Manage ment_of_AI_and_ML_Syst ems Model Risk Management of AI and Machine Learning Systems PricewaterhouseCoopers Model Risk Management of AI and Machine Learning Systems 2020 https:/ /www.pwc.co.uk/data-analytics/documents/model-risk-management-of-ai-machine-learning-systems.pdf Toward_an_understanding _of_responsible_artificial_ intelligence_practices Toward an understanding of responsible artificial intelligence practices Yichuan Wang, Mengran Xiong, and Hossein G. T. Olya In: Bui, T.X., (ed.) Proceedings of the 53rd Hawaii International Conference on System Sciences. Hawaii International Conference on System Sciences (HICSS 2020), 20202-01-07 - 2020-01-10, Maui, Hawaii, USA. Hawaii International Conference on System Sciences (HICSS) 2020 https:/ /eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf Comparing_scores_and_re ason_codes Comparing scores and reason codes in credit scoring systems: NeuroDecision vs. unconstrained neural networks Equifax Comparing scores and reason codes in credit scoring systems: NeuroDecision vs. unconstrained neural networks 2020 https:/ /assets.equifax.com/marketing/US/assets/comparing_scores_whitepaper.pdf Machine_Learning_Interpr etability_with_H20_Driverl ess_AI Machine Learning Interpretability with H20 Driverless AI Patrick Hall, Navdeep Gill, Megan Kurka, and Wen Phan; edited by Angela Bartz Machine Learning Interpretability with H20 Driverless AI 2022 https:/ /docs.h2o.ai/driverless-ai/latest-stable/docs/booklets/MLIBooklet.pdf Pattern_Recognition_and_ Machine_Learning Introduction Christopher M. Bishop Pattern Recognition and Machine Learning 1-66 2006 DOD_TEVV Technology Investment Strategy 2015-2018 United States Department of Defense's Test and Evaluation, Verification and Validation (TEVV) Working Group Technology Investment Strategy 2015-2018 2015 https:/ /defenseinnovationmarketplace.dtic.mil/wp-content/uploads/2018/02/OSD_ATEVV_STRAT_DIST_A_SIGNED.pdf Fundamentals_of_Informat ion_Systems_Security Auditing, Testing, and Monitoring David Kim and Michael G. Solomon Fundamentals of Information Systems Security 216-250 2016 https:/ /www.google.com/books/edition/Fundamentals_of_Information_Systems_Secu/Yb4eDQAAQBAJ?hl=en&gbpv=1&dq=% 22Audit+logs+Defined+events+that+provide+additional+input+to+audit+activities%22&pg=PA233&printsec=frontcover AI_Ethics_Mark_Coeckelb ergh Glossary Mark Coeckelbergh AI Ethics 203-206 2020 https:/ /www.google.com/books/edition/AI_Ethics/Gs_XDwAAQBAJ?hl=en&gbpv=1&dq=%22Trustworthy+AI+Al+that+can+be+trusted+by+humans% 22&pg=PA206&printsec=frontcover Hands-On_Smart_Contract_Dev Fundamental Concepts of Blockchain Matt Zand, Xun (Brian) Wu, and Mark Anthony Morris Hands-On Smart Contract Development with Hyperledger Fabric V2 \n\nTechTarget_target_functio n target function TechTarget TechTarget 2018 https:/ /www.techtarget.com/whatis/definition/target-function IGI_Global_reference_clas s What is Reference Class IGI Global IGI Global https:/ /www.igi-global.com/dictionary/reference-class/35564 CPO_Magazine_Amar_Kan agaraj Data Remediation and Its Role in Data Security and Privacy CPO Magazine CPO Magazine 2022 https:/ /www.cpomagazine.com/data-protection/data-remediation-and-its-role-in-data-security-and-privacy/ DEV_ranking Machine learning (ML) applications: ranking DEV Community DEV Community 2022 https:/ /dev.to/mage_ai/machine-learning-ml-applications-ranking-238d productmanagerHQ_Josh_ Fechter What Does an AI Product Manager Do? Product Manager HQ Product Manager HQ https:/ /productmanagerhq.com/ai-product-manager/ Proxy_Discrimination Proxy Discrimination in the Age of Artificial Intelligence and Big Data Anya E. R. Prince; Daniel Schwarcz Iowa Law Review 105 3 2020 https:/ /ilr.law.uiowa.edu/print/volume-105-issue-3/proxy-discrimination-in-the-age-of-artificial-intelligence-and-big-data Forbes_Tracy_Kemp Four Skills Every Successful AI Product Owner Should Possess Tracy Kemp Forbes 2021 https:/ /www.forbes.com/sites/forbestechcouncil/2021/08/24/four-skills-every-successful-ai-product-owner-should-possess/?sh=65dbfe423d3d Merriam-Webster_underrepresented underrepresented Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/underrepresented HBR_Andrew_Burt_how_t o_ensure How to Ensure Your AI Doesnâ€™t Discriminate Andrew Burt Harvard Business Review https:/ /hbr.org/2020/08/how-to-ensure-your-ai-doesnt-discriminate Cadient_EEOC Understanding and Avoiding Adverse Impact in Employment Practices Michael Baysinger; Kristin Worrell Cadient https:/ /cadienttalent.com/resources/understanding-and-avoiding-adverse-impact Ben_Green_Yiling_Chen Algorithm-in-the-Loop Decision Making Ben Green; Yiling Chen The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20) https:/ /ojs.aaai.org/index.php/AAAI/article/view/7115 IT_Governance_Blog_Luke _Irwin Personal data vs Sensitive Data: Whatâ€™s the Difference? Luke Irwin IT Governance Blog https:/ /www.itgovernance.co.uk/blog/the-gdpr-do-you-know-the-difference-between-personal-data-and-sensitive-data OED_stereotype Stereotype Oxford English Dictionary Oxford English Dictionary https:/ /www-oed-com.proxygw.wrlc.org/view/Entry/189956?rskey=JmZ8YE&result=1#eid Machine_Learning_Master y_Jason_Brownlee Machine Learning Terminology from Statistics and Computer Science Jason Brownlee Machine Learning Mastery 2016 https:/ /machinelearningmastery.com/data-terminology-in-machine-learning/#:~:text=Row%3A%20A%20row%20describes%20a,problem%20domain% 20that%20you%20have. Banking_on_Your_Data_C hristopher_Gilliard Prepared Testimony and Statement for the Record of Christopher Gilliard, PhD. Hearing on \"Banking on Your Data: the Role of Big Data in Financial Services\" before the House Financial Services Committee Task Force on Financial Technology Christopher Gilliard Prepared Testimony and Statement for the Record of Christopher Gilliard, PhD. Hearing on \"Banking on Your Data: the Role of Big Data in Financial Services\" before the House Financial Services Committee Task Force on Financial Technology https:/ /www.congress.gov/116/meeting/house/110251/witnesses/HHRG-116-BA00-Wstate-GillardC-20191121.pdf ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nMerriam-Webster_pseudoscience pseudoscience Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/pseudoscience Cost_Management_ch15 Lean Accounting and Productivity Measurement Don R. Hansen; Maryanne M. Mowen; Dan L. Heitger Cost Management 2021 https:/ /www.google.com/books/edition/Cost_Management/HhQcEAAAQBAJ?hl=en&gbpv=1&dq=% 22Velocity+has+to+do+with+how+fast+a+product+can+be+delivered+to+the+market+and+quality+is+concerned+with+providing+a+nondefective+product+ with+the+desired+features+to+customers%22&pg=PA783&printsec=frontcover Towards_Productizing Towards Productizing AI/ML Models: An Industry Perspective from Data Scientists Filippo Lanubile, Fabio Calefato, Luigi Quaranta, Maddalena Amoruso, Fabio Fumarola, and Michele Filannino arXiv https:/ /arxiv.org/abs/2103.10548 The_Art_of_Software_Tes ting The Psychology and Economics of Program Testing Glenford J. Myers The Art of Software Testing 1979 https:/ /archive.org/details/artofsoftwaretes0000myer/page/4/mode/2up William_Hetzel An Introduction William C. Hetzel The Complete Guide to Software Testing , 2nd edition 1988 https:/ /archive.org/details/completeguidetos0000hetz/page/6/mode/2up?view=theater On_Hyperparameter_Opti mization On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice Li Yang and Abdallah Shami arXiv https:/ /arxiv.org/pdf/2007.15745.pdf Security_Analysis_of_Subj ect_Access Security Analysis of Subject Access Request Procedures: How to Authenticate Data Subjects Safely When They Request for Their Data Coline Boniface, Imane Fouad, Nataliia Bielova, CÃ©dric Lauradoux, and Cristiana Santos \n\nPrivacy Technologies and Policy (7th Annual Privacy Forum, APF 2019, Rome, Italy, June 13â€“14, 2019, Proceedings) 2019 https:/ /www.google.com/books/edition/Privacy_Technologies_and_Policy/SW2cDwAAQBAJ?hl=en&gbpv=1&dq=% 22i+Impersonation+data+breach+A+malicious+individual+is+able+to+impersonate+a+legitimate+data+subject+to+the+data+controller% 22&pg=PA186&printsec=frontcover IEEE_Caught_in_the_Act Caught in the Act of an Insider Attack: Detection and Assessment of Insider Threat Philip A. Legg, Oliver Buckley, Michael Goldsmith, and Sadie Creese 2015 IEEE International Symposium on Technologies for Homeland Security (HST 2015 https:/ /ieeexplore-ieee-org.proxygw.wrlc.org/stamp/stamp.jsp?tp=&arnumber=7446229 Moradi_Samwald Post-hoc explanation of black-box classifiers using confident itemsets Milad Moradi; Matthias Samwald Expert Systems with Applications 165 2021 https:/ /reader.elsevier.com/reader/sd/pii/S0957417420307302? token=858E09A321B5727ECF889090AFB0B943768A26AFE15FB920B20176BD22F82449CE42841047FD99C51A01659C2ECE9695&originRegion=us-east-1&originCreation=20220918190335#b0035 Mind_on_Statistics Chapter 4 Jessica M. Utts and Robert F. Heckard Mind on Statistics (6th Edition) 2021 https:/ /www.google.com/books/edition/Mind_on_Statistics/npQMEAAAQBAJ?hl=en&gbpv=1&dq=% 22Practical+Versus+Statistical+Significance+Statistical+significance+does+not+necessarily+mean+that+the+relationship+between+the+two+variables+has+ practical+significance%22&pg=PA138&printsec=frontcover Jenna_Burrell How the machine 'thinks': Understanding opacity in machine learning algorithms Jenna Burrell Big Data & Society 1-12 2016 https:/ /journals.sagepub.com/doi/pdf/10.1177/2053951715622512 Hughes_Lavery_Critical_T hinking Glossary William Hughes and Jonathan Lavery Critical Thinking - Concise Edition 2015 https:/ /www.google.com/books/edition/Critical_Thinking_Concise_Edition/k0idCgAAQBAJ?hl=en&gbpv=1&dq=% 222+7+2+straw+man+a+fallacious+argument+which+irrelevantly+attacks+a+position+that+appears+similar+to+but+is+actually+different+from+an+opponen t%E2%80%99s+position+and+concludes+that+the+opponent%E2%80%99s+real+position+has+thereby+been+refuted%22&pg=PA271&printsec=frontcover NIST_SP_800-30_Rev_1 NIST Special Publication 800-30 Revision 1Ê¼ Guide for Conducting Risk Assessments NIST NIST Special Publication 800-30 Revision 1Ê¼ Guide for Conducting Risk Assessments 2012 https:/ /nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-30r1.pdf TechTarget_third_party third party TechTarget TechTarget Glossary 2014 https:/ /www.techtarget.com/whatis/definition/third-party Law_Insider_processing_e nvironment Processing Environment Insider Law https:/ /www.lawinsider.com/dictionary/processing-environment wachter_counterfactual_2 018 â€œCounterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR.â€ Wachter, S., B. D. M. Mittelstadt, and C. Russell. Harvard Journal of Law and Technology 31 2 2018 https:/ /ora.ox.ac.uk/objects/uuidÊ¼86dfcdac-10b5-4314-bbd1-08e6e78b9094 mills_study_2010 Proposed Internet Congestion Control Mechanisms. Mills, Kevin L, James J Filliben, Dong Yeon Cho, Edward Schwartz, and Daniel Genin. NIST SP 500-282. 2010 https:/ /doi.org/10.6028/NIST.SP.500-282 friedman_additive_2000 Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors) Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The Annals of Statistics 28 2 337â€“407 2000 https:/ /doi.org/10.1214/aos/1016218223 kusner_counterfactual_201 7 Counterfactual Fairness Kusner, Matt J, Joshua Loftus, Chris Russell, and Ricardo Silva. Advances in Neural Information Processing Systems (NIPS) 2017 https:/ /proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html cronbach_construct_1955 Construct Validity in Psychological Tests. Cronbach, Lee J., and Paul E. Meehl. Psychological Bulletin 52 281â€“302 1955 https:/ /doi.org/10.1037/h0040957 fink_survey_2010 Survey Research Methods Fink, A. International Encyclopedia of Education (Third Edition) 152â€“160 2010 https:/ /doi.org/10.1016/B978-0-08-044894-7.00296-7 bordens_research_2010 Research Design and Methods: A Process Approach Kenneth S. Bordens, Bruce B. Abbott Book (Eighth Edition) 2011 https:/ /www.amazon.com/Research-Design-Methods-Process-Approach/dp/B008BLHYQ8 nist_statistics_2012 NIST/SEMATECH e-Handbook of Statistical Methods https:/ /doi.org/10.18434/M32189 symeonidis_MLOps_2022 MLOps - Definitions, Tools and Challenges Symeonidis, Georgios, Evangelos Nerantzis, Apostolos Kazakis, and George A. Papakostas In 2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC) 453â€“460 2022 https:/ /ieeexplore.ieee.org/document/9720902 hardt_equality_2016 Equality of Opportunity in Supervised Learning Hardt, Moritz, Eric Price, and Nati and Srebro Advances in Neural Information Processing Systems (NIPS) 3315-3323 2016 http:/ /papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf EEOC_Q&A_Employee_Sel ection Questions and Answers to Clarify and Provide a Common Interpretation of the Uniform Guidelines on Employee Selection Procedures Equal Employment Opportunity Commission Questions and Answers to Clarify and Provide a Common Interpretation of the Uniform Guidelines on Employee Selection Procedures https:/ /www.eeoc.gov/laws/guidance/questions-and-answers-clarify-and-provide-common-interpretation-uniform-guidelines Engineering_safety_in_ma chine_learning Engineering safety in machine learning Kush R. Varshney Information Theory and Applications Workshop (ITA), 2016 2016 https:/ /ieeexplore-ieee-org.proxygw.wrlc.org/document/7888195 DOD_Modeling_and_Simul ation_Glossary DoD Modeling and Simulation (M&S) Glossary United States Department of Defense DoD Modeling and Simulation (M&S) Glossary 1998 https:/ /web.archive.org/web/20070710104756/http:/ /www.dtic.mil/whs/directives/corres/pdf/500059m.pdf Model_Cards_for_Model_ Reporting Model Cards for Model Reporting Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru arXiv https:/ /arxiv.org/abs/1810.03993 David_Leslie_Morgan_Brig gs Explaining Decisions Made with AI: A Workbook (Use Case 1Êµ AI-Assisted Recruitment Tool) David Leslie; Morgan Briggs \n\nExplaining Decisions Made with AI: A Workbook (Use Case 1Êµ AI-Assisted Recruitment Tool) 2021 https:/ /arxiv.org/ftp/arxiv/papers/2104/2104.03906.pdf deeplearningbook_intro Introduction Ian Goodfellow, Yoshua Bengio; Aaron Courville Deep Learning 2016 https:/ /www.deeplearningbook.org/contents/intro.html privacy-enhancing_technologies Chapter 5Ê¼ Privacy-enhancing technologies (PETs) UK Information Commissioner's Office DRAFT Anonymisation, pseudonymisation and privacy enhancing technologies guidance 2022 https:/ /ico.org.uk/media/about-the-ico/consultations/4021464/chapter-5-anonymisation-pets.pdf Joseph_Rocca_Ensemble_ methods Ensemble methods: bagging, boosting and stacking Joseph Rocca Towards Data Science 2019 https:/ /towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205 google_dev_classification-true-false-positive-negative Classification: True vs. False and Positive vs. Negative Google Google Machine Learning Education Foundational Courses https:/ /developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative Public_Health_and_Inform atics_MIE_2021 A Preliminary Scoping Study of Federated Learning for the Internet of Medical Things Arshad Farhad; Sandra I. Woolley; Peter Andras Public Health and Informatics: Proceedings of MIE 2021 504-505 2021 https:/ /www.google.com/books/edition/Public_Health_and_Informatics/81A2EAAAQBAJ?hl=en&gbpv=1&dq=% 22Federated+learning+1+2+is+a+learning+model+which+addresses+the+problem+of+data+governance+and+privacy+by+training+algorithms+collaboratively +without+transferring+the+data+to+another+location%22&pg=PA504&printsec=frontcover Black's_Law_Dictionary_h arm harm The Law Dictionary / Blackâ€™s Law Dictionary Second Edition The Law Dictionary / Blackâ€™s Law Dictionary Second Edition https:/ /thelawdictionary.org/harm/ dataiku_ML_and_linear_m odels Machine Learning and Linear Models: How They Work (In Plain English) Katie Gross Dataiku 2020 https:/ /blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1 ORM_model_inventory Model Inventory Open Risk Manual https:/ /www.openriskmanual.org/wiki/Model_Inventory yields.io_model_validation What Is Model Validation? Eimee V Yields.io 2020 https:/ /www.yields.io/blog/what-is-model-validation/ Open_Risk_Manual_model _validation Model Validation Open Risk Manual Open Risk Manual https:/ /www.openriskmanual.org/wiki/Model_Validation misuse_of_public_data Implicit data crimes: Machine learning bias arising from misuse of public data Efrat Shimron; Jonathan I. Tamir; Ke Wang; Michael Lustig PNAS 119 13 2022 https:/ /www.pnas.org/doi/full/10.1073/pnas.2117203119 MIT_Protected_Attributes Module 3Ê¼ Pedagogical Framework for Addressing Ethical Challenges - Protected Attritbutes and \"Fairness Through Unawareness\" MIT Open Courseware Exploring Fairness in Machine Learning for International Development 2020 https:/ /ocw.mit.edu/courses/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/pages/module-three-framework/protected-attributes/ Practical_Law_protected_ class Protected Class Thomson Reuters Practical Law Thomson Reuters Practical Law https:/ /content.next.westlaw.com/practical-law/document/Ibb0a38daef0511e28578f7ccc38dcbee/Protected-Class? viewType=FullText&transitionType=Default&contextData=(sc.Default)&firstPage=true0law. Dave_Salvator_sparsity How Sparsity Adds Umph to AI Inference Dave Salvator NVIDIA Blogs 2020 https:/ /blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/ saurabh_label_2020 A Unified View of Label Shift Estimation Garg, Saurabh, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton Advances in Neural Information Processing Systems 2020 https:/ /proceedings.neurips.cc/paper/2020/hash/219e052492f4008818b8adb6366c7ed6-Abstract.html provost_data_2013 Data Science and its Relationship to Big Data and Data-Driven Decision Making Provost, Foster and Tom Fawcett Big Data 1 1 51â€“59 2013 https:/ /www.liebertpub.com/doi/full/10.1089/big.2013.1508 hameed_data_2020 Data Preparation: A Survey of Commercial Tools Hameed, Mazhar and Felix Naumann ACM SIGMOD Record 49 3 18â€“29 2020 https:/ /doi.org/10.1145/3444831.3444835 Merriam-Webster_ranking ranking Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/ranking hunter_differential_1979 Differential validity of employment tests by race: A comprehensive review and analysis Hunter, John E. and Frank L. Schmidt and Ronda Hunter Psychological Bulletin 86 721â€“735 1979 https:/ /psycnet.apa.org/record/1979-30107-001 kelley_dogfooding_2022 â€˜Dogfoodingâ€™ Kelley, Lora The New York Times 2022 https:/ /www.nytimes.com/2022/11/14/business/dogfooding.html wolfram_math_2022 Wolfram MathWorld: The Web's Most Extensive Mathematics Resource 2022 https:/ /mathworld.wolfram.com/ aivodji_fairwashing_2019 Fairwashing: the risk of rationalization Aivodji, Ulrich, Hiromi Arai, Olivier Fortineau, SÃ©bastien Gambs, Satoshi Hara, and Alain Tapp Proceedings of the 36th International Conference on Machine Learning 2019 https:/ /proceedings.mlr.press/v97/aivodji19a.html khalid_feature_2014 A survey of feature selection and feature extraction techniques in machine learning Khalid, Samina, Tehmina Khalil, and Shamila Nasreen. 2014 Science and Information Conference 372â€“378 2014 https:/ /ieeexplore.ieee.org/abstract/document/6918213 tabassi_adversarial_2019 A Taxonomy and Terminology of Adversarial Machine Learning Tabassi, Elham, Kevin Burns, Michael Hadjimichael, Andres Molina-Markham, and Julian Sexton. NIST Internal or Interagency Report (NISTIR) 8269 (Draft) 2019 https:/ /doi.org/10.6028/NIST.IR.8269-draft measurement_iso22989_20 22 ISO/IEC 22989Ê¼2022 Information technology â€” Artificial intelligence â€” Artificial intelligence concepts and terminology ISO/IEC 22989Ê¼2022 2022 https:/ /www.iso.org/standard/74296.html aime_measruement_2022 Notes on Measurement NIST AIME Team Unpublished Manuscript 2022 <None> saarela_feature_2021 Comparison of feature importance measures as explanations for classification models Mirka Saarela and Susanne Jauhiainen SN Applied Sciences 3 2021 https:/ /link.springer.com/article/10.1007/s42452-021-04148-9 poole_mackworth_observa tion 5.3.1 Background Knowledge and Observations David Poole and Alan Mackworth Artificial Intelligence: Foundations of Computational Agents 2010 https:/ /artint.info/html/ArtInt_112.html kathleen_walch_operationa lization Operationalizing AI Kathleen Walch Forbes 2020 https:/ /www.forbes.com/sites/cognitiveworld/2020/01/26/operationalizing-ai/?sh=42ea4b2733df about_ML_packages About ML Packages UiPath UiPath AI Center Guide 2021 https:/ /docs.uipath.com/ai-fabric/v0/docs/about-ml-packages TechTarget_data_point data point Katie Terrell Hanna and Ivy Wigmore TechTarget 2022 https:/ /www.techtarget.com/whatis/definition/data-point Morris_John_data_point What is a data point in a machine learning model? John Morris VProexpert 2022 https:/ /www.vproexpert.com/what-is-a-data-point-in-a-machine-learning-model/ Artasanchez_Joshi_AI_wit h_Python Natural Language Processing Alberto Artasanchez and Prateek Joshi \n\nArtificial Intelligence with Python: Your Complete Guide to Building Intelligent Apps Using Python 3.x, 2nd Edition 351-378 2020 https:/ /www.google.com/books/edition/Artificial_Intelligence_with_Python/P0fODwAAQBAJ?hl=en&gbpv=1&dq=% 22Lemmatization+is+the+process+of+grouping+together+the+different+inflected+forms+of+a+word+so+they+can+be+analyzed+as+a+single+item% 22&pg=PA356&printsec=frontcover Techopedia_lemmatization Lemmatization Techopedia Techopedia https:/ /www.techopedia.com/definition/33256/lemmatization Techslang_lemmatization What is Lemmatization? Techslang Techslang https:/ /www.techslang.com/definition/what-is-lemmatization/ TechTarget_lemmatization Lemmatization TechTarget contributor TechTarget 2018 https:/ /www.techtarget.com/searchenterpriseai/definition/lemmatization Lim_Swee_Kiat_harms Understanding Bias Part I Lim Sweet Kiat Machines Gone Wrong 2019 https:/ /machinesgonewrong.com/bias_i/#two-types-of-harms ICO_data_minimisation Data minimisation and privacy-preserving techniques in AI systems Information Commissioner's Office ICO AI Blog 2022 https:/ /ico.org.uk/about-the-ico/media-centre/ai-blog-data-minimisation-and-privacy-preserving-techniques-in-ai-systems/ EDPS_data_minimization Data Minimization European Data Protection Supervisor Data Protection Glossary https:/ /edps.europa.eu/data-protection/data-protection/glossary/d_en Arjun_Subramonian_bias_ mitigation An Introduction to Fairness and Bias Mitigation with AllenNLP Arjun Subramonian AI2Blog 2021 https:/ /blog.allenai.org/an-introduction-to-fairness-and-bias-mitigation-with-allennlp-d1b478d44d4c Chi,_Gao,_Ma Tik Tok Unwrapped Nicole Chi, Keming Gao, Joanne Ma Tik Tok Unwrapped 2022 https:/ /www.ischool.berkeley.edu/sites/default/files/sproject_attachments/tiktok_unwrapped_zine_final.pdf ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nTechTarget_decision_supp ort_system decision support system TechTarget TechTarget 2021 https:/ /www.techtarget.com/searchcio/definition/decision-support-system Burstein_Holsapple Spreadsheet-Based Decision Support Systems Frada Burstein and Clyde W. Holsapple Handbook on Decision Support Systems 1Êµ Basic Themes 2008 https:/ /www.google.com/books/edition/Handbook_on_Decision_Support_Systems_1/q_3sRkRKZQwC?hl=en&gbpv=0 Sourabh_Mehta_determini stic Deterministic vs Stochastic Machine Learning Sourabh Mehta Analytics India Magazine 2022 https:/ /analyticsindiamag.com/deterministic-vs-stochastic-machine-learning/ jansen_graphical_1998 The Graphical User Interface. Jansen, Bernard J. ACM SIGCHI Bulletin 30 2 22â€“26 1998 https:/ /doi.org/10.1145/279044.279051 rudin_interpretable_2022 Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges. Rudin, Cynthia, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Statistics Surveys 16 1â€“85 2022 https:/ /doi.org/10.1214/21-SS133 NISTIR_8312_Full Four Principles of Explainable Artificial Intelligence Phillips, P. Jonathon; Carina A. Hahn; Peter C. Fontana; David A. Broniatowski; Mark A. Przybocki NISTIR 8312 2021 https:/ /nvlpubs.nist.gov/nistpubs/ir/2021/NIST.IR.8312.pdf arun_opportunities_2020 Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey. Das, Arun, and Paul Rad. Arxiv 2020 https:/ /doi.org/10.48550/arXiv.2006.11371 open_risk_2022 Open Risk Manual: Midel Governancde Open Risk Manual 2022 https:/ /www.openriskmanual.org/wiki/Model_Governance merriam_webster_outcom e outcome Merriam-Webster Merriam-Webster Dictionary 2022 https:/ /www.merriam-webster.com/dictionary/outcome sutton_reinforcement_201 8 Reinforcement Learning: An Introduction Sutton, Richard, and Andrew Barto Book, Published by MIT Press 2018 Covert_et_al Towards a Triad for Data Privacy Quentin Covert, Mary Francis, Dustin Steinhagen, and Kevin Streff Proceedings of the 53rd Hawaii International Conference on System Sciences 2020 https:/ /scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/5486a250-cc3c-4227-a752-7d08378afbdf/content Cami_Rosso The Human Bias in the AI Machine: How artificial intelligence is subject to cognitive bias Cami Rosso Psychology Today 2018 https:/ /www.psychologytoday.com/us/blog/the-future-brain/201802/the-human-bias-in-the-ai-machine NIST_CSRC_man-in-the-middle_attack man-in-the-middle attack (MitM) NIST Computer Security Resource Center NIST Computer Security Resource Center https:/ /csrc.nist.gov/glossary/term/man_in_the_middle_attack cambridge_dictionary_202 2 Cambridge Dictionary 2022 https:/ /dictionary.cambridge.org/us/ kulinski_feature_2020 Feature Shift Detection: Localizing Which Features Have Shifted via Conditional Distribution Tests Kulinski, Sean M., Saurabh Bagchi, and David I. Inouye NIPS 2020 https:/ /proceedings.neurips.cc/paper/2020/file/e2d52448d36918c575fa79d88647ba66-Paper.pdf settles_active_2009 Active Learning Literature Survey Burr Settles Technical Report, University of Wisconsim-Madison, Department of Computer Sciences 2009 https:/ /minds.wisconsin.edu/handle/1793/60660 informs_analytics_2022 Operations Research & Analytics INFORMS 2022 https:/ /www.informs.org/Explore/Operations-Research-Analytics guresen_definition_2011 Definition of Artificial Neural Networks with Comparison to Other Networks Guresen, Erkam, and Gulgun Kayakutlu Procedia Computer Science, World Conference on Information Technology 3 2011 https:/ /doi.org/10.1016/j.procs.2010.12.071 olson_pmlb_2017 PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison. Olson, Randal S., William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore BioData Mining 10 1 2017 https:/ /doi.org/10.1186/s13040-017-0154-4 devore_probability_2004 Probability and Statistics for Engineering and the Sciences Jay L. Devore Book, Published by Thompson, Brooks/Cole, Sixth Edition 2004 aggarwal_clustering_2013 Data Clustering: Algorithms and Applications Aggarwal, Charu C., and Chandan K. Reddy Book, Published by Chapman & Hall/CRC, First Edition 2013 techopedia_column_2022 Database Column Techopedia 2022 https:/ /www.techopedia.com/definition/8/database-column#:~:text=In%20the%20context%20of%20relational,documents%20or%20even%20video% 20clips. box_statistics_2005 Statistics for Experimenters: Design, Innovation, and Discovery George E. P. Box, J. Stuart Hunter, William G. Hunter Book, Published by Wiley, Second Edition 2005 james_statistical_2014 An Introduction to Statistical Learning: With Applications in R James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani Book, Published by Springer 2014 http:/ /www-bcf.usc.edu/~gareth/ISL/ friedler_comparative_2019 A Comparative Study of Fairness-Enhancing Interventions in Machine Learning Friedler, Sorelle A., Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P. Hamilton, and Derek Roth In Proceedings of the Conference on Fairness, Accountability, and Transparency 2019 https:/ /doi.org/10.1145/3287560.3287589 gong_differential_2020 A Survey on Differentially Private Machine Learning [Review Article]. Gong, Maoguo, Yu Xie, Ke Pan, Kaiyuan Feng, and A.K. Qin IEEE Computational Intelligence Magazine 15 2 2020 https:/ /doi.org/10.1109/MCI.2020.2976185 Bolukbasi_et_al_Debiasing _Word_Embeddings Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain 2016 https:/ /papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf google_glossary_2023 Machine Learning Glossary Google 2023 https:/ /developers.google.com/machine-learning/glossary wikipedia_graph_2023 Graph (discrete mathematics) Wikipedia Wikipedia 2023 https:/ /en.wikipedia.org/wiki/Graph_(discrete_mathematics)#Definitions Mark_Ciampa_2021 Utilizing Threat Data and Intelligence Mark Ciampa CompTIA CySA+ Guide to Cybersecurity Analyst (CS0-002) 2021 https:/ /www.google.com/books/edition/CompTIA_CySA+_Guide_to_Cybersecurity_Ana/NwpIEAAAQBAJ?hl=en&gbpv=1&dq=% 22In+cybersecurity+a+threat+actor+is+a+term+used+to+describe+individuals+or+entities+who+are+responsible+for+cyber+incidents+against+enterprises+ governments+and+users%22&pg=PA29&printsec=frontcover David_Lyon_2007 Security, Suspicion, Social Sorting David Lyon Surveillance Studies: An Overview 2007 https:/ /www.google.com/books/edition/Surveillance_Studies/_dTHJgh3-f0C?hl=en&gbpv=1&bsq=surveillance%20is Hartley_and_Zisserman_2 003 Projective Geometry and Transformations of 2D Richard Hartley; Andrew Zisserman Multiple View Geometry in Computer Vision 2003 https:/ /www.google.com/books/edition/Multiple_View_Geometry_in_Computer_Visio/si3R3Pfa98QC?hl=en&gbpv=1&dq=% 22Projection+along+rays+through+a+common+point+the+centre+of+pro+jection+defines+a+mapping+from+one+plane+to+another% 22&pg=PA34&printsec=frontcover Sloane_et_al_2020 Participation is not a Design Fix for Machine Learning Mona Sloane, Emanuel Moss, Olaitan Awomolo, Laura Forlano Proceedings of the 37th International Conference on Machine Learning, PMLR 2020 https:/ /arxiv.org/ftp/arxiv/papers/2007/2007.02423.pdf NIST_CSRC_parity parity NIST Computer Security Resource Center NIST Computer Security Resource Center https:/ /csrc.nist.gov/glossary/term/parity Dennis_Mercadal L Dennis Mercadal Dictionary of Artificial Intelligence 1990 https:/ /archive.org/details/dictionaryofarti0000merc/page/162/mode/2up?view=theater Ekaterina_et_al_2020 Why Are We Averse towards Algorithms? A Comprehensive Literature Review on Algorithmic Aversion Ekaterina Jussupow, Izak Benbasat, and Armin Heinzl Proceedings of the 28th European Conference on Information Systems (ECIS), An Online AIS Conference 2020 https:/ /aisel.aisnet.org/ecis2020_rp/168/ Gabriel_2020 Artificial Intelligence, Values, and Alignment Iason Gabriel Minds and Machines 30 2020 https:/ /link.springer.com/article/10.1007/s11023-020-09539-2 nist_800_2010 A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications. Rukhin, Andrew, Juan Soto, James Nechvatal, Miles Smid, Elaine Barker, Stefan Leigh, Mark Levenson, et al. NIST SP-800-22ra 2010 https:/ /doi.org/10.6028/NIST.SP.800-22r1a iso_2382_1997 Information technology â€” Vocabulary â€” Part 31Ê¼ Artificial intelligence â€” Machine learning ISO ISO/IEC 2382-31 1997 iso_22989_2022 Information technology â€” Artificial intelligence â€” Artificial intelligence concepts and terminology ISO ISO/IEC 22989 2022 https:/ /www.iso.org/standard/74296.html european_ethics_2019 Ethics Guidelines for Trustworthy AI High-Level Expert Group on Artificial Intelligence Draft Report 2019 https:/ /ec.europa.eu/futurium/en/ai-alliance-consultation.1.html Friedman_et_al_2017 A Survey of Value Sensitive Design Methods Batya Friedman, David G. Hendry, and Alan Borning Foundations and TrendsÂ® in Humanâ€“Computer Interaction 2017 https:/ /www.nowpublishers.com/article/Details/HCI-015 Bipartisan_Policy_Center_i mpact_assessments Explainer: Impact Assessments for Artificial Intelligence Sean Long, Jeremy Pesner, and Tom Romanoff Bipartisan Policy Center 2022 https:/ /bipartisanpolicy.org/blog/impact-assessments-for-ai/ Kimiz_Dalkir_2011 The Value of Knowledge Management Kimiz Dalkir Knowledge Management in Theory and Practice 2011 https:/ /www.google.com/books/edition/Knowledge_Management_in_Theory_and_Pract/_MrxCwAAQBAJ?hl=en&gbpv=1&dq=% 22Qualitative+measures+provide+more+context+and+details+about+the+value+e+g+perceptions+which+are+often+difficult+to+measure+quantitatively% 22&pg=PA343&printsec=frontcover Cost_Management_ch2 Basic Cost Management Concepts Don R. Hansen; Maryanne M. Mowen; Dan L. Heitger Cost Management 2021 https:/ /www.google.com/books/edition/Cost_Management/HhQcEAAAQBAJ?hl=en&gbpv=1&dq=% 22Qualitative+measurement+implies+the+use+of+data+expressed+in+categories+such+as+customer+reviews+of+new+model+jet+skis% 22&pg=PA38&printsec=frontcover Virginia_Dignum_Responsi bility_and_Artificial_Intelli gence Responsibility and Artificial Intelligence Virginia Dignum The Oxford Handbook of Ethics of AI 215-232 2020 https:/ /www.google.com/books/edition/The_Oxford_Handbook_of_Ethics_of_AI/8PQTEAAAQBAJ?hl=en&gbpv=1&dq=% 22Understanding+the+values+behind+the+technology+and+deciding+on+how+we+want+our+values+to+be+incorporated+in+AI+systems+requires+that+we +are+also+able+to+decide+on+how+and+what+we+want+AI+to+mean+in+our+societies%22&pg=PA221&printsec=frontcover yeom_avoiding_2021 Avoiding Disparity Amplification under Different Worldviews Yeom, Samuel, and Michael Carl Tschantz FAccT 2021Ê¼ In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency 273-283 2021 https:/ /doi.org/10.1145/3442188.3445892 Merriam-Webster_context context Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/context jacobs_measurement_2023 Measurement and Fairness Jacobs, Abigail Z., and Hanna Wallach FAccT 2021Ê¼ In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency 2021 https:/ /doi.org/10.1145/3442188.3445901 Merriam-Webster_impact impact Merriam-Webster Merriam-Webster https:/ /www.merriam-webster.com/dictionary/impact Lisa_M._Given_SAGE M: Mixed Methods Research Lisa M. Given The SAGE Encyclopedia of Qualitative Research Methods 491-538 2008 https:/ /www.google.com/books/edition/The_SAGE_Encyclopedia_of_Qualitative_Res/byh1AwAAQBAJ? hl=en&gbpv=1&dq=Mixed+methods+is+defined+as+research+in+which+the+inquirer+or+investigator+collects+and+analyzes+data,+integrates+the+findings, +and+draws+inferences+using+both+qualitative+and+quantitative+approaches+or+methods+in+a+single+study+or+a+program+of+study. &pg=PT584&printsec=frontcover NIST_AI_RMF_1.0 NIST AI RMF 1.0 NIST NIST AI RMF 1.0 2023 https:/ /nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf 45_CFR_46_2018_Require ments_ (2018_Common_Rule) 2018 Requirements (2018 Common Rule) United States Department of Health and Human Services (HHS) 45 CFR 46 2018 https:/ /www.hhs.gov/ohrp/regulations-and-policy/regulations/45-cfr-46/revised-common-rule-regulatory-text/index.html cambridge_causative_2023 Cambridge Dictionary: causative Cambridge Cambridge Dictionary 2023 https:/ /dictionary.cambridge.org/us/dictionary/english/causative lyons_contestability_2021 Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions Lyons, Henrietta, Eduardo Velloso, and Tim Miller Proceedings of the ACM on Human-Computer Interaction 2021 https:/ /doi.org/10.1145/3449180 cambridge_contestable_20 23 Contestable Cambridge Dictionary 2023 https:/ /dictionary.cambridge.org/us/dictionary/english/contestable Saleh_Alkhalifa_ML_in_Bi otech Supervised Machine Learning Saleh Alkhalifa \n\nMachine Learning in Biotechnology and Life Sciences: Build Machine Learning Models Using Python and Deploy Them on the Cloud 168-233 2022 https:/ /www.google.com/books/edition/Machine_Learning_in_Biotechnology_and_Li/KUVWEAAAQBAJ?hl=en&gbpv=1&dq=% 22We+can+define+supervised+learning+as+a+general+subset+of+machine+learning+in+which+data+like+its+associated+labels+is+used+to+train+models+th at+can+learn+or+generalize+from+the+data+to+make+predictions+preferably+with+a+high+degree+of+certainty%22&pg=PA168&printsec=frontcover Schneider_McGrew_in_Fla nagan_McDonough_2018 The Cattell-Horn-Carroll Theory of Cognitive Abilities W. Joel Schneider and Kevin S. McGrew; edited by Dawn P. Flanagan and Erin M. McDonough Contemporary Intellectual Assessment: Theories, Tests, and Issues 73-163 2018 https:/ /www.google.com/books/edition/Contemporary_Intellectual_Assessment/JA1mDwAAQBAJ?hl=en&gbpv=1&dq=% 22Formal+expertise+is+the+result+of+a+selfselection+of+a+domain+of+knowledge+that+is+mastered+deliberately+and+for+which+there+are+clear+bench marks+of+success+Fisher+Keil+2016%22&pg=PA117&printsec=frontcover Little_2013 The Measurement Model Todd D. Little Longitudinal Structural Equation Modeling 71-105 2013 https:/ /www.google.com/books/edition/Longitudinal_Structural_Equation_Modelin/gzeCu3FjZf4C?hl=en&gbpv=1&dq=%22Measurement+model% 22&pg=PA103&printsec=frontcover Merriam-Webster_executive executive Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/executive Dorf_2018 Measurement and Instrumentation Richard C. Dorf The Engineering Handbook 151-160 2018 https:/ /www.google.com/books/edition/The_Engineering_Handbook/l_TLBQAAQBAJ?hl=en&gbpv=1&dq=% 22The+propagation+of+uncertainty+is+defined+as+the+way+in+which+uncertainties+in+the+variables+affect+the+uncertainty+in+the+calculated+results% 22&pg=SA99-PA711&printsec=frontcover Merriam-Webster_example example Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/example Merriam-Webster_ethic ethic Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/ethic Merriam-Webster_anthropomorphis m anthropomorphism Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/anthropomorphism berthold_guide_2020 Guide to Intelligent Data Science: How to Intelligently Make Use of Real Data Berthold, Michael R., Christian Borgelt, Frank HÃ¶ppner, Frank Klawonn, and Rosaria Silipo Springer International Publishing 2020 https:/ /doi.org/10.1007/978-3-030-45574-3 alon-barkat_human_2023 Humanâ€“AI Interactions in Public Sector Decision Making: â€˜Automation Biasâ€™ and â€˜Selective Adherenceâ€™ to Algorithmic Advice. Alon-Barkat, Saar, and Madalina Busuioc Journal of Public Administration Research and Theory 33 1 153â€“169 2023 https:/ /doi.org/10.1093/jopart/muac007 humphrey_addressing_202 0 Addressing Harmful Bias and Eliminating Discrimination in Health Professions Learning Environments: An Urgent Challenge. Humphrey, Holly J., Dana Levinson, Marc A. Nivet, and Stephen C. Schoenbaum Academic Medicine 95 12S 2020 https:/ /doi.org/10.1097/ACM.0000000000003679 knuth_art_1981 The Art of Computer Programming, Volume 2Ê¼ Seminumerical Algorithms Donald Knuth Addison-Wesley 2 1981 garey_computers_1979 Computers and Intractability: A Guide to the Theory of NP-Completeness Michael Garey and David Johnson W. H. Freeman 1979 cambridge_impact_2023 Impact Cambridge Dictionary 2023 https:/ /dictionary.cambridge.org/us/dictionary/english/impact ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nlaw_policy_2023 Policy The Law Dictionary 2023 https:/ /thelawdictionary.org/policy/#:~:text=Definition%20%26%20Citations%3A,as%20directed%20to%20the%20POLICY fernandez_residual_1992 Residual Analysis and Data Transformations: Important Tools in Statistical Analysis Fernandez, George C. J. HortScience 27 4 297â€“300 1992 https:/ /journals.ashs.org/hortsci/view/journals/hortsci/27/4/article-p297.xml Leavy_OHQR_Intro Introduction Patricia Leavy The Oxford Handbook of Qualitative Research 1-13 2014 https:/ /academic.oup.com/edited-volume/38166/chapter-abstract/332997092?redirectedFrom=fulltext Barbour_2014 The scope and contribution of qualitative research Rosaline S. Barbour Introducing Qualitative Research: A Student's Guide, Second Edition 11-27 2008; 2014 https:/ /archive.org/details/introducingquali0000barb_j9h1/page/12/mode/2up?view=theater Merriam-Webster_engineer engineer Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/engineer AI_Incident_Editors Editor's Guide AI Incident Database AI Incident Database 2023 https:/ /incidentdatabase.ai/editors-guide/ interaction_context_2023 Context of Use Interaction Design: The Glossary of Human Computer Interaction Interaction Design Foundation 2023 https:/ /www.interaction-design.org/literature/book/the-glossary-of-human-computer-interaction/context-of-use AAAS_AI_and_Bias_2022-09 Artificial Intelligence and Bias â€“ An Evaluation M. Karanicolas and M. Knodel Artificial Intelligence and the Courts: Materials for Judges 2022 https:/ /doi.org/10.1126/aaas.adf0788 Seth_Boden_2020 Start Here: A Primer on Diversity and Inclusion (Part 1 of 2) Seth Boden Harvard Business Publishing 2020 https:/ /www.harvardbusiness.org/start-here-a-primer-on-diversity-and-inclusion-part-1-of-2/ GWU_diversity_and_inclus ion Diversity and Inclusion Defined George Washington University George Washington University Office for Diversity, Equity and Community Engagement https:/ /diversity.gwu.edu/diversity-and-inclusion-defined HUD_diversity_and_inclus ion Diversity and Inclusion Definitions U.S. Department of Housing and Urban Development U.S. Department of Housing and Urban Development https:/ /www.hud.gov/program_offices/administration/admabout/diversity_inclusion/definitions Jamieson_Govaart_Pownall Reflexivity in quantitative research: A rationale and beginner's guide Michelle K. Jamieson, Gisela H. Govaart, and Madeleine Pownall Social and Personality Psychology Compass e12735 2023 https:/ /doi.org/10.1111/spc3.12735 Industrial_Network_Securi ty_2011 Monitoring Enclaves Eric D. Knapp and Joel Langill \n\nIndustrial Network Security: Securing Critical Infrastructure Networks for Smart Grid, SCADA, and Other Industrial Control Systems 2011 https:/ /www.google.com/books/edition/Industrial_Network_Security/PlOtqouTwaUC?hl=en&gbpv=1&dq=% 22Data+retention+refers+to+the+amount+of+information+that+is+stored+long-term,+and+can+be+measured+in+volume+ (the+size+of+the+total+collected+logs+in+bytes)+and+time+(the+number+of+months+or+years+that+logs+are+stored+for).% 22&pg=PA243&printsec=frontcover ChatGPT ChatGPT OpenAI https:/ /chat.openai.com/chat Merriam-Webster_parity Parity Merriam-Webster Dictionary 2023 https:/ /www.merriam-webster.com/dictionary/parity barredo_explainable_2020 Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI Barredo Arrieta, Alejandro, Natalia DÃ­az-RodrÃ­guez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, et al. Information Fusion 58 82â€“115 2020 https:/ /doi.org/10.1016/j.inffus.2019.12.012 jackman_oxford_2008 Measurement Simon Jackman The Oxford Handbook of Political Methodology 2008 Hammarberg_2016_Busett o_2020 1. Qualitative research methods: when to use them and how to judge them; 2. How to use and assess qualitative research methods 1. K. Hammarberg, M. Kirkman, and S. de Lacey; 2. Loraine Busetto, Wolfgang Wick, and Christoph Gumbinger 1. Human Reproduction ; 2. Neurological Research and Practice 1. 31 1. 3; 2. 14 1. 2016; 2. 2020 1. https:/ /academic.oup.com/humrep/article/31/3/498/2384737; 2. https:/ /neurolrespract.biomedcentral.com/articles/10.1186/s42466-020-00059-z Russell_2003_Brannen_20 05 1. Evaluation of qualitative research studies; 2. Mixing Methods: The Entry of Qualitative and Quantitative Approaches into the Research Process 1. Cynthia K. Russell and David M. Gregory; 2. Julia Brannen 1. Evidence-Based Nursing ; 2. International Journal of Social Research Methodology 1. 6; 2. 8 2. 8 1. 36-40; 2. 173-184 1. 2003; 2. 2007 1. http:/ /dx.doi.org/10.1136/ebn.6.2.36; 2. https:/ /www.tandfonline.com/doi/full/10.1080/13645570500154642 Pamela_Goh_2021 Humans as the Weakest Link in Maintaining Cybersecurity: Building Cyber Resilience in Humans Pamela Goh. Edited by Majeed Khader, Loo Seng Neo, and Whistine Xiau Ting Chai \n\nIntroduction to Cyber Forensic Psychology: Understanding the Mind of the Cyber Deviant Perpetrators 287-305 2021 https:/ /www.worldscientific.com/doi/abs/10.1142/9789811232411_0014 Annie_Jacobsen_2015 Total Information Awareness Annie Jacobsen \n\nThe Pentagon's Brain: An Uncensored History of DARPA, America's Top-Secret Military Research Agency 336-352 2015 https:/ /archive.org/details/pentagonsbrainun0000jaco_c8o3/page/342/mode/2up?q=%22a+role-playing+exercise+in+which+a+problem+is+examined+from+an+adversary%E2%80%99s+or+enemy%E2%80%99s+perspective.%22 Ben_Auffarth_2021 Online Learning for Time-Series Ben Auffarth \n\nMachine Learning for Time-Series with Python: Forecast, Predict, and Detect Anomalies with State-of-the-art Machine Learning Methods 209-259 2021 https:/ /www.google.com/books/edition/Machine_Learning_for_Time_Series_with_Py/a7tLEAAAQBAJ?hl=en&gbpv=1&dq=% 22On+the+other+hand+offline+learning+the+more+commonly+known+approach+implies+that+you+have+a+static+dataset+that+you+know+from+the+start +and+the+parameters+of+your+machine+learning+algorithm+are+adjusted+to+the+whole+dataset+at+once+often+loading+the+whole+dataset+into+memo ry+or+in+batches%22&pg=PA210&printsec=frontcover FWS_062_FW_1 062 FW 1, Affirmative Employment Program and Plans Office for Human Resources of the U.S. Fish & Wildlife Service U.S. Fish & Wildlife Service 1996 https:/ /www.fws.gov/policy/062fw1.html Merriam-Webster_assessment assessment Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/assessment Anthropomorphism_in_AI_ 2020 anthropomorphism Arleen Salles, Kathinka Evers, and Michele Farisco AJOB Neuroscience 2 88-95 2020 https:/ /doi.org/10.1080/21507740.2020.1740350 OECD_Artificial_Intelligenc e_in_Society The technical landscape OECD Artificial Intelligence in Society 19-34 2019 https:/ /www.google.com/books/edition/Artificial_Intelligence_in_Society/eRmdDwAAQBAJ?hl=en&gbpv=1&dq=% 22Artificial+narrow+intelligence+ANI+or+applied+AI+is+designed+to+accomplish+a+specific+problem+solving+or+reasoning+task% 22&pg=PA22&printsec=frontcover AI_in_Medical_Imaging_gl ossary Glossary Erik R. Ranschaert, Sergey Morozov, and Paul R. Algra, eds. Artificial Intelligence in Medical Imaging: Opportunities, Applications and Risks 349-364 2019 https:/ /www.google.com/books/edition/Artificial_Intelligence_in_Medical_Imagi/ss6FDwAAQBAJ?hl=en&gbpv=1&dq=% 22The+definition+of+artificial+narrow+intelligence+is+in+contrast+to+that+of+strong+AI+or+artificial+general+intelligence+which+aims+at+providing+a+sy stem+with+consciousness+or+the+ability+to+solve+any+problems%22&pg=PA350&printsec=frontcover DOL_Practical_Significanc e Practical Significance in EEO Analysis Frequently Asked Questions U.S. Department of Labor Office of Federal Contract Compliance Programs U.S. Department of Labor Office of Federal Contract Compliance Programs 2021 https:/ /www.dol.gov/agencies/ofccp/faqs/practical-significance Cambridge_Dictionary_no n-discrimination non-discrimination Cambridge Dictionary Cambridge Dictionary https:/ /dictionary.cambridge.org/us/dictionary/english/non-discrimination Signal_Detection_Theory Signal Detection Theory N.A. Macmillan International Encyclopedia of the Social & Behavioral Sciences 14075-14078 2001 https:/ /doi.org/10.1016/B0-08-043076-7/00677-X Techopedia_kill_switch Kill Switch Techopedia Techopedia 2019 https:/ /www.techopedia.com/definition/4001/kill-switch Batya_Friedman_VSD_Intr oduction Introduction Batya Friedman and David G. Hendry Value Sensitive Design: Shaping Technology with Moral Imagination 1-17 2019 https:/ /doi-org.proxy.library.georgetown.edu/10.7551/mitpress/7585.003.0002 C3.ai_feedback_loop What Is a Feedback Loop? C3.ai C3.ai https:/ /c3.ai/glossary/features/feedback-loop/ Collins_Dictionary_ground _truth ground truth Collins Dictionary Collins Dictionary https:/ /www.collinsdictionary.com/us/dictionary/english/ground-truth Wikipedia_Decision-making Decision-making Wikipedia Wikipedia https:/ /en.wikipedia.org/wiki/Decision-making Shevlin_et_al_2019 The limits of machine intelligence Henry Shevlin, Karina Vold, Matthew Crosby, and Marta Halina EMBO Reports 20 10 e49177 2019 https:/ /www.ncbi.nlm.nih.gov/pmc/articles/PMC6776890/ EEOC_ADA_AI The Americans with Disabilities Act and the Use of Software, Algorithms, and Artificial Intelligence to Assess Job Applicants and Employees U.S. Equal Employment Opportunity Commission U.S. Equal Employment Opportunity Commission 2022 https:/ /www.eeoc.gov/laws/guidance/americans-disabilities-act-and-use-software-algorithms-and-artificial-intelligence Merriam-Webster_screen_out screen out Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/screen%20out apa_experiment_2023 experiment American Psychological Association (APA) APA Dictionary of Psychology 2023 https:/ /dictionary.apa.org/experiment APA_DoP_laboratory_rese arch laboratory research American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/laboratory-research UNODC_Glossary_QA_GL P Glossary of Terms for Quality Assurance and Good Laboratory Practices Laboratory and Scientific Section of the United Nations Office on Drugs and Crime Glossary of Terms for Quality Assurance and Good Laboratory Practices 2009 https:/ /www.unodc.org/documents/scientific/ST_NAR_26_E.pdf World_Wide_Words_In_si lico In silico World Wide Words World Wide Words http:/ /www.worldwidewords.org/weirdwords/ww-ins1.htm Bassiouni_Baffes_Evrard An Appraisal of Human Experimentation in International Law and Practice: The Need for International Regulation of Human Experimentation M. Cheriff Bassiouni, Thomas G. Baffes, and John T. Evrard Journal of Criminal Law and Criminology 72 4 1597-1666 1981 https:/ /scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=6276&context=jclc Merriam-Webster_amplify amplify Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/amplify Gupta_et_al_HAR_2022 Human activity recognition in artificial intelligence framework: a narrative review Neha Gupta, Suneet K. Gupta, Rajesh K. Pathak, Vanita Jain, Parisa Rashidi, and Jasjit S. Suri Artificial Intelligence Review 55 4755â€“4808 2022 https:/ /link.springer.com/article/10.1007/s10462-021-10116-x Merriam-Webster_annotate annotate Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/annotate Freeman_et_al_2014 Active learning increases student performance in science, engineering, and mathematics Scott Freeman, Sarah L. Eddy, Miles McDonough, Michelle K. Smith, Nnadozie Okoroafor, Hannah Jordt, and Mary Pat Wenderoth PNAS 111 23 8410-8415 2014 https:/ /www.pnas.org/doi/full/10.1073/pnas.1319030111 AI_Assurance_2022 Assuring AI methods for economic policymaking Anderson Monken, William Ampeh, Flora Haberkorn, Uma Krishnaswamy, and Feras A. Batarseh AI Assurance: Towards Trustworthy, Explainable, Safe, and Ethical AI 371-428 2022 https:/ /www.google.com/books/edition/AI_Assurance/dch6EAAAQBAJ?hl=en&gbpv=1&dq=% 22Large+language+models+LLMs+are+a+class+of+language+models+that+use+deep+learning+algorithms+and+are+trained+on+extremely+large+textual+da tasets+that+can+be+multiple+terabytes+in+size%22&pg=PA376&printsec=frontcover Poore_Lawrence_ARLIS_2 023-01 AI Engineering: An Academic Research Roadmap Joshua Poore and Craig Lawrence Applied Research Laboratory for Intelligence and Security (ARLIS) 2023 Survey_of_Hallucination_i n_NLG Survey of Hallucination in Natural Language Generation Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung ACM Computing Surveys 55 12 1-38 2023 https:/ /dl.acm.org/doi/10.1145/3571730 APA_clustering clustering American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/clustering APA_content_validity content validity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/content-validity ISO_9241-11Ê¼2018 ISO 9241-11Ê¼2018(en) Ergonomics of human-system interaction â€” Part 11Ê¼ Usability: Definitions and concepts ISO ISO Online Browsing Platform 2018 https:/ /www.iso.org/obp/ui/#iso:std:isoÊ¼9241Ê¼-11Ê¼ed-2Ê¼v1Ê¼en APA_criterion_validity criterion validity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/criterion-validity APA_data_analysis data analysis American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/data-analysis APA_decision_making decision making American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/decision-making Lehto_Nanda_2021 Decision-Making Models, Decision Support, and Problem Solving Mark R. Lehto and Gaurav Nanda Handbook of Human Factors and Ergonomics , Fifth Edition 159-202 2021 https:/ /www.wiley.com/en-us/Handbook+of+Human+Factors+and+Ergonomics%2C+5th+Edition-p-9781119636083 Baron_Thinking_and_Deci ding Jonathan Baron Thinking and Deciding 2008 EO_DEIA_2021 Executive Order on Diversity, Equity, Inclusion, and Accessibility in the Federal Workforce Joseph R. Biden Jr. The White House 2021 https:/ /www.whitehouse.gov/briefing-room/presidential-actions/2021/06/25/executive-order-on-diversity-equity-inclusion-and-accessibility-in-the-federal-workforce/ APA_ethics ethics American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/ethics APA_external_validity external validity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/external-validity CSRC_false_negative False Negative NIST CSRC Information Technology Laboratory Computer Security Resource Center Glossary https:/ /csrc.nist.gov/glossary/term/false_negative CSRC_false_positive False Positive NIST CSRC Information Technology Laboratory Computer Security Resource Center Glossary https:/ /csrc.nist.gov/glossary/term/false_positive Wilke_Mata_2012 Cognitive Bias A. Wilke and R. Mata Encyclopedia of Human Behavior 1 531-535 2012 https:/ /s3.amazonaws.com/arena-attachments/557491/b16d97da35ed37a0a022e806cc931a0d.pdf AIID_incident_response Defining an \"AI Incident Response\" Sean McGregor Artificial Intelligence Incident Database https:/ /incidentdatabase.ai/research/5-response/ APA_integrity integrity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/integrity APA_internal_validity internal validity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/internal-validity APA_learning learning American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/learning McNamara_Fallacy The McNamara Fallacy Jonathan Cook The McNamara Fallacy 2023 https:/ /mcnamarafallacy.com/ Creswell_Clark_mixed_me thods John W. Creswell and Vicki L. Plano Clark Designing and Conducting Mixed Methods Research , Third Edition 2017 APA_observation observation American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/observation ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nGlossary_of_Statistical_Te rms Glossary of Statistical Terms Philip B. Stark SticiGui 2019 https:/ /www.stat.berkeley.edu/~stark/SticiGui/Text/gloss.htm Wikipedia_RMSD Root-mean-square-deviation Wikipedia Wikipedia https:/ /en.wikipedia.org/wiki/Root-mean-square_deviation APA_recognition recognition American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/recognition APA_recall recall American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/recall APA_stereotype stereotype American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/stereotype Augoustinos_Walker_1998 The Construction of Stereotypes within Social Psychology: From Social Cognition to Ideology Martha Augoustinos and Iain Walker Theory & Psychology 8 5 629-652 1998 https:/ /doi.org/10.1177/0959354398085003 APA_autonomy autonomy American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/autonomy Charmaz_Henwood Grounded Theory Methods for Qualitative Psychology Kathy Charmaz and Karen Henwood The SAGE Handbook of Qualitative Research in Psychology 238-255 2017 https:/ /doi.org/10.4135/9781526405555 APA_reflexivity reflexivity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/reflexivity Lee_See_2004 Trust in Automation: Designing for Appropriate Reliance John D. Lee and Katrina A. See Human Factors: The Journal of the Human Factors and Ergonomics Society 46 1 50-80 2004 https:/ /doi.org/10.1518/hfes.46.1.50_30392 Mayer_Davis_Schoorman_ 1995 An Integrative Model of Organizational Trust Roger C. Mayer, James H. Davis, and F. David Schoorman The Academy of Management Review 20 3 709-734 1995 https:/ /doi.org/10.2307/258792 NISTIR_8280 NISTIR 8280. Face Recognition Vendor Test (FRVT). Part 3Ê¼ Demographic Effects Patrick Grother, Mei Ngan, and Kayee Hanaoka NIST 2019 https:/ /nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8280.pdf Usabilitygov Usability Testing Usability.gov Usabillity.gov https:/ /www.usability.gov/how-to-and-tools/methods/usability-testing.html Encyclopedia. com_underrepresentation Underrepresentation Encyclopedia.com Encyclopedia.com https:/ /www.encyclopedia.com/social-sciences/applied-and-social-sciences-magazines/underrepresentation Arham_Islam_History_202 3 A History of Generative AI: From GAN to GPT-4 Arham Islam MarkTechPost 2023 https:/ /www.marktechpost.com/2023/03/21/a-history-of-generative-ai-from-gan-to-gpt-4/ McKinsey_generative_AI What is generative AI? McKinsey & Company McKinsey & Company 2023 https:/ /www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai Seshia_et_al_2022 Toward Verified Artificial Intelligence Sanjit A. Seshia, Dorsa Sadigh, and S. Shankar Sastry Communications of the ACM 65 7 46-55 2022 https:/ /cacm.acm.org/magazines/2022/7/262079-toward-verified-artificial-intelligence/fulltext Liam_Tung_2022_Meta_h allucination Meta warns its new chatbot may forget that it's a bot Liam Tung ZDNet 2022 https:/ /www.zdnet.com/article/meta-warns-its-new-chatbot-may-not-tell-you-the-truth/ Merriam-Webster_requirement requirement Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/requirement TTC6_Taxonomy_Terminol ogy EU-U.S. Terminology and Taxonomy for Artificial Intelligence - Second Edition EU-US Trade and Technology Council (TTC) Working Group 1 (WG1) [1] Add citation to citations sheet and only list ID in these columns [2] Make sure the spelling matches another term (value in A column)", "fetched_at_utc": "2026-02-08T18:52:01Z", "sha256": "ca7880ea8842efa0458e4251b29a081d20a53817c005bc4bf63025d7b567d69e", "meta": {"file_name": "The Language of Trustworthy AI Glossary - NIST.pdf", "file_size": 640715, "relative_path": "pdfs\\The Language of Trustworthy AI Glossary - NIST.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 59669}}}
{"doc_id": "pdf-pdfs-the-protect-framework-managing-data-risks-in-the-ai-era-oliver-patel-56aa43c63ee4", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel.pdf", "title": "The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era \n\nhttps://oliverpatel.substack.com/p/the-protect-framework-managing-data  2/20 This weekâ€™s newsletter presents, for the first time, my PROTECT Framework for Managing Data Risks in the AI Era Â© 2025. Last week I published Part 1 of my 2-part series on the Top 10 Challenges for AI Governance Leaders in 2025. It focused on how the democratisation of AI, coupled with the sheer volume and rapid velocity of AI initiatives, is putting serious strain on the enterprise AI governance function. It outlined how, in response to these challenges, AI governance leaders must refine and update their risk-based approach and narrow the focus of their teamsâ€™ work, to avoid being overwhelmed, distracted, or neglecting the most serious risks. Although I promised Part 2 this week, I ended up writing a full article on the fourth challenge: protecting confidential business data. This is a hugely important topic and there was simply too much to say. The beauty of having your own Substack is that you can follow whichever creative or intellectual direction most appeals to you, rather than rigidly sticking to prior plans. I hope that you will indulge my partial detour and that you find value in this article for your work. The â€œrealâ€ Part 2, covering challenges 5-10, will have to wait another week. \n\nChallenge 4. Protecting Confidential Business Data \n\nHow can enterprises protect confidential business data when there is immense hunger to   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 3/20\n\nexperiment with and use the latest AI applications that are released on the market? \n\nThe generative AI boom has amplified and exacerbated a plethora of data risks that all enterprises are exposed to. It has never been more important to ensure that your AI governance, cybersecurity, and information security frameworks are designed to, and capable of, mitigating these risks. However, designing, implementing, and scaling robust controls that actually protect your organisationâ€™s data and intellectual property requires precise understanding of what these risks are and how they are impacted by AI. Thatâ€™s where my PROTECT Framework comes in. The PROTECT Framework empowers you to understand, map, and mitigate the most pertinent data risks that are fuelled by widespread adoption of generative AI. Below is a high-level summary of the framework, followed by a detailed breakdown of each of the 7 themes. \n\nPROTECT: Managing Data Risks in the AI Era \n\nThe PROTECT Framework focuses primarily on protecting ( no surprises there )confidential business data from exposure, disclosure, and misuseâ€”as well as associated data privacy and security risks fuelled by AI. It also outlines how   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 4/20\n\norganisations can use data in a compliant way, in the context of AI development, deployment, and use. \n\nP - Public AI Tool Usage R - Rogue Internal AI Projects O - Opportunistic Vendors T - Technical Attacks and Vulnerabilities E - Embedded Assistants and Agents C - Compliance, Copyright, and Contractual Breaches T - Transfer Violations \n\nThe rest of the article breaks down each of the seven themes, highlighting both the core risks that enterprises are exposed to, as well as practical mitigations that can be implemented to manage and control these risks. My forthcoming book, Fundamentals of AI Governance , provides a comprehensive visual overview of the PROTECT Framework, as well as a deep-dive on the Top 10 AI   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 5/20\n\nRisks impacting the modern enterprise. To secure a 25% discount during the pre-launch period, sign up at the link below. \n\nEnterprise risks: Use of publicly available AI tools is arguably the most severe data risk, because of how easy it is to do and how difficult it is to prevent. Simply put, there are thousands of publicly available AI tools that anyone can access via the internet, most of which are free or cheap. As well as mainstream generative AI chatbots like Claude, Gemini and ChatGPT, there are countless AI tools for creating presentations, managing email inboxes, transcribing meetings, and generating videos. No matter how mature your enterprise AI capabilities areâ€”and even if you are a frontier AI companyâ€” it is not going to be feasible to keep up with the latest and greatest AI tools and AI models that are released on the market each day, whilst also performing robust due diligence on AI vendors. Even with enterprise-grade licenses to mainstream generative AI services, you will not always get immediate access to the latest features included in the consumer version. This fuels immense hunger for employees to experiment with and use the most cutting-edge AI tools, irrespective of whether they are â€œinternalâ€ and approved or â€œpublicly availableâ€ and unapproved. \n\nP - Public AI Tool Usage   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 6/20\n\nThis inability to keep pace with the market, coupled with a lack of awareness regarding the risks of using publicly available AI tools and the pressure that employees and teams are under to become â€œAI-firstâ€, exacerbates the risks. Many employees may not understand the difference, from a data risk perspective, between using internal AI tools and public AI tools. But the risk is real. For example, when enterprise data is shared with publicly available AI tools, the organisation no longer has any control over what happens to it. This confidential business data could be used to train the AI models that power publicly available AI tools and in turn be disclosed, via future AI outputs, to competitors or malicious actors. It may even end up on the public internetâ€”as we saw when various shared AI chat logs were indexed and publicly accessible onlineâ€”or be retained indefinitely, as a result of court orders like the one OpenAI faced from the New York Court. Simply put, if you want to be in control of how your data and intellectual property is used, who has access to it, and for how long it is retained, your employees should avoid using publicly available AI tools. \n\nPractical mitigations: Although shadow AI use is ubiquitous, there are various controls you can implement to mitigate this risk. My 3 Gs for Governing AI Democratisation offers a useful starting point: \n\nGuidance: educate and train the workforce on the risks of using publicly available AI tools and the importance of protecting confidential business data.   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 7/20\n\nGreenlight: provide access to secure, best-in-class AI tools, platforms, and capabilities that are approved for internal use and can process confidential business data. \n\nGuardrails: implement guardrailsâ€”including technical and legal mitigationsâ€”to mitigate outstanding data risks that the use of internally approved AI tools entails. This can include scanning and blocking certain types of data from being uploaded as an input or generated as an output. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nEnterprise risks: Bypassing governance, especially when it happens at scale, creates compliance blind spots. Various risks emerge, and are difficult to manage, when teams develop and deploy AI systems, or procure AI solutions from vendors, without adhering to the mandatory AI governance, privacy, and cyber security processes. In such scenarios, there is unlikely to have been any legal or compliance review, privacy assessment, or security evaluation. In turn, this means that genuine risks are \n\nR - Rogue Internal AI Projects   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 8/20\n\nunlikely to be understood, required documentation and artefacts may not have been produced, and robust mitigations will not be in place to address any important risks. This increases organisational technical debt, which can lead to costly and burdensome efforts to retrospectively re-engineer non-compliant AI systems that are already deployed in production. Finally, it increases the likelihood that data is used without authorisation, and in a manner that constitutes a contractual breach or potential compliance violation. In most cases, this does not happen because of malicious internal actors or intentional rule-breaking. Rather, it is more likely due to enthusiasm, competitive internal pressures and competing priorities, or a lack of awareness of internal governance processes and how to navigate them. \n\nPractical mitigations: To mitigate the risk of rogue internal AI projects bypassing compliance checks, a proportionate degree of oversight must be applied to all AI projects. The level of governance scrutiny and oversight should flex in relation to the type of data being used. The use of sensitive personal data, confidential business data, or copyright protected material should entail more rigorous oversight, both at the project outset and throughout the AI lifecycle. Oversight should also be more rigorous for scenarios that involve sharing data with external vendors and the applications they provide.   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 9/20\n\nThe most important thing you can do is to make your AI and digital governance processes as easy to navigate as possible, by integrating different processes where possible, providing accessible guidance and support, and using automation to streamline processes and improve the user experience. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nEnterprise risks: Almost all enterprises must work with, and procure from, external organisations to progress with their AI ambitions. In the generative AI era, the trend is from build to buy. Whether it is leveraging pre-trained foundation models and generative AI chatbots, or working with vendors that provide bespoke AI products, exposure to third-party AI risk is unavoidable. In some cases, AI vendors and service providers may seek to use your confidential business data to train, develop, and improve their AI models and servicesâ€”potentially without your explicit knowledge or consent. \n\nO - Opportunistic Vendors   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 10/20\n\nThe terms of service for many AI platforms and products are ambiguous or difficult to understand, and grant vendors broad rights over customer data. The key risk is whether your organisationâ€™s data is used to train AI models that other customers can access and use. If so, competitors (or any other organisation) using that same vendorâ€™s products and services may benefit from insights derived from your data. The risk is lowerâ€”or potentially fully mitigatedâ€”if your data is only used to train AI models and services that only your organisation has access to. This can enable you to benefit from feature improvements and customisation whilst mitigating data exposure and leakage risks. \n\nPractical mitigations: Consider contractually prohibiting AI vendors from using your data (e.g., prompt, input, log, and output data), to train AI models and improve services that are accessible to other customers. This requires robust vendor due diligence, and a specific AI governance process pathway for AI procurement. It also requires clear guidance and training on third-party AI risks, acceptable data use terms, as well as template contracts and addendums that can be used across the business. Although the demand for AI vendors (and the products they provide) is high, the presence of opportunistic or shady operators in the marketâ€”and the immense value of the data they can obtain from enterprise customersâ€”makes rigorous due diligence and contractual safeguards essential.   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 11/20\n\nEnterprise risks: AI systems introduce novel attack vectors that are linked to the distinct vulnerabilities of these systems. In particular, generative AI and agentic AI systems can be compromised and exploited, leading to confidential dataâ€”that was part of the AI modelâ€™s training, input, or output dataâ€”being extracted and stolen. Such data exfiltration is a known and widely documented AI vulnerability and can be caused by various attack methods. Prompt injection attacks, for example, are when an AI model is provided with malicious inputs (during inference) that are designed to manipulate and steer the outputs it generates, by jailbreaking model guardrails. Simply put, the goal of prompt injection is to make the AI model do something that it is not supposed to. This includes, but is not limited to, data exfiltration, as well as reconstruction of training data. This risk of prompt injection is amplified with agentic AI, given the ability of AI agents to use tools and execute actions that can have a material impact (rather than â€œjustâ€ generate outputs for consumption). \n\nPractical Mitigations: Although there are no foolproof mitigations against prompt injection attacks, there are nonetheless important steps you can take. Consider implementing AI system-level security controls and guardrails including input \n\nT - Technical Attacks and Vulnerabilities   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 12/20\n\nvalidation, prompt sanitisation, output filtering, and incident monitoring and detection. Also, for high risk applications, conduct red-teaming and adversarial testing. Cybersecurity best practice emphasises the importance of multiple overlapping security layers. However, no technical controls can fully prevent prompt injection or data exfiltration. Therefore, carefully control who has access to sensitive AI systems, what data they can access, and what actions agentic AI can execute. \n\nEnterprise risks: The AI assistants and agents that are increasingly embedded in the core workplace software we all use pose novel data risks. In particular, these embedded AI tools can inappropriately disclose and disseminate data to people and groups that were not supposed to have access to it. For example, a personalised AI assistant that summarises your emails and daily tasks can analyse wider organisational data that you have access to, such as document libraries and shared calendars. If that data has not been protected with appropriate file sharing permissionsâ€”and moreover has erroneously been made available to the entire organisationâ€”then elements of it may be surfaced to you via your handy AI assistant. Although the root cause of this is often inappropriate or inadequate file sharing \n\nE - Embedded Assistants and Agents   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 13/20\n\npermissions, AI significantly increases the likelihood of such data being shared with the wrong person. It also provides malicious internal actors with a powerful tool for mischief. AI meeting assistants and note-taking applications are of particular concern. It is commonplace to join calls with external organisations, only to find that a random AI bot is also on the call, recording and transcribing everything that is said. From an enterprise perspective, this is akin to uploading all of this information into a publicly available AI tool (which poses similar risks to those outlined in â€˜Public AI Tool Usage â€™), unless you have assurances from the external organisation regarding the technology they are using and how it processes your data. Furthermore, be wary of individuals having access to AI meeting recordings and transcripts of parts of the discussion they were not part of. Increasingly autonomous agentic AI systems exacerbate these risks. In order to effectively determine the best course of action and use tools to execute tasks, LLM-based AI agents will need to mine, retrieve from, and synthesise myriad enterprise data sources. Establishing appropriate access controls, and maintaining AI agent audit trails, will become increasingly complex yet important. \n\nPractical mitigations: Robust data governance and data risk management is critical to ensuring your increasingly autonomous AI tools do not cause havoc. Ensuring   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 14/20\n\nappropriate file sharing permissions for sensitive and critical data sources is paramount, given the ways in which AI agents can mine through your document libraries and other repositories, as well as the obvious value this capability provides. Also, when deploying agentic AI, start with lower risk use cases, applications, and data sources. Furthermore, apply the principle of least privilege, to ensure that AI agents only have access to data that is necessary for their tasks. \n\nEnterprise risks: Data science, AI, and business teams are under significant pressure to leverage AI to deliver value for the business. To do so, they require seamless access to vast amounts of high-quality, business-critical data. However, the increasingly stringent data and AI regulatory landscapeâ€”particularly in the EUâ€”creates numerous compliance risks when using data for AI activities. It is therefore crucial to have robust controls in place to prevent unauthorised or non-compliant use of data. The most important regulatory and legal domains to consider are: \n\nPrivacy and data protection: Privacy and data protection laws restrict the way in which personal dataâ€”in particular sensitive personal dataâ€”can be used. For example, under the EUâ€™s GDPR, you must have a lawful basis to process personal data. Personal \n\nC - Compliance, Copyright, and Contractual Breaches   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 15/20\n\ndata processing is therefore only lawful if at least one of the following lawful bases apply: i) consent, ii) performance of a contract, iii) legal compliance, iv) protection of vital interests, v) performance of a task in the public interest, or vi) legitimate interests. Therefore, just because you have access to personal data, does not mean you are permitted by default to use it to develop or deploy AI. \n\nCopyright and intellectual property: Organisations must be cautious when using external data for AI development and deployment, as it is often copyright protected. Different data sources come with different licenses, terms, and conditions. This cautiousness must extend to â€œeverydayâ€ employee use of generative AI toolsâ€”in particular their prompts and document uploads. \n\nContracts: Your organisation may have access to data that another organisation provided in the course of an engagement, such as the use of a product or service you provide, that is governed by a bespoke legal agreement. Therefore, you must protect and handle that data in accordance with the applicable legal agreement. \n\nAI-specific laws: Finally, AI regulations like the EU AI Act typically include provisions that stipulate how data should be used in the context of AI activities. For example, the AI Act requires providers of high-risk AI systems to use high quality, accurate, and â€œrepresentativeâ€ training, validation, and testing data sets, in order to mitigate bias risks and promote reliable AI performance.   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 16/20\n\nPractical mitigations: The legal and regulatory domains outlined above are vast; comprehensive risk mitigation across all of them is beyond the scope of the PROTECT Framework. However, the overarching principle is that you must embed proportionate governance and oversight throughout the AI lifecycle, to prevent non-compliant or unauthorised data use. This means legal and compliance review must occur at critical stages, including before data is sourced, before AI models are trained, and before AI systems are deployed in production or released on the market. As ever, this governance must be complemented and reinforced by company-wide and role-specific training. When these governance checkpoints are bypassedâ€”as discussed in the â€˜Rogue Internal AI Projectsâ€™ themeâ€”the aforementioned data-related compliance and legal risks materialise. \n\nEnterprise risks: Leveraging cloud-based AI services, such as platforms for accessing and using foundation models, almost always involves international data transfers. Given that AI processing is rarely confined to one jurisdiction, navigating international data transfer compliance is an important part of AI governance. \n\nT - Transfer Violations   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 17/20\n\nPrivacy and data protection regimes worldwide restrict the way in which personal data can be transferred internationally. Under the GDPR, organisations can transfer personal data freely from the EU to entities in a non-EU country if there is an EU adequacy decision in place. An adequacy decision is the EUâ€™s way of â€œ protecting the rights of its citizens by insisting upon a high standard of data protection in foreign countries where their data is processed â€. 15 jurisdictions are recognised as â€œadequateâ€ by the EU. This includes the U.S.â€”but it only applies to commercial organisations participating in and certified under the EU-U.S. Data Privacy Framework. If there is no EU adequacy decision, alternative legal safeguards must be put in place (e.g., Standard Contractual Clauses), before personal data can be transferred from the EU to the non-EU jurisdiction. On a separate note, the U.S. â€œBulk Data Transfer Ruleâ€ prohibits or restricts organisations from transferring â€œU.S. sensitive personal dataâ€ and â€œgovernment-related dataâ€ to â€œcountries of concernâ€, including China, Cuba, Iran, North Korea, Russia, and Venezuela. The Rule was issued by the Department of Justice in January 2025. \n\nPractical mitigations: Although the above was far from an exhaustive overview of international data transfer regulations, the key point is that you must implement specific mitigationsâ€”as required by the applicable lawâ€”prior to transferring personal data across borders. For example, before onboarding U.S. AI vendors and service", "fetched_at_utc": "2026-02-08T18:52:03Z", "sha256": "56aa43c63ee41a83ad105145f93f00e6c12e4aa66027e4251dce4277262ee1b3", "meta": {"file_name": "The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel.pdf", "file_size": 931949, "relative_path": "pdfs\\The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4756}}}
{"doc_id": "pdf-pdfs-the-ultimate-guide-to-ai-literacy-oliver-patel-bda94abbbf34", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The Ultimate Guide to AI Literacy - Oliver Patel.pdf", "title": "The Ultimate Guide to AI Literacy - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel \n\nhttps://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy  2/20 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, subscribe below and share it with your colleagues and friends. Thank you for your support! This weekâ€™s edition is a practical guide to implementing and scaling an impactful AI literacy programme. It covers: \n\nâœ… What is AI literacy and why does it matter? \n\nâœ… What does the EU AI Act require? \n\nâœ… The 4 Layers of AI Literacy (the â€˜whatâ€™) \n\nâœ… 8 Practical Tips for AI Literacy Success (the â€˜howâ€™) \n\nâœ… The 3 Es for Impact: Educate, Engage, Empower (the â€˜whyâ€™) \n\nâœ… AI Literacy Cheat Sheet - scroll to the end to download the high-res pdf! \n\nPlease note that this analysis is geared towards larger organisations in the private and public sector, as opposed to educational institutions like schools and universities, which have a unique set of challenges and considerations.   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 3/20\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week. \n\nAI literacy is a crucial part of modern business in 2025. Any organisation investing in AI technology and implementation will struggle to maximise the value of AI without embedding AI knowledge, skills, and understanding across its workforce. If people donâ€™t know how to use AI, you are not going to achieve ROI. According to McKinsey , 48% of employees rate training as the most important factor driving their adoption of generative AI. However, many feel that they receive inadequate support. AI literacy means educating and upskilling employees on AI. A comprehensive, dual approach to AI literacy focuses on both AI risks and opportunities. Specifically, AI literacy deepens understanding of how to anticipate and mitigate AI risks, avoid harms, comply with regulations, and use AI in a safe and responsible way. It also advances \n\nWhat is AI literacy and why does it matter?   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 4/20\n\nunderstanding of what AI is, what it can and cannot do, how the technology is developing, and how it can be used in a positive, impactful, and transformative way. AI literacy matters because AI is far more sophisticated and complex than most other tools and technologies we use. AI can mimic, replicate, and even surpass the cognitive, intellectual, and creative abilities of human experts across an infinite number of tasks and topics, from coding to poetry. Across hundreds of thousands of years of human history, this has never before been possible. Therefore, unlike many other tools and technologies we use each day, safe, responsible, effective, and impactful use of AI is far from obvious or intuitive. It requires insight, understanding, and the curiousity to keep up with the latest developments. According to DataCamp :62% of leaders believe AI literacy is important for their teamsâ€™ daily tasks. Bear in mind this covers leaders working across many business functions, not just AI and data. â€˜Basic understanding of AI conceptsâ€™ and â€˜AI ethics and responsible AI best practicesâ€™ are ranked by leaders (70% and 69% respectively) as the two most important AI skills for their teams.   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 5/20\n\nArticle 4 of the EU AI Act is simply titled â€˜AI literacyâ€™. This humble article is undeniably a big part of why this has become such a hot topic. AI literacy is now mandatory. It was one of the first EU AI Act provisions to become applicable, alongside prohibited AI practices, on 2 February 2025. Here is the full text of Article 4: \n\nProviders and deployers of AI systems shall take measures to ensure, to their best extent, a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, education and training and the context the AI systems are to be used in, and considering the persons or groups of persons on whom the AI systems are to be used. \n\nAI literacy is also defined in Article 3(56) of the EU AI Act: \n\nâ€˜AI literacyâ€™ means skills, knowledge and understanding that allow providers, deployers and affected persons, taking into account their respective rights and obligations in the context of this Regulation, to make an informed deployment of AI systems, as well as to gain awareness about the opportunities and risks of AI and possible harm it can cause. \n\nWhat does the EU AI Act require?   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 6/20\n\nHere is a simplified breakdown of what this means and what you need to do. Organisations which develop and use AI must ensure their workforceâ€”especially those who develop, use, and operate high-risk AI systemsâ€”have the requisite skills, knowledge, and understanding to enable AI risk mitigation, regulatory compliance, and the protection of people from potential harms and other negative impacts of AI. Although organisations will not be fined for failure to comply with the AI literacy provision in Article 4 (as it is not explicitly covered in the penalty and enforcement regime), such non-compliance will certainly not help in the event of investigations, enforcement action, or legal proceedings relating to other EU AI Act provisions. Furthermore, it will be practically impossible to comply with many other aspects of the EU AI Act without implementing robust AI literacy. For example, Article 14 obliges deployers to: assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support .This effectively mandates tailored, role-based training for those tasked with oversight of high-risk AI systems. Although AI literacy should be interpreted as applying to the whole organisation, this is a key part of it. \n\nThe 4 Layers of AI Literacy   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 7/20\n\nThe 4 Layers of AI Literacy is a framework which I developed that describes the â€˜whatâ€™ of AI literacy. For your AI literacy programme to be robust, compliant, and effective, all 4 of these elements must be part of it. \n\n1. AI governance fundamentals \n\na. Mandatory training for the entire organisation. \n\nb. Educate the workforce on the key pillars of responsible AI and the AI policies and processes. The high level Doâ€™s and Donâ€™ts. \n\nc. Must be straightforward, accessible, and easy for all to understand. \n\n2. Generative AI empowerment \n\na. Upskill and empower the workforce to adopt and embrace AI tools and technologies, from generative AI to agentic AI. \n\nb. Incentivise uptake via gamification and leverage external expertise and resources. \n\nc. Must be inspirational, interactive, and hands-on. Provide safe environments for experimentation and discovery. \n\n3. Persona and role-based training   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 8/20\n\na. Tailored training for specific personas who build, buy, use, deploy, or govern AI as a core part of their work. \n\nb. Target key personas including AI governance, privacy, procurement, data scientists, and IT business partners. \n\nc. Must be engaging, practical, and relevant for the role, as well as likely scenarios which will arise. \n\n4. AI system specific \n\na. Mandatory training for end users and others responsible for operating high-risk AI systems. \n\nb. Bespoke instructions and guidance on implementing human oversight, transparency, and other risk mitigation measures. \n\nc. Must be context-specific, ensuring end users can interpret AI outputs and detect serious incidents. \n\nd. Requires collaboration between providers and deployers. AI literacy is about much more than generic company-wide training. Although this is important for compliance, it is only the first layer. For example, without implementing Layer 4, you cannot comply with the AI Act's human oversight requirements.   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 9/20\n\nThis AI literacy framework is not intended to cover absolutely everything. It should be complementary to relevant educational initiatives focusing on data literacy, cyber security awareness, ethics training etc. Now that you know what your AI literacy programme should consist of, here are 8 practical tips that will help you succeed. These tips are the â€˜howâ€™ of AI literacy. \n\n1. Focus on the â€˜so whatâ€™. Identify the end goal of AI literacy, then reverse engineer it. There is no point in rolling out AI literacy merely for the sake of it. In the planning phase, carefully consider what you are hoping to achieve and design your programme accordingly. Consider the skills, capabilities, and knowledge you need to embed and the type of organisation you want to become. Critical objectives should include: ensuring all employees have a baseline understanding of responsible and compliant use of AI; driving understanding regarding what AI is, how to use it effectively, which AI tools and technologies are available, and how to identify promising use cases; \n\n8 Practical Tips for AI Literacy Success   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 10/20\n\nproviding employees with high-quality training and development opportunities, to boost morale, engagement, and retention; and preparing the workforce for the jobs of the future and fostering the skills required to succeed. A world-class AI literacy programme will be designed and structured to achieve all of these objectives. But if your resources are constrained, start with the first. \n\n2. Diversify your content, formats, and pathways. Combine live, self-paced, virtual, and in-person learning. We have never had access to more high-quality, engaging, and free educational content. Your audienceâ€”whose attention you are competing forâ€”has high standards for what keeps them engaged. Therefore, your content needs to be at least as good, if not better than, what can be found externally. The best thing you can do is diversify. Offer a range of different types of content, including live keynotes, panel discussions, workshops, in-person events and conferences, and online modules with videos, infographics, and synthesised materials. Traditional teaching and learning methods are becoming archaic in the era of YouTube, TikTok, DuoLingo, and AI coaching apps; meet your learners where they are. \n\n3. Leverage internal and external experts. Partner with leading educational institutions. There are thought leaders and experts all over your organisation. Your job is to find them and give them a voice. Your AI upskilling initiatives represent an ideal opportunity to provide a platform to individuals and teams   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 11/20\n\nleading on impactful and transformative work. Being proactive in convening a broad range of voices is win-win. Your learners will benefit from a richer experience and you will forge strong relationships with key stakeholders across different functions, who will appreciate the opportunity for visibility. You should also mix it up by bringing in external experts, who can offer thought-provoking, challenging, and even divergent views. This could be leading academics and researchers, policymakers and regulators, or industry leaders from other sectors. \n\n4. Gamify your learning pathways to drive engagement and incentivise uptake. \n\nGamification, defined as â€œusing game design elements in non-game contextsâ€, is an increasingly popular approach to designing and delivering effective learning experiences, in both educational and professional settings. Gamification covers a broad range of different techniques, including points, badges, and leaderboards (PBL), levels, feedback, challenges, and even missions. Many studies prove that introducing such elements leads to more engaged, motivated, and proactive learners. To keep your learning participants inspired and excited, and to generate momentum across the organisation, offer badges, accreditations, and awards, for a more meaningful and rewarding learning journey. \n\n5. Hands-on, practical, and interactive learning is always most effective and impactful. If it's not hands on, it will quickly be forgotten. The biggest flaw with most AI trainings is that participants do nothing with AI. This makes no sense. You need to be creative in incorporating practical and scenario-based activities, which   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 12/20\n\ngive people a chance to experiment with AI tools, understand their capabilities and limitations, and tackle realistic problems and challenges. Simply learning how to 'prompt' is not going to cut it in 2025. Get people involved in deeper activities, such as hackathons, AI model red-teaming, reviewing live cases, AI incident response, use case ideation, and product development workshops. \n\n6. Enable tailored, role-based learning to be provided for every function and business unit. Different people and teams will require specialisation in different aspects of AI and AI governance, based upon the core focus of their role or work. You should offer advanced pathways which cater to different audiences. You should also provide frameworks, tools, and resources which enable every function to develop bespoke, role-based AI literacy programmes, building on the company-wide curriculum. You wonâ€™t have the bandwidth to create training for every group, but you can enable and empower this work to be done by others (e.g., local L&D teams). \n\n7. Build a community of engaged learners and create spaces for vibrant discussions. Community building is the best way you can elevate AI literacy beyond a training programme. Content on AI is abundant and never-ending. There is no shortage of educational materials for people to study and consume. What is harder to come by is opportunities for meaningful and in-depth discussions, with people from many different geographies, departments, roles, and backgrounds. Given the concurrently high levels of concern, confusion, and   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 13/20\n\nexcitement about the impact of AI on the future of work, keeping the discussion going and encouraging this collaboration is key. \n\n8. Provide successful learners with tangible opportunities for impact and career growth. There should be ample opportunities for employees to apply what they have learnt, to benefit their career and the wider organisation. Donâ€™t just reward and acknowledge with certificates and badges, without providing a meaningful follow up or next step. Connect AI literacy and upskilling achievements with opportunities for promotion, career advancement, and change. For example, empower people with specific roles or leadership positionsâ€”such as mentoring and guiding others, chairing forums, championing AI initiatives, and leading on parts of future AI literacy initiativesâ€”which they can perform alongside their day job. Alongside gamification, providing meaningful opportunities for growth and impact is the best way to keep learners engaged. Weâ€™ve covered the â€˜whatâ€™ (The 4 Layers of AI Literacy) and the â€˜howâ€™ (8 Practical Tips for AI Literacy Success). To conclude, we will focus on the â€˜whyâ€™. AI governance is change management. And AI literacy and upskilling is at the heart of driving that cultural change. \n\nThe 3 Es for Impact: Educate, engage, empower   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 14/20\n\nFirst and foremost, AI literacy is about serving others. Your role is to ensure that colleagues across your organisation feel like they are playing an active part in the AI revolution, rather than it being something passively happening to them, or worse, passing them by. You can use my 3 Es for Impact framework to shape and guide your approach, to ensure you fulfil this overall objective of serving others. Prioritising these 3 Es is vital for delivering the meaningful change and impact you hope to achieve, for your organisation and workforce. \n\nEducate: It is challenging to track and keep up with all of the developments in AI and AI governance, even for professionals working at the cutting-edge. It is even harder to cut through the noise and figure out what trends and developments really matter. Therefore, by synthesising and simplifying the vast amount of AI-related educational content which is out there, rendering it digestible, useful, and actionable for your organisation, and outlining the foundational and conceptual basis required for deeper understanding, you are providing immense value. \n\nEngage: Be creative and use all of the tools and resources at your disposal to cultivate buzz and excitement, motivate learning, facilitate community building and, ultimately, engage learners with a diverse range of practical and interactive offerings, delivered by a diverse cast of educators. As an AI governance leader, you cannot bury your head in   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 15/20\n\nthe sand. Being an impactful, visible, and engaged thought leader in your organisation is non negotiable. \n\nEmpower: AI literacy should be an empowerment programme, not a training programme. There is a palpable sense of concern, and even fear, about the impact of AI on the future of careers and work. People are uncertain regarding which jobs will be automated or outsourced, what skills they should develop, and what to prioritise. \n\nAccording to Pew Research , 52% of employed adults are worried about the future impact of AI in the workforce. By providing a safe and inspiring space for people across the organisation to come together and discuss, debate, and collaborate, you are empowering people to shape and take control of their future. This action drives empowerment. \n\nAI Literacy Cheat Sheet", "fetched_at_utc": "2026-02-08T18:52:20Z", "sha256": "bda94abbbf34e7a7e63a114749152367dff0ac99df9da45ac1e4eb58260486ac", "meta": {"file_name": "The Ultimate Guide to AI Literacy - Oliver Patel.pdf", "file_size": 1523395, "relative_path": "pdfs\\The Ultimate Guide to AI Literacy - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4047}}}
{"doc_id": "pdf-pdfs-u-s-ai-law-policy-explained-oliver-patel-77b328bb624a", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\U.S. AI Law & Policy Explained - Oliver Patel.pdf", "title": "U.S. AI Law & Policy Explained - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel \n\nhttps://oliverpatel.substack.com/p/us-ai-law-and-policy-explained  2/27 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! A lot has changed with respect to U.S. AI policy in recent months; and it can be hard for AI governance professionals to keep up. To help navigate these recent shifts, this article provides a comprehensive, up-to-date, and accessible overview of U.S. federal AI law and policy. It does not cover U.S. state AI laws and initiatives, which will be the focus of next weekâ€™s edition of Enterprise AI Governance. Despite lacking comprehensive EU-style regulation, the U.S. does have several important AI laws. In fact, there have been dozens of federal laws, regulations, and initiatives on AI, many of which will be discussed below.   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 3/27\n\nAside from President Bidenâ€™s (ultimately unsuccessful) attempts to initiate private sector AI regulation, the U.S. governmentâ€™s core focus in recent years has been on maintaining and promoting U.S. AI leadership, restricting the export of AI-related technologies, and encouraging responsible and innovative federal government use of AI. The Trump administration, which is primarily concerned with strengthening â€œU.S. global AI dominanceâ€, is pivoting away from the Biden administrationâ€™s AI governance and safety agenda. However, this does not mean that AI governance is completely off the menu. Its importance was stressed in a recent memorandum published by the White Houseâ€™s Office of Management and Budget, which described effective AI governance as â€key to accelerated innovationâ€. This overview covers: \n\nâœ… U.S. global AI leadership and the race with China \n\nâœ… Bidenâ€™s AI governance agenda \n\nâœ… Trumpâ€™s agenda: what â€˜America Firstâ€™ means for AI \n\nâœ… AI export controls and investment restrictions \n\nâœ… Federal government use and acquisition of AI \n\nâœ… NIST and the U.S. AI Safety Institute \n\nâœ… Whatâ€™s coming next?   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 4/27\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week. \n\nThe U.S. is the undisputed global AI leader, by nearly every metric. The global AI industry is dominated by U.S. companies, AI models, and hardware. Here are some stats from Stanfordâ€™s 2025 AI Index Report , to illustrate the point: In 2024, U.S. private investment in AI was $109 billion. In contrast, it was $9.3 billion in China and $4.5 billion in the UK. U.S. organisations produced 40 â€˜notableâ€™ AI models in 2024, significantly more than Chinaâ€™s 15 and Europeâ€™s 3. However, in some areas, China is quickly catching up and the U.S. is taking nothing for granted. For example, although U.S. AI model production output is higher, Chinaâ€™s advanced AI models are getting closer to U.S. models in terms of quality ( see chart below ). \n\nU.S. global AI leadership and the race with China   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 5/27\n\nSource: Stanford AI Index Report 2025 [ original ]\n\nDeepSeek recently demonstrated that it can develop AI models, which have similar performance capabilities to leading U.S. models, at a fraction of the cost. This prompted a sell-off in U.S. tech stocks, with the S&P 500 falling 1.5% on the day DeepSeek released its open-source R1 model.   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 6/27\n\nFurthermore, China is charging ahead in AI talent, research, and patent filing. \n\nIncreasing numbers of â€œtop-tierâ€ AI researchers originate from China ( see chart below ), with the share originating from the U.S. declining in recent years. Also, 300,510 AI-related patents were filed in China in 2024, compared with 67,773 in the U.S.   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 7/27\n\nImage source: Information Technology & Innovation Foundation, 2025 [ original ]\n\nThese trends explain one of the core drivers behind much of the U.S. AI policy agenda in recent years, from AI export controls and investment restrictions, to substantial support for AI infrastructure funding. Although there is no comprehensive federal AI law, like the EU AI Act, there have been a number of federal laws and initiatives, across the past few administrations, which seek to maintain and strengthen the U.S. position of global leadership. Some of the most relevant laws include: \n\nExecutive Order 14141 : Advancing United States Leadership in AI Infrastructure \n\nThis Executive Order was signed by President Biden in January 2025. At the time of writing, it has not been revoked by President Trump. The purpose of this Executive Order is to promote and encourage domestic AI infrastructure development, to â€œprotect U.S. national securityâ€ and â€œadvance U.S. economic competitivenessâ€. This includes using federal sites to build data centres for AI and prioritising clean energy techniques. \n\nCHIPS and Science Act of 2022   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 8/27\n\nEnacted in August 2022, the CHIPS Act ('Creating Helpful Incentives to Produce Semiconductors') was a key pillar of Biden's AI policy. The headline impact of this Act was to authorise and release approximately $280 billion in spending on the hardware components and infrastructure most critical for AI development. \n\nNational AI Initiative Act of 2020 \n\nThis law was enacted in January 2021 as part of the National Defense Authorization Act (NDAA) for Fiscal Year 2021. It provided over $6 billion in funding for AI R&D, education, and standards development, with the ultimate goal of strengthening U.S. AI leadership. This included a mandate which led to NIST developing the NIST AI Risk Management Framework ( discussed below ). The Act also established the National AI Advisory Committee, a high-level group of experts which advise the President on AI policy matters. \n\nExecutive Order 13859 : Maintaining American Leadership in AI \n\nThis Executive Order was signed by President Trump in February 2019. It remains in force today.   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 9/27\n\nThe purpose of this Executive Order is to promote investment and use of AI across the federal government, as well as to â€œfacilitate AI R&Dâ€ and the development of â€œbreakthrough technologyâ€. During the Biden administration, AI governance and safety was a top policy priority. Although this appeared to signal the beginning of a shift away from the historically free market approach to technology regulation adopted by previous U.S. administrations, it did not last for too long. President Biden attempted to balance promoting U.S. global AI leadership with upholding civil liberties and protecting citizens from unfair and harmful practices. As described throughout this article, various federal initiatives designed to strengthen the U.S.â€™ global positionâ€“such as on AI export controls, investment restrictions, and AI infrastructure and manufacturing spendingâ€“were complemented with initial efforts to regulate private sector AI activities and promote responsible AI. The Blueprint for an AI Bill of Rights , for example, was developed by the White House Office of Science and Technology Policy. It outlined Bidenâ€™s AI policy vision. The \n\nBidenâ€™s AI governance agenda   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 10/27\n\nBlueprint was defined by five core principles for AI development and use: i) Safe & Effective Systems, ii) Algorithmic Discrimination Protections, iii) Data Privacy, iv) Notice & Explanation and v) Human alternatives & Fallback. The Blueprint argued that AI systems used in healthcare have â€œproven unsafe, ineffective, or biasedâ€ and algorithms used in recruitment and credit scoring â€œreflect and reproduce existing unwanted inequities or embed new harmful bias and discriminationâ€. President Biden also secured â€˜Voluntary Commitments â€™ from 15 leading AI companies. These included conducting AI model security testing and sharing and publishing information on AI safety. The core purpose of these commitments was to ensure that advanced AI models were safe before being released. Building on these initiatives, Executive Order 14110 : Safe, Secure, and Trustworthy Development and Use of AI was signed by President Biden in October 2023. This represented the most comprehensive U.S. federal AI governance initiative to date. It mandated a major programme of work, entailing over 100 specific actions across over 50 federal entities. Tangible resulting actions included the establishment of the U.S. AI Safety Institute and the publication of various NIST AI safety standards, guidelines, and toolkits ( discussed below ). Also, developers of the most powerful AI models were obliged to perform   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 11/27\n\nsafety and security testing, and report results back to the U.S. government. However, this AI model evaluation regime was never fully operationalised. President Trumpâ€™s â€˜America Firstâ€™ mantra is not just about tariffs, defence spending, and immigration; it is also relevant for AI. The AI policy ambition of Trumpâ€™s second term is to strengthen U.S. global AI leadership and dominance, promote AI innovation, and advance deregulation. Two decisive actions were taken by President Trump within days of his second term commencing. On his first day in office, 20 January 2025, President Trump signed Executive Order 14148 : Initial Rescissions of Harmful Executive Orders and Actions .The purpose of this was simple: to revoke dozens of Executive Orders and Presidential Memorandums issued by President Biden. This included revocation of Executive Order 14110 : Safe, Secure, and Trustworthy Development and Use of AI , which was the cornerstone of Bidenâ€™s AI governance agenda. \n\nTrumpâ€™s agenda: what â€˜America Firstâ€™ means for AI   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 12/27\n\nThe second decisive move came 3 days later, when President Trump signed Executive Order 14179 : Removing Barriers to American Leadership in AI. Doubling down on the revocation of Bidenâ€™s AI Executive Order, this announcement deemed the previous administrationâ€™s wider AI policy agenda as a â€œbarrier to American AI innovationâ€. The stated policy of the new administration is to â€œsustain and enhance Americaâ€™s global AI dominance in order to promote human flourishing, economic competitiveness, and national securityâ€. Trumpâ€™s Executive Order mandates formulation of an â€˜AI Action Planâ€™, by July 2025, which can achieve this policy objective. Top U.S. officials are now working on this. As part of this, the federal government will run an exercise to identify, halt, and shut down any AI-related government activities or initiatives which are deemed as contrary to achieving the policy objective stated above. This may include AI governance and safety related initiatives which were pursued following the mandate from Bidenâ€™s AI Executive Order. The development of the AI Action Plan has received significant interest, with the public consultation (which has now closed) receiving 8,755 comments in under 2 months.   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 13/27\n\nDespite the policy shift, it is important to note that the Trump administration has not dismantled Bidenâ€™s entire AI policy agenda, nor has it abandoned the concept of AI governance and risk management. For example, much of what was previously implemented on AI-related export controls and investment restrictions, as well as Bidenâ€™s Executive Order on AI Infrastructure, remains in force. Furthermore, both of the recent memoranda on federal agency use and acquisition of AI, published by the Office of Management and Budget (OMB), emphasise the importance of responsible AI adoption and sound AI governance and risk management practices. What is clear, however, is that the Trump administration has no intention to impose any major AI governance related regulations or restrictions on private sector AI development and deployment. One potential point of relative harmony between the Biden and Trump administrations is the stance on AI-related export controls and investment restrictions. \n\nAI export controls and investment restrictions   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 14/27\n\nThe U.S. deems the development of AI in â€œcountries of concernâ€ as a â€œnational security threatâ€. Concerted attempts to control how, where, and at what pace AI capabilities are developed has become a fundamental element of U.S. federal AI policy and the broader mission to sustain U.S. leadership in this foundational technology. Various laws and regulations were passed during Bidenâ€™s presidency which significantly restrict which countries U.S. advanced AI computing chips and AI model weights can be exported to, as well as which countriesâ€™ AI industries U.S. persons can invest in. The key laws are summarised below: \n\nFramework for AI Diffusion \n\nAn interim final rule issued by the U.S. Department of Commerceâ€™s Bureau of Industry and Security (BIS). This is the most comprehensive U.S. regulation restricting the export of AI-related technologies. It became effective on 13 January 2025, but most requirements are not applicable until 15 May 2025, when the consultation period closes. The purpose of this rule is twofold: to ensure that a) model weights of the most advanced â€˜closedâ€™ (i.e., not open-source) U.S. AI models are only stored outside of", "fetched_at_utc": "2026-02-08T18:52:30Z", "sha256": "77b328bb624a89d0a6a2f92fd63c3b0e3eec52d835dfa04d66bae418ff299dcb", "meta": {"file_name": "U.S. AI Law & Policy Explained - Oliver Patel.pdf", "file_size": 981207, "relative_path": "pdfs\\U.S. AI Law & Policy Explained - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 3240}}}
{"doc_id": "pdf-pdfs-what-is-president-trump-s-ai-policy-by-oliver-patel-f55c8d182549", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\What is President Trump_s AI policy - by Oliver Patel.pdf", "title": "What is President Trump_s AI policy - by Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel \n\nhttps://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy  2/18 On Thursday 11 December 2025, President Trump issued an Executive Order on Ensuring a National Policy Framework for AI. This represents the administrationâ€™s latest attempt at blocking and constraining how U.S. states regulate AI. This article summarises the new Executive Order, situates it in the wider context of Trumpâ€™s AI policy, assesses the challenges the administration faces in preempting state AI laws, and reflects on what companies should do next. \n\nFor a detailed, up-to-date, and visual guide to U.S. AI law and policy (covering the federal and state levels), as well as U.S, China, and EU comparison charts, sign up to secure a 25% discount for my forthcoming book, Fundamentals of AI Governance \n\n(2026). On 11 December 2025, President Trump issued a new Executive Order that attempts to block states from enforcing existing AI regulations and deter states from enacting new AI laws. Just five months after the Senate rejected the proposed 10-year moratorium on state AI laws by 99 votes to 1, the administration is having another bite of the cherryâ€”this \n\nWhat just happened?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 3/18\n\ntime through litigation, funding restrictions, and federal preemption, potentially via new legislation. However, it is important to note that the latest development is an Executive Order (i.e., a presidential directive), not legislation. Meaningfully preempting state laws in this way would require legislation, which requires approval from both the House of Representatives and the Senate. It does not appear that substantive political changes have occurred in the past few months to render this more likely than it was back in the summer. Nonetheless, this latest Executive Order is a powerful signal of intent that highlights the administration is digging in on this particular issue, despite the large-scale opposition to the previous 10-year moratorium proposal. Indeed, the administration has repeatedly stated that its AI policy objective is to â€œsustain and enhance Americaâ€™s global AI dominanceâ€. State AI laws have been in the firing line, as part of the wider focus on pursuing deregulation as a means to propel the U.S. as the worldâ€™s dominant AI power. Preemption is a U.S. constitutional principle whereby federal law (i.e., law passed by Congress) takes precedence over state law when there are conflicts between the two. When a federal law â€œpreemptsâ€ a state law, the state law is effectively nullified.   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 4/18\n\nHowever, the controversy regarding state AI law preemption is partly due to the fact that there is no comprehensive federal AI law. And the federal AI laws that are on the books cover narrower domains such as strengthening the U.S. AI industry, support and funding for AI research and infrastructure, and export controls. Where state AI laws focus on responsible AI topics like transparency or bias, there is arguably a lack of federal law to preempt these laws. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nPresident Trump signed the Executive Order: Ensuring a National Policy Framework for Artificial Intelligence on Thursday 11 December 2025, An Executive Order is a directive issued by the President to federal agencies and executive branch officials. It is issued by the President unilaterally, does not require Congressional approval and cannot, by itself, override state laws. \n\nWhat is the new Executive Order on AI?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 5/18\n\nThe core argument Trump presents in this Executive Order is that the patchwork of many state AI laws creates compliance burdens and administrative complexity that could undermine U.S. competitiveness. Having 50 different AI regulatory regimes, the Executive Order argues, â€œ makes compliance challenging, particularly for startups â€œ. Coloradoâ€™s AI lawâ€” Consumer Protections for AI (SB24-205), effective date 30 June 2026â€”is singled out for criticism, with the Executive Order claiming that such laws tackling â€œalgorithmic discriminationâ€ may force AI models to produce false results in order to avoid differential treatment of protected groups. The declared policy of the administration is to establish a â€œminimally burdensome national standardâ€ for AI, as opposed to â€œ50 discordant State onesâ€. To achieve this, the Executive Order directs a multi-pronged set of actions and broader strategy for the U.S. federal government to pursue: First, the Attorney General must establish an AI Litigation Task Force within 30 days (of the Executive Order), with the goal of challenging state AI laws in court. The focus will be on identifying state laws that are inconsistent with the U.S. AI policy of â€œsustaining and advancing U.S. global dominance in AIâ€. The grounds for legal challenge could include â€œunconstitutional regulationâ€ of interstate commerce or preemption by existing federal regulations.   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 6/18\n\nSecond, the Secretary of Commerce, in consultation with other government leaders, must publish an evaluation of existing state AI laws within 90 days (of the Executive Order), identifying â€œonerousâ€ laws that conflict with the U.S. AI policy objective mentioned above. This evaluation must focus on laws that â€œrequire AI models to alter truthful outputsâ€ or that compel organisations to â€œdisclose information in a manner that would violate the First Amendmentâ€. This is significant, as it explicitly targets laws relating to AI transparency and bias mitigation, two core threads throughout many existing state AI laws. Third, the Order imposes funding restrictions on states that are deemed to have â€œonerous AI lawsâ€. States identified through the evaluation referenced above will be ineligible for certain categories of federal funding under the Broadband Equity Access and Deployment (BEAD) Program. U.S. government executive departments and agencies are also directed to assess whether they can condition discretionary grants on states agreeing not to enforce their existing AI laws and/or not enacting new AI laws. This could result in government agencies withholding certain types of grant funding from states. Fourth, the Federal Communications Commission (FCC), the agency that regulates U.S. communications (e.g., broadcasting, internet, and telecommunications), must initiate work to determine whether to adopt a federal reporting and disclosure standard for AI models that would preempt conflicting state laws. It is important to note that the Executive Order requires the FCC to â€œinitiate a proceedingâ€; it   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 7/18\n\ndoes not require the FCC to adopt such a standard for AI transparency. However, the implied thinking is that if there is a national standard, this could be argued to supersede state AI laws covering AI reporting and disclosure. Fifth, the FTC must issue a policy statement explaining when state laws requiring the alteration of the â€œtruthful outputsâ€ are preempted by federal prohibitions on deceptive practices, which largely stem from the FTC Act that prohibits â€œunfair and deceptive practicesâ€. Again, the implication is that such guidance would strengthen the case for existing federal laws superseding state AI laws in this domain too. Finally, the administration will prepare a legislative proposal for a uniform federal AI framework that preempts state laws. However, the proposal would be narrow and would not seek to preempt state laws covering any of the following policy areas: child safety; AI compute and data centre infrastructure; state government procurement and use of AI; and other topics to be determined. This is significant as it highlights the policy areas which the administration deem to be legitimate domains of autonomous state AI lawmaking, even if the result is regulatory   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 8/18\n\ndivergence within the U.S. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nThis latest Executive Order is not the administrationâ€™s first attempt to constrain state AI laws. Back in May 2025, the administration proposed adding a 10-year moratorium on state AI laws enforcement to Trumpâ€™s flagship domestic policy and taxation bill, the â€˜One Big Beautiful Bill Actâ€™ (H.R.1). I covered the 10-year moratorium in detail in a previous edition of Enterprise AI Governance: Unpacking the 10-year Moratorium on U.S. State AI Laws . Here is a summary of that article. The proposed moratorium was designed to prevent U.S. states from being able to enforce â€œany law or regulation limiting, restricting, or otherwise regulating AI models, AI systems, or automated decision systemsâ€ for a period of 10 years. Although this \n\nHow did Trump attempt to block state AI laws previously?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 9/18\n\nwould not technically have prevented states from introducing and passing new AI laws, the broad restriction on enforcement would have rendered doing so pointless. This was an attempt by the federal government to preempt state AI laws by blocking states from enforcing laws they had already passed, as well as any new laws they might pass in the future. The stated goal was to halt the â€œproliferation of a complex and fragmented patchwork of state AI lawsâ€, in support of AI innovation across the country. The House of Representatives voted to pass the state AI law moratorium by a narrow margin, largely along party lines, with 215 in favour and 214 against. However, following this, the moratorium was decisively rejected. In July 2025, the Senate voted 99 to 1 to remove it from the bill. This followed a significant bipartisan campaign against the provision. A June 2025 letter , signed by 260 state lawmakers, stated that â€œstates are laboratories of democracy accountable to their citizens and must maintain the flexibility to respond to new digital concernsâ€. Several Republican state governors also campaigned against the proposal. The main reason the moratorium proved so controversial was not really about AI. The case (against it) centred on the philosophical objection to the federal government constraining states in this way, particularly given that there is no comprehensive federal AI law to take precedence. The argument was that the statesâ€™ hands were being   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 10/18\n\ntied, without an alternative federal framework on the table. The fundamentals of the situation do not appear to have meaningfully changed in the months that have passed. However, the December 2025 Executive Order represents a pivot in strategy. Rather than seeking blanket preemption through legislationâ€”which requires Congressional approvalâ€”the administration is now pursuing litigation, agency action, and funding leverage. These approaches can be initiated by the executive branch alone. However, as noted above, the Presidentâ€™s legal authority to actually preempt state laws remains limited without legislation. The administration may also pursue new legislation, but this will likely face similar challenges in Congress. In lieu of comprehensive federal AI law, dozens of U.S. states have enacted AI-related laws. 131 such laws were passed between 2016 and 2024, and over 700 AI-related bills were proposed in 2024 alone. The states with the most AI-related laws are California, Colorado, Maryland, Utah, and Virginia. California has been particularly active, enacting dozens of laws that regulate AI in different ways. These laws cover themes including fairness and accountability, transparency, data privacy, deepfakes, and government use of AI. \n\nWhat AI laws do states have and will Trump succeed in blocking them?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 11/18\n\nHowever, the administration faces an uphill task in its efforts to block the enforcement and enactment of these laws. Now that the moratorium has been rejected by the Senate due to concerns regarding preemption and the ways in which the federal government can constrain the states, it is fair to argue that subsequent federal legislative proposals will be met with similar levels of scrutiny and controversy. The new Executive Order sits within a broader pro-business and pro-innovation AI policy agenda. Trump has pivoted away from the Biden administrationâ€™s AI governance and safety agenda. Promoting U.S. AI leadershipâ€”which has always been a core federal AI policy objectiveâ€”now takes centre stage. One of the first actions taken by Trump at the start of this second term, in January 2025, was to revoke various Biden-era executive orders, including the flagship Executive Order on Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. \n\nIn July 2025, the administration published Winning the Race: Americaâ€™s AI Action Plan ,which outlines how the U.S. can achieve and maintain â€œunquestioned and \n\nWhat other AI policy actions has this administration taken?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 12/18\n\nunchallenged global technological dominanceâ€ in AI. I also covered the administrationâ€™s AI Action Plan in a previous edition of Enterprise AI Governance: What Americaâ€™s AI Action Plan Means for AI Governance . A summary of that explainer piece is provided below. The AI Action Plan contains dozens of policy recommendations across three pillars: Pillar 1. Accelerate AI Innovation Pillar 2. Build American AI Infrastructure Pillar 3. Lead in International AI Diplomacy and Security Notably, under Pillar 1, the AI Action Plan recommended withholding funding for AI-related initiatives from states with â€œburdensome AI regulationsâ€. This core idea has now been operationalised through the new Executive Order and its funding restrictions. Moreover, the rejection of the 10-year moratorium by the Senate did not extinguish the underlying policy objective; rather, it has been repackaged and is now pursued through alternative means. The AI Action Plan is significant because it directly links AI regulations with the ability (or lack thereof) of companies to innovate at speed. Put simply, the Trump   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 13/18\n\nadministration believes that AI should not be constrained by regulations and that doing so would impede the U.S. economic and security prospects in a damaging way. Other recommended policy actions in the AI Action Plan included: Taking action to review and remove any existing Federal regulations that impede AI innovation. Ensure the federal government only procures â€œunbiasedâ€ and â€œideologically neutralâ€ large language models. Fast-track and streamline processes for data centre construction review, approval, and licensing. Advocate for â€œpro-innovationâ€ approaches to international AI governance, that reflect â€œAmerican valuesâ€ and shift away from â€œburdensome regulationsâ€. Other notable AI policy measures pursued by this administration include Executive Orders on advancing AI education for American youth (April 2025), accelerating federal permitting for data centre infrastructure (July 2025), preventing â€œwoke AIâ€ in federal government procurement (July 2025), and promoting the export of the American AI technology stack (July 2025). The TAKE IT DOWN Act, which criminalises the publication of non-consensual intimate deepfakes, was signed into law in May 2025. \n\nWhat should companies operating in the U.S. do now?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 14/18\n\nIf you think AI governance is not relevant for your organisation because the current administration is pro-AI and against restrictive AI regulation, you could be in for a rude awakening if things go wrong. Deregulation does not mean that AI risks no longer apply to you or that you are not exposed. Indeed, the AI Action Plan itself highlights various AI-related risks that could slow innovation, including interpretability, robustness, and misalignment. Furthermore, all enterprises using generative AI at scale are exposed to a litany of (relatively novel) data-related risks. I outlined these in my article on the PROTECT Framework: Managing Data Risks in the AI Era .The PROTECT Framework empowers you to understand, map, and mitigate the most pertinent data risks that are fuelled by widespread adoption of generative AI, covering themes such as public AI tool usage, rogue internal AI projects, opportunistic vendors, and compliance and copyright breaches. These risks cannot be mitigated without a robust approach to AI governance, which serves as a reminder that AI-specific regulatory compliance is not the sole driver for enterprise AI governance. Moreover, even the White Houseâ€™s Office of Management and Budget (OMB) describes effective AI governance as â€œkey to accelerated innovationâ€. Indeed, the AI governance framework that OMB directs federal agencies to implementâ€”covering AI   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 15/18\n\ndevelopment, deployment, procurement, and useâ€”is robust. This suggests that although the U.S. government does not want there to be any regulatory measures getting in the way of U.S. companiesâ€™ AI activities, it nonetheless recognises that a well-designed and proportionate AI governance framework is important for both risk mitigation and value generation, especially in sensitive domains. For U.S. companies, the reality today is the same as it was yesterday. There still exists a complex patchwork of many state AI laws to contend with, as well as federal and state laws that meaningfully regulate or implicate AI in specific ways, such as privacy, copyright, employment, and consumer protection laws. Given the complexity of developing different internal AI governance frameworks for different jurisdictions, I always recommend having a company-wide AI governance framework that promotes and facilitates compliance, risk management, and AI-enablement across all the important jurisdictions you operate in. \n\nThanks for reading! Subscribe below for weekly updates from Enterprise AI Governance.", "fetched_at_utc": "2026-02-08T18:52:32Z", "sha256": "f55c8d182549c50122606bab0bceb73884ac63ec81696ee81822147dc1eab25f", "meta": {"file_name": "What is President Trump_s AI policy - by Oliver Patel.pdf", "file_size": 809321, "relative_path": "pdfs\\What is President Trump_s AI policy - by Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4061}}}
{"doc_id": "pdf-pdfs-a-practical-guide-to-ai-and-copyright-oliver-patel-308aa353afd3", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\A Practical Guide to AI and Copyright - Oliver Patel.pdf", "title": "A Practical Guide to AI and Copyright - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel \n\nhttps://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright  2/20 This free newsletter delivers practical, actionable and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! For more frequent updates, be sure to follow me on LinkedIn .This special edition of Enterprise AI Governance contains everything you need to know to navigate the thorny issue of AI and copyright . It is packed full of practical resources, including: \n\nâœ… 5 practical steps to mitigate risks \n\nâœ… A primer on AI and copyright \n\nâœ… International regulatory snapshot  ðŸ‡ºðŸ‡¸  ðŸ‡ªðŸ‡º  ðŸ‡¬ðŸ‡§  ðŸ‡¯ðŸ‡µ  ðŸ‡¨ðŸ‡³  ðŸ‡°ðŸ‡· \n\nâœ… Relevant litigation and fair use arguments \n\nâœ… Generative AI and Copyright Cheat Sheet ( developed in partnership with Copyright Clearance Center ) - scroll to the bottom for this free subscriber only pdf download!   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 3/20\n\nIf this topic interests you, then check out this LinkedIn Live event I am speaking at today (Thursday 27 th February) at 3pm GMT / 10am EST. Itâ€™s a fireside chat on â€˜ Bridging Innovation and Integrity in Responsible AIâ€™ , hosted by Copyright Clearance Center . Weâ€™ll be covering how to build and implement an enterprise AI governance programme, as well as copyright and AI challenges. 800+ people have signed up so far and it would be great to see you there! \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week. \n\nFrequent readers of this newsletter know that I try to front-load value and actionable advice. Therefore, before my deeper analysis of the AI and copyright issue, here are 5 practical steps organisations can take today to mitigate copyright risks. \n\n1. Licensing: use data and content explicitly licensed for use in AI systems. Where appropriate, renegotiate contracts to enable this. \n\n5 practical steps to mitigate risks   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 4/20\n\n2. Contractual safeguards: negotiate robust clauses, such as IP indemnity protection, in contracts with AI providers. However, be wary that these indemnity clauses do not apply in all circumstances, nor do they offer complete protection. \n\n3. Data lineage: ensure copyright considerations are a core part of AI risk assessments. These assessments should track and record the lineage of all data used for AI training and development. \n\n4. AI literacy: scale employee training and awareness, particularly on document and multimedia uploads and prompting. Now that AI is at the fingertips of all employees, anyone can exacerbate copyright infringement risk. \n\n5. EU AI Act compliance: General-Purpose AI (GPAI) model providers must prepare for their obligation to publish training data summaries. This is applicable from August 2025 (or 2027 for models already on the market). The forthcoming GPAI Model Code of Practice and associated template will outline how to do this. AI deployers and downstream providers (which integrate GPAI models into new AI systems) should conduct robust due diligence and choose their providers wisely. \n\nWhat is copyright? \n\nA primer on AI and copyright   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 5/20\n\nCopyright is a form of intellectual property that protects original works of authorship, including articles and books, to encourage and promote culture, science, and innovation. Under copyright law, copyright owners, such as authors and publishers, have exclusive rights regarding how their work is used, shared, or reproduced. Copyright laws attempt to strike a balance between ensuring fair compensation for creators of original work, whilst enabling wider society to benefit from that work, for example by permitting engagement, commentary, and transformation. Throughout history, technological developments, such as the advent of radio, television, and the internet, have fundamentally altered how content is developed, distributed, and consumed. Each new wave of technological change has been accompanied with meaningful changes in copyright laws and the way they are applied. \n\nAI and copyright: what is the issue? \n\nRapid advances in the field of machine learning, and generative AI in particular, raise important legal questions and challenges relating to copyright.   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 6/20\n\nThese challenges are relevant for all organisations which use AI. Copyright infringement is a significant risk of AI development and deployment. This is because of the vast amount of published material protected by copyright, such as books, videos, music, news articles, and software, required to train certain AI models. Those AI models can also reproduce copyrighted material in their outputs. If AI-generated outputs closely resemble original works (which formed part of the training data) this can infringe copyright. Copyrighted material can also be usedâ€“whether intentionally or unintentionallyâ€“to prompt, augment, or fine-tune AI. Therefore, although the large foundation model developers are currently in the spotlight, many other organisations could get caught in the crosshairs of copyright litigation. Organisations must also consider whether their AI-generated output (e.g., content) is protected by copyright. In most jurisdictions, the key factor is the level and nature of human involvement in generating this output. AI-assisted works may be copyrightable, but purely AI-generated works may not be.   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 7/20\n\nThe copyright risks of AI are apparent in the dozens of lawsuits copyright owners have brought against unauthorised use of their protected works in AI systems. \n\nTraining AI models \n\nTraining AI models, such as Large Language Models (LLMs), often involves the collection and processing of large amounts of copyright protected material. LLM training involves breaking down this text into tokens , each of which is assigned a unique integer ID (i.e., a mathematical representation of the text, which the machine can process). During training, the model processes these tokens, identifying patterns and structures, effectively compressing and retaining representations of the training data, including potentially copyrighted content, within its parameters. During inference, the model predicts the next token which should appear in the sequence, based upon the statistical patterns and structures it has been exposed to. This process allows the model to generate new text based on learned patterns. The key question is whether the copying and use of this data, for AI training and   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 8/20\n\ninference, infringes on copyright. There are strong arguments both for and against, which I summarise below. \n\nCopyright infringement risk across the AI lifecycle AI model training : many AI models are trained on large datasets, often incorporating copyrighted material. \n\nAI model fine-tuning: the fine-tuning and adjustment of pre-trained AI models can be done leveraging copyright protected material. \n\nRetrieval-augmented generation: RAG applications can incorporate or reproduce copyrighted material as part of their retrieval corpus or augmented outputs. \n\nInputs and prompts: users can input or upload copyrighted material as part of prompts, such as images, documents, or text excerpts. \n\nAI inference & outputs: AI-generated content may reproduce or closely resemble copyrighted material, potentially infringing. \n\nAI-generated outputs   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 9/20\n\nWhilst some organisations will primarily be concerned with whether their AI development activities are copyright infringing, many others will be keen to understand whether their AI-generated outputs are copyright protected (i.e., copyrightable). The extent to which AI-generated outputs can be copyright protected varies. In most jurisdictions, human authorship is required .This is because copyright is a legal right afforded to humans. Therefore, without human authorship, there is no copyright. This has been reinforced in several legal cases , which determined that AI itself cannot be the author of a copyright protected work. In other words, AI-generated work is not copyrightable if no human was involved. â€˜AI-assistedâ€™ works may be copyrightable, whereas purely AI-generated works are most likely not. What matters is the nature and degree of human involvement and creativity. For example, the U.S. Copyright Officeâ€™s position is that merely inputting a prompt is insufficient for copyright protection. This is because simple prompting does not usually enable the user to exert meaningful control over the nature of the output.   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 10/20\n\nHowever, this changes if the humanâ€™s work or creativity is perceptible in the AI output, or if the human can modify, arrange, or shape the AI-generated output in a meaningful way. In such cases, copyright protection may be granted for a human-assisted AI generated work. This will usually be determined on a case-by-case basis. There is no global copyright law. Rather, there is significant global divergence in copyright laws and their application to AI. Also, the applicable legal framework is determined by the jurisdiction in which the copying happened. Therefore, it is not possible to make blanket claims about whether AI development or deployment infringes copyright, as it will always depend on the relevant jurisdiction, the material which has been used, and exactly how that material has been used. Crucially, there are exceptions in copyright law, such as â€˜fair useâ€™ in the U.S . These exceptions stipulate that, in specific scenarios, copyrighted material can be used without permission. \n\nInternational regulatory snapshot   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 11/20\n\nGiven that there is broad international consensus on the requirement for human authorship for AI output to be copyrightable, the below will instead focus on whether the use of copyright protected material in AI training, development, and deployment is permitted under applicable exceptions. \n\nðŸ‡ºðŸ‡¸  USA: There is no specific law or regulation regarding AI and copyright, so existing exclusive rights (like the copyright holderâ€™s right of reproduction) and exceptions and limitations (like fair use) apply. Whether AI model training constitutes fair use (and is therefore not copyright infringing) is determined by courts on a case-by-case basis. When evaluating whether an activity constitutes fair use, U.S. courts consider four key factors: \n\n1. Purpose and character of the use. \n\n2. Nature of the copyrighted work. \n\n3. Amount of the portion taken. \n\n4. Market effects. \n\nðŸ‡ªðŸ‡º  EU: The EU Copyright Directive includes two text and data mining (TDM) exceptions â€”one for scientific research and another that allows broader use (including for commercial purposes), unless the copyright holder opts out. The TDM exception could apply to AI training, but this has not been tested in courts.   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 12/20\n\nThe EU AI Act reinforces that existing EU copyright law applies to AI systems and introduces transparency requirements regarding the training data used. Specifically, providers of general-purpose AI models will be obliged to publish summaries of their training data and implement a copyright compliance policy. \n\nðŸ‡¬ðŸ‡§  UK: The UK launched a Copyright and AI consultation in December 2024, which outlined a proposal to adopt a framework akin to the EUâ€™s (i.e., with a data mining exception covering commercial use). This would create a more permissive regime than is currently in place. The consultation closed earlier this week. The governmentâ€™s proposals were widely criticised by artists and publishers, but backed by AI companies. \n\nðŸ‡¯ðŸ‡µ  Japan: Japan has a nuanced approach . While Article 30-4 of the Copyright Act permits AI training in some cases, this is not a blanket exemption, and it has limitations. For example, if copying the material for \"enjoyment purposes\" or prejudice to rights holders are involved. Such prejudice includes â€œmaterial impact on the relevant marketsâ€. Furthermore, AI-generated outputs that are \"similar\" and \"dependent\" on copyrighted works can still be infringing. \n\nðŸ‡¨ðŸ‡³  China: China has a stricter AI and copyright legal framework. There are no explicit exemptions for TDM or AI training. Enforcement by Chinese courts is expected to be relatively strict.   \n\n> 1/3/26, 4:49 PM A Practical Guide to AI and Copyright - by Oliver Patel\n> https://oliverpatel.substack.com/p/a-practical-guide-to-ai-and-copyright 13/20\n\nðŸ‡°ðŸ‡·  Republic of Korea: The Korean Copyright Commission encourages AI companies to proactively secure licenses, prior to using copyrighted material for AI training. There are dozens of ongoing legal cases around the world adjudicating AI and copyright disputes. \n\nOne tracker lists 39 relevant cases, including Getty Images v Stability AI, The New York Times v Microsoft and OpenAI, and Farnsworth v Meta. Many cases are in the U.S., where courts are determining whether various uses of copyrighted material in AI systems are infringing or constitute fair use. There are strong arguments on both sides of the debate. Those arguing that AI training does not meet the criteria for fair use highlight that original copyright protected materials (e.g., text) are copied for training and then stored, as mathematical representations, in model parameters, which is why LLMs can â€˜memoriseâ€™ and reproduce training data content, thereby infringing the right to reproduction. \n\nRelevant litigation and fair use arguments", "fetched_at_utc": "2026-02-08T19:06:44Z", "sha256": "308aa353afd3ceb132f6a93d8bf7fd6bc90ffd1e39a38378c315b9ea54002fa0", "meta": {"file_name": "A Practical Guide to AI and Copyright - Oliver Patel.pdf", "file_size": 910317, "relative_path": "pdfs\\A Practical Guide to AI and Copyright - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 3146}}}
{"doc_id": "pdf-pdfs-ai-governance-in-practice-report-2024-iapp-d10ba8f26b3a", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Governance in Practice Report 2024 - IAPP.pdf", "title": "AI Governance in Practice Report 2024 - IAPP", "text": "AI Governance in Practice Report 2024 AI Governance in Practice Report 2024  | 2\n\n# Table of \n\n# contents \n\n## What's inside? \n\nExecutive summary  3\n\nPart I. Understanding AI and governance  . . . . . . . . . . . . . . . . . . .  6\n\nPart II. The data challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  15 \n\nPart III. The  privacy and data protection challenge  23 \n\nPart IV. The  transparency, explainability \n\nand interpretability challenge  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  32 \n\nPart V. The bias, discrimination and fairness challenge  41 \n\nPart VI. The  security and robustness challenge  . . . . . . . . . . . . .  50 \n\nPart VII. AI  safety  55 \n\nPart VIII. The  copyright challenge  61 \n\nPart IX. Third-party AI assurance  . . . . . . . . . . . . . . . . . . . . . . . . .  65 \n\nConclusion  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  69 \n\nContacts  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  70 AI Governance in Practice Report 2024  | 3\n\n# Executive \n\n# summary \n\n## Recent and rapidly advancing \n\n## breakthroughs in machine \n\n## learning technology have forever \n\n## transformed the landscape of AI. \n\nAI systems have become powerful engines capable of \n\nautonomous learning across vast swaths of information and \n\ngenerating entirely new data. As a result, society is in the midst \n\nof significant disruption with the surge in AI sophistication and \n\nthe emergence of a new era of technological innovation. \n\nAs businesses grapple with a future in which the boundaries of \n\nAI only continue to expand, their leaders face the responsibility \n\nof managing the various risks and harms of AI, so its benefits \n\ncan be realized in a safe and responsible manner. \n\nCritically, these benefits are accompanied by serious \n\nconsiderations and concerns about the safety of this technology \n\nand the potential for it to disrupt the world and negatively \n\nimpact individuals when left unchecked. Confusion about how \n\nthe technology works, the introduction and proliferation of bias \n\nin algorithms, dissemination of misinformation, and privacy \n\nrights violations represent only a sliver of the potential risks. \n\nThe practice of  AI governance  is designed to tackle these \n\nissues. It encompasses the growing combination of principles, \n\nlaws, policies, processes, standards, frameworks, industry \n\nbest practices and other tools incorporated across the design, \n\ndevelopment, deployment and use of AI. â†’ Executive summary    \n\n> AI Governance in Practice Report 2024 |4\n> TABLE OF CONTENTS â†‘\n\nWhile relatively new, the field of AI governance \n\nis maturing, with government authorities \n\naround the world beginning to develop targeted \n\nregulatory requirements and governance \n\nexperts supporting the creation of accepted \n\nprinciples, such as the Organisation for \n\nEconomic Co-Operation and Development's  AI \n\nPrinciples , emerging best practices and tools for \n\nvarious uses of AI in different domains. \n\nThere are many challenges and potential \n\nsolutions for AI governance, each with unique \n\nproximity and significance based on an \n\norganization's role, footprint, broader risk-\n\ngovernance profile and maturity. This report \n\naims to inform the growing, increasingly \n\nempowered and increasingly important \n\ncommunity of AI governance professionals \n\nabout the most common and significant \n\nchallenges to be aware of when building \n\nand maturing an AI governance program. \n\nIt offers actionable, real-world insights \n\ninto applicable law and policy, a variety of \n\ngovernance approaches, and tools used to \n\nmanage risk. Indeed, some of the challenges \n\nto AI governance overlap and run through \n\na range of themes. Therefore, an emerging \n\nsolution for one thematic challenge may \n\nalso be leveraged for another. Conversely, in \n\ncertain circumstances, specific challenges and \n\nassociated solutions may conflict and require \n\nreconciliation  with other approaches. Some of \n\nthese potential overlaps and conflicts have been \n\nidentified throughout the report. \n\nGlobal AI private investment \n\n(USD billion, 2021) 2013 2014 2015 2016 2017 2018 2019 2020 2021 4810 18 22 38 42 47 94 â†’ Executive summary \n\nAI Governance in Practice Report 2024  | 5\n\nTABLE OF CONTENTS  â†‘\n\n# Questions about whether and when \n\n# organizations should prioritize \n\n# AI governance are being answered: \n\n# \"yes\" and \"now,\" respectively. \n\nQuestions about whether and when organizations should prioritize \n\nAI governance are being answered: \"yes\" and \" now ,\" respectively. \n\nThis  report is, therefore, focused on how organizations can approach, \n\nbuild and leverage AI governance in the context of the increasingly \n\nvoluminous and complex applicable landscape. \n\nJoe Jones \n\nIAPP Director of Research \n\nand Insights \n\nAshley Casovan \n\nIAPP AI Governance Center \n\nManaging Director \n\nUzma Chaudhry \n\nIAPP AI Governance \n\nCenter Research Fellow \n\nNina Bryant \n\nFTI Technology Senior \n\nManaging Director \n\nLuisa Resmerita \n\nFTI Technology \n\nSenior Director \n\nMichael Spadea \n\nFTI Technology Senior \n\nManaging Director AI Governance in Practice Report 2024  | 6\n\n# Part I. \n\n# Understanding \n\n# AI and \n\n# governance \n\n## Components of an AI system and \n\n## their governance \n\nTo understand how to govern an AI system, it is important to first \n\nunderstand what an AI system is. The EU AI Act, for example, \n\ndefines an AI system as \"a machine-based system that is designed \n\nto operate with varying levels of autonomy and that may exhibit \n\nadaptiveness after deployment, and that, for explicit or implicit \n\nobjectives, infers, from the input it receives, how to generate outputs \n\nsuch as predictions, content, recommendations, or decisions that \n\ncan influence physical or virtual environments.\" \n\nAs indicated in the OECD's Framework for the Classification of AI \n\nsystems, AI systems are comprised of data used to train and operate \n\na system, model, output and context. While a model is a fundamental \n\nbuilding block of an AI system, a single model seldom operates in \n\nisolation. Instead, multiple AI models come together and interact \n\nwith each other to form complex AI systems. Additionally, AI systems \n\nare often designed to interact with other systems for sharing data, \n\nfacilitating seamless integration into real-world environments. \n\nThis  results in a network of AI systems, each with its specialized \n\nmodels, working  together to achieve a  larger goal. \n\nWith AI poised to revolutionise many aspects of our \n\nlives, fresh cooperative governance approaches are \n\nessential. Effective collaboration between regulatory \n\nportfolios, within nations as well as across borders, \n\nis crucial: both to safeguard people from harm and to \n\nfoster innovation and growth. \n\nKate Jones \n\nU.K. Digital Regulation Cooperation Forum CEO â†’ Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 7 \n\n> TABLE OF CONTENTS â†‘\n\nAI governance is about to get a lot \n\nharder. The internal complexity of \n\ngoverning AI is growing as more \n\ninternal teams adopt AI, new AI \n\nfeatures are built, and the systems \n\nget complex, but at the same time, \n\nthe external complexity is also set to \n\ngrow rapidly with new regulations, \n\ncustomer demands, and safety \n\nresearch evolving. \n\nThe organizations who have invested \n\nin structured AI governance already \n\nhave a leg up and will continue to \n\nhave a competitive advantage. \n\nAndrew Gamino-Cheong \n\nTrustible AI Co-founder and Chief Technology Officer \n\nNavigating AI governance sources \n\nGiven the complexity and transformative \n\nnature of AI, significant work has been done \n\nby law and policymakers on what is now a \n\nvast and growing body of principles, laws, \n\npolicies, frameworks, declarations, voluntary \n\ncommitments, standards and emerging best \n\npractices that can be challenging to navigate. \n\nMany of these various sources interact with \n\neach other, either directly or by virtue of the \n\nissues covered. \n\nAI principles, such as the OECD's AI Principles \n\nor UNESCO's Recommendation on the Ethics \n\nof AI, can shape global standards, especially \n\nwhen national governments pledge to \n\nvoluntarily incorporate such guidance into \n\ntheir domestic  AI governance initiatives. \n\nThey  provide a nonbinding, principled \n\napproach to guide legal, policy and industry \n\nefforts toward tackling thematic challenges. \n\nAlgorithm Watch created an inventory of these \n\nprinciples, identifying 167  reports .\n\nLaws and regulations include existing \n\nlegislation that is not specific but is \n\nnonetheless applicable to AI, as well as \n\nemerging legislation that more specifically \n\naddresses the governance of AI systems, such \n\nas the EU AI Act. The EU AI Act is the world's \n\nfirst comprehensive AI regulation. Although \n\njurisdictional variations can be observed across \n\nthe emerging global AI regulatory landscape, \n\nmany draft regulations adopt a risk-based \n\napproach similar to the EU AI Act. \n\nThe EU AI Act mandates AI governance standards \n\nbased on the risk classification of AI systems and \n\nthe organization's role as an AI actor. Certain \n\nAI systems are deemed to pose unacceptable \n\nrisk and are prohibited by law, subject to very \n\nnarrow exceptions. The bulk of the requirements \n\nimposed by the act apply to providers of high-risk \n\nAI systems, although deployers and resellers, \n\nnamely distributers and importers, are are also \n\nsubject to direct obligations. \n\nThe act imposes regulatory obligations at \n\nenterprise, product and operational levels, \n\nsuch as establishing appropriate accountability \n\nstructures, assessing system impact, providing \n\ntechnical documentation, establishing risk \n\nmanagement protocols and monitoring \n\nperformance, among other key requirements. \n\nIn  the context of the growing variety of \n\ngenerative AI use cases and adoption of \n\nsolutions embedding generative AI such as MS \n\nCopilot, general purpose AI-specific provisions \n\nare another crucial component of the EU AI \n\nAct. Depending on their capabilities, reach and \n\ncomputing power, certain GPAI systems are \n\nconsidered to present systemic risk and attract \n\nbroadly similar obligations to those applicable \n\nto high-risk AI systems. â†’ Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 8 \n\n> TABLE OF CONTENTS â†‘\n\nIn addition to binding legislation, voluntary \n\nAI frameworks, such as the National Institute \n\nof Standards and Technology's AI Risk \n\nManagement Framework and the International \n\nOrganization for Standardization's AI Standards, \n\noffer structured and actionable guidance \n\nstakeholders can elect to use to support \n\ntheir work on implementing AI governance. \n\nVoluntary commitments are often developed to \n\nbring different stakeholders closer to a shared \n\nunderstanding of identifying, assessing and \n\nmanaging risks. Standards serve as benchmarks \n\nthat can demonstrate compliance with \n\nregulatory requirements. \n\nInternational declarations and commitments \n\nmemorialize shared commitments, often between \n\ngovernments, to specific aspects or broad \n\nswathes of AI governance. While not binding, \n\nsuch commitments can, at a minimum, indicate \n\na country's support for and intention to advance \n\nAI  governance in particular or general ways, even \n\nat the highest of levels. \n\nNavigating a growing body of draft AI laws, \n\nregulations, standards and frameworks can \n\nbe challenging for organizations pioneering \n\nwith AI. By understanding their unique AI risk \n\nprofile and adopting a risk-based approach, \n\norganizations can build a robust and scalable \n\nAI governance framework that can be deployed \n\nacross jurisdictions. \n\nIAPP Global AI Law and Policy Tracker  \n\n> This map shows the jurisdictions in focus and covered by the IAPP Global AI Law and Policy Tracker . It does not represent the extent to which jurisdictions\n> around the world are active on AI governance legislation. Tracker last updated January 2024.\n\nâ†’ Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 9\n\nTABLE OF CONTENTS  â†‘\n\nThe following are  examples  of some of the most prominent and consequential  AI governance efforts :\n\nPrinciples \n\nâ†’ OECD AI Principles \n\nâ†’ European Commission's Ethics Guidelines for Trustworthy AI \n\nâ†’ UNESCO Recommendation on the Ethics of AI \n\nâ†’ The White House Blueprint for an AI Bill of Rights \n\nâ†’ G7 Hiroshima Principles \n\nLaws and \n\nregulations \n\nâ†’ EU AI Act \n\nâ†’ EU Product Liability Directive, proposed \n\nâ†’ EU General Data Protection Regulation \n\nâ†’ Canada â€“ AI and Data Act, proposed \n\nâ†’ U.S. AI Executive Order 14110 \n\nâ†’ Sectoral U.S. legislation for employment, housing and consumer finance \n\nâ†’ U.S. state laws, such as Colorado AI Act, Senate Bill 24-205 \n\nâ†’ China's Interim Measures for the Management of Generative AI Services \n\nâ†’ The United Arab Emirates Amendment to Regulation 10 to include new rules on \n\nProcessing Personal Data through Autonomous and Semi-autonomous Systems \n\nâ†’ Digital India Act \n\nAI frameworks \n\nâ†’ OECD Framework for the classification of AI Systems \n\nâ†’ NIST AI RMF \n\nâ†’ NIST Special Publication 1270: Towards a Standard for Identifying and Managing Bias in AI \n\nâ†’ Singapore AI Verify \n\nâ†’ The Council of Europe's Human Rights, Democracy, and the Rule of Law Assurance \n\nFramework for AI systems \n\nDeclarations \n\nand voluntary \n\ncommitments \n\nâ†’ Bletchley Declaration \n\nâ†’ The Biden-Harris Administration's voluntary commitments from leading AI companies \n\nâ†’ Canada's guide on the use of generative AI \n\nStandards \n\nefforts \n\nâ†’ ISO/IEC JTC 1 SC 42 \n\nâ†’ The Institute of Electrical and Electronics Engineers Standards Association P7000 \n\nâ†’ The European Committee for Electrotechnical Standardization  AI standards for EU AI Act \n\nâ†’ The  VDE Association's AI Quality and Testing Hub \n\nâ†’ The British Standards Institution and  Alan Turing Institute AI Standards Hub \n\nâ†’ Canada's  AI and Data Standards Collaborative \n\nIt is important to take an \n\necosystem approach to AI \n\ngovernance. Policy makers and \n\nindustry need to work together \n\nat platforms such as the AI Verify \n\nFoundation to make sense of the \n\nopportunities and risks that this \n\ntechnology brings. The aim is to \n\nfind common guardrails to manage \n\nkey risks in order to create a \n\ntrusted ecosystem that promotes \n\nmaximal innovation. \n\nDenise Wong \n\nSingapore Infocomm Media Development Authority Assistant Chief \n\nExecutive, Data Innovation & Protection Group â†’ Part I. Understanding AI and governance    \n\n> AI Governance in Practice Report 2024 |10\n> TABLE OF CONTENTS â†‘\n\nThe AI governance imperative \n\nWith private investment, global adoption rates \n\nand regulatory activity on the rise, as well as \n\nthe growing maturity of the technology, AI is \n\nincreasingly becoming a strategic priority for \n\norganizations and governments worldwide. \n\nOrganizations of all sizes and industries are \n\nincreasingly engaging with AI systems at various \n\nstages of the technology product supply  chain. \n\nThe exceptional dependence on high volumes \n\nof data and endless practical applicability that \n\nmake AI technology a disruptive opportunity \n\ncan also generate uniquely multifaceted risks for \n\nbusinesses and individuals. These include legal, \n\nregulatory, reputational and/or financial risks to \n\norganizations, but also risks to individuals and \n\nthe wider society. \n\nAI Risks \n\nREPUTATIONAL \n\n> Risk of\n> damage to\n> reputation\n> and market\n> competitiveness\n\nFINANCIAL \n\n> Risk of financial\n> implications,\n> e.g., fines, legal\n> or operational\n> costs, or\n> lost profit\n\nLEGAL AND \n\nREGULATORY \n\n> Risk of\n> noncompliance\n> with legal and\n> contractual\n> obligations\n\nINDIVIDUALS \n\nAND SOCIETY     \n\n> Risk of bias\n> or other\n> detrimental\n> impact on\n> individuals â†’Part I. Understanding AI and governance\n> AI Governance in Practice Report 2024 |11\n> TABLE OF CONTENTS â†‘\n\nEnterprise governance \n\nAI governance starts with defining the corporate \n\nstrategy for AI by documenting: \n\nâ†’ Target operating models to set out clear roles \n\nand responsibilities for AI risk. \n\nâ†’ Compliance assessments to establish \n\nprogram maturity and remediation priorities. \n\nâ†’ Accountability processes to record and \n\ndemonstrate compliance. \n\nâ†’ Policies and procedures to formulate policy \n\nstandards and operational procedures. \n\nâ†’ Horizon scanning to enhance and \n\nalign  the program with ongoing \n\nregulatory  developments. \n\nProduct governance \n\nAI governance also requires enterprise policy \n\nstandards to be applied at the product level. \n\nOrganizations can ensure their AI products \n\nmatch their enterprise strategy by using: \n\nâ†’ System impact assessments to identify and \n\naddress risk prior to product development \n\nor  deployment. \n\nâ†’ Quality management procedures tailored \n\nto the software development life cycle to \n\naddress risk by design. \n\nâ†’ Risk and controls frameworks to define \n\nAI risk and treatment based on widely \n\nrecognised standards such as ISO and NIST. \n\nâ†’ Conformity assessments and declarations to \n\ndemonstrate their products are compliant. \n\nâ†’ Technical documentation including \n\nstandardized instructions of use and \n\ntechnical product specifications. \n\nâ†’ Post-market monitoring plans to monitor \n\nproduct compliance following market launch. \n\nâ†’ Third-party due diligence assessments \n\nto identify possible external risk and \n\ninform  selection. â†’ Part I. Understanding AI and governance    \n\n> AI Governance in Practice Report 2024 |12\n> TABLE OF CONTENTS â†‘\n\nOperational governance \n\nThe organization's AI strategy must ultimately be operationalized \n\nthroughout the business through the development of: \n\nâ†’ Performance monitoring protocols to ensure systems perform \n\nadequately for their intended purposes. \n\nâ†’ Transparency and human oversight initiatives to ensure \n\nindividuals are aware and can make informed choices when they \n\ninteract with AI systems or when AI-powered decisions are made. \n\nâ†’ Incident management plans to identify, escalate and respond to \n\nserious incidents, malfunctions and national risks impacting AI \n\nsystems and their operation. \n\nâ†’ Communication strategies to ensure transparency toward \n\ninternal and external stakeholders in relation to the \n\norganization's AI practices. \n\nâ†’ Training and awareness programs to enable staff with roles and \n\nresponsibilities for AI governance to help them understand and \n\nperform their respective roles. \n\nâ†’ Skills and capabilities development to assess human resources \n\ncapabilities and review or design job requirements. â†’ Part I. Understanding AI and governance \n\nAI Governance in Practice Report 2024  | 13 \n\nTABLE OF CONTENTS  â†‘\n\nAn effective AI governance \n\nmodel is about collective \n\nresponsibility and collective \n\nbusiness responsibility, \n\nwhich should encompass \n\noversight mechanisms such \n\nas privacy, accountability, \n\ncompliance, among others. \n\nThis responsibility should be \n\nshared by every stakeholder \n\nwho is part of the AI \n\ngovernance chain. \n\nVishal Parmar \n\nBritish Airways Global Lead Privacy Counsel \n\nand Data Protection Officer \n\nUnderstanding that AI systems, like all \n\nproducts, follow a life cycle is important as \n\nthere are governance considerations across \n\nthe life cycle. The  NIST AI RMF  sets out a \n\ncomprehensive articulation of the AI system \n\nlife  cycle and includes considerations for \n\ntesting, evaluation, validation, verification \n\nand key stakeholders for each phase. A more \n\nsimplified sample life cycle is included above, \n\nalong with some top-level considerations. \n\nThe AI life cycle \n\nPLANNING \n\nâ†’ Plan and document the \n\nsystem's concept and \n\nobjectives. \n\nâ†’ Plan for legal and \n\nregulatory compliance.  DESIGN \n\nâ†’ Gather data and check \n\nfor data quality. \n\nâ†’ Document and \n\nassess metadata and \n\ncharacteristics of \n\nthe  dataset. \n\nâ†’ Consider legal and \n\nregulatory requirements. \n\nDEVELOPMENT \n\nâ†’ Select the algorithm. \n\nâ†’ Train the model. \n\nâ†’ Carry out testing, \n\nvalidation and \n\nverification. \n\nâ†’ Calibrate. \n\nâ†’ Carry out output \n\ninterpretation. \n\nDEPLOYMENT \n\nâ†’ Pilot and perform \n\ncompatibility checks. \n\nâ†’ Verify legal and \n\nregulatory compliance. \n\nâ†’ Monitor performance \n\nand mitigate risks post \n\ndeployment. â†’ Part I. Understanding AI and governance    \n\n> AI Governance in Practice Report 2024 |14\n> TABLE OF CONTENTS â†‘\n\n## â†’ HOW TO \n\n## Navigate developers from deployers \n\nVarious contractual and regulatory obligations may arise \n\ndepending on whether an organization is a vendor or buyer, \n\nor  if it sources external services such as hardware, cloud or  data \n\ncollection, for the development and operations of  its  AI system. \n\nPrior IAPP  research  found more than 70% of organizations \n\nrely  at least somewhat on third-party AI, so the responsibility \n\nfor ensuring the AI system is safe and responsible may be \n\nspread  across multiple roles. \n\nIn both  current legislation  and  proposed legislation  we are starting to see \n\ndifferent obligations for those who provide and supply AI versus those who \n\ndeploy AI. Understanding whether you are a developer and/or deployer is \n\nimportant to ensuring you meet compliance obligations. Once this is understood, \n\nit is possible to establish AI-governance processes for  procurement , including \n\nevaluations and contracts to avoid taking on additional liabilities. \n\nâ†’ The World Economic Forum put together a useful  toolkit \n\nto help those who are procuring AI systems. AI Governance in Practice Report 2024  | 15 \n\n# Part II. \n\n# The data \n\n# challenge \n\n## Data is an integral part of training \n\n## and operating an AI system. \n\nMost AI requires sizeable amounts of high-quality data, \n\nespecially during the training phase to maximize the model's \n\nperformance, as well as to ensure the desired and accurate \n\noutput. With the advancement of new AI technologies, models \n\nare requiring increasingly more data, which may come from \n\na variety of sources. Given the importance of the data used to \n\nfuel the AI system, it is important to understand what data is \n\nbeing used; how, where and by whom it was collected; from \n\nwhom it was collected; if it is the right data for the desired \n\noutcome; and how it will be managed throughout the life cycle. \n\nAccessing data and identifying data  sources \n\nUnderstanding where data comes from and how it is collected is \n\nnot only necessary for AI systems, but also for building trust in AI \n\nby ensuring the lawfulness of data collection and processing. Such \n\ndocumentation can assist with  data transparency  and improve the \n\nAI system's auditability as well. \n\nAlthough data may originate from multiple sources, it can be \n\nbroadly categorized into three  types: first-party data, public \n\ndata  and third-party data. â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 16  \n\n> TABLE OF CONTENTS â†‘\n\nFirst-party data \n\nThis refers to data collected directly from \n\nindividuals by an organization through their own \n\ninteractions and transactions. Such data may \n\noriginate from sources such as website visits, \n\ncustomer feedback and surveys, subscriptions, \n\nand customer relationship management systems, \n\namong others. This data is extremely valuable for \n\norganizations as it provides direct and firsthand \n\ninsights into individuals' behavior. \n\nFirst-party data can be collected from various \n\nsources. Identifying the data channels and \n\ndocumenting the source will not only help the \n\norganization determine what types of data, e.g., \n\ntext, numerical, image or audio, will be collected \n\nfrom each source, but also alert the legal team \n\nabout where legal compliance will be required \n\non  the organization's part. \n\nPublic data \n\nThis refers to data that is available to the wider \n\npublic and encompasses a range of sources, \n\nsuch as publicly available government records, \n\npublications, and open source and web-scraped \n\ndata. Public data is a valuable resource for \n\nresearchers and innovators as it provides readily \n\navailable information. Public data can come \n\nfrom multiple sources. \n\nWhile it is arduous and cumbersome to \n\nmaintain data lineage for public datasets, \n\nit  is important for upholding organizational \n\nreputation and fostering user trust, legal \n\ncompliance and AI safety overall. A lack of \n\nunderstanding of where data comes from \n\neventually leads to a lack of understanding of \n\nthe training dataset and model performance, \n\nwhich can reinforce the black-box problem. \n\nTherefore, in the interest of transparency, \n\ntracking and documenting public-data sources \n\nas much as possible may prove beneficial for \n\nthe organization, as it can later support other \n\ntransparency efforts, such as drawing up data, \n\nmodel or system cards. \n\nMoreover, without knowledge of public-data \n\nsources, the organization may inadvertently \n\ntrain the AI system on personal, sensitive \n\nor proprietary data. From the privacy \n\nstandpoint, this can be problematic in cases \n\nof  data leakage , where personally identifiable \n\ndata may be exposed. AI security challenges \n\nmay also be amplified if data was procured \n\nfrom unsafe public sources, as that carries \n\nthe risk of introducing malicious bugs into \n\nthe system. It may also lead to biases in \n\nthe  AI system. \n\nEthical development \n\npractices start with \n\nresponsible data \n\nacquisition and \n\nmanagement systems, \n\nas well as review \n\nprocesses that track \n\nthe lineage of \n\nsourced data. \n\nChristina Montgomery \n\nIBM Vice President and \n\nChief Privacy and Trust Officer â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 17  \n\n> TABLE OF CONTENTS â†‘\n\nAn organization can begin by establishing a \n\nclear understanding of how and why public \n\ndata is being collected, how it aligns with the \n\npurposes the AI system will fulfil, if and how \n\nsystem accuracy will be affected by using \n\npublic data, what the trustworthy sources for \n\ngathering public data are, if the organization \n\nhas rights to use the public data, and other legal \n\nconsiderations that may have to be taken into \n\naccount, particularly given that public data is \n\ntreated differently across jurisdictions. \n\nThird-party data \n\nThis refers to data obtained or licensed by the \n\norganization from external entities that collect \n\nand sell data, such as data brokers. Datasets \n\npurchased from brokers are webbed together \n\nfrom a wide range of sources. While this may \n\nhave the benefit of providing insights into \n\na wider user base, the insights may not be \n\naccurate or may be missing key data. It may \n\nlack direct insights into customer behavior, as \n\nbrokers do not interact with the organization's \n\ncustomer base. \n\nThird-party data can also include open-source \n\ndata, available through open-source data \n\ncatalogues. Sometimes these databases are \n\nprovided by government or academic institutions \n\nwith a clear understanding of how the data was \n\ncollected and how it can be used, including a \n\nclear use license. Open-source data collected \n\nthrough other community efforts may not follow \n\nthe same collection and distribution practices. \n\nAs when using all data, it is important to know \n\nwhere the data came from, how it was collected, \n\nin which context it is meant to be used and what \n\nrights you have to use it. \n\nData quality \n\nThe quality of data that AI is trained and tested \n\non directly impacts the quality of the outputs and \n\nperformance, so ensuring the data is high quality \n\ncan help lay the initial foundations for a safe and \n\nresponsible AI system. Measuring  data quality \n\noften includes a few  baseline considerations .\n\nAccuracy confirms the correctness of data. \n\nThat is, whether the data collected is based \n\non real-world insights. Completeness refers to \n\nchecking for missing values, determining the \n\nusability of the data, and looking for any over \n\nor underrepresentation in the data sample. \n\nValidity ensures data is in a format that is \n\ncompatible with intended use. This may include \n\nvalid data types, metadata, ranges and patterns. \n\nConsistency refers to the relationships between \n\ndata from multiple sources and includes \n\nchecking if the data shows consistent trends \n\nand values it represents. Ideally, this process of \n\nensuring data quality is documented to support \n\ntransparency, explainability, data fairness, \n\nauditability, understanding of the data phase \n\nof  the life cycle and system performance. \n\nWithout understanding the quality \n\nof the data being ingested into an \n\nAI model, you may not know the \n\nquality of the output. Companies \n\nmust establish and define what â€˜data \n\nqualityâ€™ involves and consists of, as this \n\ndetermination is highly contextual for \n\nany organization, and can depend on \n\nbusiness goals, use cases, focus areas \n\nand fitness for purpose. \n\nRegardless of context, there are \n\nminimum baseline attributes which can \n\nand should be established: accuracy, \n\ncompleteness, consistency and \n\nvalidity. Timeliness and uniqueness \n\nmay also be important to establishing \n\nfitness for purpose. \n\nDera Nevin \n\nFTI Technology Managing Director â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 18 \n\nTABLE OF CONTENTS  â†‘\n\nAppropriate use \n\nOne of the most significant challenges when designing and \n\ndeveloping AI systems is ensuring the data used is appropriate \n\nfor the intended purpose. Often data is collected with one \n\nintention in mind or within a specific demographic area, and, \n\nwhile it might appear to be a useful dataset, upon further \n\nanalysis it might include data that does not match the industry or \n\ngeographic area of operation. When data is not fit for purpose, it \n\ncan skew the AI system's predictions or outcomes. \n\nWhen thinking about appropriate use, consider the \n\nproportionality of data required for the desired outcome. \n\nOften,  there are occurrences of collecting or acquiring more \n\ndata than necessary to achieve the outcome. It is important \n\nto understand if it is even necessary to collect and use certain \n\ndata  in your AI system. \n\nManaging unnecessary data, especially data that may contain \n\nsensitive attributes, can increase an organization's risk of a \n\nbreach or harm resulting from the use of AI. \n\nLaw and policy considerations \n\nApproaches can be categorized according to how the \n\ndata  was  collected. \n\n# Managing unnecessary data \n\n# can increase an organization's \n\n# risk of a breach or harm \n\n# resulting from the use of AI. â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 19  \n\n> TABLE OF CONTENTS â†‘\n\nFirst-party data \n\nWhere first-party data amounts to personal or \n\nsensitive data, relevant provisions may be triggered \n\nunder the data protection and privacy legislation of \n\nthe jurisdictions where the organization carries out \n\nits business, where the processing takes place or \n\nwhere the individuals concerned are located. \n\nThe EU General Data Protection Regulation, \n\nfor instance, has a default prohibition against \n\nprocessing of personal data, unless such \n\nprocessing falls under one of the  six bases  for \n\nlawful processing under Article 6(1): consent, \n\ncontractual performance, vital interest, legal \n\nobligation, public task and legitimate interest \n\npursued by a controller or third party. \n\nPublic data \n\nWeb scraping may involve compliance with the \n\nterms of service and privacy policies of websites. \n\nOtherwise, when an organization is aware the \n\npublic dataset contains personal or sensitive \n\ninformation, lawfulness of use may require \n\ncompliance with relevant data protection or \n\nprivacy laws, such as by acquiring valid consent. \n\nWhile web scraping, it is possible for copyrighted \n\ndata to be collected to train AI systems. \n\nAnother type of public data is open-source \n\ndata, which is publicly available software that \n\nmay include both code and datasets. Although \n\naccessible to the public, open-source software is \n\noften made available by the organization through \n\nvarious  open-source licensing schema . In addition \n\nto complying with the terms of the licenses, \n\norganizations using open-source data may also \n\nconsider conducting their own due diligence to \n\nensure the datasets were acquired lawfully, are \n\nsafe to use and were assessed for bias mitigation. \n\nThird-party data \n\nAs organizations have neither proximity to how \n\nthird-party data was first collected nor direct \n\ncontrol over the data governance practices of \n\nthird parties, an organization can benefit from \n\ncarrying out its own legal due diligence and \n\nthird-party risk management. The extent and \n\nintensity of this exercise will largely depend on \n\nthe organization's broader governance and risk-\n\nmanagement approach and the relevant facts. \n\nLegal due diligence may include verification \n\nof the personal data's lawful collection by the \n\ndata broker, review of contractual obligations \n\nand licenses, and identification of protected \n\nintellectual property interests. When data is \n\nlicensed, the organization will first have to \n\nlawfully procure rights to use data through a \n\nlicensing agreement. This will help maintain data \n\nprovenance and a clear understanding of data \n\nownership. The lawful and informed use of such \n\ndata at subsequent stages of the AI life cycle will \n\nalso be governed by the license. \n\nWith growing public concerns \n\nand increased regulation aimed \n\nat developing trustworthy, \n\ntransparent and performative \n\nAI systems, an internal data \n\ngovernance program is \n\nintegral to understanding and \n\ndocumenting metadata prior to \n\nusage, and to identifying risks \n\nassociated with lawful data use. \n\nChristina Montgomery \n\nIBM Vice President and \n\nChief Privacy and Trust Officer â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 20  \n\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## Joint statement by international data protection and privacy authorities on web scraping \n\nIn August 2023, 12 international data protection and privacy authorities released a  joint statement  to address data scraping on \n\nsocial media platforms and other publicly accessible websites. \n\nThe joint statement outlined: \n\nâ†’ Key privacy risks associated with data scraping, such as targeted \n\ncyberattacks, identity fraud, monitoring and profiling individuals, \n\nunauthorized political or intelligence gathering, and unwanted direct \n\nmarketing or spam. \n\nâ†’ How social media companies and other websites should protect \n\nindividuals' personal information from unlawful data scraping, such \n\nas through data security measures and multilayered technical and \n\nprocedural controls to mitigate the risk. \n\nâ†’ Steps individuals can take to minimize the privacy risks of scraping, \n\nincluding reading a website's privacy policy, limiting information \n\nposted online, and understanding and managing privacy settings. \n\nSome key takeaways from the joint statement include: \n\nâ†’ Publicly accessible personal information is still subject to data \n\nprotection and privacy laws in most jurisdictions. \n\nâ†’ Social media companies and other website operators hosting publicly \n\naccessible personal data have legal obligations to protect personal \n\ninformation on their platforms from unlawful data scraping. \n\nâ†’ Accessing personal information through mass data scraping can \n\nconstitute reportable data breaches in many jurisdictions. \n\nâ†’ Individuals can take steps to prevent their personal information from \n\nbeing scraped, and social media companies have a role to play in \n\nempowering users to engage with social media services in a manner \n\nthat upholds privacy. â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 21  \n\n> TABLE OF CONTENTS â†‘\n\nImplementing AI governance \n\nNumerous strategies are being leveraged to \n\nmanage data in the context of AI. \n\nData management plans \n\nAlongside ensuring the lawfulness of data \n\nacquisition, there are numerous measures an \n\norganization can take to keep track of where the \n\ndata used to train AI systems comes from. Such \n\norganizational practices are especially important \n\nwith the advent of generative AI, where training \n\ndata is merged from numerous sources. \n\nDeveloping a comprehensive plan for how data is \n\nmanaged across an organization is a foundational \n\nelement to managing all AI systems. Some \n\nconsiderations for data management plans include \n\nunderstanding what data is being used in which \n\nsystem; how it is collected, retained and disposed; \n\nif there is lawful consent to use the data; and who is \n\nresponsible for ensuring the appropriate oversight. \n\nIt is likely your organization is already keeping \n\ntrack of the data used across the organization. \n\nWhile there are additional considerations involved \n\nwhen using data for AI systems as discussed above, \n\nit is possible to add to your existing data workflows \n\nor management practices. It is important to \n\nconsider the use and management of data used for \n\nAI systems at every stage of the life cycle as there \n\nare different concerns and implications to consider \n\nduring different stages. If your organization does \n\nnot already have a data management practice, \n\nresources such as those from  Harvard Biomedical \n\nData Management  can help you get started. \n\nAdditionally, the data management plan should \n\nidentify relevant data standards, such as  ISO \n\n8000  for data quality, to set appropriate controls \n\nand targets for your organization to meet. Data \n\nstandards for aspects of AI are under development \n\nthrough various initiatives at the NIST, ISO/IEC \n\nand other national standards bodies. \n\nIBM believes it is essential for \n\ndata management practices \n\ntied to AI development to \n\ninclude advanced filtering \n\nand curation techniques \n\nto identify untrustworthy, \n\nprotected/sensitive, explicit, \n\nbiased/nonrepresentative or \n\notherwise unwanted data. \n\nChristina Montgomery \n\nIBM Vice President and \n\nChief Privacy and Trust Officer â†’ Part II. The data challenge \n\nAI Governance in Practice Report 2024  | 22  \n\n> TABLE OF CONTENTS â†‘\n\nData labels \n\nGrowing in importance,  data labels  are tools that can require \n\norganizations to provide information on how data was collected and \n\nused to train AI models. They are transparency artifacts of AI datasets \n\nthat explain the processes and rationale for using certain data and \n\nexplain how it was used in training, design, development and use. \n\nThis will help explain if the data being used is fit for purpose, if it is \n\nrepresentative of the demographics being served with the AI system \n\nand if the data meets relevant data quality standards. \n\nIdeally data labels are requirements of a robust data management \n\nprocess, which includes data quality and data impact assessments. \n\nWhile data labels are intended to provide documentation and \n\nawareness of the data being used, they can also assist with the \n\nassessment and review process. These tools should be aligned \n\nwhere  possible within the organization to avoid redundant efforts. \n\nData-source maintenance through documentation and inventories \n\ncan help organizations keep track of where the data is acquired \n\nand carry out relevant legal due diligence at first-party or \n\nthird-party levels. \n\nDedicated processes and functions \n\nWhen third-party data is used, it is important to follow the terms of \n\nservice and provide attribution where possible. This will also help \n\ninform users of the AI system where the data originated. Where \n\npossible, when data is being used from a third party, an appropriate \n\ndata sharing agreement with clear terms of use for both parties is \n\nhighly recommended. This helps to resolve any liability issues that \n\nmay arise as a result of using the system. \n\nINDUSTRY EXAMPLE \n\nWhen third-party organizations use publicly \n\navailable data, processes can be put into \n\nplace. Meta's  External Data Misuse  team \n\ndetects, blocks and deters web scraping. \n\nSome actions taken by the EDM team include \n\ndisabling accounts, serving cease-and-desist \n\nnotices, using CAPTCHAs for bot detection \n\nand blocking IP addresses where data \n\nscraping is identified. OpenAI has put in place \n\nan  opt-out process  for organizations that do \n\nnot want GPTbot to access their websites for \n\nthe purpose of web crawling. AI Governance in Practice Report 2024  | 23 \n\n# Part III. \n\n# The privacy and \n\n# data protection \n\n# challenge \n\nGiven that AI is a data-dependent enterprise and that privacy \n\nlaw governs the processing of personal data,  privacy laws  have \n\nemerged as a prominent mechanism for managing the key AI \n\ngovernance challenges. After all, information privacy seeks to \n\nprovide a framework \" for making ethical choices about how \n\nwe use new technologies .\"\n\nIndeed, national  data protection authorities  have been \n\namong the first to intervene and bring enforcement actions \n\nwhen AI-based products were thought to harm consumers. \n\nFor example, Italy's data protection authority, the Garante, \n\nimposed a  temporary ban  on ChatGPT after concluding the \n\nservice was in violation of the GDPR for lacking a legal basis \n\nfor processing and age-verification mechanism. \n\n## Privacy and data protection \n\n## governance practices are woven \n\n## into the AI life cycle. \n\nThe enforcement landscape for AI governance is \n\nincredibly unsettled. Which regulators will lead on what \n\nand how they will collaborate or conflict is subject \n\nto heavy debate and will differ by country, creating \n\nheightened uncertainty for organizations. Whether or \n\nnot privacy regulators have the lead remit, they will play \n\na key role given the centrality of data to AI governance. \n\nCaitlin Fennessy \n\nIAPP Vice President and Chief Knowledge Officer â†’ Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 24  \n\n> TABLE OF CONTENTS â†‘\n\nLaw and policy considerations \n\nThe OECD's  Guidelines Governing the \n\nProtection of Privacy and Transborder Flows \n\nof Personal Data  â€” developed in 1980 and \n\nrevised in 2013 â€” enshrine eight principles that \n\nhave served as the  foundation  for most global \n\nprivacy and data protection laws written over \n\nthe past several decades, including landmark \n\nlegislation such as the GDPR. These eight \n\nprinciples include collection limitation, data \n\nquality, purpose specification, use limitation, \n\nsecurity safeguards, openness, individual \n\nparticipation and accountability. \n\nMany DPAs around the world already put forth \n\nguidance  on how AI systems can work to align \n\nthemselves with these foundational principles of \n\ninformation privacy. Yet, as Australia's Office of \n\nthe Victorian Information Commissioner noted \n\nin a  resource  on issues and challenges of AI and \n\nprivacy, \"AI presents challenges to the underlying \n\nprinciples upon which the (OECD Privacy) \n\nGuidelines are based.\" To better understand \n\nwhere these challenges currently exist, each of \n\nthese principles is discussed below in the context \n\nof their applicability to â€” and potential conflict \n\nwith â€” the development of AI. \n\nCollection limitation \n\nThe principle of collection limitation states, \n\n\"There should be limits to the collection \n\nof personal data and any such data should \n\nbe obtained by lawful and fair means and, \n\nwhere appropriate, with the knowledge or \n\nconsent of the data subject.\" It most readily \n\ntranslates to the concept and practice of data \n\nminimization. GDPR Article 5(1)(c) emanates \n\nfrom this idea that data, at the collection stage, \n\nshould have some predefined limit or upper \n\nbound. Specifically, data collection should be \n\n\"... limited to what is necessary in relation to \n\nthe purposes for which they are processed.\" \n\nAs many observers have noted, this is one of \n\nthe privacy principles for which there appears \n\nto be an \" inherent conflict \" with AI systems \n\nthat rely on the collection and analysis of large \n\ndatasets. Performing adequate AI  bias testing ,\n\nfor example, requires collecting more data than \n\nmight otherwise be collected. \n\nAt Mastercard, we are testing \n\ninnovative tools and technologies \n\nto address some of the potential \n\ntensions between privacy and \n\nAI governance. For instance, we \n\nknow that a lot of data is needed, \n\nincluding sometimes sensitive \n\ndata, for AI to produce unbiased, \n\naccurate and fair outcomes. \n\nHow do you reconcile this with the \n\nprinciple of data minimization and \n\nthe need for individual's explicit \n\nconsent? We are exploring how \n\nthe creation of synthetic data can \n\nhelp, so as to achieve all desired \n\nobjectives at the same time. \n\nCaroline Louveaux \n\nMastercard Chief Privacy and Data Responsibility Officer â†’ Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |25\n> TABLE OF CONTENTS â†‘\n\nData quality \n\nThis is the principle that \"Personal data should be relevant to the \n\npurposes for which they are to be used, and, to the extent necessary for \n\nthose purposes, should be accurate, complete and kept up-to-date.\" Data \n\nquality is the privacy principle with which AI may be most in synchrony. \n\nThe  accuracy  of AI model outputs depends significantly on the quality of \n\ntheir inputs. A breakdown in  AI governance  can lead to data becoming \n\ninconsistent and error-laden, underscoring the need for AI-based \n\nsystems to orient themselves around the principle of data quality. Data \n\nbrokers and other companies can become the target of  enforcement \n\nactions  for failing to ensure the accuracy of the data they collect and sell. \n\nPurpose specification \n\nThe principle of purpose specification states, \"The purposes for which \n\npersonal data are collected should be specified ... and the subsequent \n\nuse limited to the fulfilment of those purposes ...\" Indeed, as the \n\nU.K. Information Commissioner's Office explained in the context \n\nof its  consultation  on purpose limitation in the generative AI life \n\ncycle, purposes of data processing \"must be specified and explicit: \n\norganizations need to be clear about why they are processing personal \n\ndata.\" This need for clarity applies not only to internal documentation \n\nand governance structures, but in communication with the people to \n\nwhom the personal data relates. In sum, organizations should be able \n\nto explain what personal data they process at each stage and why it is \n\nneeded to meet the specified purpose. \n\nA conflict with the purpose specification principle can arise if and \n\nwhen a developer wants to use the same training dataset to train \n\nmultiple models. The ICO advises developers reusing training data to \n\nconsider whether the purpose of training a new model is compatible \n\nwith the original purpose of collecting the training data. Considering \n\nthe reasonable expectations of those whose data is being reused can \n\nhelp an organization make a compatibility assessment. Currently, the \n\nICO considers collating repositories of web-scraped data, developing \n\na generative AI model and developing an application based on such a \n\nmodel to constitute different purposes under data protection law. â†’ Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 26  \n\n> TABLE OF CONTENTS â†‘\n\nUse limitation \n\nRelated to purpose specification, use \n\nlimitation  is the principle that states personal \n\ndata \"should not be disclosed, made available \n\nor otherwise used for purposes other than \n\nthose specified,\" except with the consent of \n\nthe data subject or by the authority of law. \n\nPurposes of use  must be specified at or before \n\nthe time of the collection, and subsequent \n\nuses must not be incompatible with the initial \n\npurposes of collection. \n\nThis is another principle that is challenged by \n\nAI systems, with potential  regulatory gaps  left \n\nby both the EU GDPR and EU AI Act. Proposals \n\nto address these gaps have included restricting \n\nthe training of models only to stated purposes \n\nand requiring alignment between training data \n\ncollection and the purpose of a model. \n\nSecurity safeguards \n\nUniting the fields of privacy, data protection \n\nand cybersecurity for decades is the principle \n\nthat \"Personal data should be protected \n\nby reasonable security safeguards against \n\nsuch risks as loss or unauthorized access, \n\ndestruction, use, modification or disclosure of \n\ndata.\" Ensuring the security of personal data \n\ncollected and processed is a key to building and \n\nmaintaining trust within the digital economy. \n\nRemedying problems of security and safety \n\nis and will remain a critical challenge for AI. \n\nEnsuring the actions of an AI system align \n\n\"with the values and preferences of humans\" is \n\ncentral to keeping these systems  safe . Yet, many \n\nAI systems remain susceptible to hacking and \n\nso-called \" adversarial attacks ,\" which are inputs \n\ndesigned to deceive an AI system, as well as data \n\npoisoning, evasion attacks and model extraction. \n\nExamples include forcing chatbots to provide \n\nanswers to responses to  harmful prompts \n\nor getting a self-driving vehicle's cameras to \n\nmisclassify a stop sign as a speed-limit sign. \n\nOpenness \n\nThe  right to be informed  and the principle of \n\ntransparency  are touchstones of global privacy \n\nand data protection laws. Beginning at the \n\ncollection stage and enduring throughout the life \n\ncycle of processing, these rights form the basis \n\nof organization's transparency obligations. They \n\noften require organizations to disclose various \n\ntypes of information, from the types of data \n\ncollected and how it is used to the availability of \n\ndata subjects' rights and how to exercise them to \n\nthe logic involved and potential consequences of \n\nany automated decision-making or profiling the \n\norganization engages in. The \"black-box\" nature \n\nof many AI systems can make this principle \n\nchallenging to navigate and adhere to. \n\nAs all AI and machine learning \n\nmodels are 100% data dependent, \n\nthe models must be fed high-\n\nquality, valid, verifiable data with \n\nthe appropriate velocity. As obvious \n\nas that may be, the challenges \n\naround establishing the governance \n\nrequirements that ensure the \n\nappropriate use of private data \n\nmay be far more complex. Modelers \n\nshould absolutely be applying the \n\nminimization principle of identifiable \n\ndata as they train. Adding private \n\ndata that could leak or cause bias \n\nneeds to be thought through early in \n\nthe design process. \n\nScott Margolis \n\nFTI Technology Managing Director â†’ Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |27\n> TABLE OF CONTENTS â†‘\n\nIndividual rights \n\nIndividual rights in privacy law commonly include the rights to \n\naccess, opt in/opt out, erasure, rectification and data portability, \n\namong others. Many privacy laws contain rights for individuals to \n\nopt-out of automated decision-making underpinned by AI systems. \n\nAccountability \n\nAccountability is arguably one of the most important principles when \n\nit comes to operationalizing organizational governance. Accountability \n\nis based on the idea that there should be a person and/or entity that is \n\nultimately responsible for any harm resulting from the use of the data, \n\nalgorithm and AI system's underlying processes. \n\nImplementing AI governance \n\nThe practice and professionalization of AI governance is a highly \n\nspecialized, stand-alone field requiring multidisciplinary expertise. \n\nA holistic approach to AI governance requires support from \n\nestablished subject-matter areas, including data protection and \n\ninformation governance practitioners. Data from past IAPP  research \n\nshows 73% of organizations are leveraging their existing privacy \n\nexpertise to manage AI governance. This is not surprising, as data is \n\na critical component of AI. Good AI governance weaves privacy and \n\ndata governance practices into the AI life cycle alongside AI-specific \n\nissues. This chapter demonstrates the overlapping nature of privacy \n\nand AI governance. \n\nApproaching the implementation of AI governance by adapting \n\nexisting governance structures and processes enables organizations \n\nto move forward quickly, responsibly and with minimal disruption to \n\ninnovation and the wider business. Target processes that may already \n\nbe established by organization's data protection program include: \n\naccountability, inventories, privacy by design and risk management. â†’ Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |28\n> TABLE OF CONTENTS â†‘\n\nAccountability \n\nPrivacy compliance programs are likely to \n\nhave established roles and responsibilities for \n\nthose with direct and indirect responsibility for \n\nprivacy compliance. These are likely supported \n\nby policies and procedures to help individuals \n\nfulfil the expectations of their role. Senior \n\nmanagement contributions are likely channeled \n\nthrough privacy committees, with mechanisms in \n\nplace to support risk-based escalation, reporting \n\non key metrics and decision-making. \n\nPrivacy leaders often have a direct line to CEOs \n\nand boards of directors, as well as a matrixed \n\nstructure of privacy champions across the \n\norganization to enable a multidisciplinary \n\napproach to privacy governance and ensure data \n\nprotection needs are considered by product and \n\nservice teams. This structure is well-suited to, and \n\ncan be leveraged for, AI governance given the need \n\nfor leadership engagement and skills spanning \n\nlegal, design, product and technical disciplines. \n\nWhere AI systems process personal data, \n\nthose  with accountability for privacy \n\ncompliance will need to ensure their existing \n\nprivacy compliance processes are set up to \n\naddress the intersection between AI and privacy. \n\nThis will include considering data inventory, \n\ntraining, privacy by design and other topics \n\nfurther outlined in this section. \n\nInventories \n\nPersonal data inventories have long been \n\nthe foundation of establishing a successful \n\nprivacy program and a key requirement of \n\nprivacy regulations. Knowing your data, \n\nhow it is collected and used, and being able \n\nto demonstrate this remains a core part of \n\naccountability. Organizations have also matured \n\nin their approaches, from lengthy spreadsheets \n\nto technology-enabled approaches. \n\nWhere AI systems use personal data, the data \n\ninventory can play a crucial role. Organizations \n\nthat have captured additional privacy compliance \n\nmetadata alongside the minimum regulatory \n\nrequirements may find their personal data \n\ninventories particularly useful in the age of AI. \n\nAdditional uses of this metadata could include a \n\nsingle source of truth for lawful basis to identify \n\nif additional use within AI models is permitted, \n\naccuracy metrics on personal data to support AI \n\nmodels to make accurate inferences based on \n\nthe latest personal data and a top-down view on \n\nprocesses relying on automated decision-making \n\nthat can be aligned with AI registries. â†’ Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 29  \n\n> TABLE OF CONTENTS â†‘\n\nEffective AI governance is underpinned by \n\nAI inventories with similar functionalities to \n\nthose of data inventories. AI registers can help \n\norganizations keep track of their AI development \n\nand deployment. Some functional requirements \n\nthat overlap with data inventories include the \n\nability to connect into the system-development \n\nlife cycle, maintenance and regular updates \n\nby  multiple users, and logging capability \n\nto  ensure integrity. \n\nPrivacy by design \n\nBy embedding privacy at the outset, privacy \n\nby design continues to be a critical part of \n\nhow organizations address privacy concerns. \n\nIn implementing privacy by design, privacy \n\nfunctions may take steps to map and embed \n\nprivacy into areas such as system-development \n\nlife cycles, project initiation and development \n\napproaches within an organization, risk \n\nmanagement and approval workflows, \n\nand  stage  gates. \n\nSteps may include developing AI-specific \n\nrisk-assessment workflows into existing \n\nrisk-assessment processes, enhancing existing \n\ncontrol catalogs with AI and privacy controls, \n\nor updating approval workflows to include \n\nstakeholders with AI accountabilities. \n\nAdditionally, the growing maturity of privacy \n\nenhancing technologies and their increasing \n\ntraction as technical measures within \n\norganizations may have benefits for the \n\ndevelopment of AI. With some PETs potentially \n\nhelping organizations reduce inherent risk \n\nof data use, an organization may be able to \n\nmaximize the strategic use of its data. Examples \n\ninclude using differential privacy in training \n\nmachine-learning models, federated learning \n\nand synthetic data. \n\nRisk management \n\nThe risk-based approach often adopted by \n\nglobal privacy regulations has been distilled into \n\norganizational risk-management efforts, which \n\nput privacy impact assessments at the heart of \n\ndeciding whether an organization can reduce \n\nharm from personal data processing through the \n\nimplementation of organizational and technical \n\nmeasures. Privacy risk can also stem from wider \n\nprivacy compliance activities and lessons learned \n\nin areas such as vendor risk, incident management \n\nand data subject requests management. \n\nLegal professionals need to keep an \n\nopen and flexible mind â€” technology \n\nbrings new challenges but also \n\nnew solutions. General counsel \n\nshould position themselves as \n\nthe center of a multidisciplinary \n\nteam of stakeholders across \n\ntheir organizations, including \n\nproduct design, compliance, data \n\nand privacy, which can deploy \n\nto manage multifaceted data \n\nrisks. Companies that strive for \n\nestablished best privacy practice \n\nwill more easily be able to comply \n\nwith the rising standards of global \n\nprivacy laws. \n\nTim de Sousa \n\nFTI Technology, Managing Director, Australia â†’ Part III. The privacy and data protections challenge \n\nAI Governance in Practice Report 2024  | 30 \n\nTABLE OF CONTENTS  â†‘\n\nPrivacy risk may already feed into wider enterprise risk-\n\nmanagement programs, such as information technology and \n\ncybersecurity risk and control frameworks. These can be enhanced \n\nto accommodate the complex types and sources of AI risk into \n\na unified risk-management framework at the enterprise level. \n\nThis approach can also facilitate crucial visibility across different \n\nsubject-matter practice areas across the business and enable a more \n\neffective analysis and treatment of  AI  risk. \n\nAs AI risk-management approaches mature, AI governance \n\nprofessionals face choices between embedding algorithmic impact \n\nassessments alongside or within PIAs. The need to align AI risk \n\nmanagement with broader enterprise risk-management efforts is \n\nof  equal importance. AI governance professionals will likely need \n\nto update enterprise risk-management strategies and frameworks to \n\nclearly factor in AI-related risks and document ongoing AI risks and \n\nremediations in a formal risk register. \n\nRisk-assessments \n\nA wide range of AI risk assessments are often talked about in the \n\nemerging global AI governance landscape. \n\nSome of these assessments are required by existing data \n\nprotection legislation, such as the GDPR, while others may \n\nemerge from AI-specific laws, policies and voluntary frameworks. \n\nFor the latter, laws and policies often provide AI governance \n\nsolutions with knowledge of the overlap. \n\n# Privacy risk may \n\n# already feed into wider \n\n# enterprise risk management \n\n# programs, such as IT and \n\n# cybersecurity risk \n\n# and control frameworks. â†’ Part III. The privacy and data protections challenge    \n\n> AI Governance in Practice Report 2024 |31\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## AI governance assessments: A closer look at EU DPIAs and FRIAs \n\nGDPR: DPIAs \n\nData protection impact assessments are required \n\nunder GDPR Article 35. DPIAs are particularly \n\nimportant where systematic and extensive evaluation \n\nof personal or sensitive aspects of natural persons \n\nthrough automated systems or profiling leads to legal \n\nconsequences for that person. Incorporating these \n\nassessments within the AI-governance life cycle can \n\nhelp organizations identify, analyze and minimize \n\ndata-related risks and demonstrate accountability. \n\nDPIAs at a minimum contain: \n\nâ†’ A systematic description of the anticipated \n\nprocessing, its purpose and pursued legitimate \n\ninterest. \n\nâ†’ A necessity and proportionality assessment in \n\nrelation to the intended purpose for processing. \n\nâ†’ An assessment of the risks to fundamental \n\nrights and freedoms. \n\nâ†’ Measures to be taken to safeguard security \n\nrisks and protect personal data. \n\nEU AI Act: FRIAs \n\nUnder the EU AI Act, FRIAS are required to be carried \n\nout in accordance with Article 27 by: \n\nâ†’ Law enforcement when they use real-time \n\nremote biometric identification AI systems, which \n\nare a prohibited AI practice under Article 5. \n\nâ†’ Deployers of high-risk AI systems that are \n\ngoverned by public law, private operators that \n\nprovide public services and operators deploying \n\ncertain high-risk AI systems referred to in \n\nAnnex III, point 5 (b) and (c), such as banking or \n\ninsurance entities. \n\nFRIAs are required only for the first use of the \n\nhigh-risk AI system, and the act permits deployers \n\nto rely on previously conducted FRIAs, provided all \n\ninformation about the system is up to date. FRIAs \n\nmust consist of: \n\nâ†’ Descriptions of the deployer's processes in \n\nline with intended use and purpose of the \n\nhigh-risk AI system. \n\nâ†’ Descriptions of the period and frequency of the \n\nhigh-risk AI system's use. \n\nâ†’ Categories of individuals or groups likely to be \n\naffected by the high-risk system. \n\nâ†’ Specific risks of harm that are likely to affect \n\nindividuals or groups. \n\nâ†’ Descriptions of the human oversight measures \n\nin place according to instructions of use. \n\nâ†’ Measures to be taken when risk \n\nmaterializes into harm, including \n\narrangements for internal governance \n\nand complaint mechanisms. \n\nHowever, AI governance solutions often foresee \n\nthe overlap with existing practices, and this is no \n\ndifferent under the EU AI Act. FRIAs, for instance, \n\ndo not need to be conducted for aspects covered \n\nunder existing legislation. As such, if a DPIA and \n\nFRIA have an overlapping aspect, that aspect \n\nneed only be covered under DPIA. AI Governance in Practice Report 2024  | 32 \n\n# Part IV. \n\n# The transparency, \n\n# explainability and \n\n# interpretability \n\n# challenge \n\n## The black-box problem \n\nOne reason for the lack of trust associated with AI systems is \n\nthe inability of users, and often creators, of AI systems to have \n\na clear understanding of how AI works. How does it arrive at a \n\ndecision? How do we know the prediction is accurate? This is \n\noften referred to as the \" black-box problem \" because the model \n\nis either too complex for human comprehension or it is closed \n\nand safeguarded by  intellectual property. \n\nAI techniques, such as deep learning, are becoming increasingly \n\ncomplex as they learn from terabytes of data, and the number \n\nof  parameters  has grown exponentially over the years. In July \n\n2023, Meta released its  Llama 2 model  with a parameter count \n\nat 70 billion. Google's  PaLM  parameter count is reported to be \n\nas large as 540 billion. Due to the self-learning abilities of AI, \n\nincluding their size and complexity, the black-box problem is \n\nincreasingly difficult to solve and often requires a trade-off to \n\nsimplify aspects of the system. \n\nTransparency is a term of broad scope, which can include the \n\nneed for technical and nontechnical documentation across the \n\nlife cycle. Having strong product documentation in place can \n\nalso provide commercial benefits by supporting the product \n\nsales cycle and helping providers to navigate prospective \n\nclients' due diligence protocols. â†’ Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 33 \n\nTABLE OF CONTENTS  â†‘\n\nIn the open-source context, transparency can also refer to providing \n\naccess to code or datasets in the open-source community to be used \n\nby AI systems. Transparency objectives can also include informing \n\nusers when they are interacting with an AI system or identifying \n\nwhen content was AI generated. Independent of how the term is \n\nused, transparency is a key tenet of AI governance due to the desire \n\nto understand how AI systems are built, managed and maintained. \n\nIt  is crucial that clear and comprehensive documentation is available \n\nto those who design and use these systems to ensure trust and help \n\nidentify where an error was made if an issue occurs. \n\nExplainability  refers to the understanding of how a black-box model, \n\ni.e., an incomprehensible or proprietary model, works. While useful, \n\nthe difficulty with black-box models is that the explanation may not \n\nbe entirely accurate or faithful to the underlying model, given its \n\nincomprehensibility. When full explainability is not possible due \n\nto  the factors mentioned above, an alternative is interpretability. \n\nInterpretability , on the other hand, refers to designing models that \n\ninherently make the reasoning process of the model understandable. \n\nIt encourages designing models that are not black boxes, with \n\ndecision or prediction processes that are comprehensible to domain \n\nexperts. In other words, interpretability is applied ante hoc. While \n\nit does away with the problems of explainable models, interpretable \n\nmodels are often domain specific and require significant effort to \n\ndevelop in terms of domain expertise. \n\nLaw and policy considerations \n\nOne proposed solution to the black-box challenge has been codifying \n\napproaches to and requirements for transparency, explainability and \n\ninterpretability in law or policy initiatives. Regulatory and voluntary \n\ngovernance tools that have established requirements for tackling the \n\nblack-box problem through transparency and explainability include \n\nthe EU GDPR and AI Act, NIST AI RMF, U.S. Executive Order 14110, \n\nChina's Interim Measures for the Management of Generative AI \n\nServices, and Singapore's AI Verify. \n\n# One reason for the lack of trust \n\n# associated with AI systems is the \n\n# inability for users, and often creators, \n\n# of AI systems to have a clear \n\n# understanding of how AI works. â†’ Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 34  \n\n> TABLE OF CONTENTS â†‘\n\nEU GDPR \n\nArguably one of the first legislative requirements \n\nfor AI governance,  GDPR  Articles 13(2)(f), 14(2) \n\n(g) and 15(1)(h) refer to providing meaningful \n\ninformation about the logic underpinning \n\nautomated decisions, as well as information \n\nabout the significance and envisaged \n\nconsequences of the automated decision-\n\nmaking for the individual. This is further \n\nsupported by Article 22 and Recital 71, which \n\nstate such decision-making should be subject to \n\nsafeguards , such as through the right to obtain \n\nan explanation to challenge an assessment. \n\nEU AI Act \n\nThe EU  AI Act  takes a risk-based approach to \n\ntransparency, with documentary and disclosure \n\nrequirements attaching to high-risk and general-\n\npurpose AI systems. \n\nIt mandates drawing up technical \n\ndocumentation for high-risk AI systems, and \n\nrequires high-risk AI systems to come with \n\ninstructions for use that disclose various \n\ninformation, including characteristics, \n\ncapabilities and performance limitations. \n\nTo  make high-risk AI systems more traceable, \n\nit also requires AI systems to be able to \n\nautomatically allow for the maintenance of \n\nlogs  throughout the AI life cycle. \n\nSimilarly, the AI Act places documentation \n\nobligations on providers of general-purpose \n\nAI systems with and without systemic risks. \n\nThis includes maintenance of technical \n\ndocumentation, including results from training, \n\ntesting and evaluation. It also requires  up-to-\n\ndate information and documentation to be \n\nmaintained for providers of AI systems who \n\nintend to integrate GPAI into their system. \n\nProviders of GPAI systems with systemic risks \n\nmust also publicly disclose sufficiently detailed \n\nsummaries of the content used for training GPAI. \n\nWith certain exceptions, the EU AI Act provides \n\nindividuals with the right to an explanation from \n\ndeployers of individual decision-making \"on the \n\nbasis of the output from a high-risk AI system \n\n... which produces legal effects or similarly \n\nsignificantly affects that person in a way that \n\nthey consider to have an adverse impact on \n\ntheir  health, safety or fundamental rights.\" \n\nIn addition to the documentary and disclosure \n\nrequirements, the AI Act seeks to foster \n\ntransparency by mandating machine-readable \n\nwatermarks. Article 50(2) requires machine-\n\nreadable watermarks for certain AI systems \n\nand  GPAI systems, so content can be detected \n\nas  AI generated or to inform users when they \n\nare  interacting with AI. \n\nThe EU is first out of the gate \n\nwith comprehensive AI legislation \n\nbut the EU AI Act is just the \n\ntip of the regulatory iceberg. \n\nMore guidance is coming and \n\nmany laws enacted since the \n\nearly 2000s, and under the recent \n\nEuropean Data Strategy, will have \n\nto be considered in AI governance \n\nprograms. The EU will continue \n\nto promote its approach to \n\nregulating AI on the global stage, \n\nfurthering the Brussels effect on \n\ndigital regulation. \n\nIsabelle Roccia \n\nIAPP Managing Director, Europe â†’ Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 35  \n\n> TABLE OF CONTENTS â†‘\n\nNIST AI RMF \n\nThe  NIST AI RMF  sees transparency, explainability and \n\ninterpretability as distinct characteristics of AI systems that \n\nsupport each other. Under the RMF, transparency is meant to \n\nanswer the \"what,\" explainability the \"how\" and interpretability \n\nthe  \"why\" of a decision. \n\nâ†’ Accountability and transparency:  The RMF defines \n\ntransparency as the extent to which information about an \n\nAI system and its outputs are made available to individuals \n\ninteracting with AI, regardless of whether they are aware \n\nof it. Meaningful transparency includes the disclosure of \n\nappropriate levels of information at different stages of the \n\nAI  life cycle, tailored to the knowledge or role of the individual \n\ninteracting with the system. This could include design \n\ndecisions, the model's training data and structure, intended \n\nuse-cases, and how and when deployment, post-deployment \n\nor end-user decisions were made and by whom. The RMF \n\nrequires AI transparency to consider human-AI interaction, \n\nsuch as by  notifying the human if a potential or actual adverse \n\noutcome  is  detected. \n\nâ†’ Explainable and interpretable AI:  The RMF defines \n\nexplainability as a representation of the underlying \n\nmechanisms of the AI system's operation, while it defines \n\ninterpretability as the meanings assigned to the AI outputs \n\nin the context of their designed functional purpose. \n\nLack  of explainability can be managed by describing how \n\nthe system functions by tailoring such descriptions to the \n\nknowledge, roles and skills of the individual, whereas lack \n\nof interpretability can be managed by describing why the \n\nAI  system gave a specific output. \n\nIt's important to align on a set of ethical AI principles \n\nthat are operationalized through tangible responsible AI \n\npractices, rooted in regulations, e.g. EU AI Act, and best \n\npractice frameworks, e.g. NIST AI RMF, when developing \n\nAI features. At Workday, we take a risk-based approach to \n\nresponsible AI governance. \n\nOur scalable risk evaluation dictates relevant guidelines \n\nsuch as requirements to map, measure, and manage \n\nunintended consequences including bias. Within the \n\nWorkday AI Feature Fact Sheets, Workday provides \n\ntransparency to customers on each feature such as, \n\nwhere relevant, how they were assessed for bias. \n\nThese safeguards are intended to document our efforts \n\nto develop AI features that are safe and secure, human \n\ncentered, and transparent and explainable. \n\nBarbara Cosgrove \n\nWorkday Vice President, Chief Privacy Officer â†’ Part IV. The transparency, explainability and interpretability challenge    \n\n> AI Governance in Practice Report 2024 |36\n> TABLE OF CONTENTS â†‘\n\nU.S. Executive Order 14110 \n\nU.S. Executive Order 14110  approaches \n\ntransparency through an AI safety perspective. \n\nUnder Section 4, the safety and security of AI \n\ntechnology is to be ensured through certain \n\ntransparency measures, such as the requirements \n\nto share results of safety tests and other \n\nimportant information with the U.S. government, \n\nthat have been imposed on developers of the \n\nmost powerful AI systems. Watermarks to label \n\nAI-generated content are also required under the \n\norder, with the purpose of protecting Americans \n\nfrom AI-enabled fraud and deception. \n\nChina's Interim Measures for the \n\nManagement of Generative AI Services \n\nArticle 10 of China's  Interim Measures for the \n\nManagement of Generative AI Services  requires \n\nproviders of AI services to clarify and disclose \n\nthe uses of the services to user groups and to \n\nguide their scientific understanding and lawful \n\nuse of generative AI. Watermarking AI-generated \n\ncontent is also a requirement under Article 11. \n\nSingapore's AI Verify \n\nSingapore's  AI Verify  is a voluntary testing \n\nframework  on AI governance for organizational \n\nuse comprised of two  parts : a testing framework \n\ngrounded in 11 internationally accepted \n\nprinciples grouped into five pillars and a \n\ntoolkit  to execute technical tests. \n\nTransparency and explainability themes \n\nare among the 11 principles embedded in \n\nAI Verify. The framework addresses the \n\ntransparency problem by providing impacted \n\nindividuals with appropriate information \n\nabout AI use in a technological system so they \n\ncan make informed decisions on whether to \n\nuse that AI enabled system. Explainability, \n\non the other hand, is achieved through an \n\nunderstanding of how an AI model reaches \n\na decision, so individuals are aware of the \n\nfactors that contributed to a resulting output. \n\nTransparency  is assessed through documentary \n\nevidence and explainability is assessed through \n\ntechnical tests. â†’ Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 37 \n\nTABLE OF CONTENTS  â†‘\n\nImplementing AI governance \n\nOrganizations have been active in coming up with tools \n\nand techniques to address the black-box transparency and \n\nexplainability challenge. \n\nModel and system cards \n\nModel cards are short documents that accompany an AI model to \n\nprovide  transparent  model reporting  by disclosing information about \n\nthe model. Information may include explanations about intended \n\nuse, performance metrics and benchmarked evaluation in various \n\nconditions such as across different cultures, demographics or race. \n\nIn addition to providing transparency, model cards are also meant to \n\ndiscourage use of models outside their  intended uses . At the industry \n\nlevel, use of model cards is becoming more prominent as evidenced \n\nby publicly accessible model cards for Meta and Microsoft's  Llama 2 ,\n\nOpenAI's  GPT-3  and Google's  face-detection model .\n\nIt may not always be easy to explain a model in a short document. \n\nModel cards are to serve a broad audience and, therefore, \n\nstandardizing explanations may prove either too simplistic for one \n\naudience or too complicated for another. Moreover, organizations \n\nshould also be mindful of how much information they reveal in the \n\ncards to prevent adversarial attacks and mitigate security risks. \n\nAI models are often part of a larger system comprised of a group \n\nof  models and technologies that work together to give outputs. \n\nAs  a result, model cards can fall short of providing a more nuanced \n\npicture of how different models interact together within the system. \n\nThat is where  system cards  can help achieve better insights. \n\n# Organizations have been active \n\n# in coming up with tools \n\n# and techniques to address \n\n# the black-box transparency \n\n# and explainability challenge. â†’ Part IV. The transparency, explainability and interpretability challenge \n\nAI Governance in Practice Report 2024  | 38  \n\n> TABLE OF CONTENTS â†‘\n\nSystem cards explain how a group of AI models \n\nand other AI and non-AI technologies work \n\ntogether as part of an AI system to achieve \n\nspecific tasks. Meta released  22 system cards \n\nexplaining how AI powers its Facebook and \n\nInstagram platforms. Each card has four \n\nsections that detail: \n\nâ†’ An overview of the AI system. \n\nâ†’ How the system works by summarizing the \n\nsteps involved in creating experiences on \n\nFacebook and Instagram. \n\nâ†’ How the shown content can be customized. \n\nâ†’ How AI delivers content as part of the \n\nwhole  system. \n\nAI systems learn from their environments and \n\nconstantly evolve, so the way they work also \n\nchanges over time, requiring updates to the \n\nsystem cards. Like  with model cards, reducing \n\ntechnical concepts to a standardized language \n\nthat serves all audiences can be challenging for \n\nsystem cards, and system cards can also attract \n\nsecurity threats based on the amount and type \n\nof  information shared. \n\nThe utility of model and system cards can go \n\nbeyond meeting transparency challenges. \n\nMaintaining standardized records about the \n\nmodel itself can facilitate communication and \n\ncollaboration between various stakeholders \n\nthroughout the life cycle. This can also help \n\nwith bias and security-risk mitigation. They are \n\nalso useful for making comparisons with future \n\nversions of the models to track improvements. \n\nThe cards provide a documented record of \n\ndesign, development and deployment, so they can \n\nfacilitate attribution of responsibility for various \n\ndecisions and outcomes related to the model or \n\nsystem. Auditors can use them not only to gain \n\na holistic understanding of the system itself, but \n\nalso to zoom in on the processes and decisions \n\nmade during different phases of the life cycle. \n\nINDUSTRY EXAMPLE \n\nMeta released  22 system cards \n\nexplaining how AI powers \n\nits Facebook and Instagram \n\nplatforms. Each card has four \n\nsections that detail: \n\n1. An overview of the AI system. \n\n2. How the system works by \n\nsummarizing the steps involved \n\nin creating experiences on \n\nFacebook and Instagram. \n\n3. How the shown content can \n\nbe customized. \n\n4. How AI delivers content as part \n\nof the whole system. â†’ Part IV. The transparency, explainability and interpretability challenge    \n\n> AI Governance in Practice Report 2024 |39\n> TABLE OF CONTENTS â†‘\n\nOpen-source AI \n\nAnother approach to addressing the black-box \n\nchallenge is making AI open source. This requires \n\nmaking the source code public and allowing users \n\nto view, modify and distribute it freely. Open \n\naccess can be especially useful for researchers \n\nand developers, as there is more potential for \n\nscrutiny by a wider, diverse and collaborative \n\ncommunity of experts. In turn, that can lead to \n\nimprovements to the transparency of algorithms, \n\nthe detection of risks and the offering of solutions \n\nif things go wrong. Open-source AI can also \n\nimprove technology access and drive collaborative \n\ninnovation, which may otherwise be limited by \n\nproprietary algorithms. \n\nWatermarking \n\nWith the rise of generative AI, it is becoming \n\nincreasingly difficult to distinguish AI-generated \n\ncontent from human-created content. To ensure \n\ntransparency, the watermarking or labeling of \n\nAI generated content has been legally mandated \n\nunder the EU AI Act, U.S. Executive Order 14110 \n\nand state-level requirements, and China's Interim \n\nMeasures for Management of Generative AI. \n\nAt IBM, we believe that \n\nopen technology and \n\ncollaboration are essential \n\nto further the responsible \n\nadoption of AI. An open \n\napproach can support \n\nefforts to develop and \n\nimplement leading \n\ntechnical methods, such \n\nas those used during the \n\ntesting and evaluation of \n\ndata and AI systems. \n\nChristina Montgomery \n\n> IBM Vice President and Chief Privacy and Trust Officer\n\nINDUSTRY EXAMPLE \n\nMeta's Llama is open source, and it \n\naims to power innovation through \n\nthe open-source community. Given \n\nthe safety and security concerns \n\nassociated with generative AI models, \n\nLlama comes with a  responsible-use \n\nguide  and an  acceptable-use policy .\n\nOn the other hand,  OpenAI  has taken \n\na closed approach toward its large \n\nlanguage models, such as GPT-3 and \n\nGPT-4, in the interest of maintaining \n\nits competitive advantage, as well as \n\nto ensure AI safety. â†’ Part IV. The transparency, explainability and interpretability challenge    \n\n> AI Governance in Practice Report 2024 |40\n> TABLE OF CONTENTS â†‘\n\nINDUSTRY EXAMPLE \n\nGoogle uses a technology called  SynthID , which directly \n\nembeds watermarks into Google's text-to-image generator \n\nImagen. Meta has moved toward labeling \n\nAI-generated images  on Facebook, Instagram and \n\nThreads. Although Meta already adds the label \"Imagined \n\nwith AI\" on images generated through its AI feature, \n\nit now also aims to work with industry partners on \n\ncommon standards to add multilingual labels on synthetic \n\ncontent generated with tools of other companies that \n\nusers post on Meta's platforms. Specifically, Meta \n\nis relying on Partnership on AI's  best practices , the \n\nCoalition for Content Provenance and Authenticity's \n\nTechnical Specifications  and the International Press \n\nTelecommunications Council's  Technical Standards  to add \n\ninvisible markers at scale to label AI generated content by \n\ntools of companies such as Google, Microsoft and OpenAI. \n\nWatermarking is gaining traction as a way for organizations to \n\npromote transparency and ensure safety against harmful content, \n\nsuch as misinformation and disinformation. Companies are \n\nembedding watermarks on AI-generated content. Watermarks are \n\ninvisible to the human eye, but are machine readable and can be \n\ndetected by computers as AI generated. \n\nWhile watermarking is becoming a popular technique for \n\ntransparency, it is still not possible to label all AI generated content. \n\nMoreover,  techniques  to break watermarks also exist. \n\nFocusing on the building blocks of AI governance \n\nâ€” like appropriate documentation of AI models and \n\nsystems â€” is important because those foundations \n\nare necessary to enable risk management, impact \n\nassessment, and third-party auditing. \n\nMiranda Bogen \n\n> Center for Democracy and Technology\n> AI Governance Lab Director\n\nAI Governance in Practice Report 2024  | 41 \n\n# Part V. The bias, \n\n# discrimination \n\n# and fairness \n\n# challenge \n\nBias, discrimination and fairness are among the most \n\nimportant challenges of AI governance, given their \n\npotentially very significant real-world impacts on individuals \n\nand communities. Leaving this challenge unaddressed can \n\nlead to discriminatory outcomes and perpetuate inequalities \n\nat scale. Healthy AI governance must promote legal and \n\nethical norms including human rights, professional \n\nresponsibility, human-centered design and control of \n\ntechnology, community development and nondiscrimination. \n\nWhile the automation of human tasks using AI has the \n\nadvantages of scalability, efficiency and accuracy, it is \n\naccompanied by the challenge of  algorithmic bias , whereby \n\na systematic error manifests through an inaccuracy in the \n\nalgorithm. It occurs when an algorithm systematically or \n\nrepeatedly misses certain groups of people more than others. \n\nWith transparency challenges around how or why an input \n\nturns into a particular output, biases in the algorithm can be \n\ndifficult to trace and identify. \n\nInstances of algorithmic bias have been well documented in \n\npolicing , criminal sentencing  and  hiring . Algorithmic bias can \n\nimpact even the most well-intentioned AI systems, and it can \n\nenter a  model  or system in numerous ways. \n\n## Hidden and harmful biases may lurk \n\n## within an AI system. â†’ Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 42  \n\n> TABLE OF CONTENTS â†‘\n\nWays biases may get into the AI system \n\nBiases may get into the AI system in multiple \n\nways during the input, training and output stages. \n\nAt the input stage \n\nâ†’ Historical data.  If historical data used \n\nto train algorithms is biased, then the \n\nalgorithm may learn those biases and \n\nperpetuate them. For example, if an AI \n\nrecruitment tool is trained on historical \n\ndata  containing gender or racial biases, \n\nthose biases will be reflected in the tool's \n\nhiring decisions or predictions. \n\nâ†’ Representation bias.  Biases can also enter \n\nthe algorithm through data that either \n\noverrepresents or underrepresents social \n\ngroups. This can make the algorithmic \n\ndecisions less accurate and create \n\ndemographic or social disparities. \n\nâ†’ Inaccurate data.  The accuracy of data \n\ncan be impaired if it is outdated or \n\ninsufficient. Such data falls short of fully \n\nrepresenting current realities, leading to \n\ninaccurate results, which may also lead to \n\nreinforcement of historical biases. \n\nAt the training stage \n\nâ†’ Model.  Biases can arise when they are an \n\nintrinsic part of the model itself. For example, \n\nmodels developed through traditional \n\nprogramming, i.e., those manually coded by \n\nhuman designers, can have  intrinsic biases \n\nif they are not based on real-world insights. \n\nAn algorithm assisting with university \n\nadmissions may be biased if the human \n\ndesigner programmed it to give a higher \n\npreference to students from private schools \n\nover students from public schools. Intrinsic \n\nbiases may be difficult to spot in AI models, \n\nas they are a result of self-learning and make \n\ncorrelations across billions of data points, \n\nwhich are often part of a black box. \n\nâ†’ Parameters.  The model adjusts its \n\nparameters, such as  weights and biases \n\nin neural networks, during the training \n\nprocess based on the training data. Bias \n\ncan manifest when the values assigned to \n\nthese parameters inadvertently reinforce \n\nthe bias present in the training data or the \n\ndecisions made by the designers during \n\narchitecture selection. In an algorithm for \n\nuniversity admissions, for example, the \n\nattributes of leadership and competitiveness \n\ncan reflect a gender stereotype present \n\nin the training data with the algorithm \n\nfavoring male candidates over female \n\nones. However, bias in parameters can also \n\nmanifest more stealthily, such as through \n\nproxies. In absence of certain data, the \n\nalgorithm will make correlations to make \n\nsense of the missing data. An algorithm \n\nfor loan approval, for example, may \n\ndisproportionately assign more weight \n\nto certain zip codes and the model may \n\ninadvertently perpetuate racial or ethnic \n\nbias by rejecting loan applications using \n\nzip  codes as a proxy. \n\nAt Microsoft, we are steadfast \n\nin our commitment to \n\ndeveloping AI technologies \n\nthat are not only innovative \n\nbut also trustworthy, safe, and \n\nsecure. We believe that the \n\ntrue measure of our progress is \n\nnot just in the capabilities we \n\nunlock, but in the assurance \n\nthat the digital experiences \n\nwe create will enhance \n\nrather than compromise the \n\nhuman experience. \n\nJulie Brill, \n\nMicrosoft Chief Privacy Officer, Corporate Vice President â†’ Part V. The bias, discrimination and fairness challenge    \n\n> AI Governance in Practice Report 2024 |43\n> TABLE OF CONTENTS â†‘\n\nAt the output stage \n\nâ†’ Self-reinforcing biases.  A feedback loop is a process through \n\nwhich the AI system continues to learn based on the outputs \n\nit generates. The output goes back into the system as an input, \n\nwhich can influence the system's behavior or performance in \n\nsome positive or negative way. While feedback loops can foster \n\ncontinuous learning and allow the system to adapt to its deployed \n\nenvironment, they can also lead to  self-reinforcing biases  if the \n\noutputs of the algorithm itself are biased. For example, if an \n\nalgorithm consistently rejects loan applications for women and \n\nconsistently approves them for men, there may be a gender bias at \n\nplay, and the algorithm could fall into a loop where it learns from \n\nthe biased outputs and continues to reinforce the biased pattern. \n\nâ†’ Human oversight.  Although it is necessary to have humans in \n\nthe loop throughout the life cycle of the AI system, there is a \n\nrisk that human biases can reenter the algorithm. For example, \n\nhuman control over a system's final output is necessary, but \n\nbias can externally impact the output based on the human \n\ninterpretation  applied to that final output. \n\nâ†’ Automation bias.  Automation bias refers to the human \n\ntendency to overly rely on automated outputs. This leads \n\nto people trusting the recommendations of algorithms \n\nwithout questioning or verifying their accuracy or being \n\nmindful of the system's limitations and errors. This can be \n\nespecially dangerous when confirmation bias about protected \n\ncharacteristics is at play. That is, users are more likely to accept \n\nthe outputs when they align with their preexisting beliefs. \n\nBias detection and mitigation is particularly challenging \n\nin the  context of foundation models due to their size and \n\ncomplex architectures. â†’ Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 44  \n\n> TABLE OF CONTENTS â†‘\n\nLaw and policy considerations \n\nMany existing equalities and antidiscrimination laws apply to AI \n\nsystems and many emerging initiatives specific to AI governance \n\ninclude provisions on bias. \n\nDepending on the jurisdiction where the organization operates, \n\nliability could also fall under relevant civil rights, human rights \n\nor  constitutional freedoms of that jurisdiction. \n\nIn the U.S., civil rights can be protected through private rights \n\nof action by individuals. For example, according to the guidance \n\nprovided by the U.S. Equal Employment Opportunity Commission, \n\nprivate rights of action against discrimination through algorithms \n\ncould occur under the  Americans with Disability Act  and  Title VII \n\nof the Civil Rights Act . Under both the ADA and Title VII, employers \n\ncan be exposed to liability even where their algorithmic decision-\n\nmaking tools are designed or administered by another entity. When \n\nindividuals think their rights under either of those laws have been \n\nviolated, they can file a charge of discrimination with EEOC. \n\nOECD AI Principles \n\nThe principle of \"human-centered values and fairness\" in the OECD \n\nAI Principles  requires respect for the rule of law, human rights and \n\ndemocratic values across the life cycle of the AI system, through \n\nrespect for nondiscrimination and equality, diversity, fairness, and \n\nsocial justice. This is to be implemented through safeguards, like \n\ncontext-appropriate human determination that is consistent with \n\nthe state of the art. The OECD AI Policy Observatory maintains \n\na catalogue on  tools and metrics  for practically aligning AI with \n\nOECD's principles, including  bias and fairness .\n\nâ†’ SPOTLIGHT \n\nJoint statement by US Federal Agencies \n\nIn April 2023, a  joint statement  made by four federal \n\nagencies, namely the EEOC, Department of Justice, \n\nFederal Trade Commission and Consumer Financial \n\nProtection Bureau, reiterated the U.S.'s commitment to \n\nthe principles of fairness, equality and justice, which \n\nare deeply embedded in federal laws. In April 2024, five \n\nadditional cabinet-level agencies joined that pledge. \n\nThe joint statement now includes the Department of \n\nEducation, Department of Health and Human Services, \n\nDepartment of Homeland Security, Department of \n\nHousing and Urban Development, and Department of \n\nLabor. The Consumer Protection Branch of the Justice \n\nDepartment's Civil Division also joined the pledge. \n\nEnforcement of discrimination through automated \n\nmeans is managed by these federal agencies. â†’ Part V. The bias, discrimination and fairness challenge    \n\n> AI Governance in Practice Report 2024 |45\n> TABLE OF CONTENTS â†‘\n\nUNESCO Recommendations on the Ethics of AI \n\nPrinciples 28, 29 and 30 of UNESCO's  Recommendations on the \n\nEthics of AI  encourage AI actors to promote access to technology \n\nto diverse groups, minimize the reinforcement or perpetuation \n\nof discriminatory or biased outcomes throughout the life cycle \n\nof the AI systems, and reduce the global digital divide. Among \n\nthe tools provided by UNESCO for the practical implementation \n\nof its recommendations is the  ethical impact assessment , which \n\nis designed primarily for government officials involved in the \n\nprocurement of AI systems but can also be used by companies to \n\nassess if an AI system aligns with UNESCO's Recommendations. \n\nEU AI Act \n\nThe EU  AI Act  provides a relevant framework for data governance \n\nof high-risk AI systems under Article 10, which permits training, \n\nvalidation and testing of datasets to examine the possibility of biases \n\nthat affect the health and safety of persons and negatively impact \n\nfundamental rights or lead to discrimination. To deal with the challenge \n\nof self-reinforcing biases, Article 15(4) also requires the elimination \n\nor reduction of biases emanating from feedback loops in high-risk AI \n\nsystems after they have been put on the market or into service. The \n\nEU AI Act calls for consideration of the European Commission's  Ethics \n\nGuidelines for Trustworthy AI , which are voluntary guidelines seeking \n\nto promote \"diversity, non-discrimination and fairness.\" \n\nSingapore \n\nSingapore's AI Verify tackles bias via the principle of \"ensuring \n\nfairness.\" This principle is made up of the pillars of data governance \n\nand fairness. While there are no specific tests for data governance \n\nin the toolkit, if the model is not giving biased outputs based on \n\nprotected characteristics, fairness can be ensured by checking the \n\nmodel against  ground truth . Process checks include the verification of \n\ndocumentary evidence that there is a strategy for fairness metrics and \n\nthat the definition of sensitive attributes is consistent with the law. â†’ Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 46  \n\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## US FTC Enforcement priorities and concerns \n\nWe have made no secret of our enforcement priorities \n\nand concerns. \n\n# 1.  There is no AI exemption from the laws on the books and \n\nbusinesses need to develop and deploy AI tools in ways that \n\nallow for an open and competitive market and protect consumers \n\nfrom potential harms. \n\n# 2.  We are scrutinizing existing and emerging bottlenecks across the AI \n\ndesign stack to ensure that businesses aren't using monopoly power \n\nto block innovation and competition. \n\n# 3.  We are acutely aware that behavioral advertising, brought on by \n\nweb 2.0, fuels the endless collection of user data and recognize \n\nthat model training is emerging as another feature that could further \n\nincentivize surveillance. \n\n# 4.  We are squarely focused on aligning liability with capability and \n\ncontrol, looking upstream and across layers of the AI stack to \n\npinpoint which actor is driving or enabling the lawbreaking. \n\n# 5.  We are focused on crafting effective remedies in cases \n\nthat establish bright-line rules on the development, use and \n\nmanagement of AI inputs, such as prohibiting the uses of inaccurate \n\nor highly-sensitive data when training models. \n\nSamuel Levine \n\nFTC Bureau of Consumer Protection Director â†’ Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 47  \n\n> TABLE OF CONTENTS â†‘\n\nThe US \n\nIn the U.S., antidiscrimination laws that also \n\nextend to AI are scattered across various sectors, \n\nsuch as employment, housing and civil rights. \n\nâ†’ Employment.  Under the  Americans with \n\nDisabilities Act , employers are prohibited from \n\nusing algorithmic decision-making tools that \n\ncould violate the act, such as in not providing \n\nreasonable accommodations, intentionally \n\nor  unintentionally screening out an individual \n\nwith a disability, or adopting disability-related \n\ninquiries and medical examinations. \n\nâ†’ Housing.  In 2023, to ensure fairness in \n\nhousing, the Biden-Harris Administration \n\nissued a proposed rule against racial bias in \n\nalgorithmic  home valuations , empowering \n\nconsumers to take action against appraisal \n\nbias, increasing transparency and leveraging \n\nfederal data to inform policy and improve \n\nenforcement against appraisal bias. \n\nâ†’ Consumer finance.  The  CFPB  confirmed \n\ncompanies are not absolved of their legal \n\nresponsibilities under existing legislation, \n\nsuch as the  Equal Credit Opportunity Act ,\n\nwhen they use AI models to make lending \n\ndecisions. Remedies include compensating \n\nthe victim, providing injunctive relief to \n\nstop unlawful conduct, or banning persons \n\nor companies from future participation in \n\nthe marketplace. \n\nâ†’ Voluntary frameworks.  The  NIST \n\nSpecial  Publication 1270: Towards a \n\nStandard for Identifying and Managing \n\nBias in AI  lays down governance standards \n\nfor managing  AI bias. These include \n\nmonitoring the system for biases, \n\nmaking  feedback channels available \n\nso users can flag incorrect or harmful \n\nresults  for which they can seek recourse, \n\nputting policies and procedures in \n\nplace for every stage of the life cycle, \n\nmaintaining model documentation to \n\nensure accountability, and embedding \n\nAI governance within the culture of \n\nthe  organization. \n\nAs we navigate the \n\ntransformative potential of \n\nAI, it is imperative that we \n\nanchor our journey in our \n\ncollective ability to protect \n\nfundamental rights and the \n\nenduring values of safety \n\nand accountability. \n\nJulie Brill \n\nMicrosoft Chief Privacy Officer, \n\nCorporate Vice President â†’ Part V. The bias, discrimination and fairness challenge    \n\n> AI Governance in Practice Report 2024 |48\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## FTC enforcement action against Rite Aid \n\nOn 19 Dec. 2023, the FTC issued an  enforcement action  against \n\nRite Aid's  discriminatory use  of facial recognition technology. \n\nRite  Aid deployed facial recognition surveillance systems for theft \n\ndeterrence without assessing the accuracy or bias of the system. \n\nRite Aid recorded thousands of false-match alerts, and the FTC's gender-based \n\nanalysis revealed Black, Asian, Latino and women consumers were more likely to \n\nbe harmed by Rite Aid's surveillance technology. \n\nThe FTC placed a five-year moratorium on Rite Aid's use of facial recognition, \n\nand if after five years Rite Aid chooses to use this technology again, it will have \n\nto implement the FTC's governance plan detailed in the  order . The enforcement \n\ndecision also included an order for  disgorgement , that is, to delete or destroy any \n\nphotos and videos including any data, models or algorithms used for surveillance. \n\nThis case serves as good indication of the nature and intensity \n\nof liability that deployers and providers of AI in the U.S. may \n\nbe exposed to for deploying discriminatory AI systems. â†’ Part V. The bias, discrimination and fairness challenge \n\nAI Governance in Practice Report 2024  | 49  \n\n> TABLE OF CONTENTS â†‘\n\nImplementing AI governance \n\nOne overarching practice used to mitigate biases is \n\nthe promotion of diversity and inclusivity among \n\nteams working across the life cycle of the AI \n\nsystem. Personnel composition is often supported \n\nby organization-level principles for safe and \n\nresponsible AI, many of which internal AI ethics \n\npolicies, e.g.,  Google , IBM  and  Microsoft .\n\nBias testing \n\nOne way to minimize bias in AI systems is by \n\ntesting the systems. While there are  numerous \n\nways  to test for bias in AI systems, it is important \n\nto understand what is being evaluated. \n\nDemographic parity may be different than equality \n\nobjectives. Establish goals based on the desired \n\nsystem outcomes to start, and then establish an \n\nappropriate technique for testing bias within the \n\nsystem. For example, does fairness mean an equal \n\nnumber of males and females will be screened for \n\na new position on your team, or that candidates \n\nwith the most distinguished resumes are identified \n\nas ideal applicants independent of their gender, \n\nrace, experience, etc. \n\nIt is important to note that often testing for bias \n\nwill require the use of personal information \n\nto determine if fairness objectives are being \n\nmet. As such, there may be a  privacy-bias \n\ntrade-off , as safeguarding privacy through data \n\nminimization creates  challenges  for mitigating \n\nbiases in AI systems. Some considerations when \n\nbalancing privacy while mitigating bias include: \n\nâ†’ Intentionally collecting sensitive data \n\ndirectly in the design phase so it is ready \n\nfor the testing phase. This can be done by \n\nprocuring consent from data subjects and \n\ndisclosing the purpose for the collection and \n\nprocessing of their data. \n\nâ†’ Creating intentional proxies to test how the \n\nsystem makes correlations without sensitive \n\ndata, such as for demographic features. \n\nâ†’ Buying missing data from data brokers, \n\npublic data or other datasets in compliance \n\nwith privacy and data governance policies. \n\nFairness tests and debiasing \n\nmethods are not created \n\nequally â€” as an AI deployer \n\nor governance professional, \n\nit is critically important to \n\nuse tools and methods that \n\nfundamentally align with \n\nequality and nondiscrimination \n\nlaw in your jurisdiction. \n\nBrent Mittelstadt \n\nUniversity of Oxford Internet Institute Director of Research, \n\nAssociate Professor and Senior Research Fellow AI Governance in Practice Report 2024  | 50 \n\n# Part VI. \n\n# The security \n\n# and robustness \n\n# challenge \n\nCompromises to the security of AI could result in manipulation \n\nof outputs, stolen sensitive information or interference with \n\nsystem operations. Unsecured AI can result in financial losses, \n\nreputational damage and even physical harm. For example, \n\nexploiting the vulnerabilities of medical AI could lead to a \n\nmisdiagnosis, and adversarial attacks on autonomous vehicles \n\ncould lead to road traffic accidents. \n\nAlthough AI security overlaps with and suffers from traditional \n\ncybersecurity risks, cybersecurity is often about protecting \n\ncomputer systems and networks from attacks, whereas AI \n\nsecurity is about guarding the AI system's components, namely \n\nthe data, model and outputs. When it comes to AI security, \n\nmalicious actors can enable adversarial attacks by exploiting \n\nthe  inherent limitations  of AI algorithms. \n\n## The pace, scale, and reach of AI \n\n## development and integration \n\n## demands strong security. â†’ Part VI. The security and robustness challenge    \n\n> AI Governance in Practice Report 2024 |51\n> TABLE OF CONTENTS â†‘\n\nAdversarial attacks \n\nAdversarial attacks are a deliberate attempt to \n\nmanipulate models in a way that leads to incorrect \n\nor harmful outputs. The intention behind \n\nthe attack could be to lead the model toward \n\nmisclassification or cause harm, and all it may take \n\nto trick the model is a slight switching of pixels or \n\nadding a bit of noise. Some types of adversarial \n\nattacks include: \n\nâ†’ Evasion attacks.  The aim of evasion \n\nattacks is to deceive the model into \n\nmisclassifying data, such as by adding a \n\nsmall perturbation to the input image, as \n\nin the  MIT example , leading to an incorrect \n\noutput with high  confidence. \n\nâ†’ Data poisoning.  This can happen in various \n\nways, such as by switching the labels of \n\nlabeled data or injecting entirely new data \n\ninto the dataset. However, for this to work, \n\nthe adversary will have to first gain access \n\nto training data.  Data poisoning  can also \n\nhelp attackers create  backdoors  so they can \n\nmanipulate model behavior in the future. \n\nâ†’ Model extraction.  The aim of model \n\nextraction is model theft by reverse \n\nengineering to reveal the hidden mechanism \n\nof the model or sensitive information, or \n\nto make the model vulnerable to further \n\nattacks. This is done by feeding carefully \n\ncrafted  queries  to a black-box model to \n\nanalyze its outputs and steal its functionality. \n\nThis can help the adversary copy the model \n\nand make financial gains. \n\nAI vulnerabilities can also be exploited through \n\nopen-source software and third-party risks. \n\nâ†’ Open-source software can be manipulated \n\nin many ways, such as through supply-chain \n\nattacks, in which open-source AI libraries \n\nare targeted by malicious code that is planted \n\nas a legitimate update or functionality. \n\nAlthough open-source software suggests \n\neverything has been made publicly available, \n\nthe original developers can restrict access \n\nto some parts of the software in the license \n\nagreement. In such cases, hackers may resort \n\nto model extraction. Even if an AI  system \n\nis not open source, the project may rely on \n\na complex ecosystem of open-source tools, \n\nexposing itself to a potential attack surface \n\nthat malicious actors can exploit. \n\nâ†’ A lack of control and visibility over third-\n\nparty governance practices makes risk \n\nmitigation more difficult, including with \n\nrespect to security. Third-party vendors \n\nmay have weaker security standards and \n\npractices, making them more vulnerable \n\nto data breaches, supply chain attacks and \n\nsystem hacks, among other security risks. â†’ Part VI. The security and robustness challenge    \n\n> AI Governance in Practice Report 2024 |52\n> TABLE OF CONTENTS â†‘\n\nLaw and policy considerations \n\nRegulatory and voluntary governance tools that \n\nhave established requirements for tackling AI \n\nsecurity issues include the NIS2 Directive, U.S. \n\nExecutive Order 14110, the NIST AI RMF and the \n\nNIST Cybersecurity Framework. \n\nNIS2 \n\nThe  NIS2  Directive replaces the EU Network \n\nand Information Security Directive from 2016. \n\nIt  aims to boost resilience and incident-response \n\ncapacities in public and private sectors through \n\nrisk management and reporting obligations. \n\nSome cybersecurity requirements under Article \n\n21 include policies on risk analysis and system \n\nsecurity, incident handling, supply-chain \n\nsecurity, policies and procedures to assess \n\ncybersecurity risk-management effectiveness, \n\ncyber hygiene practices, policies on the use of \n\ncryptography, and encryption. \n\nEU AI Act \n\nAs with most AI themes, under the EU  AI Act ,\n\nsecurity takes a risk-based approach. As such, \n\nsecurity and robustness requirements vary based \n\non if the system is high risk or if it is a GPAI system \n\nwith systemic risks. \n\nâ†’ High-risk AI systems.  The EU AI Act \n\nlays down detailed security obligations \n\nfor accuracy, security and robustness \n\nof high-risk AI systems. Technical and \n\norganizational measures are to be placed to \n\nensure high-risk systems are resilient toward \n\nerrors, faults and inconsistencies. Possible \n\nsolutions include back-up or fail-safe plans. \n\nThe act also foresees risks emerging at the \n\nthird-party level, requiring resilience against \n\nunauthorized third-party attempts to alter \n\nuse, outputs or performance by exploiting \n\nthe system vulnerabilities. Technical \n\nsolutions to handle such security risks must \n\nbe appropriate to circumstances and risk. \n\nThese can include measures to prevent, \n\ndetect, respond to, resolve and control data \n\npoisoning, model poisoning, adversarial \n\nexamples and model evasion, confidentiality \n\nattacks, or model flaws. \n\nAdditionally, the EU AI Act obliges providers \n\nof high-risk AI systems to ensure they \n\nundergo conformity assessments that \n\ndemonstrate compliance with requirements \n\nfor high-risk systems. \n\nâ†’ Obligations for providers of GPAI systems \n\nwith systemic risks.  The EU AI Act lists the \n\nsecurity requirements for high-impact AI \n\nsystems. Requirements include: \n\nâ€¢ Evaluating models in accordance with \n\nstandardized protocols, such as conducting \n\nand documenting adversarial testing to \n\nidentify and mitigate systemic risks. \n\nâ€¢ Monitoring, documenting and reporting \n\nserious incidents to the AI Office. \n\nâ€¢ Ensuring GPAI models with systemic risks \n\nand their physical infrastructures have \n\nadequate levels of cybersecurity. â†’ Part VI. The security and robustness challenge \n\nAI Governance in Practice Report 2024  | 53  \n\n> TABLE OF CONTENTS â†‘\n\nUS  Executive  Order 14110 \n\nThe U.S.  AI Executive Order 14110  calls on \n\ndevelopers of the most powerful AI systems \n\nto share their safety results and other critical \n\ninformation with the U.S. government. It also \n\ncalls on the NIST to develop rigorous standards \n\nfor extensive red-team testing to ensure safety \n\nbefore public release. \n\nNIST AI RMF \n\nThe NIST  AI RMF  identifies common security \n\nconcerns such as data poisoning and exfiltration \n\nof models, training data or other intellectual \n\nproperty through AI system endpoints. Under \n\nthe AI RMF, a system is said to be secure when \n\nit can maintain confidentiality, integrity and \n\navailability through protection mechanisms that \n\nprevent unauthorized access and use. Practical \n\nimplementation can be achieved through the \n\nNIST Cybersecurity Framework and  RMF .\n\nNIST Cybersecurity Framework \n\nThe NIST  Cybersecurity Framework  is a voluntary \n\nframework that provides standards, guidelines \n\nand best practices for organizations to mitigate \n\ncybersecurity risks. The framework is organized \n\nunder five  key functions : identify, protect, detect, \n\nrespond and recover. \n\nThere is an urgent need to \n\nrespond to the complex \n\nchallenges of AI governance \n\nby professionalizing the field. \n\nA professionalized workforce \n\ncan take AI governance from \n\ntheory to practice, spread \n\ntrustworthy and standardized \n\npractices across industries \n\nand borders, and remain \n\nadaptable to swiftly changing \n\ntechnologies and risks. \n\nJ. Trevor Hughes \n\nIAPP President and CEO â†’ Part VI. The security and robustness challenge \n\nAI Governance in Practice Report 2024  | 54  \n\n> TABLE OF CONTENTS â†‘\n\nImplementing AI governance \n\nDue diligence in the identification of security \n\nrisks throughout the life cycle of the system is \n\nan important activity, especially when a third-\n\nparty vendor is involved. Due diligence can only \n\never inform. With appropriate information, an \n\norganization can seek contract terms with third-\n\nparty vendors that mandate: \n\nâ†’ Making the vendor's security practices \n\ncompatible with the organization's \n\nown  standards. \n\nâ†’ Monitoring system robustness regularly \n\nthrough security assessments or audits to \n\nidentify third-party risks and ensure the \n\nvendor is complying with the organization's \n\nsecurity standards. \n\nâ†’ Limiting access to third-party vendors only \n\nfor the services they need to perform. \n\nRed teaming \n\nRed teaming is the process of testing the security \n\nof an AI system through an  adversarial lens  by \n\nremoving defender bias. It involves the simulation \n\nof adversarial attacks on the model to evaluate \n\nit against certain benchmarks, \" jailbreak \" it and \n\nmake it behave in unintended ways. Red teaming \n\nreveals security risks, model flaws, biases, \n\nmisinformation and other harms, and the results \n\nof such testing are passed along to the model \n\ndevelopers for remediation. Developers use red \n\nteaming to bolster and secure their product before \n\nreleasing it to the public. \n\nSecure data sharing practices \n\nDifferential privacy is primarily a  privacy-\n\nenhancing technique  that also has security \n\nbenefits, it analyzes group data while \n\npreserving individual privacy by adding \n\ncontrolled noise to the data and blurring \n\nindividual details. So, even if an attacker were \n\nto steal this data, they would not be able to \n\nlink it back to specific individuals, minimizing \n\nharm. As such, differential privacy can limit \n\nthe utility of stolen data. However, that impact \n\nto the utility of the data can also impact \n\norganizations with lawful and legitimate \n\ninterests in processing the data. Moreover, \n\ndifferential privacy can also be a costly \n\ntechnique to implement, especially where \n\nlarge  datasets are concerned. \n\nHITL \n\nHuman in the loop refers to incorporating \n\nhuman expertise and oversight into the \n\nalgorithmic decision-making process. Although \n\nHITL may provide a gateway for human \n\nbiases to reenter the algorithm when making \n\njudgements about final outputs, in the context \n\nof  AI security, HITL can make incident detection \n\nand response more efficient. This is especially \n\ntrue where subtle manipulations or attacks that \n\nthe model may not have been trained to identify \n\nare involved.  HITL  allows for continuous \n\nmonitoring and verification, however, optimal \n\nuse of this approach rests on balancing the \n\ncontradictions that may arise to address bias \n\nor  safety and security. \n\nINDUSTRY EXAMPLE \n\nOpenAI's latest text-to-video \n\nmodel,  Sora , was red teamed. \n\nIn preparation for the U.K. AI \n\nSafety Summit, Meta released \n\na document  detailing the \n\nsafety of its Llama 2 model, \n\nas well as the benchmarks and \n\npotential attack vectors it \n\nwas red teamed for. AI Governance in Practice Report 2024  | 55 \n\n# Part VII. \n\n# AI safety \n\nVarious themes, particularly value alignment, transparency \n\nand AI security, eventually culminate into the broader \n\ntheme of AI safety. Given that safety is an all-encompassing \n\ntheme, it has no settled global definition. It may include \n\npreventing so-called existential risks posed by artificial \n\ngeneral intelligence. For some, such as the  Center for AI \n\nSafety , AI risk is categorized based on malicious use, the AI \n\nrace, rogue behavior and organizational risks. For others, \n\nsuch as the country signatories to the  Bletchley Declaration ,\n\nand most recently, for parties to the  Seoul Declaration for \n\nSafe, Innovative and Inclusive AI , it is about managing risks \n\nand being prepared for unexpected risks that may arise from \n\nfrontier AI. AI safety can also be the term used to describe \n\nminimizing AI harms from misinformation, disinformation \n\nand deepfakes, and the unintended behavior of an AI system, \n\nespecially advanced AI systems. \n\n## AI safety is a cornerstone but \n\n## somewhat mercurial principle for \n\n## realizing safe and responsible AI. â†’ Part VII. AI safety    \n\n> AI Governance in Practice Report 2024 |56\n> TABLE OF CONTENTS â†‘\n\nLaw and policy considerations \n\nThe importance of AI safety is reflected in the fact \n\nthat, for some jurisdictions, it has been embedded \n\nas a main theme in national strategies toward \n\nAI. The Biden-Harris Administration's Executive \n\nOrder 14110 focuses on developing \"Safe, Secure \n\nand Trustworthy\" AI. In 2023, the U.K. brought \n\nworld leaders together for first AI Safety Summit, \n\nand the country's approach toward AI is focused \n\non the safety of advanced AI systems, or \"frontier \n\nAI.\" Safety is also an important factor under the \n\nEU AI Act, which is reflected in the security and \n\nrobustness requirements for high-impact GPAI \n\nsystems and  high-risk AI systems. \n\nAI safety institutes \n\nRecently, the NIST announced it would establish \n\nthe  U.S. AI Safety Institute . To support this \n\ninstitute, the NIST also created an  AI Safety \n\nInstitute Consortium , which brought more than \n\n200 organizations together to develop guidelines \n\nand standards for AI measurement and policy \n\nthat can lay the foundation for AI safety globally. \n\nAmong many security- and safety-related \n\ninitiatives, the AISIC is tasked with enabling \n\ncollaborative and interdisciplinary research \n\nand establishing a knowledge and data sharing \n\nspace for AI stakeholders. More specifically, \n\nthe AISIC will develop new guidelines, tools, \n\nmethods, protocols and best practices to \n\nfacilitate the evolution of industry standards \n\nfor AI safety. The AISIC will also develop \n\nbenchmarks for evaluating AI capabilities, \n\nespecially harmful ones. \n\nThe U.K. government established an  AI Safety \n\nInstitute  to build a sociotechnical infrastructure \n\nthat can minimize risks emerging from \n\nunexpected advancements in AI technology. \n\nThe institute has been entrusted with three \n\nmain functions: developing and conducting \n\nevaluations on advanced AI systems, driving \n\nfoundational AI research, and facilitating the \n\nexchange of information. â†’ Part VII. AI safety \n\nAI Governance in Practice Report 2024  | 57  \n\n> TABLE OF CONTENTS â†‘\n\nBletchley Declaration \n\nThe 2023 U.K. AI Safety Summit brought \n\ntogether international governments, leading \n\nAI  companies and civil society groups to discuss \n\nfrontier AI risks and ways to promote AI safety. \n\nAs a demonstration of their commitments to \n\nAI safety, participating nations also signed \n\nthe Bletchley Declaration, which makes \n\nvarious affirmations to cooperate globally \n\non innovation, sustainable development, \n\neconomic growth, protection of human rights \n\nand fundamental freedoms, and building public \n\ntrust and confidence in AI technology. \n\nEU AI Act \n\nThe security requirements for general-purpose \n\nAI systems under the AI Act are also focused \n\non regulating \"systemic risks.\" The EU AI act \n\ndefines this risk as one emerging from high-\n\nimpact general purpose models that \"significantly \n\nimpact the internal market, and with actual or \n\nreasonably foreseeable negative effects on public \n\nhealth, safety, public security, fundamental \n\nrights, or the society as a whole, that can be \n\npropagated at  scale across the value chain.\" \n\nAI Safety Standards \n\nISO/IEC Guide 51:2014  provides requirements \n\nand recommendations for drafters of standards \n\nto include safety aspects in those standards. It \n\napplies to safety aspects pertaining to people, \n\nenvironments or both. \n\nWe are generating 2.5 \n\nquintillion bytes of data \n\nglobally per day. Much of this \n\nis flowing into our internet. \n\nTherefore, generative AI \n\nmodels are dynamic and the \n\napplications that are built on \n\ntop of them will move. It is up \n\nto the organizations to ensure \n\nthat the movement meets \n\ntheir standards. \n\nDominique Shelton Leipzig \n\nMayer Brown Partner, Cybersecurity & Data Privacy \n\nand Leader, Global Data Innovation & AdTech â†’ Part VII. AI safety    \n\n> AI Governance in Practice Report 2024 |58\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## Compute governance \n\nOn a broader level, AI safety also refers to regulating compute, i.e., the \n\npower source of AI systems, as regulating AI at its source increases the \n\nvisibility of its technical capabilities. Unlike AI models, which can be \n\nreplicated exponentially and without control, compute must be purchased \n\nand is quantifiable. As computing chips are manufactured through highly \n\nconcentrated  supply chains  and dominated by only a few companies, \n\nregulatory interventions can be more focused. Such  regulation  can \n\npurposefully occur with AI safety in mind to control the allocation of \n\nresources for AI projects by subsidizing or limiting access to compute \n\nor by building guardrails into hardware. \n\nWith compute governance gaining traction because of advanced AI \n\nsystems, compute thresholds, i.e., numerical measures of computing \n\npower, are also being set legally, which helps distinguish AI systems with \n\nhigh capabilities from other AI systems. \n\nFor instance, U.S. Executive Order 14110 requires models using computing \n\npower greater than 10 26  integer and models using biological sequence \n\ndata and computing power greater than 10 23  integer to provide the \n\ngovernment with information and reports on the models testing and \n\nsecurity on an ongoing basis. \n\nSimilarly, under the EU AI Act, GPAI is presumed to have high-impact \n\ncapabilities when cumulative compute used for training is greater than \n\n10 25  floating-point operations. When a model meets this threshold, the \n\nprovider must notify the Commission, as meeting the threshold leads to \n\nthe presumption that this is a GPAI system with systemic risk. This means \n\nthe model can have a significant impact on the internal market, and actual \n\nor reasonably foreseeable negative effects on health, safety, fundamental \n\nrights or society. Providers need to comply with requirements on model \n\nevaluation, adversarial testing, assessing and mitigating systemic risks, \n\nand reporting any serious incidents. â†’ Part VII. AI safety \n\nAI Governance in Practice Report 2024  | 59  \n\n> TABLE OF CONTENTS â†‘\n\nImplementing AI governance \n\nThe organizational practices for security and \n\nrobustness discussed in this report, such as \n\nred teaming for adversarial testing, HITL and \n\nprivacy-preserving technologies, can apply to \n\nAI safety. Similarly, organizational practices and \n\nlaws requiring transparency and explainability, \n\nspecifically watermarks, also apply to AI safety. \n\nPrompt engineering \n\nOne of OpenAI's  safety practices  includes \n\nprompt engineering  to help generative AI \n\nunderstand prompts in a given context. This \n\npractice is aimed at minimizing harmful and \n\nundesired outputs from generative AI, and it \n\nhelps developers exercise more control over \n\nuser interactions with AI to reduce misuse at \n\nthe user level. Moreover, as part of  product \n\nsafety standards , OpenAI also has put in \n\nplace  usage policies .\n\nReports and complaints \n\nAnother safety practice of OpenAI is allowing \n\nusers to report issues that can be monitored \n\nand responded to by human operators. This is \n\nnot yet a popular practice. A 2023 study carried \n\nout by  TrustibleAI  found out of 100 random \n\norganizations, three provided an individual \n\nappeals process between the individual and \n\nthe company. It is possible internal governance \n\nand complaint mechanisms may become \n\nmore common post-EU AI Act, given that, \n\nunder Article 27 (f), deployers of AI systems \n\nmust carry out FRIAs of internal governance \n\nand complaint mechanisms where a  risk has \n\nmaterialized into a harm. \n\nSafety by design \n\nTo combat abusive AI-generated content, \n\nMicrosoft is focused on building strong safety \n\narchitecture through the  safety by design \n\napproach, which can be applied at the AI \n\nplatform, model and application levels. Some \n\nefforts include red teaming, pr eemptive \n\nclassifiers, blocking abusive prompts, automated \n\ntesting and rapid bans of users who abuse the \n\nsystem. With regard to balancing freedom of \n\nspeech against abusive content, Microsoft is \n\nalso committed to identifying and removing \n\ndeceptive and abusive content on LinkedIn, \n\nMicrosoft Gaming Network and other  services. \n\nHumans control AI, not the \n\nother way around. Generative \n\nAI models drift. The only way \n\nfor companies to know when/ \n\nhow they are drifting is to \n\ncontinuously test, monitor and \n\naudit the AI applications for high \n\nrisk use cases- every second of \n\nevery minute of every day. This \n\nis the only way to ensure that \n\nthe model output comports with \n\nthe organizationâ€™s pre-installed \n\nguardrails for accuracy, health \n\nand safety, privacy, bias. \n\nDominique Shelton Leipzig \n\nMayer Brown Partner, Cybersecurity & Data Privacy and Leader, \n\nGlobal Data Innovation & AdTech â†’ Part VII. AI safety    \n\n> AI Governance in Practice Report 2024 |60\n> TABLE OF CONTENTS â†‘\n\nSafety policies \n\nIn preparation for the U.K. AI Safety Summit, Meta released an \n\noverview of its  AI safety policies , specifically in relation to its \n\ngenerative AI Llama model. In addition to model evaluations and \n\nred-team analysis, the policy also detailed Meta's model reporting \n\nand sharing, reporting structure for vulnerabilities found after \n\nmodel release, post-deployment monitoring for patterns of misuse, \n\nidentifiers of AI generated material, data input controls and audits, \n\nand priority research on societal, safety and security risks. \n\nIndustry best practices \n\nPartnership on AI has invested extensively in AI safety research and \n\nresources. Some of its work includes  Guidance for Safe Foundation \n\nModel Deployment . This framework is a living document targeted at \n\nmodel providers on ways to operationalize AI safety for responsible \n\ndeployment. The framework provides  custom guidance  providers of \n\nfoundation models can follow throughout the deployment process \n\nthat is appropriate for their model's capabilities. Another resource \n\nis PAI's  SafeLife , which is a benchmark focused on avoiding negative \n\nside effects in complex environments.  SafeLife  is a reinforcement \n\nlearning environment that tests the \"safety of reinforcement \n\nlearning agents and the algorithms that train them.\" It allows \n\nagents to navigate a complex environment to accomplish a primary \n\ntask. The aim is to create a \"space for comparisons and improving \n\ntechniques for training non-destructive agents.\" AI Governance in Practice Report 2024  | 61 \n\n# Part VIII. \n\n# The copyright \n\n# challenge \n\nCopyright  refers to the rights that creators have over the \n\nexpression of their artistic or intellectual works. Although it is \n\nnot possible to provide an exhaustive list of \"works\" covered by \n\ncopyright legislation, globally copyright protection has been \n\nextended to include a wide range of works, such as literature, \n\nmusic, architecture and film. In the context of modern \n\ntechnology, computer software programs, e-books, online \n\njournal publications and the content of websites such as news \n\nreports and databases are also copyrightable. \n\n## Generative AI is raising new \n\n## challenges for copyright law. \n\nThe clear establishment of intellectual property \n\nrights around both inputs and outputs for generative \n\nAI models is of crucial importance to creative artists \n\nand the creative industries. In the face of dramatically \n\ngrowing machine capabilities, we need to make sure that \n\nincentives for human creation remain strong.\" \n\nLord Tim Clement-Jones \n\nU.K. House of Lords Liberal Democrat Peer and \n\nSpokesperson for Science, Innovation and Technology â†’ Part VIII. The copyright challenge \n\nAI Governance in Practice Report 2024  | 62  \n\n> TABLE OF CONTENTS â†‘\n\nLaw and policy considerations \n\nIn  most countries , and especially those party \n\nto the  Berne Convention , copyright protection \n\nis obtained automatically upon creation of the \n\nwork. In other words, copyright registration is \n\nnot necessary for proprietarily safeguarding \n\nartistic and intellectual works. Regardless, \n\nwhile offering automatic copyright protection, \n\nmany countries, including the U.S., also allow \n\nvoluntary  copyright registration. \n\nCopyright provides owners two types of rights: \n\neconomic rights, through which the owner can \n\nmake financial gains by authorizing use of their \n\nwork by others through a license, and moral \n\nrights, which include noneconomic interests such \n\nas the right to claim authorship of a work or to \n\noppose changes to a work that could harm the \n\nowner's reputation. \n\nCopyright  protects artistic and intellectual works \n\nby preventing others from copying, adapting, \n\ndistributing, performing or publicly displaying \n\nthe work, or creating derivative works. When \n\nan individual does any of these without the \n\nauthorization of the rights' owner, this may \n\nconstitute copyright infringement. \n\nThe use of copyright protected content requires \n\nthe authorization of the original author, unless a \n\nstatutory copyright exception applies. A legitimate \n\nexception to copyright infringement in some \n\njurisdictions is fair use or fair dealing. This is a \n\nlimitation on the exclusive rights of a copyright \n\nholder, which sometimes allows the use of the \n\nwork without the right holder's permission. \n\nIn the U.S., fair use is statutorily defined \n\nunder  17 U.S. Code Â§ 107 , and four \n\nfactors assist courts in making a fair-use \n\ndetermination. These include purpose and \n\ncharacter of use, nature of copyrighted work, \n\nsubstantiality of use, and impact of use on \n\nthe potential market of the copyrighted \n\nwork. Similarly, Singapore's  Copyright Act \n\nof 2021  also includes a fair-use exemption \n\nand takes into account the same four factors \n\nas the U.S. courts. Singapore's old copyright \n\nlaw also had a fifth factor, which considered \n\nthe possibility of obtaining a work within a \n\nreasonable time at an ordinary commercial \n\nprice. However, under the new law, the fifth \n\nfactor may be considered by courts only \n\nwhen relevant. \n\nThe U.K. also has a permitted exemption \n\nto copyright infringement termed  fair \n\ndealing . There is no statutory definition for \n\nfair dealing as, depending on the case, it \n\nwill always be a matter of fact, degree and \n\nimpression. Other factors the U.K. courts \n\npreviously considered to determine fair \n\ndealing include the effect on the market for \n\nthe original work and whether the amount of \n\nwork copied was reasonable and  appropriate. \n\nCommon remedies that can be granted by \n\na court ruling on copyright infringement \n\ninclude injunctions, damages for the loss \n\nsuffered, statutory damages, infringer's \n\nprofits, surrender or destruction of infringing \n\narticles, and attorney fees and costs. \n\nThough copyright has emerged \n\nas one of the first and foremost \n\nfrontiers between AI and intellectual \n\nproperty, the full gamut of IP \n\nrights are engaged by AI, and \n\nspecifically generative AI: design \n\nrights, performersâ€™ rights, patents \n\nand trademarks. Anthropocentric \n\napproaches to IP will butt up against \n\nAIâ€™s learning techniques, its scale \n\nand the nature of its outputs, leaving \n\nmuch uncertainty, complexity and \n\nvariety in the implementation of AI \n\nand IP governance. \n\nJoe Jones \n\nIAPP Director of Research and Insights â†’ Part VIII. The copyright challenge    \n\n> AI Governance in Practice Report 2024 |63\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## Generative AI copyright litigation in the U.S. \n\nTwo main lines of argument are emerging in ongoing \n\nAI  copyright litigation in the U.S. \n\nPetitioners are arguing that: \n\nâ†’ Defendants made copies of copyrighted works when ingesting them \n\nfor training foundation models. \n\nâ†’ As the generated outputs were trained on copyrighted material, \n\nthe outputs themselves are also infringing derivative works. \n\nMore specifically, in a lawsuit against OpenAI, the New York Times argued \n\nthat OpenAI and Microsoft's generative AI tools were built by copying years \n\nof journalistic work without permission or payment, and both companies \n\nare making high profits through their generative AI tools, which now \n\ncompete with the news outlet as reliable sources of information. \n\nOpenAI's motion to dismiss the lawsuit provides background on fair \n\nuse law, and it argues courts have historically used fair use to protect \n\nuseful innovations and copyright is not a veto right over transformative \n\ntechnologies that leverage existing work internally. \n\nThe assessment of fair use is likely to include an \n\nevaluation of exactly what was or is being copied, \n\nwhether ingestion of copyrighted material amounts to \n\ntransformative use, the substantiality of the copying and \n\nthe economic harm caused by using copyrighted material \n\nin developing generative AI models on the potential \n\nmarket for the copyrighted work. \n\nSimilarly, in Tremblay v. OpenAI, various authors alleged copyright \n\ninfringement based on the ingestion of training data that copied \n\nthe works of the authors without consent, credit or compensation. \n\nA California court  recently  rejected  claims on vicarious copyright \n\ninfringements, Digital Millennium Copyright Act violations, negligence \n\nand unjust enrichment. â†’ Part VIII. The copyright challenge \n\nAI Governance in Practice Report 2024  | 64  \n\n> TABLE OF CONTENTS â†‘\n\nImplementing AI governance \n\nNumerous  copyright-safety solutions  and \n\nharm-mitigation strategies  are emerging, \n\nnotwithstanding the uncertainty present due \n\nto  pending litigation. \n\nâ†’ Opt outs.  As foundation models are \n\ntrained on vast amounts of data online, \n\norganizations may not be aware that their \n\ncopyrighted material is used for training. \n\nIn those scenarios, when organizations \n\nare concerned about their webpages being \n\nscraped, an opt-out process, like that of \n\nOpenAI , may be a workable strategy to \n\nmitigate the risk of unwanted scraping. \n\nâ†’ Liability considerations.  Given the fear of \n\npotentially becoming a copyright infringer \n\nas a user of generative AI, commercial \n\nusers may avoid engaging with providers \n\nof  generative AI services. \n\nâ†’ Explore technical guardrails. \n\nOrganizations can also make use of \n\ntechnical guardrails that help them \n\nrespect  the copyrights of authors. Microsoft \n\nincorporated guardrails such as content \n\nfilters, operational monitoring, classifiers, \n\nabuse detection and other technologies to \n\nreduce the likelihood of Copilot returning \n\ncopyright-infringing content. \n\nâ†’ Generative AI requirements.  To increase \n\ntransparency around data used to train \n\ngenerative AI models, including copyrighted \n\ndata, certain jurisdictions such as the \n\nEU require system providers to publish \n\ndetailed summaries of the content used for \n\ntraining their models. Further, with respect \n\nto copyright compliance, the EU AI Act \n\nrequires providers to implement a copyright \n\npolicy mandating protocols to identify and \n\nobserve applicable copyright laws. \n\nINDUSTRY EXAMPLE \n\nMicrosoft  committed to absolve \n\nits users of liability by assuming \n\nvicarious responsibility for \n\ninfringements when use of its \n\nCopilot service leads to legal issues \n\nfor their commercial customers. \n\nMicrosoft also requires customers \n\nto use content filters and other \n\nbuilt-in product safety features, \n\nand asks customers to not generate \n\ninfringing content, such as by \n\nproviding Copilot with inputs they \n\ndo not have the right to use. AI Governance in Practice Report 2024  | 65 \n\n# Part IX. \n\n# Third-party \n\n# AI assurance \n\nIn a recent  report  released by the U.K. government, assurance \n\nis defined as \"the process of measuring, evaluating and \n\ncommunicating something about a system or process, \n\ndocumentation, a product, or an organisation.\" Many of the \n\nAI governance implementation mechanisms discussed in this \n\nreport are forms of assurance. \n\nWhile establishing core competencies within an organization \n\nis beneficial to create strong AI-governance foundations \n\nacross the different  lines of defense , utilization of third-party \n\nAI assurance mechanisms may be an important or necessary \n\nconsideration depending on the type of AI used and the \n\norganization's knowledge and capacity. \n\nIntegrating third-party assurance into an AI-governance \n\nstrategy is a consideration at various stages of the life cycle. \n\n## AI assurance methods are crucial \n\n## for demonstrating accountability \n\n## and establishing trust. â†’ Part IX. Third-party AI assurance \n\nAI Governance in Practice Report 2024  | 66  \n\n> TABLE OF CONTENTS â†‘\n\nTypes of third-party assurance \n\nSome of the most practical tools for the realization \n\nof safe and responsible AI are emerging from \n\nthird-party AI assurance methods. \n\nAssessment \n\nAssessments are key mechanisms to evaluate \n\nvarious aspects of an AI system, including to \n\ndetermine the risk of a system or identify the \n\nsource of bias or determine the reason a system \n\nis making inaccurate predictions. Various \n\nservices and off-the-shelf products can be \n\nintegrated into AI governance practices based on \n\nwhat an organization is trying to determine from \n\nits assessment. \n\nCertain assessments must be conducted by the \n\nthird party providing the system to their customers, \n\nsuch as conformity assessments and impact \n\nassessments focusing on the impacts of the datasets \n\nused and the model itself. From a deployer's \n\nperspective, third-party due diligence enquiries \n\nshould be integrated into the organization's existing \n\nthird-party risk management program and include \n\nscreening at both the vendor enterprise and \n\nproduct levels. \n\nTesting and validation \n\nTesting techniques such as statistical tests to \n\nevaluate demographic fairness, assess system \n\nperformance or detect generative AI that may \n\nlead to copyright breaches are becoming widely \n\navailable through various third-party vendors. \n\nBefore choosing a vendor, it is important to have \n\na clear understanding of what the test is for \n\nand whether the context â€” which includes the \n\ntype of AI used, applicable jurisdictions and the \n\ndomain operating in â€” will impact the types of \n\ntests to run. \n\nConformity assessments \n\nConformity assessments are reviews completed \n\nby internal or external review functions to \n\nevaluate whether a product, system, process \n\nor individual adheres to an established set of \n\nrequirements. This is typically performed in \n\nadvance of a product or system being placed on \n\nthe market. While most assessments focus on \n\nevaluating aspects of AI systems, conformity \n\nassessments have been designed to evaluate \n\nquality-management systems , a set of processes \n\nfor those who build and deploy AI systems, and \n\nindividuals  who are involved in the development, \n\nmanagement or auditing of AI systems. \n\nFrom a deployer's perspective, the third-party \n\ndue diligence process should include vendor \n\ninquiries into product documentation, such as \n\ntechnical specifications, user guides, conformity \n\nassessments and impact assessments. \n\nRisk assessments should be done \n\nat several phases of development, \n\nstarting with the proposal/idea phase. \n\nIt's easier to incorporate some \n\nâ€˜responsible by design' features early \n\non, rather than tack them on at the \n\nend. For example, filtering for toxic \n\ncontent in your training data, before \n\na model is trained, can be more \n\neffective than trying to catch toxic \n\ngenerated content afterwards. \n\nIn contrast, a full impact assessment \n\nshould be done once a model is fully \n\ndeveloped and evaluated, because it's \n\nhard to assess the impact without a lot \n\nof information about the final system. \n\nAndrew Gamino-Cheong \n\nTrustible AI Co-founder and Chief Technology Officer â†’ Part IX. Third-party AI assurance \n\nAI Governance in Practice Report 2024  | 67  \n\n> TABLE OF CONTENTS â†‘\n\nImpact assessments \n\nThe risk profile of AI systems can vary widely \n\nbased on the technical capabilities and intended \n\npurposes of the system, as well as the particular \n\ncontext of their implementation. Evaluating and \n\nmitigating the impacts of an AI system is therefore \n\na shared responsibility that must be owned by \n\nproviders and deployers alike in practice. The \n\norganization deploying a third-party AI system will \n\nhave a closer understanding of the specific context \n\nand impacts of deploying the system. Similarly, \n\nthe third-party vendor is best placed to evaluate \n\nthe impacts of the training, testing and validation \n\ndatasets, the model and infrastructure used to \n\ndesign and develop the system. \n\nAI/algorithmic auditing \n\nWhile there is not yet a formal audit practice as \n\nseen in financial services, there is a growing call \n\nfor those who audit AI systems to demonstrate \n\na common set of competencies, such as with a \n\ncertification or formal designation. These audits \n\nmay incorporate other third-party mechanisms \n\ndiscussed above to evaluate AI systems and ensure \n\nthey are safe, secure, legally compliant and meet \n\nrequisite standards, among other things. The \n\nNational Telecommunications and Information \n\nAdministration released  recommendations  for \n\nfederal agencies to use audit and auditors for the \n\nuse of high-risk AI systems. \n\nCanada's proposed Bill C-27, the Digital Charter \n\nImplementation Act, identifies that the Minister \n\nof Innovation, Science and Industry can issue an \n\nindependent audit if they have reasonable grounds \n\nto believe requirements outlined in the act have \n\nnot been met. This may encourage organizations \n\nto ensure compliance via preventative third-\n\nparty audits. Additionally, Canada identified the \n\nimportance of international standards to help \n\nsupport the desired objectives of the act. \n\nCertifications \n\nCertifications are marks or declarations provided \n\nafter evaluations or audits are performed against \n\nstandards or conformity assessments . The mark \n\nindicates the AI system adheres to certain specified \n\nrequirements. It is important to note certifications \n\ncan also be provided to quality-management \n\nsystems used throughout the life cycle of an AI \n\nsystem or to individuals, demonstrating that they \n\nmet a set of competencies. \n\nOrganizations need a clear \n\nunderstanding of how AI risk will \n\naffect their business through \n\nthird-party relationships. \n\nThey should proactively review their \n\ninventory of vendors and identify \n\nthose that provide AI solutions or \n\ncomponents. They also need to be \n\naware of the development plans for \n\nall third-party products, including \n\nwhether, how, and when AI will be \n\nintegrated. With that understanding, \n\npartners, vendors and their products \n\nmay need to be reassessed to \n\naccount for AI risk with updated due \n\ndiligence processes. \n\nAmber Gosney \n\nFTI Technology Managing Director â†’ Part IX. Third-party AI assurance    \n\n> AI Governance in Practice Report 2024 |68\n> TABLE OF CONTENTS â†‘\n\n## â†’ SPOTLIGHT \n\n## Algorithmic audits as airplane cockpits \n\nAt ORCAA, we use the analogy of an airplane cockpit \n\nto talk about algorithmic audits. In an airplane cockpit, \n\nthe dials and gauges take measurements that relate to \n\npossible failure modes. \n\nFor instance, the fuel gauge says if the plane is about to run out of gas, \n\nand the attitude indicator says if it is going to dive or roll. These dials \n\nhave 'redlines': threshold values that, if exceeded, mean the pilot needs to \n\nintervene. The auditor's job is to design a 'cockpit' for a given algorithmic \n\nsystem. This involves identifying failure modes -- how the system could \n\nresult in harm to various stakeholders -- and building 'dials' that measure \n\nconditions that lead to failures. At ORCAA, we have developed frameworks \n\nfor doing these critical tasks. \n\nSome other aspects of this analogy are worth noting. A cockpit identifies \n\nproblems but does not fix them. An indicator light will say an engine is out, \n\nbut it won't say how to repair or restart the engine. Likewise, an algorithmic \n\ncockpit should indicate when a failure is imminent, but it is the job of the \n\nsystem deployer, the 'pilot,' to intervene. A cockpit is a critical piece of \n\nairplane safety, but it's not the whole picture. Planes are tested extensively \n\nbefore being put into service, both during the design phase and when \n\nthey roll off the assembly line and are periodically taken out of service for \n\nregular inspections and maintenance. \n\nLikewise, algorithmic cockpits, which are critical \n\nfor safety while the system is deployed, should be \n\ncomplemented by predeployment testing and regular \n\ninspections and maintenance during deployment.\" \n\nCathy O'Neil \n\n> O'Neil Risk Consulting & Algorithmic Auditing CEO\n\nAI Governance in Practice Report 2024  | 69 \n\n# Conclusion \n\nOrganizations may seek to leverage existing organizational \n\nrisk frameworks to tackle AI risk at enterprise, product and \n\noperational levels. Tailoring their approach to AI governance \n\nto their specific AI product risks, business needs and \n\nbroader strategic objectives can help organizations establish \n\nthe building blocks of trustworthy and responsible AI. A key \n\ngoal of the AI governance program is to facilitate responsible \n\ninnovation. Flexibly adapting existing governance processes \n\ncan help businesses to move forward with exploring the \n\ndisruptive competitive opportunities that AI technologies \n\npresent, while minimizing associated financial, operational \n\nand reputational risks. \n\n## Bringing it all together and \n\n## putting it into action. Uzma Chaudhry \n\nIAPP AI Governance Center \n\nResearch Fellow \n\nuchaudhry@iapp.org \n\nJoe Jones \n\nIAPP Director of Research \n\nand Insights \n\njjones @iapp.org \n\nAshley Casovan \n\nIAPP AI Governance Center \n\nManaging Director \n\nacasovan@iapp.org \n\nLynsey Burke \n\nIAPP Research and Insights \n\nProject Specialist \n\nlburke@iapp.org \n\nNina Bryant \n\nFTI Technology Senior \n\nManaging Director \n\nnina.bryant@fticonsulting.com \n\nLuisa Resmerita \n\nFTI Technology \n\nSenior Director \n\nluisa.resmerita@fticonsulting.com \n\nMichael Spadea \n\nFTI Technology Senior \n\nManaging Director \n\nmicheal.spadea@fticonsulting.com \n\nFollow the IAPP on social media \n\nD C Q E\n\nPublished June 2024. \n\nIAPP disclaims all warranties, expressed or implied, with respect to \n\nthe contents of this document, including any warranties of accuracy, \n\nmerchantability, or fitness for a particular purpose. Nothing herein \n\nshould be construed as legal advice. \n\nÂ© 2024 IAPP. All rights reserved. \n\n# Contacts \n\n## Connect with the team", "fetched_at_utc": "2026-02-08T19:06:59Z", "sha256": "d10ba8f26b3a8a5f315538e4f40dcd3ba10b228963f59e2d2c3a180ca8b56e2b", "meta": {"file_name": "AI Governance in Practice Report 2024 - IAPP.pdf", "file_size": 39709902, "relative_path": "pdfs\\AI Governance in Practice Report 2024 - IAPP.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 29464}}}
{"doc_id": "pdf-pdfs-ai-risk-management-singapore-aa4ced6d9f7b", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Risk Management - Singapore.pdf", "title": "AI Risk Management - Singapore", "text": "ARTIFICIAL INTELLIGENCE \n\n# MODEL RISK \n\n# MAN AGEMENT \n\n# OBSERVATIONS FROM A THEMATIC REVIEW  \n\n> Information Paper\n> December 2024\n\nArtificial Intelligence Model Risk Management | 2\n\n## Contents \n\n1. Overview 3\n\n2. Background 3\n\n3. Objectives and Key Focus Areas 7\n\n4. Governance and Oversight 10 \n\n5. Key Risk Management Systems and Processes 12 \n\n5.1  Identification 12 \n\n5.2 Inventory 13 \n\n5.3 Risk Materiality Assessment 15 \n\n6 Development and Deployment 16 \n\n6.1 Standards and Processes 16 \n\n6.2 Data Management 18 \n\n6.3 Development 21 \n\n6.4 Validation 30 \n\n6.5 Deployment, Monitoring and Change Management 31 \n\n7 Other Key Areas 36 \n\n7.1 Generative AI 36 \n\n7.2 Third -Party AI 43 \n\n8 Conclusion 44 \n\nAnnex A - Definitions 46 \n\nAnnex B - Useful References 49 Artificial Intelligence Model Risk Management | 3\n\n# 1. Overview \n\n1.1  This information paper sets out good practices relating to Artificial Intelligence \n\n(AI) (including Generative AI)  1 model risk management (MRM)  2 that were \n\nobserved during a recent thematic review of selected banks . The information \n\npaper focuses on the following key areas 3 : AI governance and oversight ; AI \n\nidentification, inventorisation and risk materiality assessment; as well as AI \n\ndevelopment, validation, deployment, monitoring and change management. \n\n1.2  While the thematic review focused on selected banks, the good practices \n\nhighlighted in this information paper should generally appl y to other financial \n\ninstitutions (FIs), which should take reference from these when developing and \n\ndeploying AI. \n\n# 2. Background \n\nIndustry use of AI and Generative AI and associated risks \n\n2.1  The launch of ChatGPT in November 2022 and recent advancements in AI, \n\nparticularly Generative AI, has led to an increased interest in leveraging AI and \n\nGenerative AI in the banking and broader financial sector. Prior to these \n\ndevelopments, FIs have used AI in a wide range of areas and use cases. Key areas \n\nwhere we observed significant use of AI by banks during the thematic review \n\ninclude risk management, customer engagement and servicing , as well as to  \n\n> 1\n\nGenerative AI is a subset of AI , and a n AI or Generative AI system can comprise one or more AI or Generative AI models and \n\nother machine -based components . For the purposes of this paper, AI generally refers to both AI and Generative AI models and \n\nsystems . Where a point pertains specifically to an AI model or an AI system, or to Generative AI , we will use the respective \n\nterm s explicitly in the paper. We define the terms AI and Generative AI, as well as AI model, system and use case in greater \n\ndetail in Annex A.  \n\n> 2\n\nIn line with the footnote above and r ecognising that the AI MRM is intrinsically linked to the risk management of systems in \n\nwhich AI models are used, when we refer to AI MRM or AI risk management in this paper, it generally refers to the risk \n\nmanagement of AI models and systems.  \n\n> 3\n\nThe aim of this information paper is not to cover all aspects of model risk management, but to focus on good practices in \n\nareas that are more relevant to AI MRM .Artificial Intelligence Model Risk Management | 4\n\nsupport internal operational processes . For example, we have seen banks use AI, \n\nparticularly decision tree -based machine learning (ML) models such as XGBoost, \n\nLightGBM and CatBoost  4 , in financial risk management to detect abnormal \n\nfinancial market movements, or to estimate loan prepayment rates. They are also \n\ncommonly used in anti -money laundering (AML) systems to detect suspicious \n\ntransactions , and in fraud detection systems . In customer engagement and \n\nservicing, banks use AI to predict customer preferences, personalise financial \n\nproduct recommendations and manage customer feedback. AI is also widely \n\nused to support internal operational processes across a wide range of business \n\nfunctions, for example, to automate checking and verification processes (e.g. , for \n\ncustomer information ), prioritise incident management (e.g. , triaging IT incidents \n\nfor attention ), or forecast demand for services (e.g. , ATM cash withdrawals) .\n\n2.2  While the use of AI in these areas can enhance operational efficiency , facilitate \n\nrisk management and enhance financial services , they can also increase risk \n\nexposure if not developed or deployed responsibly . Po tential risks include: \n\nâ€¢ Financial risks , e.g.,  poor accuracy of AI used for risk management could  lead \n\nto poor risk assessments and consequent financial losses. \n\nâ€¢ Operational risks , e.g., unexpected behaviour of  AI used to automate financial \n\noperations could lead to  operational disruptions or errors in critical processes. \n\nâ€¢ Regulatory risks , e.g., poor performance of AI used to support AML efforts \n\ncould lead to non -compliance with regulations. \n\nâ€¢ Reputational risk s, e.g., wrong or inappropriate information from AI -based \n\ncustomer -facing systems , such as chatbots , could lead to customer complaints \n\nand negative media attention, and consequent reputational damage.                 \n\n> 4Decision tree -based ML models make predictions based on a tree -like structure learnt from data. Models such as X GBoost,\n> LightGBM and CatBoost utilise a series of decision trees together with a boosting technique . Each decision tree in the series\n> focuses on the errors made by aprior decision tree to improve predictions .Such models are also explainable as the relative\n> importance of different features to model predictions can be extracted.\n\nArtificial Intelligence Model Risk Management | 5\n\n2.3  While natural language processing (NLP) 5 and computer vision (CV) 6 techniques \n\nwere already in use in the financial sector prior to the emergence of Generative \n\nAI 7 for text or image -related tasks, recent Generative AI models such as OpenAIâ€™s \n\nGPT 8 large language models (LLM s) and D ALL -E9 image generation models , or \n\nAnthropicâ€™s Claude LLM s10  offer better performance in tasks such as cluster ing \n\ndocuments . They have also enable d new use cases , e.g. , to generat e text content \n\nand image s for marketing , or to process multimodal data 11  for financial analysis .\n\n2.4  Based on the thematic review, use of Generative AI in banks appear s to still be at \n\nan early stage . The current focus is on the use of Generative AI to assist or \n\naugment humans for productivity enhancements , and not in applying Generative \n\nAI to direct customer facing applications . Use cases being explored by banks \n\ninclude risk management (e.g., detecting emerging risks in text information );\n\ncustomer engagement and service (e.g., summarising customer interactions or \n\ngenerating marketing content ); and research and reporting (e.g., investment \n\nanalyses ). Banks are also exploring the use of Generative AI in copilots 12  to \n\nsupport staff, for example , in coding, or for general text -related tasks such as \n\nsummarisation and answering queries based on information in internal \n\nknowledge repositories. \n\n2.5  With Generative AI, existing risks associated with AI may be amplified 13 . For \n\nexample, Generative AI's potential for hallucinations and unpredictable  \n\n> 5\n\nNatural language processing (NLP) is commonly used to refer to techniques that process , analyse , make predictions or \n\ngenerate outputs relating to human language, both in its written and spoken forms.  \n\n> 6\n\nComputer vision (CV) is commonly used to refer to techniques that enable machines to process and generate outputs based \n\non visual information from the world.  \n\n> 7\n\nFor example, for news sentiment analysis, information extraction, clustering documents based on underlying topics, or \n\ndigitising physical documents . \n\n> 8\n\nGenerative Pre -trained Transformers (GPT) are a family of Generative AI models developed by OpenAI , that include s models \n\nsuch GPT 4 and GPT -4o.  \n\n> 9\n\nDALL -E is a Generative AI model that generates images from text prompts or descriptions.  \n\n> 10\n\nClaude models are a family of Generative AI models developed by Anthropic and include models such as Claude 3.5 Haiku \n\nand Sonnet.  \n\n> 11\n\nMultimodal data refers to datasets that comprise multiple types of data, e.g., text, images, audio or video.  \n\n> 12\n\nIn the context of Generative AI, the term copilot is typically used to refer to Generative AI being used to assist or augment \n\nhumans on specific tasks . \n\n> 13\n\nMore details on risks associated with Generative AI have already been covered extensively in Project Mind Forge â€™s white \n\npaper on â€œEmerging Risks and Opportunities of Generative AI for Banks â€ and will not be repeated in this information paper. \n\nThe whitepaper can be accessed at  https://www.mas.gov.sg/schemes -and -initiatives/project -mindforge .Artificial Intelligence Model Risk Management | 6\n\nbehaviours may pose significant risks  if Genera tive AI is used in mission -critical \n\nareas. The complexity of Gen erative AI models and lack of established \n\nexplainability techniques also create s challenges for understanding and \n\nexplaining decisions , while the d iverse and often opaque data sources used in \n\nGen erative AI training, coupled with difficulties in evaluating bias of Generative \n\nAI outputs , could lead to unfair decisions .\n\nMASâ€™ Efforts on Responsible AI for the Financial Sector \n\n2.6  Alongside the growing use of AI in the financial sector and such associated risks ,\n\nMAS ha d established key principles to guide financial institutions in their \n\nresponsible use of AI. \n\n2.7  In 2018, MAS co -created the principles of Fairness, Ethics, Accountability and \n\nTransparency (FEAT) with the financial industry to promote the deployment of AI \n\nand data analytics in a responsible manner. To provide guidance to FIs in \n\nimplementing FEAT, MAS started working with an industry consortium on the \n\nVeritas Initiative 14  in November 2019. The Veritas Initiative aimed to support FIs \n\nin incorporating the FEAT Principles into their AI and data analytics solutions, and \n\nhas released assessment methodologies, a toolkit , and accompanying case \n\nstudies. \n\n2.8  With the emergence of Generative AI, Project Mind Forge 15 , which is also driven \n\nby the Veritas Initiative, was established to examine the risk s and opportunities \n\nof Generative AI. The first phase of Project Mind Forge was supported by a \n\nconsortium of banks and released a risk framework for Generative AI in \n\nNovember 2023. \n\n2.9  More recently, MAS released an information paper relating to Generative AI risks \n\nin July 2024 16 . The paper provides an overview of key cyber threats arising from \n\nGenerative AI, the risk implications, and mitigation measures that FIs could take         \n\n> 14 See https://www.mas.gov.sg/schemes -and -initiatives/veritas\n> 15 See https://www.mas.gov.sg/schemes -and -initiatives/project -mindforge\n> 16 See https://www.mas.gov.sg/regulation/circulars/cyber -risks -associated -with -generative -artificial -intelligence Artificial Intelligence Model Risk Management |7\n\nto address such risks. The paper covered areas enabled by Generative AI, such as \n\ndeepfakes, phishing and malware, as well as threats to deployed Generative AI, \n\nsuch as data leakage and model manipulation. \n\n# 3. Objectives and Key Focus Area s\n\n3.1  This information paper , which focus es on AI MRM, is part of MASâ€™ incremental \n\nefforts to ensure responsible use of AI in the financial sector . A key difference \n\nbetween an AI -based system and other system s is the use of one or more AI \n\nmodels within the system, which potentially increases uncertainties in outcomes .\n\nRobust MRM of such AI models is important to support the responsible use of AI .\n\n3.2  As the maturity of AI MRM may vary significantly across different FIs, MAS \n\nconducted a thematic review of selected banksâ€™ AI MRM practices in mid -2024 .\n\nThe objective was to gather good practices for sharing across the industry .\n\n3.3  Based on information gathered during the review, MAS observed good practices \n\nby banks in the se key focus areas 17 :\n\nâ€¢ Section 4: Oversight and Governance of AI \n\n- Updating of existing policies and procedures of relevant risk management \n\nfunctions to strengthen AI governance ;\n\n- Establish ing cross -functional oversight forums to ensure that evolving AI \n\nrisks are appropriately managed across the bank; \n\n- Articulat ing clear statements and principles to govern areas such as the \n\nfair, ethical, accountable and transparent use of AI ; and           \n\n> 17 For the purposes of the subsequent parts of this information paper, the good practices relating to AI would also apply to\n> Generative AI as practicable. Specific considerations relating to Generative AI will be covered in Section 7.1 .Artificial Intelligence Model Risk Management |8\n\n- Building capabilities in AI across the bank to support both innovation and \n\nrisk managemen t. \n\nâ€¢ Section 5: Key Risk Management Systems and Processes \n\n- Identif ying AI usage and risks across the bank so that commensurate risk \n\nmanagement can be applied ;\n\n- Utilis ing AI inventories, which provide a central view of AI usage across \n\nthe bank to support oversight ; and \n\n- Assessing the materiality of risks that AI poses using key risk dimensions so \n\nthat relevant controls can be applied proportionately .\n\nâ€¢ Section 6: Development and Deployment of AI \n\n- Establishing standards and processes for k ey areas that are important for \n\nthe d evelopment of AI , such as data management, robustness and \n\nstability, explainability and fairness , reproducibility and auditability ;\n\n- Conducting independent validation or peer review s 18  of AI before \n\ndeployment based on risk materialities ; and \n\n- Instituting p re -deployment checks and monitoring of deployed AI to \n\nensure that it behave s as intended, and application of appropriate change \n\nmanagement standards and processes where necessary .       \n\n> 18 The terms validation sand reviews are usually used interchangeably by banks to refer to assessments or checks of the AI\n> model development process, whether by an independent party, or another peer developer. More details on such validations\n> and reviews are provided in Section 6. 4.Artificial Intelligence Model Risk Management |9\n> Overview of Key Thematic Focus Areas\n\n3.4  These key focus areas are generally also applicable to Generative AI, as well as AI \n\n(including Generative AI ) from third -party providers. Nonetheless, there may be \n\nadditional considerations for Generative AI , as well as AI from third -party \n\nproviders. Hence, additional observations on good practices relating to \n\nGenerative AI and third -party AI are also outlined in Sections 7 .1 and 7.2 of this \n\ninformation paper respectively .\n\n3.5  The risks posed by AI and Generative AI extend beyond MRM and relate to non -\n\nAI specific areas such as general data governance and management, technology \n\nand cyber risk management, as well as third party risk management . These are \n\nnot covered in this information paper, and existing regulatory requirements and \n\nsupervisory expectations , including but not limited to notice s, guidelines or \n\ninformation papers on data governance, technology and outsourcing risk \n\nmanagement would apply , where relevant 19 .      \n\n> 19 Links to relevant publications are provided in Annex B.Artificial Intelligence Model Risk Management |10\n\n# 4. Governance and Oversight \n\nOve rview \n\nWhile existing control functions continue to play key roles in AI risk management , most \n\nbanks have update d governance structures, roles and responsibilities, as well as policies \n\nand processes to address AI risks and keep pace with AI developments . Good practices \n\ninclude :\n\nâ€¢ establishing cross -functional oversight forums to avoid gaps in AI risk \n\nmanagement ;\n\nâ€¢ updating control standards, policies and procedures, and clearly set ting out roles \n\nand responsibilities to address AI risks ;\n\nâ€¢ developing clear statements and guidelines to govern areas such as fair, ethical, \n\naccountable and transparent use of AI across the bank ; and \n\nâ€¢ building capabilities in AI across the bank to support both innovation and risk \n\nmanagement .\n\nExisting governance structures and such good practices are important to help support \n\nBoard and Senior Management in exercis ing oversight over the bankâ€™s use of AI , and \n\nensure that the bankâ€™s risk managemen t is robust and commensurate with its state of \n\nuse of AI .\n\n4.1 While existing risk governance framework s and structure s 20  continue to be \n\nrelevant for AI governance and risk management , a number of banks have \n\nestablish ed cross -functional AI oversight forums . Such forums serve as key \n\nplatform s for coordinating governance and oversight of AI usage across various \n\nfunctions . They also play an important role in addressing emerging challenges \n\nand potential gaps in risk management as the AI landscape evolves , and ensuring \n\nthat standards and processes, such as relevant AI development and deployment \n\nstandards , are aligned across the bank.                \n\n> 20 Aside from MRM, risk governance frameworks and structures from other areas that are usually relevant to AI risk\n> management include (but are not limited to )data, technology and cyber ,third -party risk management, as well as legal and\n> compliance .Artificial Intelligence Model Risk Management |11\n\n4.2 The mandates of these forums often include establishing a consistent and \n\ncomprehensive framework for managing AI risks, evaluating use cases that \n\nrequire broader cross -functional inputs, and reviewing AI governance \n\nrequirements to ensure they keep pace with the state of AI usage in the bank. \n\nData and analytics, r isk management, legal and compliance, technology, audit, as \n\nwell as other relevant busines s and corporate functions , are typically represented \n\nat such cross -functional oversight forums .\n\n4. 3 A number of banks have also found value in compiling policies and procedures \n\nthat are relevant to AI into a central guide to ensure that consistent standards \n\nfor AI are applied across the bank.  As more AI use cases are rolled out in banks, \n\nand the state of AI technology evolves, the use of AI may accentuate existing risks \n\nor introduce new risks. Hence, most banks have  reviewed and , where necessary, \n\nupdated existing policies and procedures to keep pace with the increasing use of \n\nAI across the bank , or new AI developments, e.g., updat ing policies and \n\nprocedures relating to performance testing of AI for new use cases , or \n\nestablishing new policies and procedures for AI models that are dynamically \n\nupdated based on new data .\n\n4. 4 Given the broad range of use cases for AI, and the potential for inappropriate \n\nuse, most banks have set out central statements and principles on how they \n\nintend to use AI responsibly, including developing guidelines to govern areas such \n\nas fair, ethical, accountable, and transparent use of AI  21  . Such efforts are \n\nimportant in setting the tone and establishing clear guidance on how AI should \n\nbe used appropriately across the bank, and to prevent potential harms to \n\nconsumers and other stakeholders arising from the use of AI . In addition to \n\ncentral statement s and principles , some banks have also taken steps to \n\noperationalise such central statement s and principles by mapping them to key                                    \n\n> 21 More details on these areas can be found in MAS â€™publications relating to the FEAT principles under the Veritas Initiative .\n> Similar principles covering areas relating to the responsible or ethical use of AI in the financial sector have also been published\n> in other jurisdictions ,e.g. ,the Hong Kong Monetary Authority (HKMA) issued guiding principles for the use of big data analytics\n> and AI covering governance and accountability, fairness, transparency and disclosure, and data privacy and protection in 2019;\n> De Nederlandsche Bank (DNB) issued the SAFEST principles on s oundness, accountability, fairness, ethics, skills, and\n> transparency in 2019. Artificial Intelligence Model Risk Management |12\n\ncontrols , which are in turn mapped to the relevant functions responsible for \n\nthese controls .\n\n4. 5 Given the growing interest in AI, b anks also recognised the need to develop AI \n\ncapabilities and have established plans to upskill both their staff and senior \n\nexecutives . Aside from building awareness, banks have developed AI training that \n\nfacilitate staff in leveraging an d using AI in a n effective and responsible manner. \n\nSome banks have also set up AI Centre s of Excellence to drive innovation ,\n\npromote best practices and build AI capabilities across the bank .\n\n# 5. Key Risk Management Systems and \n\n# Processes \n\nOverview \n\nMost banks have recognised the need to establish or updat e key risk management \n\nsystems and processes for AI , particularly in the following areas: \n\nâ€¢ policies and procedures for identifying AI usage and risks across the bank , so that \n\ncommensurate risk management can be applied ;\n\nâ€¢ systems and processes to ensure the completeness of AI inventories, which \n\ncapture the approved scope of use and provide a central view of AI usage to \n\nsupport oversight ; and \n\nâ€¢ assessment of the risk materiality of AI that cover s key risk dimensions, such as \n\nAIâ€™s impact on the bank and stakeholders , the complexity of AI used , and the \n\nbankâ€™s reliance on AI , so that relevant controls can be applied proportionately. \n\n5.1  Identification \n\n5.1.1  Identif ying where AI is used is important so that the relevant governance and risk \n\nmanagement controls can be applied . Even when using widely accepted \n\ndefinition s, such as the Organisation for Economic Co -operation and Artificial Intelligence Model Risk Management | 13 \n\nDevelopmentâ€™s  definition of AI 22 , considerable ambiguity remain s around the \n\ndefinition of AI due to its broad and evolving scope .\n\n5.1.2  Most banks leverage d definitions in existing MRM policies and procedures as a\n\nfoundation for identifying AI models 23 , and extended or adapted these definitions \n\nto account for AI -specific characteristics . Some banks shared that the uncertainty \n\nof model outputs is a common source of risk for both AI and conventional \n\nmodels 24 , and that the presence of such uncertainties was a key feature that was \n\nusually considered when identifying AI. MRM control functions also typically play \n\na key role in AI identification , often serving as the key control function \n\nresponsible for AI identification systems and processes , e.g., setting up \n\nattestation processes, or acting as the final arbiter in determin ing whether AI is \n\nbeing used . Some banks have also developed tools or portals to facilitate the \n\nprocess of identifying and classifying AI across the bank in a consistent manner. \n\n5.2 Inventory \n\n5.2.1  Banks mostly maintain a formal AI inventory 25  with a comprehensive record of \n\nwhere AI is used in the bank . A key area that an AI inventory supports, alongside \n\nthe relevant policies, procedures and systems, is to ensure that AI are only used \n\nwithin the scope in which they have been approved for use, e.g. , the purpose ,\n\njurisdiction, use case, application, system, and other conditions for which they \n\nhave been developed, validated and deployed. This is critical because \n\nunapproved usage of AI, particularly in higher -risk use cases, can lead to  \n\n> 22\n\nThe OECDâ€™s definition of AI: An AI system is a machine -based system that, for explicit or implicit objectives, infers, from the \n\ninput it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence \n\nphysic al or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.  \n\n> 23\n\nThis could entail step -by -step guide to facilitate the identification of techniques that meet the bankâ€™s definition of AI.  \n\n> 24\n\nModel s usually refer to quantitative algorithms, methods or techniques that process input data into quantitative estimates \n\nwhich may be used for analysis or decision making. Apart from AI models, which typically refer to machine or deep learning \n\nmodels, banks also routinely utilise conventional models, such as economic, financial, or statistical models. Some quantitative \n\nalgorithms, methods or techniques , such as logistic regressions , are commonly regarded as both AI and statistical models. A\n\nmore detailed definition of models can be found in Annex A.  \n\n> 25\n\nMost banks have established software system s for their AI inventor ies that not only record where AI is used in the bank, but \n\nmay also include additional features outlined above, such as automated tracking of approvals and issues , and identification of \n\ninter -dependences between AI . A small number of banks still rely on spreadsheets for their AI inventories, but this approach is \n\nmore prone to operational issues, e.g., outdated records, and would not allow for the additional features outlined above. Artificial Intelligence Model Risk Management | 14 \n\nunintended consequences. For example,  AI approved for use in one jurisdiction \n\nshould not automatically be treated as approved for use in others as the data, \n\nassumptions and considerations may not be similar , and the AI may not perform \n\nas expected in a different context .\n\n5.2.2  A few banks also utilise d their AI inventory system to track the use of AI through \n\ntheir lifecycle, and to establish checkpoints for different risk management \n\nprocesses at the various stages of the AI lifecycle. A few banks also use d the AI \n\ninventory to support the identification and monitoring of aggregate AI risks and \n\ninterdependencies across different AI models and systems . The AI inventory may \n\nalso serv e as a central repository for AI artifacts needed for model maintenance, \n\nvalidation and incident or issue management .\n\n5.2.3  Most banks have established clear policies on the scope of AI assets to be \n\ninventoried, the roles responsible for maintaining the inventory, and the \n\nprocesses for updating it. AI models are typically included within regular model \n\ninventories but specific tags or fields added to identify AI and capture AI -relevant \n\nattributes. One bank built an AI use case inventory that aggregate d information \n\nfrom the AI model inventory and other inventories or repositories relating to \n\nassets and controls in areas such as data, technology and operational \n\nmanagement . This provided the bank with a comprehensive and clear view of the \n\nlinkages between AI models and other relevant assets and controls. \n\n5.2.4  Across banks, AI inventories generally capture key attributes such as the AIâ€™s \n\npurpose and description , scope of use, jurisdiction, model type, model output 26 ,\n\nupstream and downstream dependencies , model status, risk materiality rating, \n\napprovals obtained for validation and deployment , responsible AI requirements ,\n\nwaiver or dispensation details 27 , use of personally identifiable information (PII) 28 ,\n\npersonnel responsible such as owner s, sponsor s, users, developer s, and \n\nvalidator s. For third -party AI , additional attributes such as the AI provider, model              \n\n> 26 Model output refers to the type of output generated by the AI model. For example, the model output attribute could be the\n> likelihood of customer attrition, or the credit score of a customer.\n> 27 Waiver or d ispensation details refer to information about exceptions/special permissions granted, regarding the\n> development or deployment of AI, that deviate from the bank's standard policies and procedures .\n> 28 For example, full name, national identification number, personal mobile number. Artificial Intelligence Model Risk Management |15\n\nversion, endpoints utilised , as well as other details from the AI developers 29  may \n\nalso be included. \n\n5.3 Risk Materiality Assessment \n\n5.3.1.  Risk materiality assessment s are critical for banks to calibrate their approach to \n\nrisk management of AI across the diverse areas in which AI can be used (e.g., to \n\nmap the risk materiality of AI to the depth and scope of validation and monitoring \n\nrequired ). In assessing risk materiality , most banks considered both quantitative \n\nand qualitative risk dimensions that could generally be grouped into three broad \n\ncategories :\n\na.  Impact on the bank , its customers or other stakeholders , including but not \n\nlimited to financial, operational, regulatory and reputational impact . A few \n\nbanks developed granular, function -specific definitions of impact to provide \n\ngreater clarity .\n\nb.  Complexity due to the nature of the AI model or system, or the novelty of the \n\narea or use case in which AI is being applied .\n\nc.  Reliance on AI, which takes into account the autonomy granted to the AI, or \n\nthe involvement of human s in the loop as risk mitigant s.\n\n5.3. 2 Most banks have also established processes to review that risk materialit ies \n\nassigned to AI remain appropriate over time. Similarly, quantitative and \n\nqualitative measures and methods use d to assign risk materialities were also \n\nreviewed , e.g., measures used to quant ify financial impact would be updated if \n\nthe nature of the business in which AI was used had evolved.             \n\n> 29 These may be provided in AI or AI m odel cards , which are documents or information usually released alongside open -source\n> AI models that facilitate transparency and accountability by providing essential information on key areas such as the AI modelâ€™s\n> purpose, performance, limitations, ethical considerations .More information on details that may be included in such cards are\n> available in papers such as https://link.springer.com/chapter/10.1007/978 -3-031 -68024 -3_3 .Artificial Intelligence Model Risk Management |16\n\n# 6 Development and Deployment \n\nOverview \n\nMost banks have established standards and processes for development, validation, and \n\ndeployment of AI to address key risks. \n\nâ€¢ For development of AI , key a reas that banks paid greater attention to include data \n\nmanagement, model selection, robustness and stability, explainability and \n\nfairness , as well as reproducibility and auditability .\n\nâ€¢ For validation , banks required independent validation s or review s of AI of higher \n\nrisk materiality prior to deployment , to ensure that development and deployment \n\nstandards have been adhered to. For AI of lower risk materiality, most banks \n\nconducted peer reviews that are calibrated to the risks posed by the use of AI \n\nprior to deployment .\n\nâ€¢ To ensure that AI would behave as intended when deployed and that any data \n\nand model drifts are detected and addressed, banks performed pre -deployment \n\nchecks, closely monitor ed deployed AI based on appropriate metrics , and appl ied \n\nappropriate change management standards and processes. \n\n6.1 Standards and Processes \n\n6.1.1.  To support robust risk management of AI across its lifecycle, banks have \n\nestablished standards and processes in the key areas of development, validation, \n\ndeployment, monitoring and change management . Most banks built upon \n\nexisting MRM standards and processes for development, validation, deployment, \n\nmonitoring and change management , but updated these standards and \n\nprocesses to address risks posed by AI. \n\n6.1.2.  Key standards and processes relating to conventional model development ,\n\nvalidation , deployment , monitoring and change management that banks Artificial Intelligence Model Risk Management | 17 \n\ngenerally  regard as relevant to AI are listed below 30 . Observations on key areas \n\nof focus for AI, and how banks have adapted or updated these standards and \n\nprocesses in these areas to address AI risks will be outlined in the subsequent \n\nsections. \n\na.  Data management - Determining suitability of data, such as the \n\nrepresentativeness of data for the intended objective, assessment of \n\ncompleteness , reliability , quality, and relevance of data, and approaches for \n\ndetermining train ing and test ing datasets. \n\nb.  Model selection - Defining the intended objective of the model and justifying \n\nhow the selection and design of the model is relevant and appropriate for \n\nachieving the desired objective , including the selection of architectures 31  and \n\ntechniques 32  that are appropriate for the use case and objecti ve .\n\nc.  Performance evaluation - Setting appropriate evaluation approaches and \n\nthresholds, and assessing the modelâ€™s ability to perform under a range of \n\nconditions in accordance with its intended usage and objective .\n\nd.  Documentation - Providing sufficient detail to facilitate reproducibility by an \n\nindependent party, including details on data sources , lineage , and processing \n\nsteps ; model architecture and techniques ; evaluation and testing approaches \n\nand results.  \n\n> 30\n\nAs highlighted previously, even prior to the use of AI models, banks already utilise d conventional models, such as economic, \n\nfinancial, or statistical models , and would have instituted model risk management standards and processes for such models .\n\nWhile these standards and processes may have precede d the use of AI model s in the bank , their general principles and \n\nconsiderations may also be applicable to AI models.  \n\n> 31\n\nModel architecture, in the context of AI, relates to the underlying structure and design of the model . It could involve choosing \n\nbetween decision tree -based models such as XGBoost, which were previously described in Section 2, or neural network -based \n\nmodels such as recurrent neural network or transformer models , based on various considerations. For example, d ecision tree -\n\nbased models may be more suitable for structured data, such as tabular data, while recurrent neural network or transformer \n\nmodels may be more suitable for text or time -series data as they are designed for sequential data.  \n\n> 32\n\nTechniques may include methods that are used to train a model from the data . In the context of AI, these may include \n\nsupervised learning techniques that use labelled data during training to learn how to generate predictions , or unsupervised \n\nlearning techniques which learn general patterns from unlabelled data . For more details on supervised and unsupervised \n\nlearning , please refer to Annex A. Artificial Intelligence Model Risk Management | 18 \n\ne.  Validation - Setting out the d epth of review expected of validators across the \n\nareas above ; frameworks for determining the prioritisation and frequency of \n\nvalidation (including any revalidation conducted on deployed models ).\n\nf.  Mitigating model limitations - Frameworks and processes for testing key \n\nassumptions, identifying limitations and their expected impact, and \n\nestablishing appropriate mitigants which are commensurate with the impact \n\nof the limitations. \n\ng.  Monitoring and change management - Setting appropriate tests and \n\nthresholds to evaluate the ongoing performance of a deployed model, \n\nincluding the frequency of monitoring; as well as the processes to be followed \n\n(e.g. , additional validations and approvals) for changes made to a deployed \n\nmodel. \n\n6.1.3.  When implementing standards and processes for risk management of AI , most \n\nbanks establish ed baseline standards and processes that applied to all AI across \n\nthe bank , regardless of risk materiality. For AI that were of greater risk \n\nmateriality, or where there were requirements specific to the use case , baseline \n\nstandards and processes would be supplemented by enhanced standards and \n\nprocesses . For example, additional evaluation or enhanced validation standards \n\nand processes could apply to AI used for risk and regulatory use cases where \n\nthere may be heightened requirements on performance evaluation or \n\nthresholds . The alignment of baseline standards and processes across the bank \n\nhelped ensure that key model risks were addressed consistently for AI with \n\nsimilar characteristics and risks regardless of where they were used in the bank. \n\n6.2 Data Management \n\n6.2.1  Robust data management is essential to support the development and \n\ndeployment of AI . G eneral bank -wide data governance and management Artificial Intelligence Model Risk Management | 19 \n\nstandards and processes 33  would apply to data used for AI. F or example, whether \n\ndata was used for reporting purposes or for AI systems , the same data \n\ngovernance committees generally oversee approvals and management of data \n\nissues. Similarly,  standards and processes for key data management controls such \n\nas basic data quality checks would also apply . However , to address AI -specific \n\nrequirements, all banks had established additional data management standards \n\nand processes to ensure that data used for AI development and deployment are \n\nfit for purpose . An overview of key data management areas for AI development \n\nand deployment that most banks generally focus ed on are listed below. \n\nStandards or processes relating to data management that are specific to AI \n\ndevelopment, validation, deployment, monitoring or change management are \n\ncovered in the subsequent sections. \n\na.  Appropriateness of data for AI use cases - Ensuring data used for \n\ndevelopment and deployment of AI are suitable for the context in which the \n\nAI is used , including assessing the use of such data against fairness and ethical \n\nconsideration s.\n\nb.  Representativeness of data for development - Ensuring data selected for \n\ntraining and testing AI models are representative of the real -world conditions ,\n\nincluding stressed conditions, under which the AI would be used .\n\nc.  Robust data engineering during development - Ensuring data processing \n\nsteps , 34  which may include additional data quality checks  35  , feature                               \n\n> 33 Please see MASâ€™ information paper on Data Governance and Management Practices for more details on general data\n> governance and management standards and processes . The paper covered governance and oversight, data management\n> function, data quality and data issue smanagement, which would also apply to data used for AI. Other relevant regulations and\n> publications include t he Personal Data Protection Act (PDPA) , which comprises various requirements on data privacy governing\n> the collection, use, disclosure and care of personal data, and provides a baseline standard of protection for personal data in\n> Singapore ;and Advisory Guidelines on Use of Personal Data in AI Recommendation and Decision Systems issued by the\n> Personal Data Protection Commission (PDPC) in March 2024 . Please refer to Annex B for the relevant links.\n> 34 Examples of data processing steps include missing value imputation, replacement of outlier values and standardi sation or\n> normalisation of data values .\n> 35 To ensure data quality, key areas such as data relevance, accuracy, completeness and recency may be assessed.\n\nArtificial Intelligence Model Risk Management | 20 \n\nengineering 36 , augmentation and labelling 37  of datasets , are robust and free \n\nof b ias , and that the integrity and lineage of data are che cked and tracked \n\nacross these data engineering steps .\n\nd.  Robust data pipelines for deployment - Establishing r obust controls around \n\ndata pipelines for deployment , includ ing continuous monitoring of the quality \n\nof data passed to deployed AI , as well as checks for anomalies, drift s, and \n\npotential bias that may have an impact on performance or fairness .\n\ne.  Documentation of data -related aspects for reproducibility and auditability -\n\nEnsuring key data management steps, such as data sourc ing , data selection, \n\ndata lineage, data processing , approvals and remediation actions taken for \n\ndata issues are documented to enable reproducibility and auditability. \n\n6.2.2  Some banks have also established additional data management standards and \n\nprocesses in the areas below: \n\na.  To ensure that data is being used appropriately when developing or deploying \n\nAI, a few banks have required approvals to be obtained for high -risk data use \n\ncases , such as data use where a third party may have access to the bankâ€™s \n\ninternal data , use of employee data for monitoring, or the collection of \n\nbiometric data to identify individuals .                                     \n\n> 36 Features refer to the attributes of data points in a dataset, e.g., for data relating to a loan, the income of the obligor and\n> outstanding value of the loan are two possible attributes or features. Feature engineering refers to the process of selecting,\n> modifying or creating new features from the original attributes of a data set to improve a n AI modelâ€™s performance , e.g.,\n> normalising income of the obligor and outstanding value of the loan to a common scale ranging from 0 to 1; or creating new\n> derived features, such as a debt -to -income ratio , from existing attributes .\n> 37 When training AI models for a specific task, such as predicting a credit default or recommending a suitable financial product\n> to a customer ,we need data that includes the input variables (e.g. ,data relating to a past loan ,or customer history ), as well\n> as a target variable (e.g. ,whether there was a credit default for the loan, or a recommendation that the customer accepted ).\n> Data labelling refers to the process of assigning such target variables, typically based on past historical data or via human\n> annotation.\n\nArtificial Intelligence Model Risk Management | 21 \n\nb.  To support data reusability and reduce the time needed for feature \n\nengineering across the bank, as well as enhance consistency and accuracy in \n\nmodel development , a few banks have also built feature marts 38 .\n\nc.  To account for the greater use of unstructured data 39 , there were also ongoing \n\nefforts to more effectively manage such unstructured data , such as improving \n\nmetadata management and tagging for unstructured data to enable better \n\ndata governance 40 . Most of the data management areas outlined in paragraph \n\n6.2.1 are also generally appli cable to unstructured data , where relevant .\n\n6.3  Development \n\nModel Selection \n\n6.3.1  Given the trade -offs of adopting more complex AI models (e.g., higher \n\nuncertainties, limited explainability), most banks required developers to justify \n\ntheir selection of a more complex AI model over a conventional model or a\n\nsimpler AI model  41  , (e.g., balancing the need for performance against \n\nexplainability for a specific use -case ). Some banks required developers to go \n\nbeyond qualitative justifications, and develop challenger models (which could be \n\neither conventional or simpler AI models) to explicitly demonstrate the \n\nperformance uplift of the AI model over the challenger model as part of this \n\njustification. \n\n> 38\n\nA feature mart is a centralised repository or database that stores curated, pre -processed and reusable features (variables or \n\nattributes) that can be used for training models . Aside from supporting data reusability , feature marts may also help improve \n\ndata governance by maintaining metadata on each feature, including details on its source s, transformations, lineage and \n\nquality. Feature marts may also allow for v ersion control, ensuring that any updates to features are tracked. \n\n> 39\n\nUnstructured data refers to information that does not follow a predefined format or organised structure, making it more \n\ndifficult to store and analyse using traditional databases or methods for structured data . Unstructured data typically includes \n\ndata types such as text, images, videos, and audio. While the use of unstructured data is not new to banks , e.g., using \n\nsurveillance videos from cameras at ATMs, the use of such data is growing due to Generative AI .\n\n> 40\n\nThese may also include updating and adapting other areas such as data discovery and classification, access rights , data \n\nlifecycle management, data saniti sation and validation , and security controls for unstructured data . \n\n> 41\n\nFor example, a developer who wishes to use a more complex neural network -based deep learning model may be required \n\nto justify the need for such an AI model over a simpler tree -based machine learning model or a logistic regression model, and \n\nconsider the trade -offs based on the use case requirements. Artificial Intelligence Model Risk Management | 22 \n\nRobustness and Stability \n\n6.3.2  In assessing the overall suitability of AI models , banks placed heavy focus on \n\nensuring that AI models were both robust and stable 42 , and accordingly paid \n\nsignificant attention to i) the selection and processing of datasets used for \n\ntraining and testing AI models ; ii) determining appropriate approaches, measures \n\nand thresholds for evaluating AI models ; and iii) mitigating overfitting risk s43  that \n\noften arise due to the complexity of AI models . We outline some of the practices \n\nin these key areas below. \n\nSelection and Processing of Datasets for Training and Testing \n\n6.3.3  Datasets chosen for training and testing or evaluati on  44  of AI models were \n\nexpected to be representative of the full range of input values and environments \n\nunder which the AI model was intended to be used. Training and testing datasets \n\nwere also checked to ensure that their dist ribu tions or characteristics are \n\nsimilar 45 .\n\n6.3.4  Most banks also invested efforts in collecting testing datasets that allowed \n\npredictions or outputs from AI models to be tested or evaluated in the bankâ€™s \n\ncontext as far as possible . For example, curating datasets that allow ed for AI \n\nmodel generated answers to queries from customers to be compared against \n\nanswers from in -house human expert s, or getting actual feedback from the \n\nbankâ€™s customers on the quality of these AI model generated answers .\n\nEvaluation Approaches, Measures and Thresholds                                                      \n\n> 42 The concepts of robustness and stability in AI systems often overlap and what these terms cover can vary. For the purpose\n> of this information paper, r obustness refers to AIâ€™s ability to achieve its desired level of performance under real -world\n> conditions ,while stability refers to the consistent performance of AI across arepresentative range of real -world scenarios.\n> These concepts are also related to the reliability of the AI system or model.\n> 43 Overfitting is when an AI model learns the training data overly well , to the point where it perform sextremely well on training\n> data but very poorly on new data that it has not seen in the training dataset .Intuitively, t his may mean that the model has\n> memorised the training examples rather than learning general patterns, resulting in poor performance in real -world conditions .\n> 44 The terms â€œtesting â€and â€œevaluat ion â€of AI models are commonly used interchangeably to refer to the assessment of the\n> performance of AI models on datasets that it had not been trained on .\n> 45 This issue is also commonly referred to as training -testing skew, which are discrepancies between the distribution of data\n> used to train an AI model and the distribution of data it encounters during testing .Artificial Intelligence Model Risk Management |23\n\n6.3.5  Given that AI is developed to meet specific business needs or objectives, banksâ€™ \n\nstandards and processes on the robustness and stability of AI models generally \n\nrequired test ing or evaluation approaches to be aligned with the intended \n\noutcomes that the AI models were meant to support. The exact approaches \n\nselected could differ depending on the nature of the AI model s, as well as the \n\nneeds of the use case. For example, assessing a fraud detection modelâ€™s ability to \n\nflag out known fraud cases by comparing against ground truth in historical data, \n\nor the usefulness of a financial product recommendation model through human \n\nfeedback. \n\n6.3.6  Correspondingly, w hile there are many established performance measures for AI \n\nmodels 46 , banks pa id significant attention to aligning the choice of performance \n\nmeasures with the intended outcomes that the AI models were meant to \n\nsupport. In some cases, this could involve trade -offs between different \n\nperformance measures. For example, if the intended outcome was to detect as \n\nmany instances of fraud as possible, performance measurement would need to \n\nfocus more on the proportion of false negatives (i.e. fraudulent instances that \n\nwere not detected) , even though this may come at the expense of a higher \n\nproportion of false positives (i.e. instances falsely flagged by the model as being \n\nfraudulent) .\n\n6.3.7  Other tests that banks may include to ensure robustness and stability 47  include \n\nthe following :\n\na.  Sensitivity analysis to understand how predictions or outputs of AI models \n\nchange under different permutations of data inputs . This also helps to identify \n\nimportant features that significantly influence predictions or outputs , and \n\nfacilitate explanations of the behaviour of AI models .                   \n\n> 46 There are a wide range of performance measures for AI models, and these are often specific to the task or use -case ;for\n> example, recall, precision, or F1 for classification tasks, mean absolute error or root mean squared error for regression tasks,\n> mean average precision or mean reciprocal rank for recommendation tasks.\n> 47 The objectives of some of these tests overlap, and may also relate to data management aspects that we outlined earlier .\n> Nonetheless, we list all the tests that we observed across the banks for completeness. Artificial Intelligence Model Risk Management |24\n\nb.  Stability analysis to compare the stability of data distribution s and predictions \n\nor outputs, e.g ., assessing whether the distribution of a training dataset from \n\nan earlier period matches the distribution of testing dataset s from more \n\nrecent period s, and how differences affect the performance of AI models .\n\nc.  Sub -population analysis , which are evaluations of how AI model s perform \n\nacross different sub -populations or subsets within the datasets (e.g., to \n\nidentify any significant differences in performance between different \n\ncustomer segments). Such analysis of sub -populations or subsets within the \n\ndatasets help to identify potential issues that might not be obvious in the \n\naggregated testing dataset, as well as potential sources of bias, which could \n\nsupport fairness assessments of AI models where necessary (e.g., wh ere sub -\n\npopulations relate to protected features or attributes such as race or gender ). \n\nd.  Error analysis to identify potential patterns in prediction errors (e.g., \n\nmisclassified instances), which help s to understand the limitations of AI \n\nmodel s.\n\ne.  Stress testing the response of AI models to edge cases or inputs outside the \n\ntypical range of values used in training. This allowed banks to better \n\ndetermine performance boundaries and identify limitations of AI model s.\n\nSome banks also tested the behaviour of AI model s in the context of \n\nunexpected inputs or conditions. Examples included adversarial testing or red \n\nteaming types of exercises . Such testing is especially important in the context \n\nof AI models used in high risk or customer -facing applications , as it allowed \n\nthe bank to establish conditions under which AI models would not perform as \n\nexpected or could introduce potential security or ethical concerns .\n\n6.3.8  Most banks would establish criteria or thresholds for performance measures , to \n\ndefine what was considered acceptable performance. Such thresholds need to \n\nbe clearly defined and documented , as well as mutually agreed upon by \n\ndevelopers and validators. Such thresholds were usually use case specific, and \n\ncould also be used subsequently to facilitate validation, pre -deployment checks, \n\nas well as monitoring and change management. Artificial Intelligence Model Risk Management | 25 \n\nMitigating Overfitting Risks \n\n6.3.9  The large number of parameters and inherent complexity of AI models increases \n\nthe risks of them overfitting on training data (in -sample data) and hence \n\nperforming poorly when deployed on out -of -sample data. Banks employed a \n\nvariety of mitigants to address this risk: \n\na.  Model selection â€“ Generally f avouring AI models of lower complexity unless \n\nthere are clear justifications to do otherwise ; or adopting approaches that \n\nconstrained the complex ity of AI models 48 .\n\nb.  Feature selection - Applying explainability methods to identify the key input \n\nfeatures or attributes that are important for the AI model predictions or \n\noutputs 49  and assess ing that they are intuitive from a business and/or user \n\nperspective 50 .\n\nc.  Model evaluation - Additional performance testing requirements to test the \n\nperformance of AI model s on unseen data where possible, such as cross -\n\nvalidation techniques 51  and testing against more out -of -sample/out -of -time 52 \n\ndatasets. \n\nExplainability \n\n6.3.10  All banks identified explainability as a key area of focus for AI, particularly for use \n\ncases where end -users or customers need to understand key features or \n\nattributes in the data influencing predictions of AI models . For example, \n\nexplainability would be more important in higher risk materiality use cases w here  \n\n> 48\n\nExamples include regulari sation techniques or limiting the number and depth of trees for gradient boosting trees. Such \n\ntechniques generally try to limit the number of parameters used so that the trained model is less complex. For example, some \n\nregularisation  techniques  force less important parameter s to values of zero.  \n\n> 49\n\nAs discussed in the next section on explainability methods.  \n\n> 50\n\nAdditional justification would typically be required to retain features or attributes that were not intuitive, or which did n ot \n\nmeaningfully contribute to the overall performance of the models. Such data may introduce more noise, and cause the AI \n\nmodel to overfit on the noise, leading to poor performance in real world conditions.  \n\n> 51\n\nCross -validation generally refers to techniques to evaluate models by resampling the dataset for training and testing . An \n\nexample would be K-fold cross -validation (which involve s splitting the dataset into K parts for K training and testing rounds) . \n\n> 52\n\nAn out -of -sample testing dataset is a sub set of data not used in model training , whereas an out -of -time testing dataset is a\n\nsubset of data obtained from a time period distinct from the time period of the subset of data used in training the model. Artificial Intelligence Model Risk Management | 26 \n\nbank staff making decisions based on predictions  of AI models need to \n\nunderstand the key features or attributes 53  contributing to the prediction; or in \n\nuse cases where a customer may ask for reasons for being denied a financial \n\nservice. Hence, development standards for AI across all banks ha d been \n\nexpanded to include a section on explainability. \n\n6.3.11  Explainability requirements in banksâ€™ standards and processes generally required \n\ndevelopers to apply global and/or local 54  explainability methods to identify the \n\nkey features or attributes used as inputs to AI model s and their relative \n\nimportance ; assess whether these features or attributes were intuitive from a \n\nbusiness and/or user perspective ; and provide additional justification for \n\nretaining features o r attributes which were not intuitive. Such methods could \n\nalso help identify the usage of potentially sensitive features as part of fairness \n\nassessments. Some banks had set out a list of global and local explainability \n\nmethods that could be applied to explain the outputs  55  of AI models . Such \n\nmethods could be directly applied during development as part of the feature \n\nselection process, or used within explainability tools developed as part of the AI \n\nsystem so that either global and/or local explanations can be provided alongside \n\npredictions or outputs generated by AI models post -deployment. \n\n6.3.12  In terms of the level of explainability required for different use cases, some banks \n\nestablished standards and processes to clearly define the minimum level of global \n\nand/or local explainability required for different use cases. For these banks,  \n\n> 53\n\nAn example of a feature or attribute in this context could be the income of the customer.  \n\n> 54\n\nGlobal explainability is the ability to understand the overall functioning of the model by identifying how input features drive \n\nmodel outputs at an overall model level. Local explainability is the ability to identify how input feature s drive the model output \n\nfor a specific observation or instance. Taking a fraud detection model as an example, global explainability methods allow for \n\nidentification of the most important features , such as the high values of transactions, used to detect fraudulent transactions \n\nfor the model in general . However, the key features that are important for a specific transaction (i.e. the local instance) may \n\nnot necessarily be the same, e.g., the value of the transaction may be small for a specific instance but the transaction is still \n\ndetected as a fraudulent transaction due to specific characteristics of the parties involved in the transaction , such as an \n\nunfamiliar geographic location of one of the parties . Local explainability methods help to identify such features for the local \n\ninstance.  \n\n> 55\n\nCommon examples of explainability methods include SHAP (for global and local explainability) and LIME (for local \n\nexplainability). SHAP generates Shapley values for each feature based on its contribution to a given model output. A global -\n\nlevel explanation can be generated by generating a summary plot of the Shapley values of the key features, across the entire \n\nset of model outputs . LIME is based on training a separate model for the local instance that needs to be explained . The \n\nexplanation that is generated is based on the separately trained model .Artificial Intelligence Model Risk Management | 27 \n\nfactors considered  when applying a higher standard of global and/or local \n\nexplainability included risk materiality or the extent to which AI -driven decisions \n\nwere likely to require explanations (e.g. , to the bankâ€™s customers) for the use \n\ncase . For example, AI model s used for credit decisioning could require the most \n\nexacting standards for global and local explainability, requiring developers to \n\ncarefully consider all features used as inputs and provide justifications for their \n\nuse , as well as the ability for users to easily identify key features influencing any \n\ngiven prediction post -deployment. Other banks required global and/or local \n\nexplainability to be explored across all AI, but allowed users and owners to decide \n\non the acceptable level of explainability , and justify their decision based on the \n\nuse case .\n\nFairness \n\n6.3.13  The outputs of AI models are inherently influenced by the patterns learnt from \n\nits training data. If th e training data contain ed biases that unfairly represent or \n\ndisadvantage specific groups of individuals, AI model s may perpetuate these \n\nunfair biases in its predictions or outputs. This could lead to decisions or \n\nrecommendations that disproportionately and unfairly impact certain \n\ndemographic groups. \n\n6.3.14  The earlier section on data management had outlined the need for fairness to be \n\nconsidered during development, and for checks and monitoring of potential \n\nbiases during deployment. More specifically, d uring AI development, for use \n\ncases that could have a significant impact on individuals, most banks would \n\nundertake a formal assessment on whether specific groups of individuals could \n\nbe systematically disadvantaged by AI -driven decisions. The scope of such \n\nassessments could vary between banks depending on the relevant rules, \n\nregulations or expectations applicable to the bank 56 , and between use cases \n\ndepending on the risk materiality of the AI .    \n\n> 56 Examples of such expectations on fairness for AI used by banks across jurisdictions include the Principles to Promote Fairness,\n> Ethics, Accountability and Transparency (FEAT) in the use of Artificial Intelligence and Data Analytics in Singaporeâ€™s Financial\n> Sector , published by MAS in 2018; General Principles for the use of Artificial Intelligence in the Financial Sector , published by\n\nArtificial Intelligence Model Risk Management | 28 \n\n6.3.15  Generally, the approach for assessing fairness used by banks involved the \n\nfollowing steps: \n\na.  Defining a list of protected features or attributes, for which use of such \n\nfeatures or attributes in AI models would require additional analysis and \n\njustification. Common examples of such protected features or attributes \n\ninclude gender, race or age. \n\nb.  Determin ing whether such features or attributes 57  were used in training AI \n\nmodel s. Based on this assessment, to define groups of individuals at risk of \n\nbeing systematically disadvantaged by the AI -driven decisions (at -risk groups) .\n\nc.  Where necessary, determining the extent to which AI -driven decisions \n\nsystematically disadvantaged against at -risk groups. The was usually assessed \n\nvia fairness measures (e.g., fairness measures that are available in the toolkit \n\nreleased by the Veritas Initiative) .\n\nd.  Where necessary, providing adequate justifications on the use of protected \n\nfeatures or attributes in AI models (e.g. , trade -offs against the intended \n\nobjectives of the AI model 58 ). \n\nReproducibility and Auditability \n\n6.3.16  Reproducibility and auditability 59  of AI development are essential for ensuring \n\naccountability and building trust in AI systems . To facilitate reproducibility and \n\nauditability of AI, most banks expanded existing documentation requirements to \n\nincorporate the relevant AI development processes and considerations. A list of                 \n\n> De Nederlandsche Bank in 2019; and the High -level Principles on Artificial Intelligence , published by the Hong Kong Monetary\n> Authority in 2019.\n> 57 These could include proxy attributes that are heavily correlated with such protected attributes .\n> 58 This could be supported by, for example, analysis on the difference in performance between an AI model which included\n> these protected features or attributes, and an AI model which did not. An informed assessment could then be made on whether\n> this differenc e in performance was necessary to achieving the model's intended objective, taking into consideration the level\n> of potential harm done to at -risk groups arising from the use of the AI model.\n> 59 Reproducibility refers to â€œthe ability of an independent verification team to produce the same results using the same AI\n> method based on the documentation made by the organisation â€, while audibility refers to â€œthe readiness of an AI system to\n> undergo an assessment of its algorithms, data and design processes â€ ( Model AI Governance Framework, IMDA Singapore. )\n\nArtificial Intelligence Model Risk Management | 29 \n\nkey documentation requirements  for AI commonly seen across banks are as \n\nfollow s:\n\na.  Data - Documentation of key data management steps is important to facilitate \n\nreproducibility and auditability. During development, key information that \n\nwould usually be documented include datasets and data sources used for \n\nmodel development and evaluation, details of how these datasets were \n\nassessed as fit -for -purpose, processed ahead of model training, and split into \n\nrelevant training, testing and/or validation 60  datasets. \n\nb.  Model training - Details of how the AI model was trained or fit to the training \n\ndataset. Such details could include codes (along with software \n\npackages/environment used and their relevant versions), key settings (e.g. ,\n\nhyperparameters 61  used and the approach for selecting hyperparameters 62 ), \n\nrandom seed values 63  and any other configuration s required for a third party \n\nto reproduce the training process. \n\nc.  Model selection - Details of how the performance of the AI model was \n\nevaluated and how the final model was selecte d. Such details could include \n\nthe evaluation approaches, thresholds and datasets applied  64  and the \n\ncorresponding results, comparison s of performance across multiple AI models \n\nand justification s for selecting the final model. \n\nd.  Explainability - Global and/or local explainability methods used , feature \n\nselection process, analysis of results , as well as description of key features \n\nselected and additional justifications for inclusion of certain key features (e.g .,\n\nfeatures that may not have appear ed to be important to a human expert ).                          \n\n> 60 Testing and validation datasets refer to datasets used to evaluate the performance of the model outside of the dataset used\n> to train the model. This should be distinguished from independent validation, which is the process of independently assessing\n> the overall suitability of the model.\n> 61 E.g. ,number of trees and maximum tree depth for gradient boosted trees.\n> 62 E.g. ,grid search, random search of hyperparameters .\n> 63 AI models usually need to be in itialised with a random set of numbers (e.g., for the model parameters) before training , and\n> documenting the random seed value that is used to initialise the AI models is necessary to reproduce the AI modelâ€™s behaviour\n> and results.\n> 64 As detailed in the earlier sub -section on Robustness & Stability. Artificial Intelligence Model Risk Management |30\n\ne.  Fairness - Metrics and associated thresholds, results of fairness assessments \n\nand justifications for the use of any protected features or attributes .\n\n6.3.17  Alongside documentation requirements in the relevant standards and processes, \n\nmost banks also set up documentation templates that developers were required \n\nto follow for consistency . Such templates were typically designed by the bankâ€™s \n\nMRM function. Templates could differ between business domains (as different \n\nperforma nce tests or metrics could apply) or between AI of different risk \n\nmaterialities (as documentation requirements could be higher for AI of higher \n\nrisk materiality ). \n\n6.4 Validation \n\n6.4.1  Independent validation provides an objective and unbiased assessment of the \n\nsuitability, performance and limitations of AI. It acts as a n important challenge to \n\ndevelo pers , and ensures that the relevant standards and processes have been \n\nadhered to when developing AI. \n\n6.4.2  The validation process typically involves an independent unit 65  reviewing the AI \n\ndevelopment process and documentation, assessing that AI performs and \n\nbehaves as intended, and undertaking pre -deployment checks. Actions to \n\naddress issues identified during validation, such as the application of suitable \n\nadjustments or other mitig ating or compensat ory controls, would typically be \n\nproposed by developers and agreed to by validators before deploying AI. \n\n6.4.3  Building on their conventional MRM processes, banks have equipped \n\nindependent validation functions with the skills and incentives needed to \n\nconduct independent review of AI used in the bank , which include invest ments \n\nin efforts to ensure that independent validation staff have the relevant technical \n\nexpertise for AI.   \n\n> 65 For example, t he Federal Reserve/Office of the Comptroller of the Currencyâ€™s SR Letter 11 -7 on Supervisory Guidance on\n> Model Risk Management states that validation should generally be done by individuals not responsible for development or use\n> and do not have a stake in whether a model is determined to be valid.\n\nArtificial Intelligence Model Risk Management | 31 \n\n6.4.4  Banks adopted a range of approaches in establishing independent validation \n\nrequirements across different AI. One bank required all AI to be subject to \n\nindependent validation, with the depth and rigour of validation varying based on \n\nthe AIâ€™s risk materiality rating. Most other banks required independent validation \n\nonly for AI of higher risk materiality , with other AI subject only to peer review 66 .\n\nEven for AI of lower risk materiality, the involvement of either an independent \n\nvalidator or peer reviewer allowed for some degree of challenge that helped to \n\nbetter manage the added uncertainties and risks posed by AI, and check that such \n\nAI was developed in accordance with the bankâ€™s standards and processes. \n\n6.5 Deployment , Monitoring and Change Management \n\nPre -Deployment Checks \n\n6.5.1.  Aside from checks during the validation process, p re -deployment checks and \n\ntests are important to ensure that the AI has been correctly implemented and \n\nproduces the intended results before being deployed for use . Banks placed \n\nsignificant focus on implementing controls for the deployment of AI to ensure \n\nthat the AI functions as intended in the production environment 67 . These controls \n\nwere usually based on existing technology risk management guidelines . F or \n\nexample, b anks would apply standard software development lifecycle (SDLC) \n\nprocesses to ensure that the AI application or system was secure, free from error \n\nand perform ed as intended before deployment 68 . Some banks also conduct ed \n\nadditional check s to ensure that the deployed AI â€™s scope, output and \n\npe rformance , and associated controls align with that of the validated AI :\n\na.  Additional tests , such as :                 \n\n> 66 As compared to independent validation, peer reviews were usually conducted by a non -independent function (e.g. ,a\n> different development team in the same unit/reporting line as the original model developers).\n> 67 A production environment is alive operational setting where deployed systems, such as deployed AI models, are run under\n> real world condit ions to deliver services or perform tasks for end -users.\n> 68 Please see MASâ€™ Technology Risk Management Guidelines for further details on the adoption of sound and robust practices\n> for the management of technology risk in these areas: https://www.mas.gov.sg/regulation/guidelines/technology -risk -\n> management -guidelines\n\nArtificial Intelligence Model Risk Management | 32 \n\ni.  forward testing , which are experimental runs using a limited set of \n\nproduction data or with a limited set of users, for selected high materiality \n\nuse cases to assess the behaviour of AI in an environment similar to when \n\nthe AI is fully deployed ; and \n\nii.  live edge case test ing to assess how AI handles edge cases in the \n\nproduction environment , which helps to verify that AI can handle a variety \n\nof improbable but plausible scenarios when deployed .\n\nb.  Automated pipelines , such as setting up automated deployment and \n\ncontinuous integration/continuous deployment (CI/CD ) pipelines  69  to \n\nminimi se human error and maintain ing a consistent process for how AI is \n\ndeployed , monitored, and maintained , which is important for AI given the \n\nneed for regular data and model updates .\n\nc.  Process management , which includes checks to ensure that key processes \n\nimportant for the deployed AI, such as human oversight , backup models , and \n\nother appropriate controls and contingencies , are in place ; and business \n\nprocess change management, such as training users to understand AI \n\nlimitations and to use AI appropriately .\n\n6.5.2.  Non -AI specific pre -deployment checks  70  remain relevant, hence key control \n\nfunctions, such as those in the areas of technology, data, legal and compliance, \n\nthird -party and outsourcing , would also confirm that the checks have been \n\nundertaken and sign off before AI is deployed into production.               \n\n> 69 Continuous integration/continuous deployment (CI/CD) pipelines automate the process of building, testing, and deploying\n> code changes , and reduce the potential of errors arising from manual interventions .Approvals and checks are also usually\n> integrated into the CI/CD process to ensure that new code pushed into production are checked for errors .More details on\n> CI/CD, as well as other related terms such as MLOps and AIOps are provided in Annex A.\n> 70 For example, checks relating to cyber -security , or compliance with outsour cing policies .Artificial Intelligence Model Risk Management |33\n\nMonitoring Metrics and Thresholds \n\n6.5.3.  Monitoring is particularly critical for AI given their dynamic nature and the \n\npotential for AI model staleness due to drifts  71  in either data or the model \n\nbehaviour over time. All b anks pa id significant focus to the ongoing monitoring \n\nof their AI to ensure that they continue to operate as intended post -deployment. \n\nKey measures that were monitored generally follow those that were covered \n\nduring development and validation, and include robustness , stability, data \n\nquality, and fairness measures .\n\n6.5.4.  Measures used for monitoring were tracked against predefined thresholds ,\n\nusually determined at the development and validation stages,  to ensure models \n\nperform within acceptable boundaries . Some banks have also implemented \n\ntiered thresholds, for example, additional  early warning thresholds to pre -empt \n\nmodel deterioration, and different thresholds to determine when retraining or a \n\nfull redevelopment of the AI may be necessary. \n\n6.5.5.  Most banks also have a process or system for reporting , tracking and resol ving \n\nissues or incidents if breaches or anomalies arise from the monitoring process. \n\nBanks generally track issues or incidents from discovery to resolution, and \n\nincorporate a relevant escalation process based on the materiality of the issue or \n\nincident. The resolution process may include AI model retrai ning ,\n\nredevelopment , or decommissioning as possible outcomes. Where a major \n\nredevelopment was undertaken , revalidation and approval would be needed \n\nbefore the updated model could be redeployed .   \n\n> 71 AI models can perform poorly when they become stale due to factors such as data drift, concept drift or model drift, which\n> are essentially due to changes in the data distributions, relationships between input data and predictions/outputs, or the\n> general e nvironment in which the AI model is being used. More details on data, concept and model drifts are provided in Annex\n> A. Artificial Intelligence Model Risk Management |34\n\nContingency Plans \n\n6.5.6.  All banks would generally have standards and processes relating to contingency \n\nplans for AI, particularly those supporting high -risk or critical functions 72 . These \n\nplans , which may not be specific to AI, typically outline fallback options, such as \n\nalternative systems or manual processes, and would be subject to regular \n\nreviews and testing to ensure readiness for rapid activation when necessary. For \n\nmission -critical AI applications 73 , a few banks may also have kill switches in place. \n\nKill switches are used to deactivate AI if they exceed risk tolerances , and require \n\nclear contingency plans to be quickly rolled out .\n\nReview and Re validations \n\n6.5.7.  Aside from ongoing monitoring, banks also conduct ed periodic reviews of their \n\nportfolio of AI . Key aspects that that were usually reviewed include changes in \n\nthe model sâ€™ materiality, risks, scope and usage, performance, assumptions and \n\nlimitations, and identification and remediation of issues .\n\n6.5.8.  Banks also have standards and processes for ongoing re validations of AI in \n\nproduction, with the intensity and frequency based on the materiality of the AI .\n\nIn general, AI deemed critical to risk management, regulatory compliance, \n\nbusiness operations, or customer outcomes are revalidated more frequently and \n\nintensely .\n\nChange Management \n\n6.5.9.  Standards and processes relating to AI change management are needed to \n\nensure that what constitutes a change is clearly defined, and that the appropriate \n\ndevelopment and validation requirements are applied. M ost banks require d    \n\n> 72 Such contingency plans may not apply specifically to AI, but to technology systems in general. Nonetheless, they may require\n> additional considerations in the case of AI, e.g., AI -specific performance monitoring thresholds to determine when to trigger\n> the contingency plan, or a backup plan that involves another AI system or model.\n> 73 For example, for AI that are used for trading .\n\nArtificial Intelligence Model Risk Management | 35 \n\nsignificant  or material changes 74  to AI in production to be reviewed and approved \n\nby the control functions prior to implementation , so as to  ensure that any \n\nmodifications made to the model do not negatively impact its performance . To \n\nmanage changes to AI , b anks have also established systems and processes for \n\nversion control of both internal and third -party AI (which do not only cover code \n\nrelating to AI , but also data and other artifacts such as hyperparameters and the \n\ntrained model parameters or weights ). Version control enable s banks to track \n\nchanges across different aspects of AI and roll -back to previous version s of AI \n\nwhere ne cessary 75 . Most b anks have also set up processes for third -party AI \n\nproviders to provide notification s of version updates 76 .\n\n6.5.10.  AI for certain use cases, such as fraud detection, may need to be changed or \n\nupdated more frequently 77 , due to drifts in the data or the behaviour of the AI \n\nmodel over time . To deal with such frequent changes, some banks have \n\nestablished systems and processes for the automatic updating of such AI. Such \n\nAI, which some banks refer to as â€œdynamic AI â€, need to be subject to enhanced \n\nrequirements and controls to ensure that change management is well governed .\n\nKey additional requirements and controls include justification s for enabling \n\nautomatic updating of AI , clearly defining what can be updated automatically , for \n\nexample, restricting changes to the retraining of AI model with more recent \n\ndatasets , but not allowing for changes to AI model architectures or \n\nhyperparameters . Such dynamic AI would also be subject to enhanced risk  \n\n> 74\n\nExample s of significant or material change s include fundamental changes to AI model architecture s or training technique s.\n\nSuch changes may necessitate an in -depth revalidation, compared to less significant changes , such as retraining the AI model \n\nwith more recent data , which may only require checks on AI performance to ensure the AI is still behaving as expected.  \n\n> 75\n\nWhile we cover version control here under change management whe re the AI is already deployed , it is important to note \n\nthat version control for AI also plays a key role during the development and validation stages. For example, v ersion controls \n\nare needed to support iterative improvements and collaboration during development , and also help to ensure reproducibility \n\nand auditability during validation.  \n\n> 76\n\nWhile banks generally try to require third -party providers to notify them of any changes to the AI model or service, there \n\nmay be circumstances where such notifications may not happen, e.g., the third -party provider may not notify end -users on \n\nchanges that they view as immaterial. We have observed banks trying to address this by setting out clearer terms in their legal \n\nagreement s, for example, adding a clause that requires the third -party provider to notify banks on an y upcoming change s to \n\nthe AI model or system.  \n\n> 77\n\nFor example, if we compare a fraud detection use case with a n NLP use case such as summarisation of customer call \n\ntranscripts, data relating to the behaviour of scammers would usually change much more frequently than data relating to \n\ncustomer calls due to the active efforts of scammers to evade detection. Artificial Intelligence Model Risk Management | 36 \n\nmanagement requirements, such as enhanced  data management standards, e.g., \n\nadditional checks on data quality and drifts , as well as enhanced performance \n\nmonitoring requirements , e.g., more stringent monitoring notification \n\nthresholds .\n\n# 7 Other Key Areas \n\n7.1 Generative AI \n\nOverview \n\nWhile t he use of Generative AI in banks is still in the early stages , banks generally try to \n\napply existing governance and risk management structures and processes where relevant \n\nand practicable, and balance innovation and risk management by adopting :\n\nâ€¢ Strategies and approaches , where they leverage on the general -purpose nature of \n\nGenerative AI by focusing on the development of key enabling modules or services; \n\nlimit the current scope of Generative AI to use cases for assistin g/ augmenting \n\nhumans or improving internal operational efficiencies that are not direct customer \n\nfacing; and buil ding capacity and capabilities by establishing pilot and \n\nexperimentation frameworks ;\n\nâ€¢ Process controls , such as setting up cross -functional risk control checks at key \n\nstages of the Generative AI lifecycle ; establishing more detailed development and \n\nvalidation guidelines for different Generative AI task archetypes ; requir ing human \n\noversight for Generative AI decisions ; and paying close attention to user education \n\nand training on the limitations of Generative AI tools ; and \n\nâ€¢ Technical controls , such as selection, testing and evaluation of Generative AI \n\nmodels in the context of the bankâ€™s use cases; developing reusable modules to \n\nfacilitate testing and evaluatio n; assessing different aspects of Generative AI model \n\nperformance and risks; establish ing input and output filters as guardrails to address \n\ntoxicity, bias and privacy issues ; and mitigating data security risks via measures \n\nsuch as the use of private clouds or on -premise servers, data loss prevention tools, \n\nand limiting the access of Generative AI to more sensi tive information. Artificial Intelligence Model Risk Management | 37 \n\n7.1.1.  In addition to the k ey areas highlighted in the prior sections, there are some \n\naspects relating to Generative AI (compared to conventional AI) that require \n\nfurther consideration: \n\na.  Higher uncertainties associated with Generative AI â€“ The risks of \n\nhallucinations and unexpected behaviours by Generative AI given its greater \n\ncomplexity may lead to less robust and stable performance , and was a key \n\nconcern highlighted by banks . This concern was particularly pronounced for \n\nuse cases of higher risk materiality or those that are directly customer -facing ,\n\nwhere greater reliability was required .\n\nb.  Difficulties in evaluating/testing Generative AI and mitigating its limitations \n\nâ€“ Compared to conventional AI, which were typically used by banks for \n\nspecific use cases that the AI models had been trained for, Generative AI are \n\nmore general -purpose in nature and can be used in a wider range of use cases \n\nin the bank. However, there may be a lack of easily available ground truths 78 \n\nin some of these newer use cases to evaluate and test Generative AI. Use \n\ncases involving Generative AI also typically involve unstructured data , such as \n\ntext data , for which there are significantly more possible permutations ,\n\ncompared to structured data usually used for conventional AI . This makes it \n\nchallenging to foresee all potential scenarios and perform comprehensive \n\ntesting and evaluations 79 .\n\nc.  Lack of transparency from Generative AI providers - Unlike conventional AI \n\nmodels, which are often developed and trained internally by the bankâ€™s \n\ndevelopers, Generative AI used by banks were pre -dominantly based on pre -\n\ntrained models from external providers. As disclosure standards relating to \n\nsuch AI are still evolving globally, banks may lack full access to essential risk          \n\n> 78 Ground truth refers to reliable or factual information that serves as a standard against which the outputs or predictions of\n> AI models, including Generative AI models, can be evaluated.\n> 79 For example, it is significantly harder to evaluate the quality of a summary or of an image generated by Generative AI,\n> compared to evaluating the accuracy of a simple yes/no prediction from conventional AI. It is also harder to foresee all possible\n> permutations of text or images that may be used as inputs to Generative AI, as well as all possible permutations of text or\n> images that may be generated by Generative AI.\n\nArtificial Intelligence Model Risk Management | 38 \n\nmanagement information, such as details about the underlying data used in \n\nmodel training and testing, as well as the extent of evaluation or testing \n\napplied to these models. \n\nd.  Challenges in explainability and fairness with Generative AI â€“ The lack of \n\ntransparency from external providers may also contribute to challenges in \n\nunderstanding and explaining the outputs and behaviour of Generative AI ,\n\nand ensuring that the outputs generated by Generative AI are fair. There is \n\nalso a general lack of established methods currently for explain ing Generative \n\nAI outputs and assessing their fairness .\n\n7.1.2.  Most b anks are in the process of reviewing and updating parts of their AI model \n\nrisk management framework for Generative AI to balance the benefits and risks \n\nof its use. \n\n7.1.3. The subsequent paragraph s outline observations from the thematic on key \n\napproaches and controls that banks have adopted to balance innovation and risks \n\nbased on the current state of use of Generative AI . It should be noted that these \n\napproaches and controls will need to be updated as Generative AI technology \n\nevolves , and that risk management efforts will need to be scaled accordingly \n\nbased on the state of Generative AI use across the institution .\n\nStrategie s and Approaches \n\n7.1.4. Some banks have invested significant effort in identifying and building key \n\nenabling services and modules for Generative AI that can be utilised across \n\nmultiple use cases, e.g., vector databases 80 , retrieval systems 81 , evaluation and           \n\n> 80 Data, particularly unstructured data, such as text and images, need to be encoded into numerical representations before\n> they can be used for AI or Gen erative AI. Such numerical representations are commonly referred to as vectors. Vector\n> databases are speciali sed database systems designed to store, index, and efficiently query such data.\n> 81 Retrieval systems help to search information repositories and retrieve the most relevant information for a specific task. For\n> example, to help answer a query relating to information in a corporate information repository, the retrieval system will help\n> to search for the most relevant pieces of information in the corporate information repository . The retrieved information is then\n> usually used as context for the Generative AI model to generate an answer from. Artificial Intelligence Model Risk Management |39\n\ntesting modules 82 . Such an approach enables scalability, reduces time and costs \n\nfor implementation, and facilitates the development of more robust and stable \n\nGenerative AI. \n\n7.1.5. To manage the potential impact of Generative AI risks, such as hallucinations, \n\nmost banks have started with a more limited scope of use, focusing on the use of \n\nGenerative AI for assisting or augmenting humans, or improving inte rnal \n\noperational efficiencies, rather than deploying Generative AI in direct customer -\n\nfacing applications without a human -in -the -loop. Banks felt that such an \n\napproach would allow them to learn how to utilise Generative AI effectively and \n\nunderstand its limitations, while managing the potential impact of risks posed by \n\nGenerative AI. \n\n7.1.6. Similarly , to gain greater comfort with the use of Generative AI, most banks have \n\nestablished clear policies and procedures for Generative AI pilots and \n\nexperimentation frameworks. Aside from helping the bank to build capacity and \n\ncapabilities while managing risks a ssociated with Generative AI, such pilots and \n\nexperimentation frameworks are needed to evaluate and test Generative AI in \n\nreal -world scenarios and understand how Generative AI would behave when \n\ndepl oyed . Such pilots are typically bound by time and user lim its 83 .\n\nProcess Controls \n\n7.1.7. To address the cross -cutting nature of Generative AI use cases and risks, as well \n\nas the fast -evolving landscape, some banks have instituted cross -functional risk \n\ncontrol checks at key stages of the Generative AI lifecycle. \n\n7.1.8. As most Generative AI use cases usually fall within a few task archetypes, e.g., \n\nsummarisation, information extraction, conversational agents, question \n\nanswering, one bank established detailed development and validation guidelines                       \n\n> 82 An example of such a module could be a separately trained AI model that estimates the probability of an answer generated\n> by an LLM being a hallucination.\n> 83 Aside from setting time and user limits, other requirements that may apply to such pilots or experiments include setting\n> clear criteria for success at the end of the pilot ,conditions on the terms of use for owners and end -users , and close monitoring\n> of usage patterns and outputs for anomalies and to ensure compliance with the limited scope of usage. Artificial Intelligence Model Risk Management |40\n\nspecific to different Generative AI task archetypes to support development and \n\nvalidation processes. \n\n7.1.9. Due to the uncertainties associated with Generative AI, banks continue to require \n\nhuman oversight or have a human -in -the -loop when using Generative AI to aid in \n\ndecision -making . Extensive user education and training on the limitations of \n\nGenerative AI tools was another key area of focus. \n\nTechnical Controls \n\n7.1.10. As most Generative AI models used by banks, whether closed or open -source, \n\noriginate from third parties, selection of the appropriate model continues to be \n\nan important step for most banks. To assess suitability , some banks would \n\ntypically start by conduct ing significant research on the capabilities of these \n\nmodels for their needs , including utilising public benchmarks and the latest \n\nresearch papers to guide decisions. Testing and evaluation of Generative AI \n\nmodels in the context of the bankâ€™s use cases was also an important area of focus. \n\n7.1.11. More advanced banks would undertake a range of assessments, from \n\nstandalone, functional to end -to -end assessments. Standalone assessments \n\ninvolve the evaluation of the Generative AI model itself. This is usually based on \n\npublicly available data or resources , such as evaluation results in research \n\narticles , model leaderboards, or using open -source evaluation datasets .\n\nFunctional assessments involve evaluati ons of Generative AI model performance \n\non tasks and contexts specific to the bank, e.g., evaluating the performance of a \n\nGenerative AI model when used for retrieval of information from the bankâ€™s \n\nrepository . Finally, end -to -end assessments would evaluate the performance of \n\nthe entire Generative AI system, which may involve multiple Generative AI or AI \n\nmodels .\n\n7.1.12. Such banks also paid significant attention to establishing methods for assessing \n\ndifferent aspects of Generative AI model performance such as accuracy, Artificial Intelligence Model Risk Management | 41 \n\nrelevance, and bias 84 , as well as creating reusable modules to facilitate testing \n\nand evaluation .\n\n7.1.13. The more advanced banks also paid significant attention to curating testing \n\ndatasets that were specific to the use cases and tasks that Generative AI models \n\nwere being used for in the bank. Such testing datasets were critical to ensuring \n\nthat Generative AI models and systems were fit -for -purpose in the bank â€™s \n\ncontext . For example, if Generative AI was used for summarising complaints from \n\nthe bankâ€™s customers, the performance of Generative AI on general \n\nsummarisation tasks may not be indicative of its performance in the bankâ€™s \n\ncontext as it may not have been trained on such complaints that are not in the \n\npublic domain , and the complaints may also contain information specific to the \n\nbank, e.g., the bankâ€™s services. To ensure the proper evaluation of Generative AI \n\nin the bankâ€™s context , t he bank will need to curate bank -specific testing datasets \n\nfrom the bankâ€™s internal historical data, or use expert human annotators to \n\ngenerate good quality summaries for a set of customer complaints to evaluate \n\nagainst . Such testing datasets are also important for monitoring the ongoing \n\nperformance of Generative AI models, and for evaluating newer Generative AI \n\nmodels as part of the onboarding process. Other key tests that banks adopted \n\ninclude d model vulnerability testing to assess cyber security risks 85 , as well as \n\nstability and sensitivity testing to ensure consistent performance. Human \n\nfeedback also play ed a key role in testing, evaluating and monitoring Generative \n\nAI performance .\n\n7.1.14. Most banks have established input and output guardrails that utilise filters to \n\nmanage risks relating to areas such as toxicity, biasness, or leakage of sensitive \n\ninformation . Such filters may use rules or AI to detect such undesired or \n\ninappropriate information. For example, input  filters may be used to reject \n\nrequests with toxic language , or replace PII information in requests with generic                        \n\n> 84 In this context, a ccuracy refers to whether the generated text aligns with factual informatio n; r elevance refers to how\n> pertinent the generated text is to the specific query ; and b ias refers to scenarios where the generated text may be biased to\n> specific groups of people , e.g., the generated content may favour one gender over another.\n> 85 These were discussed at length in MASâ€™ information paper on Cyber Risks Associated with Generative Artificial Intelligence\n> and will not be repeated here. See Annex B for link to the paper.\n\nArtificial Intelligence Model Risk Management | 42 \n\nplaceholders. O utput filters may be used to detect biasness or toxic language in \n\nthe outputs of Generative AI and trigger a review by a human or another \n\nGenerative AI model , or redact PII information in the outputs of Generative AI \n\nbefore they are presented to the user . Similarly, some banks also focused efforts \n\non developing guardrails that were reusable. \n\n7.1.15. Banks mitigate d data security risks when using Generative AI by either using \n\nprivate cloud solutions for Generative AI models, or open -source models on -\n\npremise , which  keep sensitive data within controlled environments (either \n\ndedicated cloud resources not shared with other organisations, or on -premise \n\nserve rs) which can reduce the risks of exposure of data to external parties . Legal \n\nagreements with solution providers, data loss prevention tools, as well as limits \n\non the classification of data that could be used in Generat ive AI were also \n\nimportant to mitigate data security risks. \n\n7.1.16. Anoth er common area that banks were exploring to address Generative AI risks \n\nwere grou nding methods  86  such as retrieval augmented generation ( RAG ) 87 \n\nwhere the outputs of Generative AI models are constrained based on internal \n\nknowledge bases , and source citations are provided to allow end -users to check \n\nfor the accuracy of Generative AI outputs.                      \n\n> 86 Grounding methods help to ground or anchor the Generative AI outputs to factual, verifiable information, which can help\n> reduc ehallucinations and improv erobustness.\n> 87 Retrieval -Augmented Generation (RAG) methods typically retrieve relevant information from a pre -defined knowledge base,\n> and provide the retrieved information as context to the Generative AI model for the generation of outputs. For example, to\n> generate an answer to a question, information relevant to the questi on would be first retrieved, and the retrieved information\n> would then be provided as context to an LLM. The LLM would usually be instructed to answer the question based on the\n> retrieved information .Links to t he retrieved information could also be provided as source citations in the answer. There is\n> however still the possibility of hallucinations occurring even with such approaches. Artificial Intelligence Model Risk Management |43\n\n7.2 Third -Party AI \n\nOverview \n\nExisting third -party risk management standards and processes 88  continue to play an \n\nimportant role in banksâ€™ efforts to mitigat e risks associated with third -party AI. As far as \n\npracticable, most banks also extended controls for internally developed AI to third -party \n\nAI. When considering the use of third -party AI, b anks would weigh the potential benefits \n\nagainst the risks of using third -party AI. To address the additional risks arising from third -\n\nparty AI , banks were exploring areas such as :\n\nâ€¢ conducting c ompensatory testing ;\n\nâ€¢ enhancing c ontingency planning ;\n\nâ€¢ updating l egal agreements ; and \n\nâ€¢ investing in training and other a wareness efforts .\n\n7.2.1  The u se of third -party AI is increasingly common among banks , particularly in the \n\ncontext of Generative AI where most banks utilise Generative AI models that \n\nwere pre -trained by an external party. However, the use of such third -party AI \n\nand Generative AI presents additional risks , such as unknown biases from pre -\n\ntraining data, data protection concerns , as well as concentration risks due to \n\nincreased interdependencies , e.g., from multiple FIs or even third -party providers \n\nrelying on common underlying Generative AI model s. The lack of transparency is \n\noften cited as a key challenge in managing such third -party risks. Third -party AI \n\nproviders may be reluctant to disclose proprietary information about their \n\ntraining data or algorithms, hindering banksâ€™ efforts in risk assessment and \n\nongoing monitoring. \n\n7.2.2  To mitigate these additional risks, banks were exploring various approaches , such \n\nas:                  \n\n> 88 This includes processes required to comply with MASâ€™ Notice and Guidelines on Outsourcing (refer to\n> https://www.mas.gov.sg/regulation/third -party -risk -management ).Artificial Intelligence Model Risk Management |44\n\na.  Compensatory testing - conduct ing rigorous testing of third -party AI models \n\nusing various datasets and scenarios to verify the modelâ€™s robustness and \n\nstability in the bankâ€™s context , and to detect potential biases. \n\nb.  Contingency planning - develop ing robust contingency plan s to address \n\npotential failures, unexpected behaviour of third -party AI, or discontinuing of \n\nsupport by vendors. This can include having backup systems or manual \n\nprocesses in place to ensure business continuity. \n\nc.  Legal agreements - updat ing contracts with third -party AI providers to include \n\nclauses such as those pertaining to performance guarantee s, data protection, \n\nthe right to audit, and notification when AI is introduced (or not incorporating \n\nAI without the bankâ€™s agreement) in existing third -party providers â€™ solutions. \n\nSuch clauses could facilitate clear er expectations and responsibilities. \n\nd.  Awareness efforts â€“ invest ing in training of staff on AI literacy and risk \n\nawareness to improve understanding and mitigation of risks; conduct ing \n\nsurveys with third -party providers to gather more information about whether \n\nAI is being used in their products or services, and third -party providersâ€™ \n\npractices , including their AI development and risk management processes. \n\n# 8 Conclusion \n\n8.1.  Robust oversight and governance of AI, supported by comprehensive \n\nidentification , inventorisation of AI and appropriate risk materiality assessment, \n\nas well as rigorous development, validation and deployment standards and \n\nprocesses are important areas that FIs need to focus on when using AI. As the AI \n\nlandscape continues to evolve , AI MRM framework s will need to be regularly \n\nreviewed and updated , and risk management efforts scale d up based on the state \n\nof AI use. Aside from AI MRM, controls in non AI -speci fic areas such as general \n\ndata governance and management, technology, cyber and third party risk \n\nmanagement, and legal and compliance will also need to be reviewed to take AI \n\ndevelopments into account. Artificial Intelligence Model Risk Management | 45 \n\n8.2.  As the AI landscape continues to evolve, MAS will continue to work with the \n\nindustry to help facilitate and uplift AI and Gen erative AI governance and risk \n\nmanagement efforts across the financial industry, through information sharing \n\nefforts such as this paper to promulgat e industry best practices, and industry \n\ncollaborations such as Project Mind Forge. MAS is also considering supervisory \n\nguidance for all FIs next year, building upon the focus areas covered in this \n\ninformation paper. Artificial Intelligence Model Risk Management | 46 \n\n# Annex A - Definitions \n\nâ€¢ Model â€“ A model  is a method, system or approach which converts assumptions and \n\ninput data into quantitative estimates, decisions, or decision recommendations \n\n(based on the Global Associate of Ris k Professionals â€™ definition of a model ). Apart \n\nfrom AI models , which typically refer to machine or deep learning models which we \n\ndefine below , banks also routinely utilise conventional models , such as economic, \n\nfinancial, or statistical models. Some models , such as logistic regression model s, are \n\ncommonly used in both statistical and AI fields and may be regarded as both AI and \n\nconventional model s. \n\n> â€¢\n\nArtificial Intelligence (AI) â€“ An  AI system is a machine -based system that, for explicit \n\nor implicit objectives, infers, from the input it receives, how to generate outputs such \n\nas predictions, content, recommendations, or decisions that can influence physical \n\nor virtual environments. Different AI systems vary in their levels of autonomy and \n\nadaptiveness after deployment  (based on the Organisation for Economic Co -\n\noperation and Development â€™s definition of AI ). Such a definition would include \n\nGenerative AI. An  AI  or Generative AI  system  can be based on one or multiple AI or \n\nGenerative AI models and may also involve other machine -based components. \n\nâ€¢ AI Use Case â€“ An AI or Generative AI use case usually refers to a  specific real -world \n\ncontext that the AI or Generative AI model or system is applied to. For example, an \n\nAI recommendation model or system that is applied to a financial product \n\nrecommendation use case. \n\nâ€¢ Machine learning â€“ Machine learning is a subset of AI where the AI directly learn s\n\nfrom data. The machine learning model learns model parameters (or model weights) \n\nto transform inputs into estimates or outputs from the data by updating these \n\nparameters iteratively based on an objective. For example, the machine learning \n\nmodel may be provided with histori cal data that consists of the information on \n\ncustomer s, e.g., income and existing value of debt (which we refer to as input data) ,\n\nand whether the customer had defaulted on a loan obligation (which we refer to as \n\nthe target variable or label) . The machine learning model can then be trained by \n\nlearning model parameters that allow it to transform input data to target variables \n\nor labels with maximum accuracy (or minimum error). Artificial Intelligence Model Risk Management | 47 \n\nâ€¢ Deep learning â€“ Deep learning is a subset of machine learning, usually based on \n\nneural networks (that were inspired by how neurons in the brain recognise complex \n\npatterns in data) that comprise multiple layers of neurons. Deep learning models are \n\nable to learn more comple x patterns due to the many layers of neurons in the model. \n\nâ€¢ Discriminative versus Generative AI models â€“ AI models that generate predictions, \n\ne.g., predicting a credit default based on customer information, or recommending a \n\nfinancial product based on customer information, are usually referred to as \n\ndiscriminative AI models. This is in contrast to Generative AI models that are usually \n\nused to generat e content such as text, images, audio or videos .\n\nâ€¢ CI/CD , DevOps, MLOps, AIOps, LLMOps â€“ Continuous integration/continuous \n\ndeployment (CI/CD) or DevOps pipelines automate the process of building, testing, \n\nand deploying code changes. These terms are closely related to the term MLOps, \n\nwhich is used to describe tools and systems that help to automate the process of \n\nbuilding, testing, deploying and monitoring the performance of machine learning \n\nsystems. More recent terms such as AIOps and LLMOps have also been used to \n\ndescribe such tools and systems for AI in g eneral or for Large Language Models \n\n(LLM). \n\nâ€¢ Data Drift - This occurs when the statistical properties of the distribution of the data \n\nchanges. For example, the underlying distribution of customer data may have drifted \n\nor changed over time due to changes in the lifestyles of customers . Hence, an AI \n\nmodel that was trained on data from a more distant time period may not perform \n\nas well on data from a more recent time period due to data drift . A common measure \n\nof how much a population distribution has changed over time is the Population \n\nStability Index (PSI). \n\nâ€¢ Concept Drift - This occurs when the underlying relationships between the features \n\nin input data and what the AI model is being used to predict or generate changes .\n\nFor example, customer preferences for financial products may have shifted due to \n\nbroad industry changes ( e.g., a shift in the relationships between customer \n\ninformation and their preferences for financial products ), and an AI model used to \n\ngenerate financial product recommendations may no longer perform as well due to Artificial Intelligence Model Risk Management | 48 \n\nsuch  concept drift s. A common measure of concept drift is the Characteristic \n\nStability Index (CSI). \n\nâ€¢ Model Drift - Model drift is a broader term that usually encompasses both data drift \n\nand concept drift, as well as other factors that can cause a model's performance to \n\ndegrade over time. Aside from measures such as PSI and CSI, monitoring the \n\nstatistical characteristics of AI predictions can also be used to detect drifts in general. \n\nâ€¢ Supervised learning â€“ Supervised learning is a machine learning approach where a\n\nmodel is trained on a labelled dataset. In this process , each data point  includes input \n\nfeatures paired with the corresponding output (label).  The model learns to map \n\ninputs to outputs  by comparing its predictions with the actual labels and updating \n\nthe model parameters iteratively . Classification , which involves the prediction of \n\nclasses or categories, and regression , which involves the prediction of continuous \n\nvalues, are common examples of supervised learning. \n\nâ€¢ Unsupervised learning â€“ Unsupervised learning is a machine learning approach \n\nwhere a model discovers patterns in data without the use of label s. An e xample of \n\nunsupervised learning is clustering, where data points are grouped together based \n\non their inherent similarities or dissimilarities .Artificial Intelligence Model Risk Management | 49 \n\n# Annex B - Useful References \n\nPublications for the Financial Sector issued by MAS \n\nâ€¢ MAS FEAT Principles:  https://www.mas.gov.sg/publications/monographs -or -information -\n\npaper/2018/feat \n\nâ€¢ Veritas Initiative : https://www.mas.gov.sg/schemes -and -initiatives/veritas \n\nâ€¢ Project MindForge: https://www.mas.gov.sg/schemes -and -initiatives/project -mindforge \n\nâ€¢ Information Paper on Implementation of Fairness Principles in Financial Institutionsâ€™ use of \n\nArtificial Intelligence/Machine Learning:  https://www.mas.gov.sg/publications/monographs -or -\n\ninformation -paper/2022/implementation -of -fairness -principles -in -financial -institutions -use -of -\n\nartificial -intelligence -and -machine -learning \n\nâ€¢ Information Paper on Cyber Risks Associated with Generative Artificial Intelligence: \n\nhttps://www.mas.gov.sg/regulation/circulars/cyber -risks -associated -with -generative -artificial -\n\nintelligence \n\nâ€¢ Information Paper on Data Governance and Management Practices: \n\nhttps://www.mas.gov.sg/publications/monographs -or -information -paper/2024/data -\n\ngovernance -and -management -practices \n\nâ€¢ Technology Risk Management Guidelines :\n\nhttps://www.mas.gov.sg/regulation/guidelines/technology -risk -management -guidelines \n\nâ€¢ Business Continuity Management Guidelines :\n\nhttps://www.mas.gov.sg/regulation/guidelines/guidelines -on -business -continuity -management \n\nâ€¢ Notice and Guidelines on Third -Party Risk Management :\n\nhttps://www.mas.gov.sg/regulation/third -party -risk -management \n\nâ€¢ Information Paper on Operational Risk Management - Management of Third Party Arrangements: \n\nhttps://www.mas.gov.sg/publications/monographs -or -information -paper/2022/operational -risk -\n\nmanagement ---management -of -third -party -arrangements Artificial Intelligence Model Risk Management | 50 \n\nNon -Financial Sector Specific Publications \n\nâ€¢ AI Verify : AI governance testing framework and software toolkit: \n\nhttps://www.aiverifyfoundation.sg/what -is -ai -verify/ \n\nâ€¢ Project Moonshot:  https://www.aiverifyfoundation.sg/project -moonshot/ \n\nâ€¢ Model Governance Framework for Generative AI :\n\nhttps://www.aiverifyfoundation.sg/resources/mgf -gen -ai/ \n\nâ€¢ Trusted Data Sharing Framework:  https://www.imda.gov.sg/how -we -can -help/data -\n\ninnovation/trusted -data -sharing -framework \n\nâ€¢ Personal Data Protection Act (PDPA):  https://www.pdpc.gov.sg/overview -of -pdpa/the -\n\nlegislation/personal -data -protection -act \n\nâ€¢ Advisory Guidelines on use of Personal Data in AI Recommendation and Decision Systems :\n\nhttps://www.pdpc.gov.sg/guidelines -and -consultation/2024/02/advisory -guidelines -on -use -of -\n\npersonal -data -in -ai -recommendation -and -decision -systems \n\nâ€¢ Guidelines and Companion Guide on Securing AI Systems : https://www.csa.gov.sg/Tips -\n\nResource/publications/2024/guidelines -on -securing -ai", "fetched_at_utc": "2026-02-08T19:07:02Z", "sha256": "aa4ced6d9f7b41ac579e07dea2f87952b5eb3158f3bd0de54ba0162322b0fc36", "meta": {"file_name": "AI Risk Management - Singapore.pdf", "file_size": 765210, "relative_path": "pdfs\\AI Risk Management - Singapore.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 22410}}}
{"doc_id": "pdf-pdfs-ai-risk-management-framework-nist-92288e1fcca5", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Risk Management Framework - NIST.pdf", "title": "AI Risk Management Framework - NIST", "text": "# NIST AI 100-1 \n\n# Artificial Intelligence Risk Management Framework (AI RMF 1.0) NIST AI 100-1 \n\n# Artificial Intelligence Risk Management Framework (AI RMF 1.0) \n\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1 January 2023 \n\nU.S. Department of Commerce \n\nGina M. Raimondo, Secretary \n\nNational Institute of Standards and Technology \n\nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology Certain commercial entities, equipment, or materials may be identified in this document in order to describe \n\nan experimental procedure or concept adequately. Such identification is not intended to imply recommenda-tion or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that \n\nthe entities, materials, or equipment are necessarily the best available for the purpose. \n\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1 Update Schedule and Versions \n\nThe Artificial Intelligence Risk Management Framework (AI RMF) is intended to be a living document. NIST will review the content and usefulness of the Framework regularly to determine if an update is appro-priate; a review with formal input from the AI community is expected to take place no later than 2028. The Framework will employ a two-number versioning system to track and identify major and minor changes. The first number will represent the generation of the AI RMF and its companion documents (e.g., 1.0) and will change only with major revisions. Minor revisions will be tracked using â€œ.nâ€ after the generation number (e.g., 1.1). All changes will be tracked using a Version Control Table which identifies the history, including version number, date of change, and description of change. NIST plans to update the AI RMF Playbook frequently. Comments on the AI RMF Playbook may be sent via email to AIframework@nist.gov at any time and will be reviewed and integrated on a semi-annual basis. Table of Contents \n\nExecutive Summary 1Part 1: Foundational Information 41 Framing Risk 4\n\n1.1 Understanding and Addressing Risks, Impacts, and Harms 41.2 Challenges for AI Risk Management 51.2.1 Risk Measurement 51.2.2 Risk Tolerance 71.2.3 Risk Prioritization 71.2.4 Organizational Integration and Management of Risk 8\n\n2 Audience 93 AI Risks and Trustworthiness 12 \n\n3.1 Valid and Reliable 13 3.2 Safe 14 3.3 Secure and Resilient 15 3.4 Accountable and Transparent 15 3.5 Explainable and Interpretable 16 3.6 Privacy-Enhanced 17 3.7 Fair â€“ with Harmful Bias Managed 17 \n\n4 Effectiveness of the AI RMF 19 Part 2: Core and Profiles 20 5 AI RMF Core 20 \n\n5.1 Govern 21 5.2 Map 24 5.3 Measure 28 5.4 Manage 31 \n\n6 AI RMF Profiles 33 Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3 35 Appendix B: How AI Risks Differ from Traditional Software Risks 38 Appendix C: AI Risk Management and Human-AI Interaction 40 Appendix D: Attributes of the AI RMF 42 \n\n# List of Tables \n\nTable 1 Categories and subcategories for the GOVERN function. 22 Table 2 Categories and subcategories for the MAP function. 26 Table 3 Categories and subcategories for the MEASURE function. 29 Table 4 Categories and subcategories for the MANAGE function. 32 iNIST AI 100-1 AI RMF 1.0 \n\n# List of Figures \n\nFig. 1 Examples of potential harms related to AI systems. Trustworthy AI systems and their responsible use can mitigate negative risks and contribute to bene-fits for people, organizations, and ecosystems. 5Fig. 2 Lifecycle and Key Dimensions of an AI System. Modified from OECD (2022) OECD Framework for the Classification of AI systems â€” OECD Digital Economy Papers. The two inner circles show AI systemsâ€™ key di-mensions and the outer circle shows AI lifecycle stages. Ideally, risk man-agement efforts start with the Plan and Design function in the application context and are performed throughout the AI system lifecycle. See Figure 3 for representative AI actors. 10 Fig. 3 AI actors across AI lifecycle stages. See Appendix A for detailed descrip-tions of AI actor tasks, including details about testing, evaluation, verifica-tion, and validation tasks. Note that AI actors in the AI Model dimension (Figure 2) are separated as a best practice, with those building and using the models separated from those verifying and validating the models. 11 Fig. 4 Characteristics of trustworthy AI systems. Valid & Reliable is a necessary condition of trustworthiness and is shown as the base for other trustworthi-ness characteristics. Accountable & Transparent is shown as a vertical box because it relates to all other characteristics. 12 Fig. 5 Functions organize AI risk management activities at their highest level to govern, map, measure, and manage AI risks. Governance is designed to be a cross-cutting function to inform and be infused throughout the other three functions. 20  \n\n> Page ii NIST AI 100-1\n\nAI RMF 1.0 \n\n# Executive Summary \n\nArtificial intelligence (AI) technologies have significant potential to transform society and peopleâ€™s lives â€“ from commerce and health to transportation and cybersecurity to the envi-ronment and our planet. AI technologies can drive inclusive economic growth and support scientific advancements that improve the conditions of our world. AI technologies, how-ever, also pose risks that can negatively impact individuals, groups, organizations, commu-nities, society, the environment, and the planet. Like risks for other types of technology, AI risks can emerge in a variety of ways and can be characterized as long- or short-term, high-or low-probability, systemic or localized, and high- or low-impact. The AI RMF refers to an AI system as an engineered or machine-based system that can, for a given set of objectives, generate outputs such as predictions, recommenda-tions, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy (Adapted from: OECD Recommendation on AI:2019; ISO /IEC 22989:2022). While there are myriad standards and best practices to help organizations mitigate the risks of traditional software or information-based systems, the risks posed by AI systems are in many ways unique (See Appendix B). AI systems, for example, may be trained on data that can change over time, sometimes significantly and unexpectedly, affecting system function-ality and trustworthiness in ways that are hard to understand. AI systems and the contexts in which they are deployed are frequently complex, making it difficult to detect and respond to failures when they occur. AI systems are inherently socio-technical in nature, meaning they are influenced by societal dynamics and human behavior. AI risks â€“ and benefits â€“ can emerge from the interplay of technical aspects combined with societal factors related to how a system is used, its interactions with other AI systems, who operates it, and the social context in which it is deployed. These risks make AI a uniquely challenging technology to deploy and utilize both for orga-nizations and within society. Without proper controls, AI systems can amplify, perpetuate, or exacerbate inequitable or undesirable outcomes for individuals and communities. With proper controls, AI systems can mitigate and manage inequitable outcomes. AI risk management is a key component of responsible development and use of AI sys-tems. Responsible AI practices can help align the decisions about AI system design, de-velopment, and uses with intended aim and values. Core concepts in responsible AI em-phasize human centricity, social responsibility, and sustainability. AI risk management can drive responsible uses and practices by prompting organizations and their internal teams who design, develop, and deploy AI to think more critically about context and potential or unexpected negative and positive impacts. Understanding and managing the risks of AI systems will help to enhance trustworthiness, and in turn, cultivate public trust.  \n\n> Page 1 NIST AI 100-1\n\nAI RMF 1.0 \n\nSocial responsibility can refer to the organizationâ€™s responsibility â€œfor the impacts of its decisions and activities on society and the environment through transparent and ethical behaviorâ€ ( ISO 26000:2010). Sustainability refers to the â€œstate of the global system, including environmental, social, and economic aspects, in which the needs of the present are met without compromising the ability of future generations to meet their own needsâ€ ( ISO /IEC TR 24368:2022). Responsible AI is meant to result in technology that is also equitable and accountable. The expectation is that organizational practices are carried out in accord with â€œ professional responsibility ,â€ defined by ISO as an approach that â€œaims to ensure that professionals who design, develop, or deploy AI systems and applications or AI-based products or systems, recognize their unique position to exert influence on people, society, and the future of AIâ€ ( ISO /IEC TR 24368:2022). As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283), the goal of the AI RMF is to offer a resource to the organizations designing, developing, deploying, or using AI systems to help manage the many risks of AI and promote trustwor-thy and responsible development and use of AI systems. The Framework is intended to be \n\nvoluntary , rights-preserving, non-sector-specific, and use-case agnostic, providing flexibil-ity to organizations of all sizes and in all sectors and throughout society to implement the approaches in the Framework. The Framework is designed to equip organizations and individuals â€“ referred to here as \n\nAI actors â€“ with approaches that increase the trustworthiness of AI systems, and to help foster the responsible design, development, deployment, and use of AI systems over time. AI actors are defined by the Organisation for Economic Co-operation and Development (OECD) as â€œthose who play an active role in the AI system lifecycle, including organiza-tions and individuals that deploy or operate AIâ€ [OECD (2019) Artificial Intelligence in Societyâ€”OECD iLibrary] (See Appendix A). The AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies continue to develop, and to be operationalized by organizations in varying degrees and capacities so society can benefit from AI while also being protected from its potential harms. The Framework and supporting resources will be updated, expanded, and improved based on evolving technology, the standards landscape around the world, and AI community ex-perience and feedback. NIST will continue to align the AI RMF and related guidance with applicable international standards, guidelines, and practices. As the AI RMF is put into use, additional lessons will be learned to inform future updates and additional resources. The Framework is divided into two parts. Part 1 discusses how organizations can frame the risks related to AI and describes the intended audience. Next, AI risks and trustworthi-ness are analyzed, outlining the characteristics of trustworthy AI systems, which include  \n\n> Page 2 NIST AI 100-1\n\nAI RMF 1.0 valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy enhanced, and fair with their harmful biases managed. Part 2 comprises the â€œCoreâ€ of the Framework. It describes four specific functions to help organizations address the risks of AI systems in practice. These functions â€“ GOVERN ,\n\n> MAP\n\n, MEASURE , and MANAGE â€“ are broken down further into categories and subcate-gories. While GOVERN applies to all stages of organizationsâ€™ AI risk management pro-cesses and procedures, the MAP , MEASURE , and MANAGE functions can be applied in AI system-specific contexts and at specific stages of the AI lifecycle. Additional resources related to the Framework are included in the AI RMF Playbook, which is available via the NIST AI RMF website: https://www.nist.gov/itl/ai-risk-management-framework. Development of the AI RMF by NIST in collaboration with the private and public sec-tors is directed and consistent with its broader AI efforts called for by the National AI Initiative Act of 2020, the National Security Commission on Artificial Intelligence recom-mendations, and the Plan for Federal Engagement in Developing Technical Standards and Related Tools. Engagement with the AI community during this Frameworkâ€™s development â€“ via responses to a formal Request for Information, three widely attended workshops, public comments on a concept paper and two drafts of the Framework, discussions at mul-tiple public forums, and many small group meetings â€“ has informed development of the AI RMF 1.0 as well as AI research and development and evaluation conducted by NIST and others. Priority research and additional guidance that will enhance this Framework will be captured in an associated AI Risk Management Framework Roadmap to which NIST and the broader community can contribute.  \n\n> Page 3 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Part 1: Foundational Information \n\n# 1. Framing Risk \n\nAI risk management offers a path to minimize potential negative impacts of AI systems, such as threats to civil liberties and rights, while also providing opportunities to maximize positive impacts. Addressing, documenting, and managing AI risks and potential negative impacts effectively can lead to more trustworthy AI systems. \n\n1.1 Understanding and Addressing Risks, Impacts, and Harms \n\nIn the context of the AI RMF, risk refers to the composite measure of an eventâ€™s probability of occurring and the magnitude or degree of the consequences of the corresponding event. The impacts, or consequences, of AI systems can be positive, negative, or both and can result in opportunities or threats (Adapted from: ISO 31000:2018). When considering the negative impact of a potential event, risk is a function of 1) the negative impact, or magni-tude of harm, that would arise if the circumstance or event occurs and 2) the likelihood of occurrence (Adapted from: OMB Circular A-130:2016). Negative impact or harm can be experienced by individuals, groups, communities, organizations, society, the environment, and the planet. â€œRisk management refers to coordinated activities to direct and control an organiza-tion with regard to riskâ€ (Source: ISO 31000:2018). While risk management processes generally address negative impacts, this Framework of-fers approaches to minimize anticipated negative impacts of AI systems and identify op-portunities to maximize positive impacts. Effectively managing the risk of potential harms could lead to more trustworthy AI systems and unleash potential benefits to people (individ-uals, communities, and society), organizations, and systems/ecosystems. Risk management can enable AI developers and users to understand impacts and account for the inherent lim-itations and uncertainties in their models and systems, which in turn can improve overall system performance and trustworthiness and the likelihood that AI technologies will be used in ways that are beneficial. The AI RMF is designed to address new risks as they emerge. This flexibility is particularly important where impacts are not easily foreseeable and applications are evolving. While some AI risks and benefits are well-known, it can be challenging to assess negative impacts and the degree of harms. Figure 1 provides examples of potential harms that can be related to AI systems. AI risk management efforts should consider that humans may assume that AI systems work â€“ and work well â€“ in all settings. For example, whether correct or not, AI systems are often perceived as being more objective than humans or as offering greater capabilities than general software.  \n\n> Page 4 NIST AI 100-1\n\nAI RMF 1.0  \n\n> Fig. 1. Examples of potential harms related to AI systems. Trustworthy AI systems and their responsible use can mitigate negative risks and contribute to benefits for people, organizations, and ecosystems.\n\n1.2 Challenges for AI Risk Management \n\nSeveral challenges are described below. They should be taken into account when managing risks in pursuit of AI trustworthiness. \n\n1.2.1 Risk Measurement \n\nAI risks or failures that are not well-defined or adequately understood are difficult to mea-sure quantitatively or qualitatively. The inability to appropriately measure AI risks does not imply that an AI system necessarily poses either a high or low risk. Some risk measurement challenges include: \n\nRisks related to third-party software, hardware, and data: Third-party data or systems can accelerate research and development and facilitate technology transition. They also may complicate risk measurement. Risk can emerge both from third-party data, software or hardware itself and how it is used. Risk metrics or methodologies used by the organization developing the AI system may not align with the risk metrics or methodologies uses by the organization deploying or operating the system. Also, the organization developing the AI system may not be transparent about the risk metrics or methodologies it used. Risk measurement and management can be complicated by how customers use or integrate third-party data or systems into AI products or services, particularly without sufficient internal governance structures and technical safeguards. Regardless, all parties and AI actors should manage risk in the AI systems they develop, deploy, or use as standalone or integrated components. \n\nTracking emergent risks: Organizationsâ€™ risk management efforts will be enhanced by identifying and tracking emergent risks and considering techniques for measuring them.  \n\n> Page 5 NIST AI 100-1\n\nAI RMF 1.0 AI system impact assessment approaches can help AI actors understand potential impacts or harms within specific contexts. \n\nAvailability of reliable metrics: The current lack of consensus on robust and verifiable measurement methods for risk and trustworthiness, and applicability to different AI use cases, is an AI risk measurement challenge. Potential pitfalls when seeking to measure negative risk or harms include the reality that development of metrics is often an institu-tional endeavor and may inadvertently reflect factors unrelated to the underlying impact. In addition, measurement approaches can be oversimplified, gamed, lack critical nuance, be-come relied upon in unexpected ways, or fail to account for differences in affected groups and contexts. Approaches for measuring impacts on a population work best if they recognize that contexts matter, that harms may affect varied groups or sub-groups differently, and that communities or other sub-groups who may be harmed are not always direct users of a system. \n\nRisk at different stages of the AI lifecycle: Measuring risk at an earlier stage in the AI lifecycle may yield different results than measuring risk at a later stage; some risks may be latent at a given point in time and may increase as AI systems adapt and evolve. Fur-thermore, different AI actors across the AI lifecycle can have different risk perspectives. For example, an AI developer who makes AI software available, such as pre-trained mod-els, can have a different risk perspective than an AI actor who is responsible for deploying that pre-trained model in a specific use case. Such deployers may not recognize that their particular uses could entail risks which differ from those perceived by the initial developer. All involved AI actors share responsibilities for designing, developing, and deploying a trustworthy AI system that is fit for purpose. \n\nRisk in real-world settings: While measuring AI risks in a laboratory or a controlled environment may yield important insights pre-deployment, these measurements may differ from risks that emerge in operational, real-world settings. \n\nInscrutability: Inscrutable AI systems can complicate risk measurement. Inscrutability can be a result of the opaque nature of AI systems (limited explainability or interpretabil-ity), lack of transparency or documentation in AI system development or deployment, or inherent uncertainties in AI systems. \n\nHuman baseline: Risk management of AI systems that are intended to augment or replace human activity, for example decision making, requires some form of baseline metrics for comparison. This is difficult to systematize since AI systems carry out different tasks â€“ and perform tasks differently â€“ than humans.  \n\n> Page 6 NIST AI 100-1\n\nAI RMF 1.0 \n\n1.2.2 Risk Tolerance \n\nWhile the AI RMF can be used to prioritize risk, it does not prescribe risk tolerance. Risk tolerance refers to the organizationâ€™s or AI actorâ€™s (see Appendix A) readiness to bear the risk in order to achieve its objectives. Risk tolerance can be influenced by legal or regula-tory requirements (Adapted from: ISO GUIDE 73). Risk tolerance and the level of risk that is acceptable to organizations or society are highly contextual and application and use-case specific. Risk tolerances can be influenced by policies and norms established by AI sys-tem owners, organizations, industries, communities, or policy makers. Risk tolerances are likely to change over time as AI systems, policies, and norms evolve. Different organiza-tions may have varied risk tolerances due to their particular organizational priorities and resource considerations. Emerging knowledge and methods to better inform harm/cost-benefit tradeoffs will con-tinue to be developed and debated by businesses, governments, academia, and civil society. To the extent that challenges for specifying AI risk tolerances remain unresolved, there may be contexts where a risk management framework is not yet readily applicable for mitigating negative AI risks. The Framework is intended to be flexible and to augment existing risk practices which should align with applicable laws, regulations, and norms. Organizations should follow existing regulations and guidelines for risk criteria, tolerance, and response established by organizational, domain, discipline, sector, or professional requirements. Some sectors or industries may have established definitions of harm or established documentation, reporting, and disclosure requirements. Within sectors, risk management may depend on existing guidelines for specific applications and use case settings. Where established guidelines do not exist, organizations should define reasonable risk tolerance. Once tolerance is defined, this AI RMF can be used to manage risks and to document risk management processes. \n\n1.2.3 Risk Prioritization \n\nAttempting to eliminate negative risk entirely can be counterproductive in practice because not all incidents and failures can be eliminated. Unrealistic expectations about risk may lead organizations to allocate resources in a manner that makes risk triage inefficient or impractical or wastes scarce resources. A risk management culture can help organizations recognize that not all AI risks are the same, and resources can be allocated purposefully. Actionable risk management efforts lay out clear guidelines for assessing trustworthiness of each AI system an organization develops or deploys. Policies and resources should be prioritized based on the assessed risk level and potential impact of an AI system. The extent to which an AI system may be customized or tailored to the specific context of use by the AI deployer can be a contributing factor.  \n\n> Page 7 NIST AI 100-1\n\nAI RMF 1.0 When applying the AI RMF, risks which the organization determines to be highest for the AI systems within a given context of use call for the most urgent prioritization and most thorough risk management process. In cases where an AI system presents unacceptable negative risk levels â€“ such as where significant negative impacts are imminent, severe harms are actually occurring, or catastrophic risks are present â€“ development and deployment should cease in a safe manner until risks can be sufficiently managed. If an AI systemâ€™s development, deployment, and use cases are found to be low-risk in a specific context, that may suggest potentially lower prioritization. Risk prioritization may differ between AI systems that are designed or deployed to directly interact with humans as compared to AI systems that are not. Higher initial prioritization may be called for in settings where the AI system is trained on large datasets comprised of sensitive or protected data such as personally identifiable information, or where the outputs of the AI systems have direct or indirect impact on humans. AI systems designed to interact only with computational systems and trained on non-sensitive datasets (for example, data collected from the physical environment) may call for lower initial prioritization. Nonethe-less, regularly assessing and prioritizing risk based on context remains important because non-human-facing AI systems can have downstream safety or social implications. \n\nResidual risk â€“ defined as risk remaining after risk treatment (Source: ISO GUIDE 73) â€“ directly impacts end users or affected individuals and communities. Documenting residual risks will call for the system provider to fully consider the risks of deploying the AI product and will inform end users about potential negative impacts of interacting with the system. \n\n1.2.4 Organizational Integration and Management of Risk \n\nAI risks should not be considered in isolation. Different AI actors have different responsi-bilities and awareness depending on their roles in the lifecycle. For example, organizations developing an AI system often will not have information about how the system may be used. AI risk management should be integrated and incorporated into broader enterprise risk management strategies and processes. Treating AI risks along with other critical risks, such as cybersecurity and privacy, will yield a more integrated outcome and organizational efficiencies. The AI RMF may be utilized along with related guidance and frameworks for managing AI system risks or broader enterprise risks. Some risks related to AI systems are common across other types of software development and deployment. Examples of overlapping risks include: privacy concerns related to the use of underlying data to train AI systems; the en-ergy and environmental implications associated with resource-heavy computing demands; security concerns related to the confidentiality, integrity, and availability of the system and its training and output data; and general security of the underlying software and hardware for AI systems.  \n\n> Page 8 NIST AI 100-1\n\nAI RMF 1.0 Organizations need to establish and maintain the appropriate accountability mechanisms, roles and responsibilities, culture, and incentive structures for risk management to be ef-fective. Use of the AI RMF alone will not lead to these changes or provide the appropriate incentives. Effective risk management is realized through organizational commitment at senior levels and may require cultural change within an organization or industry. In addi-tion, small to medium-sized organizations managing AI risks or implementing the AI RMF may face different challenges than large organizations, depending on their capabilities and resources. \n\n# 2. Audience \n\nIdentifying and managing AI risks and potential impacts â€“ both positive and negative â€“ re-quires a broad set of perspectives and actors across the AI lifecycle. Ideally, AI actors will represent a diversity of experience, expertise, and backgrounds and comprise demograph-ically and disciplinarily diverse teams. The AI RMF is intended to be used by AI actors across the AI lifecycle and dimensions. The OECD has developed a framework for classifying AI lifecycle activities according to five key socio-technical dimensions, each with properties relevant for AI policy and gover-nance, including risk management [OECD (2022) OECD Framework for the Classification of AI systems â€” OECD Digital Economy Papers]. Figure 2 shows these dimensions, slightly modified by NIST for purposes of this framework. The NIST modification high-lights the importance of test, evaluation, verification, and validation (TEVV) processes throughout an AI lifecycle and generalizes the operational context of an AI system. AI dimensions displayed in Figure 2 are the Application Context, Data and Input, AI Model, and Task and Output. AI actors involved in these dimensions who perform or manage the design, development, deployment, evaluation, and use of AI systems and drive AI risk management efforts are the primary AI RMF audience. Representative AI actors across the lifecycle dimensions are listed in Figure 3 and described in detail in Appendix A. Within the AI RMF, all AI actors work together to manage risks and achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific expertise are integrated throughout the AI lifecycle and are especially likely to benefit from the Framework. Performed regularly, TEVV tasks can provide insights relative to technical, societal, legal, and ethical standards or norms, and can assist with anticipating impacts and assessing and tracking emergent risks. As a regular process within an AI lifecycle, TEVV allows for both mid-course remediation and post-hoc risk management. The People & Planet dimension at the center of Figure 2 represents human rights and the broader well-being of society and the planet. The AI actors in this dimension comprise a separate AI RMF audience who informs the primary audience. These AI actors may in-clude trade associations, standards developing organizations, researchers, advocacy groups,  \n\n> Page 9 NIST AI 100-1\n\nAI RMF 1.0  \n\n> Fig. 2. Lifecycle and Key Dimensions of an AI System. Modified from OECD (2022) OECD Framework for the Classification of AI systems â€” OECD Digital Economy Papers. The two inner circles show AI systemsâ€™ key dimensions and the outer circle shows AI lifecycle stages. Ideally, risk management efforts start with the Plan and Design function in the application context and are performed throughout the AI system lifecycle. See Figure 3 for representative AI actors.\n\nenvironmental groups, civil society organizations, end users, and potentially impacted in-dividuals and communities. These actors can: â€¢ assist in providing context and understanding potential and actual impacts; â€¢ be a source of formal or quasi-formal norms and guidance for AI risk management; â€¢ designate boundaries for AI operation (technical, societal, legal, and ethical); and â€¢ promote discussion of the tradeoffs needed to balance societal values and priorities related to civil liberties and rights, equity, the environment and the planet, and the economy. Successful risk management depends upon a sense of collective responsibility among AI actors shown in Figure 3. The AI RMF functions, described in Section 5, require diverse perspectives, disciplines, professions, and experiences. Diverse teams contribute to more open sharing of ideas and assumptions about the purposes and functions of technology â€“ making these implicit aspects more explicit. This broader collective perspective creates opportunities for surfacing problems and identifying existing and emergent risks.  \n\n> Page 10 NIST AI 100-1\n\nAI RMF 1.0  \n\n> Fig. 3.  AI actors across AI lifecycle stages. See Appendix A for detailed descriptions of AI actor tasks, including details about testing, evaluation, verification, and validation tasks. Note that AI actors in the AI Model dimension (Figure 2) are separated as a best practice, with those building and using the models separated from those verifying and validating the models.\n> Page 11 NIST AI 100-1\n\nAI RMF 1.0 \n\n# 3. AI Risks and Trustworthiness \n\nFor AI systems to be trustworthy, they often need to be responsive to a multiplicity of cri-teria that are of value to interested parties. Approaches which enhance AI trustworthiness can reduce negative AI risks. This Framework articulates the following characteristics of trustworthy AI and offers guidance for addressing them. Characteristics of trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and trans-parent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. Creating trustworthy AI requires balancing each of these characteristics based on the AI systemâ€™s context of use. While all characteristics are socio-technical system at-tributes, accountability and transparency also relate to the processes and activities internal to an AI system and its external setting. Neglecting these characteristics can increase the probability and magnitude of negative consequences.  \n\n> Fig. 4. Characteristics of trustworthy AI systems. Valid & Reliable is a necessary condition of trustworthiness and is shown as the base for other trustworthiness characteristics. Accountable & Transparent is shown as a vertical box because it relates to all other characteristics.\n\nTrustworthiness characteristics (shown in Figure 4) are inextricably tied to social and orga-nizational behavior, the datasets used by AI systems, selection of AI models and algorithms and the decisions made by those who build them, and the interactions with the humans who provide insight from and oversight of such systems. Human judgment should be employed when deciding on the specific metrics related to AI trustworthiness characteristics and the precise threshold values for those metrics. Addressing AI trustworthiness characteristics individually will not ensure AI system trust-worthiness; tradeoffs are usually involved, rarely do all characteristics apply in every set-ting, and some will be more or less important in any given situation. Ultimately, trustwor-thiness is a social concept that ranges across a spectrum and is only as strong as its weakest characteristics. When managing AI risks, organizations can face difficult decisions in balancing these char-acteristics. For example, in certain scenarios tradeoffs may emerge between optimizing for interpretability and achieving privacy. In other cases, organizations might face a tradeoff between predictive accuracy and interpretability. Or, under certain conditions such as data sparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions  \n\n> Page 12 NIST AI 100-1\n\nAI RMF 1.0 about fairness and other values in certain domains. Dealing with tradeoffs requires tak-ing into account the decision-making context. These analyses can highlight the existence and extent of tradeoffs between different measures, but they do not answer questions about how to navigate the tradeoff. Those depend on the values at play in the relevant context and should be resolved in a manner that is both transparent and appropriately justifiable. There are multiple approaches for enhancing contextual awareness in the AI lifecycle. For example, subject matter experts can assist in the evaluation of TEVV findings and work with product and deployment teams to align TEVV parameters to requirements and de-ployment conditions. When properly resourced, increasing the breadth and diversity of input from interested parties and relevant AI actors throughout the AI lifecycle can en-hance opportunities for informing contextually sensitive evaluations, and for identifying AI system benefits and positive impacts. These practices can increase the likelihood that risks arising in social contexts are managed appropriately. Understanding and treatment of trustworthiness characteristics depends on an AI actorâ€™s particular role within the AI lifecycle. For any given AI system, an AI designer or developer may have a different perception of the characteristics than the deployer. Trustworthiness characteristics explained in this document influence each other. Highly secure but unfair systems, accurate but opaque and uninterpretable systems, and inaccurate but secure, privacy-enhanced, and transparent systems are all unde-sirable. A comprehensive approach to risk management calls for balancing tradeoffs among the trustworthiness characteristics. It is the joint responsibility of all AI ac-tors to determine whether AI technology is an appropriate or necessary tool for a given context or purpose, and how to use it responsibly. The decision to commission or deploy an AI system should be based on a contextual assessment of trustworthi-ness characteristics and the relative risks, impacts, costs, and benefits, and informed by a broad set of interested parties. \n\n3.1 Valid and Reliable \n\nValidation is the â€œconfirmation, through the provision of objective evidence, that the re-quirements for a specific intended use or application have been fulfilledâ€ (Source: ISO \n\n9000:2015). Deployment of AI systems which are inaccurate, unreliable, or poorly gener-alized to data and settings beyond their training creates and increases negative AI risks and reduces trustworthiness. \n\nReliability is defined in the same standard as the â€œability of an item to perform as required, without failure, for a given time interval, under given conditionsâ€ (Source: ISO /IEC TS \n\n5723:2022). Reliability is a goal for overall correctness of AI system operation under the conditions of expected use and over a given period of time, including the entire lifetime of the system.  \n\n> Page 13 NIST AI 100-1\n\nAI RMF 1.0 Accuracy and robustness contribute to the validity and trustworthiness of AI systems, and can be in tension with one another in AI systems. \n\nAccuracy is defined by ISO /IEC TS 5723:2022 as â€œcloseness of results of observations, computations, or estimates to the true values or the values accepted as being true.â€ Mea-sures of accuracy should consider computational-centric measures (e.g., false positive and false negative rates), human-AI teaming, and demonstrate external validity (generalizable beyond the training conditions). Accuracy measurements should always be paired with clearly defined and realistic test sets â€“ that are representative of conditions of expected use â€“ and details about test methodology; these should be included in associated documen-tation. Accuracy measurements may include disaggregation of results for different data segments. \n\nRobustness or generalizability is defined as the â€œability of a system to maintain its level of performance under a variety of circumstancesâ€ (Source: ISO /IEC TS 5723:2022). Ro-bustness is a goal for appropriate system functionality in a broad set of conditions and circumstances, including uses of AI systems not initially anticipated. Robustness requires not only that the system perform exactly as it does under expected uses, but also that it should perform in ways that minimize potential harms to people if it is operating in an unexpected setting. Validity and reliability for deployed AI systems are often assessed by ongoing testing or monitoring that confirms a system is performing as intended. Measurement of validity, accuracy, robustness, and reliability contribute to trustworthiness and should take into con-sideration that certain types of failures can cause greater harm. AI risk management efforts should prioritize the minimization of potential negative impacts, and may need to include human intervention in cases where the AI system cannot detect or correct errors. \n\n3.2 Safe \n\nAI systems should â€œnot under defined conditions, lead to a state in which human life, health, property, or the environment is endangeredâ€ (Source: ISO /IEC TS 5723:2022). Safe operation of AI systems is improved through: â€¢ responsible design, development, and deployment practices; â€¢ clear information to deployers on responsible use of the system; â€¢ responsible decision-making by deployers and end users; and â€¢ explanations and documentation of risks based on empirical evidence of incidents. Different types of safety risks may require tailored AI risk management approaches based on context and the severity of potential risks presented. Safety risks that pose a potential risk of serious injury or death call for the most urgent prioritization and most thorough risk management process.  \n\n> Page 14 NIST AI 100-1\n\nAI RMF 1.0 Employing safety considerations during the lifecycle and starting as early as possible with planning and design can prevent failures or conditions that can render a system dangerous. Other practical approaches for AI safety often relate to rigorous simulation and in-domain testing, real-time monitoring, and the ability to shut down, modify, or have human inter-vention into systems that deviate from intended or expected functionality. AI safety risk management approaches should take cues from efforts and guidelines for safety in fields such as transportation and healthcare, and align with existing sector- or application-specific guidelines or standards. \n\n3.3 Secure and Resilient \n\nAI systems, as well as the ecosystems in which they are deployed, may be said to be re-silient if they can withstand unexpected adverse events or unexpected changes in their envi-ronment or use â€“ or if they can maintain their functions and structure in the face of internal and external change and degrade safely and gracefully when this is necessary (Adapted from: ISO /IEC TS 5723:2022). Common security concerns relate to adversarial examples, data poisoning, and the exfiltration of models, training data, or other intellectual property through AI system endpoints. AI systems that can maintain confidentiality, integrity, and availability through protection mechanisms that prevent unauthorized access and use may be said to be secure . Guidelines in the NIST Cybersecurity Framework and Risk Manage-ment Framework are among those which are applicable here. Security and resilience are related but distinct characteristics. While resilience is the abil-ity to return to normal function after an unexpected adverse event, security includes re-silience but also encompasses protocols to avoid, protect against, respond to, or recover from attacks. Resilience relates to robustness and goes beyond the provenance of the data to encompass unexpected or adversarial use (or abuse or misuse) of the model or data. \n\n3.4 Accountable and Transparent \n\nTrustworthy AI depends upon accountability. Accountability presupposes transparency. \n\nTransparency reflects the extent to which information about an AI system and its outputs is available to individuals interacting with such a system â€“ regardless of whether they are even aware that they are doing so. Meaningful transparency provides access to appropriate levels of information based on the stage of the AI lifecycle and tailored to the role or knowledge of AI actors or individuals interacting with or using the AI system. By promoting higher levels of understanding, transparency increases confidence in the AI system. This characteristicâ€™s scope spans from design decisions and training data to model train-ing, the structure of the model, its intended use cases, and how and when deployment, post-deployment, or end user decisions were made and by whom. Transparency is often necessary for actionable redress related to AI system outputs that are incorrect or otherwise lead to negative impacts. Transparency should consider human-AI interaction: for exam- \n\n> Page 15 NIST AI 100-1\n\nAI RMF 1.0 ple, how a human operator or user is notified when a potential or actual adverse outcome caused by an AI system is detected. A transparent system is not necessarily an accurate, privacy-enhanced, secure, or fair system. However, it is difficult to determine whether an opaque system possesses such characteristics, and to do so over time as complex systems evolve. The role of AI actors should be considered when seeking accountability for the outcomes of AI systems. The relationship between risk and accountability associated with AI and tech-nological systems more broadly differs across cultural, legal, sectoral, and societal contexts. When consequences are severe, such as when life and liberty are at stake, AI developers and deployers should consider proportionally and proactively adjusting their transparency and accountability practices. Maintaining organizational practices and governing structures for harm reduction, like risk management, can help lead to more accountable systems. Measures to enhance transparency and accountability should also consider the impact of these efforts on the implementing entity, including the level of necessary resources and the need to safeguard proprietary information. Maintaining the provenance of training data and supporting attribution of the AI systemâ€™s decisions to subsets of training data can assist with both transparency and accountability. Training data may also be subject to copyright and should follow applicable intellectual property rights laws. As transparency tools for AI systems and related documentation continue to evolve, devel-opers of AI systems are encouraged to test different types of transparency tools in cooper-ation with AI deployers to ensure that AI systems are used as intended. \n\n3.5 Explainable and Interpretable \n\nExplainability refers to a representation of the mechanisms underlying AI systemsâ€™ oper-ation, whereas interpretability refers to the meaning of AI systemsâ€™ output in the context of their designed functional purposes. Together, explainability and interpretability assist those operating or overseeing an AI system, as well as users of an AI system, to gain deeper insights into the functionality and trustworthiness of the system, including its out-puts. The underlying assumption is that perceptions of negative risk stem from a lack of ability to make sense of, or contextualize, system output appropriately. Explainable and interpretable AI systems offer information that will help end users understand the purposes and potential impact of an AI system. Risk from lack of explainability may be managed by describing how AI systems function, with descriptions tailored to individual differences such as the userâ€™s role, knowledge, and skill level. Explainable systems can be debugged and monitored more easily, and they lend themselves to more thorough documentation, audit, and governance.  \n\n> Page 16 NIST AI 100-1\n\nAI RMF 1.0 Risks to interpretability often can be addressed by communicating a description of why an AI system made a particular prediction or recommendation. (See â€œFour Principles of Explainable Artificial Intelligenceâ€ and â€œPsychological Foundations of Explainability and Interpretability in Artificial Intelligenceâ€ found here.) Transparency, explainability, and interpretability are distinct characteristics that support each other. Transparency can answer the question of â€œwhat happenedâ€ in the system. Ex-plainability can answer the question of â€œhowâ€ a decision was made in the system. Inter-pretability can answer the question of â€œwhyâ€ a decision was made by the system and its meaning or context to the user. \n\n3.6 Privacy-Enhanced \n\nPrivacy refers generally to the norms and practices that help to safeguard human autonomy, identity, and dignity. These norms and practices typically address freedom from intrusion, limiting observation, or individualsâ€™ agency to consent to disclosure or control of facets of their identities (e.g., body, data, reputation). (See The NIST Privacy Framework: A Tool for Improving Privacy through Enterprise Risk Management.) Privacy values such as anonymity, confidentiality, and control generally should guide choices for AI system design, development, and deployment. Privacy-related risks may influence security, bias, and transparency and come with tradeoffs with these other characteristics. Like safety and security, specific technical features of an AI system may promote or reduce privacy. AI systems can also present new risks to privacy by allowing inference to identify individuals or previously private information about individuals. Privacy-enhancing technologies (â€œPETsâ€) for AI, as well as data minimizing methods such as de-identification and aggregation for certain model outputs, can support design for privacy-enhanced AI systems. Under certain conditions such as data sparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions about fairness and other values in certain domains. \n\n3.7 Fair â€“ with Harmful Bias Managed \n\nFairness in AI includes concerns for equality and equity by addressing issues such as harm-ful bias and discrimination. Standards of fairness can be complex and difficult to define be-cause perceptions of fairness differ among cultures and may shift depending on application. Organizationsâ€™ risk management efforts will be enhanced by recognizing and considering these differences. Systems in which harmful biases are mitigated are not necessarily fair. For example, systems in which predictions are somewhat balanced across demographic groups may still be inaccessible to individuals with disabilities or affected by the digital divide or may exacerbate existing disparities or systemic biases.  \n\n> Page 17 NIST AI 100-1\n\nAI RMF 1.0 Bias is broader than demographic balance and data representativeness. NIST has identified three major categories of AI bias to be considered and managed: systemic, computational and statistical, and human-cognitive. Each of these can occur in the absence of prejudice, partiality, or discriminatory intent. Systemic bias can be present in AI datasets, the orga-nizational norms, practices, and processes across the AI lifecycle, and the broader society that uses AI systems. Computational and statistical biases can be present in AI datasets and algorithmic processes, and often stem from systematic errors due to non-representative samples. Human-cognitive biases relate to how an individual or group perceives AI sys-tem information to make a decision or fill in missing information, or how humans think about purposes and functions of an AI system. Human-cognitive biases are omnipresent in decision-making processes across the AI lifecycle and system use, including the design, implementation, operation, and maintenance of AI. Bias exists in many forms and can become ingrained in the automated systems that help make decisions about our lives. While bias is not always a negative phenomenon, AI sys-tems can potentially increase the speed and scale of biases and perpetuate and amplify harms to individuals, groups, communities, organizations, and society. Bias is tightly asso-ciated with the concepts of transparency as well as fairness in society. (For more informa-tion about bias, including the three categories, see NIST Special Publication 1270, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.)  \n\n> Page 18 NIST AI 100-1\n\nAI RMF 1.0 \n\n# 4. Effectiveness of the AI RMF \n\nEvaluations of AI RMF effectiveness â€“ including ways to measure bottom-line improve-ments in the trustworthiness of AI systems â€“ will be part of future NIST activities, in conjunction with the AI community. Organizations and other users of the Framework are encouraged to periodically evaluate whether the AI RMF has improved their ability to manage AI risks, including but not lim-ited to their policies, processes, practices, implementation plans, indicators, measurements, and expected outcomes. NIST intends to work collaboratively with others to develop met-rics, methodologies, and goals for evaluating the AI RMFâ€™s effectiveness, and to broadly share results and supporting information. Framework users are expected to benefit from: â€¢ enhanced processes for governing, mapping, measuring, and managing AI risk, and clearly documenting outcomes; â€¢ improved awareness of the relationships and tradeoffs among trustworthiness char-acteristics, socio-technical approaches, and AI risks; â€¢ explicit processes for making go/no-go system commissioning and deployment deci-sions; â€¢ established policies, processes, practices, and procedures for improving organiza-tional accountability efforts related to AI system risks; â€¢ enhanced organizational culture which prioritizes the identification and management of AI system risks and potential impacts to individuals, communities, organizations, and society; â€¢ better information sharing within and across organizations about risks, decision-making processes, responsibilities, common pitfalls, TEVV practices, and approaches for continuous improvement; â€¢ greater contextual knowledge for increased awareness of downstream risks; â€¢ strengthened engagement with interested parties and relevant AI actors; and â€¢ augmented capacity for TEVV of AI systems and associated risks.  \n\n> Page 19 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Part 2: Core and Profiles \n\n# 5. AI RMF Core \n\nThe AI RMF Core provides outcomes and actions that enable dialogue, understanding, and activities to manage AI risks and responsibly develop trustworthy AI systems. As illus-trated in Figure 5, the Core is composed of four functions: GOVERN , MAP , MEASURE ,\n\nand MANAGE . Each of these high-level functions is broken down into categories and sub-categories. Categories and subcategories are subdivided into specific actions and outcomes. Actions do not constitute a checklist, nor are they necessarily an ordered set of steps. \n\nFig. 5. Functions organize AI risk management activities at their highest level to govern, map, measure, and manage AI risks. Governance is designed to be a cross-cutting function to inform and be infused throughout the other three functions. \n\nRisk management should be continuous, timely, and performed throughout the AI system lifecycle dimensions. AI RMF Core functions should be carried out in a way that reflects diverse and multidisciplinary perspectives, potentially including the views of AI actors out-side the organization. Having a diverse team contributes to more open sharing of ideas and assumptions about purposes and functions of the technology being designed, developed,  \n\n> Page 20 NIST AI 100-1\n\nAI RMF 1.0 deployed, or evaluated â€“ which can create opportunities to surface problems and identify existing and emergent risks. An online companion resource to the AI RMF, the NIST AI RMF Playbook, is available to help organizations navigate the AI RMF and achieve its outcomes through suggested tactical actions they can apply within their own contexts. Like the AI RMF, the Playbook is voluntary and organizations can utilize the suggestions according to their needs and interests. Playbook users can create tailored guidance selected from suggested material for their own use and contribute their suggestions for sharing with the broader community. Along with the AI RMF, the Playbook is part of the NIST Trustworthy and Responsible AI Resource Center. Framework users may apply these functions as best suits their needs for managing AI risks based on their resources and capabilities. Some organizations may choose to select from among the categories and subcategories; others may choose and have the capacity to apply all categories and subcategories. Assuming a governance struc-ture is in place, functions may be performed in any order across the AI lifecycle as deemed to add value by a user of the framework. After instituting the outcomes in \n\n> GOVERN\n\n, most users of the AI RMF would start with the MAP function and con-tinue to MEASURE or MANAGE . However users integrate the functions, the process should be iterative, with cross-referencing between functions as necessary. Simi-larly, there are categories and subcategories with elements that apply to multiple functions, or that logically should take place before certain subcategory decisions. \n\n5.1 Govern \n\nThe GOVERN function: â€¢ cultivates and implements a culture of risk management within organizations design-ing, developing, deploying, evaluating, or acquiring AI systems; â€¢ outlines processes, documents, and organizational schemes that anticipate, identify, and manage the risks a system can pose, including to users and others across society â€“ and procedures to achieve those outcomes; â€¢ incorporates processes to assess potential impacts; â€¢ provides a structure by which AI risk management functions can align with organi-zational principles, policies, and strategic priorities; â€¢ connects technical aspects of AI system design and development to organizational values and principles, and enables organizational practices and competencies for the individuals involved in acquiring, training, deploying, and monitoring such systems; and â€¢ addresses full product lifecycle and associated processes, including legal and other issues concerning use of third-party software or hardware systems and data.  \n\n> Page 21 NIST AI 100-1\n\nAI RMF 1.0  \n\n> GOVERN\n\nis a cross-cutting function that is infused throughout AI risk management and enables the other functions of the process. Aspects of GOVERN , especially those related to compliance or evaluation, should be integrated into each of the other functions. Attention to governance is a continual and intrinsic requirement for effective AI risk management over an AI systemâ€™s lifespan and the organizationâ€™s hierarchy. Strong governance can drive and enhance internal practices and norms to facilitate orga-nizational risk culture. Governing authorities can determine the overarching policies that direct an organizationâ€™s mission, goals, values, culture, and risk tolerance. Senior leader-ship sets the tone for risk management within an organization, and with it, organizational culture. Management aligns the technical aspects of AI risk management to policies and operations. Documentation can enhance transparency, improve human review processes, and bolster accountability in AI system teams. After putting in place the structures, systems, processes, and teams described in the GOV - \n\n> ERN\n\nfunction, organizations should benefit from a purpose-driven culture focused on risk understanding and management. It is incumbent on Framework users to continue to ex-ecute the GOVERN function as knowledge, cultures, and needs or expectations from AI actors evolve over time. Practices related to governing AI risks are described in the NIST AI RMF Playbook. Table 1 lists the GOVERN functionâ€™s categories and subcategories. Table 1: Categories and subcategories for the GOVERN function.  \n\n> GOVERN\n\n1: \n\nPolicies, processes, procedures, and practices across the organization related to the mapping, measuring, and managing of AI risks are in place, transparent, and implemented effectively.  \n\n> GOVERN\n\n1.1: Legal and regulatory requirements involving AI are understood, managed, and documented.  \n\n> GOVERN\n\n1.2: The characteristics of trustworthy AI are inte-grated into organizational policies, processes, procedures, and practices.  \n\n> GOVERN\n\n1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based on the organizationâ€™s risk tolerance.  \n\n> GOVERN\n\n1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 22 NIST AI 100-1\n\nAI RMF 1.0 Table 1: Categories and subcategories for the GOVERN function. (Continued)  \n\n> GOVERN\n\n1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned and or-ganizational roles and responsibilities clearly defined, including determining the frequency of periodic review.  \n\n> GOVERN\n\n1.6: Mechanisms are in place to inventory AI systems and are resourced according to organizational risk priorities.  \n\n> GOVERN\n\n1.7: Processes and procedures are in place for decom-missioning and phasing out AI systems safely and in a man-ner that does not increase risks or decrease the organizationâ€™s trustworthiness.  \n\n> GOVERN\n\n2: \n\nAccountability structures are in place so that the appropriate teams and individuals are empowered, responsible, and trained for mapping, measuring, and managing AI risks.  \n\n> GOVERN\n\n2.1: Roles and responsibilities and lines of communi-cation related to mapping, measuring, and managing AI risks are documented and are clear to individuals and teams throughout the organization.  \n\n> GOVERN\n\n2.2: The organizationâ€™s personnel and partners receive AI risk management training to enable them to perform their du-ties and responsibilities consistent with related policies, proce-dures, and agreements.  \n\n> GOVERN\n\n2.3: Executive leadership of the organization takes re-sponsibility for decisions about risks associated with AI system development and deployment.  \n\n> GOVERN\n\n3: \n\nWorkforce diversity, equity, inclusion, and accessibility processes are prioritized in the mapping, measuring, and managing of AI risks throughout the lifecycle.  \n\n> GOVERN\n\n3.1: Decision-making related to mapping, measuring, and managing AI risks throughout the lifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, expe-rience, expertise, and backgrounds).  \n\n> GOVERN\n\n3.2: Policies and procedures are in place to define and differentiate roles and responsibilities for human-AI configura-tions and oversight of AI systems.  \n\n> GOVERN\n\n4: \n\nOrganizational teams are committed to a culture  \n\n> GOVERN\n\n4.1: Organizational policies and practices are in place to foster a critical thinking and safety-first mindset in the design, development, deployment, and uses of AI systems to minimize potential negative impacts. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 23 NIST AI 100-1\n\nAI RMF 1.0 Table 1: Categories and subcategories for the GOVERN function. (Continued) that considers and communicates AI risk.  \n\n> GOVERN\n\n4.2: Organizational teams document the risks and po-tential impacts of the AI technology they design, develop, deploy, evaluate, and use, and they communicate about the impacts more broadly.  \n\n> GOVERN\n\n4.3: Organizational practices are in place to enable AI testing, identification of incidents, and information sharing.  \n\n> GOVERN\n\n5: \n\nProcesses are in place for robust engagement with relevant AI actors.  \n\n> GOVERN\n\n5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks.  \n\n> GOVERN\n\n5.2: Mechanisms are established to enable the team that developed or deployed AI systems to regularly incorporate adjudicated feedback from relevant AI actors into system design and implementation.  \n\n> GOVERN\n\n6: Policies and procedures are in place to address AI risks and benefits arising from third-party software and data and other supply chain issues.  \n\n> GOVERN\n\n6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of in-fringement of a third-partyâ€™s intellectual property or other rights.  \n\n> GOVERN\n\n6.2: Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be high-risk. \n\nCategories Subcategories 5.2 Map \n\nThe MAP function establishes the context to frame risks related to an AI system. The AI lifecycle consists of many interdependent activities involving a diverse set of actors (See Figure 3). In practice, AI actors in charge of one part of the process often do not have full visibility or control over other parts and their associated contexts. The interdependencies between these activities, and among the relevant AI actors, can make it difficult to reliably anticipate impacts of AI systems. For example, early decisions in identifying purposes and objectives of an AI system can alter its behavior and capabilities, and the dynamics of de-ployment setting (such as end users or impacted individuals) can shape the impacts of AI system decisions. As a result, the best intentions within one dimension of the AI lifecycle can be undermined via interactions with decisions and conditions in other, later activities.  \n\n> Page 24 NIST AI 100-1\n\nAI RMF 1.0 This complexity and varying levels of visibility can introduce uncertainty into risk man-agement practices. Anticipating, assessing, and otherwise addressing potential sources of negative risk can mitigate this uncertainty and enhance the integrity of the decision process. The information gathered while carrying out the MAP function enables negative risk pre-vention and informs decisions for processes such as model management, as well as an initial decision about appropriateness or the need for an AI solution. Outcomes in the  \n\n> MAP\n\nfunction are the basis for the MEASURE and MANAGE functions. Without contex-tual knowledge, and awareness of risks within the identified contexts, risk management is difficult to perform. The MAP function is intended to enhance an organizationâ€™s ability to identify risks and broader contributing factors. Implementation of this function is enhanced by incorporating perspectives from a diverse internal team and engagement with those external to the team that developed or deployed the AI system. Engagement with external collaborators, end users, potentially impacted communities, and others may vary based on the risk level of a particular AI system, the makeup of the internal team, and organizational policies. Gathering such broad perspec-tives can help organizations proactively prevent negative risks and develop more trustwor-thy AI systems by: â€¢ improving their capacity for understanding contexts; â€¢ checking their assumptions about context of use; â€¢ enabling recognition of when systems are not functional within or out of their in-tended context; â€¢ identifying positive and beneficial uses of their existing AI systems; â€¢ improving understanding of limitations in AI and ML processes; â€¢ identifying constraints in real-world applications that may lead to negative impacts; â€¢ identifying known and foreseeable negative impacts related to intended use of AI systems; and â€¢ anticipating risks of the use of AI systems beyond intended use. After completing the MAP function, Framework users should have sufficient contextual knowledge about AI system impacts to inform an initial go/no-go decision about whether to design, develop, or deploy an AI system. If a decision is made to proceed, organizations should utilize the MEASURE and MANAGE functions along with policies and procedures put into place in the GOVERN function to assist in AI risk management efforts. It is incum-bent on Framework users to continue applying the MAP function to AI systems as context, capabilities, risks, benefits, and potential impacts evolve over time. Practices related to mapping AI risks are described in the NIST AI RMF Playbook. Table 2 lists the MAP functionâ€™s categories and subcategories.  \n\n> Page 25 NIST AI 100-1\n\nAI RMF 1.0 Table 2: Categories and subcategories for the MAP function.  \n\n> MAP\n\n1: Context is established and understood.  \n\n> MAP\n\n1.1: Intended purposes, potentially beneficial uses, context-specific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and docu-mented. Considerations include: the specific set or types of users along with their expectations; potential positive and negative im-pacts of system uses to individuals, communities, organizations, society, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or product AI lifecycle; and related TEVV and system metrics.  \n\n> MAP\n\n1.2: Interdisciplinary AI actors, competencies, skills, and capacities for establishing context reflect demographic diversity and broad domain and user experience expertise, and their par-ticipation is documented. Opportunities for interdisciplinary col-laboration are prioritized.  \n\n> MAP\n\n1.3: The organizationâ€™s mission and relevant goals for AI technology are understood and documented.  \n\n> MAP\n\n1.4: The business value or context of business use has been clearly defined or â€“ in the case of assessing existing AI systems â€“ re-evaluated.  \n\n> MAP\n\n1.5: Organizational risk tolerances are determined and documented.  \n\n> MAP\n\n1.6: System requirements (e.g., â€œthe system shall respect the privacy of its usersâ€) are elicited from and understood by rel-evant AI actors. Design decisions take socio-technical implica-tions into account to address AI risks.  \n\n> MAP\n\n2: \n\nCategorization of the AI system is performed.  \n\n> MAP\n\n2.1: The specific tasks and methods used to implement the tasks that the AI system will support are defined (e.g., classifiers, generative models, recommenders).  \n\n> MAP\n\n2.2: Information about the AI systemâ€™s knowledge limits and how system output may be utilized and overseen by humans is documented. Documentation provides sufficient information to assist relevant AI actors when making decisions and taking subsequent actions. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 26 NIST AI 100-1\n\nAI RMF 1.0 Table 2: Categories and subcategories for the MAP function. (Continued)  \n\n> MAP\n\n2.3: Scientific integrity and TEVV considerations are iden-tified and documented, including those related to experimental design, data collection and selection (e.g., availability, repre-sentativeness, suitability), system trustworthiness, and construct validation.  \n\n> MAP\n\n3: AI capabilities, targeted usage, goals, and expected benefits and costs compared with appropriate benchmarks are understood.  \n\n> MAP\n\n3.1: Potential benefits of intended AI system functionality and performance are examined and documented.  \n\n> MAP\n\n3.2: Potential costs, including non-monetary costs, which result from expected or realized AI errors or system functionality and trustworthiness â€“ as connected to organizational risk toler-ance â€“ are examined and documented.  \n\n> MAP\n\n3.3: Targeted application scope is specified and docu-mented based on the systemâ€™s capability, established context, and AI system categorization.  \n\n> MAP\n\n3.4: Processes for operator and practitioner proficiency with AI system performance and trustworthiness â€“ and relevant technical standards and certifications â€“ are defined, assessed, and documented.  \n\n> MAP\n\n3.5: Processes for human oversight are defined, assessed, and documented in accordance with organizational policies from the GOVERN function.  \n\n> MAP\n\n4: Risks and benefits are mapped for all components of the AI system including third-party software and data.  \n\n> MAP\n\n4.1: Approaches for mapping AI technology and legal risks of its components â€“ including the use of third-party data or soft-ware â€“ are in place, followed, and documented, as are risks of in-fringement of a third partyâ€™s intellectual property or other rights.  \n\n> MAP\n\n4.2: Internal risk controls for components of the AI sys-tem, including third-party AI technologies, are identified and documented.  \n\n> MAP\n\n5: Impacts to individuals, groups, communities, organizations, and society are characterized.  \n\n> MAP\n\n5.1: Likelihood and magnitude of each identified impact (both potentially beneficial and harmful) based on expected use, past uses of AI systems in similar contexts, public incident re-ports, feedback from those external to the team that developed or deployed the AI system, or other data are identified and documented. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 27 NIST AI 100-1\n\nAI RMF 1.0 Table 2: Categories and subcategories for the MAP function. (Continued)  \n\n> MAP\n\n5.2: Practices and personnel for supporting regular en-gagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented. \n\nCategories Subcategories 5.3 Measure \n\nThe MEASURE function employs quantitative, qualitative, or mixed-method tools, tech-niques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related impacts. It uses knowledge relevant to AI risks identified in the MAP function and informs the MANAGE function. AI systems should be tested before their deployment and regu-larly while in operation. AI risk measurements include documenting aspects of systemsâ€™ functionality and trustworthiness. Measuring AI risks includes tracking metrics for trustworthy characteristics, social impact, and human-AI configurations. Processes developed or adopted in the MEASURE function should include rigorous software testing and performance assessment methodologies with associated measures of uncertainty, comparisons to performance benchmarks, and formal-ized reporting and documentation of results. Processes for independent review can improve the effectiveness of testing and can mitigate internal biases and potential conflicts of inter-est. Where tradeoffs among the trustworthy characteristics arise, measurement provides a trace-able basis to inform management decisions. Options may include recalibration, impact mitigation, or removal of the system from design, development, production, or use, as well as a range of compensating, detective, deterrent, directive, and recovery controls. After completing the MEASURE function, objective, repeatable, or scalable test, evaluation, verification, and validation (TEVV) processes including metrics, methods, and methodolo-gies are in place, followed, and documented. Metrics and measurement methodologies should adhere to scientific, legal, and ethical norms and be carried out in an open and trans-parent process. New types of measurement, qualitative and quantitative, may need to be developed. The degree to which each measurement type provides unique and meaningful information to the assessment of AI risks should be considered. Framework users will en-hance their capacity to comprehensively evaluate system trustworthiness, identify and track existing and emergent risks, and verify efficacy of the metrics. Measurement outcomes will be utilized in the MANAGE function to assist risk monitoring and response efforts. It is in-cumbent on Framework users to continue applying the MEASURE function to AI systems as knowledge, methodologies, risks, and impacts evolve over time.  \n\n> Page 28 NIST AI 100-1\n\nAI RMF 1.0 Practices related to measuring AI risks are described in the NIST AI RMF Playbook. Table 3 lists the MEASURE functionâ€™s categories and subcategories. Table 3: Categories and subcategories for the MEASURE function.  \n\n> MEASURE\n\n1: \n\nAppropriate methods and metrics are identified and applied.  \n\n> MEASURE\n\n1.1: Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for imple-mentation starting with the most significant AI risks. The risks or trustworthiness characteristics that will not â€“ or cannot â€“ be measured are properly documented.  \n\n> MEASURE\n\n1.2: Appropriateness of AI metrics and effectiveness of existing controls are regularly assessed and updated, including reports of errors and potential impacts on affected communities.  \n\n> MEASURE\n\n1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are in-volved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance.  \n\n> MEASURE\n\n2: AI systems are evaluated for trustworthy characteristics.  \n\n> MEASURE\n\n2.1: Test sets, metrics, and details about the tools used during TEVV are documented.  \n\n> MEASURE\n\n2.2: Evaluations involving human subjects meet ap-plicable requirements (including human subject protection) and are representative of the relevant population.  \n\n> MEASURE\n\n2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented.  \n\n> MEASURE\n\n2.4: The functionality and behavior of the AI sys-tem and its components â€“ as identified in the MAP function â€“ are monitored when in production.  \n\n> MEASURE\n\n2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability be-yond the conditions under which the technology was developed are documented. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 29 NIST AI 100-1\n\nAI RMF 1.0 Table 3: Categories and subcategories for the MEASURE function. (Continued)  \n\n> MEASURE\n\n2.6: The AI system is evaluated regularly for safety risks â€“ as identified in the MAP function. The AI system to be de-ployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics re-flect system reliability and robustness, real-time monitoring, and response times for AI system failures.  \n\n> MEASURE\n\n2.7: AI system security and resilience â€“ as identified in the MAP function â€“ are evaluated and documented.  \n\n> MEASURE\n\n2.8: Risks associated with transparency and account-ability â€“ as identified in the MAP function â€“ are examined and documented.  \n\n> MEASURE\n\n2.9: The AI model is explained, validated, and docu-mented, and AI system output is interpreted within its context â€“ as identified in the MAP function â€“ to inform responsible use and governance.  \n\n> MEASURE\n\n2.10: Privacy risk of the AI system â€“ as identified in the MAP function â€“ is examined and documented.  \n\n> MEASURE\n\n2.11: Fairness and bias â€“ as identified in the MAP \n\nfunction â€“ are evaluated and results are documented.  \n\n> MEASURE\n\n2.12: Environmental impact and sustainability of AI model training and management activities â€“ as identified in the  \n\n> MAP\n\nfunction â€“ are assessed and documented.  \n\n> MEASURE\n\n2.13: Effectiveness of the employed TEVV met-rics and processes in the MEASURE function are evaluated and documented.  \n\n> MEASURE\n\n3: \n\nMechanisms for tracking identified AI risks over time are in place.  \n\n> MEASURE\n\n3.1: Approaches, personnel, and documentation are in place to regularly identify and track existing, unanticipated, and emergent AI risks based on factors such as intended and ac-tual performance in deployed contexts.  \n\n> MEASURE\n\n3.2: Risk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 30 NIST AI 100-1\n\nAI RMF 1.0 Table 3: Categories and subcategories for the MEASURE function. (Continued)  \n\n> MEASURE\n\n3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into AI system evaluation metrics.  \n\n> MEASURE\n\n4: \n\nFeedback about efficacy of measurement is gathered and assessed.  \n\n> MEASURE\n\n4.1: Measurement approaches for identifying AI risks are connected to deployment context(s) and informed through consultation with domain experts and other end users. Ap-proaches are documented.  \n\n> MEASURE\n\n4.2: Measurement results regarding AI system trust-worthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI ac-tors to validate whether the system is performing consistently as intended. Results are documented.  \n\n> MEASURE\n\n4.3: Measurable performance improvements or de-clines based on consultations with relevant AI actors, in-cluding affected communities, and field data about context-relevant risks and trustworthiness characteristics are identified and documented. \n\nCategories Subcategories 5.4 Manage \n\nThe MANAGE function entails allocating risk resources to mapped and measured risks on a regular basis and as defined by the GOVERN function. Risk treatment comprises plans to respond to, recover from, and communicate about incidents or events. Contextual information gleaned from expert consultation and input from relevant AI actors â€“ established in GOVERN and carried out in MAP â€“ is utilized in this function to decrease the likelihood of system failures and negative impacts. Systematic documentation practices established in GOVERN and utilized in MAP and MEASURE bolster AI risk management efforts and increase transparency and accountability. Processes for assessing emergent risks are in place, along with mechanisms for continual improvement. After completing the MANAGE function, plans for prioritizing risk and regular monitoring and improvement will be in place. Framework users will have enhanced capacity to man-age the risks of deployed AI systems and to allocate risk management resources based on assessed and prioritized risks. It is incumbent on Framework users to continue to apply the MANAGE function to deployed AI systems as methods, contexts, risks, and needs or expectations from relevant AI actors evolve over time.  \n\n> Page 31 NIST AI 100-1\n\nAI RMF 1.0 Practices related to managing AI risks are described in the NIST AI RMF Playbook. Table 4 lists the MANAGE functionâ€™s categories and subcategories. Table 4: Categories and subcategories for the MANAGE function.  \n\n> MANAGE\n\n1: AI risks based on assessments and other analytical output from the  \n\n> MAP\n\nand MEASURE \n\nfunctions are prioritized, responded to, and managed.  \n\n> MANAGE\n\n1.1: A determination is made as to whether the AI system achieves its intended purposes and stated objectives and whether its development or deployment should proceed.  \n\n> MANAGE\n\n1.2: Treatment of documented AI risks is prioritized based on impact, likelihood, and available resources or methods.  \n\n> MANAGE\n\n1.3: Responses to the AI risks deemed high priority, as identified by the MAP function, are developed, planned, and doc-umented. Risk response options can include mitigating, transfer-ring, avoiding, or accepting.  \n\n> MANAGE\n\n1.4: Negative residual risks (defined as the sum of all unmitigated risks) to both downstream acquirers of AI systems and end users are documented.  \n\n> MANAGE\n\n2: \n\nStrategies to maximize AI benefits and minimize negative impacts are planned, prepared, implemented, documented, and informed by input from relevant AI actors.  \n\n> MANAGE\n\n2.1: Resources required to manage AI risks are taken into account â€“ along with viable non-AI alternative systems, ap-proaches, or methods â€“ to reduce the magnitude or likelihood of potential impacts.  \n\n> MANAGE\n\n2.2: Mechanisms are in place and applied to sustain the value of deployed AI systems.  \n\n> MANAGE\n\n2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identified.  \n\n> MANAGE\n\n2.4: Mechanisms are in place and applied, and respon-sibilities are assigned and understood, to supersede, disengage, or deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use.  \n\n> MANAGE\n\n3: AI risks and benefits from third-party entities are managed.  \n\n> MANAGE\n\n3.1: AI risks and benefits from third-party resources are regularly monitored, and risk controls are applied and documented.  \n\n> MANAGE\n\n3.2: Pre-trained models which are used for develop-ment are monitored as part of AI system regular monitoring and maintenance. \n\nCategories Subcategories \n\nContinued on next page  \n\n> Page 32 NIST AI 100-1\n\nAI RMF 1.0 Table 4: Categories and subcategories for the MANAGE function. (Continued)  \n\n> MANAGE\n\n4: Risk treatments, including response and recovery, and communication plans for the identified and measured AI risks are documented and monitored regularly.  \n\n> MANAGE\n\n4.1: Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and eval-uating input from users and other relevant AI actors, appeal and override, decommissioning, incident response, recovery, and change management.  \n\n> MANAGE\n\n4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular engage-ment with interested parties, including relevant AI actors.  \n\n> MANAGE\n\n4.3: Incidents and errors are communicated to relevant AI actors, including affected communities. Processes for track-ing, responding to, and recovering from incidents and errors are followed and documented. \n\nCategories Subcategories \n\n# 6. AI RMF Profiles \n\nAI RMF use-case profiles are implementations of the AI RMF functions, categories, and subcategories for a specific setting or application based on the requirements, risk tolerance, and resources of the Framework user: for example, an AI RMF hiring profile or an AI RMF fair housing profile . Profiles may illustrate and offer insights into how risk can be managed at various stages of the AI lifecycle or in specific sector, technology, or end-use applications. AI RMF profiles assist organizations in deciding how they might best manage AI risk that is well-aligned with their goals, considers legal/regulatory requirements and best practices, and reflects risk management priorities. AI RMF temporal profiles are descriptions of either the current state or the desired, target state of specific AI risk management activities within a given sector, industry, organization, or application context. An AI RMF Current Profile indicates how AI is currently being managed and the related risks in terms of current outcomes. A Target Profile indicates the outcomes needed to achieve the desired or target AI risk management goals. Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk management objectives. Action plans can be developed to address these gaps to fulfill outcomes in a given category or subcategory. Prioritization of gap mitigation is driven by the userâ€™s needs and risk management processes. This risk-based approach also enables Framework users to compare their approaches with other approaches and to gauge the resources needed (e.g., staffing, funding) to achieve AI risk management goals in a cost-effective, prioritized manner.  \n\n> Page 33 NIST AI 100-1\n\nAI RMF 1.0 AI RMF cross-sectoral profiles cover risks of models or applications that can be used across use cases or sectors. Cross-sectoral profiles can also cover how to govern, map, measure, and manage risks for activities or business processes common across sectors such as the use of large language models, cloud-based services or acquisition. This Framework does not prescribe profile templates, allowing for flexibility in implemen-tation.  \n\n> Page 34 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3 \n\nAI Design tasks are performed during the Application Context and Data and Input phases of the AI lifecycle in Figure 2. AI Design actors create the concept and objectives of AI systems and are responsible for the planning, design, and data collection and processing tasks of the AI system so that the AI system is lawful and fit-for-purpose. Tasks include ar-ticulating and documenting the systemâ€™s concept and objectives, underlying assumptions, context, and requirements; gathering and cleaning data; and documenting the metadata and characteristics of the dataset. AI actors in this category include data scientists, do-main experts, socio-cultural analysts, experts in the field of diversity, equity, inclusion, and accessibility, members of impacted communities, human factors experts (e.g., UX/UI design), governance experts, data engineers, data providers, system funders, product man-agers, third-party entities, evaluators, and legal and privacy governance. \n\nAI Development tasks are performed during the AI Model phase of the lifecycle in Figure 2. AI Development actors provide the initial infrastructure of AI systems and are responsi-ble for model building and interpretation tasks, which involve the creation, selection, cali-bration, training, and/or testing of models or algorithms. AI actors in this category include machine learning experts, data scientists, developers, third-party entities, legal and privacy governance experts, and experts in the socio-cultural and contextual factors associated with the deployment setting. \n\nAI Deployment tasks are performed during the Task and Output phase of the lifecycle in Figure 2. AI Deployment actors are responsible for contextual decisions relating to how the AI system is used to assure deployment of the system into production. Related tasks include piloting the system, checking compatibility with legacy systems, ensuring regu-latory compliance, managing organizational change, and evaluating user experience. AI actors in this category include system integrators, software developers, end users, oper-ators and practitioners, evaluators, and domain experts with expertise in human factors, socio-cultural analysis, and governance. \n\nOperation and Monitoring tasks are performed in the Application Context/Operate and Monitor phase of the lifecycle in Figure 2. These tasks are carried out by AI actors who are responsible for operating the AI system and working with others to regularly assess system output and impacts. AI actors in this category include system operators, domain experts, AI designers, users who interpret or incorporate the output of AI systems, product developers, evaluators and auditors, compliance experts, organizational management, and members of the research community. \n\nTest, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout the AI lifecycle. They are carried out by AI actors who examine the AI system or its components, or detect and remediate problems. Ideally, AI actors carrying out verification  \n\n> Page 35 NIST AI 100-1\n\nAI RMF 1.0 and validation tasks are distinct from those who perform test and evaluation actions. Tasks can be incorporated into a phase as early as design, where tests are planned in accordance with the design requirement. â€¢ TEVV tasks for design, planning, and data may center on internal and external vali-dation of assumptions for system design, data collection, and measurements relative to the intended context of deployment or application. â€¢ TEVV tasks for development (i.e., model building) include model validation and assessment. â€¢ TEVV tasks for deployment include system validation and integration in production, with testing, and recalibration for systems and process integration, user experience, and compliance with existing legal, regulatory, and ethical specifications. â€¢ TEVV tasks for operations involve ongoing monitoring for periodic updates, testing, and subject matter expert (SME) recalibration of models, the tracking of incidents or errors reported and their management, the detection of emergent properties and related impacts, and processes for redress and response. \n\nHuman Factors tasks and activities are found throughout the dimensions of the AI life-cycle. They include human-centered design practices and methodologies, promoting the active involvement of end users and other interested parties and relevant AI actors, incor-porating context-specific norms and values in system design, evaluating and adapting end user experiences, and broad integration of humans and human dynamics in all phases of the AI lifecycle. Human factors professionals provide multidisciplinary skills and perspectives to understand context of use, inform interdisciplinary and demographic diversity, engage in consultative processes, design and evaluate user experience, perform human-centered evaluation and testing, and inform impact assessments. \n\nDomain Expert tasks involve input from multidisciplinary practitioners or scholars who provide knowledge or expertise in â€“ and about â€“ an industry sector, economic sector, con-text, or application area where an AI system is being used. AI actors who are domain experts can provide essential guidance for AI system design and development, and inter-pret outputs in support of work performed by TEVV and AI impact assessment teams. \n\nAI Impact Assessment tasks include assessing and evaluating requirements for AI system accountability, combating harmful bias, examining impacts of AI systems, product safety, liability, and security, among others. AI actors such as impact assessors and evaluators provide technical, human factor, socio-cultural, and legal expertise. \n\nProcurement tasks are conducted by AI actors with financial, legal, or policy management authority for acquisition of AI models, products, or services from a third-party developer, vendor, or contractor. \n\nGovernance and Oversight tasks are assumed by AI actors with management, fiduciary, and legal authority and responsibility for the organization in which an AI system is de- \n\n> Page 36 NIST AI 100-1\n\nAI RMF 1.0 signed, developed, and/or deployed. Key AI actors responsible for AI governance include organizational management, senior leadership, and the Board of Directors. These actors are parties that are concerned with the impact and sustainability of the organization as a whole. \n\nAdditional AI Actors Third-party entities include providers, developers, vendors, and evaluators of data, al-gorithms, models, and/or systems and related services for another organization or the or-ganizationâ€™s customers or clients. Third-party entities are responsible for AI design and development tasks, in whole or in part. By definition, they are external to the design, devel-opment, or deployment team of the organization that acquires its technologies or services. The technologies acquired from third-party entities may be complex or opaque, and risk tolerances may not align with the deploying or operating organization. \n\nEnd users of an AI system are the individuals or groups that use the system for specific purposes. These individuals or groups interact with an AI system in a specific context. End users can range in competency from AI experts to first-time technology end users. \n\nAffected individuals/communities encompass all individuals, groups, communities, or organizations directly or indirectly affected by AI systems or decisions based on the output of AI systems. These individuals do not necessarily interact with the deployed system or application. \n\nOther AI actors may provide formal or quasi-formal norms or guidance for specifying and managing AI risks. They can include trade associations, standards developing or-ganizations, advocacy groups, researchers, environmental groups, and civil society organizations .\n\nThe general public is most likely to directly experience positive and negative impacts of AI technologies. They may provide the motivation for actions taken by the AI actors. This group can include individuals, communities, and consumers associated with the context in which an AI system is developed or deployed.  \n\n> Page 37 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Appendix B: How AI Risks Differ from Traditional Software Risks \n\nAs with traditional software, risks from AI-based technology can be bigger than an en-terprise, span organizations, and lead to societal impacts. AI systems also bring a set of risks that are not comprehensively addressed by current risk frameworks and approaches. Some AI system features that present risks also can be beneficial. For example, pre-trained models and transfer learning can advance research and increase accuracy and resilience when compared to other models and approaches. Identifying contextual factors in the MAP \n\nfunction will assist AI actors in determining the level of risk and potential management efforts. Compared to traditional software, AI-specific risks that are new or increased include the following: â€¢ The data used for building an AI system may not be a true or appropriate representa-tion of the context or intended use of the AI system, and the ground truth may either not exist or not be available. Additionally, harmful bias and other data quality issues can affect AI system trustworthiness, which could lead to negative impacts. â€¢ AI system dependency and reliance on data for training tasks, combined with in-creased volume and complexity typically associated with such data. â€¢ Intentional or unintentional changes during training may fundamentally alter AI sys-tem performance. â€¢ Datasets used to train AI systems may become detached from their original and in-tended context or may become stale or outdated relative to deployment context. â€¢ AI system scale and complexity (many systems contain billions or even trillions of decision points) housed within more traditional software applications. â€¢ Use of pre-trained models that can advance research and improve performance can also increase levels of statistical uncertainty and cause issues with bias management, scientific validity, and reproducibility. â€¢ Higher degree of difficulty in predicting failure modes for emergent properties of large-scale pre-trained models. â€¢ Privacy risk due to enhanced data aggregation capability for AI systems. â€¢ AI systems may require more frequent maintenance and triggers for conducting cor-rective maintenance due to data, model, or concept drift. â€¢ Increased opacity and concerns about reproducibility. â€¢ Underdeveloped software testing standards and inability to document AI-based prac-tices to the standard expected of traditionally engineered software for all but the simplest of cases. â€¢ Difficulty in performing regular AI-based software testing, or determining what to test, since AI systems are not subject to the same controls as traditional code devel-opment.  \n\n> Page 38 NIST AI 100-1\n\nAI RMF 1.0 â€¢ Computational costs for developing AI systems and their impact on the environment and planet. â€¢ Inability to predict or detect the side effects of AI-based systems beyond statistical measures. Privacy and cybersecurity risk management considerations and approaches are applicable in the design, development, deployment, evaluation, and use of AI systems. Privacy and cybersecurity risks are also considered as part of broader enterprise risk management con-siderations, which may incorporate AI risks. As part of the effort to address AI trustworthi-ness characteristics such as â€œSecure and Resilientâ€ and â€œPrivacy-Enhanced,â€ organizations may consider leveraging available standards and guidance that provide broad guidance to organizations to reduce security and privacy risks, such as, but not limited to, the NIST Cy-bersecurity Framework, the NIST Privacy Framework, the NIST Risk Management Frame-work, and the Secure Software Development Framework. These frameworks have some features in common with the AI RMF. Like most risk management approaches, they are outcome-based rather than prescriptive and are often structured around a Core set of func-tions, categories, and subcategories. While there are significant differences between these frameworks based on the domain addressed â€“ and because AI risk management calls for addressing many other types of risks â€“ frameworks like those mentioned above may inform security and privacy considerations in the MAP , MEASURE , and MANAGE functions of the AI RMF. At the same time, guidance available before publication of this AI RMF does not compre-hensively address many AI system risks. For example, existing frameworks and guidance are unable to: â€¢ adequately manage the problem of harmful bias in AI systems; â€¢ confront the challenging risks related to generative AI; â€¢ comprehensively address security concerns related to evasion, model extraction, mem-bership inference, availability, or other machine learning attacks; â€¢ account for the complex attack surface of AI systems or other security abuses enabled by AI systems; and â€¢ consider risks associated with third-party AI technologies, transfer learning, and off-label use where AI systems may be trained for decision-making outside an organiza-tionâ€™s security controls or trained in one domain and then â€œfine-tunedâ€ for another. Both AI and traditional software technologies and systems are subject to rapid innovation. Technology advances should be monitored and deployed to take advantage of those devel-opments and work towards a future of AI that is both trustworthy and responsible.  \n\n> Page 39 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Appendix C: AI Risk Management and Human-AI Interaction \n\nOrganizations that design, develop, or deploy AI systems for use in operational settings may enhance their AI risk management by understanding current limitations of human-AI interaction. The AI RMF provides opportunities to clearly define and differentiate the various human roles and responsibilities when using, interacting with, or managing AI systems. Many of the data-driven approaches that AI systems rely on attempt to convert or represent individual and social observational and decision-making practices into measurable quanti-ties. Representing complex human phenomena with mathematical models can come at the cost of removing necessary context. This loss of context may in turn make it difficult to understand individual and societal impacts that are key to AI risk management efforts. Issues that merit further consideration and research include: 1. Human roles and responsibilities in decision making and overseeing AI systems need to be clearly defined and differentiated. Human-AI configurations can span from fully autonomous to fully manual. AI systems can autonomously make deci-sions, defer decision making to a human expert, or be used by a human decision maker as an additional opinion. Some AI systems may not require human oversight, such as models used to improve video compression. Other systems may specifically require human oversight. 2. Decisions that go into the design, development, deployment, evaluation, and use of AI systems reflect systemic and human cognitive biases. AI actors bring their cognitive biases, both individual and group, into the process. Biases can stem from end-user decision-making tasks and be introduced across the AI lifecycle via human assumptions, expectations, and decisions during design and modeling tasks. These biases, which are not necessarily always harmful, may be exacerbated by AI system opacity and the resulting lack of transparency. Systemic biases at the organizational level can influence how teams are structured and who controls the decision-making processes throughout the AI lifecycle. These biases can also influence downstream decisions by end users, decision makers, and policy makers and may lead to negative impacts. 3. Human-AI interaction results vary. Under certain conditions â€“ for example, in perceptual-based judgment tasks â€“ the AI part of the human-AI interaction can am-plify human biases, leading to more biased decisions than the AI or human alone. When these variations are judiciously taken into account in organizing human-AI teams, however, they can result in complementarity and improved overall perfor-mance.  \n\n> Page 40 NIST AI 100-1\n\nAI RMF 1.0 4. Presenting AI system information to humans is complex. Humans perceive and derive meaning from AI system output and explanations in different ways, reflecting different individual preferences, traits, and skills. The GOVERN function provides organizations with the opportunity to clarify and define the roles and responsibilities for the humans in the Human-AI team configurations and those who are overseeing the AI system performance. The GOVERN function also creates mechanisms for organizations to make their decision-making processes more explicit, to help counter systemic biases. The MAP function suggests opportunities to define and document processes for operator and practitioner proficiency with AI system performance and trustworthiness concepts, and to define relevant technical standards and certifications. Implementing MAP function cat-egories and subcategories may help organizations improve their internal competency for analyzing context, identifying procedural and system limitations, exploring and examining impacts of AI-based systems in the real world, and evaluating decision-making processes throughout the AI lifecycle. The GOVERN and MAP functions describe the importance of interdisciplinarity and demo-graphically diverse teams and utilizing feedback from potentially impacted individuals and communities. AI actors called out in the AI RMF who perform human factors tasks and activities can assist technical teams by anchoring in design and development practices to user intentions and representatives of the broader AI community, and societal values. These actors further help to incorporate context-specific norms and values in system design and evaluate end user experiences â€“ in conjunction with AI systems. AI risk management approaches for human-AI configurations will be augmented by on-going research and evaluation. For example, the degree to which humans are empowered and incentivized to challenge AI system output requires further studies. Data about the fre-quency and rationale with which humans overrule AI system output in deployed systems may be useful to collect and analyze.  \n\n> Page 41 NIST AI 100-1\n\nAI RMF 1.0 \n\n# Appendix D: Attributes of the AI RMF \n\nNIST described several key attributes of the AI RMF when work on the Framework first began. These attributes have remained intact and were used to guide the AI RMFâ€™s devel-opment. They are provided here as a reference. The AI RMF strives to: 1. Be risk-based, resource-efficient, pro-innovation, and voluntary. 2. Be consensus-driven and developed and regularly updated through an open, trans-parent process. All stakeholders should have the opportunity to contribute to the AI RMFâ€™s development. 3. Use clear and plain language that is understandable by a broad audience, including senior executives, government officials, non-governmental organization leadership, and those who are not AI professionals â€“ while still of sufficient technical depth to be useful to practitioners. The AI RMF should allow for communication of AI risks across an organization, between organizations, with customers, and to the public at large. 4. Provide common language and understanding to manage AI risks. The AI RMF should offer taxonomy, terminology, definitions, metrics, and characterizations for AI risk. 5. Be easily usable and fit well with other aspects of risk management. Use of the Framework should be intuitive and readily adaptable as part of an organizationâ€™s broader risk management strategy and processes. It should be consistent or aligned with other approaches to managing AI risks. 6. Be useful to a wide range of perspectives, sectors, and technology domains. The AI RMF should be universally applicable to any AI technology and to context-specific use cases. 7. Be outcome-focused and non-prescriptive. The Framework should provide a catalog of outcomes and approaches rather than prescribe one-size-fits-all requirements. 8. Take advantage of and foster greater awareness of existing standards, guidelines, best practices, methodologies, and tools for managing AI risks â€“ as well as illustrate the need for additional, improved resources. 9. Be law- and regulation-agnostic. The Framework should support organizationsâ€™ abilities to operate under applicable domestic and international legal or regulatory regimes. 10. Be a living document. The AI RMF should be readily updated as technology, under-standing, and approaches to AI trustworthiness and uses of AI change and as stake-holders learn from implementing AI risk management generally and this framework in particular. \n\n> Page 42\n\nThis publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1", "fetched_at_utc": "2026-02-08T19:07:04Z", "sha256": "92288e1fcca5951721db4482b109fad7dd2e72207c9e9994d5fed41491c176f8", "meta": {"file_name": "AI Risk Management Framework - NIST.pdf", "file_size": 1946127, "relative_path": "pdfs\\AI Risk Management Framework - NIST.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 20922}}}
{"doc_id": "pdf-pdfs-ai-risk-management-framework-playbook-nist-af493a99a01e", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Risk Management Framework Playbook - NIST.pdf", "title": "AI Risk Management Framework Playbook - NIST", "text": "AI RMF AI RMF \n\n# PLAYBOOK PLAYBOOK Table of Contents \n\nGOVERN ................................ ................................ ................................ ................................ ................................ ........... 4\n\nGOVERN 1.1 ................................ ................................ ................................ ................................ ................................ ..................... 4\n\nGOVERN 1.2 ................................ ................................ ................................ ................................ ................................ ..................... 5\n\nGOVERN 1.3 ................................ ................................ ................................ ................................ ................................ ..................... 7\n\nGOVERN 1.4 ................................ ................................ ................................ ................................ ................................ ..................... 9\n\nGOVERN 1.5 ................................ ................................ ................................ ................................ ................................ .................. 11 \n\nGOVERN 1.6 ................................ ................................ ................................ ................................ ................................ .................. 12 \n\nGOVERN 1.7 ................................ ................................ ................................ ................................ ................................ .................. 13 \n\nGOVERN 2.1 ................................ ................................ ................................ ................................ ................................ .................. 15 \n\nGOVERN 2.2 ................................ ................................ ................................ ................................ ................................ .................. 17 \n\nGOVERN 2.3 ................................ ................................ ................................ ................................ ................................ .................. 18 \n\nGOVERN 3.1 ................................ ................................ ................................ ................................ ................................ .................. 19 \n\nGOVERN 3.2 ................................ ................................ ................................ ................................ ................................ .................. 21 \n\nGOVERN 4.1 ................................ ................................ ................................ ................................ ................................ .................. 23 \n\nGOVERN 4.2 ................................ ................................ ................................ ................................ ................................ .................. 24 \n\nGOVERN 4.3 ................................ ................................ ................................ ................................ ................................ .................. 27 \n\nGOVERN 5.1 ................................ ................................ ................................ ................................ ................................ .................. 28 \n\nGOVERN 5.2 ................................ ................................ ................................ ................................ ................................ .................. 30 \n\nGOVERN 6.1 ................................ ................................ ................................ ................................ ................................ .................. 32 \n\nGOVERN 6.2 ................................ ................................ ................................ ................................ ................................ .................. 33 \n\nMANAGE ................................ ................................ ................................ ................................ ................................ ........ 35 \n\nMANAGE 1.1 ................................ ................................ ................................ ................................ ................................ ................. 35 \n\nMANAGE 1.2 ................................ ................................ ................................ ................................ ................................ ................. 36 \n\nMANAGE 1.3 ................................ ................................ ................................ ................................ ................................ ................. 37 \n\nMANAGE 1.4 ................................ ................................ ................................ ................................ ................................ ................. 39 \n\nMANAGE 2.1 ................................ ................................ ................................ ................................ ................................ ................. 40 \n\nMANAGE 2.2 ................................ ................................ ................................ ................................ ................................ ................. 42 \n\nMANAGE 2.3 ................................ ................................ ................................ ................................ ................................ ................. 48 \n\nMANAGE 2.4 ................................ ................................ ................................ ................................ ................................ ................. 49 \n\nMANAGE 3.1 ................................ ................................ ................................ ................................ ................................ ................. 51 \n\nMANAGE 3.2 ................................ ................................ ................................ ................................ ................................ ................. 52 \n\nMANAGE 4.1 ................................ ................................ ................................ ................................ ................................ ................. 53 \n\nMANAGE 4.2 ................................ ................................ ................................ ................................ ................................ ................. 54 \n\nMANAGE 4.3 ................................ ................................ ................................ ................................ ................................ ................. 56 \n\nMAP ................................ ................................ ................................ ................................ ................................ ................ 58 \n\nMAP 1.1 ................................ ................................ ................................ ................................ ................................ ........................... 58 \n\nMAP 1.2 ................................ ................................ ................................ ................................ ................................ ........................... 62 \n\nMAP 1.3 ................................ ................................ ................................ ................................ ................................ ........................... 63 \n\nMAP 1.4 ................................ ................................ ................................ ................................ ................................ ........................... 65 \n\nMAP 1.5 ................................ ................................ ................................ ................................ ................................ ........................... 66 \n\nMAP 1.6 ................................ ................................ ................................ ................................ ................................ ........................... 68 \n\nMAP 2.1 ................................ ................................ ................................ ................................ ................................ ........................... 70 MAP 2.2 ................................ ................................ ................................ ................................ ................................ ........................... 71 \n\nMAP 2.3 ................................ ................................ ................................ ................................ ................................ ........................... 74 \n\nMAP 3.1 ................................ ................................ ................................ ................................ ................................ ........................... 77 \n\nMAP 3.2 ................................ ................................ ................................ ................................ ................................ ........................... 79 \n\nMAP 3.3 ................................ ................................ ................................ ................................ ................................ ........................... 80 \n\nMAP 3.4 ................................ ................................ ................................ ................................ ................................ ........................... 82 \n\nMAP 3.5 ................................ ................................ ................................ ................................ ................................ ........................... 84 \n\nMAP 4.1 ................................ ................................ ................................ ................................ ................................ ........................... 86 \n\nMAP 4.2 ................................ ................................ ................................ ................................ ................................ ........................... 88 \n\nMAP 5.1 ................................ ................................ ................................ ................................ ................................ ........................... 89 \n\nMAP 5.2 ................................ ................................ ................................ ................................ ................................ ........................... 90 \n\nMEASURE ................................ ................................ ................................ ................................ ................................ ...... 93 \n\nMEASURE 1.1 ................................ ................................ ................................ ................................ ................................ ............... 93 \n\nMEASURE 1.2 ................................ ................................ ................................ ................................ ................................ ............... 95 \n\nMEASURE 1.3 ................................ ................................ ................................ ................................ ................................ ............... 96 \n\nMEASURE 2.1 ................................ ................................ ................................ ................................ ................................ ............... 98 \n\nMEASURE 2.2 ................................ ................................ ................................ ................................ ................................ ............... 99 \n\nMEASURE 2.3 ................................ ................................ ................................ ................................ ................................ ............ 102 \n\nMEASURE 2.4 ................................ ................................ ................................ ................................ ................................ ............ 104 \n\nMEASURE 2.5 ................................ ................................ ................................ ................................ ................................ ............ 106 \n\nMEASURE 2.6 ................................ ................................ ................................ ................................ ................................ ............ 108 \n\nMEASURE 2.7 ................................ ................................ ................................ ................................ ................................ ............ 110 \n\nMEASURE 2.8 ................................ ................................ ................................ ................................ ................................ ............ 112 \n\nMEASURE 2.9 ................................ ................................ ................................ ................................ ................................ ............ 115 \n\nMEASURE 2.10 ................................ ................................ ................................ ................................ ................................ ......... 118 \n\nMEASURE 2.11 ................................ ................................ ................................ ................................ ................................ ......... 121 \n\nMEASURE 2.12 ................................ ................................ ................................ ................................ ................................ ......... 126 \n\nMEASURE 2.13 ................................ ................................ ................................ ................................ ................................ ......... 128 \n\nMEASURE 3.1 ................................ ................................ ................................ ................................ ................................ ............ 129 \n\nMEASURE 3.2 ................................ ................................ ................................ ................................ ................................ ............ 131 \n\nMEASURE 3.3 ................................ ................................ ................................ ................................ ................................ ............ 132 \n\nMEASURE 4.1 ................................ ................................ ................................ ................................ ................................ ............ 134 \n\nMEASURE 4.2 ................................ ................................ ................................ ................................ ................................ ............ 137 \n\nMEASURE 4.3 ................................ ................................ ................................ ................................ ................................ ............ 140 The Playbook provides suggested actions for achieving the outcomes laid out in \n\n# the AI Risk Management Framework (AI RMF) Core (Tables 1 â€“ 4 in AI RMF \n\n# 1.0). Suggestions are aligned to each sub-category within the four AI RMF \n\n# functions (Govern, Map, Measure, Manage). \n\n# The Playbook is neither a checklist nor set of steps to be followed in its entirety. \n\n# Playbook suggestions are voluntary. Organizations may utilize this information \n\n# by borrowing as many â€“ or as few â€“ suggestions as apply to their industry use \n\n# case or interests. \n\n# About the Playbook \n\n# Govern  Map  Measure  Manage \n\n# FORWARD GOVERN 4 of 142 \n\n# Govern \n\nPolicies, processes, procedures and practices across the organization related to the \n\nmapping, measuring and managing of AI risks are in place, transparent, and implemented \n\neffectively. \n\n# GOVERN 1.1 \n\nLegal and regulatory requirements involving AI are understood, managed, and documented. \n\nAbout \n\nAI systems may be subject to specific applicable legal and regulatory requirements. Some \n\nlegal requirements can mandate (e.g., nondiscrimination, data privacy and security \n\ncontrols) documentation, disclosure, and increased AI system transparency. These \n\nrequirements are complex and may not be applicable or differ across applications and \n\ncontexts. \n\nFor example, AI system testing processes for bias measurement, such as disparate impact, \n\nare not applied uniformly within the legal context. Disparate impact is broadly defined as a \n\nfacially neutral policy or practice that disproportionately harms a group based on a \n\nprotected trait. Notably, some modeling algorithms or debiasing techniques that rely on \n\ndemographic information, could also come into tension with legal prohibitions on disparate \n\ntreatment (i.e., intentional discrimination). \n\nAdditionally, some intended users of AI systems may not have consistent or reliable access \n\nto fundamental internet technologies (a phenomenon widely described as the â€œdigital \n\ndivideâ€) or may experience difficulties interacting with AI systems due to disabilities or \n\nimpairments. Such factors may mean different communities experience bias or other \n\nnegative impacts when trying to access AI systems. Failure to address such design issues \n\nmay pose legal risks, for example in employment related activities affecting persons with \n\ndisabilities. \n\nSuggested Actions \n\nâ€¢ Maintain awareness of the applicable legal and regulatory considerations and \n\nrequirements specific to industry, sector, and business purpose, as well as the \n\napplication context of the deployed AI system. \n\nâ€¢ Align risk management efforts with applicable legal standards. \n\nâ€¢ Maintain policies for training (and re -training) organizational staff about necessary \n\nlegal or regulatory considerations that may impact AI -related design, development and \n\ndeployment activities. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent has the entity defined and documented the regulatory environment â€”\n\nincluding minimum requirements in laws and regulations? 5 of 142 \n\nâ€¢ Has the system been reviewed for its compliance to applicable laws, regulations, \n\nstandards, and guidance? \n\nâ€¢ To what extent has the entity defined and documented the regulatory environment â€”\n\nincluding applicable requirements in laws and regulations? \n\nâ€¢ Has the system been reviewed for its compliance to relevant applicable laws, \n\nregulations, standards, and guidance? \n\nAI Transparency Resources \n\nGAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nAndrew Smith, \"Using Artificial Intelligence and Algorithms,\" FTC Business Blog (2020). \n\nRebecca Kelly Slaughter, \"Algorithms and Economic Justice,\" ISP Digital Future Whitepaper \n\n& YJoLT Special Publication (2021). \n\nPatrick Hall, Benjamin Cox, Steven Dickerson, Arjun Ravi Kannan, Raghu Kulkarni, and \n\nNicholas Schmidt, \"A United States fair lending perspective on machine learning,\" Frontiers \n\nin Artificial Intelligence 4 (2021). \n\nAI Hiring Tools and the Law, Partnership on Employment & Accessible Technology (PEAT, \n\npeatworks.org). \n\n# GOVERN 1.2 \n\nThe characteristics of trustworthy AI are integrated into organizational policies, processes, \n\nand procedures. \n\nAbout \n\nPolicies, processes, and procedures are central components of effective AI risk management \n\nand fundamental to individual and organizational accountability. All stakeholders benefit \n\nfrom policies, processes, and procedures which require preventing harm by design and \n\ndefault. \n\nOrganizational policies and procedures will vary based on available resources and risk \n\nprofiles, but can help systematize AI actor roles and responsibilities throughout the AI \n\nlifecycle. Without such policies, risk management can be subjective across the organization, \n\nand exacerbate rather than minimize risks over time. Policies, or summaries thereof, are \n\nunderstandable to relevant AI actors. Policies reflect an understanding of the underlying \n\nmetrics, measurements, and tests that are necessary to support policy and AI system design, \n\ndevelopment, deployment and use. \n\nLack of clear information about responsibilities and chains of command will limit the \n\neffectiveness of risk management. \n\nSuggested Actions \n\nOrganizational AI risk management policies should be designed to: 6 of 142 \n\nâ€¢ Define key terms and concepts related to AI systems and the scope of their purposes \n\nand intended uses. \n\nâ€¢ Connect AI governance to existing organizational governance and risk controls. \n\nâ€¢ Align to broader data governance policies and practices, particularly the use of sensitive \n\nor otherwise risky data. \n\nâ€¢ Detail standards for experimental design, data quality, and model training. \n\nâ€¢ Outline and document risk mapping and measurement processes and standards. \n\nâ€¢ Detail model testing and validation processes. \n\nâ€¢ Detail review processes for legal and risk functions. \n\nâ€¢ Establish the frequency of and detail for monitoring, auditing and review processes. \n\nâ€¢ Outline change management requirements. \n\nâ€¢ Outline processes for internal and external stakeholder engagement. \n\nâ€¢ Establish whistleblower policies to facilitate reporting of serious AI system concerns. \n\nâ€¢ Detail and test incident response plans. \n\nâ€¢ Verify that formal AI risk management policies align to existing legal standards, and \n\nindustry best practices and norms. \n\nâ€¢ Establish AI risk management policies that broadly align to AI system trustworthy \n\ncharacteristics. \n\nâ€¢ Verify that formal AI risk management policies include currently deployed and third -\n\nparty AI systems. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent do these policies foster public trust and confidence in the use of the AI \n\nsystem? \n\nâ€¢ What policies has the entity developed to ensure the use of the AI system is consistent \n\nwith its stated values and principles? \n\nâ€¢ What policies and documentation has the entity developed to encourage the use of its AI \n\nsystem as intended? \n\nâ€¢ To what extent are the model outputs consistent with the entityâ€™s values and principles \n\nto foster public trust and equity? \n\nAI Transparency Resources \n\nGAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nOff. Comptroller Currency, Comptrollerâ€™s Handbook: Model Risk Management (Aug. 2021). \n\nGAO, â€œArtificial Intelligence: An Accountability Framework for Federal Agencies and Other \n\nEntities,â€ GAO@100 (GAO -21 -519SP), June 2021. \n\nNIST, \"U.S. Leadership in AI: A Plan for Federal Engagement in Developing Technical \n\nStandards and Related Tools\". 7 of 142 \n\nLipton, Zachary and McAuley, Julian and Chouldechova, Alexandra, Does mitigating MLâ€™s \n\nimpact disparity require treatment disparity? Advances in Neural Information Processing \n\nSystems, 2018. \n\nJessica Newman (2023) â€œA Taxonomy of Trustworthiness for Artificial Intelligence: \n\nConnecting Properties of Trustworthiness with Risk Management and the AI Lifecycle,â€ UC \n\nBerkeley Center for Long -Term Cybersecurity. \n\nEmily Hadley (2022). Prioritizing Policies for Furthering Responsible Artificial Intelligence \n\nin the United States. 2022 IEEE International Conference on Big Data (Big Data), 5029 -5038. \n\nSAS Institute, â€œThe SASÂ® Data Governance Framework: A Blueprint for Successâ€. \n\nISO, â€œInformation technology â€” Reference Model of Data Management, â€œ ISO/IEC TR \n\n10032:200. \n\nâ€œPlay 5: Create a formal policy,â€ Partnership on Employment & Accessible Technology \n\n(PEAT, peatworks.org). \n\n\"National Institute of Standards and Technology. (2018). Framework for improving critical \n\ninfrastructure cybersecurity. \n\nKaitlin R. Boeckl and Naomi B. Lefkovitz. \"NIST Privacy Framework: A Tool for Improving \n\nPrivacy Through Enterprise Risk Management, Version 1.0.\" National Institute of Standards \n\nand Technology (NIST), January 16, 2020. \n\nâ€œplainlanguage.gov â€“ Home,â€ The U.S. Government. \n\n# GOVERN 1.3 \n\nProcesses and procedures are in place to determine the needed level of risk management \n\nactivities based on the organization's risk tolerance. \n\nAbout \n\nRisk management resources are finite in any organization. Adequate AI governance policies \n\ndelineate the mapping, measurement, and prioritization of risks to allocate resources \n\ntoward the most material issues for an AI system to ensure effective risk management. \n\nPolicies may specify systematic processes for assigning mapped and measured risks to \n\nstandardized risk scales. \n\nAI risk tolerances range from negligible to critical â€“ from, respectively, almost no risk to \n\nrisks that can result in irredeemable human, reputational, financial, or environmental \n\nlosses. Risk tolerance rating policies consider different sources of risk, (e.g., financial, \n\noperational, safety and wellbeing, business, reputational, or model risks). A typical risk \n\nmeasurement approach entails the multiplication, or qualitative combination, of measured \n\nor estimated impact and likelihood of impacts into a risk score (risk â‰ˆ impact x likelihood). \n\nThis score is then placed on a risk scale. Scales for risk may be qualitative, such as red -\n\namber -green (RAG), or may entail simulations or econometric approaches. Impact 8 of 142 \n\nassessments are a common tool for understanding the severity of mapped risks. In the most \n\nfulsome AI risk management approaches, all models are assigned to a risk level. \n\nSuggested Actions \n\nâ€¢ Establish policies to define mechanisms for measuring or understanding an AI systemâ€™s \n\npotential impacts, e.g., via regular impact assessments at key stages in the AI lifecycle, \n\nconnected to system impacts and frequency of system updates. \n\nâ€¢ Establish policies to define mechanisms for measuring or understanding the likelihood \n\nof an AI systemâ€™s impacts and their magnitude at key stages in the AI lifecycle. \n\nâ€¢ Establish policies that define assessment scales for measuring potential AI system \n\nimpact. Scales may be qualitative, such as red -amber -green (RAG), or may entail \n\nsimulations or econometric approaches. \n\nâ€¢ Establish policies for assigning an overall risk measurement approach for an AI system, \n\nor its important components, e.g., via multiplication or combination of a mapped riskâ€™s \n\nimpact and likelihood (risk â‰ˆ impact x likelihood). \n\nâ€¢ Establish policies to assign systems to uniform risk scales that are valid across the \n\norganizationâ€™s AI portfolio (e.g. documentation templates), and acknowledge risk \n\ntolerance and risk levels may change over the lifecycle of an AI system. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ How do system performance metrics inform risk tolerance decisions? \n\nâ€¢ What policies has the entity developed to ensure the use of the AI system is consistent \n\nwith organizational risk tolerance? \n\nâ€¢ How do the entityâ€™s data security and privacy assessments inform risk tolerance \n\ndecisions? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nThe Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, \n\n2019). \n\nBrenda Boultwood, How to Develop an Enterprise Risk -Rating Approach (Aug. 26, 2021). \n\nGlobal Association of Risk Professionals (garp.org). Accessed Jan. 4, 2023. \n\nGAO -17 -63: Enterprise Risk Management: Selected Agenciesâ€™ Experiences Illustrate Good \n\nPractices in Managing Risk. 9 of 142 \n\n# GOVERN 1.4 \n\nThe risk management process and its outcomes are established through transparent \n\npolicies, procedures, and other controls based on organizational risk priorities. \n\nAbout \n\nClear policies and procedures relating to documentation and transparency facilitate and \n\nenhance efforts to communicate roles and responsibilities for the Map, Measure and \n\nManage functions across the AI lifecycle. Standardized documentation can help \n\norganizations systematically integrate AI risk management processes and enhance \n\naccountability efforts. For example, by adding their contact information to a work product \n\ndocument, AI actors can improve communication, increase ownership of work products, \n\nand potentially enhance consideration of product quality. Documentation may generate \n\ndownstream benefits related to improved system replicability and robustness. Proper \n\ndocumentation storage and access procedures allow for quick retrieval of critical \n\ninformation during a negative incident. Explainable machine learning efforts (models and \n\nexplanatory methods) may bolster technical documentation practices by introducing \n\nadditional information for review and interpretation by AI Actors. \n\nSuggested Actions \n\nâ€¢ Establish and regularly review documentation policies that, among others, address \n\ninformation related to: \n\nâ€¢ AI actors contact informations \n\nâ€¢ Business justification \n\nâ€¢ Scope and usages \n\nâ€¢ Expected and potential risks and impacts \n\nâ€¢ Assumptions and limitations \n\nâ€¢ Description and characterization of training data \n\nâ€¢ Algorithmic methodology \n\nâ€¢ Evaluated alternative approaches \n\nâ€¢ Description of output data \n\nâ€¢ Testing and validation results (including explanatory visualizations and \n\ninformation) \n\nâ€¢ Down - and up -stream dependencies \n\nâ€¢ Plans for deployment, monitoring, and change management \n\nâ€¢ Stakeholder engagement plans \n\nâ€¢ Verify documentation policies for AI systems are standardized across the organization \n\nand remain current. \n\nâ€¢ Establish policies for a model documentation inventory system and regularly review its \n\ncompleteness, usability, and efficacy. \n\nâ€¢ Establish mechanisms to regularly review the efficacy of risk management processes. \n\nâ€¢ Identify AI actors responsible for evaluating efficacy of risk management processes and \n\napproaches, and for course -correction based on results. 10 of 142 \n\nâ€¢ Establish policies and processes regarding public disclosure of the use of AI and risk \n\nmanagement material such as impact assessments, audits, model documentation and \n\nvalidation and testing results. \n\nâ€¢ Document and review the use and efficacy of different types of transparency tools and \n\nfollow industry standards at the time a model is in use. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\nâ€¢ What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\nâ€¢ How will the appropriate performance metrics, such as accuracy, of the AI be monitored \n\nafter the AI is deployed? How much distributional shift or model drift from baseline \n\nperformance is acceptable? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011). \n\nOff. Comptroller Currency, Comptrollerâ€™s Handbook: Model Risk Management (Aug. 2021). \n\nMargaret Mitchell et al., â€œModel Cards for Model Reporting.â€ Proceedings of 2019 FATML \n\nConference. \n\nTimnit Gebru et al., â€œDatasheets for Datasets,â€ Communications of the ACM 64, No. 12, 2021. \n\nEmily M. Bender, Batya Friedman, Angelina McMillan -Major (2022). A Guide for Writing \n\nData Statements for Natural Language Processing. University of Washington. Accessed July \n\n14, 2022. \n\nM. Arnold, R. K. E. Bellamy, M. Hind, et al. FactSheets: Increasing trust in AI services through \n\nsupplierâ€™s declarations of conformity. IBM Journal of Research and Development 63, 4/5 \n\n(July -September 2019), 6:1 -6:13. \n\nNavdeep Gill, Abhishek Mathur, Marcos V. Conde (2022). A Brief Overview of AI Governance \n\nfor Responsible Machine Learning Systems. ArXiv, abs/2211.13130. \n\nJohn Richards, David Piorkowski, Michael Hind, et al. A Human -Centered Methodology for \n\nCreating AI FactSheets. Bulletin of the IEEE Computer Society Technical Committee on Data \n\nEngineering. 11 of 142 \n\nChristoph Molnar, Interpretable Machine Learning, lulu.com. \n\nDavid A. Broniatowski. 2021. Psychological Foundations of Explainability and \n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology \n\n(NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD. \n\nOECD (2022), â€œOECD Framework for the Classification of AI systemsâ€, OECD Digital \n\nEconomy Papers, No. 323, OECD Publishing, Paris. \n\n# GOVERN 1.5 \n\nOngoing monitoring and periodic review of the risk management process and its outcomes \n\nare planned, organizational roles and responsibilities are clearly defined, including \n\ndetermining the frequency of periodic review. \n\nAbout \n\nAI systems are dynamic and may perform in unexpected ways once deployed or after \n\ndeployment. Continuous monitoring is a risk management process for tracking unexpected \n\nissues and performance changes, in real -time or at a specific frequency, across the AI \n\nsystem lifecycle. \n\nIncident response and â€œappeal and overrideâ€ are commonly used processes in information \n\ntechnology management. These processes enable real -time flagging of potential incidents, \n\nand human adjudication of system outcomes. \n\nEstablishing and maintaining incident response plans can reduce the likelihood of additive \n\nimpacts during an AI incident. Smaller organizations which may not have fulsome \n\ngovernance programs, can utilize incident response plans for addressing system failures, \n\nabuse or misuse. \n\nSuggested Actions \n\nâ€¢ Establish policies to allocate appropriate resources and capacity for assessing impacts \n\nof AI systems on individuals, communities and society. \n\nâ€¢ Establish policies and procedures for monitoring and addressing AI system \n\nperformance and trustworthiness, including bias and security problems, across the \n\nlifecycle of the system. \n\nâ€¢ Establish policies for AI system incident response, or confirm that existing incident \n\nresponse policies apply to AI systems. \n\nâ€¢ Establish policies to define organizational functions and personnel responsible for AI \n\nsystem monitoring and incident response activities. \n\nâ€¢ Establish mechanisms to enable the sharing of feedback from impacted individuals or \n\ncommunities about negative impacts from AI systems. \n\nâ€¢ Establish mechanisms to provide recourse for impacted individuals or communities to \n\ncontest problematic AI system outcomes. \n\nâ€¢ Establish opt -out mechanisms. 12 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent does the system/entity consistently measure progress towards stated \n\ngoals and objectives? \n\nâ€¢ Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\nâ€¢ Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nReferences \n\nNational Institute of Standards and Technology. (2018). Framework for improving critical \n\ninfrastructure cybersecurity. \n\nNational Institute of Standards and Technology. (2012). Computer Security Incident \n\nHandling Guide. NIST Special Publication 800 -61 Revision 2. \n\n# GOVERN 1.6 \n\nMechanisms are in place to inventory AI systems and are resourced according to \n\norganizational risk priorities. \n\nAbout \n\nAn AI system inventory is an organized database of artifacts relating to an AI system or \n\nmodel. It may include system documentation, incident response plans, data dictionaries, \n\nlinks to implementation software or source code, names and contact information for \n\nrelevant AI actors, or other information that may be helpful for model or system \n\nmaintenance and incident response purposes. AI system inventories also enable a holistic \n\nview of organizational AI assets. A serviceable AI system inventory may allow for the quick \n\nresolution of: \n\nâ€¢ specific queries for single models, such as â€œwhen was this model last refreshed?â€ \n\nâ€¢ high -level queries across all models, such as, â€œhow many models are currently deployed \n\nwithin our organization?â€ or â€œhow many users are impacted by our models?â€ \n\nAI system inventories are a common element of traditional model risk management \n\napproaches and can provide technical, business and risk management benefits. Typically \n\ninventories capture all organizational models or systems, as partial inventories may not \n\nprovide the value of a full inventory. 13 of 142 \n\nSuggested Actions \n\nâ€¢ Establish policies that define the creation and maintenance of AI system inventories. \n\nâ€¢ Establish policies that define a specific individual or team that is responsible for \n\nmaintaining the inventory. \n\nâ€¢ Establish policies that define which models or systems are inventoried, with preference \n\nto inventorying all models or systems, or minimally, to high risk models or systems, or \n\nsystems deployed in high -stakes settings. \n\nâ€¢ Establish policies that define model or system attributes to be inventoried, e.g, \n\ndocumentation, links to source code, incident response plans, data dictionaries, AI actor \n\ncontact information. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Who is responsible for documenting and maintaining the AI system inventory details? \n\nâ€¢ What processes exist for data generation, acquisition/collection, ingestion, \n\nstaging/storage, transformations, security, maintenance, and dissemination? \n\nâ€¢ Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nReferences \n\nâ€œA risk -based integrity level schemaâ€, in IEEE 1012, IEEE Standard for System, Software, \n\nand Hardware Verification and Validation. See Annex B. \n\nOff. Comptroller Currency, Comptrollerâ€™s Handbook: Model Risk Management (Aug. 2021). \n\nSee â€œModel Inventory,â€ pg. 26. \n\nVertaAI, â€œModelDB: An open -source system for Machine Learning model versioning, \n\nmetadata, and experiment management.â€ Accessed Jan. 5, 2023. \n\n# GOVERN 1.7 \n\nProcesses and procedures are in place for decommissioning and phasing out of AI systems \n\nsafely and in a manner that does not increase risks or decrease the organizationâ€™s \n\ntrustworthiness. \n\nAbout \n\nIrregular or indiscriminate termination or deletion of models or AI systems may be \n\ninappropriate and increase organizational risk. For example, AI systems may be subject to \n\nregulatory requirements or implicated in future security or legal investigations. To maintain \n\ntrust, organizations may consider establishing policies and processes for the systematic and \n\ndeliberate decommissioning of AI systems. Typically, such policies consider user and 14 of 142 \n\ncommunity concerns, risks in dependent and linked systems, and security, legal or \n\nregulatory concerns. Decommissioned models or systems may be stored in a model \n\ninventory along with active models, for an established length of time. \n\nSuggested Actions \n\nâ€¢ Establish policies for decommissioning AI systems. Such policies typically address: \n\nâ€¢ User and community concerns, and reputational risks. \n\nâ€¢ Business continuity and financial risks. \n\nâ€¢ Up and downstream system dependencies. \n\nâ€¢ Regulatory requirements (e.g., data retention). \n\nâ€¢ Potential future legal, regulatory, security or forensic investigations. \n\nâ€¢ Migration to the replacement system, if appropriate. \n\nâ€¢ Establish policies that delineate where and for how long decommissioned systems, \n\nmodels and related artifacts are stored. \n\nâ€¢ Establish practices to track accountability and consider how decommission and other \n\nadaptations or changes in system deployment contribute to downstream impacts for \n\nindividuals, groups and communities. \n\nâ€¢ Establish policies that address ancillary data or artifacts that must be preserved for \n\nfulsome understanding or execution of the decommissioned AI system, e.g., predictions, \n\nexplanations, intermediate input feature representations, usernames and passwords, \n\netc. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What processes exist for data generation, acquisition/collection, ingestion, \n\nstaging/storage, transformations, security, maintenance, and dissemination? \n\nâ€¢ To what extent do these policies foster public trust and confidence in the use of the AI \n\nsystem? \n\nâ€¢ If anyone believes that the AI no longer meets this ethical framework, who will be \n\nresponsible for receiving the concern and as appropriate investigating and remediating \n\nthe issue? Do they have authority to modify, limit, or stop the use of the AI? \n\nâ€¢ If it relates to people, were there any ethical review applications/reviews/approvals? \n\n(e.g. Institutional Review Board applications) \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nâ€¢ Datasheets for Datasets. 15 of 142 \n\nReferences \n\nMichelle De Mooy, Joseph Jerome and Vijay Kasschau, â€œShould It Stay or Should It Go? The \n\nLegal, Policy and Technical Landscape Around Data Deletion,â€ Center for Democracy and \n\nTechnology, 2017. \n\nBurcu Baykurt, \"Algorithmic accountability in US cities: Transparency, impact, and political \n\neconomy.\" Big Data & Society 9, no. 2 (2022): 20539517221115426. \n\nUpol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. â€œThe Algorithmic Imprint.â€ \n\nProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency \n\n(2022). \n\nâ€œInformation System Decommissioning Guide,â€ Bureau of Land Management, 2011. \n\n# GOVERN 2.1 \n\nRoles and responsibilities and lines of communication related to mapping, measuring, and \n\nmanaging AI risks are documented and are clear to individuals and teams throughout the \n\norganization. \n\nAbout \n\nThe development of a risk -aware organizational culture starts with defining \n\nresponsibilities. For example, under some risk management structures, professionals \n\ncarrying out test and evaluation tasks are independent from AI system developers and \n\nreport through risk management functions or directly to executives. This kind of structure \n\nmay help counter implicit biases such as groupthink or sunk cost fallacy and bolster risk \n\nmanagement functions, so efforts are not easily bypassed or ignored. \n\nInstilling a culture where AI system design and implementation decisions can be questioned \n\nand course - corrected by empowered AI actors can enhance organizationsâ€™ abilities to \n\nanticipate and effectively manage risks before they become ingrained. \n\nSuggested Actions \n\nâ€¢ Establish policies that define the AI risk management roles and responsibilities for \n\npositions directly and indirectly related to AI systems, including, but not limited to \n\nâ€¢ Boards of directors or advisory committees \n\nâ€¢ Senior management \n\nâ€¢ AI audit functions \n\nâ€¢ Product management \n\nâ€¢ Project management \n\nâ€¢ AI design \n\nâ€¢ AI development \n\nâ€¢ Human -AI interaction \n\nâ€¢ AI testing and evaluation \n\nâ€¢ AI acquisition and procurement 16 of 142 \n\nâ€¢ Impact assessment functions \n\nâ€¢ Oversight functions \n\nâ€¢ Establish policies that promote regular communication among AI actors participating in \n\nAI risk management efforts. \n\nâ€¢ Establish policies that separate management of AI system development functions from \n\nAI system testing functions, to enable independent course -correction of AI systems. \n\nâ€¢ Establish policies to identify, increase the transparency of, and prevent conflicts of \n\ninterest in AI risk management efforts. \n\nâ€¢ Establish policies to counteract confirmation bias and market incentives that may \n\nhinder AI risk management efforts. \n\nâ€¢ Establish policies that incentivize AI actors to collaborate with existing legal, oversight, \n\ncompliance, or enterprise risk functions in their AI risk management activities. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\nâ€¢ Who is ultimately responsible for the decisions of the AI and is this person aware of the \n\nintended uses and limitations of the analytic? \n\nâ€¢ Are the responsibilities of the personnel involved in the various AI governance \n\nprocesses clearly defined? \n\nâ€¢ What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\nâ€¢ Did your organization implement accountability -based practices in data management \n\nand protection (e.g. the PDPA and OECD Privacy Principles)? \n\nAI Transparency Resources \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nAndrew Smith, â€œUsing Artificial Intelligence and Algorithms,â€ FTC Business Blog (Apr. 8, \n\n2020). \n\nOff. Superintendent Fin. Inst. Canada, Enterprise -Wide Model Risk Management for Deposit -\n\nTaking Institutions, E -23 (Sept. 2017). \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011). \n\nOff. Comptroller Currency, Comptrollerâ€™s Handbook: Model Risk Management (Aug. 2021). 17 of 142 \n\nISO, â€œInformation Technology â€” Artificial Intelligence â€” Guidelines for AI applications,â€ \n\nISO/IEC CD 5339. See Section 6, â€œStakeholdersâ€™ perspectives and AI application framework.â€ \n\n# GOVERN 2.2 \n\nThe organizationâ€™s personnel and partners receive AI risk management training to enable \n\nthem to perform their duties and responsibilities consistent with related policies, \n\nprocedures, and agreements. \n\nAbout \n\nTo enhance AI risk management adoption and effectiveness, organizations are encouraged \n\nto identify and integrate appropriate training curricula into enterprise learning \n\nrequirements. Through regular training, AI actors can maintain awareness of: \n\nâ€¢ AI risk management goals and their role in achieving them. \n\nâ€¢ Organizational policies, applicable laws and regulations, and industry best practices and \n\nnorms. \n\nSee [MAP 3.4]() and [3.5]() for additional relevant information. \n\nSuggested Actions \n\nâ€¢ Establish policies for personnel addressing ongoing education about: \n\nâ€¢ Applicable laws and regulations for AI systems. \n\nâ€¢ Potential negative impacts that may arise from AI systems. \n\nâ€¢ Organizational AI policies. \n\nâ€¢ Trustworthy AI characteristics. \n\nâ€¢ Ensure that trainings are suitable across AI actor sub -groups - for AI actors carrying out \n\ntechnical tasks (e.g., developers, operators, etc.) as compared to AI actors in oversight \n\nroles (e.g., legal, compliance, audit, etc.). \n\nâ€¢ Ensure that trainings comprehensively address technical and socio -technical aspects of \n\nAI risk management. \n\nâ€¢ Verify that organizational AI policies include mechanisms for internal AI personnel to \n\nacknowledge and commit to their roles and responsibilities. \n\nâ€¢ Verify that organizational policies address change management and include \n\nmechanisms to communicate and acknowledge substantial AI system changes. \n\nâ€¢ Define paths along internal and external chains of accountability to escalate risk \n\nconcerns. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Are the relevant staff dealing with AI systems properly trained to interpret AI model \n\noutput and decisions as well as to detect and manage bias in data? 18 of 142 \n\nâ€¢ How does the entity determine the necessary skills and experience needed to design, \n\ndevelop, deploy, assess, and monitor the AI system? \n\nâ€¢ How does the entity assess whether personnel have the necessary skills, training, \n\nresources, and domain knowledge to fulfill their assigned responsibilities? \n\nâ€¢ What efforts has the entity undertaken to recruit, develop, and retain a workforce with \n\nbackgrounds, experience, and perspectives that reflect the community impacted by the \n\nAI system? \n\nAI Transparency Resources \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nOff. Comptroller Currency, Comptrollerâ€™s Handbook: Model Risk Management (Aug. 2021). \n\nâ€œDeveloping Staff Trainings for Equitable AI,â€ Partnership on Employment & Accessible \n\nTechnology (PEAT, peatworks.org). \n\n# GOVERN 2.3 \n\nExecutive leadership of the organization takes responsibility for decisions about risks \n\nassociated with AI system development and deployment. \n\nAbout \n\nSenior leadership and members of the C -Suite in organizations that maintain an AI portfolio, \n\nshould maintain awareness of AI risks, affirm the organizational appetite for such risks, and \n\nbe responsible for managing those risks.. \n\nAccountability ensures that a specific team and individual is responsible for AI risk \n\nmanagement efforts. Some organizations grant authority and resources (human and \n\nbudgetary) to a designated officer who ensures adequate performance of the institutionâ€™s AI \n\nportfolio (e.g. predictive modeling, machine learning). \n\nSuggested Actions \n\nâ€¢ Organizational management can: \n\nâ€¢ Declare risk tolerances for developing or using AI systems. \n\nâ€¢ Support AI risk management efforts, and play an active role in such efforts. \n\nâ€¢ Integrate a risk and harm prevention mindset throughout the AI lifecycle as part of \n\norganizational culture \n\nâ€¢ Support competent risk management executives. \n\nâ€¢ Delegate the power, resources, and authorization to perform risk management to \n\neach appropriate level throughout the management chain. 19 of 142 \n\nâ€¢ Organizations can establish board committees for AI risk management and oversight \n\nfunctions and integrate those functions within the organizationâ€™s broader enterprise \n\nrisk management approaches. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Did your organizationâ€™s board and/or senior management sponsor, support and \n\nparticipate in your organizationâ€™s AI governance? \n\nâ€¢ What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\nâ€¢ Do AI solutions provide sufficient information to assist the personnel to make an \n\ninformed decision and take actions accordingly? \n\nâ€¢ To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\nAI Transparency Resources \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011) \n\nOff. Superintendent Fin. Inst. Canada, Enterprise -Wide Model Risk Management for Deposit -\n\nTaking Institutions, E -23 (Sept. 2017). \n\n# GOVERN 3.1 \n\nDecision -makings related to mapping, measuring, and managing AI risks throughout the \n\nlifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, \n\nexperience, expertise, and backgrounds). \n\nAbout \n\nA diverse team that includes AI actors with diversity of experience, disciplines, and \n\nbackgrounds to enhance organizational capacity and capability for anticipating risks is \n\nbetter equipped to carry out risk management. Consultation with external personnel may \n\nbe necessary when internal teams lack a diverse range of lived experiences or disciplinary \n\nexpertise. \n\nTo extend the benefits of diversity, equity, and inclusion to both the users and AI actors, it is \n\nrecommended that teams are composed of a diverse group of individuals who reflect a \n\nrange of backgrounds, perspectives and expertise. \n\nWithout commitment from senior leadership, beneficial aspects of team diversity and \n\ninclusion can be overridden by unstated organizational incentives that inadvertently \n\nconflict with the broader values of a diverse workforce. 20 of 142 \n\nSuggested Actions \n\nOrganizational management can: \n\nâ€¢ Define policies and hiring practices at the outset that promote interdisciplinary roles, \n\ncompetencies, skills, and capacity for AI efforts. \n\nâ€¢ Define policies and hiring practices that lead to demographic and domain expertise \n\ndiversity; empower staff with necessary resources and support, and facilitate the \n\ncontribution of staff feedback and concerns without fear of reprisal. \n\nâ€¢ Establish policies that facilitate inclusivity and the integration of new insights into \n\nexisting practice. \n\nâ€¢ Seek external expertise to supplement organizational diversity, equity, inclusion, and \n\naccessibility where internal expertise is lacking. \n\nâ€¢ Establish policies that incentivize AI actors to collaborate with existing \n\nnondiscrimination, accessibility and accommodation, and human resource functions, \n\nemployee resource group (ERGs), and diversity, equity, inclusion, and accessibility \n\n(DEIA) initiatives. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Are the relevant staff dealing with AI systems properly trained to interpret AI model \n\noutput and decisions as well as to detect and manage bias in data? \n\nâ€¢ Entities include diverse perspectives from technical and non -technical communities \n\nthroughout the AI life cycle to anticipate and mitigate unintended consequences \n\nincluding potential bias and discrimination. \n\nâ€¢ Stakeholder involvement: Include diverse perspectives from a community of \n\nstakeholders throughout the AI life cycle to mitigate risks. \n\nâ€¢ Strategies to incorporate diverse perspectives include establishing collaborative \n\nprocesses and multidisciplinary teams that involve subject matter experts in data \n\nscience, software development, civil liberties, privacy and security, legal counsel, and \n\nrisk management. \n\nâ€¢ To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\nAI Transparency Resources \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nDylan Walsh, â€œHow can human -centered AI fight bias in machines and people?â€ MIT Sloan \n\nMgmt. Rev., 2021. \n\nMichael Li, â€œTo Build Less -Biased AI, Hire a More Diverse Team,â€ Harvard Bus. Rev., 2020. 21 of 142 \n\nBo Cowgill et al., â€œBiased Programmers? Or Biased Data? A Field Experiment in \n\nOperationalizing AI Ethics,â€ 2020. \n\nNaomi Ellemers, Floortje Rink, â€œDiversity in work groups,â€ Current opinion in psychology, \n\nvol. 11, pp. 49 â€“53, 2016. \n\nKatrin Talke, SÃ¸ren Salomo, Alexander Kock, â€œTop management team diversity and strategic \n\ninnovation orientation: The relationship and consequences for innovativeness and \n\nperformance,â€ Journal of Product Innovation Management, vol. 28, pp. 819 â€“832, 2011. \n\nSarah Myers West, Meredith Whittaker, and Kate Crawford,, â€œDiscriminating Systems: \n\nGender, Race, and Power in AI,â€ AI Now Institute, Tech. Rep., 2019. \n\nSina Fazelpour, Maria De -Arteaga, Diversity in sociotechnical machine learning systems. Big \n\nData & Society. January 2022. doi:10.1177/20539517221082027 \n\nMary L. Cummings and Songpo Li, 2021a. Sources of subjectivity in machine learning \n\nmodels. ACM Journal of Data and Information Quality, 13(2), 1 â€“9\n\nâ€œStaffing for Equitable AI: Roles & Responsibilities,â€ Partnership on Employment & \n\nAccessible Technology (PEAT, peatworks.org). Accessed Jan. 6, 2023. \n\n# GOVERN 3.2 \n\nPolicies and procedures are in place to define and differentiate roles and responsibilities for \n\nhuman -AI configurations and oversight of AI systems. \n\nAbout \n\nIdentifying and managing AI risks and impacts are enhanced when a broad set of \n\nperspectives and actors across the AI lifecycle, including technical, legal, compliance, social \n\nscience, and human factors expertise is engaged. AI actors include those who operate, use, \n\nor interact with AI systems for downstream tasks, or monitor AI system performance. \n\nEffective risk management efforts include: \n\nâ€¢ clear definitions and differentiation of the various human roles and responsibilities for \n\nAI system oversight and governance \n\nâ€¢ recognizing and clarifying differences between AI system overseers and those using or \n\ninteracting with AI systems. \n\nSuggested Actions \n\nâ€¢ Establish policies and procedures that define and differentiate the various human roles \n\nand responsibilities when using, interacting with, or monitoring AI systems. \n\nâ€¢ Establish procedures for capturing and tracking risk information related to human -AI \n\nconfigurations and associated outcomes. \n\nâ€¢ Establish policies for the development of proficiency standards for AI actors carrying \n\nout system operation tasks and system oversight tasks. 22 of 142 \n\nâ€¢ Establish specified risk management training protocols for AI actors carrying out \n\nsystem operation tasks and system oversight tasks. \n\nâ€¢ Establish policies and procedures regarding AI actor roles, and responsibilities for \n\nhuman oversight of deployed systems. \n\nâ€¢ Establish policies and procedures defining human -AI configurations (configurations \n\nwhere AI systems are explicitly designated and treated as team members in primarily \n\nhuman teams) in relation to organizational risk tolerances, and associated \n\ndocumentation. \n\nâ€¢ Establish policies to enhance the explanation, interpretation, and overall transparency \n\nof AI systems. \n\nâ€¢ Establish policies for managing risks regarding known difficulties in human -AI \n\nconfigurations, human -AI teaming, and AI system user experience and user interactions \n\n(UI/UX). \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nâ€¢ To what extent has the entity documented the appropriate level of human involvement \n\nin AI -augmented decision -making? \n\nâ€¢ How will the accountable human(s) address changes in accuracy and precision due to \n\neither an adversaryâ€™s attempts to disrupt the AI or unrelated changes in \n\noperational/business environment, which may impact the accuracy of the AI? \n\nâ€¢ To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\nâ€¢ How does the entity assess whether personnel have the necessary skills, training, \n\nresources, and domain knowledge to fulfill their assigned responsibilities? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nReferences \n\nMadeleine Clare Elish, \"Moral Crumple Zones: Cautionary tales in human -robot interaction,\" \n\nEngaging Science, Technology, and Society, Vol. 5, 2019. \n\nâ€œHuman -AI Teaming: State -Of -The -Art and Research Needs,â€ National Academies of \n\nSciences, Engineering, and Medicine, 2022. \n\nBen Green, \"The Flaws Of Policies Requiring Human Oversight Of Government Algorithms,\" \n\nComputer Law & Security Review 45 (2022). 23 of 142 \n\nDavid A. Broniatowski. 2021. Psychological Foundations of Explainability and \n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology \n\n(NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD. \n\nOff. Comptroller Currency, Comptrollerâ€™s Handbook: Model Risk Management (Aug. 2021). \n\n# GOVERN 4.1 \n\nOrganizational policies, and practices are in place to foster a critical thinking and safety -first \n\nmindset in the design, development, deployment, and uses of AI systems to minimize \n\nnegative impacts. \n\nAbout \n\nA risk culture and accompanying practices can help organizations effectively triage the most \n\ncritical risks. Organizations in some industries implement three (or more) â€œlines of \n\ndefense,â€ where separate teams are held accountable for different aspects of the system \n\nlifecycle, such as development, risk management, and auditing. While a traditional three -\n\nlines approach may be impractical for smaller organizations, leadership can commit to \n\ncultivating a strong risk culture through other means. For example, â€œeffective challenge,â€ is a \n\nculture - based practice that encourages critical thinking and questioning of important \n\ndesign and implementation decisions by experts with the authority and stature to make \n\nsuch changes. \n\nRed -teaming is another risk measurement and management approach. This practice \n\nconsists of adversarial testing of AI systems under stress conditions to seek out failure \n\nmodes or vulnerabilities in the system. Red -teams are composed of external experts or \n\npersonnel who are independent from internal AI actors. \n\nSuggested Actions \n\nâ€¢ Establish policies that require inclusion of oversight functions (legal, compliance, risk \n\nmanagement) from the outset of the system design process. \n\nâ€¢ Establish policies that promote effective challenge of AI system design, implementation, \n\nand deployment decisions, via mechanisms such as the three lines of defense, model \n\naudits, or red -teaming â€“ to minimize workplace risks such as groupthink. \n\nâ€¢ Establish policies that incentivize safety -first mindset and general critical thinking and \n\nreview at an organizational and procedural level. \n\nâ€¢ Establish whistleblower protections for insiders who report on perceived serious \n\nproblems with AI systems. \n\nâ€¢ Establish policies to integrate a harm and risk prevention mindset throughout the AI \n\nlifecycle. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent has the entity documented the AI systemâ€™s development, testing \n\nmethodology, metrics, and performance outcomes? 24 of 142 \n\nâ€¢ Are organizational information sharing practices widely followed and transparent, such \n\nthat related past failed designs can be avoided? \n\nâ€¢ Are training manuals and other resources for carrying out incident response \n\ndocumented and available? \n\nâ€¢ Are processes for operator reporting of incidents and near -misses documented and \n\navailable? \n\nâ€¢ How might revealing mismatches between claimed and actual system performance help \n\nusers understand limitations and anticipate risks and impacts?â€ \n\nAI Transparency Resources \n\nâ€¢ Datasheets for Datasets. \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011) \n\nPatrick Hall, Navdeep Gill, and Benjamin Cox, â€œResponsible Machine Learning,â€ Oâ€™Reilly \n\nMedia, 2020. \n\nOff. Superintendent Fin. Inst. Canada, Enterprise -Wide Model Risk Management for Deposit -\n\nTaking Institutions, E -23 (Sept. 2017). \n\nGAO, â€œArtificial Intelligence: An Accountability Framework for Federal Agencies and Other \n\nEntities,â€ GAO@100 (GAO -21 -519SP), June 2021. \n\nDonald Sull, Stefano Turconi, and Charles Sull, â€œWhen It Comes to Culture, Does Your \n\nCompany Walk the Talk?â€ MIT Sloan Mgmt. Rev., 2020. \n\nKathy Baxter, AI Ethics Maturity Model, Salesforce. \n\nUpol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, and Hal DaumÃ©. 2024. Seamful XAI: \n\nOperationalizing Seamful Design in Explainable AI. Proc. ACM Hum. -Comput. Interact. 8, \n\nCSCW1, Article 119. https://doi.org/10.1145/3637396 \n\n# GOVERN 4.2 \n\nOrganizational teams document the risks and potential impacts of the AI technology they \n\ndesign, develop, deploy, evaluate and use, and communicate about the impacts more \n\nbroadly. \n\nAbout \n\nImpact assessments are one approach for driving responsible technology development \n\npractices. And, within a specific use case, these assessments can provide a high -level \n\nstructure for organizations to frame risks of a given algorithm or deployment. Impact 25 of 142 \n\nassessments can also serve as a mechanism for organizations to articulate risks and \n\ngenerate documentation for managing and oversight activities when harms do arise. \n\nImpact assessments may: \n\nâ€¢ be applied at the beginning of a process but also iteratively and regularly since goals \n\nand outcomes can evolve over time. \n\nâ€¢ include perspectives from AI actors, including operators, users, and potentially \n\nimpacted communities (including historically marginalized communities, those with \n\ndisabilities, and individuals impacted by the digital divide), \n\nâ€¢ assist in â€œgo/no -goâ€ decisions for an AI system. \n\nâ€¢ consider conflicts of interest, or undue influence, related to the organizational team \n\nbeing assessed. \n\nSee the MAP function playbook guidance for more information relating to impact \n\nassessments. \n\nSuggested Actions \n\nâ€¢ Establish impact assessment policies and processes for AI systems used by the \n\norganization. \n\nâ€¢ Align organizational impact assessment activities with relevant regulatory or legal \n\nrequirements. \n\nâ€¢ Verify that impact assessment activities are appropriate to evaluate the potential \n\nnegative impact of a system and how quickly a system changes, and that assessments \n\nare applied on a regular basis. \n\nâ€¢ Utilize impact assessments to inform broader evaluations of AI system risk. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ How has the entity identified and mitigated potential impacts of bias in the data, \n\nincluding inequitable or discriminatory outcomes? \n\nâ€¢ How has the entity documented the AI systemâ€™s data provenance, including sources, \n\norigins, transformations, augmentations, labels, dependencies, constraints, and \n\nmetadata? \n\nâ€¢ To what extent has the entity clearly defined technical specifications and requirements \n\nfor the AI system? \n\nâ€¢ To what extent has the entity documented and communicated the AI systemâ€™s \n\ndevelopment, testing methodology, metrics, and performance outcomes? \n\nâ€¢ Have you documented and explained that machine errors may differ from human \n\nerrors? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Datasheets for Datasets. 26 of 142 \n\nReferences \n\nDillon Reisman, Jason Schultz, Kate Crawford, Meredith Whittaker, â€œAlgorithmic Impact \n\nAssessments: A Practical Framework For Public Agency Accountability,â€ AI Now Institute, \n\n2018. \n\nH.R. 2231, 116th Cong. (2019). \n\nBSA The Software Alliance (2021) Confronting Bias: BSAâ€™s Framework to Build Trust in AI. \n\nAnthony M. Barrett, Dan Hendrycks, Jessica Newman and Brandie Nonnecke. Actionable \n\nGuidance for High -Consequence AI Risk Management: Towards Standards Addressing AI \n\nCatastrophic Risks. ArXiv abs/2206.08966 (2022) https://arxiv.org/abs/2206.08966 \n\nDavid Wright, â€œMaking Privacy Impact Assessments More Effective.\" The Information \n\nSociety 29, 2013. \n\nKonstantinia Charitoudi and Andrew Blyth. A Socio -Technical Approach to Cyber Risk \n\nManagement and Impact Assessment. Journal of Information Security 4, 1 (2013), 33 -41. \n\nEmanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, & Jacob Metcalf. \n\n2021. â€œAssembling Accountability: Algorithmic Impact Assessment for the Public Interestâ€. \n\nMicrosoft. Responsible AI Impact Assessment Template. 2022. \n\nMicrosoft. Responsible AI Impact Assessment Guide. 2022. \n\nMicrosoft. Foundations of assessing harm. 2022. \n\nMauritz Kop, â€œAI Impact Assessment & Code of Conduct,â€ Futurium, May 2019. \n\nDillon Reisman, Jason Schultz, Kate Crawford, and Meredith Whittaker, â€œAlgorithmic Impact \n\nAssessments: A Practical Framework For Public Agency Accountability,â€ AI Now, Apr. 2018. \n\nAndrew D. Selbst, â€œAn Institutional View Of Algorithmic Impact Assessments,â€ Harvard \n\nJournal of Law & Technology, vol. 35, no. 1, 2021 \n\nAda Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. \n\nAccessed July 14, 2022. \n\nKathy Baxter, AI Ethics Maturity Model, Salesforce \n\nRavit Dotan, Borhane Blili -Hamelin, Ravi Madhavan, Jeanna Matthews, Joshua Scarpino, & \n\nCarol Anderson. (2024). A Flexible Maturity Model for AI Governance Based on the NIST AI \n\nRisk Management Framework [Technical Report]. IEEE. https://ieeeusa.org/product/a -\n\nflexible -maturity -model -for -ai -governance 27 of 142 \n\n# GOVERN 4.3 \n\nOrganizational practices are in place to enable AI testing, identification of incidents, and \n\ninformation sharing. \n\nAbout \n\nIdentifying AI system limitations, detecting and tracking negative impacts and incidents, \n\nand sharing information about these issues with appropriate AI actors will improve risk \n\nmanagement. Issues such as concept drift, AI bias and discrimination, shortcut learning or \n\nunderspecification are difficult to identify using current standard AI testing processes. \n\nOrganizations can institute in -house use and testing policies and procedures to identify and \n\nmanage such issues. Efforts can take the form of pre -alpha or pre -beta testing, or deploying \n\ninternally developed systems or products within the organization. Testing may entail \n\nlimited and controlled in -house, or publicly available, AI system testbeds, and accessibility \n\nof AI system interfaces and outputs. \n\nWithout policies and procedures that enable consistent testing practices, risk management \n\nefforts may be bypassed or ignored, exacerbating risks or leading to inconsistent risk \n\nmanagement activities. \n\nInformation sharing about impacts or incidents detected during testing or deployment can: \n\nâ€¢ draw attention to AI system risks, failures, abuses or misuses, \n\nâ€¢ allow organizations to benefit from insights based on a wide range of AI applications \n\nand implementations, and \n\nâ€¢ allow organizations to be more proactive in avoiding known failure modes. \n\nOrganizations may consider sharing incident information with the AI Incident Database, the \n\nAIAAIC, users, impacted communities, or with traditional cyber vulnerability databases, \n\nsuch as the MITRE CVE list. \n\nSuggested Actions \n\nâ€¢ Establish policies and procedures to facilitate and equip AI system testing. \n\nâ€¢ Establish organizational commitment to identifying AI system limitations and sharing of \n\ninsights about limitations within appropriate AI actor groups. \n\nâ€¢ Establish policies for reporting and documenting incident response. \n\nâ€¢ Establish policies and processes regarding public disclosure of incidents and \n\ninformation sharing. \n\nâ€¢ Establish guidelines for incident handling related to AI system risks and performance. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? Consulting the community or end users at the earliest 28 of 142 \n\nstages of development to ensure there is transparency on the technology used and how \n\nit is deployed. \n\nâ€¢ Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\nâ€¢ To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nAI Transparency Resources \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nReferences \n\nSean McGregor, â€œPreventing Repeated Real World AI Failures by Cataloging Incidents: The \n\nAI Incident Database,â€ arXiv:2011.08512 [cs], Nov. 2020, arXiv:2011.08512. \n\nChristopher Johnson, Mark Badger, David Waltermire, Julie Snyder, and Clem Skorupka, \n\nâ€œGuide to cyber threat information sharing,â€ National Institute of Standards and Technology, \n\nNIST Special Publication 800 -150, Nov 2016. \n\nMengyi Wei, Zhixuan Zhou (2022). AI Ethics Issues in Real World: Evidence from AI Incident \n\nDatabase. ArXiv, abs/2206.07635. \n\nBSA The Software Alliance (2021) Confronting Bias: BSAâ€™s Framework to Build Trust in AI. \n\nâ€œUsing Combined Expertise to Evaluate Web Accessibility,â€ W3C Web Accessibility Initiative. \n\n# GOVERN 5.1 \n\nOrganizational policies and practices are in place to collect, consider, prioritize, and \n\nintegrate feedback from those external to the team that developed or deployed the AI \n\nsystem regarding the potential individual and societal impacts related to AI risks. \n\nAbout \n\nBeyond internal and laboratory -based system testing, organizational policies and practices \n\nmay consider AI system fitness -for -purpose related to the intended context of use. \n\nParticipatory stakeholder engagement is one type of qualitative activity to help AI actors \n\nanswer questions such as whether to pursue a project or how to design with impact in \n\nmind. This type of feedback, with domain expert input, can also assist AI actors to identify \n\nemergent scenarios and risks in certain AI applications. The consideration of when and how \n\nto convene a group and the kinds of individuals, groups, or community organizations to \n\ninclude is an iterative process connected to the system's purpose and its level of risk. Other \n\nfactors relate to how to collaboratively and respectfully capture stakeholder feedback and \n\ninsight that is useful, without being a solely perfunctory exercise. 29 of 142 \n\nThese activities are best carried out by personnel with expertise in participatory practices, \n\nqualitative methods, and translation of contextual feedback for technical audiences. \n\nParticipatory engagement is not a one -time exercise and is best carried out from the very \n\nbeginning of AI system commissioning through the end of the lifecycle. Organizations can \n\nconsider how to incorporate engagement when beginning a project and as part of their \n\nmonitoring of systems. Engagement is often utilized as a consultative practice, but this \n\nperspective may inadvertently lead to â€œparticipation washing.â€ Organizational transparency \n\nabout the purpose and goal of the engagement can help mitigate that possibility. \n\nOrganizations may also consider targeted consultation with subject matter experts as a \n\ncomplement to participatory findings. Experts may assist internal staff in identifying and \n\nconceptualizing potential negative impacts that were previously not considered. \n\nSuggested Actions \n\nâ€¢ Establish AI risk management policies that explicitly address mechanisms for collecting, \n\nevaluating, and incorporating stakeholder and user feedback that could include: \n\nâ€¢ Recourse mechanisms for faulty AI system outputs. \n\nâ€¢ Bug bounties. \n\nâ€¢ Human -centered design. \n\nâ€¢ User -interaction and experience research. \n\nâ€¢ Participatory stakeholder engagement with individuals and communities that may \n\nexperience negative impacts. \n\nâ€¢ Verify that stakeholder feedback is considered and addressed, including environmental \n\nconcerns, and across the entire population of intended users, including historically \n\nexcluded populations, people with disabilities, older people, and those with limited \n\naccess to the internet and other basic technologies. \n\nâ€¢ Clarify the organizationâ€™s principles as they apply to AI systems â€“ considering those \n\nwhich have been proposed publicly â€“ to inform external stakeholders of the \n\norganizationâ€™s values. Consider publishing or adopting AI principles. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nâ€¢ To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\nâ€¢ How easily accessible and current is the information available to external stakeholders? \n\nâ€¢ What was done to mitigate or reduce the potential for harm? \n\nâ€¢ Stakeholder involvement: Include diverse perspectives from a community of \n\nstakeholders throughout the AI life cycle to mitigate risks. 30 of 142 \n\nAI Transparency Resources \n\nâ€¢ Datasheets for Datasets. \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\nâ€¢ Stakeholders in Explainable AI, Sep. 2018. \n\nReferences \n\nISO, â€œErgonomics of human -system interaction â€” Part 210: Human -centered design for \n\ninteractive systems,â€ ISO 9241 -210:2019 (2nd ed.), July 2019. \n\nRumman Chowdhury and Jutta Williams, \"Introducing Twitterâ€™s first algorithmic bias \n\nbounty challenge,\" \n\nLeonard Haas and Sebastian GieÃŸler, â€œIn the realm of paper tigers â€“ exploring the failings of \n\nAI ethics guidelines,â€ AlgorithmWatch, 2020. \n\nJosh Kenway, Camille Francois, Dr. Sasha Costanza -Chock, Inioluwa Deborah Raji, & Dr. Joy \n\nBuolamwini. 2022. Bug Bounties for Algorithmic Harms? Algorithmic Justice League. \n\nAccessed July 14, 2022. \n\nMicrosoft Community Jury , Azure Application Architecture Guide. \n\nâ€œDefinition of independent verification and validation (IV&V)â€, in IEEE 1012, IEEE Standard \n\nfor System, Software, and Hardware Verification and Validation. Annex C, \n\n# GOVERN 5.2 \n\nMechanisms are established to enable AI actors to regularly incorporate adjudicated \n\nfeedback from relevant AI actors into system design and implementation. \n\nAbout \n\nOrganizational policies and procedures that equip AI actors with the processes, knowledge, \n\nand expertise needed to inform collaborative decisions about system deployment improve \n\nrisk management. These decisions are closely tied to AI systems and organizational risk \n\ntolerance. \n\nRisk tolerance, established by organizational leadership, reflects the level and type of risk \n\nthe organization will accept while conducting its mission and carrying out its strategy. \n\nWhen risks arise, resources are allocated based on the assessed risk of a given AI system. \n\nOrganizations typically apply a risk tolerance approach where higher risk systems receive \n\nlarger allocations of risk management resources and lower risk systems receive less \n\nresources. \n\nSuggested Actions \n\nâ€¢ Explicitly acknowledge that AI systems, and the use of AI, present inherent costs and \n\nrisks along with potential benefits. 31 of 142 \n\nâ€¢ Define reasonable risk tolerances for AI systems informed by laws, regulation, best \n\npractices, or industry standards. \n\nâ€¢ Establish policies that ensure all relevant AI actors are provided with meaningful \n\nopportunities to provide feedback on system design and implementation. \n\nâ€¢ Establish policies that define how to assign AI systems to established risk tolerance \n\nlevels by combining system impact assessments with the likelihood that an impact \n\noccurs. Such assessment often entails some combination of: \n\nâ€¢ Econometric evaluations of impacts and impact likelihoods to assess AI system risk. \n\nâ€¢ Red -amber -green (RAG) scales for impact severity and likelihood to assess AI \n\nsystem risk. \n\nâ€¢ Establishment of policies for allocating risk management resources along \n\nestablished risk tolerance levels, with higher -risk systems receiving more risk \n\nmanagement resources and oversight. \n\nâ€¢ Establishment of policies for approval, conditional approval, and disapproval of the \n\ndesign, implementation, and deployment of AI systems. \n\nâ€¢ Establish policies facilitating the early decommissioning of AI systems that surpass an \n\norganizationâ€™s ability to reasonably mitigate risks. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Who is ultimately responsible for the decisions of the AI and is this person aware of the \n\nintended uses and limitations of the analytic? \n\nâ€¢ Who will be responsible for maintaining, re -verifying, monitoring, and updating this AI \n\nonce deployed? \n\nâ€¢ Who is accountable for the ethical considerations during all stages of the AI lifecycle? \n\nâ€¢ To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\nâ€¢ Does the AI solution provide sufficient information to assist the personnel to make an \n\ninformed decision and take actions accordingly? \n\nAI Transparency Resources \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nâ€¢ Stakeholders in Explainable AI, Sep. 2018. \n\nâ€¢ AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011) \n\nOff. Comptroller Currency, Comptrollerâ€™s Handbook: Model Risk Management (Aug. 2021). 32 of 142 \n\nThe Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, \n\n2019). Retrieved on July 12, 2022. \n\n# GOVERN 6.1 \n\nPolicies and procedures are in place that address AI risks associated with third -party \n\nentities, including risks of infringement of a third partyâ€™s intellectual property or other \n\nrights. \n\nAbout \n\nRisk measurement and management can be complicated by how customers use or integrate \n\nthird -party data or systems into AI products or services, particularly without sufficient \n\ninternal governance structures and technical safeguards. \n\nOrganizations usually engage multiple third parties for external expertise, data, software \n\npackages (both open source and commercial), and software and hardware platforms across \n\nthe AI lifecycle. This engagement has beneficial uses and can increase complexities of risk \n\nmanagement efforts. \n\nOrganizational approaches to managing third -party (positive and negative) risk may be \n\ntailored to the resources, risk profile, and use case for each system. Organizations can apply \n\ngovernance approaches to third -party AI systems and data as they would for internal \n\nresources â€” including open source software, publicly available data, and commercially \n\navailable models. \n\nSuggested Actions \n\nâ€¢ Collaboratively establish policies that address third -party AI systems and data. \n\nâ€¢ Establish policies related to: \n\nâ€¢ Transparency into third -party system functions, including knowledge about training \n\ndata, training and inference algorithms, and assumptions and limitations. \n\nâ€¢ Thorough testing of third -party AI systems. (See MEASURE for more detail) \n\nâ€¢ Requirements for clear and complete instructions for third -party system usage. \n\nâ€¢ Evaluate policies for third -party technology. \n\nâ€¢ Establish policies that address supply chain, full product lifecycle and associated \n\nprocesses, including legal, ethical, and other issues concerning procurement and use of \n\nthird -party software or hardware systems and data. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Did you establish mechanisms that facilitate the AI systemâ€™s auditability (e.g. \n\ntraceability of the development process, the sourcing of training data and the logging of \n\nthe AI systemâ€™s processes, outcomes, positive and negative impact)? 33 of 142 \n\nâ€¢ If a third party created the AI, how will you ensure a level of explainability or \n\ninterpretability? \n\nâ€¢ Did you ensure that the AI system can be audited by independent third parties? \n\nâ€¢ Did you establish a process for third parties (e.g. suppliers, end users, subjects, \n\ndistributors/vendors or workers) to report potential vulnerabilities, risks or biases in \n\nthe AI system? \n\nâ€¢ To what extent does the plan specifically address risks associated with acquisition, \n\nprocurement of packaged software from vendors, cybersecurity controls, computational \n\ninfrastructure, data, data science, deployment mechanics, and system failure? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nâ€¢ AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\nâ€¢ Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI -\n\n2019. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011) \n\nâ€œProposed Interagency Guidance on Third -Party Relationships: Risk Management,â€ 2021. \n\nOff. Comptroller Currency, Comptrollerâ€™s Handbook: Model Risk Management (Aug. 2021). \n\n# GOVERN 6.2 \n\nContingency processes are in place to handle failures or incidents in third -party data or AI \n\nsystems deemed to be high -risk. \n\nAbout \n\nTo mitigate the potential harms of third -party system failures, organizations may \n\nimplement policies and procedures that include redundancies for covering third -party \n\nfunctions. \n\nSuggested Actions \n\nâ€¢ Establish policies for handling third -party system failures to include consideration of \n\nredundancy mechanisms for vital third -party AI systems. \n\nâ€¢ Verify that incident response plans address third -party AI systems. 34 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent does the plan specifically address risks associated with acquisition, \n\nprocurement of packaged software from vendors, cybersecurity controls, computational \n\ninfrastructure, data, data science, deployment mechanics, and system failure? \n\nâ€¢ Did you establish a process for third parties (e.g. suppliers, end users, subjects, \n\ndistributors/vendors or workers) to report potential vulnerabilities, risks or biases in \n\nthe AI system? \n\nâ€¢ If your organization obtained datasets from a third party, did your organization assess \n\nand manage the risks of using such datasets? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nâ€¢ AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\nReferences \n\nBd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter \n\n11 -7 (Apr. 4, 2011) \n\nâ€œProposed Interagency Guidance on Third -Party Relationships: Risk Management,â€ 2021. \n\nOff. Comptroller Currency, Comptrollerâ€™s Handbook: Model Risk Management (Aug. 2021). MANAGE 35 of 142 \n\n# Manage \n\nAI risks based on assessments and other analytical output from the Map and Measure \n\nfunctions are prioritized, responded to, and managed. \n\n# MANAGE 1.1 \n\nA determination is made as to whether the AI system achieves its intended purpose and \n\nstated objectives and whether its development or deployment should proceed. \n\nAbout \n\nAI systems may not necessarily be the right solution for a given business task or problem. A \n\nstandard risk management practice is to formally weigh an AI systemâ€™s negative risks \n\nagainst its benefits, and to determine if the AI system is an appropriate solution. Tradeoffs \n\namong trustworthiness characteristics â€”such as deciding to deploy a system based on \n\nsystem performance vs system transparency â€“may require regular assessment throughout \n\nthe AI lifecycle. \n\nSuggested Actions \n\nâ€¢ Consider trustworthiness characteristics when evaluating AI systemsâ€™ negative risks \n\nand benefits. \n\nâ€¢ Utilize TEVV outputs from map and measure functions when considering risk treatment. \n\nâ€¢ Regularly track and monitor negative risks and benefits throughout the AI system \n\nlifecycle including in post -deployment monitoring. \n\nâ€¢ Regularly assess and document system performance relative to trustworthiness \n\ncharacteristics and tradeoffs between negative risks and opportunities. \n\nâ€¢ Evaluate tradeoffs in connection with real -world use cases and impacts and as \n\nenumerated in Map function outcomes. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ How do the technical specifications and requirements align with the AI systemâ€™s goals \n\nand objectives? \n\nâ€¢ To what extent are the metrics consistent with system goals, objectives, and constraints, \n\nincluding ethical and compliance considerations? \n\nâ€¢ What goals and objectives does the entity expect to achieve by designing, developing, \n\nand/or deploying the AI system? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF Companion to the Model AI Governance Framework â€“ Implementation and Self -\n\nAssessment Guide for Organizations 36 of 142 \n\nReferences \n\nArvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. \n\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, \n\n2021). \n\nFraser, Henry L and Bello y Villarino, Jose -Miguel, Where Residual Risks Reside: A \n\nComparative Approach to Art 9(4) of the European Union's Proposed AI Regulation \n\n(September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), \n\nMicrosoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. \n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* \n\n'20). Association for Computing Machinery, New York, NY, USA, 695. \n\n# MANAGE 1.2 \n\nTreatment of documented AI risks is prioritized based on impact, likelihood, or available \n\nresources or methods. \n\nAbout \n\nRisk refers to the composite measure of an eventâ€™s probability of occurring and the \n\nmagnitude (or degree) of the consequences of the corresponding events. The impacts, or \n\nconsequences, of AI systems can be positive, negative, or both and can result in \n\nopportunities or risks. \n\nOrganizational risk tolerances are often informed by several internal and external factors, \n\nincluding existing industry practices, organizational values, and legal or regulatory \n\nrequirements. Since risk management resources are often limited, organizations usually \n\nassign them based on risk tolerance. AI risks that are deemed more serious receive more \n\noversight attention and risk management resources. \n\nSuggested Actions \n\nâ€¢ Assign risk management resources relative to established risk tolerance. AI systems \n\nwith lower risk tolerances receive greater oversight, mitigation and management \n\nresources. \n\nâ€¢ Document AI risk tolerance determination practices and resource decisions. \n\nâ€¢ Regularly review risk tolerances and re -calibrate, as needed, in accordance with \n\ninformation from AI system monitoring and assessment . 37 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\nâ€¢ What assessments has the entity conducted on data security and privacy impacts \n\nassociated with the AI system? \n\nâ€¢ Does your organization have an existing governance structure that can be leveraged to \n\noversee the organizationâ€™s use of AI? \n\nAI Transparency Resources \n\nâ€¢ WEF Companion to the Model AI Governance Framework â€“ Implementation and Self -\n\nAssessment Guide for Organizations \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nReferences \n\nArvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. \n\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, \n\n2021). \n\nFraser, Henry L and Bello y Villarino, Jose -Miguel, Where Residual Risks Reside: A \n\nComparative Approach to Art 9(4) of the European Union's Proposed AI Regulation \n\n(September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), \n\nMicrosoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. \n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* \n\n'20). Association for Computing Machinery, New York, NY, USA, 695. \n\n# MANAGE 1.3 \n\nResponses to the AI risks deemed high priority as identified by the Map function, are \n\ndeveloped, planned, and documented. Risk response options can include mitigating, \n\ntransferring, avoiding, or accepting. 38 of 142 \n\nAbout \n\nOutcomes from GOVERN -1, MAP -5 and MEASURE -2, can be used to address and document \n\nidentified risks based on established risk tolerances. Organizations can follow existing \n\nregulations and guidelines for risk criteria, tolerances and responses established by \n\norganizational, domain, discipline, sector, or professional requirements. In lieu of such \n\nguidance, organizations can develop risk response plans based on strategies such as \n\naccepted model risk management, enterprise risk management, and information sharing \n\nand disclosure practices. \n\nSuggested Actions \n\nâ€¢ Observe regulatory and established organizational, sector, discipline, or professional \n\nstandards and requirements for applying risk tolerances within the organization. \n\nâ€¢ Document procedures for acting on AI system risks related to trustworthiness \n\ncharacteristics. \n\nâ€¢ Prioritize risks involving physical safety, legal liabilities, regulatory compliance, and \n\nnegative impacts on individuals, groups, or society. \n\nâ€¢ Identify risk response plans and resources and organizational teams for carrying out \n\nresponse functions. \n\nâ€¢ Store risk management and system documentation in an organized, secure repository \n\nthat is accessible by relevant AI Actors and appropriate personnel. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Has the system been reviewed to ensure the AI system complies with relevant laws, \n\nregulations, standards, and guidance? \n\nâ€¢ To what extent has the entity defined and documented the regulatory environment â€”\n\nincluding minimum requirements in laws and regulations? \n\nâ€¢ Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nArvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). 39 of 142 \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. \n\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, \n\n2021). \n\nFraser, Henry L and Bello y Villarino, Jose -Miguel, Where Residual Risks Reside: A \n\nComparative Approach to Art 9(4) of the European Union's Proposed AI Regulation \n\n(September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), \n\nMicrosoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. \n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* \n\n'20). Association for Computing Machinery, New York, NY, USA, 695. \n\n# MANAGE 1.4 \n\nNegative residual risks (defined as the sum of all unmitigated risks) to both downstream \n\nacquirers of AI systems and end users are documented. \n\nAbout \n\nOrganizations may choose to accept or transfer some of the documented risks from MAP \n\nand MANAGE 1.3 and 2.1. Such risks, known as residual risk, may affect downstream AI \n\nactors such as those engaged in system procurement or use. Transparent monitoring and \n\nmanaging residual risks enables cost benefit analysis and the examination of potential \n\nvalues of AI systems versus its potential negative impacts. \n\nSuggested Actions \n\nâ€¢ Document residual risks within risk response plans, denoting risks that have been \n\naccepted, transferred, or subject to minimal mitigation. \n\nâ€¢ Establish procedures for disclosing residual risks to relevant downstream AI actors . \n\nâ€¢ Inform relevant downstream AI actors of requirements for safe operation, known \n\nlimitations, and suggested warning labels as identified in MAP 3.4. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\nâ€¢ Who will be responsible for maintaining, re -verifying, monitoring, and updating this AI \n\nonce deployed? \n\nâ€¢ How will updates/revisions be documented and communicated? How often and by \n\nwhom? \n\nâ€¢ How easily accessible and current is the information available to external stakeholders? 40 of 142 \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nArvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. \n\nAssembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, \n\n2021). \n\nFraser, Henry L and Bello y Villarino, Jose -Miguel, Where Residual Risks Reside: A \n\nComparative Approach to Art 9(4) of the European Union's Proposed AI Regulation \n\n(September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), \n\nMicrosoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. \n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* \n\n'20). Association for Computing Machinery, New York, NY, USA, 695. \n\n# MANAGE 2.1 \n\nResources required to manage AI risks are taken into account, along with viable non -AI \n\nalternative systems, approaches, or methods â€“ to reduce the magnitude or likelihood of \n\npotential impacts. \n\nAbout \n\nOrganizational risk response may entail identifying and analyzing alternative approaches, \n\nmethods, processes or systems, and balancing tradeoffs between trustworthiness \n\ncharacteristics and how they relate to organizational principles and societal values. Analysis \n\nof these tradeoffs is informed by consulting with interdisciplinary organizational teams, \n\nindependent domain experts, and engaging with individuals or community groups. These \n\nprocesses require sufficient resource allocation. \n\nSuggested Actions \n\nâ€¢ Plan and implement risk management practices in accordance with established \n\norganizational risk tolerances. \n\nâ€¢ Verify risk management teams are resourced to carry out functions, including 41 of 142 \n\nâ€¢ Establishing processes for considering methods that are not automated; semi -\n\nautomated; or other procedural alternatives for AI functions. \n\nâ€¢ Enhance AI system transparency mechanisms for AI teams. \n\nâ€¢ Enable exploration of AI system limitations by AI teams. \n\nâ€¢ Identify, assess, and catalog past failed designs and negative impacts or outcomes to \n\navoid known failure modes. \n\nâ€¢ Identify resource allocation approaches for managing risks in systems: \n\nâ€¢ deemed high -risk, \n\nâ€¢ that self -update (adaptive, online, reinforcement self -supervised learning or \n\nsimilar), \n\nâ€¢ trained without access to ground truth (unsupervised, semi -supervised, learning or \n\nsimilar), \n\nâ€¢ with high uncertainty or where risk management is insufficient. \n\nâ€¢ Regularly seek and integrate external expertise and perspectives to supplement \n\norganizational diversity (e.g. demographic, disciplinary), equity, inclusion, and \n\naccessibility where internal capacity is lacking. \n\nâ€¢ Enable and encourage regular, open communication and feedback among AI actors and \n\ninternal or external stakeholders related to system design or deployment decisions. \n\nâ€¢ Prepare and document plans for continuous monitoring and feedback mechanisms. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Are mechanisms in place to evaluate whether internal teams are empowered and \n\nresourced to effectively carry out risk management functions? \n\nâ€¢ How will user and other forms of stakeholder engagement be integrated into risk \n\nmanagement processes? \n\nAI Transparency Resources \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ Datasheets for Datasets. \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nReferences \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nDavid Wright. 2013. Making Privacy Impact Assessments More Effective. The Information \n\nSociety, 29 (Oct 2013), 307 -315. 42 of 142 \n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model \n\nReporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency \n\n(FAT* '19). Association for Computing Machinery, New York, NY, USA, 220 â€“229. \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. \n\narXiv:1803.09010. \n\n# MANAGE 2.2 \n\nMechanisms are in place and applied to sustain the value of deployed AI systems. \n\nAbout \n\nSystem performance and trustworthiness may evolve and shift over time, once an AI system \n\nis deployed and put into operation. This phenomenon, generally known as drift, can degrade \n\nthe value of the AI system to the organization and increase the likelihood of negative \n\nimpacts. Regular monitoring of AI systemsâ€™ performance and trustworthiness enhances \n\norganizationsâ€™ ability to detect and respond to drift, and thus sustain an AI systemâ€™s value \n\nonce deployed. Processes and mechanisms for regular monitoring address system \n\nfunctionality and behavior - as well as impacts and alignment with the values and norms \n\nwithin the specific context of use. For example, considerations regarding impacts on \n\npersonal or public safety or privacy may include limiting high speeds when operating \n\nautonomous vehicles or restricting illicit content recommendations for minors. \n\nRegular monitoring activities can enable organizations to systematically and proactively \n\nidentify emergent risks and respond according to established protocols and metrics. \n\nOptions for organizational responses include 1) avoiding the risk, 2)accepting the risk, 3) \n\nmitigating the risk, or 4) transferring the risk. Each of these actions require planning and \n\nresources. Organizations are encouraged to establish risk management protocols with \n\nconsideration of the trustworthiness characteristics, the deployment context, and real \n\nworld impacts. \n\nSuggested Actions \n\nâ€¢ Establish risk controls considering trustworthiness characteristics, including: \n\nâ€¢ Data management, quality, and privacy (e.g. minimization, rectification or \n\ndeletion requests) controls as part of organizational data governance policies. \n\nâ€¢ Machine learning and end -point security countermeasures (e.g., robust models, \n\ndifferential privacy, authentication, throttling). \n\nâ€¢ Business rules that augment, limit or restrict AI system outputs within certain \n\ncontexts \n\nâ€¢ Utilizing domain expertise related to deployment context for continuous \n\nimprovement and TEVV across the AI lifecycle. \n\nâ€¢ Development and regular tracking of human -AI teaming configurations. 43 of 142 \n\nâ€¢ Model assessment and test, evaluation, validation and verification (TEVV) \n\nprotocols. \n\nâ€¢ Use of standardized documentation and transparency mechanisms. \n\nâ€¢ Software quality assurance practices across AI lifecycle. \n\nâ€¢ Mechanisms to explore system limitations and avoid past failed designs or \n\ndeployments. \n\nâ€¢ Establish mechanisms to capture feedback from system end users and potentially \n\nimpacted groups while system is in deployment. \n\nâ€¢ stablish mechanisms to capture feedback from system end users and potentially \n\nimpacted groups about how changes in system deployment (e.g., introducing new \n\ntechnology, decommissioning algorithms and models, adapting system, model or \n\nalgorithm) may create negative impacts that are not visible along the AI lifecycle. \n\nâ€¢ Review insurance policies, warranties, or contracts for legal or oversight requirements \n\nfor risk transfer procedures. \n\nâ€¢ Document risk tolerance decisions and risk acceptance procedures. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nâ€¢ Could the AI system expose people to harm or negative impacts? What was done to \n\nmitigate or reduce the potential for harm? \n\nâ€¢ How will the accountable human(s) address changes in accuracy and precision due to \n\neither an adversaryâ€™s attempts to disrupt the AI or unrelated changes in the operational \n\nor business environment? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nSafety, Validity and Reliability Risk Management Approaches and Resources \n\nAI Incident Database. 2022. AI Incident Database. \n\nAIAAIC Repository. 2022. AI, algorithmic and automation incidents collected, dissected, \n\nexamined, and divulged. \n\nAlexander D'Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification \n\nPresents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. 44 of 142 \n\nAndrew L. Beam, Arjun K. Manrai, Marzyeh Ghassemi. 2020. Challenges to the \n\nReproducibility of Machine Learning Models in Health Care. Jama 323, 4 (January 6, 2020), \n\n305 -306. \n\nAnthony M. Barrett, Dan Hendrycks, Jessica Newman et al. 2022. Actionable Guidance for \n\nHigh -Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic \n\nRisks. arXiv:2206.08966. \n\nDebugging Machine Learning Models, In Proceedings of ICLR 2019 Workshop, May 6, 2019, \n\nNew Orleans, Louisiana. \n\nJessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, \n\nExploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. \n\nJoelle Pineau, Philippe Vincent -Lamarre, Koustuv Sinha, et al. 2020. Improving \n\nReproducibility in Machine Learning Research (A Report from the NeurIPS 2019 \n\nReproducibility Program) arXiv:2003.12206. \n\nKirstie Whitaker. 2017. Showing your working: a how to guide to reproducible research. \n\n(August 2017). \n\n[LINK](https://github.com/WhitakerLab/ReproducibleResearch/blob/master/PRESENTA \n\nTIONS/Whitaker_ICON_August2017.pdf), \n\nNetflix. Chaos Monkey. \n\nPeter Henderson, Riashat Islam, Philip Bachman, et al. 2018. Deep reinforcement learning \n\nthat matters. Proceedings of the AAAI Conference on Artificial Intelligence. 32, 1 (Apr. \n\n2018). \n\nSuchi Saria, Adarsh Subbaswamy. 2019. Tutorial: Safe and Reliable Machine Learning. \n\narXiv:1904.07204. \n\nKang, Daniel, Deepti Raghavan, Peter Bailis, and Matei Zaharia. \"Model assertions for \n\nmonitoring and improving ML models.\" Proceedings of Machine Learning and Systems 2 \n\n(2020): 481 -496. \n\nManaging Risk Bias \n\nNational Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et \n\nal. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing \n\nBias in Artificial Intelligence. \n\nBias Testing and Remediation Approaches \n\nAlekh Agarwal, Alina Beygelzimer, Miroslav DudÃ­k, et al. 2018. A Reductions Approach to \n\nFair Classification. arXiv:1803.02453. \n\nBrian Hu Zhang, Blake Lemoine, Margaret Mitchell. 2018. Mitigating Unwanted Biases with \n\nAdversarial Learning. arXiv:1801.07593. 45 of 142 \n\nDrago PleÄko, Nicolas Bennett, Nicolai Meinshausen. 2021. Fairadapt: Causal Reasoning for \n\nFair Data Pre -processing. arXiv:2110.10200. \n\nFaisal Kamiran, Toon Calders. 2012. Data Preprocessing Techniques for Classification \n\nwithout Discrimination. Knowledge and Information Systems 33 (2012), 1 â€“33. \n\nFaisal Kamiran; Asim Karim; Xiangliang Zhang. 2012. Decision Theory for Discrimination -\n\nAware Classification. In Proceedings of the 2012 IEEE 12th International Conference on \n\nData Mining, December 10 -13, 2012, Brussels, Belgium. IEEE, 924 -929. \n\nFlavio P. Calmon, Dennis Wei, Karthikeyan Natesan Ramamurthy, et al. 2017. Optimized \n\nData Pre -Processing for Discrimination Prevention. arXiv:1704.03354. \n\nGeoff Pleiss, Manish Raghavan, Felix Wu, et al. 2017. On Fairness and Calibration. \n\narXiv:1709.02012. \n\nL. Elisa Celis, Lingxiao Huang, Vijay Keswani, et al. 2020. Classification with Fairness \n\nConstraints: A Meta -Algorithm with Provable Guarantees. arXiv:1806.06055. \n\nMichael Feldman, Sorelle Friedler, John Moeller, et al. 2014. Certifying and Removing \n\nDisparate Impact. arXiv:1412.3756. \n\nMichael Kearns, Seth Neel, Aaron Roth, et al. 2017. Preventing Fairness Gerrymandering: \n\nAuditing and Learning for Subgroup Fairness. arXiv:1711.05144. \n\nMichael Kearns, Seth Neel, Aaron Roth, et al. 2018. An Empirical Study of Rich Subgroup \n\nFairness for Machine Learning. arXiv:1808.08166. \n\nMoritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of Opportunity in Supervised \n\nLearning. In Proceedings of the 30th Conference on Neural Information Processing Systems \n\n(NIPS 2016), 2016, Barcelona, Spain. \n\nRich Zemel, Yu Wu, Kevin Swersky, et al. 2013. Learning Fair Representations. In \n\nProceedings of the 30th International Conference on Machine Learning 2013, PMLR 28, 3, \n\n325 -333. \n\nToshihiro Kamishima, Shotaro Akaho, Hideki Asoh & Jun Sakuma. 2012. Fairness -Aware \n\nClassifier with Prejudice Remover Regularizer. In Peter A. Flach, Tijl De Bie, Nello Cristianini \n\n(eds) Machine Learning and Knowledge Discovery in Databases. European Conference \n\nECML PKDD 2012, Proceedings Part II, September 24 -28, 2012, Bristol, UK. Lecture Notes in \n\nComputer Science 7524. Springer, Berlin, Heidelberg. \n\nSecurity and Resilience Resources \n\nFTC Start With Security Guidelines. 2015. \n\nGary McGraw et al. 2022. BIML Interactive Machine Learning Risk Framework. Berryville \n\nInstitute for Machine Learning. 46 of 142 \n\nIlia Shumailov, Yiren Zhao, Daniel Bates, et al. 2021. Sponge Examples: Energy -Latency \n\nAttacks on Neural Networks. arXiv:2006.03463. \n\nMarco Barreno, Blaine Nelson, Anthony D. Joseph, et al. 2010. The Security of Machine \n\nLearning. Machine Learning 81 (2010), 121 -148. \n\nMatt Fredrikson, Somesh Jha, Thomas Ristenpart. 2015. Model Inversion Attacks that \n\nExploit Confidence Information and Basic Countermeasures. In Proceedings of the 22nd \n\nACM SIGSAC Conference on Computer and Communications Security (CCS '15), October \n\n2015. Association for Computing Machinery, New York, NY, USA, 1322 â€“1333. \n\nNational Institute for Standards and Technology (NIST). 2022. Cybersecurity Framework. \n\nNicolas Papernot. 2018. A Marauder's Map of Security and Privacy in Machine Learning. \n\narXiv:1811.01134. \n\nReza Shokri, Marco Stronati, Congzheng Song, et al. 2017. Membership Inference Attacks \n\nagainst Machine Learning Models. arXiv:1610.05820. \n\nAdversarial Threat Matrix (MITRE). 2021. \n\nInterpretability and Explainability Approaches \n\nChaofan Chen, Oscar Li, Chaofan Tao, et al. 2019. This Looks Like That: Deep Learning for \n\nInterpretable Image Recognition. arXiv:1806.10574. \n\nCynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes \n\ndecisions and use interpretable models instead. arXiv:1811.10154. \n\nDaniel W. Apley, Jingyu Zhu. 2019. Visualizing the Effects of Predictor Variables in Black Box \n\nSupervised Learning Models. arXiv:1612.08468. \n\nDavid A. Broniatowski. 2021. Psychological Foundations of Explainability and \n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology \n\n(NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD. \n\nForough Poursabzi -Sangdeh, Daniel G. Goldstein, Jake M. Hofman, et al. 2021. Manipulating \n\nand Measuring Model Interpretability. arXiv:1802.07810. \n\nHongyu Yang, Cynthia Rudin, Margo Seltzer. 2017. Scalable Bayesian Rule Lists. \n\narXiv:1602.08610. \n\nP. Jonathon Phillips, Carina A. Hahn, Peter C. Fontana, et al. 2021. Four Principles of \n\nExplainable Artificial Intelligence. National Institute of Standards and Technology (NIST) IR \n\n8312. National Institute of Standards and Technology, Gaithersburg, MD. \n\nScott Lundberg, Su -In Lee. 2017. A Unified Approach to Interpreting Model Predictions. \n\narXiv:1705.07874. 47 of 142 \n\nSusanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in \n\ndeployment of clinical decision -aids. npj Digital Medicine 4, Article 31 (2021). \n\nYin Lou, Rich Caruana, Johannes Gehrke, et al. 2013. Accurate intelligible models with \n\npairwise interactions. In Proceedings of the 19th ACM SIGKDD international conference on \n\nKnowledge discovery and data mining (KDD '13), August 2013. Association for Computing \n\nMachinery, New York, NY, USA, 623 â€“631. \n\nPost -Decommission \n\nUpol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. â€œThe Algorithmic Imprint.â€ \n\nProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency \n\n(2022). \n\nPrivacy Resources \n\nNational Institute for Standards and Technology (NIST). 2022. Privacy Framework. \n\nData Governance \n\nMarijn Janssen, Paul Brous, Elsa Estevez, Luis S. Barbosa, Tomasz Janowski, Data \n\ngovernance: Organizing data for trustworthy Artificial Intelligence, Government \n\nInformation Quarterly, Volume 37, Issue 3, 2020, 101493, ISSN 0740 -624X. \n\nSoftware Resources \n\nâ€¢ PiML  (explainable models, performance assessment) \n\nâ€¢ Interpret  (explainable models) \n\nâ€¢ Iml  (explainable models) \n\nâ€¢ Drifter  library (performance assessment) \n\nâ€¢ Manifold  library (performance assessment) \n\nâ€¢ SALib  library (performance assessment) \n\nâ€¢ What -If Tool  (performance assessment) \n\nâ€¢ MLextend  (performance assessment) \n\n- AI Fairness 360: \n\nâ€¢ Python  (bias testing and mitigation) \n\nâ€¢ R (bias testing and mitigation) \n\nâ€¢ Adversarial -robustness -toolbox  (ML security) \n\nâ€¢ Robustness  (ML security) \n\nâ€¢ tensorflow/privacy  (ML security) \n\nâ€¢ NIST De -identification Tools  (Privacy and ML security) \n\nâ€¢ Dvc  (MLops, deployment) \n\nâ€¢ Gigantum  (MLops, deployment) \n\nâ€¢ Mlflow  (MLops, deployment) \n\nâ€¢ Mlmd  (MLops, deployment) \n\nâ€¢ Modeldb  (MLops, deployment) 48 of 142 \n\n# MANAGE 2.3 \n\nProcedures are followed to respond to and recover from a previously unknown risk when it \n\nis identified. \n\nAbout \n\nAI systems â€“ like any technology â€“ can demonstrate non -functionality or failure or \n\nunexpected and unusual behavior. They also can be subject to attacks, incidents, or other \n\nmisuse or abuse â€“ which their sources are not always known apriori. Organizations can \n\nestablish, document, communicate and maintain treatment procedures to recognize and \n\ncounter, mitigate and manage risks that were not previously identified. \n\nSuggested Actions \n\nâ€¢ Protocols, resources, and metrics are in place for continual monitoring of AI systemsâ€™ \n\nperformance, trustworthiness, and alignment with contextual norms and values \n\nâ€¢ Establish and regularly review treatment and response plans for incidents, negative \n\nimpacts, or outcomes. \n\nâ€¢ Establish and maintain procedures to regularly monitor system components for drift, \n\ndecontextualization, or other AI system behavior factors, \n\nâ€¢ Establish and maintain procedures for capturing feedback about negative impacts. \n\nâ€¢ Verify contingency processes to handle any negative impacts associated with mission -\n\ncritical AI systems, and to deactivate systems. \n\nâ€¢ Enable preventive and post -hoc exploration of AI system limitations by relevant AI actor \n\ngroups. \n\nâ€¢ Decommission systems that exceed risk tolerances. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Who will be responsible for maintaining, re -verifying, monitoring, and updating this AI \n\nonce deployed? \n\nâ€¢ Are the responsibilities of the personnel involved in the various AI governance \n\nprocesses clearly defined? (Including responsibilities to decommission the AI system.) \n\nâ€¢ What processes exist for data generation, acquisition/collection, ingestion, \n\nstaging/storage, transformations, security, maintenance, and dissemination? \n\nâ€¢ How will the appropriate performance metrics, such as accuracy, of the AI be monitored \n\nafter the AI is deployed? \n\nAI Transparency Resources \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF - Companion to the Model AI Governance Framework â€“ Implementation and Self -\n\nAssessment Guide for Organizations. \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. 49 of 142 \n\nReferences \n\nAI Incident Database. 2022. AI Incident Database. \n\nAIAAIC Repository. 2022. AI, algorithmic and automation incidents collected, dissected, \n\nexamined, and divulged. \n\nAndrew Burt and Patrick Hall. 2018. What to Do When AI Fails. Oâ€™Reilly Media, Inc. (May 18, \n\n2020). Retrieved October 17, 2022. \n\nNational Institute for Standards and Technology (NIST). 2022. Cybersecurity Framework. \n\nSANS Institute. 2022. Security Consensus Operational Readiness Evaluation (SCORE) \n\nSecurity Checklist [or Advanced Persistent Threat (APT) Handling Checklist]. \n\nSuchi Saria, Adarsh Subbaswamy. 2019. Tutorial: Safe and Reliable Machine Learning. \n\narXiv:1904.07204. \n\n# MANAGE 2.4 \n\nMechanisms are in place and applied, responsibilities are assigned and understood to \n\nsupersede, disengage, or deactivate AI systems that demonstrate performance or outcomes \n\ninconsistent with intended use. \n\nAbout \n\nPerformance inconsistent with intended use does not always increase risk or lead to \n\nnegative impacts. Rigorous TEVV practices are useful for protecting against negative \n\nimpacts regardless of intended use. When negative impacts do arise, superseding \n\n(bypassing), disengaging, or deactivating/decommissioning a model, AI system \n\ncomponent(s), or the entire AI system may be necessary, such as when: \n\nâ€¢ a system reaches the end of its lifetime \n\nâ€¢ detected or identified risks exceed tolerance thresholds \n\nâ€¢ adequate system mitigation actions are beyond the organizationâ€™s capacity \n\nâ€¢ feasible system mitigation actions do not meet regulatory, legal, norms or standards. \n\nâ€¢ impending risk is detected during continual monitoring, for which feasible mitigation \n\ncannot be identified or implemented in a timely fashion. \n\nSafely removing AI systems from operation, either temporarily or permanently, under these \n\nscenarios requires standard protocols that minimize operational disruption and \n\ndownstream negative impacts. Protocols can involve redundant or backup systems that are \n\ndeveloped in alignment with established system governance policies (see GOVERN 1.7), \n\nregulatory compliance, legal frameworks, business requirements and norms and l standards \n\nwithin the application context of use. Decision thresholds and metrics for action s to bypass \n\nor deactivate system components are part of continual monitoring procedures. Incidents \n\nthat result in a bypass/deactivate decision require documentation and review to \n\nunderstand root causes, impacts, and potential opportunities for mitigation and \n\nredeployment. Organizations are encouraged to develop risk and change management 50 of 142 \n\nprotocols that consider and anticipate upstream and downstream consequences of both \n\ntemporary and/or permanent decommissioning, and provide contingency options. \n\nSuggested Actions \n\nâ€¢ Regularly review established procedures for AI system bypass actions, including plans \n\nfor redundant or backup systems to ensure continuity of operational and/or business \n\nfunctionality. \n\nâ€¢ Regularly review Identify system incident thresholds for activating bypass or \n\ndeactivation responses. \n\nâ€¢ Apply change management processes to understand the upstream and downstream \n\nconsequences of bypassing or deactivating an AI system or AI system components. \n\nâ€¢ Apply protocols, resources and metrics for decisions to supersede, bypass or deactivate \n\nAI systems or AI system components. \n\nâ€¢ Preserve materials for forensic, regulatory, and legal review. \n\nâ€¢ Conduct internal root cause analysis and process reviews of bypass or deactivation \n\nevents. \n\nâ€¢ Decommission and preserve system components that cannot be updated to meet \n\ncriteria for redeployment. \n\nâ€¢ Establish criteria for redeploying updated system components, in consideration of \n\ntrustworthy characteristics \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\nâ€¢ Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\nâ€¢ What testing, if any, has the entity conducted on the AI system to identify errors and \n\nlimitations (i.e. adversarial or stress testing)? \n\nâ€¢ To what extent does the entity have established procedures for retiring the AI system, if \n\nit is no longer needed? \n\nâ€¢ How did the entity use assessments and/or evaluations to determine if the system can \n\nbe scaled up, continue, or be decommissioned? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nReferences \n\nDecommissioning Template. Application Lifecycle And Supporting Docs. Cloud and \n\nInfrastructure Community of Practice. 51 of 142 \n\nDevelop a Decommission Plan. M3 Playbook. Office of Shared Services and Solutions and \n\nPerformance Improvement. General Services Administration. \n\n# MANAGE 3.1 \n\nAI risks and benefits from third -party resources are regularly monitored, and risk controls \n\nare applied and documented. \n\nAbout \n\nAI systems may depend on external resources and associated processes, including third -\n\nparty data, software or hardware systems. Third partiesâ€™ supplying organizations with \n\ncomponents and services, including tools, software, and expertise for AI system design, \n\ndevelopment, deployment or use can improve efficiency and scalability. It can also increase \n\ncomplexity and opacity, and, in -turn, risk. Documenting third -party technologies, personnel, \n\nand resources that were employed can help manage risks. Focusing first and foremost on \n\nrisks involving physical safety, legal liabilities, regulatory compliance, and negative impacts \n\non individuals, groups, or society is recommended. \n\nSuggested Actions \n\nâ€¢ Have legal requirements been addressed? \n\nâ€¢ Apply organizational risk tolerance to third -party AI systems. \n\nâ€¢ Apply and document organizational risk management plans and practices to third -party \n\nAI technology, personnel, or other resources. \n\nâ€¢ Identify and maintain documentation for third -party AI systems and components. \n\nâ€¢ Establish testing, evaluation, validation and verification processes for third -party AI \n\nsystems which address the needs for transparency without exposing proprietary \n\nalgorithms . \n\nâ€¢ Establish processes to identify beneficial use and risk indicators in third -party systems \n\nor components, such as inconsistent software release schedule, sparse documentation, \n\nand incomplete software change management (e.g., lack of forward or backward \n\ncompatibility). \n\nâ€¢ Organizations can establish processes for third parties to report known and potential \n\nvulnerabilities, risks or biases in supplied resources. \n\nâ€¢ Verify contingency processes for handling negative impacts associated with mission -\n\ncritical third -party AI systems. \n\nâ€¢ Monitor third -party AI systems for potential negative impacts and risks associated with \n\ntrustworthiness characteristics. \n\nâ€¢ Decommission third -party systems that exceed risk tolerances. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ If a third party created the AI system or some of its components, how will you ensure a \n\nlevel of explainability or interpretability? Is there documentation? 52 of 142 \n\nâ€¢ If your organization obtained datasets from a third party, did your organization assess \n\nand manage the risks of using such datasets? \n\nâ€¢ Did you establish a process for third parties (e.g. suppliers, end users, subjects, \n\ndistributors/vendors or workers) to report potential vulnerabilities, risks or biases in \n\nthe AI system? \n\nâ€¢ Have legal requirements been addressed? \n\nAI Transparency Resources \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF - Companion to the Model AI Governance Framework â€“ Implementation and Self -\n\nAssessment Guide for Organizations. \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nOffice of the Comptroller of the Currency. 2021. Proposed Interagency Guidance on Third -\n\nParty Relationships: Risk Management. July 12, 2021. \n\n# MANAGE 3.2 \n\nPre -trained models which are used for development are monitored as part of AI system \n\nregular monitoring and maintenance. \n\nAbout \n\nA common approach in AI development is transfer learning, whereby an existing pre -\n\ntrained model is adapted for use in a different, but related application. AI actors in \n\ndevelopment tasks often use pre -trained models from third -party entities for tasks such as \n\nimage classification, language prediction, and entity recognition, because the resources to \n\nbuild such models may not be readily available to most organizations. Pre -trained models \n\nare typically trained to address various classification or prediction problems, using \n\nexceedingly large datasets and computationally intensive resources. The use of pre -trained \n\nmodels can make it difficult to anticipate negative system outcomes or impacts. Lack of \n\ndocumentation or transparency tools increases the difficulty and general complexity when \n\ndeploying pre -trained models and hinders root cause analyses. \n\nSuggested Actions \n\nâ€¢ Identify pre -trained models within AI system inventory for risk tracking. \n\nâ€¢ Establish processes to independently and continually monitor performance and \n\ntrustworthiness of pre -trained models, and as part of third -party risk tracking. \n\nâ€¢ Monitor performance and trustworthiness of AI system components connected to pre -\n\ntrained models, and as part of third -party risk tracking. \n\nâ€¢ Identify, document and remediate risks arising from AI system components and pre -\n\ntrained models per organizational risk management procedures, and as part of third -\n\nparty risk tracking. \n\nâ€¢ Decommission AI system components and pre -trained models which exceed risk \n\ntolerances, and as part of third -party risk tracking. 53 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ How has the entity documented the AI systemâ€™s data provenance, including sources, \n\norigins, transformations, augmentations, labels, dependencies, constraints, and \n\nmetadata? \n\nâ€¢ Does this dataset collection/processing procedure achieve the motivation for creating \n\nthe dataset stated in the first section of this datasheet? \n\nâ€¢ How does the entity ensure that the data collected are adequate, relevant, and not \n\nexcessive in relation to the intended purpose? \n\nâ€¢ If the dataset becomes obsolete how will this be communicated? \n\nAI Transparency Resources \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF - Companion to the Model AI Governance Framework â€“ Implementation and Self -\n\nAssessment Guide for Organizations. \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nLarysa Visengeriyeva et al. â€œAwesome MLOps,â€œ GitHub. Accessed January 9, 2023. \n\n# MANAGE 4.1 \n\nPost -deployment AI system monitoring plans are implemented, including mechanisms for \n\ncapturing and evaluating input from users and other relevant AI actors, appeal and \n\noverride, decommissioning, incident response, recovery, and change management. \n\nAbout \n\nAI system performance and trustworthiness can change due to a variety of factors. Regular \n\nAI system monitoring can help deployers identify performance degradations, adversarial \n\nattacks, unexpected and unusual behavior, near -misses, and impacts. Including pre - and \n\npost -deployment external feedback about AI system performance can enhance \n\norganizational awareness about positive and negative impacts, and reduce the time to \n\nrespond to risks and harms. \n\nSuggested Actions \n\nâ€¢ Establish and maintain procedures to monitor AI system performance for risks and \n\nnegative and positive impacts associated with trustworthiness characteristics. \n\nâ€¢ Perform post -deployment TEVV tasks to evaluate AI system validity and reliability, bias \n\nand fairness, privacy, and security and resilience. \n\nâ€¢ Evaluate AI system trustworthiness in conditions similar to deployment context of use, \n\nand prior to deployment. \n\nâ€¢ Establish and implement red -teaming exercises at a prescribed cadence, and evaluate \n\ntheir efficacy. 54 of 142 \n\nâ€¢ Establish procedures for tracking dataset modifications such as data deletion or \n\nrectification requests. \n\nâ€¢ Establish mechanisms for regular communication and feedback between relevant AI \n\nactors and internal or external stakeholders to capture information about system \n\nperformance, trustworthiness and impact. \n\nâ€¢ Share information about errors, near -misses, and attack patterns with incident \n\ndatabases, other organizations with similar systems, and system users and \n\nstakeholders. \n\nâ€¢ Respond to and document detected or reported negative impacts or issues in AI system \n\nperformance and trustworthiness. \n\nâ€¢ Decommission systems that exceed establish risk tolerances. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent has the entity documented the post -deployment AI systemâ€™s testing \n\nmethodology, metrics, and performance outcomes? \n\nâ€¢ How easily accessible and current is the information available to external stakeholders? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities, \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nNavdeep Gill, Patrick Hall, Kim Montgomery, and Nicholas Schmidt. \"A Responsible Machine \n\nLearning Workflow with Focus on Interpretable Models, Post -hoc Explanation, and \n\nDiscrimination Testing.\" Information 11, no. 3 (2020): 137. \n\n# MANAGE 4.2 \n\nMeasurable activities for continual improvements are integrated into AI system updates \n\nand include regular engagement with interested parties, including relevant AI actors. \n\nAbout \n\nRegular monitoring processes enable system updates to enhance performance and \n\nfunctionality in accordance with regulatory and legal frameworks, and organizational and \n\ncontextual values and norms. These processes also facilitate analyses of root causes, system \n\ndegradation, drift, near -misses, and failures, and incident response and documentation. \n\nAI actors across the lifecycle have many opportunities to capture and incorporate external \n\nfeedback about system performance, limitations, and impacts, and implement continuous \n\nimprovements. Improvements may not always be to model pipeline or system processes, \n\nand may instead be based on metrics beyond accuracy or other quality performance \n\nmeasures. In these cases, improvements may entail adaptations to business or \n\norganizational procedures or practices. Organizations are encouraged to develop 55 of 142 \n\nimprovements that will maintain traceability and transparency for developers, end users, \n\nauditors, and relevant AI actors. \n\nSuggested Actions \n\nâ€¢ Integrate trustworthiness characteristics into protocols and metrics used for continual \n\nimprovement. \n\nâ€¢ Establish processes for evaluating and integrating feedback into AI system \n\nimprovements. \n\nâ€¢ Assess and evaluate alignment of proposed improvements with relevant regulatory and \n\nlegal frameworks \n\nâ€¢ Assess and evaluate alignment of proposed improvements connected to the values and \n\nnorms within the context of use. \n\nâ€¢ Document the basis for decisions made relative to tradeoffs between trustworthy \n\ncharacteristics, system risks, and system opportunities \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ How will user and other forms of stakeholder engagement be integrated into the model \n\ndevelopment process and regular performance review once deployed? \n\nâ€¢ To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nâ€¢ To what extent has the entity defined and documented the regulatory environment â€”\n\nincluding minimum requirements in laws and regulations? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities, \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nYen, Po -Yin, et al. \"Development and Evaluation of Socio -Technical Metrics to Inform HIT \n\nAdaptation.\" \n\nCarayon, Pascale, and Megan E. Salwei. \"Moving toward a sociotechnical systems approach \n\nto continuous health information technology design: the path forward for improving \n\nelectronic health record usability and reducing clinician burnout.\" Journal of the American \n\nMedical Informatics Association 28.5 (2021): 1026 -1028. \n\nMishra, Deepa, et al. \"Organizational capabilities that enable big data and predictive \n\nanalytics diffusion and organizational performance: A resource -based perspective.\" \n\nManagement Decision (2018). 56 of 142 \n\n# MANAGE 4.3 \n\nIncidents and errors are communicated to relevant AI actors including affected \n\ncommunities. Processes for tracking, responding to, and recovering from incidents and \n\nerrors are followed and documented. \n\nAbout \n\nRegularly documenting an accurate and transparent account of identified and reported \n\nerrors can enhance AI risk management activities., Examples include: \n\nâ€¢ how errors were identified, \n\nâ€¢ incidents related to the error, \n\nâ€¢ whether the error has been repaired, and \n\nâ€¢ how repairs can be distributed to all impacted stakeholders and users. \n\nSuggested Actions \n\nâ€¢ Establish procedures to regularly share information about errors, incidents and \n\nnegative impacts with relevant stakeholders, operators, practitioners and users, and \n\nimpacted parties. \n\nâ€¢ Maintain a database of reported errors, near -misses, incidents and negative impacts \n\nincluding date reported, number of reports, assessment of impact and severity, and \n\nresponses. \n\nâ€¢ Maintain a database of system changes, reason for change, and details of how the change \n\nwas made, tested and deployed. \n\nâ€¢ Maintain version history information and metadata to enable continuous improvement \n\nprocesses. \n\nâ€¢ Verify that relevant AI actors responsible for identifying complex or emergent risks are \n\nproperly resourced and empowered. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What corrective actions has the entity taken to enhance the quality, accuracy, reliability, \n\nand representativeness of the data? \n\nâ€¢ To what extent does the entity communicate its AI strategic goals and objectives to the \n\ncommunity of stakeholders? How easily accessible and current is the information \n\navailable to external stakeholders? \n\nâ€¢ What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities, 57 of 142 \n\nReferences \n\nWei, M., & Zhou, Z. (2022). AI Ethics Issues in Real World: Evidence from AI Incident \n\nDatabase. ArXiv, abs/2206.07635. \n\nMcGregor, Sean. \"Preventing repeated real world AI failures by cataloging incidents: The AI \n\nincident database.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. \n\nNo. 17. 2021. \n\nMacrae, Carl. \"Learning from the failure of autonomous and intelligent systems: Accidents, \n\nsafety, and sociotechnical sources of risk.\" Risk analysis 42.9 (2022): 1999 -2025. MAP 58 of 142 \n\n# Map \n\nContext is established and understood. \n\n# MAP 1.1 \n\nIntended purpose, potentially beneficial uses, context -specific laws, norms and \n\nexpectations, and prospective settings in which the AI system will be deployed are \n\nunderstood and documented. Considerations include: specific set or types of users along \n\nwith their expectations; potential positive and negative impacts of system uses to \n\nindividuals, communities, organizations, society, and the planet; assumptions and related \n\nlimitations about AI system purposes; uses and risks across the development or product AI \n\nlifecycle; TEVV and system metrics. \n\nAbout \n\nHighly accurate and optimized systems can cause harm. Relatedly, organizations should \n\nexpect broadly deployed AI tools to be reused, repurposed, and potentially misused \n\nregardless of intentions. \n\nAI actors can work collaboratively, and with external parties such as community groups, to \n\nhelp delineate the bounds of acceptable deployment, consider preferable alternatives, and \n\nidentify principles and strategies to manage likely risks. Context mapping is the first step in \n\nthis effort, and may include examination of the following: \n\nâ€¢ intended purpose and impact of system use. \n\nâ€¢ concept of operations. \n\nâ€¢ intended, prospective, and actual deployment setting. \n\nâ€¢ requirements for system deployment and operation. \n\nâ€¢ end user and operator expectations. \n\nâ€¢ specific set or types of end users. \n\nâ€¢ potential negative impacts to individuals, groups, communities, organizations, and \n\nsociety â€“ or context -specific impacts such as legal requirements or impacts to the \n\nenvironment. \n\nâ€¢ unanticipated, downstream, or other unknown contextual factors. \n\nâ€¢ how AI system changes connect to impacts. \n\nThese types of processes can assist AI actors in understanding how limitations, constraints, \n\nand other realities associated with the deployment and use of AI technology can create \n\nimpacts once they are deployed or operate in the real world. When coupled with the \n\nenhanced organizational culture resulting from the established policies and procedures in \n\nthe Govern function, the Map function can provide opportunities to foster and instill new \n\nperspectives, activities, and skills for approaching risks and impacts. \n\nContext mapping also includes discussion and consideration of non -AI or non -technology \n\nalternatives especially as related to whether the given context is narrow enough to manage 59 of 142 \n\nAI and its potential negative impacts. Non -AI alternatives may include capturing and \n\nevaluating information using semi -autonomous or mostly -manual methods. \n\nSuggested Actions \n\nâ€¢ Maintain awareness of industry, technical, and applicable legal standards. \n\nâ€¢ Examine trustworthiness of AI system design and consider, non -AI solutions \n\nâ€¢ Consider intended AI system design tasks along with unanticipated purposes in \n\ncollaboration with human factors and socio -technical domain experts. \n\nâ€¢ Define and document the task, purpose, minimum functionality, and benefits of the AI \n\nsystem to inform considerations about whether the utility of the project or its lack of. \n\nâ€¢ Identify whether there are non -AI or non -technology alternatives that will lead to more \n\ntrustworthy outcomes. \n\nâ€¢ Examine how changes in system performance affect downstream events such as \n\ndecision -making (e.g: changes in an AI model objective function create what types of \n\nimpacts in how many candidates do/do not get a job interview). \n\nâ€¢ Determine actions to map and track post -decommissioning stages of AI deployment and \n\npotential negative or positive impacts to individuals, groups and communities. \n\nâ€¢ Determine the end user and organizational requirements, including business and \n\ntechnical requirements. \n\nâ€¢ Determine and delineate the expected and acceptable AI system context of use, \n\nincluding: \n\nâ€¢ social norms \n\nâ€¢ Impacted individuals, groups, and communities \n\nâ€¢ potential positive and negative impacts to individuals, groups, communities, \n\norganizations, and society \n\nâ€¢ operational environment \n\nâ€¢ Perform context analysis related to time frame, safety concerns, geographic area, \n\nphysical environment, ecosystems, social environment, and cultural norms within the \n\nintended setting (or conditions that closely approximate the intended setting. \n\nâ€¢ Gain and maintain awareness about evaluating scientific claims related to AI system \n\nperformance and benefits before launching into system design. \n\nâ€¢ Identify human -AI interaction and/or roles, such as whether the application will \n\nsupport or replace human decision making. \n\nâ€¢ Plan for risks related to human -AI configurations, and document requirements, roles, \n\nand responsibilities for human oversight of deployed systems. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent is the output of each component appropriate for the operational \n\ncontext? 60 of 142 \n\nâ€¢ Which AI actors are responsible for the decisions of the AI and is this person aware of \n\nthe intended uses and limitations of the analytic? \n\nâ€¢ Which AI actors are responsible for maintaining, re -verifying, monitoring, and updating \n\nthis AI once deployed? \n\nâ€¢ Who is the person(s) accountable for the ethical considerations across the AI lifecycle? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities, \n\nâ€¢ â€œStakeholders in Explainable AI,â€ Sep. 2018. \n\nâ€¢ \"Microsoft Responsible AI Standard, v2\". \n\nReferences \n\nSocio -technical systems \n\nAndrew D. Selbst, danah boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in \n\nSociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and \n\nTransparency (FAccT'19). Association for Computing Machinery, New York, NY, USA, 59 â€“68. \n\nProblem formulation \n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial \n\nintelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004 -3702. \n\nSamir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of \n\nthe Conference on Fairness, Accountability, and Transparency (FAccT'19). Association for \n\nComputing Machinery, New York, NY, USA, 39 â€“48. \n\nContext mapping \n\nEmilio GÃ³mez -GonzÃ¡lez and Emilia GÃ³mez. 2020. Artificial intelligence in medicine and \n\nhealthcare. Joint Research Centre (European Commission). \n\nSarah Spiekermann and Till Winkler. 2020. Value -based Engineering for Ethics by Design. \n\narXiv:2004.13676. \n\nSocial Impact Lab. 2017. Framework for Context Analysis of Technologies in Social Change \n\nProjects (Draft v2.0). \n\nSolon Barocas, Asia J. Biega, Margarita Boyarskaya, et al. 2021. Responsible computing \n\nduring COVID -19 and beyond. Commun. ACM 64, 7 (July 2021), 30 â€“32. \n\nIdentification of harms \n\nHarini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm \n\nthroughout the Machine Learning Life Cycle. arXiv:1901.10002. \n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of \n\nImagination in AI Infused System Development and Deployment. arXiv:2011.13416. \n\nMicrosoft. Foundations of assessing harm. 2022. 61 of 142 \n\nUnderstanding and documenting limitations in ML \n\nAlexander D'Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification \n\nPresents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. \n\nArvind Narayanan. \"How to Recognize AI Snake Oil.\" Arthur Miller Lecture on Science and \n\nEthics (2019). \n\nJessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, \n\nExploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. \n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model \n\nReporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency \n\n(FAT* '19). Association for Computing Machinery, New York, NY, USA, 220 â€“229. \n\nMatthew Arnold, Rachel K. E. Bellamy, Michael Hind, et al. 2019. FactSheets: Increasing \n\nTrust in AI Services through Supplier's Declarations of Conformity. arXiv:1808.07261. \n\nMatthew J. Salganik, Ian Lundberg, Alexander T. Kindel, Caitlin E. Ahearn, Khaled Al -\n\nGhoneim, Abdullah Almaatouq, Drew M. Altschul et al. \"Measuring the Predictability of Life \n\nOutcomes with a Scientific Mass Collaboration.\" Proceedings of the National Academy of \n\nSciences 117, No. 15 (2020): 8398 -8403. \n\nMichael A. Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co -\n\nDesigning Checklists to Understand Organizational Challenges and Opportunities around \n\nFairness in AI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing \n\nSystems (CHI â€˜20). Association for Computing Machinery, New York, NY, USA, 1 â€“14. \n\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. \n\narXiv:1803.09010. \n\nBender, E. M., Friedman, B. & McMillan -Major, A., (2022). A Guide for Writing Data \n\nStatements for Natural Language Processing. University of Washington. Accessed July 14, \n\n2022. \n\nMeta AI. System Cards, a new resource for understanding how AI systems work, 2021. \n\nWhen not to deploy \n\nSolon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. \n\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* \n\n'20). Association for Computing Machinery, New York, NY, USA, 695. \n\nPost -decommission \n\nUpol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. â€œThe Algorithmic Imprint.â€ \n\nProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency \n\n(2022). 62 of 142 \n\nStatistical balance \n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting \n\nracial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 \n\nOct. 2019), 447 -453. \n\nAssessment of science in AI \n\nArvind Narayanan. How to recognize AI snake oil. \n\nEmily M. Bender. 2022. On NYT Magazine on AI: Resist the Urge to be Impressed. (April 17, \n\n2022). \n\n# MAP 1.2 \n\nInter -disciplinary AI actors, competencies, skills and capacities for establishing context \n\nreflect demographic diversity and broad domain and user experience expertise, and their \n\nparticipation is documented. Opportunities for interdisciplinary collaboration are \n\nprioritized. \n\nAbout \n\nSuccessfully mapping context requires a team of AI actors with a diversity of experience, \n\nexpertise, abilities and backgrounds, and with the resources and independence to engage in \n\ncritical inquiry. \n\nHaving a diverse team contributes to more broad and open sharing of ideas and \n\nassumptions about the purpose and function of the technology being designed and \n\ndeveloped â€“ making these implicit aspects more explicit. The benefit of a diverse staff in \n\nmanaging AI risks is not the beliefs or presumed beliefs of individual workers, but the \n\nbehavior that results from a collective perspective. An environment which fosters critical \n\ninquiry creates opportunities to surface problems and identify existing and emergent risks. \n\nSuggested Actions \n\nâ€¢ Establish interdisciplinary teams to reflect a wide range of skills, competencies, and \n\ncapabilities for AI efforts. Verify that team membership includes demographic diversity, \n\nbroad domain expertise, and lived experiences. Document team composition. \n\nâ€¢ Create and empower interdisciplinary expert teams to capture, learn, and engage the \n\ninterdependencies of deployed AI systems and related terminologies and concepts from \n\ndisciplines outside of AI practice such as law, sociology, psychology, anthropology, \n\npublic policy, systems design, and engineering. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent do the teams responsible for developing and maintaining the AI system \n\nreflect diverse opinions, backgrounds, experiences, and perspectives? 63 of 142 \n\nâ€¢ Did the entity document the demographics of those involved in the design and \n\ndevelopment of the AI system to capture and communicate potential biases inherent to \n\nthe development process, according to forum participants? \n\nâ€¢ What specific perspectives did stakeholders share, and how were they integrated across \n\nthe design, development, deployment, assessment, and monitoring of the AI system? \n\nâ€¢ To what extent has the entity addressed stakeholder perspectives on the potential \n\nnegative impacts of the AI system on end users and impacted populations? \n\nâ€¢ What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nâ€¢ Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? Consulting the community or end users at the earliest \n\nstages of development to ensure there is transparency on the technology used and how \n\nit is deployed. \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nâ€¢ AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\nReferences \n\nSina Fazelpour and Maria De -Arteaga. 2022. Diversity in sociotechnical machine learning \n\nsystems. Big Data & Society 9, 1 (Jan. 2022). \n\nMicrosoft Community Jury , Azure Application Architecture Guide. \n\nFernando Delgado, Stephen Yang, Michael Madaio, Qian Yang. (2021). Stakeholder \n\nParticipation in AI: Beyond \"Add Diverse Stakeholders and Stir\". \n\nKush Varshney, Tina Park, Inioluwa Deborah Raji, Gaurush Hiranandani, Narasimhan \n\nHarikrishna, Oluwasanmi Koyejo, Brianna Richardson, and Min Kyung Lee. Participatory \n\nspecification of trustworthy machine learning, 2021. \n\nDonald Martin, Vinodkumar Prabhakaran, Jill A. Kuhlberg, Andrew Smart and William S. \n\nIsaac. â€œParticipatory Problem Formulation for Fairer Machine Learning Through \n\nCommunity Based System Dynamicsâ€, ArXiv abs/2005.07572 (2020). \n\n# MAP 1.3 \n\nThe organizationâ€™s mission and relevant goals for the AI technology are understood and \n\ndocumented. 64 of 142 \n\nAbout \n\nDefining and documenting the specific business purpose of an AI system in a broader \n\ncontext of societal values helps teams to evaluate risks and increases the clarity of â€œgo/no -\n\ngoâ€ decisions about whether to deploy. \n\nTrustworthy AI technologies may present a demonstrable business benefit beyond implicit \n\nor explicit costs, provide added value, and don't lead to wasted resources. Organizations can \n\nfeel confident in performing risk avoidance if the implicit or explicit risks outweigh the \n\nadvantages of AI systems, and not implementing an AI solution whose risks surpass \n\npotential benefits. \n\nFor example, making AI systems more equitable can result in better managed risk, and can \n\nhelp enhance consideration of the business value of making inclusively designed, accessible \n\nand more equitable AI systems. \n\nSuggested Actions \n\nâ€¢ Build transparent practices into AI system development processes. \n\nâ€¢ Review the documented system purpose from a socio -technical perspective and in \n\nconsideration of societal values. \n\nâ€¢ Determine possible misalignment between societal values and stated organizational \n\nprinciples and code of ethics. \n\nâ€¢ Flag latent incentives that may contribute to negative impacts. \n\nâ€¢ Evaluate AI system purpose in consideration of potential risks, societal values, and \n\nstated organizational principles. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ How does the AI system help the entity meet its goals and objectives? \n\nâ€¢ How do the technical specifications and requirements align with the AI systemâ€™s goals \n\nand objectives? \n\nâ€¢ To what extent is the output appropriate for the operational context? \n\nAI Transparency Resources \n\nâ€¢ Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI â€“\n\n2019, [LINK](https://altai.insight -centre.org/), \n\nâ€¢ Including Insights from the Comptroller Generalâ€™s Forum on the Oversight of Artificial \n\nIntelligence An Accountability Framework for Federal Agencies and Other Entities, \n\n2021, \n\nReferences \n\nM.S. Ackerman (2000). The Intellectual Challenge of CSCW: The Gap Between Social \n\nRequirements and Technical Feasibility. Human â€“Computer Interaction, 15, 179 - 203. 65 of 142 \n\nMcKane Andrus, Sarah Dean, Thomas Gilbert, Nathan Lambert, Tom Zick (2021). AI \n\nDevelopment for the Public Interest: From Abstraction Traps to Sociotechnical Risks. \n\nAbeba Birhane, Pratyusha Kalluri, Dallas Card, et al. 2022. The Values Encoded in Machine \n\nLearning Research. arXiv:2106.15590. \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nIason Gabriel, Artificial Intelligence, Values, and Alignment. Minds & Machines 30, 411 â€“437 \n\n(2020). \n\nPEAT â€œBusiness Case for Equitable AIâ€. \n\n# MAP 1.4 \n\nThe business value or context of business use has been clearly defined or â€“ in the case of \n\nassessing existing AI systems â€“ re -evaluated. \n\nAbout \n\nSocio -technical AI risks emerge from the interplay between technical development \n\ndecisions and how a system is used, who operates it, and the social context into which it is \n\ndeployed. Addressing these risks is complex and requires a commitment to understanding \n\nhow contextual factors may interact with AI lifecycle actions. One such contextual factor is \n\nhow organizational mission and identified system purpose create incentives within AI \n\nsystem design, development, and deployment tasks that may result in positive and negative \n\nimpacts. By establishing comprehensive and explicit enumeration of AI systemsâ€™ context of \n\nof business use and expectations, organizations can identify and manage these types of \n\nrisks. \n\nSuggested Actions \n\nâ€¢ Document business value or context of business use \n\nâ€¢ Reconcile documented concerns about the systemâ€™s purpose within the business context \n\nof use compared to the organizationâ€™s stated values, mission statements, social \n\nresponsibility commitments, and AI principles. \n\nâ€¢ Reconsider the design, implementation strategy, or deployment of AI systems with \n\npotential impacts that do not reflect institutional values. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What goals and objectives does the entity expect to achieve by designing, developing, \n\nand/or deploying the AI system? \n\nâ€¢ To what extent are the system outputs consistent with the entityâ€™s values and principles \n\nto foster public trust and equity? \n\nâ€¢ To what extent are the metrics consistent with system goals, objectives, and constraints, \n\nincluding ethical and compliance considerations? 66 of 142 \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nReferences \n\nAlgorithm Watch. AI Ethics Guidelines Global Inventory. \n\nEthical OS toolkit. \n\nEmanuel Moss and Jacob Metcalf. 2020. Ethics Owners: A New Model of Organizational \n\nResponsibility in Data -Driven Technology Companies. Data & Society Research Institute. \n\nFuture of Life Institute. Asilomar AI Principles. \n\nLeonard Haas, Sebastian GieÃŸler, and Veronika Thiel. 2020. In the realm of paper tigers â€“\n\nexploring the failings of AI ethics guidelines. (April 28, 2020). \n\n# MAP 1.5 \n\nOrganizational risk tolerances are determined and documented. \n\nAbout \n\nRisk tolerance reflects the level and type of risk the organization is willing to accept while \n\nconducting its mission and carrying out its strategy. \n\nOrganizations can follow existing regulations and guidelines for risk criteria, tolerance and \n\nresponse established by organizational, domain, discipline, sector, or professional \n\nrequirements. Some sectors or industries may have established definitions of harm or may \n\nhave established documentation, reporting, and disclosure requirements. \n\nWithin sectors, risk management may depend on existing guidelines for specific \n\napplications and use case settings. Where established guidelines do not exist, organizations \n\nwill want to define reasonable risk tolerance in consideration of different sources of risk \n\n(e.g., financial, operational, safety and wellbeing, business, reputational, and model risks) \n\nand different levels of risk (e.g., from negligible to critical). \n\nRisk tolerances inform and support decisions about whether to continue with development \n\nor deployment - termed â€œgo/no -goâ€. Go/no -go decisions related to AI system risks can take \n\nstakeholder feedback into account, but remain independent from stakeholdersâ€™ vested \n\nfinancial or reputational interests. \n\nIf mapping risk is prohibitively difficult, a \"no -go\" decision may be considered for the \n\nspecific system. \n\nSuggested Actions \n\nâ€¢ Utilize existing regulations and guidelines for risk criteria, tolerance and response \n\nestablished by organizational, domain, discipline, sector, or professional requirements. 67 of 142 \n\nâ€¢ Establish risk tolerance levels for AI systems and allocate the appropriate oversight \n\nresources to each level. \n\nâ€¢ Establish risk criteria in consideration of different sources of risk, (e.g., financial, \n\noperational, safety and wellbeing, business, reputational, and model risks) and different \n\nlevels of risk (e.g., from negligible to critical). \n\nâ€¢ Identify maximum allowable risk tolerance above which the system will not be \n\ndeployed, or will need to be prematurely decommissioned, within the contextual or \n\napplication setting. \n\nâ€¢ Articulate and analyze tradeoffs across trustworthiness characteristics as relevant to \n\nproposed context of use. When tradeoffs arise, document them and plan for traceable \n\nactions (e.g.: impact mitigation, removal of system from development or use) to inform \n\nmanagement decisions. \n\nâ€¢ Review uses of AI systems for â€œoff -labelâ€ purposes, especially in settings that \n\norganizations have deemed as high -risk. Document decisions, risk -related trade -offs, \n\nand system limitations. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Which existing regulations and guidelines apply, and the entity has followed, in the \n\ndevelopment of system risk tolerances? \n\nâ€¢ What criteria and assumptions has the entity utilized when developing system risk \n\ntolerances? \n\nâ€¢ How has the entity identified maximum allowable risk tolerance? \n\nâ€¢ What conditions and purposes are considered â€œoff -labelâ€ for system use? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nReferences \n\nBoard of Governors of the Federal Reserve System. SR 11 -7: Guidance on Model Risk \n\nManagement. (April 4, 2011). \n\nThe Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, \n\n2019). \n\nBrenda Boultwood, How to Develop an Enterprise Risk -Rating Approach (Aug. 26, 2021). \n\nGlobal Association of Risk Professionals (garp.org). Accessed Jan. 4, 2023. \n\nVirginia Eubanks, 1972 -, Automating Inequality: How High -tech Tools Profile, Police, and \n\nPunish the Poor. New York, NY, St. Martin's Press, 2018. 68 of 142 \n\nGAO -17 -63: Enterprise Risk Management: Selected Agenciesâ€™ Experiences Illustrate Good \n\nPractices in Managing Risk. \n\nNIST Risk Management Framework. \n\n# MAP 1.6 \n\nSystem requirements (e.g., â€œthe system shall respect the privacy of its usersâ€) are elicited \n\nfrom and understood by relevant AI actors. Design decisions take socio -technical \n\nimplications into account to address AI risks. \n\nAbout \n\nAI system development requirements may outpace documentation processes for traditional \n\nsoftware. When written requirements are unavailable or incomplete, AI actors may \n\ninadvertently overlook business and stakeholder needs, over -rely on implicit human biases \n\nsuch as confirmation bias and groupthink, and maintain exclusive focus on computational \n\nrequirements. \n\nEliciting system requirements, designing for end users, and considering societal impacts \n\nearly in the design phase is a priority that can enhance AI systemsâ€™ trustworthiness. \n\nSuggested Actions \n\nâ€¢ Proactively incorporate trustworthy characteristics into system requirements. \n\nâ€¢ Establish mechanisms for regular communication and feedback between relevant AI \n\nactors and internal or external stakeholders related to system design or deployment \n\ndecisions. \n\nâ€¢ Develop and standardize practices to assess potential impacts at all stages of the AI \n\nlifecycle, and in collaboration with interdisciplinary experts, actors external to the team \n\nthat developed or deployed the AI system, and potentially impacted communities . \n\nâ€¢ Include potentially impacted groups, communities and external entities (e.g. civil society \n\norganizations, research institutes, local community groups, and trade associations) in \n\nthe formulation of priorities, definitions and outcomes during impact assessment \n\nactivities. \n\nâ€¢ Conduct qualitative interviews with end user(s) to regularly evaluate expectations and \n\ndesign plans related to Human -AI configurations and tasks. \n\nâ€¢ Analyze dependencies between contextual factors and system requirements. List \n\npotential impacts that may arise from not fully considering the importance of \n\ntrustworthiness characteristics in any decision making. \n\nâ€¢ Follow responsible design techniques in tasks such as software engineering, product \n\nmanagement, and participatory engagement. Some examples for eliciting and \n\ndocumenting stakeholder requirements include product requirement documents \n\n(PRDs), user stories, user interaction/user experience (UI/UX) research, systems \n\nengineering, ethnography and related field methods. 69 of 142 \n\nâ€¢ Conduct user research to understand individuals, groups and communities that will be \n\nimpacted by the AI, their values & context, and the role of systemic and historical biases. \n\nIntegrate learnings into decisions about data selection and representation. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nâ€¢ To what extent is this information sufficient and appropriate to promote transparency? \n\nPromote transparency by enabling external stakeholders to access information on the \n\ndesign, operation, and limitations of the AI system. \n\nâ€¢ To what extent has relevant information been disclosed regarding the use of AI systems, \n\nsuch as (a) what the system is for, (b) what it is not for, (c) how it was designed, and (d) \n\nwhat its limitations are? (Documentation and external communication can offer a way \n\nfor entities to provide transparency.) \n\nâ€¢ How will the relevant AI actor(s) address changes in accuracy and precision due to \n\neither an adversaryâ€™s attempts to disrupt the AI system or unrelated changes in the \n\noperational/business environment, which may impact the accuracy of the AI system? \n\nâ€¢ What metrics has the entity developed to measure performance of the AI system? \n\nâ€¢ What justifications, if any, has the entity provided for the assumptions, boundaries, and \n\nlimitations of the AI system? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Stakeholders in Explainable AI, Sep. 2018. \n\nâ€¢ High -Level Expert Group on Artificial Intelligence set up by the European Commission, \n\nEthics Guidelines for Trustworthy AI. \n\nReferences \n\nNational Academies of Sciences, Engineering, and Medicine 2022. Fostering Responsible \n\nComputing Research: Foundations and Practices. Washington, DC: The National Academies \n\nPress. \n\nAbeba Birhane, William S. Isaac, Vinodkumar Prabhakaran, Mark Diaz, Madeleine Clare \n\nElish, Iason Gabriel and Shakir Mohamed. â€œPower to the People? Opportunities and \n\nChallenges for Participatory AI.â€ Equity and Access in Algorithms, Mechanisms, and \n\nOptimization (2022). \n\nAmit K. Chopra, Fabiano Dalpiaz, F. BaÅŸak Aydemir, et al. 2014. Protos: Foundations for \n\nengineering innovative sociotechnical systems. In 2014 IEEE 22nd International \n\nRequirements Engineering Conference (RE) (2014), 53 -62. 70 of 142 \n\nAndrew D. Selbst, danah boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in \n\nSociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and \n\nTransparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 59 â€“68. \n\nGordon Baxter and Ian Sommerville. 2011. Socio -technical systems: From design methods \n\nto systems engineering. Interacting with Computers, 23, 1 (Jan. 2011), 4 â€“17. \n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial \n\nintelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004 -3702. \n\nYilin Huang, Giacomo Poderi, Sanja Å Ä‡epanoviÄ‡, et al. 2019. Embedding Internet -of -Things in \n\nLarge -Scale Socio -technical Systems: A Community -Oriented Design in Future Smart Grids. \n\nIn The Internet of Things for Smart Urban Ecosystems (2019), 125 -150. Springer, Cham. \n\nVictor Udoewa, (2022). An introduction to radical participatory design: decolonising \n\nparticipatory design processes. Design Science. 8. 10.1017/dsj.2022.24. \n\n# MAP 2.1 \n\nThe specific task, and methods used to implement the task, that the AI system will support \n\nis defined (e.g., classifiers, generative models, recommenders). \n\nAbout \n\nAI actors define the technical learning or decision -making task(s) an AI system is designed \n\nto accomplish, or the benefits that the system will provide. The clearer and narrower the \n\ntask definition, the easier it is to map its benefits and risks, leading to more fulsome risk \n\nmanagement. \n\nSuggested Actions \n\nâ€¢ Define and document AI systemâ€™s existing and potential learning task(s) along with \n\nknown assumptions and limitations. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent has the entity clearly defined technical specifications and requirements \n\nfor the AI system? \n\nâ€¢ To what extent has the entity documented the AI systemâ€™s development, testing \n\nmethodology, metrics, and performance outcomes? \n\nâ€¢ How do the technical specifications and requirements align with the AI systemâ€™s goals \n\nand objectives? \n\nâ€¢ Did your organization implement accountability -based practices in data management \n\nand protection (e.g. the PDPA and OECD Privacy Principles)? \n\nâ€¢ How are outputs marked to clearly show that they came from an AI? \n\nAI Transparency Resources \n\nâ€¢ Datasheets for Datasets. 71 of 142 \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nâ€¢ ATARC Model Transparency Assessment (WD) â€“ 2020. \n\nâ€¢ Transparency in Artificial Intelligence - S. Larsson and F. Heintz â€“ 2020. \n\nReferences \n\nLeong, Brenda (2020). The Spectrum of Artificial Intelligence - An Infographic Tool. Future \n\nof Privacy Forum. \n\nBrownlee, Jason (2020). A Tour of Machine Learning Algorithms. Machine Learning \n\nMastery. \n\n# MAP 2.2 \n\nInformation about the AI systemâ€™s knowledge limits and how system output may be utilized \n\nand overseen by humans is documented. Documentation provides sufficient information to \n\nassist relevant AI actors when making informed decisions and taking subsequent actions. \n\nAbout \n\nAn AI lifecycle consists of many interdependent activities involving a diverse set of actors \n\nthat often do not have full visibility or control over other parts of the lifecycle and its \n\nassociated contexts or risks. The interdependencies between these activities, and among the \n\nrelevant AI actors and organizations, can make it difficult to reliably anticipate potential \n\nimpacts of AI systems. For example, early decisions in identifying the purpose and objective \n\nof an AI system can alter its behavior and capabilities, and the dynamics of deployment \n\nsetting (such as end users or impacted individuals) can shape the positive or negative \n\nimpacts of AI system decisions. As a result, the best intentions within one dimension of the \n\nAI lifecycle can be undermined via interactions with decisions and conditions in other, later \n\nactivities. This complexity and varying levels of visibility can introduce uncertainty. And, \n\nonce deployed and in use, AI systems may sometimes perform poorly, manifest \n\nunanticipated negative impacts, or violate legal or ethical norms. These risks and incidents \n\ncan result from a variety of factors. For example, downstream decisions can be influenced \n\nby end user over -trust or under -trust, and other complexities related to AI -supported \n\ndecision -making. \n\nAnticipating, articulating, assessing and documenting AI systemsâ€™ knowledge limits and how \n\nsystem output may be utilized and overseen by humans can help mitigate the uncertainty \n\nassociated with the realities of AI system deployments. Rigorous design processes include \n\ndefining system knowledge limits, which are confirmed and refined based on TEVV \n\nprocesses. \n\nSuggested Actions \n\nâ€¢ Document settings, environments and conditions that are outside the AI systemâ€™s \n\nintended use. 72 of 142 \n\nâ€¢ Design for end user workflows and toolsets, concept of operations, and explainability \n\nand interpretability criteria in conjunction with end user(s) and associated qualitative \n\nfeedback. \n\nâ€¢ Plan and test human -AI configurations under close to real -world conditions and \n\ndocument results. \n\nâ€¢ Follow stakeholder feedback processes to determine whether a system achieved its \n\ndocumented purpose within a given use context, and whether end users can correctly \n\ncomprehend system outputs or results. \n\nâ€¢ Document dependencies on upstream data and other AI systems, including if the \n\nspecified system is an upstream dependency for another AI system or other data. \n\nâ€¢ Document connections the AI system or data will have to external networks (including \n\nthe internet), financial markets, and critical infrastructure that have potential for \n\nnegative externalities. Identify and document negative impacts as part of considering \n\nthe broader risk thresholds and subsequent go/no -go deployment as well as post -\n\ndeployment decommissioning decisions. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Does the AI system provide sufficient information to assist the personnel to make an \n\ninformed decision and take actions accordingly? \n\nâ€¢ What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nâ€¢ Based on the assessment, did your organization implement the appropriate level of \n\nhuman involvement in AI -augmented decision -making? \n\nAI Transparency Resources \n\nâ€¢ Datasheets for Datasets. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nâ€¢ ATARC Model Transparency Assessment (WD) â€“ 2020. \n\nâ€¢ Transparency in Artificial Intelligence - S. Larsson and F. Heintz â€“ 2020. \n\nReferences \n\nContext of use \n\nInternational Standards Organization (ISO). 2019. ISO 9241 -210:2019 Ergonomics of \n\nhuman -system interaction â€” Part 210: Human -centred design for interactive systems. \n\nNational Institute of Standards and Technology (NIST), Mary Theofanos, Yee -Yin Choong, et \n\nal. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: \n\nEnsuring Successful Systems for First Responders. 73 of 142 \n\nHuman -AI interaction \n\nCommittee on Human -System Integration Research Topics for the 711th Human \n\nPerformance Wing of the Air Force Research Laboratory and the National Academies of \n\nSciences, Engineering, and Medicine. 2022. Human -AI Teaming: State -of -the -Art and \n\nResearch Needs. Washington, D.C. National Academies Press. \n\nHuman Readiness Level Scale in the System Development Process, American National \n\nStandards Institute and Human Factors and Ergonomics Society, ANSI/HFES 400 -2021 \n\nMicrosoft Responsible AI Standard, v2. \n\nSaar Alon -Barkat, Madalina Busuioc, Human â€“AI Interactions in Public Sector Decision \n\nMaking: â€œAutomation Biasâ€ and â€œSelective Adherenceâ€ to Algorithmic Advice, Journal of \n\nPublic Administration Research and Theory, 2022;, muac007. \n\nZana BuÃ§inca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: \n\nCognitive Forcing Functions Can Reduce Overreliance on AI in AI -assisted Decision -making. \n\nProc. ACM Hum. -Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. \n\nMary L. Cummings. 2006 Automation and accountability in decision support system \n\ninterface design.The Journal of Technology Studies 32(1): 23 â€“31. \n\nEngstrom, D. F., Ho, D. E., Sharkey, C. M., & CuÃ©llar, M. F. (2020). Government by algorithm: \n\nArtificial intelligence in federal administrative agencies. NYU School of Law, Public Law \n\nResearch Paper, (20 -54). \n\nSusanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in \n\ndeployment of clinical decision -aids. npj Digital Medicine 4, Article 31 (2021). \n\nBen Green. 2021. The Flaws of Policies Requiring Human Oversight of Government \n\nAlgorithms. Computer Law & Security Review 45 (26 Apr. 2021). \n\nBen Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to \n\nA.I. Harm. (June 15, 2021). \n\nGrgiÄ‡ -HlaÄa, N., Engel, C., & Gummadi, K. P. (2019). Human decision making with machine \n\nassistance: An experiment on bailing and jailing. Proceedings of the ACM on Human -\n\nComputer Interaction, 3(CSCW), 1 -25. \n\nForough Poursabzi -Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating \n\nand Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on \n\nHuman Factors in Computing Systems (CHI '21). Association for Computing Machinery, New \n\nYork, NY, USA, Article 237, 1 â€“52. \n\nC. J. Smith (2019). Designing trustworthy AI: A human -machine teaming framework to \n\nguide development. arXiv preprint arXiv:1910.03515. 74 of 142 \n\nT. Warden, P. Carayon, EM et al. The National Academies Board on Human System \n\nIntegration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine \n\nTeaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. \n\n2019;63(1):631 -635. doi:10.1177/1071181319631100. \n\n# MAP 2.3 \n\nScientific integrity and TEVV considerations are identified and documented, including those \n\nrelated to experimental design, data collection and selection (e.g., availability, \n\nrepresentativeness, suitability), system trustworthiness, and construct validation. \n\nAbout \n\nStandard testing and evaluation protocols provide a basis to confirm assurance in a system \n\nthat it is operating as designed and claimed. AI systemsâ€™ complexities create challenges for \n\ntraditional testing and evaluation methodologies, which tend to be designed for static or \n\nisolated system performance. Opportunities for risk continue well beyond design and \n\ndeployment, into system operation and application of system -enabled decisions. Testing \n\nand evaluation methodologies and metrics therefore address a continuum of activities. \n\nTEVV is enhanced when key metrics for performance, safety, and reliability are interpreted \n\nin a socio -technical context and not confined to the boundaries of the AI system pipeline. \n\nOther challenges for managing AI risks relate to dependence on large scale datasets, which \n\ncan impact data quality and validity concerns. The difficulty of finding the â€œrightâ€ data may \n\nlead AI actors to select datasets based more on accessibility and availability than on \n\nsuitability for operationalizing the phenomenon that the AI system intends to support or \n\ninform. Such decisions could contribute to an environment where the data used in \n\nprocesses is not fully representative of the populations or phenomena that are being \n\nmodeled, introducing downstream risks. Practices such as dataset reuse may also lead to \n\ndisconnect from the social contexts and time periods of their creation. This contributes to \n\nissues of validity of the underlying dataset for providing proxies, measures, or predictors \n\nwithin the model. \n\nSuggested Actions \n\nâ€¢ Identify and document experiment design and statistical techniques that are valid for \n\ntesting complex socio -technical systems like AI, which involve human factors, emergent \n\nproperties, and dynamic context(s) of use. \n\nâ€¢ Develop and apply TEVV protocols for models, system and its subcomponents, \n\ndeployment, and operation. \n\nâ€¢ Demonstrate and document that AI system performance and validation metrics are \n\ninterpretable and unambiguous for downstream decision making tasks, and take socio -\n\ntechnical factors such as context of use into consideration. \n\nâ€¢ Identify and document assumptions, techniques, and metrics used for testing and \n\nevaluation throughout the AI lifecycle including experimental design techniques for data \n\ncollection, selection, and management practices in accordance with data governance \n\npolicies established in GOVERN. 75 of 142 \n\nâ€¢ Identify testing modules that can be incorporated throughout the AI lifecycle, and verify \n\nthat processes enable corroboration by independent evaluators. \n\nâ€¢ Establish mechanisms for regular communication and feedback among relevant AI \n\nactors and internal or external stakeholders related to the validity of design and \n\ndeployment assumptions. \n\nâ€¢ Establish mechanisms for regular communication and feedback between relevant AI \n\nactors and internal or external stakeholders related to the development of TEVV \n\napproaches throughout the lifecycle to detect and assess potentially harmful impacts \n\nâ€¢ Document assumptions made and techniques used in data selection, curation, \n\npreparation and analysis, including: \n\nâ€¢ identification of constructs and proxy targets, \n\nâ€¢ development of indices â€“ especially those operationalizing concepts that are \n\ninherently unobservable (e.g. â€œhireability,â€ â€œcriminality.â€ â€œlendabilityâ€). \n\nâ€¢ Map adherence to policies that address data and construct validity, bias, privacy and \n\nsecurity for AI systems and verify documentation, oversight, and processes. \n\nâ€¢ Identify and document transparent methods (e.g. causal discovery methods) for \n\ninferring causal relationships between constructs being modeled and dataset attributes \n\nor proxies. \n\nâ€¢ Identify and document processes to understand and trace test and training data lineage \n\nand its metadata resources for mapping risks. \n\nâ€¢ Document known limitations, risk mitigation efforts associated with, and methods used \n\nfor, training data collection, selection, labeling, cleaning, and analysis (e.g. treatment of \n\nmissing, spurious, or outlier data; biased estimators). \n\nâ€¢ Establish and document practices to check for capabilities that are in excess of those \n\nthat are planned for, such as emergent properties, and to revisit prior risk management \n\nsteps in light of any new capabilities. \n\nâ€¢ Establish processes to test and verify that design assumptions about the set of \n\ndeployment contexts continue to be accurate and sufficiently complete. \n\nâ€¢ Work with domain experts and other external AI actors to: \n\nâ€¢ Gain and maintain contextual awareness and knowledge about how human \n\nbehavior, organizational factors and dynamics, and society influence, and are \n\nrepresented in, datasets, processes, models, and system output. \n\nâ€¢ Identify participatory approaches for responsible Human -AI configurations and \n\noversight tasks, taking into account sources of cognitive bias. \n\nâ€¢ Identify techniques to manage and mitigate sources of bias (systemic, \n\ncomputational, human - cognitive) in computational models and systems, and the \n\nassumptions and decisions in their development.. \n\nâ€¢ Investigate and document potential negative impacts due related to the full product \n\nlifecycle and associated processes that may conflict with organizational values and \n\nprinciples. 76 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Are there any known errors, sources of noise, or redundancies in the data? \n\nâ€¢ Over what time -frame was the data collected? Does the collection time -frame match the \n\ncreation time -frame \n\nâ€¢ What is the variable selection and evaluation process? \n\nâ€¢ How was the data collected? Who was involved in the data collection process? If the \n\ndataset relates to people (e.g., their attributes) or was generated by people, were they \n\ninformed about the data collection? (e.g., datasets that collect writing, photos, \n\ninteractions, transactions, etc.) \n\nâ€¢ As time passes and conditions change, is the training data still representative of the \n\noperational environment? \n\nâ€¢ Why was the dataset created? (e.g., were there specific tasks in mind, or a specific gap \n\nthat needed to be filled?) \n\nâ€¢ How does the entity ensure that the data collected are adequate, relevant, and not \n\nexcessive in relation to the intended purpose? \n\nAI Transparency Resources \n\nâ€¢ Datasheets for Datasets. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ ATARC Model Transparency Assessment (WD) â€“ 2020. \n\nâ€¢ Transparency in Artificial Intelligence - S. Larsson and F. Heintz â€“ 2020. \n\nReferences \n\nChallenges with dataset selection \n\nAlexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social Data: \n\nBiases, Methodological Pitfalls, and Ethical Boundaries. Front. Big Data 2, 13 (11 July 2019). \n\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its \n\n(dis)contents: A survey of dataset development and use in machine learning research. \n\narXiv:2012.05345. \n\nCatherine D'Ignazio and Lauren F. Klein. 2020. Data Feminism. The MIT Press, Cambridge, \n\nMA. \n\nMiceli, M., & Posada, J. (2022). The Data -Production Dispositif. ArXiv, abs/2205.11963. \n\nBarbara Plank. 2016. What to do about non -standard (or non -canonical) language in NLP. \n\narXiv:1608.07836. 77 of 142 \n\nDataset and test, evaluation, validation and verification (TEVV) processes in AI system \n\ndevelopment \n\nNational Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et \n\nal. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing \n\nBias in Artificial Intelligence. \n\nInioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, et al. 2021. AI and the \n\nEverything in the Whole Wide World Benchmark. arXiv:2111.15366. \n\nStatistical balance \n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting \n\nracial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 \n\nOct. 2019), 447 -453. \n\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its \n\n(dis)contents: A survey of dataset development and use in machine learning research. \n\narXiv:2012.05345. \n\nSolon Barocas, Anhong Guo, Ece Kamar, et al. 2021. Designing Disaggregated Evaluations of \n\nAI Systems: Choices, Considerations, and Tradeoffs. Proceedings of the 2021 AAAI/ACM \n\nConference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, \n\nUSA, 368 â€“378. \n\nMeasurement and evaluation \n\nAbigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of \n\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT â€˜21). \n\nAssociation for Computing Machinery, New York, NY, USA, 375 â€“385. \n\nBen Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in \n\nMachine Learning Practice. arXiv:2205.05256. \n\nLaura Freeman, \"Test and evaluation for artificial intelligence.\" Insight 23.1 (2020): 27 -30. \n\nExisting frameworks \n\nNational Institute of Standards and Technology. (2018). Framework for improving critical \n\ninfrastructure cybersecurity. \n\nKaitlin R. Boeckl and Naomi B. Lefkovitz. \"NIST Privacy Framework: A Tool for Improving \n\nPrivacy Through Enterprise Risk Management, Version 1.0.\" National Institute of Standards \n\nand Technology (NIST), January 16, 2020. \n\n# MAP 3.1 \n\nPotential benefits of intended AI system functionality and performance are examined and \n\ndocumented. 78 of 142 \n\nAbout \n\nAI systems have enormous potential to improve quality of life, enhance economic prosperity \n\nand security costs. Organizations are encouraged to define and document system purpose \n\nand utility, and its potential positive impacts and benefits beyond current known \n\nperformance benchmarks. \n\nIt is encouraged that risk management and assessment of benefits and impacts include \n\nprocesses for regular and meaningful communication with potentially affected groups and \n\ncommunities. These stakeholders can provide valuable input related to systemsâ€™ benefits \n\nand possible limitations. Organizations may differ in the types and number of stakeholders \n\nwith which they engage. \n\nOther approaches such as human -centered design (HCD) and value -sensitive design (VSD) \n\ncan help AI teams to engage broadly with individuals and communities. This type of \n\nengagement can enable AI teams to learn about how a given technology may cause positive \n\nor negative impacts, that were not originally considered or intended. \n\nSuggested Actions \n\nâ€¢ Utilize participatory approaches and engage with system end users to understand and \n\ndocument AI systemsâ€™ potential benefits, efficacy and interpretability of AI task output. \n\nâ€¢ Maintain awareness and documentation of the individuals, groups, or communities who \n\nmake up the systemâ€™s internal and external stakeholders. \n\nâ€¢ Verify that appropriate skills and practices are available in -house for carrying out \n\nparticipatory activities such as eliciting, capturing, and synthesizing user, operator and \n\nexternal feedback, and translating it for AI design and development functions. \n\nâ€¢ Establish mechanisms for regular communication and feedback between relevant AI \n\nactors and internal or external stakeholders related to system design or deployment \n\ndecisions. \n\nâ€¢ Consider performance to human baseline metrics or other standard benchmarks. \n\nâ€¢ Incorporate feedback from end users, and potentially impacted individuals and \n\ncommunities about perceived system benefits . \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Have the benefits of the AI system been communicated to end users? \n\nâ€¢ Have the appropriate training material and disclaimers about how to adequately use the \n\nAI system been provided to end users? \n\nâ€¢ Has your organization implemented a risk management system to address risks \n\ninvolved in deploying the identified AI system (e.g. personnel risk or changes to \n\ncommercial objectives)? \n\nAI Transparency Resources \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. 79 of 142 \n\nâ€¢ Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI â€“\n\n2019. [LINK](https://altai.insight -centre.org/), \n\nReferences \n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial \n\nintelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004 -3702. \n\nSamir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of \n\nthe Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for \n\nComputing Machinery, New York, NY, USA, 39 â€“48. \n\nVincent T. Covello. 2021. Stakeholder Engagement and Empowerment. In Communicating in \n\nRisk, Crisis, and High Stress Situations (Vincent T. Covello, ed.), 87 -109. \n\nYilin Huang, Giacomo Poderi, Sanja Å Ä‡epanoviÄ‡, et al. 2019. Embedding Internet -of -Things in \n\nLarge -Scale Socio -technical Systems: A Community -Oriented Design in Future Smart Grids. \n\nIn The Internet of Things for Smart Urban Ecosystems (2019), 125 -150. Springer, Cham. \n\nEloise Taysom and Nathan Crilly. 2017. Resilience in Sociotechnical Systems: The \n\nPerspectives of Multiple Stakeholders. She Ji: The Journal of Design, Economics, and \n\nInnovation, 3, 3 (2017), 165 -182, ISSN 2405 -8726. \n\n# MAP 3.2 \n\nPotential costs, including non -monetary costs, which result from expected or realized AI \n\nerrors or system functionality and trustworthiness - as connected to organizational risk \n\ntolerance - are examined and documented. \n\nAbout \n\nAnticipating negative impacts of AI systems is a difficult task. Negative impacts can be due \n\nto many factors, such as system non -functionality or use outside of its operational limits, \n\nand may range from minor annoyance to serious injury, financial losses, or regulatory \n\nenforcement actions. AI actors can work with a broad set of stakeholders to improve their \n\ncapacity for understanding systemsâ€™ potential impacts â€“ and subsequently â€“ systemsâ€™ risks. \n\nSuggested Actions \n\nâ€¢ Perform context analysis to map potential negative impacts arising from not integrating \n\ntrustworthiness characteristics. When negative impacts are not direct or obvious, AI \n\nactors can engage with stakeholders external to the team that developed or deployed \n\nthe AI system, and potentially impacted communities, to examine and document: \n\nâ€¢ Who could be harmed? \n\nâ€¢ What could be harmed? \n\nâ€¢ When could harm arise? \n\nâ€¢ How could harm arise? 80 of 142 \n\nâ€¢ Identify and implement procedures for regularly evaluating the qualitative and \n\nquantitative costs of internal and external AI system failures. Develop actions to \n\nprevent, detect, and/or correct potential risks and related impacts. Regularly evaluate \n\nfailure costs to inform go/no -go deployment decisions throughout the AI system \n\nlifecycle. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent does the system/entity consistently measure progress towards stated \n\ngoals and objectives? \n\nâ€¢ To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nâ€¢ Have you documented and explained that machine errors may differ from human \n\nerrors? \n\nAI Transparency Resources \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI â€“\n\n2019. [LINK](https://altai.insight -centre.org/), \n\nReferences \n\nAbagayle Lee Blank. 2019. Computer vision machine learning and future -oriented ethics. \n\nHonors Project. Seattle Pacific University (SPU), Seattle, WA. \n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of \n\nImagination in AI Infused System Development and Deployment. arXiv:2011.13416. \n\nJeff Patton. 2014. User Story Mapping. O'Reilly, Sebastopol, CA. \n\nMargarita Boenig -Liptsin, Anissa Tanweer & Ari Edmundson (2022) Data Science Ethos \n\nLifecycle: Interplay of ethical thinking and data science practice, Journal of Statistics and \n\nData Science Education, DOI: 10.1080/26939169.2022.2089411 \n\nJ. Cohen, D. S. Katz, M. Barker, N. Chue Hong, R. Haines and C. Jay, \"The Four Pillars of \n\nResearch Software Engineering,\" in IEEE Software, vol. 38, no. 1, pp. 97 -105, Jan. -Feb. 2021, \n\ndoi: 10.1109/MS.2020.2973362. \n\nNational Academies of Sciences, Engineering, and Medicine 2022. Fostering Responsible \n\nComputing Research: Foundations and Practices. Washington, DC: The National Academies \n\nPress. \n\n# MAP 3.3 \n\nTargeted application scope is specified and documented based on the systemâ€™s capability, \n\nestablished context, and AI system categorization. 81 of 142 \n\nAbout \n\nSystems that function in a narrow scope tend to enable better mapping, measurement, and \n\nmanagement of risks in the learning or decision -making tasks and the system context. A \n\nnarrow application scope also helps ease TEVV functions and related resources within an \n\norganization. \n\nFor example, large language models or open -ended chatbot systems that interact with the \n\npublic on the internet have a large number of risks that may be difficult to map, measure, \n\nand manage due to the variability from both the decision -making task and the operational \n\ncontext. Instead, a task -specific chatbot utilizing templated responses that follow a defined \n\nâ€œuser journeyâ€ is a scope that can be more easily mapped, measured and managed. \n\nSuggested Actions \n\nâ€¢ Consider narrowing contexts for system deployment, including factors related to: \n\nâ€¢ How outcomes may directly or indirectly affect users, groups, communities and \n\nthe environment. \n\nâ€¢ Length of time the system is deployed in between re -trainings. \n\nâ€¢ Geographical regions in which the system operates. \n\nâ€¢ Dynamics related to community standards or likelihood of system misuse or \n\nabuses (either purposeful or unanticipated). \n\nâ€¢ How AI system features and capabilities can be utilized within other \n\napplications, or in place of other existing processes. \n\nâ€¢ Engage AI actors from legal and procurement functions when specifying target \n\napplication scope. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent has the entity clearly defined technical specifications and requirements \n\nfor the AI system? \n\nâ€¢ How do the technical specifications and requirements align with the AI systemâ€™s goals \n\nand objectives? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI â€“\n\n2019. [LINK](https://altai.insight -centre.org/), \n\nReferences \n\nMark J. Van der Laan and Sherri Rose (2018). Targeted Learning in Data Science. Cham: \n\nSpringer International Publishing, 2018. \n\nAlice Zheng. 2015. Evaluating Machine Learning Models (2015). O'Reilly. 82 of 142 \n\nBrenda Leong and Patrick Hall (2021). 5 things lawyers should know about artificial \n\nintelligence. ABA Journal. \n\nUK Centre for Data Ethics and Innovation, â€œThe roadmap to an effective AI assurance \n\necosystemâ€. \n\n# MAP 3.4 \n\nProcesses for operator and practitioner proficiency with AI system performance and \n\ntrustworthiness â€“ and relevant technical standards and certifications â€“ are defined, \n\nassessed and documented. \n\nAbout \n\nHuman -AI configurations can span from fully autonomous to fully manual. AI systems can \n\nautonomously make decisions, defer decision -making to a human expert, or be used by a \n\nhuman decision -maker as an additional opinion. In some scenarios, professionals with \n\nexpertise in a specific domain work in conjunction with an AI system towards a specific end \n\ngoal â€”for example, a decision about another individual(s). Depending on the purpose of the \n\nsystem, the expert may interact with the AI system but is rarely part of the design or \n\ndevelopment of the system itself. These experts are not necessarily familiar with machine \n\nlearning, data science, computer science, or other fields traditionally associated with AI \n\ndesign or development and - depending on the application - will likely not require such \n\nfamiliarity. For example, for AI systems that are deployed in health care delivery the experts \n\nare the physicians and bring their expertise about medicine â€”not data science, data \n\nmodeling and engineering, or other computational factors. The challenge in these settings is \n\nnot educating the end user about AI system capabilities, but rather leveraging, and not \n\nreplacing, practitioner domain expertise. \n\nQuestions remain about how to configure humans and automation for managing AI risks. \n\nRisk management is enhanced when organizations that design, develop or deploy AI \n\nsystems for use by professional operators and practitioners: \n\nâ€¢ are aware of these knowledge limitations and strive to identify risks in human -AI \n\ninteractions and configurations across all contexts, and the potential resulting impacts, \n\nâ€¢ define and differentiate the various human roles and responsibilities when using or \n\ninteracting with AI systems, and \n\nâ€¢ determine proficiency standards for AI system operation in proposed context of use, as \n\nenumerated in MAP -1 and established in GOVERN -3.2. \n\nSuggested Actions \n\nâ€¢ Identify and declare AI system features and capabilities that may affect downstream AI \n\nactorsâ€™ decision -making in deployment and operational settings for example how \n\nsystem features and capabilities may activate known risks in various human -AI \n\nconfigurations, such as selective adherence. \n\nâ€¢ Identify skills and proficiency requirements for operators, practitioners and other \n\ndomain experts that interact with AI systems,Develop AI system operational 83 of 142 \n\ndocumentation for AI actors in deployed and operational environments, including \n\ninformation about known risks, mitigation criteria, and trustworthy characteristics \n\nenumerated in Map -1. \n\nâ€¢ Define and develop training materials for proposed end users, practitioners and \n\noperators about AI system use and known limitations. \n\nâ€¢ Define and develop certification procedures for operating AI systems within defined \n\ncontexts of use, and information about what exceeds operational boundaries. \n\nâ€¢ Include operators, practitioners and end users in AI system prototyping and testing \n\nactivities to help inform operational boundaries and acceptable performance. Conduct \n\ntesting activities under scenarios similar to deployment conditions. \n\nâ€¢ Verify model output provided to AI system operators, practitioners and end users is \n\ninteractive, and specified to context and user requirements defined in MAP -1. \n\nâ€¢ Verify AI system output is interpretable and unambiguous for downstream decision \n\nmaking tasks. \n\nâ€¢ Design AI system explanation complexity to match the level of problem and context \n\ncomplexity. \n\nâ€¢ Verify that design principles are in place for safe operation by AI actors in decision -\n\nmaking environments. \n\nâ€¢ Develop approaches to track human -AI configurations, operator, and practitioner \n\noutcomes for integration into continual improvement. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What policies has the entity developed to ensure the use of the AI system is consistent \n\nwith its stated values and principles? \n\nâ€¢ How will the accountable human(s) address changes in accuracy and precision due to \n\neither an adversaryâ€™s attempts to disrupt the AI or unrelated changes in \n\noperational/business environment, which may impact the accuracy of the AI? \n\nâ€¢ How does the entity assess whether personnel have the necessary skills, training, \n\nresources, and domain knowledge to fulfill their assigned responsibilities? \n\nâ€¢ Are the relevant staff dealing with AI systems properly trained to interpret AI model \n\noutput and decisions as well as to detect and manage bias in data? \n\nâ€¢ What metrics has the entity developed to measure performance of various components? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - 2020. \n\nReferences \n\nNational Academies of Sciences, Engineering, and Medicine. 2022. Human -AI Teaming: \n\nState -of -the -Art and Research Needs. Washington, DC: The National Academies Press. 84 of 142 \n\nHuman Readiness Level Scale in the System Development Process, American National \n\nStandards Institute and Human Factors and Ergonomics Society, ANSI/HFES 400 -2021. \n\nHuman -Machine Teaming Systems Engineering Guide. P McDermott, C Dominguez, N \n\nKasdaglis, M Ryan, I Trahan, A Nelson. MITRE Corporation, 2018. \n\nSaar Alon -Barkat, Madalina Busuioc, Human â€“AI Interactions in Public Sector Decision \n\nMaking: â€œAutomation Biasâ€ and â€œSelective Adherenceâ€ to Algorithmic Advice, Journal of \n\nPublic Administration Research and Theory, 2022;, muac007. \n\nBreana M. Carter -Browne, Susannah B. F. Paletz, Susan G. Campbell , Melissa J. Carraway, \n\nSarah H. Vahlkamp, Jana Schwartz , Polly Oâ€™Rourke, â€œThere is No â€œAIâ€ in Teams: A \n\nMultidisciplinary Framework for AIs to Work in Human Teams; Applied Research \n\nLaboratory for Intelligence and Security (ARLIS) Report, June 2021. \n\nR Crootof, ME Kaminski, and WN Price II. Humans in the Loop (March 25, 2022). Vanderbilt \n\nLaw Review, Forthcoming 2023, U of Colorado Law Legal Studies Research Paper No. 22 -10, \n\nU of Michigan Public Law Research Paper No. 22 -011. \n\nS Mo Jones -Jang, Yong Jin Park, How do people react to AI failure? Automation bias, \n\nalgorithmic aversion, and perceived controllability, Journal of Computer -Mediated \n\nCommunication, Volume 28, Issue 1, January 2023, zmac029. \n\nA Knack, R Carter and A Babuta, \"Human -Machine Teaming in Intelligence Analysis: \n\nRequirements for developing trust in machine learning systems,\" CETaS Research Reports \n\n(December 2022). \n\nSD Ramchurn, S Stein , NR Jennings. Trustworthy human -AI partnerships. iScience. \n\n2021;24(8):102891. Published 2021 Jul 24. doi:10.1016/j.isci.2021.102891. \n\nM. Veale, M. Van Kleek, and R. Binns, â€œFairness and Accountability Design Needs for \n\nAlgorithmic Support in High -Stakes Public Sector Decision -Making,â€ in Proceedings of the \n\n2018 CHI Conference on Human Factors in Computing Systems - CHI â€™18. Montreal QC, \n\nCanada: ACM Press, 2018, pp. 1 â€“14. \n\n# MAP 3.5 \n\nProcesses for human oversight are defined, assessed, and documented in accordance with \n\norganizational policies from GOVERN function. \n\nAbout \n\nAs AI systems have evolved in accuracy and precision, computational systems have moved \n\nfrom being used purely for decision support â€”or for explicit use by and under the \n\ncontrol of a human operator â€”to automated decision making with limited input from \n\nhumans. Computational decision support systems augment another, typically human, \n\nsystem in making decisions.These types of configurations increase the likelihood of outputs \n\nbeing produced with little human involvement. 85 of 142 \n\nDefining and differentiating various human roles and responsibilities for AI systemsâ€™ \n\ngovernance, and differentiating AI system overseers and those using or interacting with AI \n\nsystems can enhance AI risk management activities. \n\nIn critical systems, high -stakes settings, and systems deemed high -risk it is of vital \n\nimportance to evaluate risks and effectiveness of oversight procedures before an AI system \n\nis deployed. \n\nUltimately, AI system oversight is a shared responsibility, and attempts to properly \n\nauthorize or govern oversight practices will not be effective without organizational buy -in \n\nand accountability mechanisms, for example those suggested in the GOVERN function. \n\nSuggested Actions \n\nâ€¢ Identify and document AI systemsâ€™ features and capabilities that require human \n\noversight, in relation to operational and societal contexts, trustworthy characteristics, \n\nand risks identified in MAP -1. \n\nâ€¢ Establish practices for AI systemsâ€™ oversight in accordance with policies developed in \n\nGOVERN -1. \n\nâ€¢ Define and develop training materials for relevant AI Actors about AI system \n\nperformance, context of use, known limitations and negative impacts, and suggested \n\nwarning labels. \n\nâ€¢ Include relevant AI Actors in AI system prototyping and testing activities. Conduct \n\ntesting activities under scenarios similar to deployment conditions. \n\nâ€¢ Evaluate AI system oversight practices for validity and reliability. When oversight \n\npractices undergo extensive updates or adaptations, retest, evaluate results, and course \n\ncorrect as necessary. \n\nâ€¢ Verify that model documents contain interpretable descriptions of system mechanisms, \n\nenabling oversight personnel to make informed, risk -based decisions about system \n\nrisks. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\nâ€¢ How does the entity assess whether personnel have the necessary skills, training, \n\nresources, and domain knowledge to fulfill their assigned responsibilities? \n\nâ€¢ Are the relevant staff dealing with AI systems properly trained to interpret AI model \n\noutput and decisions as well as to detect and manage bias in data? \n\nâ€¢ To what extent has the entity documented the AI systemâ€™s development, testing \n\nmethodology, metrics, and performance outcomes? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. 86 of 142 \n\nReferences \n\nBen Green, â€œThe Flaws of Policies Requiring Human Oversight of Government Algorithms,â€ \n\nSSRN Journal, 2021. \n\nLuciano Cavalcante Siebert, Maria Luce Lupetti, Evgeni Aizenberg, Niek Beckers, Arkady \n\nZgonnikov, Herman Veluwenkamp, David Abbink, Elisa Giaccardi, Geert -Jan Houben, \n\nCatholijn Jonker, Jeroen van den Hoven, Deborah Forster, & Reginald Lagendijk (2021). \n\nMeaningful human control: actionable properties for AI system development. AI and Ethics. \n\nMary Cummings, (2014). Automation and Accountability in Decision Support System \n\nInterface Design. The Journal of Technology Studies. 32. 10.21061/jots.v32i1.a.4. \n\nMadeleine Elish, M. (2016). Moral Crumple Zones: Cautionary Tales in Human -Robot \n\nInteraction (WeRobot 2016). SSRN Electronic Journal. 10.2139/ssrn.2757236. \n\nR Crootof, ME Kaminski, and WN Price II. Humans in the Loop (March 25, 2022). Vanderbilt \n\nLaw Review, Forthcoming 2023, U of Colorado Law Legal Studies Research Paper No. 22 -10, \n\nU of Michigan Public Law Research Paper No. 22 -011. \n\n[LINK](https://ssrn.com/abstract=4066781), \n\nBogdana Rakova, Jingying Yang, Henriette Cramer, & Rumman Chowdhury (2020). Where \n\nResponsible AI meets Reality. Proceedings of the ACM on Human -Computer Interaction, 5, 1 \n\n- 23. \n\n# MAP 4.1 \n\nApproaches for mapping AI technology and legal risks of its components â€“ including the use \n\nof third -party data or software â€“ are in place, followed, and documented, as are risks of \n\ninfringement of a third -partyâ€™s intellectual property or other rights. \n\nAbout \n\nTechnologies and personnel from third -parties are another potential sources of risk to \n\nconsider during AI risk management activities. Such risks may be difficult to map since risk \n\npriorities or tolerances may not be the same as the deployer organization. \n\nFor example, the use of pre -trained models, which tend to rely on large uncurated dataset \n\nor often have undisclosed origins, has raised concerns about privacy, bias, and \n\nunanticipated effects along with possible introduction of increased levels of statistical \n\nuncertainty, difficulty with reproducibility, and issues with scientific validity. \n\nSuggested Actions \n\nâ€¢ Review audit reports, testing results, product roadmaps, warranties, terms of service, \n\nend user license agreements, contracts, and other documentation related to third -party \n\nentities to assist in value assessment and risk management activities. \n\nâ€¢ Review third -party software release schedules and software change management plans \n\n(hotfixes, patches, updates, forward - and backward - compatibility guarantees) for \n\nirregularities that may contribute to AI system risks. 87 of 142 \n\nâ€¢ Inventory third -party material (hardware, open -source software, foundation models, \n\nopen source data, proprietary software, proprietary data, etc.) required for system \n\nimplementation and maintenance. \n\nâ€¢ Review redundancies related to third -party technology and personnel to assess \n\npotential risks due to lack of adequate support. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Did you establish a process for third parties (e.g. suppliers, end users, subjects, \n\ndistributors/vendors or workers) to report potential vulnerabilities, risks or biases in \n\nthe AI system? \n\nâ€¢ If your organization obtained datasets from a third party, did your organization assess \n\nand manage the risks of using such datasets? \n\nâ€¢ How will the results be independently verified? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nReferences \n\nLanguage models \n\nEmily M. Bender, Timnit Gebru, Angelina McMillan -Major, and Shmargaret Shmitchell. 2021. \n\nOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ . In Proceedings \n\nof the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21). \n\nAssociation for Computing Machinery, New York, NY, USA, 610 â€“623. \n\nJulia Kreutzer, Isaac Caswell, Lisa Wang, et al. 2022. Quality at a Glance: An Audit of Web -\n\nCrawled Multilingual Datasets. Transactions of the Association for Computational \n\nLinguistics 10 (2022), 50 â€“72. \n\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, et al. 2022. Taxonomy of Risks posed by \n\nLanguage Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency \n\n(FAccT '22). Association for Computing Machinery, New York, NY, USA, 214 â€“229. \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. \n\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. 2021. On the Opportunities and Risks \n\nof Foundation Models. arXiv:2108.07258. \n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani \n\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, 88 of 142 \n\nOriol Vinyals, Percy Liang, Jeff Dean, William Fedus. â€œEmergent Abilities of Large Language \n\nModels.â€ ArXiv abs/2206.07682 (2022). \n\n# MAP 4.2 \n\nInternal risk controls for components of the AI system including third -party AI technologies \n\nare identified and documented. \n\nAbout \n\nIn the course of their work, AI actors often utilize open -source, or otherwise freely \n\navailable, third -party technologies â€“ some of which may have privacy, bias, and security \n\nrisks. Organizations may consider internal risk controls for these technology sources and \n\nbuild up practices for evaluating third -party material prior to deployment. \n\nSuggested Actions \n\nâ€¢ Track third -parties preventing or hampering risk -mapping as indications of increased \n\nrisk. \n\nâ€¢ Supply resources such as model documentation templates and software safelists to \n\nassist in third -party technology inventory and approval activities. \n\nâ€¢ Review third -party material (including data and models) for risks related to bias, data \n\nprivacy, and security vulnerabilities. \n\nâ€¢ Apply traditional technology risk controls â€“ such as procurement, security, and data \n\nprivacy controls â€“ to all acquired third -party technologies. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Can the AI system be audited by independent third parties? \n\nâ€¢ To what extent do these policies foster public trust and confidence in the use of the AI \n\nsystem? \n\nâ€¢ Are mechanisms established to facilitate the AI systemâ€™s auditability (e.g. traceability of \n\nthe development process, the sourcing of training data and the logging of the AI \n\nsystemâ€™s processes, outcomes, positive and negative impact)? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nâ€¢ WEF Model AI Governance Framework Assessment 2020. \n\nâ€¢ Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI -\n\n2019. [LINK](https://altai.insight -centre.org/), \n\nReferences \n\nOffice of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk \n\nManagement, Version 1.0, August 2021. Retrieved on July 7, 2022. \n\nProposed Interagency Guidance on Third -Party Relationships: Risk Management, 2021. 89 of 142 \n\nKang, D., Raghavan, D., Bailis, P.D., & Zaharia, M.A. (2020). Model Assertions for Monitoring \n\nand Improving ML Models. ArXiv, abs/2003.01668. \n\n# MAP 5.1 \n\nLikelihood and magnitude of each identified impact (both potentially beneficial and \n\nharmful) based on expected use, past uses of AI systems in similar contexts, public incident \n\nreports, feedback from those external to the team that developed or deployed the AI system, \n\nor other data are identified and documented. \n\nAbout \n\nAI actors can evaluate, document and triage the likelihood of AI system impacts identified in \n\nMap 5.1 Likelihood estimates may then be assessed and judged for go/no -go decisions \n\nabout deploying an AI system. If an organization decides to proceed with deploying the \n\nsystem, the likelihood and magnitude estimates can be used to assign TEVV resources \n\nappropriate for the risk level. \n\nSuggested Actions \n\nâ€¢ Establish assessment scales for measuring AI systemsâ€™ impact. Scales may be qualitative, \n\nsuch as red -amber -green (RAG), or may entail simulations or econometric approaches. \n\nDocument and apply scales uniformly across the organizationâ€™s AI portfolio. \n\nâ€¢ Apply TEVV regularly at key stages in the AI lifecycle, connected to system impacts and \n\nfrequency of system updates. \n\nâ€¢ Identify and document likelihood and magnitude of system benefits and negative \n\nimpacts in relation to trustworthiness characteristics. \n\nâ€¢ Establish processes for red teaming to identify and connect system limitations to AI \n\nlifecycle stage(s) and potential downstream impacts \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Which population(s) does the AI system impact? \n\nâ€¢ What assessments has the entity conducted on trustworthiness characteristics for \n\nexample data security and privacy impacts associated with the AI system? \n\nâ€¢ Can the AI system be tested by independent third parties? \n\nAI Transparency Resources \n\nâ€¢ Datasheets for Datasets. \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nâ€¢ Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI -\n\n2019. [LINK](https://altai.insight -centre.org/), 90 of 142 \n\nReferences \n\nEmilio GÃ³mez -GonzÃ¡lez and Emilia GÃ³mez. 2020. Artificial intelligence in medicine and \n\nhealthcare. Joint Research Centre (European Commission). \n\nArtificial Intelligence Incident Database. 2022. \n\nAnthony M. Barrett, Dan Hendrycks, Jessica Newman and Brandie Nonnecke. â€œActionable \n\nGuidance for High -Consequence AI Risk Management: Towards Standards Addressing AI \n\nCatastrophic Risks\". ArXiv abs/2206.08966 (2022) \n\nGanguli, D., et al. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling \n\nBehaviors, and Lessons Learned. arXiv. https://arxiv.org/abs/2209.07858 \n\nUpol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, and Hal DaumÃ©. 2024. Seamful XAI: \n\nOperationalizing Seamful Design in Explainable AI. Proc. ACM Hum. -Comput. Interact. 8, \n\nCSCW1, Article 119. https://doi.org/10.1145/3637396 \n\n# MAP 5.2 \n\nPractices and personnel for supporting regular engagement with relevant AI actors and \n\nintegrating feedback about positive, negative, and unanticipated impacts are in place and \n\ndocumented. \n\nAbout \n\nAI systems are socio -technical in nature and can have positive, neutral, or negative \n\nimplications that extend beyond their stated purpose. Negative impacts can be wide -\n\nranging and affect individuals, groups, communities, organizations, and society, as well as \n\nthe environment and national security. \n\nOrganizations can create a baseline for system monitoring to increase opportunities for \n\ndetecting emergent risks. After an AI system is deployed, engaging different stakeholder \n\ngroups â€“ who may be aware of, or experience, benefits or negative impacts that are \n\nunknown to AI actors involved in the design, development and deployment activities â€“\n\nallows organizations to understand and monitor system benefits and potential negative \n\nimpacts more readily. \n\nSuggested Actions \n\nâ€¢ Establish and document stakeholder engagement processes at the earliest stages of \n\nsystem formulation to identify potential impacts from the AI system on individuals, \n\ngroups, communities, organizations, and society. \n\nâ€¢ Employ methods such as value sensitive design (VSD) to identify misalignments \n\nbetween organizational and societal values, and system implementation and impact. \n\nâ€¢ Identify approaches to engage, capture, and incorporate input from system end users \n\nand other key stakeholders to assist with continuous monitoring for potential impacts \n\nand emergent risks. 91 of 142 \n\nâ€¢ Incorporate quantitative, qualitative, and mixed methods in the assessment and \n\ndocumentation of potential impacts to individuals, groups, communities, organizations, \n\nand society. \n\nâ€¢ Identify a team (internal or external) that is independent of AI design and development \n\nfunctions to assess AI system benefits, positive and negative impacts and their \n\nlikelihood and magnitude. \n\nâ€¢ Evaluate and document stakeholder feedback to assess potential impacts for actionable \n\ninsights regarding trustworthiness characteristics and changes in design approaches \n\nand principles. \n\nâ€¢ Develop TEVV procedures that incorporate socio -technical elements and methods and \n\nplan to normalize across organizational culture. Regularly review and refine TEVV \n\nprocesses. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ If the AI system relates to people, does it unfairly advantage or disadvantage a \n\nparticular social group? In what ways? How was this managed? \n\nâ€¢ If the AI system relates to other ethically protected groups, have appropriate obligations \n\nbeen met? (e.g., medical data might include information collected from animals) \n\nâ€¢ If the AI system relates to people, could this dataset expose people to harm or legal \n\naction? (e.g., financial social or otherwise) What was done to mitigate or reduce the \n\npotential for harm? \n\nAI Transparency Resources \n\nâ€¢ Datasheets for Datasets. \n\nâ€¢ GAO -21 -519SP: AI Accountability Framework for Federal Agencies & Other Entities. \n\nâ€¢ AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. \n\nâ€¢ Intel.gov: AI Ethics Framework for Intelligence Community - 2020. \n\nâ€¢ Assessment List for Trustworthy AI (ALTAI) - The High -Level Expert Group on AI -\n\n2019. [LINK](https://altai.insight -centre.org/), \n\nReferences \n\nSusanne Vernim, Harald Bauer, Erwin Rauch, et al. 2022. A value sensitive design approach \n\nfor designing AI -based worker assistance systems in manufacturing. Procedia Comput. Sci. \n\n200, C (2022), 505 â€“516. \n\nHarini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm \n\nthroughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from \n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of \n\nImagination in AI Infused System Development and Deployment. arXiv:2011.13416. \n\nKonstantinia Charitoudi and Andrew Blyth. A Socio -Technical Approach to Cyber Risk \n\nManagement and Impact Assessment. Journal of Information Security 4, 1 (2013), 33 -41. 92 of 142 \n\nRaji, I.D., Smart, A., White, R.N., Mitchell, M., Gebru, T., Hutchinson, B., Smith -Loud, J., Theron, \n\nD., & Barnes, P. (2020). Closing the AI accountability gap: defining an end -to -end framework \n\nfor internal algorithmic auditing. Proceedings of the 2020 Conference on Fairness, \n\nAccountability, and Transparency. \n\nEmanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, & Jacob Metcalf. \n\n2021. Assemlbing Accountability: Algorithmic Impact Assessment for the Public Interest. \n\nData & Society. Accessed 7/14/2022 at \n\nShari Trewin (2018). AI Fairness for People with Disabilities: Point of View. ArXiv, \n\nabs/1811.10670. \n\nAda Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. \n\nAccessed July 14, 2022. \n\nMicrosoft Responsible AI Impact Assessment Template. 2022. Accessed July 14, 2022. \n\nMicrosoft Responsible AI Impact Assessment Guide. 2022. Accessed July 14, 2022. \n\nMicrosoft Responsible AI Standard, v2. \n\nMicrosoft Research AI Fairness Checklist. \n\nPEAT AI & Disability Inclusion Toolkit â€“ Risks of Bias and Discrimination in AI Hiring Tools. MEASURE 93 of 142 \n\n# Measure \n\nAppropriate methods and metrics are identified and applied. \n\n# MEASURE 1.1 \n\nApproaches and metrics for measurement of AI risks enumerated during the Map function \n\nare selected for implementation starting with the most significant AI risks. The risks or \n\ntrustworthiness characteristics that will not â€“ or cannot â€“ be measured are properly \n\ndocumented. \n\nAbout \n\nThe development and utility of trustworthy AI systems depends on reliable measurements \n\nand evaluations of underlying technologies and their use. Compared with traditional \n\nsoftware systems, AI technologies bring new failure modes, inherent dependence on \n\ntraining data and methods which directly tie to data quality and representativeness. \n\nAdditionally, AI systems are inherently socio -technical in nature, meaning they are \n\ninfluenced by societal dynamics and human behavior. AI risks â€“ and benefits â€“ can emerge \n\nfrom the interplay of technical aspects combined with societal factors related to how a \n\nsystem is used, its interactions with other AI systems, who operates it, and the social \n\ncontext in which it is deployed. In other words, What should be measured depends on the \n\npurpose, audience, and needs of the evaluations. \n\nThese two factors influence selection of approaches and metrics for measurement of AI \n\nrisks enumerated during the Map function. The AI landscape is evolving and so are the \n\nmethods and metrics for AI measurement. The evolution of metrics is key to maintaining \n\nefficacy of the measures. \n\nSuggested Actions \n\nâ€¢ Establish approaches for detecting, tracking and measuring known risks, errors, \n\nincidents or negative impacts. \n\nâ€¢ Identify testing procedures and metrics to demonstrate whether or not the system is fit \n\nfor purpose and functioning as claimed. \n\nâ€¢ Identify testing procedures and metrics to demonstrate AI system trustworthiness \n\nâ€¢ Define acceptable limits for system performance (e.g. distribution of errors), and \n\ninclude course correction suggestions if/when the system performs beyond acceptable \n\nlimits. \n\nâ€¢ Define metrics for, and regularly assess, AI actor competency for effective system \n\noperation, \n\nâ€¢ Identify transparency metrics to assess whether stakeholders have access to necessary \n\ninformation about system design, development, deployment, use, and evaluation. \n\nâ€¢ Utilize accountability metrics to determine whether AI designers, developers, and \n\ndeployers maintain clear and transparent lines of responsibility and are open to \n\ninquiries. \n\nâ€¢ Document metric selection criteria and include considered but unused metrics. 94 of 142 \n\nâ€¢ Monitor AI system external inputs including training data, models developed for other \n\ncontexts, system components reused from other contexts, and third -party tools and \n\nresources. \n\nâ€¢ Report metrics to inform assessments of system generalizability and reliability. \n\nâ€¢ Assess and document pre - vs post -deployment system performance. Include existing \n\nand emergent risks. \n\nâ€¢ Document risks or trustworthiness characteristics identified in the Map function that \n\nwill not be measured, including justification for non - measurement. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ How will the appropriate performance metrics, such as accuracy, of the AI be monitored \n\nafter the AI is deployed? \n\nâ€¢ What corrective actions has the entity taken to enhance the quality, accuracy, reliability, \n\nand representativeness of the data? \n\nâ€¢ Are there recommended data splits or evaluation measures? (e.g., training, \n\ndevelopment, testing; accuracy/AUC) \n\nâ€¢ Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? \n\nâ€¢ What testing, if any, has the entity conducted on the AI system to identify errors and \n\nlimitations (i.e. manual vs automated, adversarial and stress testing)? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nSara R. Jordan. â€œDesigning Artificial Intelligence Review Boards: Creating Risk Metrics for \n\nReview of AI.â€ 2019 IEEE International Symposium on Technology and Society (ISTAS), \n\n2019. \n\nIEEE. â€œIEEE -1012 -2016: IEEE Standard for System, Software, and Hardware Verification and \n\nValidation.â€ IEEE Standards Association. \n\nACM Technology Policy Council. â€œStatement on Principles for Responsible Algorithmic \n\nSystems.â€ Association for Computing Machinery (ACM), October 26, 2022. \n\nPerez, E., et al. (2022). Discovering Language Model Behaviors with Model -Written \n\nEvaluations. arXiv. https://arxiv.org/abs/2212.09251 \n\nGanguli, D., et al. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling \n\nBehaviors, and Lessons Learned. arXiv. https://arxiv.org/abs/2209.07858 95 of 142 \n\nDavid Piorkowski, Michael Hind, and John Richards. \"Quantitative AI Risk Assessments: \n\nOpportunities and Challenges.\" arXiv preprint, submitted January 11, 2023. \n\nDaniel Schiff, Aladdin Ayesh, Laura Musikanski, and John C. Havens. â€œIEEE 7010: A New \n\nStandard for Assessing the Well -Being Implications of Artificial Intelligence.â€ 2020 IEEE \n\nInternational Conference on Systems, Man, and Cybernetics (SMC), 2020. \n\n# MEASURE 1.2 \n\nAppropriateness of AI metrics and effectiveness of existing controls is regularly assessed \n\nand updated including reports of errors and impacts on affected communities. \n\nAbout \n\nDifferent AI tasks, such as neural networks or natural language processing, benefit from \n\ndifferent evaluation techniques. Use -case and particular settings in which the AI system is \n\nused also affects appropriateness of the evaluation techniques. Changes in the operational \n\nsettings, data drift, model drift are among factors that suggest regularly assessing and \n\nupdating appropriateness of AI metrics and their effectiveness can enhance reliability of AI \n\nsystem measurements. \n\nSuggested Actions \n\nâ€¢ Assess external validity of all measurements (e.g., the degree to which measurements \n\ntaken in one context can generalize to other contexts). \n\nâ€¢ Assess effectiveness of existing metrics and controls on a regular basis throughout the \n\nAI system lifecycle. \n\nâ€¢ Document reports of errors, incidents and negative impacts and assess sufficiency and \n\nefficacy of existing metrics for repairs, and upgrades \n\nâ€¢ Develop new metrics when existing metrics are insufficient or ineffective for \n\nimplementing repairs and upgrades. \n\nâ€¢ Develop and utilize metrics to monitor, characterize and track external inputs, including \n\nany third -party tools. \n\nâ€¢ Determine frequency and scope for sharing metrics and related information with \n\nstakeholders and impacted communities. \n\nâ€¢ Utilize stakeholder feedback processes established in the Map function to capture, act \n\nupon and share feedback from end users and potentially impacted communities. \n\nâ€¢ Collect and report software quality metrics such as rates of bug occurrence and severity, \n\ntime to response, and time to repair (See Manage 4.3). \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What metrics has the entity developed to measure performance of the AI system? \n\nâ€¢ To what extent do the metrics provide accurate and useful measure of performance? \n\nâ€¢ What corrective actions has the entity taken to enhance the quality, accuracy, reliability, \n\nand representativeness of the data? 96 of 142 \n\nâ€¢ How will the accuracy or appropriate performance metrics be assessed? \n\nâ€¢ What is the justification for the metrics selected? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nACM Technology Policy Council. â€œStatement on Principles for Responsible Algorithmic \n\nSystems.â€ Association for Computing Machinery (ACM), October 26, 2022. \n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical \n\nLearning: Data Mining, Inference, and Prediction. 2nd ed. Springer -Verlag, 2009. \n\nHarini Suresh and John Guttag. â€œA Framework for Understanding Sources of Harm \n\nThroughout the Machine Learning Life Cycle.â€ Equity and Access in Algorithms, \n\nMechanisms, and Optimization, October 2021. \n\nChristopher M. Bishop. Pattern Recognition and Machine Learning. New York: Springer, \n\n2006. \n\nSolon Barocas, Anhong Guo, Ece Kamar, Jacquelyn Krones, Meredith Ringel Morris, Jennifer \n\nWortman Vaughan, W. Duncan Wadsworth, and Hanna Wallach. â€œDesigning Disaggregated \n\nEvaluations of AI Systems: Choices, Considerations, and Tradeoffs.â€ Proceedings of the 2021 \n\nAAAI/ACM Conference on AI, Ethics, and Society, July 2021, 368 â€“78. \n\n# MEASURE 1.3 \n\nInternal experts who did not serve as front -line developers for the system and/or \n\nindependent assessors are involved in regular assessments and updates. Domain experts, \n\nusers, AI actors external to the team that developed or deployed the AI system, and affected \n\ncommunities are consulted in support of assessments as necessary per organizational risk \n\ntolerance. \n\nAbout \n\nThe current AI systems are brittle, the failure modes are not well described, and the systems \n\nare dependent on the context in which they were developed and do not transfer well \n\noutside of the training environment. A reliance on local evaluations will be necessary along \n\nwith a continuous monitoring of these systems. Measurements that extend beyond classical \n\nmeasures (which average across test cases) or expand to focus on pockets of failures where \n\nthere are potentially significant costs can improve the reliability of risk management \n\nactivities. Feedback from affected communities about how AI systems are being used can \n\nmake AI evaluation purposeful. Involving internal experts who did not serve as front -line \n\ndevelopers for the system and/or independent assessors regular assessments of AI systems \n\nhelps a fulsome characterization of AI systemsâ€™ performance and trustworthiness . 97 of 142 \n\nSuggested Actions \n\nâ€¢ Evaluate TEVV processes regarding incentives to identify risks and impacts. \n\nâ€¢ Utilize separate testing teams established in the Govern function (2.1 and 4.1) to enable \n\nindependent decisions and course -correction for AI systems. Track processes and \n\nmeasure and document change in performance. \n\nâ€¢ Plan and evaluate AI system prototypes with end user populations early and \n\ncontinuously in the AI lifecycle. Document test outcomes and course correct. \n\nâ€¢ Assess independence and stature of TEVV and oversight AI actors, to ensure they have \n\nthe required levels of independence and resources to perform assurance, compliance, \n\nand feedback tasks effectively \n\nâ€¢ Evaluate interdisciplinary and demographically diverse internal team established in \n\nMap 1.2 \n\nâ€¢ Evaluate effectiveness of external stakeholder feedback mechanisms, specifically related \n\nto processes for eliciting, evaluating and integrating input from diverse groups. \n\nâ€¢ Evaluate effectiveness of external stakeholder feedback mechanisms for enhancing AI \n\nactor visibility and decision making regarding AI system risks and trustworthy \n\ncharacteristics. \n\nâ€¢ Identify and utilize participatory approaches for assessing impacts that may arise from \n\nchanges in system deployment (e.g., introducing new technology, decommissioning \n\nalgorithms and models, adapting system, model or algorithm) \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\nâ€¢ How easily accessible and current is the information available to external stakeholders? \n\nâ€¢ To what extent does the entity communicate its AI strategic goals and objectives to the \n\ncommunity of stakeholders? \n\nâ€¢ To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nâ€¢ To what extent is this information sufficient and appropriate to promote transparency? \n\nDo external stakeholders have access to information on the design, operation, and \n\nlimitations of the AI system? \n\nâ€¢ What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. 98 of 142 \n\nReferences \n\nBoard of Governors of the Federal Reserve System. â€œSR 11 -7: Guidance on Model Risk \n\nManagement.â€ April 4, 2011. \n\nâ€œDefinition of independent verification and validation (IV&V)â€, in IEEE 1012, IEEE Standard \n\nfor System, Software, and Hardware Verification and Validation. Annex C, \n\nMona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. â€œParticipation Is Not a \n\nDesign Fix for Machine Learning.â€ Equity and Access in Algorithms, Mechanisms, and \n\nOptimization, October 2022. \n\nRediet Abebe and Kira Goldner. â€œMechanism Design for Social Good.â€ AI Matters 4, no. 3 \n\n(October 2018): 27 â€“34. \n\nUpol Ehsan, Ranjit Singh, Jacob Metcalf and Mark O. Riedl. â€œThe Algorithmic Imprint.â€ \n\nProceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency \n\n(2022). \n\n# MEASURE 2.1 \n\nTest sets, metrics, and details about the tools used during test, evaluation, validation, and \n\nverification (TEVV) are documented. \n\nAbout \n\nDocumenting measurement approaches, test sets, metrics, processes and materials used, \n\nand associated details builds foundation upon which to build a valid, reliable measurement \n\nprocess. Documentation enables repeatability and consistency, and can enhance AI risk \n\nmanagement decisions. \n\nSuggested Actions \n\nâ€¢ Leverage existing industry best practices for transparency and documentation of all \n\npossible aspects of measurements. Examples include: data sheet for data sets, model \n\ncards \n\nâ€¢ Regularly assess the effectiveness of tools used to document measurement approaches, \n\ntest sets, metrics, processes and materials used \n\nâ€¢ Update the tools as needed \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\nâ€¢ To what extent has the entity documented the AI systemâ€™s development, testing \n\nmethodology, metrics, and performance outcomes? 99 of 142 \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. \n\nReferences \n\nEmily M. Bender and Batya Friedman. â€œData Statements for Natural Language Processing: \n\nToward Mitigating System Bias and Enabling Better Science.â€ Transactions of the \n\nAssociation for Computational Linguistics 6 (2018): 587 â€“604. \n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben \n\nHutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. â€œModel Cards for \n\nModel Reporting.â€ FAT *19: Proceedings of the Conference on Fairness, Accountability, and \n\nTransparency, January 2019, 220 â€“29. \n\nIEEE Computer Society. â€œSoftware Engineering Body of Knowledge Version 3: IEEE \n\nComputer Society.â€ IEEE Computer Society. \n\nIEEE. â€œIEEE -1012 -2016: IEEE Standard for System, Software, and Hardware Verification and \n\nValidation.â€ IEEE Standards Association. \n\nBoard of Governors of the Federal Reserve System. â€œSR 11 -7: Guidance on Model Risk \n\nManagement.â€ April 4, 2011. \n\nAbigail Z. Jacobs and Hanna Wallach. â€œMeasurement and Fairness.â€ FAccT '21: Proceedings \n\nof the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, \n\n375 â€“85. \n\nJeanna Matthews, Bruce Hedin, Marc Canellas. Trustworthy Evidence for Trustworthy \n\nTechnology: An Overview of Evidence for Assessing the Trustworthiness of Autonomous \n\nand Intelligent Systems. IEEE -USA, September 29 2022. \n\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. â€œHard Choices in Artificial \n\nIntelligence.â€ Artificial Intelligence 300 (November 2021). \n\n# MEASURE 2.2 \n\nEvaluations involving human subjects meet applicable requirements (including human \n\nsubject protection) and are representative of the relevant population. \n\nAbout \n\nMeasurement and evaluation of AI systems often involves testing with human subjects or \n\nusing data captured from human subjects. Protection of human subjects is required by law \n\nwhen carrying out federally funded research, and is a domain specific requirement for some \n\ndisciplines. Standard human subjects protection procedures include protecting the welfare 100 of 142 \n\nand interests of human subjects, designing evaluations to minimize risks to subjects, and \n\ncompletion of mandatory training regarding legal requirements and expectations. \n\nEvaluations of AI system performance that utilize human subjects or human subject data \n\nshould reflect the population within the context of use. AI system activities utilizing non -\n\nrepresentative data may lead to inaccurate assessments or negative and harmful outcomes. \n\nIt is often difficult â€“ and sometimes impossible, to collect data or perform evaluation tasks \n\nthat reflect the full operational purview of an AI system. Methods for collecting, annotating, \n\nor using these data can also contribute to the challenge. To counteract these challenges, \n\norganizations can connect human subjects data collection, and dataset practices, to AI \n\nsystem contexts and purposes and do so in close collaboration with AI Actors from the \n\nrelevant domains. \n\nSuggested Actions \n\nâ€¢ Follow human subjects research requirements as established by organizational and \n\ndisciplinary requirements, including informed consent and compensation, during \n\ndataset collection activities. \n\nâ€¢ Analyze differences between intended and actual population of users or data subjects, \n\nincluding likelihood for errors, incidents or negative impacts. \n\nâ€¢ Utilize disaggregated evaluation methods (e.g. by race, age, gender, ethnicity, ability, \n\nregion) to improve AI system performance when deployed in real world settings. \n\nâ€¢ Establish thresholds and alert procedures for dataset representativeness within the \n\ncontext of use. \n\nâ€¢ Construct datasets in close collaboration with experts with knowledge of the context of \n\nuse. \n\nâ€¢ Follow intellectual property and privacy rights related to datasets and their use, \n\nincluding for the subjects represented in the data. \n\nâ€¢ Evaluate data representativeness through \n\nâ€¢ investigating known failure modes, \n\nâ€¢ assessing data quality and diverse sourcing, \n\nâ€¢ applying public benchmarks, \n\nâ€¢ traditional bias testing, \n\nâ€¢ chaos engineering, \n\nâ€¢ stakeholder feedback \n\nâ€¢ Use informed consent for individuals providing data used in system testing and \n\nevaluation. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? 101 of 142 \n\nâ€¢ How has the entity identified and mitigated potential impacts of bias in the data, \n\nincluding inequitable or discriminatory outcomes? \n\nâ€¢ To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\nâ€¢ To what extent has the entity identified and mitigated potential bias â€”statistical, \n\ncontextual, and historical â€”in the data? \n\nâ€¢ If it relates to people, were they told what the dataset would be used for and did they \n\nconsent? What community norms exist for data collected from human communications? \n\nIf consent was obtained, how? Were the people provided with any mechanism to revoke \n\ntheir consent in the future or for certain uses? \n\nâ€¢ If human subjects were used in the development or testing of the AI system, what \n\nprotections were put in place to promote their safety and wellbeing?. \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nUnited States Department of Health, Education, and Welfare's National Commission for the \n\nProtection of Human Subjects of Biomedical and Behavioral Research. The Belmont Report: \n\nEthical Principles and Guidelines for the Protection of Human Subjects of Research. Volume \n\nII. United States Department of Health and Human Services Office for Human Research \n\nProtections. April 18, 1979. \n\nOffice for Human Research Protections (OHRP). â€œ45 CFR 46.â€ United States Department of \n\nHealth and Human Services Office for Human Research Protections, March 10, 2021. \n\nOffice for Human Research Protections (OHRP). â€œHuman Subject Regulations Decision \n\nChart.â€ United States Department of Health and Human Services Office for Human Research \n\nProtections, June 30, 2020. \n\nJacob Metcalf and Kate Crawford. â€œWhere Are Human Subjects in Big Data Research? The \n\nEmerging Ethics Divide.â€ Big Data and Society 3, no. 1 (2016). \n\nBoaz Shmueli, Jan Fell, Soumya Ray, and Lun -Wei Ku. \"Beyond Fair Pay: Ethical Implications \n\nof NLP Crowdsourcing.\" arXiv preprint, submitted April 20, 2021. \n\nDivyansh Kaushik, Zachary C. Lipton, and Alex John London. \"Resolving the Human Subjects \n\nStatus of Machine Learning's Crowdworkers.\" arXiv preprint, submitted June 8, 2022. 102 of 142 \n\nOffice for Human Research Protections (OHRP). â€œInternational Compilation of Human \n\nResearch Standards.â€ United States Department of Health and Human Services Office for \n\nHuman Research Protections, February 7, 2022. \n\nNational Institutes of Health. â€œDefinition of Human Subjects Research.â€ NIH Central \n\nResource for Grants and Funding Information, January 13, 2020. \n\nJoy Buolamwini and Timnit Gebru. â€œGender Shades: Intersectional Accuracy Disparities in \n\nCommercial Gender Classification.â€ Proceedings of the 1st Conference on Fairness, \n\nAccountability and Transparency in PMLR 81 (2018): 77 â€“91. \n\nEun Seo Jo and Timnit Gebru. â€œLessons from Archives: Strategies for Collecting Sociocultural \n\nData in Machine Learning.â€ FAT* '20: Proceedings of the 2020 Conference on Fairness, \n\nAccountability, and Transparency, January 2020, 306 â€“16. \n\nMarco Gerardi, Katarzyna Barud, Marie -Catherine Wagner, Nikolaus Forgo, Francesca \n\nFallucchi, Noemi Scarpato, Fiorella Guadagni, and Fabio Massimo Zanzotto. \"Active \n\nInformed Consent to Boost the Application of Machine Learning in Medicine.\" arXiv \n\npreprint, submitted September 27, 2022. \n\nShari Trewin. \"AI Fairness for People with Disabilities: Point of View.\" arXiv preprint, \n\nsubmitted November 26, 2018. \n\nAndrea Brennen, Ryan Ashley, Ricardo Calix, JJ Ben -Joseph, George Sieniawski, Mona Gogia, \n\nand BNH.AI. AI Assurance Audit of RoBERTa, an Open source, Pretrained Large Language \n\nModel. IQT Labs, December 2022. \n\n# MEASURE 2.3 \n\nAI system performance or assurance criteria are measured qualitatively or quantitatively \n\nand demonstrated for conditions similar to deployment setting(s). Measures are \n\ndocumented. \n\nAbout \n\nThe current risk and impact environment suggests AI system performance estimates are \n\ninsufficient and require a deeper understanding of deployment context of use. \n\nComputationally focused performance testing and evaluation schemes are restricted to test \n\ndata sets and in silico techniques. These approaches do not directly evaluate risks and \n\nimpacts in real world environments and can only predict what might create impact based \n\non an approximation of expected AI use. To properly manage risks, more direct information \n\nis necessary to understand how and under what conditions deployed AI creates impacts, \n\nwho is most likely to be impacted, and what that experience is like. \n\nSuggested Actions \n\nâ€¢ Conduct regular and sustained engagement with potentially impacted communities \n\nâ€¢ Maintain a demographically diverse and multidisciplinary and collaborative internal \n\nteam 103 of 142 \n\nâ€¢ Regularly test and evaluate systems in non -optimized conditions, and in collaboration \n\nwith AI actors in user interaction and user experience (UI/UX) roles. \n\nâ€¢ Evaluate feedback from stakeholder engagement activities, in collaboration with human \n\nfactors and socio -technical experts. \n\nâ€¢ Collaborate with socio -technical, human factors, and UI/UX experts to identify notable \n\ncharacteristics in context of use that can be translated into system testing scenarios. \n\nâ€¢ Measure AI systems prior to deployment in conditions similar to expected scenarios. \n\nâ€¢ Measure and document performance criteria such as validity (false positive rate, false \n\nnegative rate, etc.) and efficiency (training times, prediction latency, etc.) related to \n\nground truth within the deployment context of use. \n\nâ€¢ Measure assurance criteria such as AI actor competency and experience. \n\nâ€¢ Document differences between measurement setting and the deployment \n\nenvironment(s). \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What experiments were initially run on this dataset? To what extent have experiments \n\non the AI system been documented? \n\nâ€¢ To what extent does the system/entity consistently measure progress towards stated \n\ngoals and objectives? \n\nâ€¢ How will the appropriate performance metrics, such as accuracy, of the AI be monitored \n\nafter the AI is deployed? How much distributional shift or model drift from baseline \n\nperformance is acceptable? \n\nâ€¢ As time passes and conditions change, is the training data still representative of the \n\noperational environment? \n\nâ€¢ What testing, if any, has the entity conducted on theAI system to identify errors and \n\nlimitations (i.e.adversarial or stress testing)? \n\nAI Transparency Resources \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical \n\nLearning: Data Mining, Inference, and Prediction. 2nd ed. Springer -Verlag, 2009. \n\nJessica Zosa Forde, A. Feder Cooper, Kweku Kwegyir -Aggrey, Chris De Sa, and Michael \n\nLittman. \"Model Selection's Disparate Impact in Real -World Deep Learning Applications.\" \n\narXiv preprint, submitted September 7, 2021. 104 of 142 \n\nInioluwa Deborah Raji, I. Elizabeth Kumar, Aaron Horowitz, and Andrew Selbst. â€œThe Fallacy \n\nof AI Functionality.â€ FAccT '22: 2022 ACM Conference on Fairness, Accountability, and \n\nTransparency, June 2022, 959 â€“72. \n\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex \n\nHanna. â€œData and Its (Dis)Contents: A Survey of Dataset Development and Use in Machine \n\nLearning Research.â€ Patterns 2, no. 11 (2021): 100336. \n\nChristopher M. Bishop. Pattern Recognition and Machine Learning. New York: Springer, \n\n2006. \n\nMd Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. \"A Comprehensive Study \n\non Deep Learning Bug Characteristics.\" arXiv preprint, submitted June 3, 2019. \n\nSwaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva, Chris Bryan, and Chitta Baral. \n\n\"DQI: Measuring Data Quality in NLP.\" arXiv preprint, submitted May 2, 2020. \n\nDoug Wielenga. \"Paper 073 -2007: Identifying and Overcoming Common Data Mining \n\nMistakes.\" SAS Global Forum 2007: Data Mining and Predictive Modeling, SAS Institute, \n\n2007. \n\nSoftware Resources \n\nâ€¢ Drifter  library (performance assessment) \n\nâ€¢ Manifold  library (performance assessment) \n\nâ€¢ MLextend  library (performance assessment) \n\nâ€¢ PiML  library (explainable models, performance assessment) \n\nâ€¢ SALib  library (performance assessment) \n\nâ€¢ What -If Tool  (performance assessment) \n\n# MEASURE 2.4 \n\nThe functionality and behavior of the AI system and its components â€“ as identified in the \n\nMAP function â€“ are monitored when in production. \n\nAbout \n\nAI systems may encounter new issues and risks while in production as the environment \n\nevolves over time. This effect, often referred to as â€œdriftâ€, means AI systems no longer meet \n\nthe assumptions and limitations of the original design. Regular monitoring allows AI Actors \n\nto monitor the functionality and behavior of the AI system and its components â€“ as \n\nidentified in the MAP function - and enhance the speed and efficacy of necessary system \n\ninterventions. \n\nSuggested Actions \n\nâ€¢ Monitor and document how metrics and performance indicators observed in production \n\ndiffer from the same metrics collected during pre -deployment testing. When differences \n\nare observed, consider error propagation and feedback loop risks. 105 of 142 \n\nâ€¢ Utilize hypothesis testing or human domain expertise to measure monitored \n\ndistribution differences in new input or output data relative to test environments \n\nâ€¢ Monitor for anomalies using approaches such as control limits, confidence intervals, \n\nintegrity constraints and ML algorithms. When anomalies are observed, consider error \n\npropagation and feedback loop risks. \n\nâ€¢ Verify alerts are in place for when distributions in new input data or generated \n\npredictions observed in production differ from pre -deployment test outcomes, or when \n\nanomalies are detected. \n\nâ€¢ Assess the accuracy and quality of generated outputs against new collected ground -\n\ntruth information as it becomes available. \n\nâ€¢ Utilize human review to track processing of unexpected data and reliability of generated \n\noutputs; warn system users when outputs may be unreliable. Verify that human \n\noverseers responsible for these processes have clearly defined responsibilities and \n\ntraining for specified tasks. \n\nâ€¢ Collect uses cases from the operational environment for system testing and monitoring \n\nactivities in accordance with organizational policies and regulatory or disciplinary \n\nrequirements (e.g. informed consent, institutional review board approval, human \n\nresearch protections), \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent is the output of each component appropriate for the operational \n\ncontext? \n\nâ€¢ What justifications, if any, has the entity provided for the assumptions, boundaries, and \n\nlimitations of the AI system? \n\nâ€¢ How will the appropriate performance metrics, such as accuracy, of the AI be monitored \n\nafter the AI is deployed? \n\nâ€¢ As time passes and conditions change, is the training data still representative of the \n\noperational environment? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nLuca Piano, Fabio Garcea, Valentina Gatteschi, Fabrizio Lamberti, and Lia Morra. â€œDetecting \n\nDrift in Deep Learning: A Methodology Primer.â€ IT Professional 24, no. 5 (2022): 53 â€“60. \n\nLarysa Visengeriyeva, et al. â€œAwesome MLOps.â€œ GitHub. 106 of 142 \n\n# MEASURE 2.5 \n\nThe AI system to be deployed is demonstrated to be valid and reliable. Limitations of the \n\ngeneralizability beyond the conditions under which the technology was developed are \n\ndocumented. \n\nAbout \n\nAn AI system that is not validated or that fails validation may be inaccurate or unreliable or \n\nmay generalize poorly to data and settings beyond its training, creating and increasing AI \n\nrisks and reducing trustworthiness. AI Actors can improve system validity by creating \n\nprocesses for exploring and documenting system limitations. This includes broad \n\nconsideration of purposes and uses for which the system was not designed. \n\nValidation risks include the use of proxies or other indicators that are often constructed by \n\nAI development teams to operationalize phenomena that are either not directly observable \n\nor measurable (e.g, fairness, hireability, honesty, propensity to commit a crime). Teams can \n\nmitigate these risks by demonstrating that the indicator is measuring the concept it claims \n\nto measure (also known as construct validity). Without this and other types of validation, \n\nvarious negative properties or impacts may go undetected, including the presence of \n\nconfounding variables, potential spurious correlations, or error propagation and its \n\npotential impact on other interconnected systems. \n\nSuggested Actions \n\nâ€¢ Define the operating conditions and socio -technical context under which the AI system \n\nwill be validated. \n\nâ€¢ Define and document processes to establish the systemâ€™s operational conditions and \n\nlimits. \n\nâ€¢ Establish or identify, and document approaches to measure forms of validity, including: \n\nâ€¢ construct validity (the test is measuring the concept it claims to measure) \n\nâ€¢ internal validity (relationship being tested is not influenced by other factors or \n\nvariables) \n\nâ€¢ external validity (results are generalizable beyond the training condition) \n\nâ€¢ the use of experimental design principles and statistical analyses and modeling. \n\nâ€¢ Assess and document system variance. Standard approaches include confidence \n\nintervals, standard deviation, standard error, bootstrapping, or cross -validation. \n\nâ€¢ Establish or identify, and document robustness measures. \n\nâ€¢ Establish or identify, and document reliability measures. \n\nâ€¢ Establish practices to specify and document the assumptions underlying measurement \n\nmodels to ensure proxies accurately reflect the concept being measured. \n\nâ€¢ Utilize standard software testing approaches (e.g. unit, integration, functional and chaos \n\ntesting, computer -generated test cases, etc.) \n\nâ€¢ Utilize standard statistical methods to test bias, inferential associations, correlation, and \n\ncovariance in adopted measurement models. 107 of 142 \n\nâ€¢ Utilize standard statistical methods to test variance and reliability of system outcomes. \n\nâ€¢ Monitor operating conditions for system performance outside of defined limits. \n\nâ€¢ Identify TEVV approaches for exploring AI system limitations, including testing \n\nscenarios that differ from the operational environment. Consult experts with knowledge \n\nof specific context of use. \n\nâ€¢ Define post -alert actions. Possible actions may include: \n\nâ€¢ alerting other relevant AI actors before action, \n\nâ€¢ requesting subsequent human review of action, \n\nâ€¢ alerting downstream users and stakeholder that the system is operating outside itâ€™s \n\ndefined validity limits, \n\nâ€¢ tracking and mitigating possible error propagation \n\nâ€¢ action logging \n\nâ€¢ Log input data and relevant system configuration information whenever there is an \n\nattempt to use the system beyond its well -defined range of system validity. \n\nâ€¢ Modify the system over time to extend its range of system validity to new operating \n\nconditions. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What testing, if any, has the entity conducted on theAI system to identify errors and \n\nlimitations (i.e.adversarial or stress testing)? \n\nâ€¢ Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\nâ€¢ How has the entity identified and mitigated potential impacts of bias in the data, \n\nincluding inequitable or discriminatory outcomes? \n\nâ€¢ To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\nâ€¢ What goals and objectives does the entity expect to achieve by designing, developing, \n\nand/or deploying the AI system? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nReferences \n\nAbigail Z. Jacobs and Hanna Wallach. â€œMeasurement and Fairness.â€ FAccT '21: Proceedings \n\nof the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, \n\n375 â€“85. \n\nDebugging Machine Learning Models. Proceedings of ICLR 2019 Workshop, May 6, 2019, \n\nNew Orleans, Louisiana. 108 of 142 \n\nPatrick Hall. â€œStrategies for Model Debugging.â€ Towards Data Science, November 8, 2019. \n\nSuchi Saria and Adarsh Subbaswamy. \"Tutorial: Safe and Reliable Machine Learning.\" arXiv \n\npreprint, submitted April 15, 2019. \n\nGoogle Developers. â€œOverview of Debugging ML Models.â€ Google Developers Machine \n\nLearning Foundational Courses, n.d. \n\nR. Mohanani, I. Salman, B. Turhan, P. RodrÃ­guez and P. Ralph, \"Cognitive Biases in Software \n\nEngineering: A Systematic Mapping Study,\" in IEEE Transactions on Software Engineering, \n\nvol. 46, no. 12, pp. 1318 -1339, Dec. 2020, \n\nSoftware Resources \n\nâ€¢ Drifter  library (performance assessment) \n\nâ€¢ Manifold  library (performance assessment) \n\nâ€¢ MLextend  library (performance assessment) \n\nâ€¢ PiML  library (explainable models, performance assessment) \n\nâ€¢ SALib  library (performance assessment) \n\nâ€¢ What -If Tool  (performance assessment) \n\n# MEASURE 2.6 \n\nAI system is evaluated regularly for safety risks â€“ as identified in the MAP function. The AI \n\nsystem to be deployed is demonstrated to be safe, its residual negative risk does not exceed \n\nthe risk tolerance, and can fail safely, particularly if made to operate beyond its knowledge \n\nlimits. Safety metrics implicate system reliability and robustness, real -time monitoring, and \n\nresponse times for AI system failures. \n\nAbout \n\nMany AI systems are being introduced into settings such as transportation, manufacturing \n\nor security, where failures may give rise to various physical or environmental harms. AI \n\nsystems that may endanger human life, health, property or the environment are tested \n\nthoroughly prior to deployment, and are regularly evaluated to confirm the system is safe \n\nduring normal operations, and in settings beyond its proposed use and knowledge limits. \n\nMeasuring activities for safety often relate to exhaustive testing in development and \n\ndeployment contexts, understanding the limits of a systemâ€™s reliable, robust, and safe \n\nbehavior, and real -time monitoring of various aspects of system performance. These \n\nactivities are typically conducted along with other risk mapping, management, and \n\ngovernance tasks such as avoiding past failed designs, establishing and rehearsing incident \n\nresponse plans that enable quick responses to system problems, the instantiation of \n\nredundant functionality to cover failures, and transparent and accountable governance. \n\nSystem safety incidents or failures are frequently reported to be related to organizational \n\ndynamics and culture. Independent auditors may bring important independent perspectives \n\nfor reviewing evidence of AI system safety. 109 of 142 \n\nSuggested Actions \n\nâ€¢ Thoroughly measure system performance in development and deployment contexts, \n\nand under stress conditions. \n\nâ€¢ Employ test data assessments and simulations before proceeding to production \n\ntesting. Track multiple performance quality and error metrics. \n\nâ€¢ Stress -test system performance under likely scenarios (e.g., concept drift, high load) \n\nand beyond known limitations, in consultation with domain experts. \n\nâ€¢ Test the system under conditions similar to those related to past known incidents or \n\nnear -misses and measure system performance and safety characteristics \n\nâ€¢ Apply chaos engineering approaches to test systems in extreme conditions and \n\ngauge unexpected responses. \n\nâ€¢ Document the range of conditions under which the system has been tested and \n\ndemonstrated to fail safely. \n\nâ€¢ Measure and monitor system performance in real -time to enable rapid response when \n\nAI system incidents are detected. \n\nâ€¢ Collect pertinent safety statistics (e.g., out -of -range performance, incident response \n\ntimes, system down time, injuries, etc.) in anticipation of potential information sharing \n\nwith impacted communities or as required by AI system oversight personnel. \n\nâ€¢ Align measurement to the goal of continuous improvement. Seek to increase the range \n\nof conditions under which the system is able to fail safely through system modifications \n\nin response to in -production testing and events. \n\nâ€¢ Document, practice and measure incident response plans for AI system incidents, \n\nincluding measuring response and down times. \n\nâ€¢ Compare documented safety testing and monitoring information with established risk \n\ntolerances on an on -going basis. \n\nâ€¢ Consult MANAGE for detailed information related to managing safety risks. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ What testing, if any, has the entity conducted on the AI system to identify errors and \n\nlimitations (i.e.adversarial or stress testing)? \n\nâ€¢ To what extent has the entity documented the AI systemâ€™s development, testing \n\nmethodology, metrics, and performance outcomes? \n\nâ€¢ Did you establish mechanisms that facilitate the AI systemâ€™s auditability (e.g. \n\ntraceability of the development process, the sourcing of training data and the logging of \n\nthe AI systemâ€™s processes, outcomes, positive and negative impact)? \n\nâ€¢ Did you ensure that the AI system can be audited by independent third parties? \n\nâ€¢ Did you establish a process for third parties (e.g. suppliers, end -users, subjects, \n\ndistributors/vendors or workers) to report potential vulnerabilities, risks or biases in \n\nthe AI system? 110 of 142 \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nAI Incident Database. 2022. \n\nAIAAIC Repository. 2022. \n\nNetflix. Chaos Monkey. \n\nIBM. â€œIBM's Principles of Chaos Engineering.â€ IBM, n.d. \n\nSuchi Saria and Adarsh Subbaswamy. \"Tutorial: Safe and Reliable Machine Learning.\" arXiv \n\npreprint, submitted April 15, 2019. \n\nDaniel Kang, Deepti Raghavan, Peter Bailis, and Matei Zaharia. \"Model assertions for \n\nmonitoring and improving ML models.\" Proceedings of Machine Learning and Systems 2 \n\n(2020): 481 -496. \n\nLarysa Visengeriyeva, et al. â€œAwesome MLOps.â€œ GitHub. \n\nMcGregor, S., Paeth, K., & Lam, K.T. (2022). Indexing AI Risks with Incidents, Issues, and \n\nVariants. ArXiv, abs/2211.10384. \n\n# MEASURE 2.7 \n\nAI system security and resilience â€“ as identified in the MAP function â€“ are evaluated and \n\ndocumented. \n\nAbout \n\nAI systems, as well as the ecosystems in which they are deployed, may be said to be resilient \n\nif they can withstand unexpected adverse events or unexpected changes in their \n\nenvironment or use â€“ or if they can maintain their functions and structure in the face of \n\ninternal \n\nand external change and degrade safely and gracefully when this is necessary. Common \n\nsecurity concerns relate to adversarial examples, data poisoning, and the exfiltration of \n\nmodels, training data, or other intellectual property through AI system endpoints. AI \n\nsystems that can maintain confidentiality, integrity, and availability through protection \n\nmechanisms that prevent unauthorized access and use may be said to be secure. \n\nSecurity and resilience are related but distinct characteristics. While resilience is the ability \n\nto return to normal function after an unexpected adverse event, security includes resilience \n\nbut also encompasses protocols to avoid, protect against, respond to, or recover 111 of 142 \n\nfrom attacks. Resilience relates to robustness and encompasses unexpected or adversarial \n\nuse (or abuse or misuse) of the model or data. \n\nSuggested Actions \n\nâ€¢ Establish and track AI system security tests and metrics (e.g., red -teaming activities, \n\nfrequency and rate of anomalous events, system down -time, incident response times, \n\ntime -to -bypass, etc.). \n\nâ€¢ Use red -team exercises to actively test the system under adversarial or stress \n\nconditions, measure system response, assess failure modes or determine if system can \n\nreturn to normal function after an unexpected adverse event. \n\nâ€¢ Document red -team exercise results as part of continuous improvement efforts, \n\nincluding the range of security test conditions and results. \n\nâ€¢ Use red -teaming exercises to evaluate potential mismatches between claimed and actual \n\nsystem performance. \n\nâ€¢ Use countermeasures (e.g, authentication, throttling, differential privacy, robust ML \n\napproaches) to increase the range of security conditions under which the system is able \n\nto return to normal function. \n\nâ€¢ Modify system security procedures and countermeasures to increase robustness and \n\nresilience to attacks in response to testing and events experienced in production. \n\nâ€¢ Verify that information about errors and attack patterns is shared with incident \n\ndatabases, other organizations with similar systems, and system users and stakeholders \n\n(MANAGE -4.1). \n\nâ€¢ Develop and maintain information sharing practices with AI actors from other \n\norganizations to learn from common attacks. \n\nâ€¢ Verify that third party AI resources and personnel undergo security audits and \n\nscreenings. Risk indicators may include failure of third parties to provide relevant \n\nsecurity information. \n\nâ€¢ Utilize watermarking technologies as a deterrent to data and model extraction attacks. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent does the plan specifically address risks associated with acquisition, \n\nprocurement of packaged software from vendors, cybersecurity controls, computational \n\ninfrastructure, data, data science, deployment mechanics, and system failure? \n\nâ€¢ What assessments has the entity conducted on data security and privacy impacts \n\nassociated with the AI system? \n\nâ€¢ What processes exist for data generation, acquisition/collection, security, maintenance, \n\nand dissemination? \n\nâ€¢ What testing, if any, has the entity conducted on the AI system to identify errors and \n\nlimitations (i.e. adversarial or stress testing)? \n\nâ€¢ If a third party created the AI, how will you ensure a level of explainability or \n\ninterpretability? 112 of 142 \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nMatthew P. Barrett. â€œFramework for Improving Critical Infrastructure Cybersecurity \n\nVersion 1.1.â€ National Institute of Standards and Technology (NIST), April 16, 2018. \n\nNicolas Papernot. \"A Marauder's Map of Security and Privacy in Machine Learning.\" arXiv \n\npreprint, submitted on November 3, 2018. \n\nGary McGraw, Harold Figueroa, Victor Shepardson, and Richie Bonett. â€œBIML Interactive \n\nMachine Learning Risk Framework.â€ Berryville Institute of Machine Learning (BIML), 2022. \n\nMitre Corporation. â€œMitre/Advmlthreatmatrix: Adversarial Threat Landscape for AI \n\nSystems.â€ GitHub, 2023. \n\nNational Institute of Standards and Technology (NIST). â€œCybersecurity Framework.â€ NIST, \n\n2023. \n\nUpol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, and Hal DaumÃ©. 2024. Seamful XAI: \n\nOperationalizing Seamful Design in Explainable AI. Proc. ACM Hum. -Comput. Interact. 8, \n\nCSCW1, Article 119. https://doi.org/10.1145/3637396 \n\nSoftware Resources \n\nâ€¢ adversarial -robustness -toolbox \n\nâ€¢ counterfit \n\nâ€¢ foolbox \n\nâ€¢ ml_privacy_meter \n\nâ€¢ robustness \n\nâ€¢ tensorflow/privacy \n\nâ€¢ projectGuardRail \n\n# MEASURE 2.8 \n\nRisks associated with transparency and accountability â€“ as identified in the MAP function â€“\n\nare examined and documented. \n\nAbout \n\nTransparency enables meaningful visibility into entire AI pipelines, workflows, processes or \n\norganizations and decreases information asymmetry between AI developers and operators \n\nand other AI Actors and impacted communities. Transparency is a central element of \n\neffective AI risk management that enables insight into how an AI system is working, and the \n\nability to address risks if and when they emerge. The ability for system users, individuals, or \n\nimpacted communities to seek redress for incorrect or problema tic AI system outcomes is 113 of 142 \n\none control for transparency and accountability. Higher level recourse processes are \n\ntypically enabled by lower level implementation efforts directed at explainability and \n\ninterpretability functionality. See Measure 2.9. \n\nTransparency and accountability across organizations and processes is crucial to reducing \n\nAI risks. Accountable leadership â€“ whether individuals or groups â€“ and transparent roles, \n\nresponsibilities, and lines of communication foster and incentivize quality assurance and \n\nrisk management activities within organizations. \n\nLack of transparency complicates measurement of trustworthiness and whether AI systems \n\nor organizations are subject to effects of various individual and group biases and design \n\nblindspots and could lead to diminished user, organizational and community trust, and \n\ndecreased overall system value. Enstating accountable and transparent organizational \n\nstructures along with documenting system risks can enable system improvement and risk \n\nmanagement efforts, allowing AI actors along the lifecycle to identify errors, suggest \n\nimprovements, and figure out new ways to contextualize and generalize AI system features \n\nand outcomes. \n\nSuggested Actions \n\nâ€¢ Instrument the system for measurement and tracking, e.g., by maintaining histories, \n\naudit logs and other information that can be used by AI actors to review and evaluate \n\npossible sources of error, bias, or vulnerability. \n\nâ€¢ Calibrate controls for users in close collaboration with experts in user interaction and \n\nuser experience (UI/UX), human computer interaction (HCI), and/or human -AI teaming. \n\nâ€¢ Test provided explanations for calibration with different audiences including operators, \n\nend users, decision makers and decision subjects (individuals for whom decisions are \n\nbeing made), and to enable recourse for consequential system decisions that affect end \n\nusers or subjects. \n\nâ€¢ Measure and document human oversight of AI systems: \n\nâ€¢ Document the degree of oversight that is provided by specified AI actors regarding \n\nAI system output. \n\nâ€¢ Maintain statistics about downstream actions by end users and operators such as \n\nsystem overrides. \n\nâ€¢ Maintain statistics about and document reported errors or complaints, time to \n\nrespond, and response types. \n\nâ€¢ Maintain and report statistics about adjudication activities. \n\nâ€¢ Track, document, and measure organizational accountability regarding AI systems via \n\npolicy exceptions and escalations, and document â€œgoâ€ and â€œno/goâ€ decisions made by \n\naccountable parties. \n\nâ€¢ Track and audit the effectiveness of organizational mechanisms related to AI risk \n\nmanagement, including: 114 of 142 \n\nâ€¢ Lines of communication between AI actors, executive leadership, users and \n\nimpacted communities. \n\nâ€¢ Roles and responsibilities for AI actors and executive leadership. \n\nâ€¢ Organizational accountability roles, e.g., chief model risk officers, AI oversight \n\ncommittees, responsible or ethical AI directors, etc. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent has the entity clarified the roles, responsibilities, and delegated \n\nauthorities to relevant stakeholders? \n\nâ€¢ What are the roles, responsibilities, and delegation of authorities of personnel involved \n\nin the design, development, deployment, assessment and monitoring of the AI system? \n\nâ€¢ Who is accountable for the ethical considerations during all stages of the AI lifecycle? \n\nâ€¢ Who will be responsible for maintaining, re -verifying, monitoring, and updating this AI \n\nonce deployed? \n\nâ€¢ Are the responsibilities of the personnel involved in the various AI governance \n\nprocesses clearly defined? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nNational Academies of Sciences, Engineering, and Medicine. Human -AI Teaming: State -of -\n\nthe -Art and Research Needs. 2022. \n\nInioluwa Deborah Raji and Jingying Yang. \"ABOUT ML: Annotation and Benchmarking on \n\nUnderstanding and Transparency of Machine Learning Lifecycles.\" arXiv preprint, \n\nsubmitted January 8, 2020. \n\nAndrew Smith. \"Using Artificial Intelligence and Algorithms.\" Federal Trade Commission \n\nBusiness Blog, April 8, 2020. \n\nBoard of Governors of the Federal Reserve System. â€œSR 11 -7: Guidance on Model Risk \n\nManagement.â€ April 4, 2011. \n\nJoshua A. Kroll. â€œOutlining Traceability: A Principle for Operationalizing Accountability in \n\nComputing Systems.â€ FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, \n\nAccountability, and Transparency, March 1, 2021, 758 â€“71. \n\nJennifer Cobbe, Michelle Seng Lee, and Jatinder Singh. â€œReviewable Automated Decision -\n\nMaking: A Framework for Accountable Algorithmic Systems.â€ FAccT '21: Proceedings of the \n\n2021 ACM Conference on Fairness, Accountability, and Transparency, March 1, 2021, 598 â€“\n\n609. 115 of 142 \n\n# MEASURE 2.9 \n\nThe AI model is explained, validated, and documented, and AI system output is interpreted \n\nwithin its context â€“ as identified in the MAP function â€“ and to inform responsible use and \n\ngovernance. \n\nAbout \n\nExplainability and interpretability assist those operating or overseeing an AI system, as well \n\nas users of an AI system, to gain deeper insights into the functionality and trustworthiness \n\nof the system, including its outputs. \n\nExplainable and interpretable AI systems offer information that help end users understand \n\nthe purposes and potential impact of an AI system. Risk from lack of explainability may be \n\nmanaged by describing how AI systems function, with descriptions tailored to individual \n\ndifferences such as the userâ€™s role, knowledge, and skill level. Explainable systems can be \n\ndebugged and monitored more easily, and they lend themselves to more thorough \n\ndocumentation, audit, and governance. \n\nRisks to interpretability often can be addressed by communicating a description of why \n\nan AI system made a particular prediction or recommendation. \n\nTransparency, explainability, and interpretability are distinct characteristics that support \n\neach other. Transparency can answer the question of â€œwhat happenedâ€. Explainability can \n\nanswer the question of â€œhowâ€ a decision was made in the system. Interpretability can \n\nanswer the question of â€œwhyâ€ a decision was made by the system and its \n\nmeaning or context to the user. \n\nSuggested Actions \n\nâ€¢ Verify systems are developed to produce explainable models, post -hoc explanations and \n\naudit logs. \n\nâ€¢ When possible or available, utilize approaches that are inherently explainable, such as \n\ntraditional and penalized generalized linear models , decision trees, nearest -neighbor \n\nand prototype -based approaches, rule -based models, generalized additive models , \n\nexplainable boosting machines and neural additive models. \n\nâ€¢ Test explanation methods and resulting explanations prior to deployment to gain \n\nfeedback from relevant AI actors, end users, and potentially impacted individuals or \n\ngroups about whether explanations are accurate, clear, and understandable. \n\nâ€¢ Document AI model details including model type (e.g., convolutional neural network, \n\nreinforcement learning, decision tree, random forest, etc.) data features, training \n\nalgorithms, proposed uses, decision thresholds, training data, evaluation data, and \n\nethical considerations. \n\nâ€¢ Establish, document, and report performance and error metrics across demographic \n\ngroups and other segments relevant to the deployment context. 116 of 142 \n\nâ€¢ Explain systems using a variety of methods, e.g., visualizations, model extraction, \n\nfeature importance, and others. Since explanations may not accurately summarize \n\ncomplex systems, test explanations according to properties such as fidelity, consistency, \n\nrobustness, and interpretability. \n\nâ€¢ Assess the characteristics of system explanations according to properties such as \n\nfidelity (local and global), ambiguity, interpretability, interactivity, consistency, and \n\nresilience to attack/manipulation. \n\nâ€¢ Test the quality of system explanations with end -users and other groups. \n\nâ€¢ Secure model development processes to avoid vulnerability to external manipulation \n\nsuch as gaming explanation processes. \n\nâ€¢ Test for changes in models over time, including for models that adjust in response to \n\nproduction data. \n\nâ€¢ Use transparency tools such as data statements and model cards to document \n\nexplanatory and validation information. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Given the purpose of the AI, what level of explainability or interpretability is required \n\nfor how the AI made its determination? \n\nâ€¢ Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\nâ€¢ How has the entity documented the AI systemâ€™s data provenance, including sources, \n\norigins, transformations, augmentations, labels, dependencies, constraints, and \n\nmetadata? \n\nâ€¢ What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. \n\nReferences \n\nChaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin. \n\n\"This Looks Like That: Deep Learning for Interpretable Image Recognition.\" arXiv preprint, \n\nsubmitted December 28, 2019. \n\nCynthia Rudin. \"Stop Explaining Black Box Machine Learning Models for High Stakes \n\nDecisions and Use Interpretable Models Instead.\" arXiv preprint, submitted September 22, \n\n2019. 117 of 142 \n\nDavid A. Broniatowski. \"NISTIR 8367 Psychological Foundations of Explainability and \n\nInterpretability in Artificial Intelligence. National Institute of Standards and Technology \n\n(NIST), 2021. \n\nAlejandro Barredo Arrieta, Natalia DÃ­az -RodrÃ­guez, Javier Del Ser, Adrien Bennetot, Siham \n\nTabik, Alberto Barbado, Salvador Garcia, et al. â€œExplainable Artificial Intelligence (XAI): \n\nConcepts, Taxonomies, Opportunities, and Challenges Toward Responsible AI.â€ Information \n\nFusion 58 (June 2020): 82 â€“115. \n\nZana BuÃ§inca, Phoebe Lin, Krzysztof Z. Gajos, and Elena L. Glassman. â€œProxy Tasks and \n\nSubjective Measures Can Be Misleading in Evaluating Explainable AI Systems.â€ IUI '20: \n\nProceedings of the 25th International Conference on Intelligent User Interfaces, March 17, \n\n2020, 454 â€“64. \n\nP. Jonathon Phillips, Carina A. Hahn, Peter C. Fontana, Amy N. Yates, Kristen Greene, David \n\nA. Broniatowski, and Mark A. Przybocki. \"NISTIR 8312 Four Principles of Explainable \n\nArtificial Intelligence.\" National Institute of Standards and Technology (NIST), September \n\n2021. \n\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben \n\nHutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. â€œModel Cards for \n\nModel Reporting.â€ FAT *19: Proceedings of the Conference on Fairness, Accountability, and \n\nTransparency, January 2019, 220 â€“29. \n\nKe Yang, Julia Stoyanovich, Abolfazl Asudeh, Bill Howe, HV Jagadish, and Gerome Miklau. â€œA \n\nNutritional Label for Rankings.â€ SIGMOD '18: Proceedings of the 2018 International \n\nConference on Management of Data, May 27, 2018, 1773 â€“76. \n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \"'Why Should I Trust You?': \n\nExplaining the Predictions of Any Classifier.\" arXiv preprint, submitted August 9, 2016. \n\nScott M. Lundberg and Su -In Lee. \"A unified approach to interpreting model predictions.\" \n\nNIPS'17: Proceedings of the 31st International Conference on Neural Information \n\nProcessing Systems, December 4, 2017, 4768 -4777. \n\nDylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. â€œFooling \n\nLIME and SHAP: Adversarial Attacks on Post Hoc Explanation Methods.â€ AIES '20: \n\nProceedings of the AAAI/ACM Conference on AI, Ethics, and Society, February 7, 2020, 180 â€“\n\n86. \n\nDavid Alvarez -Melis and Tommi S. Jaakkola. \"Towards robust interpretability with self -\n\nexplaining neural networks.\" NIPS'18: Proceedings of the 32nd International Conference on \n\nNeural Information Processing Systems, December 3, 2018, 7786 -7795. \n\nFinRegLab, Laura Biattner, and Jann Spiess. \"Machine Learning Explainability & Fairness: \n\nInsights from Consumer Lending.\" FinRegLab, April 2022. 118 of 142 \n\nMiguel Ferreira, Muhammad Bilal Zafar, and Krishna P. Gummadi. \"The Case for Temporal \n\nTransparency: Detecting Policy Change Events in Black -Box Decision Making Systems.\" \n\narXiv preprint, submitted October 31, 2016. \n\nHimabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. \"Interpretable & \n\nExplorable Approximations of Black Box Models.\" arXiv preprint, July 4, 2017. \n\nSoftware Resources \n\nâ€¢ SHAP \n\nâ€¢ LIME \n\nâ€¢ Interpret \n\nâ€¢ PiML \n\nâ€¢ Iml \n\nâ€¢ Dalex \n\n# MEASURE 2.10 \n\nPrivacy risk of the AI system â€“ as identified in the MAP function â€“ is examined and \n\ndocumented. \n\nAbout \n\nPrivacy refers generally to the norms and practices that help to safeguard human autonomy, \n\nidentity, and dignity. These norms and practices typically address freedom from intrusion, \n\nlimiting observation, or individualsâ€™ agency to consent to disclosure or control of facets of \n\ntheir identities (e.g., body, data, reputation). \n\nPrivacy values such as anonymity, confidentiality, and control generally should guide \n\nchoices for AI system design, development, and deployment. Privacy -related risks may \n\ninfluence security, bias, and transparency and come with tradeoffs with these other \n\ncharacteristics. Like safety and security, specific technical features of an AI system may \n\npromote or reduce privacy. AI systems can also present new risks to privacy by allowing \n\ninference to identify individuals or previously private information about individuals. \n\nPrivacy -enhancing technologies (â€œPETsâ€) for AI, as well as data minimizing methods such as \n\nde -identification and aggregation for certain model outputs, can support design for privacy -\n\nenhanced AI systems. Under certain conditions such as data sparsity, privacy enhancing \n\ntechniques can result in a loss in accuracy, impacting decisions about fairness and other \n\nvalues in certain domains. \n\nSuggested Actions \n\nâ€¢ Specify privacy -related values, frameworks, and attributes that are applicable in the \n\ncontext of use through direct engagement with end users and potentially impacted \n\ngroups and communities. \n\nâ€¢ Document collection, use, management, and disclosure of personally sensitive \n\ninformation in datasets, in accordance with privacy and data governance policies 119 of 142 \n\nâ€¢ Quantify privacy -level data aspects such as the ability to identify individuals or groups \n\n(e.g. k -anonymity metrics, l -diversity, t -closeness). \n\nâ€¢ Establish and document protocols (authorization, duration, type) and access controls \n\nfor training sets or production data containing personally sensitive information, in \n\naccordance with privacy and data governance policies. \n\nâ€¢ Monitor internal queries to production data for detecting patterns that isolate personal \n\nrecords. \n\nâ€¢ Monitor PSI disclosures and inference of sensitive or legally protected attributes \n\nâ€¢ Assess the risk of manipulation from overly customized content. Evaluate \n\ninformation presented to representative users at various points along axes of \n\ndifference between individuals (e.g. individuals of different ages, genders, races, \n\npolitical affiliation, etc.). \n\nâ€¢ Use privacy -enhancing techniques such as differential privacy, when publicly sharing \n\ndataset information. \n\nâ€¢ Collaborate with privacy experts, AI end users and operators, and other domain experts \n\nto determine optimal differential privacy metrics within contexts of use. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Did your organization implement accountability -based practices in data management \n\nand protection (e.g. the PDPA and OECD Privacy Principles)? \n\nâ€¢ What assessments has the entity conducted on data security and privacy impacts \n\nassociated with the AI system? \n\nâ€¢ Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\nâ€¢ Does the dataset contain information that might be considered sensitive or confidential? \n\n(e.g., personally identifying information) \n\nâ€¢ If it relates to people, could this dataset expose people to harm or legal action? (e.g., \n\nfinancial, social or otherwise) What was done to mitigate or reduce the potential for \n\nharm? \n\nAI Transparency Resources \n\nâ€¢ WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. ( \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nKaitlin R. Boeckl and Naomi B. Lefkovitz. \"NIST Privacy Framework: A Tool for Improving \n\nPrivacy Through Enterprise Risk Management, Version 1.0.\" National Institute of Standards \n\nand Technology (NIST), January 16, 2020. 120 of 142 \n\nLatanya Sweeney. â€œK -Anonymity: A Model for Protecting Privacy.â€ International Journal of \n\nUncertainty, Fuzziness and Knowledge -Based Systems 10, no. 5 (2002): 557 â€“70. \n\nAshwin Machanavajjhala, Johannes Gehrke, Daniel Kifer, and Muthuramakrishnan \n\nVenkitasubramaniam. â€œL -Diversity: Privacy beyond K -Anonymity.â€ 22nd International \n\nConference on Data Engineering (ICDE'06), 2006. \n\nNinghui Li, Tiancheng Li, and Suresh Venkatasubramanian. \"CERIAS Tech Report 2007 -78 t -\n\nCloseness: Privacy Beyond k -Anonymity and -Diversity.\" Center for Education and Research, \n\nInformation Assurance and Security, Purdue University, 2001. \n\nJ. Domingo -Ferrer and J. Soria -Comas. \"From t -closeness to differential privacy and vice \n\nversa in data anonymization.\" arXiv preprint, submitted December 21, 2015. \n\nJoseph Near, David Darais, and Kaitlin Boeckly. \"Differential Privacy for Privacy -Preserving \n\nData Analysis: An Introduction to our Blog Series.\" National Institute of Standards and \n\nTechnology (NIST), July 27, 2020. \n\nCynthia Dwork. â€œDifferential Privacy.â€ Automata, Languages and Programming, 2006, 1 â€“12. \n\nZhanglong Ji, Zachary C. Lipton, and Charles Elkan. \"Differential Privacy and Machine \n\nLearning: a Survey and Review.\" arXiv preprint, submitted December 24,2014. \n\nMichael B. Hawes. \"Implementing Differential Privacy: Seven Lessons From the 2020 United \n\nStates Census.\" Harvard Data Science Review 2, no. 2 (2020). \n\nHarvard University Privacy Tools Project. â€œDifferential Privacy.â€ Harvard University, n.d. \n\nJohn M. Abowd, Robert Ashmead, Ryan Cumings -Menon, Simson Garfinkel, Micah Heineck, \n\nChristine Heiss, Robert Johns, Daniel Kifer, Philip Leclerc, Ashwin Machanavajjhala, Brett \n\nMoran, William Matthew Spence Sexton and Pavel Zhuravlev. \"The 2020 Census Disclosure \n\nAvoidance System TopDown Algorithm.\" United States Census Bureau, April 7, 2022. \n\nNicolas Papernot and Abhradeep Guha Thakurta. \"How to deploy machine learning with \n\ndifferential privacy.\" National Institute of Standards and Technology (NIST), December 21, \n\n2021. \n\nClaire McKay Bowen. \"Utility Metrics for Differential Privacy: No One -Size -Fits -All.\" National \n\nInstitute of Standards and Technology (NIST), November 29, 2021. \n\nHelen Nissenbaum. \"Contextual Integrity Up and Down the Data Food Chain.\" Theoretical \n\nInquiries in Law 20, L. 221 (2019): 221 -256. \n\nSebastian Benthall, Seda GÃ¼rses, and Helen Nissenbaum. â€œContextual Integrity through the \n\nLens of Computer Science.â€ Foundations and Trends in Privacy and Security 2, no. 1 \n\n(December 22, 2017): 1 â€“69. 121 of 142 \n\nJenifer Sunrise Winter and Elizabeth Davidson. â€œBig Data Governance of Personal Health \n\nInformation and Challenges to Contextual Integrity.â€ The Information Society: An \n\nInternational Journal 35, no. 1 (2019): 36 â€“51. \n\n# MEASURE 2.11 \n\nFairness and bias â€“ as identified in the MAP function â€“ is evaluated and results are \n\ndocumented. \n\nAbout \n\nFairness in AI includes concerns for equality and equity by addressing issues such as \n\nharmful bias and discrimination. Standards of fairness can be complex and difficult to define \n\nbecause perceptions of fairness differ among cultures and may shift depending on \n\napplication. Organizationsâ€™ risk management efforts will be enhanced by recognizing and \n\nconsidering these differences. Systems in which harmful biases are mitigated are not \n\nnecessarily fair. For example, systems in which predictions are somewhat balanced across \n\ndemographic groups may still be inaccessible to individuals with disabilities or affected by \n\nthe digital divide or may exacerbate existing disparities or systemic biases. \n\nBias is broader than demographic balance and data representativeness. NIST has identified \n\nthree major categories of AI bias to be considered and managed: systemic, computational \n\nand statistical, and human -cognitive. Each of these can occur in the absence of prejudice, \n\npartiality, or discriminatory intent. \n\nâ€¢ Systemic bias can be present in AI datasets, the organizational norms, practices, and \n\nprocesses across the AI lifecycle, and the broader society that uses AI systems. \n\nâ€¢ Computational and statistical biases can be present in AI datasets and algorithmic \n\nprocesses, and often stem from systematic errors due to non -representative samples. \n\nâ€¢ Human -cognitive biases relate to how an individual or group perceives AI system \n\ninformation to make a decision or fill in missing information, or how humans think \n\nabout purposes and functions of an AI system. Human -cognitive biases are omnipresent \n\nin decision -making processes across the AI lifecycle and system use, including the \n\ndesign, implementation, operation, and maintenance of AI. \n\nBias exists in many forms and can become ingrained in the automated systems that help \n\nmake decisions about our lives. While bias is not always a negative phenomenon, AI systems \n\ncan potentially increase the speed and scale of biases and perpetuate and amplify harms to \n\nindividuals, groups, communities, organizations, and society. \n\nSuggested Actions \n\nâ€¢ Conduct fairness assessments to manage computational and statistical forms of bias \n\nwhich include the following steps: \n\nâ€¢ Identify types of harms, including allocational, representational, quality of service, \n\nstereotyping, or erasure \n\nâ€¢ Identify across, within, and intersecting groups that might be harmed 122 of 142 \n\nâ€¢ Quantify harms using both a general fairness metric, if appropriate (e.g. \n\ndemographic parity, equalized odds, equal opportunity, statistical hypothesis tests), \n\nand custom, context -specific metrics developed in collaboration with affected \n\ncommunities \n\nâ€¢ Analyze quantified harms for contextually significant differences across groups, \n\nwithin groups, and among intersecting groups \n\nâ€¢ Refine identification of within -group and intersectional group disparities. \n\nâ€¢ Evaluate underlying data distributions and employ sensitivity analysis during \n\nthe analysis of quantified harms. \n\nâ€¢ Evaluate quality metrics including false positive rates and false negative rates. \n\nâ€¢ Consider biases affecting small groups, within -group or intersectional \n\ncommunities, or single individuals. \n\nâ€¢ Understand and consider sources of bias in training and TEVV data: \n\nâ€¢ Differences in distributions of outcomes across and within groups, including \n\nintersecting groups. \n\nâ€¢ Completeness, representativeness and balance of data sources. \n\nâ€¢ Identify input data features that may serve as proxies for demographic group \n\nmembership (i.e., credit score, ZIP code) or otherwise give rise to emergent bias \n\nwithin AI systems. \n\nâ€¢ Forms of systemic bias in images, text (or word embeddings), audio or other \n\ncomplex or unstructured data. \n\nâ€¢ Leverage impact assessments to identify and classify system impacts and harms to end \n\nusers, other individuals, and groups with input from potentially impacted communities. \n\nâ€¢ Identify the classes of individuals, groups, or environmental ecosystems which might be \n\nimpacted through direct engagement with potentially impacted communities. \n\nâ€¢ Evaluate systems in regards to disability inclusion, including consideration of disability \n\nstatus in bias testing, and discriminatory screen out processes that may arise from non -\n\ninclusive design or deployment decisions. \n\nâ€¢ Develop objective functions in consideration of systemic biases, in -group/out -group \n\ndynamics. \n\nâ€¢ Use context -specific fairness metrics to examine how system performance varies across \n\ngroups, within groups, and/or for intersecting groups. Metrics may include statistical \n\nparity, error -rate equality, statistical parity difference, equal opportunity difference, \n\naverage absolute odds difference, standardized mean difference, percentage point \n\ndifferences. \n\nâ€¢ Customize fairness metrics to specific context of use to examine how system \n\nperformance and potential harms vary within contextual norms. \n\nâ€¢ Define acceptable levels of difference in performance in accordance with established \n\norganizational governance policies, business requirements, regulatory compliance, legal \n\nframeworks, and ethical standards within the context of use 123 of 142 \n\nâ€¢ Define the actions to be taken if disparity levels rise above acceptable levels. \n\nâ€¢ Identify groups within the expected population that may require disaggregated analysis, \n\nin collaboration with impacted communities. \n\nâ€¢ Leverage experts with knowledge in the specific context of use to investigate substantial \n\nmeasurement differences and identify root causes for those differences. \n\nâ€¢ Monitor system outputs for performance or bias issues that exceed established \n\ntolerance levels. \n\nâ€¢ Ensure periodic model updates; test and recalibrate with updated and more \n\nrepresentative data to stay within acceptable levels of difference. \n\nâ€¢ Apply pre -processing data transformations to address factors related to demographic \n\nbalance and data representativeness. \n\nâ€¢ Apply in -processing to balance model performance quality with bias considerations. \n\nâ€¢ Apply post -processing mathematical/computational techniques to model results in \n\nclose collaboration with impact assessors, socio -technical experts, and other AI actors \n\nwith expertise in the context of use. \n\nâ€¢ Apply model selection approaches with transparent and deliberate consideration of bias \n\nmanagement and other trustworthy characteristics. \n\nâ€¢ Collect and share information about differences in outcomes for the identified groups. \n\nâ€¢ Consider mediations to mitigate differences, especially those that can be traced to past \n\npatterns of unfair or biased human decision making. \n\nâ€¢ Utilize human -centered design practices to generate deeper focus on societal impacts \n\nand counter human -cognitive biases within the AI lifecycle. \n\nâ€¢ Evaluate practices along the lifecycle to identify potential sources of human -cognitive \n\nbias such as availability, observational, and confirmation bias, and to make implicit \n\ndecision making processes more explicit and open to investigation. \n\nâ€¢ Work with human factors experts to evaluate biases in the presentation of system \n\noutput to end users, operators and practitioners. \n\nâ€¢ Utilize processes to enhance contextual awareness, such as diverse internal staff and \n\nstakeholder engagement. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\nâ€¢ If it relates to people, does it unfairly advantage or disadvantage a particular social \n\ngroup? In what ways? How was this mitigated? \n\nâ€¢ Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\nâ€¢ How has the entity identified and mitigated potential impacts of bias in the data, \n\nincluding inequitable or discriminatory outcomes? \n\nâ€¢ To what extent has the entity identified and mitigated potential bias â€”statistical, \n\ncontextual, and historical â€”in the data? 124 of 142 \n\nâ€¢ Were adversarial machine learning approaches considered or used for measuring bias \n\n(e.g.: prompt engineering, adversarial models) \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF Companion to the Model AI Governance Framework - WEF - Companion to the \n\nModel AI Governance Framework, 2020. \n\nâ€¢ Datasheets for Datasets. \n\nReferences \n\nAli Hasan, Shea Brown, Jovana Davidovic, Benjamin Lange, and Mitt Regan. â€œAlgorithmic \n\nBias and Risk Assessments: Lessons from Practice.â€ Digital Society 1 (2022). \n\nRichard N. Landers and Tara S. Behrend. â€œAuditing the AI Auditors: A Framework for \n\nEvaluating Fairness and Bias in High Stakes AI Predictive Models.â€ American Psychologist \n\n78, no. 1 (2023): 36 â€“49. \n\nNinareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. â€œA \n\nSurvey on Bias and Fairness in Machine Learning.â€ ACM Computing Surveys 54, no. 6 (July \n\n2021): 1 â€“35. \n\nMichele Loi and Christoph Heitz. â€œIs Calibration a Fairness Requirement?â€ FAccT '22: 2022 \n\nACM Conference on Fairness, Accountability, and Transparency, June 2022, 2026 â€“34. \n\nShea Brown, Ryan Carrier, Merve Hickok, and Adam Leon Smith. â€œBias Mitigation in Data \n\nSets.â€ SocArXiv, July 8, 2021. \n\nReva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, Andrew Burt, and Patrick Hall. \n\n\"NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in \n\nArtificial Intelligence.\" National Institute of Standards and Technology (NIST), 2022. \n\nMicrosoft Research. â€œAI Fairness Checklist.â€ Microsoft, February 7, 2022. \n\nSamir Passi and Solon Barocas. â€œProblem Formulation and Fairness.â€ FAT* '19: Proceedings \n\nof the Conference on Fairness, Accountability, and Transparency, January 2019, 39 â€“48. \n\nJade S. Franklin, Karan Bhanot, Mohamed Ghalwash, Kristin P. Bennett, Jamie McCusker, and \n\nDeborah L. McGuinness. â€œAn Ontology for Fairness Metrics.â€ AIES '22: Proceedings of the \n\n2022 AAAI/ACM Conference on AI, Ethics, and Society, July 2022, 265 â€“75. \n\nZhang, B., Lemoine, B., & Mitchell, M. (2018). Mitigating Unwanted Biases with Adversarial \n\nLearning. Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. \n\nhttps://arxiv.org/pdf/1801.07593.pdf 125 of 142 \n\nGanguli, D., et al. (2023). The Capacity for Moral Self -Correction in Large Language Models. \n\narXiv. https://arxiv.org/abs/2302.07459 \n\nArvind Narayanan. â€œTl;DS - 21 Fairness Definition and Their Politics by Arvind Narayanan.â€ \n\nDora's world, July 19, 2019. \n\nBen Green. â€œEscaping the Impossibility of Fairness: From Formal to Substantive Algorithmic \n\nFairness.â€ Philosophy and Technology 35, no. 90 (October 8, 2022). \n\nAlexandra Chouldechova. â€œFair Prediction with Disparate Impact: A Study of Bias in \n\nRecidivism Prediction Instruments.â€ Big Data 5, no. 2 (June 1, 2017): 153 â€“63. \n\nSina Fazelpour and Zachary C. Lipton. â€œAlgorithmic Fairness from a Non -Ideal Perspective.â€ \n\nAIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, February 7, \n\n2020, 57 â€“63. \n\nHemank Lamba, Kit T. Rodolfa, and Rayid Ghani. â€œAn Empirical Comparison of Bias \n\nReduction Methods on Real -World Problems in High -Stakes Policy Settings.â€ ACM SIGKDD \n\nExplorations Newsletter 23, no. 1 (May 29, 2021): 69 â€“85. \n\nISO. â€œISO/IEC TR 24027:2021 Information technology â€” Artificial intelligence (AI) â€” Bias \n\nin AI systems and AI aided decision making.â€ ISO Standards, November 2021. \n\nShari Trewin. \"AI Fairness for People with Disabilities: Point of View.\" arXiv preprint, \n\nsubmitted November 26, 2018. \n\nMathWorks. â€œExplore Fairness Metrics for Credit Scoring Model.â€ MATLAB & Simulink, \n\n2023. \n\nAbigail Z. Jacobs and Hanna Wallach. â€œMeasurement and Fairness.â€ FAccT '21: Proceedings \n\nof the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, \n\n375 â€“85. \n\nTolga Bolukbasi, Kai -Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. \n\n\"Quantifying and Reducing Stereotypes in Word Embeddings.\" arXiv preprint, submitted \n\nJune 20, 2016. \n\nAylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. â€œSemantics Derived Automatically \n\nfrom Language Corpora Contain Human -Like Biases.â€ Science 356, no. 6334 (April 14, \n\n2017): 183 â€“86. \n\nSina Fazelpour and Maria De -Arteaga. â€œDiversity in Sociotechnical Machine Learning \n\nSystems.â€ Big Data and Society 9, no. 1 (2022). \n\nFairlearn. â€œFairness in Machine Learning.â€ Fairlearn 0.8.0 Documentation, n.d. \n\nSafiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. New \n\nYork, NY: New York University Press, 2018. 126 of 142 \n\nZiad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. â€œDissecting \n\nRacial Bias in an Algorithm Used to Manage the Health of Populations.â€ Science 366, no. \n\n6464 (October 25, 2019): 447 â€“53. \n\nAlekh Agarwal, Alina Beygelzimer, Miroslav DudÃ­k, John Langford, and Hanna Wallach. \"A \n\nReductions Approach to Fair Classification.\" arXiv preprint, submitted July 16, 2018. \n\nMoritz Hardt, Eric Price, and Nathan Srebro. \"Equality of Opportunity in Supervised \n\nLearning.\" arXiv preprint, submitted October 7, 2016. \n\nAlekh Agarwal, Miroslav Dudik, Zhiwei Steven Wu. \"Fair Regression: Quantitative \n\nDefinitions and Reduction -Based Algorithms.\" Proceedings of the 36th International \n\nConference on Machine Learning, PMLR 97:120 -129, 2019. \n\nAndrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet \n\nVertesi. â€œFairness and Abstraction in Sociotechnical Systems.â€ FAT* '19: Proceedings of the \n\nConference on Fairness, Accountability, and Transparency, January 29, 2019, 59 â€“68. \n\nMatthew Kay, Cynthia Matuszek, and Sean A. Munson. â€œUnequal Representation and Gender \n\nStereotypes in Image Search Results for Occupations.â€ CHI '15: Proceedings of the 33rd \n\nAnnual ACM Conference on Human Factors in Computing Systems, April 18, 2015, 3819 â€“28. \n\nSoftware Resources \n\nâ€¢ aequitas \n\n- AI Fairness 360: \n\nâ€¢ Python \n\nâ€¢ R\n\nâ€¢ algofairness \n\nâ€¢ fairlearn \n\nâ€¢ fairml \n\nâ€¢ fairmodels \n\nâ€¢ fairness \n\nâ€¢ solas -ai -disparity \n\nâ€¢ tensorflow/fairness -indicators \n\nâ€¢ Themis \n\n# MEASURE 2.12 \n\nEnvironmental impact and sustainability of AI model training and management activities â€“\n\nas identified in the MAP function â€“ are assessed and documented. \n\nAbout \n\nLarge -scale, high -performance computational resources used by AI systems for training and \n\noperation can contribute to environmental impacts. Direct negative impacts to the \n\nenvironment from these processes are related to energy consumption, water consumption, 127 of 142 \n\nand greenhouse gas (GHG) emissions. The OECD has identified metrics for each type of \n\nnegative direct impact. \n\nIndirect negative impacts to the environment reflect the complexity of interactions between \n\nhuman behavior, socio -economic systems, and the environment and can include induced \n\nconsumption and â€œrebound effectsâ€, where efficiency gains are offset by accelerated \n\nresource consumption. \n\nOther AI related environmental impacts can arise from the production of computational \n\nequipment and networks (e.g. mining and extraction of raw materials), transporting \n\nhardware, and electronic waste recycling or disposal. \n\nSuggested Actions \n\nâ€¢ Include environmental impact indicators in AI system design and development plans, \n\nincluding reducing consumption and improving efficiencies. \n\nâ€¢ Identify and implement key indicators of AI system energy and water consumption and \n\nefficiency, and/or GHG emissions. \n\nâ€¢ Establish measurable baselines for sustainable AI system operation in accordance with \n\norganizational policies, regulatory compliance, legal frameworks, and environmental \n\nprotection and sustainability norms. \n\nâ€¢ Assess tradeoffs between AI system performance and sustainable operations in \n\naccordance with organizational principles and policies, regulatory compliance, legal \n\nframeworks, and environmental protection and sustainability norms. \n\nâ€¢ Identify and establish acceptable resource consumption and efficiency, and GHG \n\nemissions levels, along with actions to be taken if indicators rise above acceptable \n\nlevels. \n\nâ€¢ Estimate AI system emissions levels throughout the AI lifecycle via carbon calculators or \n\nsimilar process. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Are greenhouse gas emissions, and energy and water consumption and efficiency \n\ntracked within the organization? \n\nâ€¢ Are deployed AI systems evaluated for potential upstream and downstream \n\nenvironmental impacts (e.g., increased consumption, increased emissions, etc.)? \n\nâ€¢ Could deployed AI systems cause environmental incidents, e.g., air or water pollution \n\nincidents, toxic spills, fires or explosions? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ Datasheets for Datasets. 128 of 142 \n\nReferences \n\nOrganisation for Economic Co -operation and Development (OECD). \"Measuring the \n\nenvironmental impacts of artificial intelligence compute and applications: The AI footprint.â€ \n\nOECD Digital Economy Papers, No. 341, OECD Publishing, Paris. \n\nVictor Schmidt, Alexandra Luccioni, Alexandre Lacoste, and Thomas Dandres. â€œMachine \n\nLearning CO2 Impact Calculator.â€ ML CO2 Impact, n.d. \n\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. \"Quantifying \n\nthe Carbon Emissions of Machine Learning.\" arXiv preprint, submitted November 4, 2019. \n\nMatthew Hutson. â€œMeasuring AIâ€™s Carbon Footprint: New Tools Track and Reduce \n\nEmissions from Machine Learning.â€ IEEE Spectrum, November 22, 2022. \n\nAssociation for Computing Machinery (ACM). \"TechBriefs: Computing and Climate Change.\" \n\nACM Technology Policy Council, November 2021. \n\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. â€œGreen AI.â€ Communications of \n\nthe ACM 63, no. 12 (December 2020): 54 â€“63. \n\n# MEASURE 2.13 \n\nEffectiveness of the employed TEVV metrics and processes in the MEASURE function are \n\nevaluated and documented. \n\nAbout \n\nThe development of metrics is a process often considered to be objective but, as a human \n\nand organization driven endeavor, can reflect implicit and systemic biases, and may \n\ninadvertently reflect factors unrelated to the target function. Measurement approaches can \n\nbe oversimplified, gamed, lack critical nuance, become used and relied upon in unexpected \n\nways, fail to account for differences in affected groups and contexts. \n\nRevisiting the metrics chosen in Measure 2.1 through 2.12 in a process of continual \n\nimprovement can help AI actors to evaluate and document metric effectiveness and make \n\nnecessary course corrections. \n\nSuggested Actions \n\nâ€¢ Review selected system metrics and associated TEVV processes to determine if they are \n\nable to sustain system improvements, including the identification and removal of errors. \n\nâ€¢ Regularly evaluate system metrics for utility, and consider descriptive approaches in \n\nplace of overly complex methods. \n\nâ€¢ Review selected system metrics for acceptability within the end user and impacted \n\ncommunity of interest. \n\nâ€¢ Assess effectiveness of metrics for identifying and measuring risks. 129 of 142 \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent does the system/entity consistently measure progress towards stated \n\ngoals and objectives? \n\nâ€¢ Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\nâ€¢ What corrective actions has the entity taken to enhance the quality, accuracy, reliability, \n\nand representativeness of the data? \n\nâ€¢ To what extent are the model outputs consistent with the entityâ€™s values and principles \n\nto foster public trust and equity? \n\nâ€¢ How will the accuracy or appropriate performance metrics be assessed? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nArvind Narayanan. \"The limits of the quantitative approach to discrimination.\" 2022 James \n\nBaldwin lecture, Princeton University, October 11, 2022. \n\nDevansh Saxena, Karla Badillo -Urquiola, Pamela J. Wisniewski, and Shion Guha. â€œA Human -\n\nCentered Review of Algorithms Used within the U.S. Child Welfare System.â€ CHI â€˜20: \n\nProceedings of the 2020 CHI Conference on Human Factors in Computing Systems, April 23, \n\n2020, 1 â€“15. \n\nRachel Thomas and David Uminsky. â€œReliance on Metrics Is a Fundamental Challenge for \n\nAI.â€ Patterns 3, no. 5 (May 13, 2022): 100476. \n\nMomin M. Malik. \"A Hierarchy of Limitations in Machine Learning.\" arXiv preprint, \n\nsubmitted February 29, 2020. \n\n# MEASURE 3.1 \n\nApproaches, personnel, and documentation are in place to regularly identify and track \n\nexisting, unanticipated, and emergent AI risks based on factors such as intended and actual \n\nperformance in deployed contexts. \n\nAbout \n\nFor trustworthy AI systems, regular system monitoring is carried out in accordance with \n\norganizational governance policies, AI actor roles and responsibilities, and within a culture \n\nof continual improvement. If and when emergent or complex risks arise, it may be \n\nnecessary to adapt internal risk management procedures, such as regular monitoring, to \n\nstay on course. Documentation, resources, and training are part of an overall strategy to 130 of 142 \n\nsupport AI actors as they investigate and respond to AI system errors, incidents or negative \n\nimpacts. \n\nSuggested Actions \n\nâ€¢ Compare AI system risks with: \n\nâ€¢ simpler or traditional models \n\nâ€¢ human baseline performance \n\nâ€¢ other manual performance benchmarks \n\nâ€¢ Compare end user and community feedback about deployed AI systems to internal \n\nmeasures of system performance. \n\nâ€¢ Assess effectiveness of metrics for identifying and measuring emergent risks. \n\nâ€¢ Measure error response times and track response quality. \n\nâ€¢ Elicit and track feedback from AI actors in user support roles about the type of metrics, \n\nexplanations and other system information required for fulsome resolution of system \n\nissues. Consider: \n\nâ€¢ Instances where explanations are insufficient for investigating possible error \n\nsources or identifying responses. \n\nâ€¢ System metrics, including system logs and explanations, for identifying and \n\ndiagnosing sources of system error. \n\nâ€¢ Elicit and track feedback from AI actors in incident response and support roles about \n\nthe adequacy of staffing and resources to perform their duties in an effective and timely \n\nmanner. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\nâ€¢ To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nâ€¢ What metrics has the entity developed to measure performance of the AI system, \n\nincluding error logging? \n\nâ€¢ To what extent do the metrics provide accurate and useful measure of performance? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF Companion to the Model AI Governance Framework â€“ Implementation and Self -\n\nAssessment Guide for Organizations 131 of 142 \n\nReferences \n\nISO. \"ISO 9241 -210:2019 Ergonomics of human -system interaction â€” Part 210: Human -\n\ncentred design for interactive systems.\" 2nd ed. ISO Standards, July 2019. \n\nLarysa Visengeriyeva, et al. â€œAwesome MLOps.â€œ GitHub. \n\n# MEASURE 3.2 \n\nRisk tracking approaches are considered for settings where AI risks are difficult to assess \n\nusing currently available measurement techniques or where metrics are not yet available. \n\nAbout \n\nRisks identified in the Map function may be complex, emerge over time, or difficult to \n\nmeasure. Systematic methods for risk tracking, including novel measurement approaches, \n\ncan be established as part of regular monitoring and improvement processes. \n\nSuggested Actions \n\nâ€¢ Establish processes for tracking emergent risks that may not be measurable with \n\ncurrent approaches. Some processes may include: \n\nâ€¢ Recourse mechanisms for faulty AI system outputs. \n\nâ€¢ Bug bounties. \n\nâ€¢ Human -centered design approaches. \n\nâ€¢ User -interaction and experience research. \n\nâ€¢ Participatory stakeholder engagement with affected or potentially impacted \n\nindividuals and communities. \n\nâ€¢ Identify AI actors responsible for tracking emergent risks and inventory methods. \n\nâ€¢ Determine and document the rate of occurrence and severity level for complex or \n\ndifficult -to -measure risks when: \n\nâ€¢ Prioritizing new measurement approaches for deployment tasks. \n\nâ€¢ Allocating AI system risk management resources. \n\nâ€¢ Evaluating AI system improvements. \n\nâ€¢ Making go/no -go decisions for subsequent system iterations. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Who is ultimately responsible for the decisions of the AI and is this person aware of the \n\nintended uses and limitations of the analytic? \n\nâ€¢ Who will be responsible for maintaining, re -verifying, monitoring, and updating this AI \n\nonce deployed? \n\nâ€¢ To what extent does the entity communicate its AI strategic goals and objectives to the \n\ncommunity of stakeholders? 132 of 142 \n\nâ€¢ Given the purpose of this AI, what is an appropriate interval for checking whether it is \n\nstill accurate, unbiased, explainable, etc.? What are the checks for this model? \n\nâ€¢ If anyone believes that the AI no longer meets this ethical framework, who will be \n\nresponsible for receiving the concern and as appropriate investigating and remediating \n\nthe issue? Do they have authority to modify, limit, or stop the use of the AI? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nISO. \"ISO 9241 -210:2019 Ergonomics of human -system interaction â€” Part 210: Human -\n\ncentred design for interactive systems.\" 2nd ed. ISO Standards, July 2019. \n\nMark C. Paulk, Bill Curtis, Mary Beth Chrissis, and Charles V. Weber. â€œCapability Maturity \n\nModel, Version 1.1.â€ IEEE Software 10, no. 4 (1993): 18 â€“27. \n\nJeff Patton, Peter Economy, Martin Fowler, Alan Cooper, and Marty Cagan. User Story \n\nMapping: Discover the Whole Story, Build the Right Product. O'Reilly, 2014. \n\nRumman Chowdhury and Jutta Williams. \"Introducing Twitterâ€™s first algorithmic bias \n\nbounty challenge.\" Twitter Engineering Blog, July 30, 2021. \n\nHackerOne. \"Twitter Algorithmic Bias.\" HackerOne, August 8, 2021. \n\nJosh Kenway, Camille FranÃ§ois, Sasha Costanza -Chock, Inioluwa Deborah Raji, and Joy \n\nBuolamwini. \"Bug Bounties for Algorithmic Harms?\" Algorithmic Justice League, January \n\n2022. \n\nMicrosoft. â€œCommunity Jury.â€ Microsoft Learn's Azure Application Architecture Guide, 2023. \n\nMargarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. \"Overcoming Failures of \n\nImagination in AI Infused System Development and Deployment.\" arXiv preprint, submitted \n\nDecember 10, 2020. \n\n# MEASURE 3.3 \n\nFeedback processes for end users and impacted communities to report problems and \n\nappeal system outcomes are established and integrated into AI system evaluation metrics. \n\nAbout \n\nAssessing impact is a two -way effort. Many AI system outcomes and impacts may not be \n\nvisible or recognizable to AI actors across the development and deployment dimensions of \n\nthe AI lifecycle, and may require direct feedback about system outcomes from the \n\nperspective of end users and impacted groups. 133 of 142 \n\nFeedback can be collected indirectly, via systems that are mechanized to collect errors and \n\nother feedback from end users and operators \n\nMetrics and insights developed in this sub -category feed into Manage 4.1 and 4.2. \n\nSuggested Actions \n\nâ€¢ Measure efficacy of end user and operator error reporting processes. \n\nâ€¢ Categorize and analyze type and rate of end user appeal requests and results. \n\nâ€¢ Measure feedback activity participation rates and awareness of feedback activity \n\navailability. \n\nâ€¢ Utilize feedback to analyze measurement approaches and determine subsequent \n\ncourses of action. \n\nâ€¢ Evaluate measurement approaches to determine efficacy for enhancing organizational \n\nunderstanding of real world impacts. \n\nâ€¢ Analyze end user and community feedback in close collaboration with domain experts. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nâ€¢ Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? \n\nâ€¢ How easily accessible and current is the information available to external stakeholders? \n\nâ€¢ What type of information is accessible on the design, operations, and limitations of the \n\nAI system to external stakeholders, including end users, consumers, regulators, and \n\nindividuals impacted by use of the AI system? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ WEF Companion to the Model AI Governance Framework â€“ Implementation and Self -\n\nAssessment Guide for Organizations \n\nReferences \n\nSasha Costanza -Chock. Design Justice: Community -Led Practices to Build the Worlds We \n\nNeed. Cambridge: The MIT Press, 2020. \n\nDavid G. Robinson. Voices in the Code: A Story About People, Their Values, and the \n\nAlgorithm They Made. New York: Russell Sage Foundation, 2022. \n\nFernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder \n\nParticipation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted \n\nNovember 1, 2021. 134 of 142 \n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. â€œHuman -\n\nCentered Design of Artificial Intelligence.â€ In Handbook of Human Factors and Ergonomics, \n\nedited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085 â€“1106. John Wiley & \n\nSons, 2021. \n\nBen Shneiderman. Human -Centered AI. Oxford: Oxford University Press, 2022 \n\nBatya Friedman, David G. Hendry, and Alan Borning. â€œA Survey of Value Sensitive Design \n\nMethods.â€ Foundations and Trends in Human -Computer Interaction 11, no. 2 (November \n\n22, 2017): 63 â€“125. \n\nBatya Friedman, Peter H. Kahn, Jr., and Alan Borning. \"Value Sensitive Design: Theory and \n\nMethods.\" University of Washington Department of Computer Science & Engineering \n\nTechnical Report 02 -12 -01, December 2002. \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. \n\nâ€œAssembling Accountability: Algorithmic Impact Assessment for the Public Interest.â€ SSRN, \n\nJuly 8, 2021. \n\nAlexandra Reeve Givens, and Meredith Ringel Morris. â€œCentering Disability Perspectives in \n\nAlgorithmic Fairness, Accountability, & Transparency.â€ FAT* '20: Proceedings of the 2020 \n\nConference on Fairness, Accountability, and Transparency, January 27, 2020, 684 -84. \n\n# MEASURE 4.1 \n\nMeasurement approaches for identifying AI risks are connected to deployment context(s) \n\nand informed through consultation with domain experts and other end users. Approaches \n\nare documented. \n\nAbout \n\nAI Actors carrying out TEVV tasks may have difficulty evaluating impacts within the system \n\ncontext of use. AI system risks and impacts are often best described by end users and others \n\nwho may be affected by output and subsequent decisions. AI Actors can elicit feedback from \n\nimpacted individuals and communities via participatory engagement processes established \n\nin Govern 5.1 and 5.2, and carried out in Map 1.6, 5.1, and 5.2. \n\nActivities described in the Measure function enable AI actors to evaluate feedback from \n\nimpacted individuals and communities. To increase awareness of insights, feedback can be \n\nevaluated in close collaboration with AI actors responsible for impact assessment, human -\n\nfactors, and governance and oversight tasks, as well as with other socio -technical domain \n\nexperts and researchers. To gain broader expertise for interpreting evaluation outcomes, \n\norganizations may consider collaborating with advocacy groups and civil society \n\norganizations. \n\nInsights based on this type of analysis can inform TEVV -based decisions about metrics and \n\nrelated courses of action. 135 of 142 \n\nSuggested Actions \n\nâ€¢ Support mechanisms for capturing feedback from system end users (including domain \n\nexperts, operators, and practitioners). Successful approaches are: \n\nâ€¢ conducted in settings where end users are able to openly share their doubts and \n\ninsights about AI system output, and in connection to their specific context of use \n\n(including setting and task -specific lines of inquiry) \n\nâ€¢ developed and implemented by human -factors and socio -technical domain experts \n\nand researchers \n\nâ€¢ designed to ensure control of interviewer and end user subjectivity and biases \n\nâ€¢ Identify and document approaches \n\nâ€¢ for evaluating and integrating elicited feedback from system end users \n\nâ€¢ in collaboration with human -factors and socio -technical domain experts, \n\nâ€¢ to actively inform a process of continual improvement. \n\nâ€¢ Evaluate feedback from end users alongside evaluated feedback from impacted \n\ncommunities (MEASURE 3.3). \n\nâ€¢ Utilize end user feedback to investigate how selected metrics and measurement \n\napproaches interact with organizational and operational contexts. \n\nâ€¢ Analyze and document system -internal measurement processes in comparison to \n\ncollected end user feedback. \n\nâ€¢ Identify and implement approaches to measure effectiveness and satisfaction with end \n\nuser elicitation techniques, and document results. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ Did your organization address usability problems and test whether user interfaces \n\nserved their intended purposes? \n\nâ€¢ How will user and peer engagement be integrated into the model development process \n\nand periodic performance review once deployed? \n\nâ€¢ To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nâ€¢ To what extent are the established procedures effective in mitigating bias, inequity, and \n\nother concerns resulting from the system? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nâ€¢ WEF Companion to the Model AI Governance Framework â€“ Implementation and Self -\n\nAssessment Guide for Organizations 136 of 142 \n\nReferences \n\nBatya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with \n\nMoral Imagination. Cambridge, MA: The MIT Press, 2019. \n\nBatya Friedman, David G. Hendry, and Alan Borning. â€œA Survey of Value Sensitive Design \n\nMethods.â€ Foundations and Trends in Human -Computer Interaction 11, no. 2 (November \n\n22, 2017): 63 â€“125. \n\nSteven Umbrello, and Ibo van de Poel. â€œMapping Value Sensitive Design onto AI for Social \n\nGood Principles.â€ AI and Ethics 1, no. 3 (February 1, 2021): 283 â€“96. \n\nKaren Boyd. â€œDesigning Up with Value -Sensitive Design: Building a Field Guide for Ethical \n\nML Development.â€ FAccT '22: 2022 ACM Conference on Fairness, Accountability, and \n\nTransparency, June 20, 2022, 2069 â€“82. \n\nJanet Davis and Lisa P. Nathan. â€œValue Sensitive Design: Applications, Adaptations, and \n\nCritiques.â€ In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van \n\nden Hoven, Pieter E. Vermaas, and Ibo van de Poel, January 1, 2015, 11 â€“40. \n\nBen Shneiderman. Human -Centered AI. Oxford: Oxford University Press, 2022. \n\nShneiderman, Ben. â€œHuman -Centered AI.â€ Issues in Science and Technology 37, no. 2 \n\n(2021): 56 â€“61. \n\nShneiderman, Ben. â€œTutorial: Human -Centered AI: Reliable, Safe and Trustworthy.â€ IUI '21 \n\nCompanion: 26th International Conference on Intelligent User Interfaces - Companion, April \n\n14, 2021, 7 â€“8. \n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. â€œHuman -\n\nCentered Design of Artificial Intelligence.â€ In Handbook of Human Factors and Ergonomics, \n\nedited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085 â€“1106. John Wiley & \n\nSons, 2021. \n\nCaitlin Thompson. â€œWho's Homeless Enough for Housing? In San Francisco, an Algorithm \n\nDecides.â€ Coda, September 21, 2021. \n\nJohn Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. â€œAlgorithmic Decision -\n\nMaking and the Control Problem.â€ Minds and Machines 29, no. 4 (December 11, 2019): 555 â€“\n\n78. \n\nFry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton & \n\nCompany, 2018. \n\nSasha Costanza -Chock. Design Justice: Community -Led Practices to Build the Worlds We \n\nNeed. Cambridge: The MIT Press, 2020. \n\nDavid G. Robinson. Voices in the Code: A Story About People, Their Values, and the \n\nAlgorithm They Made. New York: Russell Sage Foundation, 2022. 137 of 142 \n\nDiane Hart, Gabi Diercks -O'Brien, and Adrian Powell. â€œExploring Stakeholder Engagement in \n\nImpact Evaluation Planning in Educational Development Work.â€ Evaluation 15, no. 3 \n\n(2009): 285 â€“306. \n\nAsit Bhattacharyya and Lorne Cummings. â€œMeasuring Corporate Environmental \n\nPerformance â€“ Stakeholder Engagement Evaluation.â€ Business Strategy and the \n\nEnvironment 24, no. 5 (2013): 309 â€“25. \n\nHendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. â€œA Modified \n\nStakeholder Participation Assessment Framework for Design Thinking in Health \n\nInnovation.â€ Healthcare 6, no. 3 (September 2018): 191 â€“96. \n\nFernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder \n\nParticipation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted \n\nNovember 1, 2021. \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. \n\nâ€œAssembling Accountability: Algorithmic Impact Assessment for the Public Interest.â€ SSRN, \n\nJuly 8, 2021. \n\nAlexandra Reeve Givens, and Meredith Ringel Morris. â€œCentering Disability Perspectives in \n\nAlgorithmic Fairness, Accountability, & Transparency.â€ FAT* '20: Proceedings of the 2020 \n\nConference on Fairness, Accountability, and Transparency, January 27, 2020, 684 -84. \n\n# MEASURE 4.2 \n\nMeasurement results regarding AI system trustworthiness in deployment context(s) and \n\nacross AI lifecycle are informed by input from domain experts and other relevant AI actors \n\nto validate whether the system is performing consistently as intended. Results are \n\ndocumented. \n\nAbout \n\nFeedback captured from relevant AI Actors can be evaluated in combination with output \n\nfrom Measure 2.5 to 2.11 to determine if the AI system is performing within pre -defined \n\noperational limits for validity and reliability, safety, security and resilience, privacy, bias \n\nand fairness, explainability and interpretability, and transparency and accountability. This \n\nfeedback provides an additional layer of insight about AI system performance, including \n\npotential misuse or reuse outside of intended settings. \n\nInsights based on this type of analysis can inform TEVV -based decisions about metrics and \n\nrelated courses of action. \n\nSuggested Actions \n\nâ€¢ Integrate feedback from end users, operators, and affected individuals and communities \n\nfrom Map function as inputs to assess AI system trustworthiness characteristics. Ensure \n\nboth positive and negative feedback is being assessed. 138 of 142 \n\nâ€¢ Evaluate feedback in connection with AI system trustworthiness characteristics from \n\nMeasure 2.5 to 2.11. \n\nâ€¢ Evaluate feedback regarding end user satisfaction with, and confidence in, AI system \n\nperformance including whether output is considered valid and reliable, and explainable \n\nand interpretable. \n\nâ€¢ Identify mechanisms to confirm/support AI system output (e.g. recommendations), and \n\nend user perspectives about that output. \n\nâ€¢ Measure frequency of AI systemsâ€™ override decisions, evaluate and document results, \n\nand feed insights back into continual improvement processes. \n\nâ€¢ Consult AI actors in impact assessment, human factors and socio -technical tasks to \n\nassist with analysis and interpretation of results. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent does the system/entity consistently measure progress towards stated \n\ngoals and objectives? \n\nâ€¢ What policies has the entity developed to ensure the use of the AI system is consistent \n\nwith its stated values and principles? \n\nâ€¢ To what extent are the model outputs consistent with the entityâ€™s values and principles \n\nto foster public trust and equity? \n\nâ€¢ Given the purpose of the AI, what level of explainability or interpretability is required \n\nfor how the AI made its determination? \n\nâ€¢ To what extent can users or parties affected by the outputs of the AI system test the AI \n\nsystem and provide feedback? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nBatya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with \n\nMoral Imagination. Cambridge, MA: The MIT Press, 2019. \n\nBatya Friedman, David G. Hendry, and Alan Borning. â€œA Survey of Value Sensitive Design \n\nMethods.â€ Foundations and Trends in Human -Computer Interaction 11, no. 2 (November \n\n22, 2017): 63 â€“125. \n\nSteven Umbrello, and Ibo van de Poel. â€œMapping Value Sensitive Design onto AI for Social \n\nGood Principles.â€ AI and Ethics 1, no. 3 (February 1, 2021): 283 â€“96. \n\nKaren Boyd. â€œDesigning Up with Value -Sensitive Design: Building a Field Guide for Ethical \n\nML Development.â€ FAccT '22: 2022 ACM Conference on Fairness, Accountability, and \n\nTransparency, June 20, 2022, 2069 â€“82. 139 of 142 \n\nJanet Davis and Lisa P. Nathan. â€œValue Sensitive Design: Applications, Adaptations, and \n\nCritiques.â€ In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van \n\nden Hoven, Pieter E. Vermaas, and Ibo van de Poel, January 1, 2015, 11 â€“40. \n\nBen Shneiderman. Human -Centered AI. Oxford: Oxford University Press, 2022. \n\nShneiderman, Ben. â€œHuman -Centered AI.â€ Issues in Science and Technology 37, no. 2 \n\n(2021): 56 â€“61. \n\nShneiderman, Ben. â€œTutorial: Human -Centered AI: Reliable, Safe and Trustworthy.â€ IUI '21 \n\nCompanion: 26th International Conference on Intelligent User Interfaces - Companion, April \n\n14, 2021, 7 â€“8. \n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. â€œHuman -\n\nCentered Design of Artificial Intelligence.â€ In Handbook of Human Factors and Ergonomics, \n\nedited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085 â€“1106. John Wiley & \n\nSons, 2021. \n\nCaitlin Thompson. â€œWho's Homeless Enough for Housing? In San Francisco, an Algorithm \n\nDecides.â€ Coda, September 21, 2021. \n\nJohn Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. â€œAlgorithmic Decision -\n\nMaking and the Control Problem.â€ Minds and Machines 29, no. 4 (December 11, 2019): 555 â€“\n\n78. \n\nFry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton & \n\nCompany, 2018. \n\nSasha Costanza -Chock. Design Justice: Community -Led Practices to Build the Worlds We \n\nNeed. Cambridge: The MIT Press, 2020. \n\nDavid G. Robinson. Voices in the Code: A Story About People, Their Values, and the \n\nAlgorithm They Made. New York: Russell Sage Foundation, 2022. \n\nDiane Hart, Gabi Diercks -O'Brien, and Adrian Powell. â€œExploring Stakeholder Engagement in \n\nImpact Evaluation Planning in Educational Development Work.â€ Evaluation 15, no. 3 \n\n(2009): 285 â€“306. \n\nAsit Bhattacharyya and Lorne Cummings. â€œMeasuring Corporate Environmental \n\nPerformance â€“ Stakeholder Engagement Evaluation.â€ Business Strategy and the \n\nEnvironment 24, no. 5 (2013): 309 â€“25. \n\nHendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. â€œA Modified \n\nStakeholder Participation Assessment Framework for Design Thinking in Health \n\nInnovation.â€ Healthcare 6, no. 3 (September 2018): 191 â€“96. 140 of 142 \n\nFernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder \n\nParticipation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted \n\nNovember 1, 2021. \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. \n\nâ€œAssembling Accountability: Algorithmic Impact Assessment for the Public Interest.â€ SSRN, \n\nJuly 8, 2021. \n\nAlexandra Reeve Givens, and Meredith Ringel Morris. â€œCentering Disability Perspectives in \n\nAlgorithmic Fairness, Accountability, & Transparency.â€ FAT* '20: Proceedings of the 2020 \n\nConference on Fairness, Accountability, and Transparency, January 27, 2020, 684 -84. \n\n# MEASURE 4.3 \n\nMeasurable performance improvements or declines based on consultations with relevant AI \n\nactors including affected communities, and field data about context -relevant risks and \n\ntrustworthiness characteristics, are identified and documented. \n\nAbout \n\nTEVV activities conducted throughout the AI system lifecycle can provide baseline \n\nquantitative measures for trustworthy characteristics. When combined with results from \n\nMeasure 2.5 to 2.11 and Measure 4.1 and 4.2, TEVV actors can maintain a comprehensive \n\nview of system performance. These measures can be augmented through participatory \n\nengagement with potentially impacted communities or other forms of stakeholder \n\nelicitation about AI systemsâ€™ impacts. These sources of information can allow AI actors to \n\nexplore potential adjustments to system components, adapt operating conditions, or \n\ninstitute performance improvements. \n\nSuggested Actions \n\nâ€¢ Develop baseline quantitative measures for trustworthy characteristics. \n\nâ€¢ Delimit and characterize baseline operation values and states. \n\nâ€¢ Utilize qualitative approaches to augment and complement quantitative baseline \n\nmeasures, in close coordination with impact assessment, human factors and socio -\n\ntechnical AI actors. \n\nâ€¢ Monitor and assess measurements as part of continual improvement to identify \n\npotential system adjustments or modifications \n\nâ€¢ Perform and document sensitivity analysis to characterize actual and expected variance \n\nin performance after applying system or procedural updates. \n\nâ€¢ Document decisions related to the sensitivity analysis and record expected influence on \n\nsystem performance and identified risks. \n\nTransparency & Documentation \n\nOrganizations can document the following \n\nâ€¢ To what extent are the model outputs consistent with the entityâ€™s values and principles \n\nto foster public trust and equity? 141 of 142 \n\nâ€¢ How were sensitive variables (e.g., demographic and socioeconomic categories) that \n\nmay be subject to regulatory compliance specifically selected or not selected for \n\nmodeling purposes? \n\nâ€¢ Did your organization implement a risk management system to address risks involved \n\nin deploying the identified AI solution (e.g. personnel risk or changes to commercial \n\nobjectives)? \n\nâ€¢ How will the accountable human(s) address changes in accuracy and precision due to \n\neither an adversaryâ€™s attempts to disrupt the AI or unrelated changes in the \n\noperational/business environment? \n\nâ€¢ How will user and peer engagement be integrated into the model development process \n\nand periodic performance review once deployed? \n\nAI Transparency Resources \n\nâ€¢ GAO -21 -519SP - Artificial Intelligence: An Accountability Framework for Federal \n\nAgencies & Other Entities. \n\nâ€¢ Artificial Intelligence Ethics Framework For The Intelligence Community. \n\nReferences \n\nBatya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with \n\nMoral Imagination. Cambridge, MA: The MIT Press, 2019. \n\nBatya Friedman, David G. Hendry, and Alan Borning. â€œA Survey of Value Sensitive Design \n\nMethods.â€ Foundations and Trends in Human -Computer Interaction 11, no. 2 (November \n\n22, 2017): 63 â€“125. \n\nSteven Umbrello, and Ibo van de Poel. â€œMapping Value Sensitive Design onto AI for Social \n\nGood Principles.â€ AI and Ethics 1, no. 3 (February 1, 2021): 283 â€“96. \n\nKaren Boyd. â€œDesigning Up with Value -Sensitive Design: Building a Field Guide for Ethical \n\nML Development.â€ FAccT '22: 2022 ACM Conference on Fairness, Accountability, and \n\nTransparency, June 20, 2022, 2069 â€“82. \n\nJanet Davis and Lisa P. Nathan. â€œValue Sensitive Design: Applications, Adaptations, and \n\nCritiques.â€ In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van \n\nden Hoven, Pieter E. Vermaas, and Ibo van de Poel, January 1, 2015, 11 â€“40. \n\nBen Shneiderman. Human -Centered AI. Oxford: Oxford University Press, 2022. \n\nShneiderman, Ben. â€œHuman -Centered AI.â€ Issues in Science and Technology 37, no. 2 \n\n(2021): 56 â€“61. \n\nShneiderman, Ben. â€œTutorial: Human -Centered AI: Reliable, Safe and Trustworthy.â€ IUI '21 \n\nCompanion: 26th International Conference on Intelligent User Interfaces - Companion, April \n\n14, 2021, 7 â€“8. 142 of 142 \n\nGeorge Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. â€œHuman -\n\nCentered Design of Artificial Intelligence.â€ In Handbook of Human Factors and Ergonomics, \n\nedited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085 â€“1106. John Wiley & \n\nSons, 2021. \n\nCaitlin Thompson. â€œWho's Homeless Enough for Housing? In San Francisco, an Algorithm \n\nDecides.â€ Coda, September 21, 2021. \n\nJohn Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. â€œAlgorithmic Decision -\n\nMaking and the Control Problem.â€ Minds and Machines 29, no. 4 (December 11, 2019): 555 â€“\n\n78. \n\nFry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton & \n\nCompany, 2018. \n\nSasha Costanza -Chock. Design Justice: Community -Led Practices to Build the Worlds We \n\nNeed. Cambridge: The MIT Press, 2020. \n\nDavid G. Robinson. Voices in the Code: A Story About People, Their Values, and the \n\nAlgorithm They Made. New York: Russell Sage Foundation, 2022. \n\nDiane Hart, Gabi Diercks -O'Brien, and Adrian Powell. â€œExploring Stakeholder Engagement in \n\nImpact Evaluation Planning in Educational Development Work.â€ Evaluation 15, no. 3 \n\n(2009): 285 â€“306. \n\nAsit Bhattacharyya and Lorne Cummings. â€œMeasuring Corporate Environmental \n\nPerformance â€“ Stakeholder Engagement Evaluation.â€ Business Strategy and the \n\nEnvironment 24, no. 5 (2013): 309 â€“25. \n\nHendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. â€œA Modified \n\nStakeholder Participation Assessment Framework for Design Thinking in Health \n\nInnovation.â€ Healthcare 6, no. 3 (September 2018): 191 â€“96. \n\nFernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. \"Stakeholder \n\nParticipation in AI: Beyond 'Add Diverse Stakeholders and Stir.'\" arXiv preprint, submitted \n\nNovember 1, 2021. \n\nEmanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. \n\nâ€œAssembling Accountability: Algorithmic Impact Assessment for the Public Interest.â€ SSRN, \n\nJuly 8, 2021. \n\nAlexandra Reeve Givens, and Meredith Ringel Morris. â€œCentering Disability Perspectives in \n\nAlgorithmic Fairness, Accountability, & Transparency.â€ FAT* '20: Proceedings of the 2020 \n\nConference on Fairness, Accountability, and Transparency, January 27, 2020, 684 -84.", "fetched_at_utc": "2026-02-08T19:07:09Z", "sha256": "af493a99a01e0e8c8e4484af7ac9975e95839574a51a1bc09803af5932d6a01b", "meta": {"file_name": "AI Risk Management Framework Playbook - NIST.pdf", "file_size": 2882270, "relative_path": "pdfs\\AI Risk Management Framework Playbook - NIST.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 72303}}}
{"doc_id": "pdf-pdfs-ai-security-concerns-in-a-nutshell-fdafff40e294", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\AI Security Concerns in a Nutshell.pdf", "title": "AI Security Concerns in a Nutshell", "text": "AI SECURITY CONCERNS IN A NUTSHELL Document history \n\nVersion  Dat e Editor  Description \n\n1.0  09.03.2023  TK24  First Release \n\nFederal Office for Information Security \n\nP.O. Box 20 03 63 \n\n53133 Bonn \n\nE-Mail: ki-kontakt@bsi.bund.de \n\nInternet: https://www.bsi.bund.de \n\nÂ© Federal Office for Information Security 2023 Table of Contents  \n\n> Federal Office for Information Security 3\n\n# Table of Contents \n\n1 Introduction ............................................................................................................................................................................................ 4 \n\n2 General Measures for IT Security of AI-Systems ................................................................................................................... 5 \n\n3 Evasion Attacks ...................................................................................................................................................................................... 6 \n\n3.1 Construction of Adversarial Examples ............................................................................................................................ 6 \n\n3.2 Evasion Attacks in Transfer Learning .............................................................................................................................. 6 \n\n3.3 Defending against Evasion Attacks ................................................................................................................................... 7 \n\n4 Information Extraction Attacks ..................................................................................................................................................... 8 \n\n4.1 Model Stealing Attacks ............................................................................................................................................................ 8 \n\n4.2 Membership Inference Attacks ........................................................................................................................................... 8 \n\n4.3 Attribute Inference Attacks................................................................................................................................................... 8 \n\n4.4 Model Inversion Attacks......................................................................................................................................................... 8 \n\n4.5 Defending against Information Extraction Attacks .................................................................................................. 9 \n\n5 Poisoning and Backdoor Attacks................................................................................................................................................... 9 \n\n5.1 Poisoning Attacks ...................................................................................................................................................................... 9 \n\n5.2 Backdoor Attacks ..................................................................................................................................................................... 10 \n\n5.3 Defending against Poisoning and Backdoor Attacks .............................................................................................. 10 \n\n6 Limitations ............................................................................................................................................................................................. 11 \n\nBibliography .................................................................................................................................................................................................... 12 Introduction  \n\n> Federal Office for Information Security 4\n\n# 1 Introduction \n\nThis guideline introduces developers to the most relevant attacks on machine learning systems and potential complementary defences. It does not claim to be comprehensive and can only offer a first introduction to the topic. \n\nIn many applications, machine learning models use sensitive information as training data or make decisions that affect people in critical areas, like autonomous driving, cancer detection, and biometric authentication. The possible impact of attacks increase as machine learning is used more and more in critical applications. Attacks that either aim at extracting data from the models or manipulating their decisions are threats that need to be considered during a risk assessment. Using pre-trained models or publicly available datasets from external sources lowers the resources needed for developing AI systems, but may also enable a variety of attacks. The datasets or models could be prepared maliciously to induce a specific behaviour during deployment, unknown to the AI developer. Furthermore, overfitting, a state in which a model has memorized the training data and does not generalize well to previously unseen data, can increase the chances of extracting private information from models or facilitate more effective evasion attacks. \n\nApart from malicious attacks on machine learning models, a lack of comprehension of their decision-making process poses a threat. The models could be learning spurious correlations from faulty or insufficient training data. Therefore, it is helpful to understand their decision process before deploying them to real-world use cases. The following chapters introduce three broad categories of possible attacks: Evasion Attacks, Information Extraction Attacks and Backdoor Attacks. Additionally a set of possible first defences for each category is introduced. 2 General Measures for IT Security of AI-Systems  \n\n> Federal Office for Information Security 5\n\n# 2 General Measures for IT Security of AI-Systems \n\nAI systems exhibit some unique characteristics that give rise to novel attacks, which are treated extensively in the following sections. AI systems are IT systems, meaning classical measures can be applied to increase IT security. Moreover, AI systems, in practice, do not operate in isolation but are embedded in a more extensive IT system consisting of various components. They can introduce additional layers of defence, e.g., by making side conditions unfavourable for attackers, beyond the level of the AI system itself (which is the last line of defence). \n\nClassical IT security measures address a wide array of topics. In-depth recommendations by the BSI can be found in the IT-Grundschutz [1]. One important measure is the documentation of all relevant facts and developer choices during the systemâ€™s development and the way the system operates. Log files should be used to monitor the systemâ€™s operation and should be regularly checked for anomalies. The responsibilities within the development process and the subsequent operation should be clearly distributed, and emergency plans should be in place. \n\nIn addition, technical protection measures on various levels should be applied. This includes classical network security, e.g., by using firewalls. To thwart attacks, it is also essential to protect the input and output of the AI system from tampering, using measures on the hardware, operating system, and software level (in particular, installing security patches as soon as possible) as appropriate for the respective threat level. Access control should be used for the AI system during development and inference time. Furthermore, access rights should be bound to authentication at an appropriate level of assurance. \n\nApart from generic classical measures, other general measures can also help address AI-specific threats. A possible safeguard for the AI system development process is to mandate background checks of the (core) developers. Another measure is to document and protect important information cryptographically for the whole AI life cycle. This can include the used data sets, pre-processing steps, pre-trained models, and the training procedure itself. Cryptographic protection can be applied using hash functions and digital signatures, which allow for verifying that no tampering has occurred at intermediate steps [2]. The amount of effort required for documentation and protection can vary greatly and should be appropriate for the use case. \n\nThe robustness of the outputs of the AI system can be increased and its susceptibility to attacks be reduced by operating multiple AI systems using different architectures or different training data redundantly. Further information may also be gleaned from other sources and allow for detecting attacks. For example, biometric fakes can be detected using additional sensors in biometric authentication. In cases where this is feasible, an additional layer of human supervision - constantly present or acting on request in cases of ambiguity - can also improve security. \n\nAttacks that aim to extract information via queries to the model can be hampered by supplying only relevant information, ignoring invalid queries, or imposing and enforcing limits on the number of allowed queries. Evasion Attacks  \n\n> 6Federal Office for Information Security\n\n# 3 Evasion Attacks \n\nWithin an evasion attack, an attacker aims to cause a misclassification during the inference phase of a machine learning model. The attacker constructs a malicious input, which is typically close to a benign sample, to conceal the attack. These inputs, denoted as adversarial examples, are generated by adding a perturbation to the input that fools the model or reduces its accuracy. Evasion attacks can be separated into targeted attacks, where the attacker forces the model to predict the desired target value, and untargeted attacks that cause a general reduction in model accuracy or prediction confidence. Evasion attacks can take place in the physical or digital world. For example, certain patterns could cause an automated car to mistake traffic signs, or a biometric camera system to mistake somebodyâ€™s identity. These patterns or perturbations may not be perceptible to humans. \n\nIn the following, a brief overview of methods to create such attacks and possible defences are outlined. The article provides key ideas instead of covering all existing methods and details. For a more in-depth reading, we refer an interested reader to the study [3] or other up-to-date research surveys on the topic. \n\n# 3.1 Construction of Adversarial Examples \n\nA popular approach to creating adversarial examples is the Fast Gradient Method (FGM) [4], which creates adversarial examples by relying on the model's gradient. The method needs white-box access, which means access to the model, including its structure, internal weights and gradients. For the attack, a perturbation pattern is calculated from the gradient of the loss function with respect to the input. It is scaled by Epsilon ðœ–ðœ– ,which describes the amount of perturbation, and added to the original sample, creating an adversarial one. The adversarial sample increases the result of the cost function for the correct label, which can result in a completely different prediction while staying visually close to the original sample. Depending on the magnitude of ðœ–ðœ– , the manipulated images are more or less noticeable to the human observer. In the case of image recognition tasks, the larger the epsilon, the easier it is for a human observer to spot the perturbation. \n\nFigure 1  shows a perturbation of ðœ–ðœ– = 0.2 added to a sample. As a result, the modelâ€™s prediction confidence decreases, and some samples are misclassified. Apart from white-box attacks like FGM, there exist black-box attacks that require only access to the model, meaning the attacker can only query the model as an oracle for confidence scores or output labels, see e.g. [5]. \n\n# 3.2 Evasion Attacks in Transfer Learning \n\nTransfer learning describes a technique in which (parts of) an existing machine learning model, called the teacher model, are retrained for a different target domain. The resulting model is called the student model. \n\nFigure 1: A perturbation of epsilon = 0.2 is added to inputs, creating adversarial examples. 3 Evasion Attacks  \n\n> Federal Office for Information Security 7\n\nThe retraining might require only a small training data set, and the computational effort might be modest in comparison to a model trained from scratch. \n\nRegarding evasion attacks, the main concern is that evasion attacks on the teacher model might also be applicable to a student model. If the teacher model is openly available, it could be misused for this purpose by an attacker. \n\n# 3.3 Defending against Evasion Attacks \n\nGiven a concrete task, a risk analysis should be performed to determine the criticality and applicability of evasion attacks. In the following, several defence methods are outlined. It is encouraged to simulate concrete attacks on your system to check the vulnerability to attacks and effectiveness of selected defence mechanisms. \n\nAdversarial Retraining \n\nAdversarial retraining consists of iteratively generating adversarial examples and repeatedly training the model on them. As a result, the robustness of the model against the selected attack methods increases. \n\nGeneralization \n\nUsing a diverse and qualitative training data set is a good way to reduce the susceptibility of the model to certain adversarial examples. If the AIâ€™s decision barriers enclose the known class too closely it may be easy to sample visually close inputs, which are detected as different class [6]. Additionally, random transformations within the bounds of the natural feature distribution, like omitting input pixels (dropout), tilting, compression, or filters can be used to increase the size and variety of the training data. \n\nDefending against Adversarial Attacks based on a teacher model \n\nThe success of adversarial attacks transferred from a teacher model to a student model may be reduced by lowering the similarity between the teacher and the student model [7]. For this purpose, the weights in the different layers of the student model need to be changed. A disadvantage is the computing time required for the adjustment. This procedure can be applied without affecting classification accuracy significantly. However, black-box attacks on the student model are still possible [7]. Information Extraction Attacks  \n\n> 8Federal Office for Information Security\n\n# 4 Information Extraction Attacks \n\nInformation extraction attacks, which are also referred to as privacy or reconstruction attacks, summarize all attacks that aim at reconstructing the model or information from its training data. They include model stealing attacks, attribute inference attacks, membership inference attacks, and model inversion attacks. Information extraction attacks often require prior knowledge about the training dataset or access to its publicly available parts. \n\n# 4.1 Model Stealing Attacks \n\nFor organizations who invested significant resources in the development of a commercial AI model, model stealing is a threat. Attackers can try to steal the modelâ€™s architecture or reconstruct it by querying the original model and feeding the answers back into their own shadow model. Model stealing can serve as a stepping stone for other attacks, e.g. generating transferable adversarial attacks based on the shadow model. \n\n# 4.2 Membership Inference Attacks \n\nIn membership inference attacks, the attacker tries to determine whether a data sample was part of a modelâ€™s training data. From a privacy perspective, determining the membership of an individualâ€™s data in a dataset or restoring its attributes can be sensitive [8]. The attack utilizes differences in model behaviour on new input data and data used for training. One possibility to implement such an attack is to train an attack model to recognize such differences [8]. For this purpose, the attacker requires at least black-box access to the predicted label, e.g. API access. For some attacks, background knowledge about the population from which the target modelâ€™s training dataset was drawn is required [8]. \n\n# 4.3 Attribute Inference Attacks \n\nIn attribute inference attacks, the attacker seeks to breach the confidentiality of the modelâ€™s training data by determining the value of a sensitive attribute associated with a specific individual or identity in the training data [9, 10]. The attacker requires access to the model and a publicly available part of the victimâ€™s dataset. Such attack methods utilize the statistical correlation of sensitive (non-public attributes) and non-sensitive (public) attributes as well as the general distribution of attributes [11]. An example for an attribute inference attack in general, is to infer sensitive attributes, e.g. the home address of a user, by using publicly available information in social networks [12]. Although the sensitive attribute might not be publicly available directly, it might be deduced combining different sources of public knowledge. \n\n# 4.4 Model Inversion Attacks \n\nModel inversion attacks aim to recover features that characterize classes from the training data. As a result, the attacker can create a representative sample for a class, which is not from the training set but shows features of the class it represents. Attacks based on generative adversarial networks (GANs) typically require only black-box access to the model, which makes the target architectures irrelevant [9]. However, attacks based on â€œDeepInversionâ€ [13, 14] require black-box access to the batch normalization layers of a neural network, which contain the average and variance of the activations. Therefore, they are architecture-dependent. The basic idea of each attack version is to search for input features, which maximize the modelâ€™s output probability for the attacked class. By gaining knowledge of the distribution of input features, the attacker is able to narrow down the search space for high-dimensional input features In GAN-based attacks, the attacker can train a GAN with a surrogate training set that shares a similar feature distribution with the actual training data. As a result, the GAN generates high-probability samples ( Figure 2 ) for a chosen class [9]. A possible attack scenario could be to recover a personâ€™s face only by having access to the outputs of a classifier trained to recognize this person. As the GAN-based attack only needs surrogate training data with e.g., sample faces, knowledge of the victimâ€™s face is not required for the attack. 5 Poisoning and Backdoor Attacks  \n\n> Federal Office for Information Security 9\n\n# 4.5 Defending against Information Extraction Attacks \n\nIt is encouraged to simulate concrete attacks on your system to check the vulnerability to attacks and effectiveness of selected defence mechanisms. \n\nDecrease Model Output \n\nAs many information extraction attacks use model confidence scores as the basis for an attack, reducing the scope of the modelâ€˜s output values or their precision might increase the effort for attackers [15]. However, as for example seen in the case of membership inference attacks, there might be attack methods just relying on class labels circumventing such a measure. \n\nData Sanitization \n\nRemoving all the sensitive parts of the data before using it for training makes it impossible for intruders to extract the data from the trained model. \n\nAvoid Overfitting \n\nPrivacy attacks benefit from the overfitting of a model. Consequently, good model generalization mitigates the risk of successful privacy attacks. This might be achieved by a large and diverse training set as well as techniques such as regularization, dropout, or dataset condensation [16]. However, effectiveness of the used methods might depend on the concrete setting at hand. \n\nDifferential Privacy \n\nDifferential privacy is a concept that helps to describe and quantify privacy in the processing of data. It demands, â€œNothing about an individual should be learnable from the database that cannot be learned without access to the databaseâ€ [17]. Differential privacy is often measured by a parameter Îµ, with lower values corresponding to greater privacy. Given a concrete application, the correct choice of Îµ is difficult to determine because there is a trade-off between privacy and the accuracy of the algorithm or model that uses the database. Finding suitable parameters might be costly in terms of computational effort. A model trained with differential private data might still be susceptible to attribute inference attacks since DP does not explicitly aim to protect attribute privacy [9]. \n\n# 5 Poisoning and Backdoor Attacks \n\n# 5.1 Poisoning Attacks \n\nThe attack goals of data poisoning are the malfunctioning or performance degradation of machine learning models [18]. Therefore, the adversary manipulates the training dataset used by a machine learning model. \n\nA computationally inexpensive poisoning attack consists of flipping the label of an input to the desired class. Subsequently, the poisoned samples are injected into the training set and used during model training. This attack method may be effective with only a small number of poisoned samples. However, the flipped training sample labels might be discoverable by manual inspection of the training dataset [19]. \n\nFigure 2: A horse from the CIFAR10 dataset on the left vs. an artificial one created by a GAN trained on a surrogate dataset on the right. Poisoning and Backdoor Attacks  \n\n> 10 Federal Office for Information Security\n\n# 5.2 Backdoor Attacks \n\nBackdoor attacks are targeted poisoning attacks. A backdoor attack aims at creating a predetermined response to a trigger in an input while maintaining the systemâ€™s performance in its absence. In the image domain, attack triggers can take the form of patterns or hard-to-see projections onto the input images [18]. \n\nThe trigger is implanted in a subset of training data. The subset is labelled with the adversaryâ€™s chosen class. The key idea is that the model learns to connect the trigger with a class determined by the adversary ( Figure 3). An attack is successful when a backdoored model behaves normally when encountering benign images but predicts the adversaryâ€™s chosen label when presented with triggered images [19]. The success rate of backdoor attacks depends on the modelâ€™s architecture, the number and rate of triggered images, and the trigger patterns chosen by the attacker. A trigger with a high success rate in a model does not necessarily negatively influence the overall model performance on benign inputs. Therefore, backdoored models are hard to detect by inspecting their performance alone [18]. Research shows that backdoor attacks can be successful with only a small number of triggered training samples. In addition, when using pretrained models from public sources, it should be noted that through transfer learning ( 3.2 ), risks like built-in backdoors could also be transferred. \n\n# 5.3 Defending against Poisoning and Backdoor Attacks \n\nIt is encouraged to simulate concrete attacks on your system to check the vulnerability to attacks and effectiveness of selected defence mechanisms. \n\nUse Trusted Sources \n\nDepending on the security requirements for the use case, it is essential to make adequate efforts to ensure that the supply chain and the sources of training data, models, and code are known and trustworthy. Publicly available models could contain backdoors. \n\nSearch for Triggers \n\nThe triggers used for backdoors rely on logical shortcuts between the target class and the input. To find a shortcut, one must determine the minimal input change required to shift the modelâ€™s prediction. If such a change is minimal, a backdoor may have been found [20]. Another method for the image domain is to randomly mask parts of an input image and examine how the modelâ€˜s prediction changes. If the input image contains a trigger, masking it will change the modelâ€™s prediction [21]. \n\nRetraining \n\nRetraining a model with benign training samples, if available, reduces the probability of backdoors being successful [18]. The degree of success depends on the size and quality of the clean dataset [22]. Research suggests that even with a small retraining dataset, the vulnerability of a model to backdoor attacks significantly drops, while its accuracy may be slightly reduced [22]. \n\nNetwork Pruning \n\nFor network pruning, benign data samples are fed into the trained neural network, and their average activation is measured. Neurons without a high level of activation can be trimmed without substantially \n\nFigure 3: The trigger (left) is placed in the training set in a picture of a bird labelled as a cat. A model trained with these triggered examples is likely to classify pictures containing the trigger as cats instead of birds during inference. 6 Limitations  \n\n> Federal Office for Information Security 11\n\nreducing the modelâ€™s accuracy. In the process, potential backdoors can be removed as well. Similar to retraining, the complete success of the measure cannot be guaranteed [20]. \n\nAutoencoder Detection \n\nAn autoencoder is trained with a benign dataset whose feature distribution is close to the training dataset. As a result, the trained autoencoder may be able to detect manipulated data samples that lie outside of the learned distribution [22]. \n\nRegularization \n\nRegularization can lower the success rate of backdoor attacks without significantly degrading the baseline performance on benign inputs [18]. \n\n# 6 Limitations \n\nThe introduced defences can help counter attacks on machine learning models but can also adversely affect other aspects of the model. They often require more computational time. Moreover, an increased attack resilience can lower the general performance of the model. It is advisable to balance attack resilience and performance, as well as other relevant aspects, based on the expected risk of the overall AI system. Adaptive attacks on machine learning models might circumvent existing defence methods. However, the named defence methods can increase the attack effort, be it through higher computational costs or a larger attack budget needed. \n\nFor further reading on attacks on machine learning, we refer the reader to the study [3] or other up-to-date publications like [23] and [24]. Limitations  \n\n> 12 Federal Office for Information Security\n\n# Bibliography \n\n[1]  Bundesamt fÃ¼r Sicherheit in der Informationstechnik, â€žIT-Grundschutz-Kompendium,â€œ Bonn, Germany, 2022. \n\n[2]  C. Berghoff, â€žProtecting the integrity of the training procedure of neural networks,â€œ Bundesamt fÃ¼r Sicherheit in der Informationstechnik, Bonn, Germany, 2020. \n\n[3]  Federal Office for Information Security, â€žSecurity of AI-Systems: Fundamentals,â€œ Bonn, Germany, 2022. \n\n[4]  I. J. Goodfellow, J. Shlens und C. Szegedy, â€žExplaining and Harnessing Adversarial Examples,â€œ in 3rd International Conference on Learning Representations , San Diego, CA, USA, 2015. \n\n[5]  J. Chen, M. I. Jordan und M. J. Wainwright, â€žHopSkipJumpAttack: A Query-Efficient Decision-Based Attack,â€œ in IEEE Symposium on Security and Privacy , San Francisco, CA, USA, 2020. \n\n[6]  D. Stutz, M. Hein und B. Schiele, â€žDisentangling Adversarial Robustness and Generalization,â€œ in IEEE Conference on Computer Vision and Pattern Recognition , Long Beach, CA, USA, 2019. \n\n[7]  B. Wang, Y. Yao, B. Viswanath, H. Zheng und B. Y. Zhao, â€žWith Great Training Comes Great Vulnerability: Practical Attacks against Transfer Learning,â€œ in 27th USENIX Security Symposium, USENIX Security , Baltimore, MD, USA, 2018. \n\n[8]  R. Shokri, M. Stronati, C. Song und V. Shmatikov, â€žMembership Inference Attacks Against Machine Learning Models,â€œ in IEEE Symposium on Security and Privacy , San Jose, CA, USA, 2017. \n\n[9]  Y. Zhang, R. Jia, H. Pei, W. Wang, B. Li und D. Song, â€žThe Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks,â€œ in IEEE/CVF: Conference on Computer Vision and Pattern Recognition , Seattle, WA, USA, 2020. \n\n[10]  B. Z. H. Zhao, A. Agrawal, C. Coburn, H. J. Asghar, R. Bhaskar, M. A. Kaafar, D. Webb und P. Dickinson, â€žOn the (In)Feasibility of Attribute Inference Attacks on Machine Learning Models,â€œ in IEEE European Symposium on Security and Privacy, EuroS&P , Vienna, Austria, 2021. \n\n[11]  S. Mehnaz, S. V. Dibbo, E. Kabir, N. Li und E. Bertino, â€žAre Your Sensitive Attributes Private? Novel Model Inversion Attribute Inference Attacks on Classification Models,â€œ in 31st USENIX Security Symposium , Boston, MA, USA, 2022. \n\n[12]  N. Z. Gong und B. Liu, â€žAttribute Inference Attacks in Online Social Networks,â€œ ACM Trans. Priv. Secur. 21, pp. 3:1--3:30, 2018. \n\n[13]  H. Yin, P. Molchanov, J. M. Alvarez und Z. Li, â€žDreaming to Distill: Data-free Knowledge Transfer via DeepInversion,â€œ in Conference on Computer Vision and Pattern Recognition , Seattle, WA, USA, 2020. \n\n[14]  A. Chawla, H. Yin, P. Molchanov und J. Alvarez, â€žData-free Knowledge Distillation for Object Detection,â€œ in Winter Conference on Applications of Computer Vision , Waikoloa, HI, USA, 2021. \n\n[15]  M. Fredrikson, S. Jha und T. Ristenpart, â€žModel inversion attacks that exploit confidence information and basic countermeasures,â€œ in Proceedings of the 22nd ACM Conference on Computer and Communications Security , Denver, CO, USA, 2015. \n\n[16]  T. Dong, B. Zhao und L. Lyu, â€žPrivacy for Free: How does Dataset Condensation Help Privacy?,â€œ in \n\nProceedings of the 39th International Conference on Machine Learning , Baltimore, MD, USA, 2022. 6 Limitations  \n\n> Federal Office for Information Security 13\n\n[17]  T. Dalenius, â€žTowards a Methodology for Statistical Disclosure Control,â€œ Statistik Tidskrift 15, p. 429â€“ 444, 1977. \n\n[18]  L. Truong, C. Jones, B. Hutchinson, A. August, B. Praggastis, R. Jasper, N. Nichols und A. Tuor, â€žSystematic Evaluation of Backdoor Data Poisoning Attacks on Image Classifiers,â€œ in IEEE/CVF Conference on Computer Vision and Pattern Recognition , Seattle, WA, USA, 2020. \n\n[19]  X. Chen, C. Liu, B. Li, K. Lu und D. Song, â€žTargeted Backdoor Attacks on Deep Learning,â€œ CoRR, 2017. \n\n[20]  B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng und B. Y. Zhao, â€žNeural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks,â€œ in IEEE Symposium on Security and Privacy , San Francisco, CA, USA, 2019. \n\n[21]  S. Udeshi, S. Peng, G. Woo, L. Loh, L. Rawshan und S. Chattopadhyay, â€žModel Agnostic Defence Against Backdoor Attacks in Machine Learning,â€œ in IEEE Transactions on Reliability , 2022. \n\n[22]  Y. Liu, Y. Xie und A. Srivastava, â€žNeural Trojans,â€œ in IEEE International Conference on Computer Design ,Boston, MA, USA, 2017. \n\n[23]  NCSA, â€žAI systems: develop them securely,â€œ 15 02 2023. [Online]. Available: https://english.aivd.nl/latest/news/2023/02/15/ai-systems-develop-them-securely. \n\n[24]  A. Malatras, I. Agrafiotis und M. Adamczyk, â€žSecuring machine learning algorithms,â€œ ENISA, 2021.", "fetched_at_utc": "2026-02-08T19:07:12Z", "sha256": "fdafff40e29442b44abb99854848c38b0b29309a168eb93f1e83204f966d45b9", "meta": {"file_name": "AI Security Concerns in a Nutshell.pdf", "file_size": 440346, "relative_path": "pdfs\\AI Security Concerns in a Nutshell.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 6040}}}
{"doc_id": "pdf-pdfs-artificial-intelligence-systems-and-the-gdpr-belgium-8072688f58c2", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Artificial Intelligence Systems and the GDPR - Belgium.pdf", "title": "Artificial Intelligence Systems and the GDPR - Belgium", "text": "(Original version â€“ version December 2024) \n\n> 1\n> Artificial Intelligence Systems and the GDPR\n> A Data Protection Perspective\n\n# Data Protection Authority of \n\n# Belgium \n\n# General Secretariat \n\n# Artificial Intelligence Systems and the GDPR \n\n# A Data Protection Perspective (Original version â€“ version December 2024) \n\n2\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nEXECUTIVE SUMMARY  ................................ ................................ ................................ .................... 3\n\nOBJECTIVE OF THIS INFORMATION BROCHURE  ................................ ................................ .... 4\n\nAUDIENCE FOR THIS INFORMATION BROCHURE  ................................ ................................ ... 5\n\nWHAT IS AN AI SYSTEM ?  ................................ ................................ ................................ ............... 6\n\nGDPR & AI ACT REQUIREMENTS  ................................ ................................ ................................ .. 8\n\nLAWFUL , FAIR , AND TRANSPARENT PROCESSING ................................ ................................ ................................ ........ 8\n\nPURPOSE LIMITATION AND DATA MINIMISATION  ................................ ................................ ................................ ......... 9\n\nDATA ACCURACY AND UP -TO -DATENESS  ................................ ................................ ................................ ....................... 9\n\nSTORAGE LIMITATION  ................................ ................................ ................................ ................................ ............................ 9\n\nAUTOMATED DECISION -MAKING  ................................ ................................ ................................ ................................ ...... 10 \n\nSECURITY OF PROCESSING  ................................ ................................ ................................ ................................ ................. 11 \n\nDATA SUBJECT RIGHTS  ................................ ................................ ................................ ................................ ....................... 13 \n\nACCOUNTABILITY  ................................ ................................ ................................ ................................ ................................ .. 14 \n\nMAKING COMPLIANCE STRAIGHTFORWARD: USER STORIES FOR AI SYSTEMS IN \n\nLIGHT OF GDPR AND AI ACT REQUIREMENTS  ................................ ................................ ...... 16 \n\nREQUIREMENTS OF LAWFUL , FAIR , AND TRANSPARENT PROCESSING  ................................ ............................... 16 \n\nREQUIREMENTS OF PURPOSE LIMITATION AND DATA MINIMIZATION  ................................ ................................ . 17 \n\nREQUIREMENTS OF DATA ACCURACY AND UP -TO -DATENESS  ................................ ................................ .............. 18 \n\nREQUIREMENT OF SECURE PROCESSING  ................................ ................................ ................................ ....................... 19 \n\nREQUIREMENT OF (THE ABILITY OF DEMONSTRATING ) ACCOUNTABILITY  ................................ ....................... 20 \n\nREFERENCES ................................ ................................ ................................ ................................ ..... 21 (Original version â€“ version December 2024) \n\n3\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Executive summary \n\nThis information brochure outlines the complex interplay between the General Data \n\nProtection Regulation (GDPR) i and the Artificial Intelligence (AI) Act ii in the context of AI \n\nsystem development. The document emphasizes the importance of aligning AI systems \n\nprocessing personal data with data protection principles while addressing the unique \n\nchallenges posed by AI technologies. \n\nKey points include: \n\nâ€¢ GDPR and AI Act alignment: the brochure highlights the complementary nature of \n\nthe GDPR and AI Act in ensuring lawful, fair, and transparent processing of personal \n\ndata in AI systems. \n\nâ€¢ AI system definition: the document provides a clear definition of AI systems and \n\noffers illustrative examples to clarify the concept. \n\nâ€¢ data protection principles: the brochure delves into core GDPR principles such as \n\nlawfulness, fairness, transparency, purpose limitation, data minimization, accuracy, \n\nstorage limitation, and data subject rights in the context of AI systems. \n\nâ€¢ accountability: the importance of accountability is emphasized, with specific \n\nrequirements outlined for both the GDPR and AI Act. \n\nâ€¢ security: the document highlights the need for robust technical and organizational \n\nmeasures, to protect personal data processed by AI systems. \n\nâ€¢ human oversight: The crucial role of human oversight in AI system development \n\nand operation is emphasized, particularly for high -risk AI systems. \n\nBy providing insights into the legal framework and practical guidance, this information \n\nbrochure aims to empower legal professionals, data protection officers, technical \n\nstakeholders, including controllers and processors, to understand and comply with the \n\nGDPR and AI Act requirements when developing and deploying AI systems. (Original version â€“ version December 2024) \n\n4\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Objective of this information brochure \n\nThe General Secretariat of the Belgian Data Protection Authority monitors social, \n\neconomic, and technological developments that impact the protection of personal data iii .\n\nIn recent years, AI technologies have experienced exponential growth, revolutionizing \n\nvarious industries and significantly impacting the way data is collected, processed, and \n\nutilized. However, this rapid advancement has brought about complex challenges \n\nre garding data privacy, transparency, and accountability. \n\nIn this context, the General Secretariat of the Belgian Data Protection Authority publishes \n\nthis information brochure to provide insights on data protection and the development and \n\nimplementation of AI systems. \n\nUnderstanding and adhering to the GDPR principles and provisions is crucial for ensuring \n\nthat AI systems operate ethically, responsibly, and in compliance with legal standards. This \n\ninformation brochure aims to elucidate the GDPR requirements specifically applicable to \n\nAI systems that process personal data , offering more clarity and useful insights to \n\nstakeholders involved in the development, implementation, and (internal) regulation of AI \n\ntechnologies. \n\nIn addition to the GDPR, the Artificial Intelligence Act (AI Act), which entered into force on \n\n1st of August 2024 , will also significantly impact the regulation of AI system development \n\nand use. This information brochure will also address the requirements of the AI Act. \n\nThe examples included in this brochure serve a purely pedagogical purpose, they are \n\nsometimes hypothetical and do not take into account certain exceptions iv and \n\nimperfections v in the regulation. (Original version â€“ version December 2024) \n\n5\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Audience for this information brochure \n\nThis information brochure is intended for a diverse audience comprising legal \n\nprofessionals, Data Protection Officers (DPOs), and individuals with technical backgrounds \n\nincluding business analysts, architects, and developers. It also targets controllers and \n\nprocessors involved in the development and deployment of AI systems. Given the \n\nintersection of legal and technical considerations inherent in the application of the G DPR \n\nto AI systems, this information brochure seeks to bridge the gap between legal \n\nrequirements and technical implementation. \n\nLegal professionals and DPOs play a crucial role in ensuring organizational compliance with \n\nGDPR obligations, specifically those relevant to AI systems that process personal data. By \n\nproviding insights into GDPR requirements specific to AI, this information brochure equips \n\nlegal professionals and DPOs with useful knowledge to navigate the complexities of AI -\n\nrelated data processing activities, assess risks, and implement appropriate measures. \n\nAt the same time, individuals with technical backgrounds such as business analysts, \n\narchitects, and developers are integral to the design, development, and deployment of AI \n\nsystems. Recognizing their pivotal role, this information brochure aims to elucidate GDPR \n\nrequirements in a manner accessible to technical stakeholders. \n\nConcrete examples are incorporated into the text to illustrate how GDPR principles \n\ntranslate into practical considerations during the lifecycle of AI projects. By offering \n\nrelatively simple and actionable insights, this information brochure empowers \n\nprofessionals with various backgrounds to design AI systems that are compliant with \n\nGDPR obligations , embed data protection -by -design principles, and mitigate potential legal \n\nand ethical risks. (Original version â€“ version December 2024) \n\n6\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# What is an AI system ? \n\nThe term \"AI system\" encompasses a wide range of interpretations. \n\nThis information brochure will not delve into the intricacies and nuances that distinguish \n\nthese various definitions. \n\nInstead, we will begin by examining the definition of an AI system as outlined in the AI Act vi :\n\nFor the purposes of this Regulation, the following definitions apply: \n\n(1) â€˜AI systemâ€™ means a machine -based system that is designed to operate with varying \n\nlevels of autonomy and that may exhibit adaptiveness after deployment, and that, for \n\nexplicit or implicit objectives, infers, from the input it receives, how to generate outputs \n\nsuch as predictions, content, recommendations, or decisions that can influence physical \n\nor virtual environments; \n\nIn other terms : \n\nAn AI system is a computer system specifically designed to analyze data, identify patterns, \n\nand use that knowledge to make informed decisions or predictions. \n\nIn some cases, AI systems can learn from data and adapt over time. This learning capability \n\nallows them to improve their performance, identify complex patterns across different data \n\nsets, and make more accurate or nuanced decisions. \n\nExamples of AI systems in everyday life: \n\nSpam filters in email : spam filters analyze incoming emails and identify patterns that \n\ndistinguish spam messages from legitimate emails. Over time, as people mark emails as \n\nspam or not spam, the AI system can learn and improve its filtering accuracy. This is an \n\nexample of an AI system that meets the criteria of an AI system :\n\nâ€¢ machine -based system: it's a computer program. \n\nâ€¢ analyzes data: it analyzes the content of emails. \n\nâ€¢ identifies patterns: it identifies patterns in emails that suggest spam. \n\nâ€¢ makes decisions: it decides whether to categorize an email as spam or not. \n\nRecommendation systems on streaming services : movie streaming services utilize AI \n\nsystems to generate recommendations for users. These systems analyze a user's past \n\nviewing habits, along with the habits of similar users, to recommend content they might be \n\ninterested in. This is another example of an AI system : (Original version â€“ version December 2024) \n\n7\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nâ€¢ machine -based system: it's a computer program. \n\nâ€¢ analyzes data: it analyzes a user's viewing/listening history. \n\nâ€¢ identifies patterns: it identifies patterns in user preferences and those of similar \n\nusers. \n\nâ€¢ makes recommendations: it recommends content based on the identified patterns. \n\nVirtual assistants : virtual assistants respond to voice commands and complete tasks like \n\nsetting alarms, playing music, or controlling smart home devices. These systems use \n\nspeech recognition and natural language processing to understand user requests and take \n\naction. This is again an example of an AI system :\n\nâ€¢ machine -based system: it's a computer program. \n\nâ€¢ analyzes data: it analyzes user voice commands. \n\nâ€¢ identifies patterns: it identifies patterns in speech to understand user requests. \n\nâ€¢ makes decisions: it decides how to respond to commands based on its \n\nunderstanding. \n\nâ€¢ may exhibit adaptiveness: some virtual assistants can learn user preferences and \n\nadapt their responses over time. \n\nAI -powered medical imaging analysis : many hospitals and healthcare providers are utilizing \n\nAI systems to assist doctors in analyzing medical images, such as X -rays, CT scans, and \n\nMRIs. These systems are trained on vast datasets of labeled medical images, allowing them \n\nto identify patterns and potential abnormalities. \n\nâ€¢ machine -based system: it's a computer program. \n\nâ€¢ analyzes data: it analyzes the digital medical images. \n\nâ€¢ identifies patterns: it identifies patterns in the images that might indicate the \n\npresence of a disease or abnormality. \n\nâ€¢ supports decision -making: the system highlights potential areas of concern in the \n\nimages, which can help doctors make more informed diagnoses. (Original version â€“ version December 2024) \n\n8\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# GDPR & AI Act requirements \n\n## Lawful, fair, and transparent processing \n\nThe GDPR requires lawfulness, fairness and transparency. \n\nLeveraging GDPR lawfulness of processing : The GDPR establishes six legal bases for \n\nprocessing personal data in Article 6 (consent, contract, legal obligation, vital interests, \n\npublic interest , and legitimate interest svii ). These same legal bases remain applicable for AI \n\nsystems that process personal data under the AI Act. \n\nProhibited AI Systems : t he AI Act introduces additional prohibitions beyond the GDPR for \n\ncertain high -risk AI systems. While the GDPR focuses on protecting personal data through \n\nvarious principles, the AI Act directly prohibits specific types of high -risk AI applications. \n\nHere ar e some examples: \n\nâ€¢ Social scoring systems: these systems assign a score to individuals based on \n\nvarious factors, potentially leading to discrimination and limitations on \n\nopportunities. \n\nâ€¢ AI systems for real time remote biometric identification for the purpose of law \n\nenforcement in public places (with limited exceptions) : these systems raise \n\nconcerns about privacy, freedom of movement, and potential misuse for mass \n\nsurveillance. \n\nFairness: \n\nâ€¢ While the AI Act doesn't have a dedicated section titled â€œfairnessâ€, it builds upon \n\nthe GDPR's principle of fair processing (art. 5.1.a) as the AI Act focuses on \n\nmitigating bias and discrimination in the development, deployment, and use of AI \n\nsystems. \n\nTransparency: \n\nâ€¢ the AI Act requires a baseline level of transparency for certain AI systems. This \n\nmeans users should be informed that they're interacting with an AI system. For \n\ninstance, a chatbot could begin an interaction with a message like \"Hello, I am \n\nNelson, a chatbot . How can I assist you today?\" \n\nâ€¢ the AI Act requires a higher transparency level for high -risk AI systems. This \n\nincludes providing clear and accessible information about how data is used in these \n\nsystems, particularly regarding the decision -making process. Users should \n\nunderstand the factors influencing AI -based decisions and how potential bias is \n\nmitigated. (Original version â€“ version December 2024) \n\n9\n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Purpose limitation and data minimi sation \n\nThe GDPR requires purpose limitation (art. 5.1.b) and data minimi sation (art. 5.1.c) . This \n\nmeans personal data must be collected for specific and legitimate purposes, and limited to \n\nwhat is necessary for those purposes. Th ese principles ensure that AI systems don't use \n\npersonal data for purposes beyond their intended function or collect excessive data .\n\nThe AI Act strengthens the principle of purpose limitation â€“ from the GDPR â€“ for high -risk \n\nAI systems by emphasizing the need for a well -defined and documented intended purpose. \n\nHypothetical e xample : A loan approval AI system of a financial institution , in addition to \n\nstandard identification data and credit bureau information, also utilizes geolocation data \n\n(e.g., past locations visited) and social media data (e.g., friends' profiles and interests) of a \n\ndata subject . This extensive data collection, including geolocation and social media data, \n\nraises concerns about the system's compliance with the GDPR. \n\n## Data accuracy and up-to -dateness \n\nThe GDPR requires personal data to be accurate and, where necessary, kept up -to -date \n\n(art. 5.1.d) . Organizations must take reasonable steps to ensure this. The AI Act builds upon \n\nthis principle by requiring high -risk AI systems to use high -quality and unbiased data to \n\nprevent discriminatory outcomes. \n\nHypothetical example : a financial institution develops a n AI system to automate loan \n\napprovals. The system analyzes various data points about loan applicants, including credit \n\nhistory, income, and demographics (postal code). However, t he training data for the AI \n\nsystem unknowingly reflects historical biases : the data stems from a period when loans \n\nwere more readily granted in wealthier neighborhoods ( with a higher average income) . The \n\nAI system perpetuates these biases as l oan applicants from lower -income neighborhoods \n\nmight be systematically denied loans, even if they are financially qualified. This results in a \n\ndiscriminatory outcome , and might raise serious concerns about the system's compliance \n\nwith the AI Act. \n\n## Storage limitation \n\nThe GDPR requires personal data to be stored only for as long as necessary to achieve the \n\npurposes for which it was collected (art. 5.1.e) . The AI Act doesn't explicitly introduce \n\nan other or an extra requirement on storage limitation for high -risk AI systems. (Original version â€“ version December 2024) \n\n10 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Automated decision -making \n\nThe GDPR and the AI Act both address the importance of human involvement in automated \n\ndecision -making processes that impact individuals. However, they differ in their focus: \n\nâ€¢ The GDPR grants individuals the right not to be subject solely to automated \n\nprocessing for decisions that produce legal effects concerning them ( art . 22). This \n\nmeans data subjects have the right to request a reconsideration of an automated \n\ndecision by a human decision -maker. This functions as an individual right to \n\nchallenge decisions perceived as unfair or inaccurate. \n\nâ€¢ The AI Act strengthens the focus on human involvement by requiring meaningful \n\nhuman oversight throughout the development, deployment, and use of high -risk AI \n\nsystems. This acts as a governance measure to ensure responsible AI development \n\nand use. Human ove rsight under the AI Act encompasses a broader range of \n\nactivities than just reconsideration of individual decisions. It includes, for example, \n\nreviewing the AI system's training data and algorithms for potential biases, \n\nmonitoring the system's performance, and intervening in critical decision -making \n\npathways. \n\nIn essence, the GDPR empowers individuals to object to solely automated decisions, while \n\nthe AI Act requires proactive human oversight for high -risk AI systems to safeguard \n\nagainst potential biases and ensure responsible development and use of such systems. \n\nHypothetical example : a government agency uses an AI system to assess eligibility for \n\nsocial welfare benefits based on income, employment status, and family situation .\n\nFollowing the GDPR, i ndividuals have the right not to be subject solely to automated \n\nprocessing for social welfare benefits eligibility ( art. 22). This means they can request a\n\nreconsideration of an automated decision by a human decision -maker .\n\nFollowing the AI Act, this AI system is classified as an high -risk system (as it has a \n\nsignificant impact on individuals' livelihoods). This requires the government agency to \n\nimplement human oversight throughout the development, deployment, and use of the AI \n\nsystem. (Original version â€“ version December 2024) \n\n11 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Security of Processing \n\nBoth the G DPR and the AI Act emphasize the importance of securing personal data \n\nthroughout its processing lifecycle. However, AI systems introduce specific risks that \n\nrequire additional security measures beyond traditional data protection practices. \n\nThe GDPR requires organizations to implement technical and organizational measures \n\n(TOMs) that are appropriate to the risk associated with their data processing activities. This \n\ninvolves conducting risk assessments to identify potential threats and vulnera bilities. The \n\nselected TOMs should mitigate these risks and ensure a baseline level of security for \n\npersonal data. \n\nThe AI Act builds upon this foundation by mandating robust security measures for high -\n\nrisk AI systems. This is because AI systems introduce specific risks that go beyond \n\ntraditional data processing, such as: \n\nâ€¢ potential bias in training data: biased training data can lead to biased decisions by \n\nthe AI system, impacting individuals unfairly. \n\nâ€¢ manipulation by unauthorized individuals: for example, a hacker could potentially \n\nmanipulate the AI system's training data to influence its decisions in a harmful way. \n\nImagine a system trained to approve loan applications being tricked into rejecting \n\nqualified applicants based on irrelevant factors. \n\nTo address these unique risks, the AI Act emphasizes proactive measures such as: \n\nâ€¢ identifying and planning for potential problems: This involves brainstorming what \n\ncould go wrong with the AI system and how likely it is to happen (risk assessment). \n\nThis is a core practice under both the GDPR and AI Act. \n\nâ€¢ continuous monitoring and testing: This involves regularly evaluating the AI \n\nsystem's performance for several aspects including: \n\no security flaws: identifying vulnerabilities in the system's code or design that \n\ncould be exploited by attackers. \n\no bias: checking for potential biases in the system's training data or decision -\n\nmaking processes. \n\nâ€¢ human oversight: the AI Act emphasizes the importance of meaningful human \n\noversight throughout the development, deployment, and use of high -risk AI \n\nsystems. This ensures that humans are involved in critical decisions and (Original version â€“ version December 2024) \n\n12 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nunderstand the system's vulnerabilities. Human oversight under the AI Act goes \n\nbeyond just security processes and encompasses various aspects, such as: \n\no reviewing training data and algorithms for potential biases. \n\no monitoring the system's performance for fairness, accuracy, and potential \n\nunintended behaviour .\n\no intervening in critical decision -making pathways, especially when they \n\ncould significantly impact individuals. \n\nExample : AI -powered Lung Cancer Diagnosis System .\n\nAn AI system used by a hospital to diagnose lung cancer exemplifies a high -risk AI system \n\ndue to several factors: \n\nâ€¢ highly sensitive data: it processes highly sensitive personal data, including patients' \n\nhealth information (lungs) and diagnoses (special category data under article 9 of \n\nthe GDPR) ;\n\nâ€¢ data breach impact: a data breach could expose critical health information about \n\npatients, potentially leading to privacy violations and reputational harm for the \n\nhospital ;\n\nâ€¢ life -altering decisions: the system's output directly impacts patients' lives. A \n\ndiagnosis based on inaccurate or compromised data could have serious \n\nconsequences for their health and well -being. \n\nBoth the GDPR and the AI Act emphasize the importance of security measures for data \n\nprocessing activities, especially those involving sensitive data. \n\nâ€¢ the GDPR establishes a foundation for data security: It requires organizations to \n\nimplement appropriate technical and organizational measures (TOMs) to protect \n\npersonal data based on a risk assessment. For health data, these measures would \n\nbe particularly strong due to its sensitive nature. Examples under the GDPR could \n\ninclude: \n\no data encryption: encrypting patient data at rest and in transit ensures its \n\nconfidentiality even if a breach occurs ;\n\no access controls: implementing strict access controls limits who can access \n\nand modify patient data ;\n\no penetration testing: regularly conducting penetration tests helps identify \n\nand address vulnerabilities in the system's security posture ;\n\no logging and auditing: maintaining detailed logs of system activity allows for \n\nmonitoring and investigation of any suspicious behavior. \n\nâ€¢ The AI Act builds upon this foundation for high -risk AI systems: recognizing the \n\nspecific risks of AI, the AI Act mandates robust security measures. These might (Original version â€“ version December 2024) \n\n13 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\ninclude additional measures tailored to the  specific vulnerabilities of the AI system, \n\nsuch as data validation and quality assurance : t he AI Act emphasizes the \n\nimportance of ensuring the quality and integrity of the data used to train and \n\noperate the AI system. This could involve techniques for: \n\no data provenance: tracking the origin of data to identify potential sources of \n\nbias or manipulation in the training data, such as incorrect X -ray labeling. \n\no anomaly detection: identifying and flagging unusual patterns in the training \n\ndata that might indicate malicious tampering, such as a sudden influx of X -\n\nrays with unrealistic characteristics. \n\no human review of high -risk data points: Having healthcare professionals \n\nreview critical X -rays before they are used to train the AI system, especially \n\nthose that show unusual features or could significantly impact patient \n\noutcomes. \n\nBy implementing these security measures the hospital can mitigate the risks associated \n\nwith the AI -powered lung cancer diagnosis system and ensure patient privacy, data \n\nsecurity, and ultimately, the best possible patient outcomes. \n\n## Data Subject Rights \n\nThe GDPR grants natural persons data subject rights, empowering them to control their \n\npersonal data and how it's used. These rights include access (seeing what data is \n\nprocessed, art. 15 ), rectification (correcting inaccurate data and completing data , art. 16 ), \n\nerasure (requesting data deletion , art. 17 ), restriction of processing (limiting how data is \n\nused , art. 18 ), and data portability (transferring data to another service , art. 20 ). \n\nTo effectively exercise these rights, natural persons need to understand how their data is \n\nbeing processed . The AI Act reinforces this by emphasizing the importance of clear \n\nexplanations about how data is used in certain AI systems. With this transparency, \n\nindividuals can make informed decisions about their data and utilize their data subject \n\nrights more effectively. \n\nExample : an AI system used to determine premiums for life insurance assigns a relatively \n\nhigh premium to a particular customer (data subject). The AI Act entitles this customer to \n\na clear explanation of how their premium is calculated. For example, the insurer (data \n\ncontroller) could explain that various data points were u sed, such as medical problems \n\ncustomers have faced in the past. This information, in turn, allows the customer to exercise \n\ntheir data subject rights under the GDPR , such as the right to rectification (correction of \n\ninaccurate personal data or completion of personal data ). (Original version â€“ version December 2024) \n\n14 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n## Accountability \n\nThe GDPR requires (organizations to demonstrate ) accountability for personal data \n\nprocessing through several measures , such as : \n\nâ€¢ Transparent processing: individuals must understand how their data is collected, \n\nused, stored and shared (f.e. by a clear and concise data protection statement , by \n\ndata subject access rights , â€¦) . This transparency allows them to see if their data is \n\nbeing handled lawfully and fairly ;\n\nâ€¢ Policies and procedures for handling personal data: documented policies ensure \n\nconsistent data handling practices across the organization ;\n\nâ€¢ Documented legal basis for processing : for each data processing activity, \n\norganizations need documented proof of the lawful justification (consent, contract, \n\nlegitimate interest, etc.) ;\n\nâ€¢ Keeping different records (like the Register Of Processing Activities ( ROPA ), data \n\nsubject requests, data breaches) is required: maintaining accurate records \n\ndemonstrates a commitment to accountability and allows organizations to prove \n\ncompliance during audits or investigations ;\n\nâ€¢ Security measures : implementing and correctly maintaining appropriate technical \n\nand organizational measures (TOMs) to protect personal data is crucial for \n\ndemonstrating accountability ;\n\nâ€¢ A Data Protection Impact Assessment ( DPIAs ) is required in some cases: these are \n\nmandatory when processing high -risk data or implementing new technologies ;\n\nâ€¢ A Data Protection Officer ( DPO ) is required in some cases: f.e. governmental \n\norganizations, regardless of their core activities, are required to have a DPO. \n\nWhile the AI Act doesn't have a dedicated section on demonstrating accountability, it \n\nbuilds upon the GDPR's principles. The AI Act requires organizations to implement : \n\nâ€¢ a two -step risk management approach for AI systems. First, there's an initial \n\nclassification process that categorizes the risk the AI poses to individuals (ranging \n\nfrom minimal to high). \n\nFor high -risk systems, a more in -depth risk assessment is required in some cases .\n\nThis dives deeper into the specific risks and identifies potential harms associated \n\nwith the AI system , and is also called a FRIA (Fundamental Rights Impact \n\nAssessment) ;(Original version â€“ version December 2024) \n\n15 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nâ€¢ clear documentation of the design and implementation of AI systems ;\n\nâ€¢ processes dealing with human oversight in high -risk AI systems. This could involve \n\nhuman intervention or approval for critical decisions made by the AI system ;\n\nâ€¢ a formal incident reporting process for reporting incidents related to AI system \n\nmalfunctions or unintended behaviour .(Original version â€“ version December 2024) \n\n16 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# Making compliance straightforward: user \n\n# stories for AI systems in light of GDPR and AI Act \n\n# requirements \n\nTranslating regulatory requirements into technical specifications for AI systems presents \n\nsignificant challenges. This document focuses on using user stories to bridge the gap \n\nbetween legal obligations and system development. \n\nUser stories offer a practical approach to understanding and addressing regulatory \n\nrequirements in the context of AI system design. By adopting a user -centric perspective, \n\norganizations can effectively translate legal obligations into actionable steps. \n\nThis document uses a life insurance premium calculation system as an example to illustrate \n\nthe application of user stories in the AI domain. \n\n## Requirements of lawful, fair, and transparent processing \n\nUser story : ensuring lawfulness â€“ correct legal basis \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremium s, we need to conduct a thorough legal basis assessment to determine the most \n\nappropriate legal justification for collecting and using customer data in our AI system. This \n\nis important to comply with the GDPR principle of lawfulness. \n\nUser story : ensuring lawfulness - prohibited data \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure our system complies with the GDPR and AI Act prohibitions \n\non processing certain types of personal data. This includes special categories of personal \n\ndata such as racial or ethnic origin, political opinions, religious beliefs, health, etc. This is \n\nimportant to comply with the GDPR's protection of sensitive personal data and the AI Act's \n\nemphasis on preventing discriminatory outcomes. \n\nUser story : ensuring fairness \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure fair and non -discriminatory processing of customer data. \n\nThis is important to comply with the GDPR principle of fairness and the specific AI Act's \n\nfocus on preventing biased outcomes that could disadvantage certain groups. (Original version â€“ version December 2024) \n\n17 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nThe life insurance company can achieve fairness by: \n\nâ€¢ data source review: analyze the data sources used to train the AI system to identify \n\nand mitigate potential biases based on factors like postal code, gender, age , â€¦ .\n\nEnsure these factors are used in a way that is relevant and necessary for premium \n\ncalculations , avoiding any discriminatory outcomes. \n\nâ€¢ fairness testing: regularly test the AI system for potential biases in its outputs. This \n\nmight involve comparing life insurance premium calculations for similar customer \n\nprofiles to identify any unexplainable disparities. \n\nâ€¢ human oversight: implement a human review process for high -impact decisions \n\nmade by the AI system, such as significant life insurance premium increases or \n\neven policy denials .\n\nUser story : ensuring transparency \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to be transparent about how our customers' data is used. This is \n\nimportant to comply with the general GDPR principle of transparency and the specific AI \n\nActâ€™s focus on transparency for high -risk AI systems. \n\nThe life insurance company can achieve transparency by : \n\nâ€¢ a data protection statement : clearly explain in the company's data protection \n\nstatement how customer data is collected, used, and stored in the AI system for \n\npremium calculations .\n\nâ€¢ easy -to -understand explanations : provide customer -friendly explanations of the \n\nAI premium calculations process. This could involve using simple language, visuals, \n\nor FAQs to demystify the AI's role in determining life insurance premiums. \n\nâ€¢ right to access information : implement mechanisms for customers to easily access \n\ninformation about the data points used in their specific premium calculations. \n\n## Requirements of purpose limitation and data minimization \n\nUser story : ensuring purpose limitation \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure that the data we collect from our customers is limited to what \n\nis strictly necessary for the accurate premium calculations . This is important to comply with \n\nthe principle of purpose limitation under the GDPR. (Original version â€“ version December 2024) \n\n18 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nUser story : ensuring data minimization \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to implement a data minimization strategy to ensure we only collect \n\nand use the minimum amount of customer data necessary for the accurate premium \n\ncalculations . This is important to comply with the principle of data minimization under the \n\nGDPR. \n\n## Requirements of data accuracy and up -to -dateness \n\nUser story : ensuring data accuracy and up -to -dateness \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to implement processes to ensure the accuracy and up -to -dateness of \n\ncustomer data used in the system. This is important to comply with the principle of data \n\naccuracy under the GDPR. \n\nThe life insurance company can achieve accuracy and up -to -dateness of customer data by: \n\nâ€¢ data verification mechanisms: offer customers easy -to -use mechanisms to verify \n\nand update their personal data within the life insurance system. This could be \n\nthrough an online portal, mobile app, or dedicated phone line. \n\nâ€¢ regular data refresh: establish procedures for regularly refreshing customer data \n\nused in the AI system. This might involve requesting customers to update their \n\ninformation periodically or integrating with external data sources to automatically \n\nupdate relevant data points. \n\nâ€¢ data quality alerts: implement alerts for missing or potentially inaccurate data \n\npoints in customer profiles. This allows the company to proactively reach out to \n\ncustomers and request updates. \n\nâ€¢ clearly communicate to customers their right to rectification under the GDPR. This \n\nright allows them to request corrections of any inaccurate personal data or \n\ncompletion of missing data used in the premium calculations system. \n\nUser story : ensuring use of unbiased data \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to ensure that the data used to train and operate the system is of free \n\nfrom bias. This is important to comply with the specific AI Act's focus on preventing biased \n\noutcomes that could disadvantage certain groups. (Original version â€“ version December 2024) \n\n19 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nThe life insurance company can achieve u nbiased data for fair AI premium calculations by : \n\nâ€¢ data source evaluation: Analyze the sources of data used to train the AI system. \n\nIdentify potential biases based on factors like socioeconomic background in the \n\ndata collection process. \n\nâ€¢ regular monitoring and bias testing: Continuously monitor the AI system's \n\nperformance for potential biases in its outputs. Conduct regular bias testing to \n\nidentify and address any discriminatory outcomes in premium calculations .\n\nâ€¢ human oversight: implement a human review process for high -impact decisions \n\nmade by the AI system, such as significant life insurance premium increases or \n\neven policy denials . This allows human intervention to prevent biased out comes. \n\nâ€¢ transparency with customers: Inform customers in the data protection statement \n\nabout the company's commitment to using high -quality, unbiased data in the AI \n\nsystem. \n\n## Requirement of secure processing \n\nUser story : implementing appropriate security measures for life insurance AI \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to conduct a thorough risk assessment to identify potential threats and \n\nvulnerabilities that could impact our customer data . This assessment will consider various \n\nfactors, including the type of data ( health data vs. basic customer information), processing \n\nactivities, and potential impact of a security breach. Based on this assessment, we will \n\nimplement appropriate technical and organizational measures (TOMs) to mitigate these \n\nrisks and ensure the security of our customer data. This is important to comply with the \n\nrequirement of security of the processing under the GDPR .\n\nExamples of TOMs may include: \n\nâ€¢ data encryption: encrypting customer data at rest and in transit to protect \n\nconfidentiality ;\n\nâ€¢ access controls: implementing strict access controls to limit who can access and \n\nmodify customer data ;\n\nâ€¢ regular penetration testing: conducting penetration tests to identify and address \n\nvulnerabilities in the system's security posture ;\n\nâ€¢ logging and auditing: maintaining detailed logs of system activity for monitoring \n\nand investigation of any suspicious behavior .(Original version â€“ version December 2024) \n\n20 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\nUser story : implementing specific security measures for life insurance AI \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we recognize that AI systems introduce specific risks beyond traditional data \n\nprocessing. These risks might include potential bias in training data or manipulation by \n\nunauthorized actors. To address these specific risks we will implement additional \n\nmeasures in conjunction with the baseline GDPR -compliant TOMs. This is important to \n\ncomply with the requirement of security of the processing under the AI Act .\n\nExamples of these additional measures may include: \n\nâ€¢ data validation and quality assurance: implementing processes to ensure the \n\nquality and integrity of the data used to train and operate the AI system. This could \n\ninvolve data provenance tracking and anomaly detection to identify potential \n\nbiases or manipulation attempts. \n\nâ€¢ human oversight: establishing a framework for human oversight throughout the AI \n\nsystem's lifecycle. This could involve human review of high -risk data points, \n\nmonitoring the system's performance for fairness and accuracy, and intervening in \n\ncritical decision -making pathways. \n\n## Requirement of (the ability of demonstrating) accountability \n\nUser story : documenting the legal basis \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to have a clear and concise record of the legal basis for collecting and \n\nusing customer data in the AI system. This is important to comply with the GDPR principle \n\nof (demonstrating) accountability (also in the context of audits or investigations). \n\nUser story : conducting a Fundamental Rights Impact Assessment (FRIA) \n\nAs a life insurance company implementing an AI system to calculate life insurance \n\npremiums , we need to develop and maintain a comprehensive FRIA (Fundamental Rights \n\nImpact Assessment) to proactively identify and mitigate potential risks associated with \n\nthis AI system. This is important to comply with the AI Act's requirements for high -risk AI \n\nsystems and promote fair and non -discriminatory premium calculations for our customers. \n\nThis obligation is in addition to the GDPR rules on data protection impact assessment. \n\n* * *(Original version â€“ version December 2024) \n\n21 \n\nArtificial Intelligence Systems and the GDPR \n\nA Data Protection Perspective \n\n# References  \n\n> 0\n\nThis paper also utilized spelling and grammar checking, and a large language model, as a \n\ntool for refining and correcting initial text section s.  \n\n> i\n\nRegulation (EU)2016/679 of the European Parliament and of the Council of 27 April 2016 \n\non the protection of natural persons with regard to the processing of personal data and on \n\nthe free movement of such data, and repealing Directive 95/46/EC (General Data \n\nProtection Regulation),  Official Journal of the European Union L 119/1, 4.5.2016, p. 1 â€“88. \n\nii  Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June \n\n2024 laying down harmonised rules on artificial intelligence  (Artificial Intelligence Act), \n\nOfficial Journal of the European Union L 199/1, 12.7.2024, p. 1 â€“120. \n\niii  Art. 20, Â§1, 1Â°, Data Protection Authority Act of 3 December 2017, amended by the Act of \n\n25 December 2023. \n\niv In the examples discussing high -risk AI systems listed in Annex III of the AI act, the \n\npossible exceptions referred to in section 6.3 of the AI act are not taken into account. \n\nv In the examples addressing life insurance, possible gaps in the legal basis (see decision \n\n109/2024 of the Litigation Chamber of the Belgian DPA) are not taken into account. \n\nvi  Artificial Intelligence Act, Article 3 (1) \n\nvii For more information on the legal basis â€˜legitimate interestâ€™, see the following opinion of \n\nthe European Data Protection Board: Opinion 28/2024 on certain data protection aspects \n\nrelated to the processing  of personal data in the context of AI models \n\n(https://www.edpb.europa.eu/news/news/2024/edpb -opinion -ai -models -gdpr -principles -\n\nsupport -responsible -ai_en )", "fetched_at_utc": "2026-02-08T19:07:18Z", "sha256": "8072688f58c2242ab3989cdf7d78db7dfec1eb82dd1953c8ab68f0227e59f3d3", "meta": {"file_name": "Artificial Intelligence Systems and the GDPR - Belgium.pdf", "file_size": 489694, "relative_path": "pdfs\\Artificial Intelligence Systems and the GDPR - Belgium.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 8308}}}
{"doc_id": "pdf-pdfs-bsi-eu-ai-act-whitepaper-final-2-9-24-aa10c02636c1", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\BSI_EU_AI_Act_Whitepaper_Final_2_9_24.pdf", "title": "BSI_EU_AI_Act_Whitepaper_Final_2_9_24", "text": "Artificial Intelligence Act What AI providers and  deployers need to know. @2024 BSI. All rights reserved. \n\nDisclaimer \n\nThe AI Act text used for the analysis is the text voted in the Plenary of the European Parliament on the 13th of Mar 2024  (P9_ TA(2024)0138 Artificial Intelligence Act). This paper is BSIâ€™s interpretation of the AI Act and is currently not legally binding. Artificial Intelligence Act (AI Act) What AI providers and deployers need to know. Authors: Alex Zaretsky *,Daniela Seneca *, Inma PÃ©rez *,Sarah Mathew *, Aris Tzavaras ** .* Regulatory Lead, Artificial Intelligence Notified Body, BSI group. ** Head of Artificial Intelligence Notified Body, BSI group. @2024 BSI. All rights reserved. We did not wake up to a world where Artificial Intelligence (AI) was just born. The genesis of AI as an idea is evident from ancient times in the form of myths. 1However, the term â€œAIâ€ more recently has been attributed to John McCarthy of the Massachusetts Institute of Technology (MIT) who first suggested the concept at a 1956 conference at Dartmouth College. The technological evolution permitted AI to become accessible , both in terms of computation power as well as in terms of the tooling and availability of digital data for facilitating development of AI systems. \n\nSince 1956, AI has shown significant progress \n\nin performing â€œnarrowâ€ tasks, in most cases, better than the average human and, in some, better than experts . A landmark victory of AIâ€™s progress became clear when the Deep Blue expert system played chess against the world champion Garry Kasparov in the 90s. 2  \n\n> 1Stanford researcher examines earliest concepts of artificial intelligence, robots in ancient myths 220 Years after Deep Blue: How AI Has Advanced Since Conquering Chess\n\nNow, why is there an increasing global concern to regulate and/or control AI? The short answer to this question is that we may lose to an opponent we created. Society cannot afford to leave AI \n\nunregulated as this could lead to the misuse of this technology. AI also needs vast amounts of data to become increasingly intelligent, and there is the risk that fundamental rights would be violated. For example, an algorithm that processes profiles to evaluate candidates for a job position, may be biased against people of a certain ethnicity, limiting exposure of their profiles for opportunities. These algorithmic biases have serious real-world implications. In this context, to prevent any form of manipulation or biased outcome, several regions are leaning towards AI regulation. The need for Artificial Intelligence legislation @2024 BSI. All rights reserved. \n\nLetâ€™s not forget about the geopolitics linked to regulating AI. The AI industry is growing at an extremely rapid pace and AI has become of strategic importance for governments across the world. Countries are competing to win the â€œAI raceâ€ and those able to successfully lead on AI innovation will be well positioned in global affairs. \n\nRegulating a technology sector is not something new. Typical examples are regulations bestowed on the pharmaceutical and medical industries, as well as on more abstract technologies like those processing our digital data. 3 Justifications behind market \n\nregulation includes market acceleration and harmonization as well as protecting consumers \n\nfrom the negative effects of such technologies. In the context of AI systems, harms may arise from the deployment of AI and its after effects. Therefore, such justifications can be used to limit AI. 4\n\nReasons to regulate AI may differ across regions. \n\nThe European Unionâ€™s (EU) approach, for instance, has been characterized by its focus on the protection of human rights â€“ or as it is called in Europe, fundamental rights. Those rights are enshrined in the Charter of Fundamental Rights of the EU and in other binding legislation such as the General Data Protection Regulation (GDPR). As technology is becoming an ever more central part of citizensâ€™ life, the EU understands that trust, is a prerequisite for AI uptake in Europe . We use, as consumers, products and services coming from â€œunfamiliar sourcesâ€ and we need to have the assurance that those products are safe, trustworthy, and ethical for us to consume. Regulations set the 3 â€˜Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard to the Processing of Personal Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC (General Data Protection Regulation)â€™, OJ L 119 (2016). 4 Chesterman, Simon, From Ethics to Law: Why, When, and How to Regulate AI (April 29, 2023). Forthcoming in The Handbook of the Ethics of AI edited by David J. Gunkel (Edward Elgar Publishing Ltd.), NUS Law Working Paper No. 2023/014 5 Smuha, Nathalie A., Beyond a Human Rights-based approach to AI Governance: Promise, Pitfalls, Plea (February 1, 2020). Published in Philosophy & Technology, 2020 6 Idem from footnote 4, p.5 7 High-Level Expert Group on AI, â€˜Ethics Guidelines for Trustworthy AIâ€™, 8 April 2019 8 European Parliament, â€˜EU AI Act: first regulation on artificial intelligenceâ€™ basic â€œrules of the gameâ€, ensuring consumers that unfamiliar sources deliver a product or service that obeys those rules. In most cases, legislation goes beyond the first entry of a product to the market, they additionally dictate the need for monitoring the product while in use and to deliver feedback to relevant authorities and action when something goes wrong; this is known as Post Market Surveillance (PMS). The use of AI comes with more sophisticated and nuanced challenges: some philosophical, some practical. Due to the increasing concerns about the adverse impact that AI systems may have on individuals, EU lawmakers have the \n\nchallenge of ensuring that AI is used for good. But what is â€˜â€™good AIâ€™â€™? This quest is relatively new, however, defining what is â€œgoodâ€ has been a long-standing question with different answers depending on the moral theory that you consider. 5 For this reason, since human rights are universally recognized, the EU decided to take a human-centric based approach to AI governance 6 and, in April 2019, the European Commission published its conceptualization of â€˜â€™good AIâ€™â€™: the Ethics Guidelines for Trustworthy AI. 7 The non-binding nature of these guidelines were criticized, however, in 2021, the EU Commission published the proposal for an Artificial Intelligence Act \n\n(AI Act) that largely codifies the ethics \n\nrequirements proposed by the High-Level Expert Group on AI in its Guidelines. 8 Since then, the final text has gained political agreement and has been voted by the EU Parliament in March 2024. @2024 BSI. All rights reserved. The AI Act is a â€œhorizontalâ€ legislation as it does not target a specific industry sector but rather any industry that uses AI. The AI Act sets requirements that products must comply with, as well as obligations for all parties involved (economic operators). The horizontal nature of this legislation is envisioned to â€œbuild onâ€ sectorial legislations, regulating only the AI aspects of those products. Furthermore, because the AI Act is technology agnostic, it does not prescribe specific rules for specific types of AI techniques, with the exemption of General-Purpose AI systems (GPAI). To determine if the upcoming legislation is applicable to oneâ€™s product, one must first define whether their product is or uses AI. Unfortunately, the definition of AI has been the â€œholy grailâ€ of the last decades, as there is no globally acceptable definition of â€œintelligence.â€ The above-mentioned â€œAI raceâ€ has forced governments as well as supranational and intergovernmental organizations to attempt to find a common definition of AI. 9 The OECD defines AI systems as follows: â€œAn AI system is a machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy.â€ See OECD, â€˜Recommendation of the Council on Artificial Intelligenceâ€™ 10  See Recital 12 of AI Act For instance in the AI Act, the EU ultimately suggested a definition aligned to the Organization for Economic Co-operation and Development (OECD)â€™s AI definition, 9 ensuring the text \n\nâ€œdistinguish[es] it from simpler traditional software systems or programming approaches and should not \n\ncover systems that are based on the rules defined solely \n\nby natural persons to automatically execute operations.â€ 10 The AI Act defines an AI system in Article 3 as: â€œa machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, \n\nrecommendations, or decisions that can influence \n\nphysical or virtual environments. â€@2024 BSI. All rights reserved. AI Act scope and territorial implications \n\nThere are two key actors caught under the scope of the AI Act which are providers 11 and deployers 12 of AI systems. Moreover, certain obligations have been introduced for importers, distributors, product manufacturers and authorized representatives of providers. The AI Act states 13 that the regulation applies to providers and deployers of AI systems established in the EU, as well as externally, to any provider and deployer of AI systems outside the EU, if the output of the AI system is used within the EU. What â€œoutputâ€ means is not defined under the AI Act, however, the definition of AI system refers to outputs in the form of content (generative AI systems) including text, video, or image 14 and \n\npredictions, recommendations, or decisions that can \n\ninfluence physical or virtual environments. 15 The key thing to ask here is whether the impact of the AI system occurs within the EU, regardless of where the provider and deployer is established. \n\n11  Article 3(3) of P9_TA(2024)0138 AI Act defines providers as â€œa natural or legal person, public authority, agency or other body that develops an AI system or a general purpose AI model or that has an AI system or a general purpose AI model developed and places them on the market or puts the system into service under its own name or trademark, whether for payment or free of chargeâ€ \n\n12  Article 3(4) of AI Act defines deployers as â€œany natural or legal person, public authority, agency or other body using an AI system under its authority except where the AI system is used in the course of a personal non-professional activityâ€ 13  See Article 2 of AI Act 14  Throughout the text we see that the AIA includes image, audio or video as example of AI-generated content. 15  See Article 3 and Recital 12 of AI Act 16  Bradford, Anu. (2020). The Brussels Effect: How the European Union Rules the World. Similar to the GDPR, one of the most important consequences of the AI Act extraterritorial scope is that it will impose significant obligations on non-EU businesses, even if they do not have a legal presence in the EU. The rationale behind this approach is linked to the EUâ€™s growing concern on how authoritarian governments use AI and its potential impact on the rights and freedoms of individuals. Consequently, the AI Act aims to level the playing field and make the AI Act applicable in a non-discriminatory manner. \n\nFurthermore, it is important to mention that the AI Act will potentially become another example of the phenomenon called the \n\nâ€œBrussels Effect,â€ a concept originally coined by Anu Bradford, 16 a professor at Columbia University. The â€œBrussels effectâ€ refers to â€œthe EUâ€™s unilateral power to regulate global markets.â€ @2024 BSI. All rights reserved. Some may argue that the upcoming AI Act Regulation will hamper AI innovation in Europe as it is more stringent in comparison to other more flexible countries that encourage self-regulation and voluntary commitments. However, for economic operators, it is more beneficial to adopt a uniform global standard rather than adhering to multiple, including laxer, regulatory standards, as this brings legal certainty. This would be the case for those organizations operating globally, who have multiple production locations where it is not legally, technically, or even economically viable, for the company to comply with multiple regulatory regimes. 17 This business approach would explain why so many large non-EU companies follow the GDPR and many other EU environmental regulations across their global operations. \n\nLetâ€™s not also forget that the EU was the first \n\nmover when it comes to regulating AI. It is true that due to the slow pace of the EU legislative process, there have been delays in the AI Act negotiations and other jurisdictions have adopted comprehensive regulation for parts of the AI ecosystem before the EU. 18 For example, China has \n\nbeen one of the first countries to implement AI \n\nregulations, including new rules on the use of recommendation algorithms. 19 Despite this, the AI Act has been the first draft to be published and this may be the reason why other jurisdictions have sped up their legislative processes. 17  Bradford, Anu, The Brussels Effect (2012). Northwestern University Law Review, Vol. 107, No. 1, 2012, Columbia Law and Economics Working Paper No. 533 18  Siegmann, Charlotte, Anderljung, Markus, The Brussels effect and Artificial Intelligence: How EU regulation will impact the global AI market (2022). Centre for the Governance of AI. P.39 19  See translation of the text here Translation: Internet Information Service Algorithmic Recommendation Management Provisions â€“ Effective March 1, 2022 (stanford.edu)   \n\n> 20 Siegmann, Charlotte, Anderljung, Markus, The Brussels effect and Artificial Intelligence: How EU regulation will impact the global AI market (2022). Centre for the Governance of AI. P.3 21 See interview to Bradford What is the Brussels Effect, and what does it mean for global regulation? (microsoft.com)\n\nTaking the success of the GDPR adoption as an example, the EU plans to promote its blueprint on AI globally. 20 It is fair to assume that the AI Actâ€™s human-centered approach, strong focus on ethics, transparency and fundamental rights, will serve as an inspiration to like-minded countries. \n\nEspecially after seeing over 100 countries today with GDPR-like data privacy rules. 21 @2024 BSI. All rights reserved. To sum up, the AI Act will have an extra-territorial impact on AI providers and deployers in non-EU jurisdictions, if their AI systems and/or outputs are used within the EU. Moreover, its existing extraterritorial scope has the potential to become the gold standard when it comes to AI governance. Experience has proved that European values have a broad appeal, and it is fair to assume that the AI Act will be globally widespread. Companies operating globally may be glad to follow only one set of rules, even if they are more stringent. However, the extraterritorial scope of the AI Act might raise compliance challenges, especially to those AI providers established in third countries if they are not aware that the output of their AI system will be used in the EU. Therefore, all relevant operators need to understand which role they play along the AI value chain and properly determine the scope of their AI systems to see if they fall within the scope of the AI Act. @2024 BSI. All rights reserved. As we have seen in this whitepaper, the AI Actâ€™s \n\ndefinition of AI is close to the one proposed by \n\nthe OECD and this seems to be an advantage as it maintains a semantic alignment with international partners. The EU believes that this definition gives a clear criterion for differentiating AI systems from traditional software, thus ensuring a proportionate regulatory approach. However, this definition has \n\ndrawn much criticism for still remaining too broad . In any case, it is important to understand that not all AI technologies defined as an AI system under the AI Act will be subject to obligations, however one must consider the degree of risk they pose to the health, safety, and fundamental rights of individuals.   \n\n> 22 European Commission, White Paper on Artificial Intelligence - A European approach to excellence and trust, COM(2020) 65 final, 2020. P. 17 23 See Article 5 of AI Act\n\nThe EU believes that a risk-based approach is impor-tant to help ensure that the regulatory intervention is proportionate. 22 To that end, the AI Act distin-guishes between AI systems posing (i) unacceptable risk, (ii) high risk, (iii) limited risk, and (iv) low or minimal risk. \n\nThe Commission judges the level of risk by the likelihood that the system may harm the health \n\nand safety of specific individuals, and/or \n\npotentially violate their fundamental rights. The obligations imposed on such systems range from prohibitions to the voluntary codes of conduct. The AI Act proposes prohibitions on AI applications that pose â€œunacceptable risksâ€ to peopleâ€™s safety, health and rights. 23 \n\n# AI products falling under the AI Act @2024 BSI. All rights reserved. The AI Act considers these practices to be harmful and abusive and should be prohibited because they contradict Union values. 24 Accordingly, these systems would be prohibited to be placed on the market, put into service, or used in the EU: 1 AI systems that deploy harmful manipulative â€œsubliminal techniques.â€ 25 \n\nâ€¢ Example: â€œAn inaudible sound is played in truck driversâ€™ cabins to push them to drive longer than \n\nhealthy and safe. AI is used to find the frequency maximising this effect on drivers.â€ 26 2 AI systems that exploit specific vulnerable \n\ngroups (due to their age, physical or mental disabilities). 27 \n\nâ€¢ Example: â€œA doll with an integrated voice assistant encourages a minor to engage in progressively dangerous behaviour or challenges in the guise of a fun or cool game.â€ 28 3 AI systems used for social scoring purposes for public and private purposes- in particular, to classify the reliability of people based on their social behaviour or personality traits. 29 \n\nâ€¢ Example: â€œAn AI system identifies at-risk children in need of social care based on insignificant or irrele -vant social â€˜misbehaviourâ€™ of parents, e.g. missing a doctorâ€™s appointment or divorce.â€ 30 4 Biometric categorization of natural persons based on biometric data to deduce or infer their race, political opinions, trade union, membership, religious or philosophical beliefs, sex life or sexual orientation. 31 \n\nâ€¢ Example: â€œAI systems that infer â€˜criminalityâ€™ based on data about peopleâ€™s facial structure or biological characteristics, for example, the colour of the skin.â€            \n\n> 24 See recital 28 of AI Act 25 See Article 5(1)(a) of AI Act 26 For the sake of clarity, the Commission has presented some examples of the above prohibitions. Some argue that these are borderline fantastical, however, being AI such an innovative technology, who knows where it will take us. See https://cor.europa.eu/en/events/Documents/SEDEC/FINAL%20PDF%20AI%20Presentatiofor%20COR%20Sedec%20Committee%20 meeting%2023%2006%2021.pdf 27 See Article 5(1)(b) of AI Act 28 See https://cor.europa.eu/en/events/Documents/SEDEC/FINAL%20PDF%20AI%20Presentatiofor%20COR%20Sedec%20Committee%20meeting%2023%2006%2021.pdf 29 See Article 5(1)(c) of AI Act 30 See https://cor.europa.eu/en/events/Documents/SEDEC/FINAL%20PDF%20AI%20 Presentatiofor%20COR%20Sedec%20Committee%20meeting%2023%2006%2021.pdf 31 See Article 5(1)(g) of AI Act 32 See Article 5(1)(h) of AI Act 33 See Article 5(1)(d) of AI Act 34 See Recital 42 of AI Act\n\n5 Real-time remote biometric identification \n\nin publicly accessible spaces by law enforce-ment. 32 \n\nâ€¢ Example: â€œAll faces captured live in a public space by video cameras checked, in real time, against a database to identify a criminal in the crowd.â€ \n\n6 Individual predictive policing; except for law enforcement if based on objective and \n\nverifiable facts. 33 \n\nâ€¢ Example: â€œAI-predicted behaviour based solely on \n\ntheir profiling, personality traits or characteristics, \n\nsuch as nationality, place of birth, place of residence, number of children, debt, their type of car, without a reasonable suspicion of that person being involved \n\nin a criminal activity based on objective verifiable \n\nfacts and without human assessment thereof.â€ 34 @2024 BSI. All rights reserved. 7 Emotion recognition in the workplace and education institutions, unless for medical or safety reasons (i.e. AI systems used in detecting the state of fatigue of professional pilots or drivers for the purpose of preventing accidents). 35 \n\nâ€¢ Example : â€AI recruitment tools that assess a candidateâ€™s emotional state or truthfulness through analysis of facial expressions, voice modulation, or body language during interviews.â€ \n\n8 AI systems using indiscriminate scraping of biometric data from the internet or CCTV footage to create facial recognition data-bases. 36 \n\nâ€¢ Example: â€œAI system that collects facial images \n\nfrom social media without any specific targeting or \n\nconsent, amassing a vast database of faces.â€ \n\nHowever, it is important to mention that the use of real-time remote biometric identification in point 5 above has some exceptions related to the safety of society as a whole. In particular, the use of real-time remote biometric identification systems (such as facial recognition) in public spaces for law enforce-ment purposes will be allowed when the use of such systems can be justified by â€œthree exhaustively listed and narrowly defined situations.â€ These narrowly 35  See Article 5(1)(f) of AI Act 36  See Article 5(1)(e) of AI Act 37  See Article 5(1)(h)(i) of AI Act 38  See Article 5(1)(h)(ii) of AI Act 39  The list of the 16 crimes in Annex II of AI Act: Terrorism; Trafficking in human beings; Sexual exploitation of children and child sexual abuse material; Illicit trafficking in narcotic drugs and psychotropic substances; Illicit trafficking in weapons, munitions and explosives; Murder; Grievous bodily injury; Illicit trade in human organs and tissue; Illicit trafficking in nuclear or radioactive materials; Kidnapping, illegal restraint and hostage-taking; Crimes within the jurisdiction of the International Criminal Court; Unlawful seizure of aircraft/ships; Rape; Environmental crime; Organised or armed robbery; Sabotage, participation in a criminal organisation involved in one or more crimes listed above.   \n\n> 40 See Article 5(1)(h)(iii) of AI Act 41 See Article 26 (10) of AI Act\n\ndefined exceptions cover a rather broad range of situations: â€¢ a â€œtargeted search for specific victims of abduction, trafficking in human beings or sexual exploitation \n\nof human beings, as well as searching for missing persons.â€ 37 â€¢ the â€œprevention of a specific, substantial and \n\nimminent threat to the life or physical safety of natural persons or (...) of a terrorist attack.â€ 38 â€¢ the â€œlocalisation or identificationâ€ of a person suspected of having committed a crime 39 with a maximum sentence of at least 4 years that would allow for the issuing of a European Arrest Warrant. 40 \n\nTherefore, if the above is fulfilled, real-time biome-tric identification by law enforcement authorities may be permitted, if it is accompanied by safeguards for fundamental rights, including the ex-ante involvement of judicial authorities and prior funda-mental rights impact assessment (unless in duly justified situations of urgency). \n\nThese safeguards will also be mandatory for the usage of AI systems for post remote biometric \n\nidentification of persons under investigation. 41 An example of this would be the use of biometric surveillance to analyse footage during a protest to identify an individual that has committed a crime. \n\nIt is important to state that the above prohibition does not ban actors from using remote biometric \n\nidentification for non-law enforcement purposes .This means that private entities may use such systems (e.g. marketplaces, public transport and even schools) if they go through a third-party conformity assessment or comply with harmonized European standards that are to be published later on. @2024 BSI. All rights reserved. Moving on to the next category under the AI Act, â€œhigh-risk AI systemsâ€ are those systems that create adverse impact on peopleâ€™s health and safety or their fundamental rights in a number of defined applications, products and sectors. This is the main focus of the regulation. \n\nBefore going into more detail, it is important to clarify that AI systems can be used on a stand-alone basis or as a component of a product ,irrespective of whether the system is physically integrated into the product (embedded) or serve the functionality of the product without being integrated therein (non-embedded). 42 \n\nThe high-risk regime is based on the intended purpose of the AI system, in line with the New Legislative Framework (NLF), a common EU approach to the regulation of certain products such as machinery, lifts, medical devices, personal protec-tive equipment and toys. 43 The AI Act distinguishes between two categories of high-risk AI systems: 1 AI systems that are products or safety compo-nents of products covered by certain Union health and safety harmonization legislation (such as toys, machinery, lifts, or medical devices) and are required to undergo a third party conformity assessment. 44 \n\n2 Stand-alone AI systems deployed in eight \n\nspecific areas: 45 \n\na.  biometric identification, categorization and emotion recognition (outside prohibited categories); \n\nExample: AI systems used for facial recognition. \n\nb.  management and operation of critical infra-structure; \n\nExample: AI systems used in road traffic, the \n\nsupply of water, gas, heating, and electricity.          \n\n> 42 See Recital 12 of AI Act 43 See New legislative framework - European Commission (europa.eu)\n> 44 See Annex I of AI Act with the list of NLF legislation. Annex I (B) is older-style product safety legislation where Title XII introduces new AI Actâ€“related considerations for future delegated acts in those areas. 45 See the list in AI Act 46 See recital 33 of AI Act 47 See Articles 7 and AI Act 48 See Article 6 (3) of AI Act 49 See Article 6 (3) of AI Act 50 See Article 6 (3) of AI Act\n\nc.  educational and vocational training; \n\nExample: AI systems used in evaluating students on \n\ntests required for university admission. \n\nd.  employment, worker management and access to self-employment; \n\nExample: AI systems used to place targeted \n\njob advertisements, to analyse and filter job \n\napplications, and to evaluate candidates. \n\ne.  access to and enjoyment of essential services and benefits; \n\nExample: AI systems used for creditworthiness evaluation of natural persons. \n\nf.  law enforcement; \n\nExample: AI systems used for detection, \n\ninvestigation, or prosecution of criminal offenses. \n\ng.  migration, asylum and border management; \n\nExample: AI systems that identify a person who, \n\nduring an identity check, either refuses to be identified \n\nor is unable to state or prove his or her identity .46 \n\nh.  administration of justice and democracy; \n\nExample: AI systems aimed at helping analyse and interpret facts regarding judicial authority. \n\nThe above list of high-risk AI systems may be updated over time as the EU Commission may modify or add additional use cases if they pose similar risks to the uses currently on the list. However, it can remove areas if they do no longer pose a significant risk to health, safety and funda-mental rights. 47 \n\nA stand-alone AI system can be classified as high-risk if it falls under any of the eight specific \n\nareas. However, as always, the AI Act has excep-tions. If the system does not pose significant harm to the health, safety, or fundamental rights of people, including not materially influencing the outcome of decision-making, it may be exempt. This would be the case of the following systems: 48 â€¢ Intended to perform a narrow procedural task. 49 â€¢ Intended to improve the result of a previously completed human activity. 50 @2024 BSI. All rights reserved. â€¢ Intended to detect decision-making patterns or deviations from prior decision-making patterns. 51 â€¢ Intended to perform a task that is only prepara-tory to an assessment relevant for the purpose of the high-risk use cases in Annex III. 52 It is important to note that an AI system will always be considered at a minimum high-risk if the AI system performs profiling of natural persons. 53 The next type of AI system risk level is â€œlimited riskâ€. \n\nThese are systems that have special disclosure obligations due to their particular interaction with humans given that they may pose the risk of manipulation . These include AI systems that generate or manipulate image, audio or video content (i.e. deep fakes 54 ), AI systems that are intended to interact with people (e.g. chatbots), and AI-powered emotion recognition systems and biometric categorization systems. With this inclusion, the AI Act ensures that EU customers make informed decisions as they are aware that they are interacting with a machine. 55 Finally, the last category is â€œminimal riskâ€. These AI \n\nsystems do not fit in any of the other categories \n\nand present only low or minimal risk. They can be developed and used within the EU without conforming to any additional legal requirements. However, the AI Act envisages the creation of codes of conduct to encourage providers of non high-risk AI systems to voluntarily apply the mandatory requirements for high-risk AI systems. These codes of conduct should be based on clear objectives and key performance indicators to measure the achievement of those objectives. 56 This could include elements around inclusiveness, fairness, transparency, confidentiality, and environmental sustainability. 51  See Article 6 (3) of AI Act 52  See Article 6 (3) of AI Act 53  See Article 6 (3) of AI Act 54  See Article 3(6) of of AI Act 55  See Article 50 of AI Act 56  See Recital 165 of AI Act 57  See Article 95 of AI Act The Commission and Member States will encourage the creation and voluntary compliance with these codes. 57 It is important to underline that existing EU law, such as the GDPR, still applies when the use of AI systems falls within the scope of that law, no matter if classi-fied as no risk under the AI Act. @2024 BSI. All rights reserved. AI systems falling under other EU legislation. Given the horizontal nature of the AI Act, all AI systems across sectors are subject to the same risk-assessment criteria and legal requirements .The AI Act interrelates with other EU legal instru-ments, for example rules on data protection, privacy, civil liability or sectorial law such as the Machinery Regulation or the Medical Devices Regulation (MDR). This horizontal approach prevents companies from â€œshopping aroundâ€ between sectors and thus ensuring that all players conform to the same legal requirements. This approach might seem preferable as it is uniform, stable, and fair across industries. However, it is important to say that, if this approach is not properly addressed, it may lead to conflicting obligations and procedures for AI providers. \n\nThe AI Act draws on the New Legal Framework (NLF) regime, 58 designed to improve the EU internal market , and increase the quality of conformity assessment of certain products such as medical devices, machinery, or toys. As described by Veale and Zuiderveen Borgesius, 59 â€œunder NLF \n\n58  New legislative framework, European Commission 59  Veale, Michael and Zuiderveen Borgesius, Frederik, Demystifying the Draft EU Artificial Intelligence Act (July 31, 2021). Computer Law Review International (2021) 22(4) 97-112, P.6 60  Union harmonization legislation refers to Union legislation that harmonizes the conditions for the marketing of product. This list can be found here and also in Annex II of AI Act 61  See Recital 64 of P9_TA(2024)0138 Artificial Intelligence Act 62  Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (MDR). \n\nregimes, a manufacturer must undertake pre-mar-keting controls undertaken to establish productsâ€™ safety and performance, through conformity assessment to \n\ncertain essential requirements laid out in law. Manufac -turers then mark conforming products with â€œCEâ€; marked products enjoy EU freedom of movement.â€ \n\nThe AI Act acknowledges that a single AI system may be affected by different Union Harmonization legislation. 60 61 For example, a medical device product incorporating AI might present risks not addressed by the Medical Devices Regulation (MDR). 62 This calls for a simultaneous and complementary application of several EU laws. @2024 BSI. All rights reserved. The NLF legal acts are built on the legal concept that \n\nwhenever a matter is regulated by two rules, the \n\nmore specific one should be applied first. 63 With this, it is ensured that products incorporating AI are not subject to a double regulatory burden. This was the intention of the Commission when it proposed the AI Act: \n\nâ€œTo achieve those objectives, this proposal presents a balanced and proportionate horizontal regulatory approach to AI that is limited to the minimum necessary \n\nrequirements to address the risks and problems linked \n\nto AI, without unduly constraining or hindering technological development or otherwise disproportion-ately increasing the cost of placing AI solutions on the market.â€ 64 \n\nIf we take AI-enabled medical devices as an example, the AI Act tries to ensure consistency , avoid duplications, and minimize additional burdens associated with the cumulative application of the AI Act and MDR. It allows AI providers to integrate the necessary measures to comply with the AI Act into the procedures and documents already required under MDR. 65 In practice, this means that the AI-enabled medical device manufacturer would be allowed to integrate the testing and reporting processes, information and documentation required under the AI Act into the already existing documentation and procedures required under the MDR. This is because the MDR is considered the more specific rule and, therefore, will take precedence. However, as previously mentioned, there are certain tensions and inconsistencies that make it difficult to apply the AI Act in conjunction with other Union harmonization laws. For example, as seen in the previous section, the AI Act categorizes as high-risk AI systems those that are products or safety components of products already covered by Union harmonization legislation. For AI-enabled medical devices both, the AI Act and MDR, would be applicable to the same product. The problem here is that there is no 63  Commission notice The â€˜Blue Guideâ€™ on the implementation of EU product rules 2022 (Text with EEA relevance) 2022/C 247/01 C/2022/3637, p. 11 64  Explanatory Memorandum AI Act, p. 3 65  See Recital 64 of AI Act 66  Article 3(14) of AI Act defines safety component, however, we do not see a similar definition under the MDR. 67  Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (GDPR) definition of â€œsafety componentâ€ under the MDR and it is not clear for a medical device what a â€˜safety componentâ€™ is. 66 Additionally, the AI Act interplays not only with Union Harmonization law, but also with other hori-zontal legislation, such as the GDPR. 67 The AI Act makes several references to the GDPR throughout the text and assures that it is without prejudice and complements the GDPR. Accordingly, both regulations apply side by side. In practice, this means that all AI systems must strictly adhere to the GDPR if they use personal data belonging to EU citizens, or plan to be deployed for usage within the EU. However, again there are some tensions when it comes to an AI system processing personal data. @2024 BSI. All rights reserved. For example, the AI Act states that it does not provide legal grounds for processing personal data \n\nand refers back to the GDPR to find a justifiable \n\nground for this processing. 68 However, it does not give clarity on how to apply the GDPRâ€™s requirements for collecting and processing personal data. In this context, it is difficult to find a legal ground for the processing of personal data by Large Language Models (LLMs) as there is no controller/data subject relationship (e.g. contract), nor can the data subject expect their data to be used as training data for an app, and there is no possibility for the data subject to object to such processing (e.g. no explicit consent). In some cases, AI can handle personal data based on the justified grounds of â€œlegitimate interest.â€ 69 Never-theless, this needs to be balanced to ensure that the data subjectâ€™s rights are not compromised. It is expected that the Commission will issue guidelines clarifying how to train AI models \n\nwithout violating personal data protection rules, 68  See Recital 63 of AI Act 69  See Article 6(1)(f) Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (GDPR) 70  Regulation (EU) 2019/881 on ENISA and on information and communications technology cybersecurity certification and repealing Regulation (EU) No 526/2013 (Cybersecurity Act). 71  See Art 42(2) of AI Act 72  Established in 2004 and strengthened by the EU Cybersecurity Act, the European Union Agency for Cybersecurity, ENISA contributes to EU cyber policy, enhances the trustworthiness of ICT products, services and processes with cybersecurity certification schemes. 73  ENISA: AI Cybersecurity Challenges - Threat Landscape for Artificial Intelligence. Artificial Intelligence Cybersecurity Challenges â€” ENISA (europa.eu) 74  See Article 40 of AI Act 75  See Article 41 of AI Act 76  Regulation 2022/0272 (CRA). 77  See Article 8 of Regulation 2022/0272 (CRA). 78  Essential cybersecurity requirements are in Article 10 and Annex I of Regulation 2022/0272 (CRA). including the Data Act, Data Governance Act and the Copyright Directive. Regarding the GDPR, legal grounds for the processing of personal data might require a significant rethink for AI systems. \n\nOn the cybersecurity side, the AI Act also overlaps with the EU Cybersecurity Act 70 , including a presumption of conformity in Article 42(2) of the AI Act. The clause acknowledges that high-risk AI systems that have been certified under a cybersecu-rity scheme created according to the process provided by the Cybersecurity Act â€œshall be presumed to be in \n\ncompliance with the cybersecurity requirements set out in Article 15 of this Regulation.â€ 71 Article 15 and recital 49 of the AI Act state that high-risk AI systems should perform consistently throughout their lifecycle and meet an appropriate level of accuracy, robustness and cybersecurity in accordance with the generally acknowledged state of the art. It is fair to say that Article 15 of the AI Act is very general and does not cover the entirety of potential cyberthreats to AI-powered systems such as those identified by ENISA 72 in its â€œArtificial intelligence and \n\nCybersecurity Challengesâ€ report. 73 Therefore, if AI developers want to ensure the highest level of cybersecurity for their AI system, they will need to either rely on the available cybersecurity schemes or, most likely, apply the harmonized standards 74 or common specifications defined by the Commission, 75 which are not available at this time. On another note, but still within the EU cybersecurity framework, the AI Act will also interplay with the new Cyber Resilience Act (CRA). 76 Like the AI Act, this regulation is expected to enter into force in 2024. In the CRA, 77 we find a presumption of conformity with the AI Act. It states that products with digital elements classified as high-risk AI systems under the AI Act should comply with the essential cybersecurity requirements set out under the CRA. 78 @2024 BSI. All rights reserved. If those high-risk AI systems fulfil the CRAâ€™s cybersecurity essential requirements, they should be deemed compliant with the cybersecurity requirements set out in Article 15 of the AI Act, as long as those requirements are covered by the EU declaration of conformity issued under the CRA. 79 Moreover, the CRA clearly states that the AI Act is the reference act and that the AI Actâ€™s conformity assessment procedures are the ones to be followed . In addition, the CRA clarifies that AI Notified Bodies under the AI Act can also control the conformity of high-risk AI systems with the CRA essential requirements. 80 However, there is an exception to this: if a high-risk AI system also falls under the CRAâ€™s scope as a â€˜critical product with digital elementsâ€™ and to which internal control of the AI Act applies, then the conformity assessment to follow is the one under the CRA insofar as the essential cybersecurity requirements are concerned. The other aspects of the product can still follow the AI Actâ€™s internal control procedure. The reason behind this is that â€˜critical products with digital elementsâ€™ create greater cybersecurity risks and, 79  See Recital 77 of AI Act 80  See Article 8(2) of Regulation 2022/0272 (CRA). therefore, the conformity assessment should always involve a third-party conformity assessment body. \n\nFinally, something important to mention is that high-risk AI providers also need to comply with accessibility requirements , including the EU directives 2016/2102 and 2019/882. The AI Act intends to ensure equal access to technology for all persons with disabilities. Therefore, AI providers will need to ensure compliance with these requirements by design. All the above points have shown that, before placing in the EU market or putting into service a high-risk AI system, AI providers will need to consider multiple horizontal and sector-specific laws if they want to guarantee a holistic compliance of their products to EU law. Despite the contradictions and overlaps between the AI Act and other horizontal and sectorial laws, it is expected that the Commission will perform an in-depth gap analysis where it will provide clarification about the relationship between those laws. @2024 BSI. All rights reserved. For AI systems falling under the high-risk classifica-tion, there are stringent requirements. The legal act \n\ndoes not specify how to fulfill its requirements at \n\ntechnical level â€“ therefore, the AI Act will be supported by a series of technical specifications produced by European Standardization Organiza-tions (ESOs) 81 following a mandate by the Commis-sion. The standards will translate the AI Actâ€™s requirements into actionable steps. Although these standards are not mandatory, AI providers that follow harmonized standards adopted by CEN/ CENELEC will benefit from the â€œpresumption of conformityâ€ with the AI Act. There are still a greater number of AI-specific standards under development. 81  ESO are the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC) and the European Telecommunications Standards Institute (ETSI). 82  See Articles 50(2), 96 & Recitals 116, 121 of AI Act The AI Act lists requirements for high-risk AI systems in Chapter III, Section 2 (articles 9 to 15). \n\nCompliance with the requirements should take into account the AI systemâ€™s intended purpose and what is generally acknowledged as State of the Art (SotA). Table 1 provides a high level summary of Chapter III requirements. SotA is not a well-defined term, neither in the AI Act nor under other relevant NLF legislations. However, we can find multiple references to the SotA in the AI Act â€“ harmonized standards, common specifications, technical standards, and codes of practice. 82 Requirements & Obligations @2024 BSI. All rights reserved. Table 1: High-risk AI systems requirements 83 Requirement Summary \n\nRisk management (RM) system The RM system planned and run throughout the entire lifecycle shall comprise of: \n\nâ€¢ Identification and analysis of the known and foreseeable risks to health, safety, or fundamental rights. â€¢ Evaluation of risks, including the analysis of data gathered from the post-market monitoring system. â€¢ Adoption of appropriate and targeted RM measures. Most appropriate RM measures shall reduce risks as far as technically feasible, with a view to minimizing risks effectively so that each residual risk as well as the overall residual risk are acceptable, considering the context and the deployerâ€™s technical knowledge, experience, education, and training. Specific considerations are made for vulnerable persons and those under the age of 18. \n\nData and data governance Data governance practices shall concern: \n\nâ€¢ Design choices. â€¢ Data collection processes and the original purpose of data collection. â€¢ Data-preparation processing operations such as annotation, labelling, cleaning, updating, enrichment, and aggregation. â€¢ Relevant assumptions on information that the data are supposed to measure. â€¢ Prior assessment of the availability, quantity, and suitability of the needed datasets. â€¢ Biases affecting health and safety or leading to discrimination. â€¢ Being free of errors and complete, to the best extent possible. \n\nTraining, validation and testing data sets shall: \n\nâ€¢ To the best extent possible free of errors and complete, and having the appropriate statistical properties at the level of individual data sets (or a combination thereof), including in regards to the persons or groups on which the system is intended to be used. â€¢ Take into account the intended purpose, the characteristics, or the elements that are particular to the specific geographical, behavioral, or functional setting within which the system is intended to be used. 83  See chapter III, Section 2 of AI Act @2024 BSI. All rights reserved. Requirement Summary \n\nTechnical documentation Technical documentation shall: \n\nâ€¢ Be drawn up before placing on the market or put into service and shall be kept up-to date. â€¢ Provide national competent authorities and notified bodies with all the necessary information in a clear and compre-hensive form to assess the compliance of the AI system with requirements. â€¢ Contain, at a minimum, the elements set out in Annex IV (amendable by Commissionâ€™s delegated acts). In the case of small and medium-sized enterprises (SMEs), including start-ups, any equivalent documentation should meet the same objectives, unless deemed inappropriate by the competent authority. Where a high-risk AI system is placed on the market or put into service, one single technical document shall be drawn up containing all the information required under those legal acts listed in Annex I. Annex IV describes the required content of the technical documentation. \n\nRecord-keeping AI systems shall technically allow for the automatic recording of events (â€˜logsâ€™) over their lifecycle. Logging capabilities shall enable: \n\nâ€¢ The recording of events relevant for identification of situations that may result in risks to health or safety or fundamental rights of persons. â€¢ Post-market monitoring. â€¢ Monitoring of the operations. â€¢ Recording of each use period of the system. â€¢ The reference database against which input data has been checked by the system. â€¢ The input data for which the search has led to a match. â€¢ The identification of the natural persons involved in the verifi-cation of results. \n\nTransparency and provision of information to deployers \n\nAI systems shall be designed and developed to ensure sufficiently transparent operation, achieving compliance with the relevant obligations, and enabling deployers to understand and use the system appropriately. It shall be accompanied by instructions for use (IFU) in an appropriate digital format or otherwise including concise, complete, correct, and clear information, which is relevant, accessible, and comprehensible to deployers. @2024 BSI. All rights reserved. Requirement Summary \n\nHuman oversight AI systems shall be designed to be effectively overseen by natural persons, aiming at preventing or minimizing the risks to health, safety, or fundamental rights. Human oversight shall be ensured through â€œpreâ€ built-in measures (by the provider) and â€œpostâ€ measures (identified by the provider, but implemented by the user), and be enabled to understand capacities and limitations, automation bias, the systemâ€™s output, as well a the ability to override or reverse the output and interrupt the system. For remote biometric identification systems, two natural persons separate verification and confirmation is required. \n\nAccuracy, robustness and cybersecurity AI systems shall be designed and developed to achieve an appropriate level of accuracy, robustness, and cybersecurity, and to perform consistently in those respects throughout their lifecycle. Levels of accuracy and accuracy metrics shall be declared in instructions for use. High-risk AI systems shall be resilient as regards errors, faults, or inconsistencies due to their interaction with natural persons or other systems. The robustness may be achieved through technical redundancy solutions (backup or fail-safe plans). Learning AI systems shall be developed to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (â€˜feedback loopsâ€™). Appropriate cybersecurity solutions to address AI specific vulnerabilities shall include measures to prevent and control for attacks trying to manipulate the training dataset (â€˜data poisoningâ€™), inputs designed to cause the model to make a mistake (â€˜adversarial examplesâ€™), or model flaws. The AI Act goes further than other NLF legislations, as it details obligations not only for economic operators (e.g., providers, importers, distributors), but also obligations for deployers, providers, and deployers of certain AI systems (Chapter IV), providers of GPAI models (Chapter V, Section 2) and GPAI models with systemic risk (Chapter V, Section 3). These obligations are detailed in Table 2. @2024 BSI. All rights reserved. Table 2: AI Act obligations Obligations per economic operators Summary \n\nObligations of providers of high-risk AI systems 84 Providers shall ensure compliance with AI Act requirements, indicate in the AI system the provider information, comply with Article 17 (QMS), keep Article 18 documentation available for 10 years, keep automatic generated logs for at least six months, follow appropriate conformity assessment procedures, comply with registration obligations (Article 49), affix CE and draw up an EU declaration of conformity, ensure compliance with accessibility requirements, and investigate and inform non-conformities and corrective actions to appropriate stakeholders. \n\nAuthorized representatives of providers of high-risk AI systems 85 \n\nEuropean Union providers shall mandate authorized representatives for specific tasks - namely, an EU declaration of conformity and technical documentation verification, keeping the prementioned plus the issued certificate and providing contact details for 10 years, provide national competent authorities with documentation and access to logs, cooperating to reduce/mitigate risks, and where applicable complying with registration obligations. \n\nObligations of importers 86 Importers shall ensure their systems are in conformity with the AI Act, verifying that a conformity assessment according to Article 43 has been carried out, technical documentation, CE marking, EU declaration and IFUs are in place, and that the authorized representative is assigned. Packaging or documentations should indicate the importerâ€™s details. Importers should cooperate with national competent authorities and keep the relevant documentation for 10 years. \n\nObligations of distributors 87 Distributors shall verify the required CE marking, EU declaration of conformity and instruction of use, and that provider and importer have complied with their obligations. They shall inform providers or importers of risks to the health or safety or to fundamental rights of persons, take the corrective actions necessary to bring system into conformity or ensure provider, importer or, operator do, and inform national competent authorities of the non-compliance or any corrective actions taken. Distributors shall cooperate and provide national competent authorities, upon reasoned request, information and documentation. 84  See Article 16 of AI Act 85  See Article 22 of AI Act 86  See Article 23 of AI Act 87  See Article 24 of AI Act @2024 BSI. All rights reserved. Obligations per economic operators Summary \n\nObligations of deployers of high-risk AI systems 88 Deployers shall use the system in accordance with the instructions of use, assign human oversight to natural persons with the necessary competence, training, and authority, and ensure input data is relevant and sufficiently representative. They shall inform the provider or distributor in case of risks or serious incidents and interrupt the use of the system, fulfill rules on internal governance arrangements, processes, and mechanisms pursuant to the relevant financial service legislation in case of financial institutions, and keep logs for at least six months. They are required to comply with the registration obligations, carry out data protection impact assessment (GDPR) and cooperate with national competent authorities. \n\nTransparency obligations for providers and deployers of certain AI systems 89 \n\nAI systems intended to interact directly with natural persons, should be designed to inform interaction with AI, unless this is obvious. Obligations are not applicable to AI systems authorized by law to detect, prevent, investigate, or prosecute criminal offences. AI systems, generating synthetic audio, image, video, or text content shall ensure outputs are marked as AI generated or manipulated. Deployers of an emotion recognition or biometric categorization system, excluding systems permitted by law to detect, prevent, or investigate criminal offences shall inform the natural person on the exposure to the system. \n\nObligations for providers of general-purpose AI models 90 Providers of GPAI models, other than those which are free & open license, should draw and maintain technical documentation according to Annex XI, and draw up information/documentation to be provided to other AI system providers intending to integrate the GPAI model into their system. \n\nObligations for providers of general-purpose AI models with systemic risk 91 \n\nIn addition to GPAI providers obligations, systemic risk GPAI providers need to perform model evaluation including adversarial testing, assessing & mitigating systemic risks at the EU level, reporting appropriate incidents, and ensurng cybersecurity protection of the model and the physical infrastructure. Any distributor, importer, deployer or other third-party that makes a substantial modification of a high-risk AI system OR changes the intended purpose of a non-high risk AI turning it into a high risk one, it will be considered the provider and will be subject to the AI Act providersâ€™ obligations. 92 88  See Article 26 of AI Act 89  See Article 50 of AI Act 90  See Article 53 of AI Act 91  See Article 55 of AI Act 92  See Article 25 of AI Act @2024 BSI. All rights reserved. The AI Actâ€™s regulatory framework introduces a structured ecosystem of entities entrusted with the assessment, certification, and oversight of AI systems. This framework aims to harmonize approaches, ensuring the safe and ethical deployment of AI systems. To unpack this regulatory landscape, we explain and clarifying the definitions of the involved stakeholders: \n\n# AI Act ecosystem @2024 BSI. All rights reserved. National Level \n\nConformity Assessment Body (CAB): 93 A separate legal entity that performs third-party conformity assessment activities including testing, certification, and inspection. The primary objective of a CAB is to ascertain that AI systems meet requirements of the relevant applicable standards. \n\nNotified Body (NB): 94 A specialized form of CABs, NBs undergo formal notification in accordance with the EU AI Act and relevant EU harmonization legislation. Their tasks include conducting conformity assessment activities for high-risk AI systems, adhering to organizational, quality management, resource, process, and cybersecurity requirements. NBs maintain independence from evaluated AI system providers, ensuring impartiality and confidentiality. Articles 31 to 34 of the EU AI Act delineate specific obligations and operational criteria for NBs. \n\nNotifying Authority (NA): 95 Designated within each Member State, NAs manage the procedural framework for assessing, designating, and notifying CABs, alongside ongoing supervision. Operating under a mandate to prevent conflicts of interest, NAs uphold principles of objectivity and impartiality. They are structured to separate decision-making from assessment activi-ties, explicitly prohibiting any commercial or competitive offerings. NAs ensure their personnel are highly qualified in relevant fields, including information technologies, artificial intelligence, and law. \n\nMarket Surveillance Authority (MSA): 96 Designated as the national authority responsible for overseeing market activities to ensure compliance with legal requirements, particularly for high-risk AI systems. They enforce the regulation by monitoring, identifying non-compliance, and oversight of the corrective actions implemented by the AI providers to protect public inter-ests, health, safety, and fundamental rights. \n\nNational Competent Authority (NCA): 97 It includes the NA and the MSA. It represents the authoritative entities designated by EU member states to oversee the regulation and compliance of AI systems within their jurisdictions, focusing on ensuring the safety, security, and rights compliance of AI technologies. EU Level \n\nArtificial Intelligence Office (AIO): 98 An office within the European Commission tasked with monitoring and supervising AI systems, general-purpose AI models, and AI governance. The AIO plays a central role in fostering a coherent regulatory framework for AI across the EU, ensuring compliance with legislative mandates, facilitating enforcement, and overseeing AI governance to safeguard public interest and uphold standards. \n\nEuropean Artificial Intelligence Board (AIB): 99 An advisory and coordinating body established to support the consistent and effective application of the AI Act across the EU. It functions to enhance cooperation among national competent author-ities tasked with the Regulationâ€™s enforcement, share technical and regulatory expertise, and promote best practices among Member States. 93  See Article 3 (21) of AI Act 94  See Article 3 (22) of AI Act 95  See Article 3 (19) of AI Act 96  See Article 3 (26) of AI Act 97  See Article 3 (48) of AI Act 98  See Article 3 (47) of AI Act 99  See Article 65 of AI Act @2024 BSI. All rights reserved. The AI Act lists the responsibilities of \n\nNotified Bodies (NBs): 100 01. Conformity Assessment 101 :\n\nâ€¢ NBs, or CABs acting on their behalf, impartially evaluate the quality management system (AI management system) 102 implemented by the provider of a high-risk AI system and the tech-nical documentation 103 , submitted by that provider. These assessments aim to verify compliance with the AI Actâ€™s harmonized stand-ards and common specifications, focusing on applicable requirements. Each assessment is completed with an audit report 104 detailing the outcomes, identifying areas of compliance, and highlighting non-compliant areas (so-called â€œnon-conformitiesâ€) with the AI Actâ€™s regulatory framework. â€¢ NBs are granted necessary and relevant access to the training, validation, and testing datasets utilized by AI systems, potentially through application programming interfaces (APIs) or other mechanisms facilitating remote access, underpinned by adequate security safeguards. NBs are also granted access to the training and trained models of the AI system, including relevant parameters (e.g., weights, architecture), after alternative conformity verification methods and finding them inadequate. NBs must conduct direct testing if they find the tests provided by the high-risk AI system provider inadequate. 105 \n\n02. Issuance of Certificates: 106 \n\nâ€¢ Upon successful conformity assessment, NBs issue certificates to AI system providers, signi-fying compliance with the AI Act and associated requirements. 100  See Articles 31, 34 of AI Act 101  See Recitals 50,78,86, 123, 125 and Articles 45, 46,57 Section 7 of AI Act 102  See Recital 173, Article 43 and Annex VII, Section 5 of AI Act 103  See Recitals 66,173, Articles 11, 43 Annex VII, Section 4.3 of AI Act 104  See Annex VII, Section 5.3 of AI Act 105  See Annex VII, Sections 4.3-4.5 of AI Act 106  See Article 44 and Annex VII of AI Act 107  See Annex VII, Section 5 of AI Act 108  See Annex VII, Sections 3.4 and 4.7 of AI Act \n\n03. Continuous Monitoring and Surveillance: 107 \n\nâ€¢ Following a successful conformity assessment, NBs undertake ongoing surveillance of EU conformity certificates issued to AI system providers. This surveillance focuses on ensuring continual compliance with the AI Act and includes regular audits of quality management system to verify continuous compliance with applicable regulatory and technical requirements (harmonized standards, common specifications, and industry practices, in the absence of harmo-nized standards and common specifications). â€¢ NBs must be informed by providers of high-risk AI systems of any proposed changes to the AI management system or AI system itself 108 that could impact compliance (and/or intended purpose) and NBs review the relevant documentation to approve changes where compliance is maintained. @2024 BSI. All rights reserved. 04. Cooperation among Notified Bodies: \n\nâ€¢ NBs communicate with other NBs regarding any refusals, suspensions, or withdrawals of quality management system or EU technical documentation approvals. Additionally, upon request, the NB must provide information regarding quality system approvals it has issued. 109 \n\n05. Documentation and Reporting: \n\nâ€¢ NBs maintain records and associated evidence of all assessments, decisions, and certifications, which are made accessible to NCAs for oversight purposes. 110 \n\nThe key responsibilities of NCAs 111 within the regulatory framework under the AI Act entail diverse roles essential for ensuring effective governance and oversight of the national-level implementation of the AI Act: \n\n01. Regulatory Oversight: \n\nâ€¢ NCAs can request and review documentation from providers of non-high risk AI systems and general-purpose AI models (GPAI), ensuring transparency and compliance with AI Act regula-tory requirements. 112 NCAs also request technical documentation from providers of high-risks AI systems (when the NB is not involved). 113 â€¢ NCAs may support the provision of high-quality data for AI system training, which aims to support innovations, as well as the transparency and reliability of the data framework. 114 109  See Article 45, of AI Act 110  See Articles 45 AI Act 111  See Article 28 of AI Act 112  See Article 6, Section 6 and Article 53, Section 1 AI Act 113  See Article 11, Section 1 AI Act 114  See Recital 68 of AI Act 115  See Article 70 of AI Act 116  See Article 30 of AI Act 117  See Article 57 of AI Act 118  See Article 70 of AI Act â€¢ NCAs serve as primary points of contact, facili-tating communication, regulatory, and technical compliance. 115 â€¢ NCAs support the European Commission and exchange information between NCAs. 116 \n\n02. Support for Innovations: \n\nâ€¢ NCAs establish national-level AI regulatory sandboxes to foster innovation and enable testing of AI systems and inform the AIO about the progress. Collaboration between NCAs, the AIO, and other relevant authorities facilitates knowledge exchange and best practices dissemi-nation. 117 â€¢ NCAs can offer SMEs and startups guidance on the AI Act, aligned with the Board and Commis-sion position. For AI systems under other EU laws, relevant authorities must be consulted. 118 @2024 BSI. All rights reserved. 03. Enforcement Actions: 119 \n\nâ€¢ NCAs (specifically MSA) are responsible for oversight of corrective actions implemented by the AI model providers to address incidents with involved AI systems, issuing warnings, imposing fines, and mandating compliance measures to uphold regulatory integrity. NCAs also act as the single point of contact. \n\n04. Designation, Monitoring, and Coordination of \n\nNotified Bodies (NBs): 120 \n\nâ€¢ NCAs designate NBs and ensure their qualifications comply with the AI Actâ€™s requirements for conducting conformity assessments. NCAs monitor and audit NBs, ensuring their compliance with the AI Actâ€™s requirements, which include the maintenance of competence, impartiality, and other legal, regulatory, and technical requirements applicable to the NBs 121. \n\nThe essential duties of the AIO within the regulatory framework established by the AI Act \n\nand its official website 122 are outlined as follows: \n\n01. Central Coordination, Governance, and Oversight of GPAI: \n\nâ€¢ The AIO serves as the central coordinating body among EU member states, providing guidance and support for consistent AI Act implementa-tion. It facilitates best practice exchange and ensures alignment across the EU. The AIO leverages its expertise in establishing EU-level advisory bodies (such as the Artificial Intelligence Board) fostering collaboration to ensure coherent AI Act application across all Member States. 123 119  See Recital 153, Article 89 of AI Act 120  See Article 38 of AI Act 121  See Article 28 of AI Act 122  See https://digital-strategy.ec.europa.eu/en/policies/ai-office 123  See Recital 148 of AI Act 124  See Articles 88, 91, 92 of the EU AI Act â€¢ The AIO supervises GPAI (requirements for providers, authorized representatives, deployers) to ensure compliance with the AI Act, standards, and associated requirements. It develops tools, methodologies, and benchmarks for evaluating the capabilities and reach of GPAI models, classifying models with systemic risks. The AIO also provides oversight of corrective actions taken by GPAI provider in case of non-compliance. The AIO has the power to request documentation and information from the GPAI provider and its authorized representatives, conduct evaluation of GPAI-models and request measures (including to restrict making GPAI available on the market, or to withdraw or recall the model). It also monitors fulfilment of obligations by the providers of GPAI. 124 @2024 BSI. All rights reserved. â€¢ The AIO provides coordination support for joint investigations in case of market surveillance \n\nwith involvement of specific categories of \n\nhigh-risk AI system(s), supervises and monitors the compliance of GPAI, as well as taking necessary action(s) to monitor effective implementation and continuing compliance for providers of GPAI models with the AI Act. 125 \n\n02. Policy Development: \n\nâ€¢ The AIO provides standardized templates for the areas required by the AI Act (e.g., summary of content used for training of the GPAI, summary of the content used codes of practice, questionnaire for deployers). 126 â€¢ The AIO advises on best practices, facilitates access to AI testing environments, and promotes innovative ecosystems to boost EU competitiveness. 127 â€¢ The AIO ensures the regulatory framework adapts to technological advancements and societal needs by engaging with diverse stakeholders, including AI developers, SMEs, and experts. It fosters continuous dialogue to inform policy formulation and develop codes of practice aligned with the AI Act. 128 \n\n03. Cooperation with stakeholders: \n\nâ€¢ The AIO collaborates with a diverse range of stakeholders, such as NCAs, providers of \n\nGPAI, scientific panels (including independent experts), institutions, the European Artificial Intelligence Board, and the European Centre for Algorithmic Transparency (ECAT), to gather and share technical and regulatory expertise, including knowledge gathered from the establishment, running, and oversight of AI sandbox. 129 125  See Recitals 112,114, 160, 161, 162, 164 and Article 75 of AI Act 126  See Recitals 107,108 and Articles 25, 27, 50, 56, 95 of AI Act 127  See Recitals 116, 117, 179 of AI Act 128  See Recitals 113, 151 and Article 90 of AI Act 129  See Recital 111, 116, 163 and Articles 57, 68 of AI Act 130  See Articles 65 and 66 of AI Act 131  See Article 74, Section 11 of AI Act \n\nThe European Artificial Intelligence Board 130 plays a  major role in robust oversight and the application of the AI Act by the Commission and Member states. It achieves this through several key activities: 1 Facilitating coordination among NCAs responsible for enforcing the AI Act and endorsing joint market surveillance activities. 131 2 Maintaining technical expertise and best practices across EU Member States, as well as contributing to a cohesive understanding and application of the AI Act. 3 Delivering strategic advice on the regulationâ€™s enforcement, focusing on GPAI models and aiming to standardize approaches and interpretations. 4 Emphasizing the importance of consistent conformity assessments, the effective use of regulatory sandboxes, and the value of testing AI systems in real-world scenarios. 5 Playing a critical role in advising on the regulationâ€™s implementation and offering recommendations on various fronts including codes of conduct, the evolution of AI standards, and the integration of emerging technologies. 6 Engaging in broad educational efforts to boost AI literacy and fostering the publicâ€™s awareness of AIâ€™s benefits and risks, while facilitating cross-sectoral and international cooperation to enhance the regulationâ€™s global relevance and effectiveness. @2024 BSI. All rights reserved. The development process of the AI Act has been characterized by significant complexity and anticipa-tion over the past several months, as this period was fraught by a series of strategic discussions, uncer-tainties, and widespread speculations regarding the outcome. Nevertheless, the legislative journey succeeded with the final political agreement and the adoption of the Act on 13th March 2024. This result indicates a procedural timeline which is anticipated to take approximately up to two months for the formal publication of the legislation in the Official Journal of the EU. The legislationâ€™s entry into force, the significant milestone for the new AI Act, will be twenty days after its publication in the Official Journal. To operate within the timelines in practical perspective, we analysed associated key milestones and relevant actions to be completed. The results of this analysis are presented in Table 3 .\n\n# AI Act timelines @2024 BSI. All rights reserved. Table 3 â€“ Key timelines of the AI Act implementation Timeline Relevant Action 13 March 2024 â€¢ Adoption on the EU Artificial Intelligence Act in the European Parliamentâ€™s plenary. \n\n22â€“25 April 2024 â€¢ Approval of the AI Act corrigenda in the plenary of the parliament Entry into force 132 \n\n1 August 2024 \n\nâ€¢ 20 days after publication in the Official Journal of the EU. Entry into force + Three months 133 \n\n2 November 2024 \n\nâ€¢ Member States must list, publish, and keep up to date list of public authorities and/or bodies. Entry into force + Six months 134 \n\n2 February 2025 \n\nâ€¢ Date of application for prohibited AI systems to be available on the market (Chapter II) â€¢ Date of application of general provisions (Chapter I) Entry into force + Nine months 135 \n\n2 May 2025 \n\nâ€¢ Readiness of codes of practices to be published by the AIO. Entry into force + 12 months \n\n2 August 2025 \n\nâ€¢ Chapter III (High-Risk AI Systems) Section 4, Chapter V (GPAI Models), Chapter VII, and Chapter XII, except Article 101. 136 â€¢ If the AI Office finds the code of practice insufficient or unfinalized 12 months after Entry into Force, the Commission, via implementing acts, may establish common rules for obligations in Articles 53 and 55, aligning with the examination process of Article 98(2). 137 â€¢ Member states to have implemented rules on penalties, including administrative fines. 138 â€¢ Readiness of NBs and governance structure, including conformity assessments. 139 â€¢ If no code of practice is finalized within 12 months after entry into force, or if deemed inadequate by the AIO, the Commission may issue implementing acts for Articles 53 and 55 obligations, following Article 98(2)â€™s examination procedure. 140 â€¢ Member States must inform the Commission of NAs and MSAs, including their tasks, and provide publicly accessible information on how NCAs (in a form of single point contact) can be contacted. 141 â€¢ The Commission will provide guidance for high-risk AI system providers to comply with obligations of reporting of serious incidents to MSAs. 142 â€¢ Member States must inform the Commission about their NCAâ€™s financial and human resources, assessing their sufficiency, with this being repeated every two years afterwards. The Commission will share this data with the AI Board for analysis and potential advice. 143 â€¢ Following the entry into force and until the delegated powers in Article 97 expire, the Commission is tasked with annually evaluating the necessity for updates to Annex IIIâ€™s list and Article 5â€™s catalogue of banned AI practices. The outcomes of these assessments will be systematically presented to both the European Parliament and the Council. 144 132  See Article 113 of AI Act 133  See Article 77 of AI Act 134  See Article 113 of AI Act 135  See Recital 179 and Article 56, Section 9 of AI Act 136  See Recital 179 and Article 113 of AI Act 137  See Article 56, Section 9 of AI Act 138  See Recital 179 of AI Act 139  See Recital 179 of AI Act 140  See Article 56 of AI Act 141  See Article 70, 59 of AI Act 142  See Article 73 of AI Act 143  See Article 70 of AI Act 144  See Article 112 of AI Act @2024 BSI. All rights reserved. Timeline Relevant Action \n\nEntry into force + 18 months \n\n2 February 2026 \n\nâ€¢ The Commission, as well as the consulting AI Board, are to provide guidelines for the AI Actâ€™s entry, including examples of high/non-high risk AI use cases as required by Article 96. 145 Entry into force + 24 months \n\n2 August 2026 \n\nâ€¢ Application of the AI Act, which includes obligations on high-risk AI systems specifically listed in An-nex III, inlcuding AI systems in biometrics, critical infrastructure, education, employment, access to essential public services, law enforcement, immigration, and the administration of justice to comply with the AI Act. It applies when significant design changes occur from that timeframe, aligning with Article 5 under Article 113(3)(a). 146 â€¢ Member States are mandated to create at least one national AI regulatory sandbox. Implementation of this requirement can be a collaborative effort among different Member States. The EU Commis-sion offers support for sandbox development and operation. States may also join existing sandboxes, provided they ensure equivalent national coverage. 147 Entry into force + 36 months \n\n2 August 2027 \n\nâ€¢ Applicability of Article 6(1) (classification rules for Annex I Union harmonization legislation) and corre-lated obligations of the AI Act. 148 â€¢ GPAI model providers with products placed on the EU market 12 months prior to the AI Actâ€™s entry into force are required to undertake essential measures to meet the relevant obligations of GPAI model providers. 149 Entry into force + 48 months 150 \n\n2 August 2028 \n\nâ€¢ Every 48 months starting from the AI Actâ€™s entry into force, the Commission will analyse and inform the European Parliament and Council on potential amendments to Annex III, Article 50â€™s AI system transparency requirements, and improvements to supervisory and governance frameworks. â€¢ Every 48 months from the AI Actâ€™s entry into force, the Commission will report its evaluation to the Parliament and Council, focusing on enforcement structure and the potential for an EU agency to address gaps. Amendments may be proposed based on these insights. All reports shall be made publicly available. â€¢ The Commission will assess the AI Officeâ€™s performance, examining if it possesses adequate powers and competences for its duties, and if needed enhancing its role and enforcement capabilities, along with increasing its resources. The evaluation report will be submitted to the European Parliament and Council. â€¢ Every 48 months from the AI Actâ€™s entry into force, the Commission reports on the advancement of standardisation in energy-efficient development of general-purpose models. This includes assessing the necessity for additional measures, which are potentially binding. The final evaluation report will be submitted to the European Parliament and the Council and made publicly available. â€¢ The Commission is mandated to assess the influence and efficacy of voluntary codes of conduct every three years afterwards. These evaluations are aimed to optimize adoption of Chapter II, Section 2â€™s requirements for providers of AI systems not classified as high-risk and to explore the potential for integrating additional mandates, notably concerning environmental sustainability. 151 Entry into force + 60 months 152 \n\n2 August 2029 \n\nâ€¢ The Commission is tasked with conducting a review of the AI Act, at intervals of four years and to report the findings to the European Parliament and the Council. An annual assessment is to be provided by the Commission to evaluate potential revisions to the lists of high-risk AI systems and prohibited practices. Entry into force + 72 months \n\n2 August 2030 \n\nâ€¢ Providers and deployers of high-risk AI systems designated for public authority use shall comply with AI Actâ€™s requirements. 153 145  See Article 6 of AI Act 146  See Articles 111 and 113 of AI Act 147  See Article 57 of AI Act 148  See Article 113 of AI Act 149  See Article 111 of AI Act 150  See Article 112 of AI Act 151  See Recital 174 of AI Act 152  See Recital 174 of AI Act 153  See Article 111 of AI Act @2024 BSI. All rights reserved. Timeline Relevant Action 31 December 2030 â€¢ AI systems integrated within large-scale IT frameworks, as specified in Annex X, and operational 36 months before the AI Actâ€™s entry into force must comply with the AI Act. Evaluations of these large-scale IT systems, mandated by the legal acts in Annex X, will incorporate the AI Actâ€™s requirements, especially when these acts undergo revisions or updates. 154 Entry into force + 84 months 155 \n\n2 August 2031 \n\nâ€¢ The Commission is required to execute an assessment of its enforcement. This analysis will be re-ported to the European Parliament, the Council, and the European Economic and Social Committee, reflecting on the initial years of the AI Actâ€™s application. Based on findings, if and when necessary, the final report shall be accompanied by a proposal for AI Actâ€™s amendment regarding the structure of enforcement and changes needed to be implemented by the EU agency to resolve any identified negative findings. 154  See Article 111 of AI Act 155  See Article 112, Section 13 of AI Act @2024 BSI. All rights reserved. The AI Act introduces the term â€œsubstantial modificationâ€, referring to a change to an AI system already placed on the market or put into service which is not foreseen in the initial conformity, and may affect compliance or modify its intended purpose. 156 The AI Act also introduces the term â€œpredetermined changesâ€, a term used to \n\ndescribe predefined changes subject to an initial \n\nconformity assessment of AI systems which are not static but continue to learn or evolve following placement on the market. 157 For high-risk AI systems that have been placed on the market prior to the application of the AI Act, the AI Act applies only if following the AI Actâ€™s application date there are significant changes in the design or intended purpose. Furthermore, the AI Act makes no distinction for the prementioned purpose between the terms â€œsignificant changeâ€ and â€œsubstantial modificationsâ€. 158 156  See Article 3(23) of AI Act 157  See Recital 128 and Article 43(4) of AI Act 158  See Recital 177 of AI Act 159  See Recital 84 of AI Act The AI Act makes an exemption for AI systems which are components of the large-scale IT systems and high-risk AI systems intended to be used by public authorities â€“ compliance of those systems with the AI Act requirements is required by end of 2030, or by six years after the entry into force. \n\nHowever, it is not immediately clear when a high-risk AI system falls under another EU legislation â€“ as in the case of Annex I section A products â€“ which legislation prevails in \n\nterms of substantial modifications. The answer to this question can be found in the recitals of the AI Act. 159 If a change is not consid-ered significant under a more specific EU legislation (e.g., Regulation 2017/745 MDR), then the change should not trigger substantial modification under the relevant clauses of the AI Act. \n\n# AI products already on the market @2024 BSI. All rights reserved. The AI Act brings scrutiny also to sectors that were not previously subject to regulation. Due to its horizontal nature and levels of risk, the AI Act has different obligations for AI providers and deployers to ensure conformity of AI systems. The principal mechanisms of compliance within the AI Act are: \n\nThe use of a quality management system (QMS): 160 Although the recently published standard ISO/IEC 42001:2023 Information Technology Artificial Intelligence Management System (AIMS) is not yet harmonized with the AI Act, it is expected to be the reference standard for conformity with the relevant requirements. The AIMS should cover a strategy for compliance, processes on design, development, data governance, testing and validation of AI systems, risk management, post-market monitoring, and incident reporting. Providers and deployers of high-risk AI systems are obliged to use AIMS, with this also being an obligation for AI providers that are required to follow conformity routes stated in Annex VI (internal control) and Annex VII (assessment of QMS and Technical Documentation). When AI systems are subject to obligations for a QMS under other sectorial EU legislations, AIMS aspects may be covered as part of the sectorial QMS standard. 160  See Article 17 of AI Act 161  See Articles 11 and 53 of AI Act 162  See Articles 8 to 14 of AI Act \n\nThe creation and maintenance of technical documentation: 161 High-risk AI systems that may follow the conformity routes of Annex VI (internal control) and Annex VII (assessment of QMS and technical documentation (TD)), and providers of general-purpose AI models, will require putting in place TD for assessing compliance with the AI Act Chapter III, Section 2 requirements. Annex XI (TD for providers of general-purpose AI models, Article 53(1))) and Annex IV (TD referred to in Article 11(1)), describe the content of the TD. TD will need to be drawn up before placing the AI system on the market. When the AI system is subject to obligations of TD under other sectorial EU legislations, AI Act aspects may be covered as part of the sectorial TD. \n\nConformity with high-risk AI system requirements: 162 Chapter III, Section 2, lists high-risk AI systems requirements. Although the ISO/IEC JTC 1/SC 42 committee has already published multiple standards, none has as of yet been harmonized with the AI Act. The legislation clearly states (Article 40) that conformity with the AI Actâ€™s harmonized standards is a presumption of conformity with AI Act requirements. \n\n# Understanding AI Act conformity @2024 BSI. All rights reserved. Transparency obligations: 163 Providers of AI systems, including general-purpose AI systems, as well as deployers of certain AI systems, need to comply with transparency obligations set in Chapter IV, Article 50. \n\nSandboxes: 164 Sandboxes are established by members states, competent authorities. Sandboxes are controlled environments to facilitate development, training, testing, and validation of innovative AI systems. AI systems might use sandboxes prior to being placed on the market or put into service. The output (exit reports) of the sandboxes may be used to demonstrate compliance with the AI Act, as part of documentation provided for the conformity assessment process, or provided to relevant market surveillance authorities. \n\nRoutes to conformity: 165 â€¢ When a high-risk AI system is subject to other sectorial EU legislations (Annex I, section A) the provider shall follow the relevant conformity assessment procedure as required under those legal acts. Requirements set out in the AI Act apply to these AI systems, and Notified Bodies may request datasets and the AI model for carrying out additional testing (Annex VII). â€¢ Internal control (Annex VI) is available as a conformity assessment route to Annex III high-risk AI systems, however, Annex III point 1 AI systems (Biometric identification) may opt for (or be forced to follow) Annex VII conformity assessments which require the Notified Bodyâ€™s involvement for the assessment of AIMS and TD. 163  See Article 50 of AI Act 164  See Article 57 of AI Act 165  See Article 43 of AI Act 166  See Article 51 and 56 of AI Act 167  See Article 56 of AI Act \n\nGeneral-purpose AI (GPAI) models: The main oversight authority for GPAIs is the AI Office. Article 53 states obligations for GPAI providers and Annex XI describes technical documentation requirements for GPAIs models. Obligations are not applicable for free and open-license providers. There are additional obligations for GPAI models with systemic risks, including model performance evaluation, mitigation measures of systemic risks, and cybersecurity protection. 166 Compliance with codes of practice 167 will be considered sufficient for GPAIs systemic risk obligations, until harmonized standards are released. @2024 BSI. All rights reserved. The AI Act is en route to the Official Journal of the \n\nEuropean Union, and the world is watching. What started out as conjecture has evolved into an emerging and impactful industry, not without its risks. It is evident in the legislative text that not one person or one group of persons can manage this new landscape on its own. It will take the collective effort of providers, deployers, the Commission, the AI Office, the AI Board, member states, National Competent Authorities, the public, and others to ensure the predictability and deployment of these systems is effectively controlled. The momentum of the AI Act has left many wondering what the next steps are to prepare themselves for its impact. We anticipate that this paper will support organizations to take the firsts steps to determine if and how AI systems or models affect their organization, identifying the organiza-tionâ€™s role and obligations, supporting under-standing of AI systems classification, and shedding 168  See Recital 178 of AI Act light on the requirements need to be fulfilled, as well as the methods those requirements are met. Organi-zations should be able to understand conformity routes for their products, allowing them to proac-tively seek accredited or designated organizations under the schemes of interest to consolidate assess-ments where possible. \n\nThe AI Act encourages providers of high-risk AI systems to start the compliance journey on a voluntary basis during the transitional period. 168 BSI shares this view; all stakeholders will face a steep learning curve. BSI deeply values the AI Act and the approach adopted by the community engaged in the development of this regulation, recognizing that the publishing of the AI Act by legislators and achieving compliance are not end goals. Instead, they represent a journey of ongoing enhancement of the regulatory framework, evolving in tandem with technological advancements. \n\n# Conclusion", "fetched_at_utc": "2026-02-08T19:07:23Z", "sha256": "aa10c02636c147ef42826a5d78a3d6a203b40a5cb97638d275bf3b7a0d5b8c76", "meta": {"file_name": "BSI_EU_AI_Act_Whitepaper_Final_2_9_24.pdf", "file_size": 4255354, "relative_path": "pdfs\\BSI_EU_AI_Act_Whitepaper_Final_2_9_24.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 18373}}}
{"doc_id": "pdf-pdfs-debunking-10-common-eu-ai-act-misconceptions-part-1-oliver-patel-70e65b20e395", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel.pdf", "title": "Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1) \n\nhttps://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions  2/21 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! The EU AI Act entered into force in August 2024 and its first provisions became applicable in February 2025. Due to a combination of factors, such as the complexity and novelty of the law, and the lack of guidance and standards, some unhelpful misconceptions have taken hold. This two-part series on Enterprise AI Governance presents and debunks 10 common misconceptions about the AI Act, providing a detailed explanation for each one. The first 5 are covered in part 1 and the second 5 will be covered in part 2. \n\nThe ten misconceptions are: \n\n1. The EU AI Act has a two-year grace period and applies in full from August 2026.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 3/21\n\n2. All open-source AI systems and models are exempt from the EU AI Act. \n\n3. High-risk AI models are explicitly regulated under the EU AI Act. \n\n4. Emotion recognition is prohibited under the EU AI Act. \n\n5. Facial recognition is prohibited under the EU AI Act. \n\n6. Transparency is required for â€˜limited riskâ€™ AI systems. \n\n7. Third-party conformity assessments are required for all high-risk AI systems. \n\n8. Fundamental rights impact assessments are required for all high-risk AI systems. \n\n9. All high-risk AI systems must be registered in the public EU-wide database. \n\n10. Deployers do not need to register their use of high-risk AI systems. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nMisconception 1: The EU AI Act has a two-year grace period and applies in full from August 2026   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 4/21\n\nIt is commonly remarked that the EU AI Act has a two-year grace period, which allows organisations to prepare to be compliant. Although there are grace periods for compliance, they vary in length, and certain provisions are already applicable today. In fairness, Article 113 does state that the AI Act â€œ shall apply from 2 August 2026 â€. However, given the number of exceptions to this, simply claiming there is a â€˜two-year grace periodâ€™ is misleading. The reality is that different provisions become applicable at different times. Although most provisions apply from August 2026, the provisions on AI literacy and prohibited AI practices became applicable in February 2025, and the obligations for providers of new general-purpose AI models become applicable in August 2025. Here is a breakdown of when the most significant provisions apply: 2 February 2025: prohibition of specific AI practices became applicable. 2 February 2025: AI literacy provisions (for deployers and providers of AI systems) became applicable. 2 August 2025: obligations for providers of â€˜newâ€™ general-purpose AI models become applicable (i.e., general-purpose AI models placed on the market from 2 August 2025 onwards).   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 5/21\n\n2 August 2026: many of the AI Actâ€™s provisions become applicable, including obligations and requirements for high-risk AI systems listed in Annex III. 2 August 2027: obligations and requirements for high-risk AI systems which are products, or safety components of products, regulated by specific EU product safety laws (listed in Annex I) become applicable. 2 August 2027: obligations for providers of â€˜oldâ€™ general-purpose AI models become applicable (i.e., general-purpose AI models placed on the market before 2 August 2025). Although the provisions on prohibited AI practices became applicable earlier this year, meaningful enforcement will come later. This is because the applicability of the penalty and governance regime, including the deadline for member states to designate their AI regulators, lands on 2 August 2025. This creates a unique situation where there is a 6-month lag between important provisions becoming applicable and the regulatory enforcement structure and regime being operational. \n\nMisconception 2: All open-source AI systems and models are exempt from the EU AI Act   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 6/21\n\nAlthough there are broad exemptions for open-source AI systems and models, there are also several important ways in which the AI Act regulates them. For example, high-risk AI systems which are open-source are still classified as high-risk, and providers of general-purpose AI models which are open-source must adhere to specific obligations (which are trimmed down in some cases). Article 2(12) states that the AI Act â€œ does not apply to AI systems released under free and open-source licenses, unless they are placed on the market or put into service as high-risk AI systems or as an AI system that falls under Article 5 or Article 50â€. This has the following meaning: Providers and deployers of high-risk AI systems which are open-source must adhere to all the obligations and requirements for high-risk AI systems, despite their AI systemâ€™s open-source nature. The reference to Article 5 means that prohibited AI practices are prohibited, irrespective of whether or not they leverage open-source AI systems. Finally, providers and deployers of open-source AI systems which interact with individuals, generate synthetic content and deep fakes, or perform emotion recognition or biometric categorisation, must adhere to the transparency obligations outlined in Article 50.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 7/21\n\nArticle 53(2) refers to open-source AI models as â€œ models that are released under a free and open-source licence that allows for the access, usage, modification, and distribution of the model, and whose parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly availableâ€. \n\nProviders of open-source general-purpose AI models must adhere to a limited set of obligations, such as publishing a summary of the modelâ€™s training data and implementing a copyright compliance policy. However, providers of these models do not have to adhere to the obligations to produce and make available technical documentation about their general-purpose AI model. Nor do they have to appoint an authorised representative in the EU if they are established in a third country. For providers of open-source general-purpose AI models with systemic risk, the full and extensive set of obligations for general-purpose AI models with systemic risk applies. This includes all the above obligations, as well as performing model evaluations, systemic risk assessment and mitigation, and ensuring adequate cybersecurity protection. In practice, this will mean that despite the broad exemptions for open-source AI, it is   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 8/21\n\nlikely that providers of some of the most advanced and widely used open-source AI models will have extensive compliance obligations. The AI Act regulates the following types of AI, with explicit and targeted provisions: prohibited AI practices; high-risk AI systems; general-purpose AI models; general-purpose AI models with systemic risk; and certain AI systems which require transparency. The AI Act does not explicitly refer to â€˜high-risk AI modelsâ€™ as a regulated category, nor are there specific provisions relating directly to them. Moreover, there are no specific provisions relating to AI models which are not general-purpose AI models. This means that unless an AI model is general-purpose, or part of an AI system or practice which is high-risk, transparency requiring, or prohibited, it is not in scope of \n\nMisconception 3: High-risk AI models are explicitly regulated under the EU AI Act   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 9/21\n\nthe AI Act. However, in most scenarios, an AI model (or AI models) will be one of the most important components of a broader AI system, which could either be high-risk or used for a prohibited practice. Therefore, AI models are regulated in this â€˜indirectâ€™ but consequential sense.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 10/21\n\n1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1) \n\nhttps://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions  11/21 Image credit: This helpful infographic from the team at Digiphile highlights what is and what is not regulated under the AI Act. \n\nInterestingly, the AI Act does not include a legal definition of the term â€˜AI modelâ€™. Article 3 lists 68 different definitions, for terms like â€˜AI systemâ€™, â€˜training dataâ€™, and â€˜general-purpose AI modelâ€™ (see below). Despite there being a definition of an important and common type of AI model (i.e., a general-purpose one), there is no definition of an â€˜AI modelâ€™ itself, which is a broader and arguably more important and foundational concept. In my view, given the centrality of AI models to high-risk AI systems, general-purpose AI models, virtually any type of prohibited AI practice, and the wider field of AI, it would have been helpful for the AI Act to include an official legal definition of an AI model. For context, the AI Act defines an AI system as: â€œa machine-based system that is designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environmentsâ€ .  \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 12/21\n\nA general-purpose AI model is defined as: â€œ an AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale, that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market â€. The concept of an AI model has been defined in other sources. \n\nNIST defines an AI model as: â€œ a component of an information system that implements AI technology and uses computational, statistical, or machine-learning techniques to produce outputs from a given set of inputs â€. In its paper on the updated definition of an AI system, the OECD refers to an AI model as â€œa core component of an AI system used to make inferences from inputs to produce outputsâ€.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 13/21\n\nImage credit: OECD visual on the definition of an AI system, which highlights that an AI model is an essential component of an AI system. \n\nArticle 5(1) stipulates that the following AI practice is prohibited : placing on the market, putting into service, or using â€œAI systems to infer emotions of a natural person in the areas of workplace and education institutions, except where the use of the AI system is intended to be put in place or into the market for medical or safety reasonsâ€. \n\nMisconception 4: Emotion recognition is prohibited under the EU AI Act   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 14/21\n\nThis means that the use of AI-enabled emotion recognition is only prohibited in specific, pre-defined contexts; namely, the workplace and educational institutions. It also means that AI-enabled emotion recognition could potentially be lawfully used in workplace and educational settings, if it can be demonstrated that this supports medical or safety objectives. For example, using AI to infer and predict the emotional state of a pilot while they are flying a plane, for the sole purpose of determining the pilotâ€™s future bonus or compensation package, would almost certainly be prohibited. However, if the AI system is used solely to initiate safety-critical interventions, which could prevent potentially harmful incidents, then this would likely be permitted under the AI Act. Furthermore, AI systems used for emotion recognition in settings other than the workplace and educational institutions are classified as high-risk, not prohibited. This is clarified in Annex III(1), which lists high-risk AI systems and includes â€œ AI systems intended to be used for emotion recognition â€. This means that providers and deployers developing, making available, or using emotion recognition systems in contexts other than the workplace and educational institutions, as well as emotion recognition systems for safety or medical reasons in the two aforementioned settings, must adhere to the obligations and requirements for high-risk AI systems.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 15/21\n\nTo understand exactly what constitutes an emotion recognition system (that could either be high-risk or prohibited, depending on the context), we need to consult the definition provided in Article 3(39): â€œâ€˜ an AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data â€. In practice, this implies that unless it is based on biometric data (e.g., voice or facial expressions), using AI for emotional recognition would not be classified as high-risk or prohibited, and thus may not even be in scope of the AI Act. For example, if I copy and paste a colleagueâ€™s email into a generative AI tool, in advance of an important meeting, and ask it to predict their emotional state at the time of writing, this would most likely not be considered a prohibited AI practice entailing a hefty penalty, given the lack of biometric data being processed. However, if this was done via an AI assessing a video recording of a meeting in which their camera was on, the situation could be quite different. \n\nEuropean Commission guidelines on the topic have confirmed that an AI system inferring emotions from written text (i.e., content/sentiment analysis) is not prohibited as it does not constitute emotion recognition, because it is not based on biometric data.   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 16/21\n\nSimilarly, there is no blanket prohibition on the development and use of facial recognition systems, which are already widely deployed in society. However, there are prohibitions on the ways in which law enforcement can use facial recognition and similar technologies to perform remote biometric identification of people in real-time. Also, if facial recognition technology was used for a prohibited AI practice, this would still be prohibited. However, this does not amount to a blanket prohibition. Concretely, law enforcement use of â€˜real-timeâ€™ remote biometric identification systems in public (e.g., facial recognition used to identify and stop flagged people in public) is prohibited, apart from in specific and narrowly defined scenarios. Acceptable scenarios for law enforcement use of such technology includes searching for victims of serious crime, preventing imminent threats to life (e.g., terrorist attacks), and locating suspects or perpetrators of serious crimes (e.g., murder). \n\nMisconception 5: Facial recognition is prohibited under the EU AI Act   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 17/21\n\nBefore AI is used in this way, independent judicial or administrative authorisation must be granted. Also, the use can only occur for a limited time period, with safeguards to protect privacy and fundamental rights. This became a totemic issue during the AI Act trilogue negotiations and legislative process. The European Parliament's initial AI Act proposals called for an outright ban on the use of real-time biometric identification systems in public, like facial recognition. This ban would have applied to law enforcement authorities and any other organisation, with no exceptions. The Council (EU member states) were never going to accept an outright prohibition and a compromise was brokered. Some types of AI-enabled facial recognition, which are permitted, would be classified as a high-risk AI system. Annex III(1) clarifies that â€˜remote biometric identification systemsâ€™, â€˜AI systems intended to be used for biometric categorisationâ€™, and â€˜AI systems intended to be used for emotion recognitionâ€™ are high-risk AI systems. It is also conceivable that a facial recognition system or component could be used as part of any other high-risk AI system listed in the AI Act. However, it is also conceivable that a facial recognition system or component could be   \n\n> 1/3/26, 5:17 PM Debunking 10 Common EU AI Act Misconceptions (part 1)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions 18/21\n\npart of an AI system which is not high-risk, nor in scope of the AI Act. This is because AI systems used for â€˜biometric verificationâ€™, which perform the sole function of confirming or authenticating that an individual is who they claim to be, are not high-risk. A facial recognition system used to unlock your phone would not be classified as high-risk, as it is merely confirming that you are who you claim to be (i.e., the owner of the phone). This is in contrast to live facial recognition cameras used by police to identify potential suspects from a crowd, as their facial data is being compared to a larger reference database of faces, with the goal of establishing a match. Finally, as per Article 5(e), it is prohibited to use AI systems to conduct untargeted scraping of facial images, from the internet or CCTV footage, with the goal of creating or expanding databases which are used to develop or operate facial recognition systems. However, this does not prohibit the use of facial recognition in a broader sense, merely a specific data scraping practice. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.", "fetched_at_utc": "2026-02-08T19:07:25Z", "sha256": "70e65b20e395a424a8e0ab0583f0112f8d14da8c1436ab726c56614d5cc47e26", "meta": {"file_name": "Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel.pdf", "file_size": 1601603, "relative_path": "pdfs\\Debunking 10 Common EU AI Act Misconceptions - Part 1 - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4392}}}
{"doc_id": "pdf-pdfs-debunking-10-common-eu-ai-act-misconceptions-part-2-oliver-patel-f6590c49a8c2", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel.pdf", "title": "Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2) \n\nhttps://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070  2/23 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! This two-part series on Enterprise AI Governance presents and debunks 10 common misconceptions about the AI Act, providing a detailed explanation for each one. The first 5 were covered in part 1 and the second 5 are covered in this edition. \n\nThe ten misconceptions are: \n\n1. The EU AI Act has a two-year grace period and applies in full from August 2026. \n\n2. All open-source AI systems and models are exempt from the EU AI Act. \n\n3. High-risk AI models are explicitly regulated under the EU AI Act. \n\n4. Emotion recognition is prohibited under the EU AI Act. \n\n5. Facial recognition is prohibited under the EU AI Act. \n\n6. Transparency is required for â€˜limited riskâ€™ AI systems.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 3/23\n\n7. Third-party conformity assessments are required for all high-risk AI systems. \n\n8. Fundamental rights impact assessments are required for all high-risk AI systems. \n\n9. All high-risk AI systems must be registered in the public EU-wide database. \n\n10. Deployers do not need to register their use of high-risk AI systems. \n\nðŸ§‘â€ðŸŽ“ If you want to dive much deeper, register interest for my EU AI Act Compliance Bootcamp here . This will be an exclusive and intimate masterclass for AI governance leaders, breaking down how to implement AI Act compliance in an enterprise setting. More information will be shared later in the year. Note: if you already registered interest for my AI Usage Policy Bootcamp, you do not need to register here again. Stay tuned for further info. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nMisconception 6. Transparency is required for â€˜limited riskâ€™ AI systems.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 4/23\n\nThis is an important example, because the AI Act does not use the term â€˜limited riskâ€™ to refer to the AI systems for which transparency is required. Therefore, it does not make sense to use this term. It would be like using completely different and inappropriate terminology when referring to â€˜high-risk AI systemsâ€™ or â€˜prohibited AI practicesâ€™. What the AI Act actually says is that for certain AI systems, there are transparency obligations for providers and deployers. Therefore, more accurate terminology than â€˜limited riskâ€™ is â€˜transparency requiring AI systemsâ€™. It may not roll off the tongue as smoothly, but it does the job. All the AI Act risk pyramid images which are widely circulated (including sometimes by EU departments), which use the term â€˜limited riskâ€™ instead of â€˜transparency requiringâ€™ (or something of that nature) are arguably fuelling this misconception. This misconception has taken hold due to a lack of precision regarding the language which is used to talk about the AI Act. This is problematic, because in a complex legal text like this, there are many different terms which, although they may sound conceptually similar, mean different things and can give rise to very different real-world consequences. To accurately and effectively understand, interpret, and comply with a law of this complexity, precision and care with language is key.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 5/23\n\nArticle 50 outlines the transparency obligations for certain AI systems. The AI systems are: AI systems intended to interact directly with people; AI systems that generate synthetic image, audio, video or text content; emotion recognition systems; biometric identification systems; and AI systems which generate or manipulate deep fake content. Providers and deployers of these AI systems are obliged to implement additional transparency measures, such as notification and disclosure, which are detailed in Article 50. These â€˜transparency requiring AI systemsâ€™ can simultaneously be high-risk AI systems, or AI systems which are not high-risk. In the former scenario, all applicable obligations and requirements for high-risk AI systems would also apply. In the latter scenario, only the transparency obligations in Article 50 would apply. This is another reason why the â€˜classicâ€™ yet misleading AI Act risk pyramid doesnâ€™t   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 6/23\n\nwork: it falsely implies that the obligations for high-risk AI systems and â€˜transparency requiring AI systemsâ€™ are mutually exclusive. This point has been observed and elaborated on by other analysts, such as Aleksandr Tiulkanov , who has produced two helpful infographics to help conclusively debunk this misconception. \n\nSee below for the two original images, which have not been modified, and were posted on Aleksandrâ€™s article â€˜ EU AI Act: Getting the Basics Straightâ€™ .  \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 7/23\n\n1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2) \n\nhttps://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070  8/23 Misconception 7: Third-party conformity assessments are required for all high-risk AI systems \n\n1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2) \n\nhttps://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070  9/23 Under the AI Act, providers are obliged to ensure that their high-risk AI systems undergo a conformity assessment before they are placed on the market or put into service. A conformity assessment is an established and longstanding practice for many products regulated by EU product safety laws. The AI Act defines a â€˜conformity assessmentâ€™ as â€œ the process of demonstrating whether the requirements set out in Chapter III, Section 2 relating to a high-risk AI system have been fulfilledâ€. This means the focus of the conformity assessment is largely on the following requirements: risk management system, data and data governance; technical documentation; record-keeping; transparency and provision of information to deployers; human oversight; and accuracy, robustness and cybersecurity.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 10/23\n\nOnce the conformity assessment is completed, the provider must produce the â€˜declaration of conformityâ€™, which can be inspected by other parties, like importers or distributors. However, not all high-risk AI systems are obliged to undergo the same conformity assessment procedure. The key distinction is between the two main categories of high-risk AI system: \n\n1. AI systems which are products, or safety components of products, regulated by one of the EU product safety laws listed in Annex I; and \n\n2. AI systems listed in Annex III. For the first category of high-risk AI system, the requirement to undergo a third-party conformity assessment is already stipulated in the existing laws which are referenced in Annex I. Moreover, the AI systems covered by these existing laws are not classified as high-risk under the AI Act unless the relevant product is already required to undergo a third-party conformity assessment. To avoid burdensome and duplicative compliance work, the AI Act does not create a new and additional conformity assessment regime for these AI systems. What it says is that the existing conformity assessment procedure, performed by a third-party (i.e., a notified body) must continue to be followed, but that it must also   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 11/23\n\nnow consider and include the new requirements outlined in the AI Act. This necessarily entails an update to, and augmentation of, those existing third-party conformity assessment procedures. However, for the second category of high-risk AI systems (i.e., those listed in Annex III), there is no obligation for a third-party conformity assessment. Rather, providers must perform a â€˜self-assessmentâ€™, which does not involve a third-party (i.e., a notified body). Article 43(2) refers to this as the â€œ conformity assessment procedure based on internal control â€. Annex VI provides further information about how providers must perform this self-assessment. EU legislators opted for this approach due to the lack of AI certification expertise and maturity in the wider market, which was deemed an impediment to notified bodies performing conformity assessments across all these domains, at least for now. However, Recital 125 suggests that in future, as the market matures, these conformity assessments may also be performed by notified bodies. The only partial exception to this is high-risk AI systems used for biometrics, including: remote biometric identification; biometric categorisation; and   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 12/23\n\nemotion recognition. In certain scenarios, a third-party conformity is required for these high-risk AI systems. For example, if the provider has not fully applied an official technical standard to demonstrate compliance, or if such a standard does not exist, then the conformity assessment must be performed by a notified body. The procedure for this is outlined in Annex VII. Performing a fundamental rights impact assessment is an important obligation which applies to certain deployers of specific high-risk AI systems. Where applicable, a fundamental rights impact assessment must be performed by the deployer prior to using the AI system. The fundamental rights impact assessment is a formal exercise where deployers must consider, describe, and document how and when they will use the AI system, which people or groups will be impacted by it, the specific harms likely to impact those \n\nMisconception 8: Fundamental rights impact assessments are required for all high-risk AI systems   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 13/23\n\npeople, how human oversight will be implemented, and what will be done by the deployer if any risks materialise or harm arises. It does not apply to providers, nor does it apply to all deployers or all high-risk AI systems. The obligation only applies to the following deployers: bodies governed by public law; private entities providing public services; deployers of AI systems used for life and health insurance risk assessment and pricing; and deployers of AI systems used for creditworthiness evaluation and credit score assessment. For the deployers which are governed by public law, or private entities providing public services, they must only perform a fundamental rights impact assessment before using one of the high-risk AI systems in Annex III. However, this excludes AI systems used for safety management and operation of critical infrastructure, for which no fundamental rights impact assessment is required.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 14/23\n\nIn practice, this means that most businesses will not have to perform a fundamental rights impact assessment prior to using a high-risk AI system. This is either because it does not apply to them as a deployer, or it does not apply to the high-risk AI system they are using, or both. However, for some financial services and insurance firms, as well as companies which provide AI-driven public services in domains like welfare, education, and border control, this will become an important part of their AI governance and compliance work. Once the fundamental rights impact assessment has been completed, the deployer must notify the relevant regulator and, if applicable, summarise its findings in their registration entry in the EU database for high-risk AI systems ( see misconception 10 below ). Whilst providers of all high-risk AI systems are required to perform risk assessments and implement risk management measures (see Article 9)â€”which includes considering the potential risk to fundamental rightsâ€”this is not the same as a dedicated fundamental rights impact assessment. \n\nMisconception 9: All high-risk AI systems must be registered in the public EU-wide database   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 15/23\n\nArticle 71 of the AI Act mandates the European Commission to establish and maintain an EU database for high-risk AI systems. It must be â€œ publicly accessible, free of charge, and easy to navigate â€. Providers of certain high-risk AI systems are obliged to register information about their AI systems and their organisation in this database. They must do this before placing the high-risk AI system on the market or putting it into service. To dispel this misconception, there are four important points you should understand, each of which are explained below: \n\n1. The registration obligation does not apply to all high-risk AI systems. \n\n2. The registration obligation applies to some AI systems which are not technically high-risk. \n\n3. Not all information in the database will be available to the public. \n\n4. Some high-risk AI systems must be registered in a national database, instead of the EU database. \n\n1. The registration obligation does not apply to all high-risk AI systems   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 16/23\n\nThe registration obligation, outlined in Article 49, only applies to high-risk AI systems listed in Annex III. This includes AI systems used for recruitment, health insurance pricing, and educational admissions. However, this excludes AI systems which are products, or safety components of products, regulated by one of the EU safety laws listed in Annex I. This means that AI systems used in critical, regulated domains, like medical devices, vehicles, and aviation do not need to be registered in the EUâ€™s public database. There are various other ways in which the AI Act compliance obligations and requirements differ across these two main categories of high-risk AI system, such as with respect to conformity assessments ( as outlined in misconception 7 above ). \n\n2. The registration obligation applies to some AI systems which are not technically high-risk \n\nArticle 6(3) specifies an important caveat for the classification of high-risk AI systems. It states that high-risk AI systems listed in Annex III are not high-risk if they do not â€œpose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing the outcome of decision making â€. Four potential conditions are provided. If one of these is met, then the AI system does   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 17/23\n\nnot pose this type of risk and is thus not classified as high-risk. For example, one condition is that the AI system performs only a â€œ narrow procedural task â€. Interestingly, even where a provider legitimately determinesâ€”as per the Article 6(3) exception procedureâ€”that their AI system is not high-risk, they must still register that AI system in the EUâ€™s public database. The logic of this is to promote transparency regarding how providers are determining and documenting that AI systems are not high-risk, despite being used in sensitive (Annex III) domains. This could lead to a potentially vast number of AI systems needing to be registered and many organisations being unaware that they are obliged to do so. \n\n3. Not all information in the database will be available to the public \n\nFor certain high-risk AI systems listed in Annex III, including AI systems used for law enforcement, migration, asylum, and border control management, the information will be registered and stored in a â€œ secure non-public section â€ of the EU database. This includes lawful AI systems used for remote biometric identification, biometric categorisation, and emotion recognition, in the context of law enforcement, migration, asylum, and border control management.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 18/23\n\nThese providers are obliged to register less information than the providers of the other high-risk AI systems, and that information can only be viewed by the European Commission and the specific member state regulators who have been designated to lead on AI Act enforcement for those sensitive sectors. Therefore, many of the AI systems which are the most sensitive, and arguably pose the greatest risk to fundamental rights, will not be publicly registered. \n\n4. Some high-risk AI systems must be registered in a national database, instead of the EU database. \n\nProviders of high-risk AI systems that are used as safety components for the operation and management of critical infrastructure, like water and energy, are obliged to register their AI systems, and themselves, at the â€œnational levelâ€. This means that the registration of these AI systems will be in different databasesâ€” maintained by member state regulators and/or governmentsâ€”separate from the public database maintained by the European Commission. The AI Act does not reveal much about these national level databases. However, there is no provision which states that they must be public. This signals an acknowledgement of the sensitivity and secrecy of these domains, because of their importance to national security and economic stability.   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 19/23\n\nCertain deployers of specific high-risk AI systems are also obliged to register information about their organisation, and the high-risk AI system they are using, in the EU database. They must do this before using the AI system. This obligation only applies to deployers that are â€œ public authorities, EU institutions, bodies, offices or agencies or persons acting on their behalf â€. The focus is therefore on public sector organisations in the EU using high-risk AI, as opposed to private sector and commercial AI usage. These organisations must register themselves, and select which high-risk AI system they are using, in the EU database. It will not be possible to do this if the provider has not already registered the high-risk AI system. Therefore, this is something which should be checked by the deployer, as part of procurement and partnership due diligence. This also means that the deployer registration obligation only applies to the high-risk AI systems which providers are obliged to register. This includes the high-risk AI \n\nMisconception 10: Deployers do not need to register their use of high-risk AI systems   \n\n> 1/3/26, 5:18 PM Debunking 10 Common EU AI Act Misconceptions (part 2)\n> https://oliverpatel.substack.com/p/debunking-10-common-eu-ai-act-misconceptions-070 20/23\n\nsystems used for law enforcement, migration, asylum and border control management, however the deployer information will also be registered the â€œ secure non-public section â€ of the EU database, which only the European Commission and certain public authorities can access. There is no explicit provision which stipulates that deployers must register their use of AI systems used for critical infrastructure safety management and operation, which providers must register at the national level instead of the EU-wide database. There is also nothing which indicates that deployers are obliged to register their use of AI systems which are not high-risk, but have nonetheless been registered as they fell under the scope of an Article 6(3) derogation. Finally, there are certain scenarios when deployers can become providers of high-risk AI systems, either intentionally or inadvertently. For example, if they make a â€œsubstantial modificationâ€ to the AI system and it remains high-risk. In these scenarios, the new provider would be required to register the high-risk AI system. \n\nðŸ§‘â€ðŸŽ“ If you found this post useful, register interest for my EU AI Act Compliance Bootcamp here . This will be an exclusive and intimate masterclass for AI governance leaders, breaking down how to implement AI Act compliance in an enterprise setting.", "fetched_at_utc": "2026-02-08T19:07:27Z", "sha256": "f6590c49a8c26344df407e9f9ebc458e9630517cf28bb66045609149b1361557", "meta": {"file_name": "Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel.pdf", "file_size": 1467187, "relative_path": "pdfs\\Debunking 10 Common EU AI Act Misconceptions - Part 2 - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4772}}}
{"doc_id": "pdf-pdfs-eu-ai-act-simplification-oliver-patel-597a6bfd3c39", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\EU AI Act simplification - Oliver Patel.pdf", "title": "EU AI Act simplification - Oliver Patel", "text": "Hey ðŸ‘‹ \n\n1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"? \n\nhttps://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification  2/16 Iâ€™m Oliver Patel , author and creator of Enterprise AI Governance and author of the forthcoming book, Fundamentals of AI Governance (2026). \n\nOn Wednesday 19 November 2025, the European Commission will publish its Digital Omnibus Packageâ€”a sweeping proposal to reform the EUâ€™s digital legislative framework. The EU AI Act is in sharp focus, alongside the GDPR and a host of other flagship EU laws. This article explains how we got here, whatâ€™s at stake, and the potential changes being discussed in Brussels. It will be followed by in-depth analysis of the proposed changes, once they are published. \n\nThe EU has consistently sought to shape global regulatory standards on digital technology, data, and AI. The EU has been open about its policy objectives of protecting citizens and promoting global regulatory convergence towards its high standards, in domains like data protection and privacy. And in some ways, it has succeeded. With GDPR, for example, there has been a demonstrable â€œBrussels effectâ€. Many countries worldwide have enacted data protection laws of a similar flavour and \n\nHow did we get here?   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 3/16\n\ncorporations have prioritised aligning their internal frameworks accordingly. Despite the lack of U.S. federal privacy law (which renders the U.S. a global outlier), almost all major American enterprises are significantly impacted by the GDPR and have implemented dedicated privacy compliance programmes. However, as EU laws covering digital governance have proliferated, the picture has become increasingly complex and convoluted. Keeping track of the EUâ€™s regulations and directives covering the digital sphere is no mean feat. CEPS and Kai Zennerâ€™s dataset of EU digital sector legislation , updated in July 2025, lists 101 different laws , including the EU AI Act and the GDPR. The authors remark that there has been an â€œ absolute explosion â€ of EU digital laws and that â€œeven the best experts struggle to keep up with this torrent of legal and policy instrumentsâ€. To complement the 101 laws, there exists a far greater number of regulatory bodies covering digital issues. Enterprises require large teams of people to make sense of these developments and determine how to be compliant, not least because obtaining deep expertise in just one legislative area takes years. In response, enterprises have also established a range of somewhat separate yet overlapping digital governance capabilities to address key legal obligations and requirements. This includes implementing enterprise AI governance, which has become a core priority in recent years due to both the EU AI   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 4/16\n\nAct and the surge in AI adoption. But you didnâ€™t need me to tell you thatâ€¦ For AI governance professionals, this regulatory complexity is especially acute, as AI systems frequently trigger multiple regulatory requirements simultaneouslyâ€”from GDPRâ€™s rules on automated decision-making and use of sensitive personal data, to the AI Actâ€™s requirements for high-risk and transparency-requiring AI systems, cybersecurity mandates, copyright and intellectual property, and existing sectoral laws. In recent months, discussion regarding simplification of the EUâ€™s digital rulebook has intensifiedâ€”both within the EU and externally. It is widely reported that EU officials and member states have faced sustained lobbying from U.S. industry and the Trump administration. There are even reports of President Trump considering imposing tariffs and sanctions on the EU and its officials, due to the perceived impact of EU digital laws on U.S. companies and citizens. This pressure has been accompanied by increasingly vocal calls from segments of European industry that EU digital regulations need to be reformed, streamlined, simplified, to reduce compliance burdens. For example, in July 2025, dozens of companiesâ€”including industry heavyweights like Mercedes Benz, Deutsche Bank, and Lâ€™Orealâ€” signed an open letter to the EU urging for a two-year delay to the EU AI Actâ€™s key provisions on general-purpose AI (GPAI) models and high-risk AI systems.   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 5/16\n\nThis is coming to a head on Wednesday 19 November, as the European Commission is set to publish its â€œDigital Omnibus Packageâ€ proposal. This will outline, for the first time, the comprehensive set of legislative amendments the Commission proposes, to simplify the EUâ€™s digital rulebook. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nAlthough this situation has evolved over several years, the September 2024 publication of the â€œDraghi reportâ€ on The Future of European Competitiveness was an important milestone in this story. Mario Draghi is the former president of the European Central Bank and former prime minister of Italy. This independent report, commissioned by the EU, is shaping aspects of the European Commissionâ€™s policy agenda. The report presents the EUâ€™s economic challenges, focusing on diminished competitiveness, weakening productivity, and slowing growth. Draghi frames the economic outlook as an â€œexistential challengeâ€ for the EU, arguing that it will be unable \n\nWhat was the Draghi report?   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 6/16\n\nto finance its social model, achieve its environmental ambitions, and deliver the prosperous and fair society that represents its raison dâ€™Ãªtre if it fails to make radical changes. The Draghi report outlines three action areas to â€œreignite growthâ€. These are: \n\n1. Closing the innovation gap with the U.S. and China, especially in advanced technologies. \n\n2. Forging a joint plan for decarbonisation and competitiveness. \n\n3. Increasing security and reducing dependencies. It is the first action area which is most relevant for our focusâ€”EU AI Act simplification. The Draghi report argues that the EUâ€™s complex digital regulatory environment impedes innovation and growth. It elevates â€œreducing the regulatory burdenâ€ as a core priority for transforming the EUâ€™s economic prospects. Examining the â€œinnovation gapâ€ between the U.S. and China on the one hand and the EU on the other, the report claims that innovative European companies attempting to scale up are â€œhindered at every stage by inconsistent and restrictive regulationsâ€. This is why, the report argues, European tech entrepreneurs routinely seek to grow their businesses in the U.S. The report also cites that 55% of SMEs flag â€œregulatory obstacles and administrative burdenâ€ as their greatest challenge, arguing that these â€œregulatory   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 7/16\n\nburdensâ€ are particularly damaging for digital sector SMEs. The report criticises the â€œprecautionary approachâ€ taken by EU digital laws, including the EU AI Act. It specifically calls out the compute threshold for determining whether a general-purpose AI (GPAI) model poses â€œsystemic riskâ€, noting that various frontier AI models already exceed the threshold, despite the EU AI Actâ€™s nascency. It also highlights the increasing difficulty that companies face in navigating the various overlapping laws relevant for AI and the hundreds of regulatory bodies across the EU responsible for digital governance. It is important to note that a vast array of other impediments and challenges are highlightedâ€”from inadequate research talent pipelines to low investment in innovation commercialisationâ€”as contributing factors to the EUâ€™s competitiveness challenge. Therefore, my intention is not to claim that the Draghi report is all about digital â€œregulatory burdensâ€, as that would be misleading. It would be equally misleading to claim the Digital Omnibus Package results solely from the Draghi report. However, the complexity of the EUâ€™s digital legislative framework has undeniably become a totemic issue, which the Draghi report shone a very bright light on. Next, we turn our attention to the European Commissionâ€™s Digital Omnibus Package. Will it deliver the changes that Draghi seeks? And what could this mean for the EU AI Act?   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 8/16\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nThe Digital Omnibus Package is the European Commissionâ€™s much anticipated proposal to reform the EUâ€™s digital legislative framework. It is due to be announced and published on Wednesday 19 November. This follows public consultations earlier in the year. The proposal will feature meaningful amendments to some of the EUâ€™s flagship laws and perhaps even the repeal of specific instruments. The Digital Omnibus Package is expected to focus on the EU AI Act, the GDPR, the ePrivacy Directive, the Data Act, and the NIS2 Directiveâ€”horizontally cutting across the core digital governance domains of AI, data protection and privacy, and cyber security. The purpose of the Digital Omnibus Package is to simplify and streamline the EUâ€™s digital legislative framework, to ease compliance burdens and cut costs for organisations (particularly startups and SMEs), promote the growth and \n\nWhat is the EUâ€™s Digital Omnibus Package?   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 9/16\n\ncompetitiveness of European companies, and boost innovation. It is inevitable that the core threads from the Draghi report will be woven into the European Commissionâ€™s proposal. Before engaging in any further analysis of the imminent proposal, several caveats are required. First, the current discussion, including this article, is based on leaked documents, media reporting, speculation, and rumours. Brussels is a famously leaky city, so this is nothing new. However, until the European Commission officially publishes its proposal, we do not know exactly what the proposal consists of. At the time of writing, nothing official has been published. Second, the Digital Omnibus Package that will be published later this week merely represents the European Commissionâ€™s initial proposal on this controversial set of issues. Therefore, even when we have the proposal, all we will have is the official starting position of one of the EUâ€™s institutions. To amend EU laws in this way will require extensive trilogue negotiations and approval from the European Parliament and EU member states via the Council of the EU (the Council). Third, the aforementioned trilogue negotiationsâ€”and the process of amending and repealing a suite of EU laws in this wayâ€”are bound to be lengthy, complex, and   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 10/16\n\nfraught with drama. Although the precise legislative instrument to operationalise the Digital Omnibus Package is yet to be confirmed, it is most likely to be an EU regulation (which is the same legal instrument as laws like the GDPR and the EU AI Act). To amend existing EU laws via a new regulation, the EUâ€™s â€œ ordinary legislative procedureâ€ must be followed. This necessitates dual approval from both the European Parliament and the Council, following trilogue negotiations where both institutions have several opportunities to amend and update the legislative proposal. On average, it takes the EU 19 months to agree new laws, from the initial Commission proposal to formal adoption. Plainly speaking, what all this means is that: We donâ€™t yet have an official proposal from the European Commission, merely leaks and media speculation. When we do, it will be subject to lengthy and fraught trilogue negotiations, the outcome of which is impossible to predict. However, what can be predicted with a degree of certainty is that there will be a substantial difference between the European Commissionâ€™s initial legislative proposal and the final text that is voted on.   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 11/16\n\nFollowing this, formal approval will be required from both the European Parliament and the Council to pass any regulation that meaningfully amends existing EU laws. It is impossible to predict what the final legislative text will consist of and how long the negotiation and approval process will take. Finally, there is no guarantee that any legislative changes will be approvedâ€” although this does seem unlikely given the various points made above. Caveats aside, the final part of this article will highlight some of the potential changes that are reported to be on the cards for the EU AI Act. Although based primarily on leaks and tip-offs, recent media reporting nonetheless shines a light on what is being discussed in the EUâ€™s corridors of power. Below is a list of the various potential EU AI Act changes that have been reported. A special shout out for the reporting from MLexâ€™s Luca Bertuzzi (who is a must follow for AI governance practitioners), which this list is largely based on: \n\nHow could the EU AI Act change?   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 12/16\n\nDelaying the applicable date for the obligations for providers and deployers of transparency-requiring AI systems. Delaying the applicable date for the obligations for providers and deployers of high-risk AI systems. These obligations apply from 2 August 2026, but this enforcement could reportedly be delayed for one year. This is partly due to delays in finalising technical standards that organisations can use to comply with these provisions. Scrapping the AI literacy obligation for organisations and shifting it to governments, regulators, and EU institutions. Centralising enforcement powers with the European Commissionâ€™s AI Office. This could be done by designating the AI Office as the regulatory authority responsible for supervising AI systems based on GPAI models. This is partly driven by concerns regarding readiness and capacity of EU member state regulators (some of which have not yet been designated), as well as the complexity firms could face in engaging with many different regulatory bodies. Introducing smaller and more proportionate compliance penalties for â€˜small mid-capsâ€™ (defined as companies that employ up to 750 people and have an annual turnover of under â‚¬150 million).   \n\n> 1/3/26, 5:24 PM What's next for EU AI Act â€œsimplification\"?\n> https://oliverpatel.substack.com/p/whats-next-for-eu-ai-act-simplification 13/16\n\nThis would complement and expand the scope of the existing proportionality for compliance penalties for SMEs. Removing the obligation for providers to register, in the EUâ€™s public database, AI systems that are used in a high-risk domain which they have deemed and demonstrated are not high-risk due to the nature of the use. It will be interesting to see which (if any) of these potential changes survive in the Digital Omnibus Package that the European Commission publishes on Wednesday. Given the speculative nature of this, I will not provide any further analysis on these potential changes in this article. One final point to note is that the European Commission does have powers to amend or change aspects of the EU AI Act, via instruments called delegated acts and implementing acts. For example, the European Commission can modify the compute threshold for GPAI models with systemic risk via a delegated act. Where these powers exist, the European Commission is able to drive changes without the need for the ordinary legislative procedure and formal approval from the European Parliament and Council. Although, there is always a degree of oversight from these institutions, who retain veto powers. However, the EU AI Act â€œsimplificationâ€ changes discussed above extend far beyond the scope of the Commissionâ€™s powers to drive changes via delegated and implementing acts.", "fetched_at_utc": "2026-02-08T19:07:32Z", "sha256": "597a6bfd3c390cadfeded04f8589f4aedfe4d341d7d76aca6c51a25f008e7e03", "meta": {"file_name": "EU AI Act simplification - Oliver Patel.pdf", "file_size": 682333, "relative_path": "pdfs\\EU AI Act simplification - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 3502}}}
{"doc_id": "pdf-pdfs-european-union-artificial-intelligence-act-bird-bird-d085ee7a46ea", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\European Union Artificial Intelligence Act - Bird & Bird.pdf", "title": "European Union Artificial Intelligence Act - Bird & Bird", "text": "European Union Artificial Intelligence Act: a guide \n\n# 7 April 2025 21 2 3 4 5 6 7 8 9 10 \n\n# Contents \n\nOVERVIEW, KEY CONCEPTS & TIMING OF IMPLEMENTATION \n\nOverview Key concepts Timeline \n\nGENERAL-PURPOSE AI MODELS \n\nBackground and relevance of general-purpose AI models of personal data Terminology and general-purpose AI value chain Obligations for providers of general-purpose AI models General-purpose AI models with systemic risk \n\nTRANSPARENCY OBLIGATIONS \n\nGeneral transparency obligations Transparency obligations for high-risk AI systems Timing and format Transparency obligations at the national level and codes of practice Relationship with other regulatory frameworks \n\nREGULATORY SANDBOXES \n\nAI regulatory sandboxes Real-world testing of AI systems \n\nENFORCEMENT & GOVERNANCE \n\nOverview Post-marking obligations Market surveillance authorities Procedures for enforcement Authorities protecting fundamental rights General-purpose AI models Penalties Remedies for third parties Governance \n\nAI ACT: WHATâ€™S NEXT \n\nAI Act application deadlines Delegated Acts Implementing Acts Commission Guidelines Codes of conduct and practice Standards Liability \n\nOUR GLOBAL CONTRIBUTORS MATERIAL AND TERRITORIAL SCOPE \n\nMaterial scope Territorial scope Exclusions Relationship with other regulatory frameworks \n\nPROHIBITED AI PRACTICES \n\nProhibited AI practices To whom do the prohibitions apply? Enforcement and Fines \n\nHIGH-RISK AI SYSTEMS \n\nClassification of an AI system as a high-risk AI system Obligations for providers of high-risk AI systems Harmonised standards and conformity assessment procedure for providers of high-risk AI systems Obligations for deployers of high-risk AI systems Obligations for other parties in connection with high-risk AI systems 15678910 234\n\nRanked Tier 1 \n\n# Legal 500 for Artificial Intelligence Distinguished for our client satisfaction 31 2 3 4 5 6 7 8 9 10 Overview, key concepts & timing of implementation \n\n# Overview \n\nThe European Union (EU) stands as a pioneer \n\nin the regulation of artificial intelligence (AI), setting a global benchmark with its proactive approach to ensuring ethical and responsible AI development. Indeed, it seems we may witness a new Brussels effect, reminiscent of the influence wielded by the GDPR. The EUâ€™s comprehensive and precautionary framework prioritises transparency, accountability, and human rights. The AI Act applies beyond the borders of the EU - many of its provisions apply regardless of whether the providers are established or located within the EU or in a third country. The AI Act applies to any provider or entity responsible for deploying an AI system if â€œ the output produced by the system is intended to be used â€ in the EU. Foreign suppliers must appoint an authorised representative in the Union to ensure compliance with the Actâ€™s provisions. However, the AI Act does not apply to public authorities of third countries or to international organisations under police and judicial cooperation agreements with the Union, nor to AI systems placed on the market for military defence or national security purposes. This broad scope aims to ensure comprehensive regulation of AI systems and their uses. \n\nWhat you can expect from this guide \n\nâ€¢ This chapter provides an overview of the whole AI Act, its key concepts and the dates from when its provisions will apply. \n\nâ€¢ Chapter 2 looks at the territorial and material scope of the AI Act. \n\nâ€¢ Chapters 3, 4, 5 and 6 address the \n\nrequirements the AI Act imposes on different types of AI - prohibited practices; high risk systems; general purpose AI; and AI where greater transparency is needed. \n\nâ€¢ Chapter 7 explains the AI Actâ€™s arrangements for testing AI in regulatory sandboxes. Chapter 8 looks at governance and enforcement. \n\nâ€¢ Chapter 9 summarises the numerous further measures that have to follow the adoption of the AI Act. \n\nâ€¢ Last, Chapter 10 includes all the contributors to this guide. \n\nA risk-focused approach \n\nThe EU approach to AI regulation is characterised by its risk-based framework. This regulation adopts a technology-neutral perspective, categorising AI systems based on their risk level, ranging from minimal to high risk. This system ensures that higher-risk AI applications, particularly those that can significantly impact fundamental rights, are either prohibited or subjected to stricter requirements and oversight. The EU places a strong emphasis on promoting the development and use of responsible AI. The AI Act mandates strict measures for data security and user privacy, ensuring that AI systems are designed and deployed with these considerations at the forefront. This includes rigorous requirements for how data is handled and protected, ensuring that usersâ€™ personal information remains secure. Additionally, the AI Act requires comprehensive risk assessments for AI systems. These assessments help identify and mitigate potential risks associated with AI technologies, fostering transparency and accountability among AI providers. By making these evaluations mandatory, the EU ensures that AI developers thoroughly understand and address the implications of their technologies. This proactive approach aims to build public trust in AI technologies by protecting usersâ€™ rights and well-being. By prioritising data security, privacy, and risk management, the EU seeks to reassure the public that AI can be used safely and ethically. This focus on responsible development helps to promote broader acceptance and integration \n\nCHAPTER 1 41 2 3 4 5 6 7 8 9 10 \n\nof AI technologies, ultimately benefiting society as a whole. The AI Act has been developed not only to create laws for AI systems, but also to establish an ethical framework for their use, to ensure that organisations consider the impact of their AI systems on people, other businesses, the environment and many other aspects of our lives. \n\nEthics at the heart of the AI Act \n\nThe AI Act explicitly builds on the Ethical Guidelines on Trustworthy AI, which were published by the European Commission in 2019. While these guidelines remain non-binding, many of their principles have been directly incorporated into the AI Act. The best example of this approach is that in many of its provisions, the AI Act refers directly to the fundamental rights enshrined in the Charter of Fundamental Rights of the European Union. For example, \n\nhigh-risk AI systems are those that have a significant harmful impact on the health, \n\nsafety and fundamental rights of persons \n\nin the Union. The proper application of the AI Act will in many cases require an analysis of the risks to fundamental rights, which includes both legal and ethical issues. It can therefore be said that ethics has been embedded into the AI Act. \n\nGovernance \n\nThe European Union adopts a decentralised supervision model, promoting collaboration with various national authorities. The AI Act establishes the European Artificial Intelligence Office (the AI Office) as an independent entity, serving as the central authority on AI expertise across the EU, and playing a crucial role in implementing of the legal framework. This office will encourage the development of trustworthy AI and support international collaboration. The European Artificial Intelligence Board will be composed of one representative per Member State and the European Data Protection Supervisor shall participate as observer. The AI Office aims to promote and facilitate the creation, review, and adaptation of codes of good practice, considering international approaches. To ensure these codes reflect the current state of the art and incorporate diverse perspectives, the AI Office will collaborate with relevant national authorities and may consult with civil society organisations, stakeholders, and experts, including scientific experts. \n\n# Key concepts \n\nAI systems (see also Chapter 2) \n\nMost of the AI Act applies to â€œAI systemsâ€ ,\n\nwhich the Act defines as â€œ a machine-based system designed to operate with varying levels of autonomy and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can \n\ninfluence physical or virtual environments â€. It is worth noting that the AI Act does not define \n\nâ€œartificial intelligenceâ€ , but only the term â€œartificial \n\nintelligence systemâ€ . The definition of an AI system is intentionally consistent with the OECD definition of an AI system. The definition does not mention any specific technology or currently known approaches to artificial intelligence systems. With the rapidly evolving nature of AI, this prevents the AI Act from becoming obsolete due to technological developments. A key element of this definition is the AI systemâ€™s ability to â€œinferâ€ . This should allow for a clear distinction between AI systems and traditional software. If a computer program operates according to rules defined in advance by the programmers, it is not an AI system; if a system is built using techniques that allow the program to create rules of its own based on input data or data sets provided to the program, then it is an AI system. The definition of an AI system is discussed further in guidelines published by the Commission on 6 February 2025. \n\nObligations across the supply chain \n\n(see also Chapter 2) \n\nThe AI Act applies to all participants in the supply chain, starting with the â€œproviderâ€ and also \n\nencompassing the â€œimporterâ€ , â€œdistributorâ€ and \n\nâ€œdeployerâ€ of the system. Most responsibilities lie with the provider, and next with the deployer. An importer, distributor or deployer may become a provider of the high-risk AI system if they have put their name or trademark on the system. They may become a provider of a high-risk system (see page 5) if they make substantial modifications to, or modify the intended purpose of the AI system, which renders the system high-risk. 51 2 3 4 5 6 7 8 9 10 \n\nRisk approach to classification of AI systems \n\nThe AI Act defines risk as â€œ the combination of the probability of harm occurring and the severity of that harm.â€ \n\nThe risk-based classification of AI systems is a fundamental aspect of the AI Act, focusing on the potential harm to health, safety, and fundamental human rights that an AI system may cause. This approach categorises AI systems into four distinct risk levels: 1.  Unacceptable risk: AI systems that pose such significant risks are unacceptable and therefore prohibited. 2.  High risk: High-risk AI systems are subject to stringent regulatory requirements. 3.  Limited risk: AI systems in this category pose a limited risk, but have specific transparency obligations. 4.  Minimal or no risk: AI systems that pose minimal or no risk have no regulatory restrictions under the AI Act. \n\nUnacceptable risk: prohibited practices \n\n(see also Chapter 3) \n\nThe AI Act contains a list of prohibited AI practices, which should be understood as a prohibition on placing on the market, putting into service, or using an AI system that employs any of these practices. The list prohibits: \n\nâ€¢ using subliminal techniques or purposefully manipulative or deceptive techniques to materially distort behaviour, leading to significant harm; \n\nâ€¢ exploiting vulnerabilities of an individual or group due to their specific characteristics, leading to significant harm; \n\nâ€¢ social scoring systems i.e. evaluating or classifying of an individual or group based on their social behaviour or personal characteristics, leading to detrimental or unfavourable treatment; \n\nâ€¢ evaluating a personâ€™s likelihood of committing a criminal offence, based solely on profiling or personal characteristics; except when used to support human assessment based on objective and verifiable facts linked to a criminal activity; \n\nâ€¢ facial recognition databases based on untargeted scraping from the internet or CCTV; \n\nâ€¢ inferring emotions in workplaces or educational institutions, except for medical \n\nor safety reasons; \n\nâ€¢ biometric categorisation systems that categorise a person based on their sensitive data, except for labelling or filtering lawfully acquired biometric datasets such as images in the area of law enforcement; \n\nâ€¢ real-time remote biometric identification systems in publicly available spaces for law enforcement purposes, except in narrowly defined circumstances. In some cases, the AI Act contains exceptions that allow these â€œprohibitedâ€ practices to be used in certain situations. A good example is real-time biometric identification, where the Regulation allows its use in exceptional circumstances. The application of these exceptions requires notifications or prior authorisations. The Commission published guidelines on prohibited AI practices on 4 February 2025. \n\nHigh-risk AI systems (see also Chapter 4) \n\nThe extensive regulation of high-risk AI systems constitutes a major part of the AI Act. AI systems are identified as high-risk AI systems if they have a significant harmful impact on the health, safety and fundamental rights of persons in the Union. There are two categories of high-risk AI systems which are regulated differently: \n\nâ€¢ AI systems intended to be used as a product or a safety component of a product which is covered by EU harmonisation legislation, such as civil aviation, vehicle security, marine equipment, radio equipment, toys, lifts, pressure equipment, medical devices, personal protective equipment (listed in Annex I to the AI Act). \n\nâ€¢ AI systems listed in Annex III, such as AI used in education, employment, credit scoring, law enforcement, migration, remote biometric identification systems, and AI systems used as a safety component in critical infrastructure. This list can be amended by the Commission. The first category of high-risk systems is covered by both the harmonisation legislation and the AI Act. 61 2 3 4 5 6 7 8 9 10 \n\nProviders have an option of integrating the requirements of the AI Act into the procedures required under the respective Union harmonisation legislation listed in Section A of Annex I. In addition, only selected provisions of the AI Act apply to high-risk AI systems in relation to products covered by Union harmonisation legislation listed in Section B of Annex I (such as aviation equipment). Practical assistance in the classification of high-risk AI systems will be provided no later than 2 February 2026 by the Commission, to include a comprehensive list of practical examples of use cases of high-risk and non-high-risk AI systems. \n\nExceptions to the qualification of high-risk \n\nAI system \n\nIf a high-risk AI system listed in Annex III does not pose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not materially influencing the outcome of decision making, it will not be treated as a high-risk AI system. Such situations may only arise in four cases where the AI system is intended to: \n\nâ€¢ perform a narrow procedural task; \n\nâ€¢ improve the result of a previously completed human activity; \n\nâ€¢ detect decision-making patterns or deviations from prior decision-making patterns, and is not meant to replace or influence the previously completed human assessment without proper human review; or \n\nâ€¢ perform a preparatory task to an assessment relevant for the purposes of the use cases listed in Annex III. If, however, the AI system performs profiling of natural persons, it is always considered a high-risk AI system and cannot fall into one of the above exceptions. This exemption is likely to play an important role in practice, as it allows avoiding the obligations and costs associated with placing a high-risk AI system on the market. One of the options is, for example, to carve out those parts of an AI system that can take advantage of this exemption to limit the scope of the high-risk AI system. However, even if a provider relies on the exemption, its assessment of the system must be documented, and the system must still be registered in the EU database for high-risk systems before it is placed on the market or put into service. \n\nExtensive obligations for high-risk AI systems \n\nThe requirements that must be met by providers of high-risk AI systems are strict. These requirements include, in particular, the need to document every stage of the development of the AI system, to meet obligations regarding the use of high-quality data for training, to produce system documentation that provides users with full information about the nature and purpose of the system, or to ensure the accuracy, robustness and cybersecurity of the systems. High-risk AI systems will also have to be registered in an EU database, which will be publicly available. \n\nObligations across the supply chain of AI systems \n\nThe AI Act imposes obligations on all participants in the supply chain of a high-risk system throughout its life cycle. The responsibilities are not only those of the â€˜providerâ€™, but also those of the â€˜importerâ€™, â€˜distributorâ€™ and â€˜deployerâ€™ of the system, although most of the responsibilities lie with the provider and the deployer. The primary duty of the importer and distributor is to verify that the high-risk AI system being imported or distributed meets the requirements of the AI Act. Moreover, an importer, distributor or deployer may become a provider of the high-risk AI system if they have put their name or trademark on the system, made substantial modifications or they have modified the intended purpose of the AI system, which renders the system high-risk. \n\nGeneral-purpose AI models (see also Chapter 5) \n\nThe distinction between AI models and AI systems is crucial for the application of the AI Act. AI models are essential components of AI systems, but they do not constitute AI systems on their own. AI models require the addition of other components, such as a user interface, to become AI systems. The AI Act mostly regulates AI systems, not models. However, it does contain rules on general-purpose AI models. The AI Act provides rules for all general-purpose AI models and additional rules for general-purpose AI models that pose systemic risks. They apply in the following situations: 71 2 3 4 5 6 7 8 9 10 \n\nâ€¢ where the provider of a general-purpose AI model integrates its own model into its own AI system that is made available on the market or put into service; \n\nâ€¢ where the provider of a general-purpose AI model only offers its own model to providers of AI systems. The distinction may be particularly important in cases where a general-purpose AI model of one provider is used in a general-purpose AI system of a second provider, which in turn is integrated into another AI system with a more specific purpose, built by a third provider. \n\nTransparency obligations (see also Chapter 6) \n\nThe AI Act includes transparency obligations for four types of AI systems: \n\nâ€¢ AI systems designed to interact directly with natural persons; \n\nâ€¢ AI systems, including general-purpose AI systems, that generate synthetic audio, image, video or text content; \n\nâ€¢ emotion recognition or biometric categorisation systems; \n\nâ€¢ AI systems that generate or manipulate images, audio or video that are deepfakes. In all these cases, the user must be informed about the use of the AI system. There are also more detailed obligations, for example to mark the output in a machine-readable way so that it can be identified as artificially generated or manipulated. \n\nComplex supervision and enforcement structure (see also Chapter 8) \n\nThe AI Act provides for a complex, multi-level structure for overseeing implementation. It includes both national and EU level entities. At each level there will be several types of bodies, such as notifying authorities and notified bodies, conformity assessment bodies, the European AI Board, the AI Office, national competent authorities and market surveillance authorities. These authorities will not only control compliance, but also support the market by, among other things, developing codes of conduct, organising AI regulatory sandboxes and providing support for SMEs and start-ups. \n\nRole of technical standards, codes of practice and guidelines (see also Chapters 7, 8 and 9) \n\nThe AI Act requires providers of high-risk AI systems to affix a European Conformity (CE) marking. The CE marking will show compliance with the requirements of the AI Act. For the mark to be issued, providers will have to apply harmonised technical standards. In addition, high-risk AI systems or general-purpose AI models which are in conformity with harmonised standards shall be presumed to be in conformity with the requirements of the AI Act to the extent that those standards cover those requirements or obligations. Consequently, the rather general provisions of the AI Act will be complemented by technical standards that will provide the concrete forms of compliance with the AI Act. Thus, we can expect that the CE marking and technical standards will play very important role in practical application of the AI Act. Codes of practice should also form an important role. If they are not prepared by market participants, the Commission may provide the common rules within implementing acts. The Commission can also, by way of an implementing act, approve a code of practice and give it a general validity within the Union. In addition, the Commission has the obligation to develop several guidelines on the practical implementation of the Regulation. The AI Act can therefore be seen as just a framework for more detailed obligation that will result from many further documents and legal acts. \n\nEnforcement (see also Chapter 8) \n\nThe AI Act stipulates significant penalties \n\nfor non-compliance, which vary depending \n\non the nature of the violation and the size \n\nof the entity involved. Actions that may incur high penalties include: \n\nâ€¢ non-compliance with the rules on prohibited AI practices outlined in article 5. Offenders in such cases may face administrative fines of up to â‚¬35,000,000 or up to 7% of annual worldwide turnover, whichever is higher, for undertakings. 81 2 3 4 5 6 7 8 9 10 \n\nâ€¢ violations related to data, data governance, and transparency: AI systems found in breach of these provisions could be fined up to â‚¬20 million or 4% of annual global turnover. \n\nâ€¢ failure to comply with any of the provisions set out in article 99 (e.g. relating to high-risk AI systems), will be subject to administrative fines of up to â‚¬15 million or, if the offender is a company, up to 3% of its global turnover in the preceding financial year, whichever is higher. These penalties underscore the importance of complying with the AI Actâ€™s regulations. It is essential for companies to fully grasp these penalties and ensure that their AI systems meet the Actâ€™s requirements. \n\n# Timeline \n\nThe AI Act becomes applicable on a staggered basis. There are also transitional arrangements for AI systems that had been placed on the market or put into service before certain dates. The AI Act applies to all operators of high-risk AI systems that have been placed on the market or put into service before 2 August 2026, unless those systems are subsequently subject to significant change in design (in which case, the provisions would apply in full with respect to the redesigned system). The relevant dates of application are set out below. \n\n12 July 2024 The AI Act was published in the Official Journal of the EU, triggering the dates for specific provisions in the Regulation becoming applicable. \n\n2 February 2025 Prohibited practices ban applies (Chapter II). AI literacy rules apply (article 4). \n\n2 May 2025 Codes of practice for general-purpose AI must be ready (article 56 (9)). \n\n2 August 2025 National authorities designated (Chapter III Section 4). Obligations for General-purpose AI (GPAI) (Chapter V). Governance (at EU and national level) (Chapter VII). Confidentiality and penalties (other than in relation to gen-AI) \n\n(Chapter XII). \n\n2 August 2026 Start of application of all other provisions of the EU AI Act (unless a later date applies below). \n\n2 August 2027 High-risk categories listed in Annex I. General-purpose AI models placed on the market before 2 August 2025 (article 111). \n\n2 August 2030 High-risk AI systems (other than those listed below), which have been placed on the market or put into service before 2 August 2026 and which are intended to be used by public authorities (article 111). \n\n31 December 2030 Components of large-scale IT systems listed in Annex X, which have \n\nbeen placed on the market or put into service before 2 August 2027 \n\n(article 111). 91 2 3 4 5 6 7 8 9 10 \n\nIf you or your supply chain fall within the scope of the AI Act, check whether any AI systems or AI models fall within one or more of the regulated categories. If you are a provider or deployer of AI systems within the scope of the AI Act, ensure you have taken steps to comply with the Actâ€™s AI literacy requirements. Determine whether you, your suppliers or your customers will be an operator falling within the material and territorial scope of the AI Act. \n\n# To do list At a glance \n\nCHAPTER 2 \n\n# Material and territorial scope \n\nâ€¢ The AI Act covers AI systems, general-purpose AI models and prohibited AI practices. \n\nâ€¢ Obligations can be imposed on six categories of economic actors: providers, importers, distributors, product manufacturers, authorised representatives and deployers. \n\nâ€¢ Economic operators involved with high-risk \n\nAI systems have significant obligations. Providers and deployers of certain \n\ncategories of AI systems are also subject to transparency obligations. \n\nâ€¢ Providers of general-purpose AI models are subject to obligations. \n\nâ€¢ The AI Act applies when an AI system or general-purpose AI model is placed on \n\nthe EU market, put into service in the EU, imported into or distributed in the EU. It also applies where an AI system is used \n\nby a deployer who has their place of establishment or is in the EU. \n\nâ€¢ Providers and deployers of AI systems who fall within scope of the AI Act are subject to AI literacy requirements from 2 February 2025. 10 1 2 3 4 5 6 7 8 9 10 \n\n# Material scope \n\nThe AI Act primarily provides harmonised rules for the placing on the market, the putting into service, and the use of AI systems. It imposes an extensive set of obligations on â€œhigh-riskâ€ AI \n\nsystems and transparency obligations on certain AI systems. It also prohibits certain AI practices and regulates the supply of general-purpose AI models in the EU. The AI Act also sets out rules for market monitoring, market surveillance, governance and enforcement, which includes administrative fines, as well as measures to support innovation, with a particular focus on small and medium enterprises, such as through the operation of AI sandboxes. It also establishes two new bodies: (i) the European Artificial Intelligence Board â€“ which is tasked with advising and assisting the European Commission and EU Member States to facilitate the consistent and effective application of the AI Act; and (ii) the AI Office, which has been established within the European Commission and is tasked with implementing the AI Act, fostering the development and use of trustworthy AI and promoting international cooperation. \n\nRegulated persons: Operators \n\nThe AI Act imposes obligations on six categories of entities: providers, deployers, importers, distributors, product manufacturers and authorised representatives â€“ the term â€œoperatorâ€ \n\nis used to describe all of them. There will always be a provider for an AI system or a general-purpose AI model. Whether there will also be other operators will depend on the way in which the AI system or general-purpose AI model is being supplied and deployed. Most operators are defined with reference to three \n\nkey terms adapted from the EU product legislation referenced in Annex I of the AI Act: \n\nâ€œmaking availableâ€ , â€œplacing on the marketâ€ and \n\nâ€œputting into serviceâ€ .\n\nâ€œmaking availableâ€ is the supply of an AI system or a general-purpose AI model for distribution or use on the EU market in the course of a commercial activity, whether in return for payment or free of charge; \n\nâ€œplacing on the marketâ€ is the first making available of an AI system or a general-purpose AI model on the EU market; and \n\nâ€œputting into serviceâ€ is the supply of an AI system for first use directly to the deployer or for own use in the EU for its intended purposes. The term â€œuseâ€ is not defined in the AI Act. In essence, â€œuseâ€ would be perceived by reference to the key characteristic of an AI system which is to infer, from inputs it receives, how to generate outputs. These three terms are discussed in section 2.3 of the Commissionâ€™s Guidelines on prohibited AI practices, which provides illustrative examples of each activity in the context of the restrictions on prohibited practices. The regulated operators under the AI Act are: \n\nOperator Role Relevant for both AI systems and general-purpose AI models Provider (article 3(3)) \n\nDevelops an AI system or a general-purpose AI model or has an AI system or a general-purpose AI model developed and places it on the market or puts the AI system into service under its own name or trademark, whether for payment or free of charge. Although the definition of â€œplacing on the marketâ€ refers to the \n\nEU market, a person can still be deemed a provider regulated by the AI Act even if they do not place an AI system on the EU market, where the output of the AI system is used in the EU. See \n\nâ€œTerritorial Scopeâ€ further below. A provider can be a natural or legal person, public authority, agency or other body. EU institutions, bodies, offices and agencies may also act as a provider of an AI system. 11 1 2 3 4 5 6 7 8 9 10 \n\nIt is also possible to become a provider where an AI system has already been placed on the market or put into service in the EU by another provider, by taking one of the steps set out in article 25(1) (a)-(c). See further below, under â€œHigh-risk AI systemsâ€ .\n\nAuthorised representative (article 3(5)) \n\nAn EU-established natural or legal person appointed \n\nby a provider established outside the EU to act as their authorised representative. The role includes ensuring that the documentation required by the AI Act is available to the competent authorities and co-operating with those authorities. See article 22 (for high-risk AI systems) and article 54 (for general-purpose AI models). \n\nRelevant for AI systems only Deployer (article 3(4)) \n\nUses an AI system under its authority (excluding use in the course of personal, non-professional activity). A deployer can be a natural or legal person, public authority, agency or other body. EU institutions, bodies, offices and agencies may also act as a deployer of an AI system. \n\nImporter (article 3(6)) \n\nNatural or legal person located or established in the EU that places an AI system bearing the name or trademark of a person not established in the EU on the EU market. \n\nDistributor (article 3(7)) \n\nNatural or legal person in the supply chain, other than the provider or the importer, that makes an AI system available on the EU market. \n\nProduct manufacturer (article 25(3)) \n\nIn certain circumstances, a product manufacturer will be considered the â€œproviderâ€ of a high-risk AI system where: this is a safety component of a product covered by the AI Act (by virtue of being subject to the EU product safety legislation referenced in Section A of Annex I), and the manufacturer places the AI system on the EU market or puts it into service in the EU together with that product and under its own name or trademark. The term â€œproduct manufacturerâ€ is not defined in the AI Act â€“ but Recital 87 clarifies that this is the â€œmanufacturerâ€ defined under the EU product safety legislation referenced in Annex I to the AI Act. \n\nIndirect obligations under the AI Act \n\nThe AI Act imposes indirect obligations on component suppliers to providers of high-risk \n\nAI systems. Those supplying AI systems, \n\ntools, services, components, or processes that are used or integrated in a high-risk AI system are required to enter into a written agreement with the provider of the high-risk AI system and to enable the latter to comply with its obligations under the AI Act (article 25(4)). This obligation does not apply to third parties who make such tools, services, processes or components (other than general-purpose AI models) accessible to the public under a free and open-source licence. \n\nRights granted by the AI Act \n\nUnlike the GDPR, which provides a comprehensive set of rights to individuals, the rights under the AI Act are limited. The AI Act only confers a right to explanation of individual decision-making on affected persons located in the EU (article 86). Affected persons are those who are subject to a decision which has a legal or similarly significant effect on them and which is based on the output of one of the high-risk AI systems identified in Annex III. The wording used here is similar to that used under the automated decision-making provisions of the GDPR (article 22 GDPR); the scope of the two provisions however is not identical. 12 1 2 3 4 5 6 7 8 9 10 \n\nRegulated subject matter: AI systems \n\nAn AI system is defined broadly in article 3(1) as: â€œ a machine-based system that is designed to operate with varying levels of autonomy, and that may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, \n\nor decisions that can influence physical or \n\nvirtual environmentsâ€ .This definition is intended to align with the definition used by the OECD AI Principles. A key characteristic of AI systems is their capability to infer, i.e. to obtain outputs and to derive models or algorithms, or both, from inputs or data. Instead, traditional software, which executes operations based solely on rules defined by natural persons is not, on its own, considered an AI system. In February 2025, the Commission published guidelines for this definition. These guidelines \n\nprovide further explanations for each aspect of the definition, with a clear emphasis on the â€œability to infer.â€ In a positive sense, the guidelines outline various machine learning approaches that enable this ability. At the same time, they list systems - particularly those primarily based on mathematical or statistical methods - that do not possess this ability and should therefore not fall within the scope of the AI Act. A noteworthy negative example is â€œlogistic regression,â€ which is widely used in the financial sector. An AI system can be used on a standalone basis or as a component of a product, irrespective of whether the AI system is physically integrated into the product or serves the productâ€™s functionality without being integrated into it. Under the AI Act, AI systems fall into the following categories: \n\nâ€¢ high-risk AI systems; \n\nâ€¢ AI systems with transparency risks; and \n\nâ€¢ all other AI systems. An AI system can also form part of a prohibited AI practice. This can be because of certain features of that AI system or because of the way the AI system would be used. \n\nHigh-risk AI systems \n\nSection III of the AI Act regulates high-risk AI systems. These are AI systems that pose a significant risk of harm to the health, safety \n\nand fundamental rights of persons in the EU. \n\nAn AI system may be classified as high-risk in \n\ntwo ways: \n\nâ€¢ Article 6(1): The AI system is used as a \n\nsafety component in a product that is regulated by certain EU product safety legislation (the Union harmonisation legislation listed in Annex I of the AI Act) and is subject to the conformity assessment procedure with a third-party conformity assessment body under such legislation, or constitutes on its own such a product (e.g. an AI system which is used for medical diagnostic purposes will itself be a regulated medical device); or \n\nâ€¢ Article 6(2): The AI system falls within one of the eight categories set out in Annex III of the AI Act â€“ unless the provider can demonstrate and document that such AI system does not pose a significant risk of harm. Most of the obligations regarding high-risk AI systems fall on providers (which includes product manufacturers as we describe further above), whilst a more limited set of obligations is imposed on deployers, on importers and distributors, and where relevant, authorised representatives. See Chapter 4 of this guide for more details. \n\nAI systems with transparency risks \n\nThe AI Act imposes certain transparency \n\nobligations on: \n\nâ€¢ providers of AI systems intended to interact directly with natural persons (article 50(1)); \n\nâ€¢ providers of AI systems generating synthetic audio, image, video or text content (article 50(2)); \n\nâ€¢ deployers of an emotion recognition system or a biometric categorisation system (article 50(3)); and \n\nâ€¢ deployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake (article 50(4)). See Chapter 6 of this guide for more details. 13 1 2 3 4 5 6 7 8 9 10 \n\nAll other AI systems \n\nAll other types of AI systems, which do not fall under the above categories and are not used for prohibited AI practices are not subject to direct legal obligations under the AI Act. Voluntary codes of conduct may be drawn up in future covering this broader category of AI systems and those deploying them (article 95). Providers and deployers may choose to adhere to these codes of conduct. Aside from rules relating to specific categories of AI systems, those qualifying as the provider or deployer of any AI system under the AI Act are required to take AI literacy measures to ensure that their staff and other persons dealing with the operation and use of AI systems on their behalf, have a sufficient level of knowledge, skills and understanding regarding the deployment of AI systems, their opportunities and risks (article 4). This obligation aims to foster the development, operation and use of AI in a trustworthy manner in the EU â€“ however, it is worth noting that this provision refers to voluntary codes of conduct and that administrative fines are not foreseen for failure to comply with the AI literacy obligation. \n\nRegulated subject matter: Prohibited AI practices \n\nThe AI Act prohibits the placing on the market, putting into service and use of AI systems that have certain prohibited features and/or are intended to be used for certain prohibited purposes, e.g. AI systems that create or expand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage. These practices are deemed to be particularly harmful and abusive and contradict EU values and fundamental rights. The prohibited AI practices are listed in article 5 of the AI Act. This list does not affect the prohibitions of AI practices that infringe other EU law (such as data protection, non-discrimination, consumer protection and competition law). See Chapter 3 of this guide for more detail. \n\nRegulated subject matter: general-purpose AI models \n\nA general-purpose AI model is defined in article 3(63) as: â€œ an AI model, including where such an AI model is trained with a large amount of data using self-supervision \n\nat scale, that displays significant generality \n\nand is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications, except AI models that are used for research, development or prototyping activities before they are placed on the market â€\n\nThe AI Act does not provide a definition of an â€œAI model â€; recital 97 notes that although AI models are essential components of AI systems, they do not constitute AI systems on their own and require further components, such as a user interface, to become AI systems. The characteristics of general-purpose AI models are discussed further in recitals 98 and 99. The AI Act regulates general-purpose AI models and imposes additional obligations for general-purpose AI models with systemic risks. The rules apply to providers of general-purpose AI models, once these models are placed on the market: this can be done in various ways, such as through libraries, APIs, as a direct download or as a physical copy. Recital 97 suggests that the rules on general-purpose AI models can also apply when these models are integrated into or form part of an AI system. When the provider of a general-purpose AI model integrates its own model into its own AI system that is made available in the market or put into service, then recital 97 suggests that model will be viewed as being placed on the market and the general-purpose AI model provisions will apply, in addition to those regarding AI systems. \n\n# Where can I find this? \n\nMaterial Scope  article 1  recitals 1-3, 6-8 14 1 2 3 4 5 6 7 8 9 10 \n\nThose who integrate third party general-purpose AI models into their own AI systems are considered â€œdownstream providersâ€ and are granted certain rights under the AI Act. However, the AI Act appears to envisage that a provider who fine-tunes a third party general-purpose AI model and integrates that fine-tuned model into their own AI system (or otherwise places a fine-tuned general-purpose AI model on the market or puts it into service) will be considered the provider of this with respect to that fine-tuning only (see recital109). See Chapter 5 of this guide for more detail. \n\n# Territorial scope \n\nAI System provisions \n\nThe AI Act is intended to have a broad jurisdictional scope for its AI system provisions: these are engaged when an AI system, either on its own or as part of a product covered by the EU product safety legislation in Annex I, is: \n\nâ€¢ placed on the EU market, put into service \n\nin the EU, imported into or distributed in \n\nthe EU; or \n\nâ€¢ used by a deployer who has their place of establishment or is located in the EU. The first point applies applies irrespective of where the provider of the AI system is established. The concept of â€œ establishment â€ is \n\nnot defined in the AI Act. It is expected that this would be interpretated broadly, similar to the use of this term under other EU legislation, such as the GDPR. In addition to those cases, the AI system provisions also apply if the output produced \n\nby an AI system outside the EU is used in the EU. In that case, the non-EU established/ \n\nlocated providers and deployers will also be caught by the scope of the AI Act. Recital 22 clarifies that in those instances the AI Act will apply even though the relevant AI systems are not placed on the market, put into service or used in the EU. \n\nProhibited AI Practices \n\nThe AI Actâ€™s provisions relating to prohibited AI practices apply to the placing on the EU market, putting into service in the EU and use of the relevant AI practices set out in Article 5. As we saw above, the definitions of â€œplacing on the marketâ€ and â€œputting into serviceâ€ refer to the EU \n\nmarket. The AI Act itself does not specify what a prohibited â€œuseâ€ would entail. The Commissionâ€™s Guidelines on prohibited AI practices suggest \n\nthat use â€œshould be understood in a broad manner to cover the use or deployment of the system at any moment of its lifecycle after having been placed on the market or put into serviceâ€ and further that use \n\nâ€œmay also cover the integration of the AI system in the services and processes of the person(s) making use of the AI system, including as part of more complex systems, processes or infrastructure.â€ \n\nGeneral-purpose AI Models \n\nThe AI Actâ€™s general-purpose AI model provisions will be engaged where a provider of a general-purpose AI model places it on the market in the EU or puts it into service in the EU â€“ irrespective of where the provider is located or established. \n\n# Where can I find this? \n\nTerritorial Scope  article 2  recitals 9-11 15 1 2 3 4 5 6 7 8 9 10 \n\n# Exclusions \n\nCertain activities are entirely outside the AI Actâ€™s scope. The AI Act does not apply to: \n\nâ€¢ areas outside the scope of EU law (e.g. activities concerning national security). This is the case irrespective of the type of entity entrusted under national legislation with carrying out the exempted activities. Given the very broad competences of the EU, as set out in the TFEU, this provision will have very limited scope of application in practice; \n\nâ€¢ AI systems placed on the market, put into service, or used with or without modification â€“ or where their output is used in the EU, exclusively for military, defence or national security purposes, regardless of the type of entity carrying out those activities. An AI system placed on the market or put into service for an excluded purpose (military, defence or national security) and one or more non-excluded purposes (e.g. civilian purposes or law enforcement) is subject to the AI Act and providers of those systems should ensure compliance with the AI Act; \n\nâ€¢ public authorities in a third country or international organisations that use AI systems in the framework of international cooperation or agreements for law enforcement and judicial cooperation with the EU or EU member states, provided that such a third country or international organisation provides adequate safeguards for the protection of fundamental rights and freedoms of individuals. The \n\nnational authorities and EU institutions, \n\nbodies, offices and agencies making use of those outputs remain subject to EU law; \n\nâ€¢ AI systems and models, including their output, specifically developed and put into service for the sole purpose of scientific research and development; \n\nâ€¢ research, testing or development of AI systems or models prior to their being placed on the market or put into service, excluding though testing in real world conditions; \n\nâ€¢ deployers who are individuals and use the AI system in the course of a purely personal, non-professional activity. This is similar to \n\nthe GDPRâ€™s â€œhousehold exemptionâ€ â€“ whilst providers of those AI systems continue to be subject to the AI Act; and \n\nâ€¢ AI systems released under free and open-source licences, unless they are placed on the market or put into service as high-risk AI systems, as a prohibited AI system or as a system that is covered by the Actâ€™s transparency obligations. \n\n# Relationship with other regulatory frameworks \n\nâ€¢ As a Regulation, the AI Act is directly applicable in EU Member States without the need for implementing legislation. EU Member States are prevented from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by the AI Act. This is only provided for in limited circumstances: for example, EU member states may introduce more restrictive laws on the use of remote biometric identification systems â€“ some of which constitute prohibited AI practices (article 5(5)) and the use of post-remote biometric identification systems, which constitute high-risk AI systems (article 26(10)). \n\nâ€¢ The AI Actâ€™s provisions on high-risk AI systems are built around the New Legislative Framework for EU products. This is a legislative package that sets out rules for the placing of products on the EU market, enhances market surveillance rules and rules for conformity assessments and CE marking. It also establishes a common legal framework for industrial products in the form of a toolbox of measures for use in future legislation. The AI Act specifies how these tools set out in the New Legislative Framework should apply in the context of AI systems. \n\nâ€¢ In parallel, the AI Act complements Union harmonisation legislation â€“ this is the set of \n\nEU product safety legislation on the basis of which certain AI systems are to be classified \n\nas high-risk. \n\nâ€¢ The obligations of the AI Act apply in addition to and without prejudice to the obligations under GDPR, the e-Privacy Directive and the Law Enforcement Directive. 16 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ Article 5 lists eight prohibited practices \n\nwhich are deemed to pose an unacceptable level of risk. \n\nâ€¢ Prohibitions come into effect on \n\n2 February 2025. \n\nâ€¢ The prohibited practices are: \n\nâ€” Subliminal, manipulative, or deceptive techniques \n\nâ€” Techniques exploiting vulnerable groups in each case which materially distorts behaviour and risks significant harm \n\nâ€” Social scoring in certain use cases \n\nâ€” Predicting criminality based on profiling \n\nâ€” Scraping the web or CCTV for facial recognition databases \n\nâ€” Inferences of emotions at workplaces \n\nor schools \n\nâ€” Biometric categorisation to infer race, political opinion, trade union membership, religious or political beliefs, sex life or sexual \n\norientation \n\nâ€” Real-time remote biometric identification in public spaces for law enforcement purposes. \n\nâ€¢ Many of the prohibitions have exceptions -case by case analysis is needed. \n\nâ€¢ The list is not final: it will be re-assessed annually. \n\nâ€¢ Non-compliance sanctioned by fines up to â‚¬35 million or 7% of total worldwide annual turnover for the proceeding financial year (whichever is higher). \n\nâ€¢ The prohibitions are operator-agnostic \n\nand apply irrespective of the role of the \n\nactor (i.e. whether provider, deployer, distributor or importer). Check for updates to this list annually as the list of prohibited practices may change over time. Consider whether any exceptions apply. The prohibited practices are not absolute; many have exceptions. Check the AI systems you use to see if they fall under the prohibited category. \n\n# To do list At a glance \n\nCHAPTER 3 \n\n# Prohibited AI Practices 17 1 2 3 4 5 6 7 8 9 10 \n\n# Prohibited AI practices \n\nThe AI Act relies on a risk-based approach, so different requirements apply in accordance with the level of risk. This chapter concentrates on prohibited practices i.e. those which conflict with the values of the European Union and are a clear threat to fundamental rights such as freedom, equality and privacy. The prohibitions are an attempt by law makers to respond to transparency and ethics concerns and to guarantee the protection of human rights. The prohibited practices are listed exhaustively in article 5 (and are further explained in recitals 28 â€“ 45 of the Act and by guidelines issued by the Commission on 4 February 2025) and provide a clear framework for what AI can and cannot do within the EU. The prohibitions in Article 5 apply from 2 February 2025 and are therefore the first provisions to come into force, highlighting their importance.The list of prohibited practices in article 5 is exhaustive, but not final. The Commission will assess the need for amendment of the list of prohibited practices annually (article 112) and can submit findings to the European Parliament and Council. So, there may be variations to the list of prohibited practices in due course. There are currently eight prohibited practices, which focus on practices that materially distort peoplesâ€™ behaviour, or raise concerns in democratic societies. Special attention has been given to biometric identification systems. However, there are detailed exceptions to many of the prohibitions and each practice should be considered on a case-by-case basis. \n\nArticle 5(1)(a) Subliminal, manipulative or deceptive techniques \n\nThe first prohibition concerns AI systems deploying subliminal, manipulative or deceptive techniques in cases where: \n\nâ€¢ the techniques either aim to, or actually have, the effect of materially distorting the behaviour of an individual or a group; \n\nâ€¢ by appreciably impairing the ability of individuals to make informed decisions; and \n\nâ€¢ causing them to take decisions they would \n\nnot otherwise have taken, and that either \n\ncause or are reasonably likely to cause them significant harm. The techniques expressly mentioned in recital 29 involve: deployment of subliminal components such as audio, image, video stimuli that persons cannot perceive, or other manipulative or deceptive techniques that subvert or impair a personâ€™s autonomy, decision-making, or free choice, in ways so that people are not consciously aware of those techniques or, where they are aware of them, can still be deceived or are not able to control or resist them. The reference in recital 29 to machine-brain interfaces having the capability to materially distort human behaviour in a significantly harmful manner may also be the Actâ€™s attempt to regulate tools that employ neural data which is currently under discussion in other jurisdictions such as Colorado, California, and Chile. For an AI system to be prohibited, there needs to be a causal link between the deceptive techniques and the significant harm caused. The \n\nthreshold of â€œsignificantâ€ harm was added in the legislative process and makes clear that not all dark patterns would fall under this provision. The provision is open for interpretation and, in particular, the word â€œdeceptiveâ€ will lead to further discussions. According to the Commissionâ€™s guidelines, deceptive techniques could cover presenting false or misleading information with the objective or effect of misleading individuals, if the other requirements of the first prohibition are met. \n\nArticle 5(1)(b) Exploitation of vulnerabilities \n\nThe second category of prohibited AI practices aims to protect vulnerable people. There are three groups: vulnerability due to age, disability, or due to specific social or economic situations. An AI system is only prohibited if it has the objective or the effect of materially distorting the behaviour of an individual and does so in a manner that causes or is likely to cause someone significant harm. An exploitation from a socio-economic perspective does not exist, according to the Commission guidelines on prohibited practices, if the situation may be experienced by any person irrespective of their socio-economic situation (e.g. grievances or loneliness). In such case, however, an exploitation may be covered under Article 5(1)(a) AI Act. 18 1 2 3 4 5 6 7 8 9 10 \n\nAI systems that inadvertently impact socio-disadvantaged groups due to biased training data do not automatically exploit vulnerabilities, as there is no intentional targeting. However, under the Commission guidelines on prohibited practices, if AI providers or deployers are aware that their systems unlawfully discriminate against socio-economically disadvantaged persons and foresee significant harm without taking corrective action, they may still be considered to exploit these vulnerabilities. An exploitation of a personâ€™s economic situation could exist in cases where an AI system is used to find persons in poverty to exploit their weaknesses economically. Organisations using AI systems for marketing and sales should make sure they test their systems against this requirement. The concept of significant harm is common to both subliminal techniques and exploitation of vulnerable groups. In the legislative process, requirements that the harm needed to be physical or psychological were dropped. It seems that a broad approach is intended to be taken to the concept of harm, although recital 29 still gives the examples of important adverse impacts on physical and psychological health, alongside financial interests. The recital also notes that harms can be accumulated over time. This prohibition is not intended to affect lawful medical treatment (e.g. psychological treatment of a mental disease carried out with consent). Recital 29 also implicitly recognises that advertising and some other commercial practices inherently depend on nudging â€“ and states that the intent is not to prohibit common, legitimate and lawful commercial practices, particularly in the field of advertising. Consent can play a crucial role in these scenarios. In persuasive interactions, individuals are aware of the influence attempt and can make choices freely and autonomously. \n\nArticle 5(1)(c) Social scoring \n\nThe third prohibition concerns so-called social scoring, i.e. classifying individuals or groups over a period based on their social behaviour, or known, inferred, or predicted personal characteristics. Social scoring is prohibited in \n\ntwo cases: \n\nâ€¢ if it leads to unfavourable treatment in social contexts that are unrelated to the context in which the data was originally generated; and \n\nâ€¢ if this leads to unfavourable treatment of individuals or groups that is unjustified or disproportionate to their social behaviour or \n\nits gravity. Social scoring is used by several governments around the world. The government in the Netherlands stepped down in 2021 due to a flawed risk-scoring algorithm, which lead to unjustified accusation of fraud for welfare benefits based on personal characteristics and behaviour. The algorithm in that case targeted minorities and people based on their economic situation. Whilst governments might be the first example that comes to mind when thinking about social scoring, the provision is wider and encompasses all social scoring systems in public or private contexts. Many algorithms inherently depend on behavioural scores. However, the AI Act only prohibits those scoring systems resulting in unfavourable treatment in unrelated social contexts. This key restriction targets the consequences of social scoring, preventing unjust outcomes, or discrimination of individuals or groups. The social scoring prohibition under the AI Act therefore depends on the context the data has been obtained from and the context the data is being used. As the Commission guidelines on prohibited practices illustrate, lawful activities, like credit and risk scoring in financial services, are permitted if they improve service quality or prevent fraud. Conversely, an insurance company using spending and other financial data from a bank to set life insurance premiums is provided as an example of unlawful social scoring. \n\nArticle 5(1)(d) Profiling for criminal risk \n\nassessment \n\nThe fourth prohibition is placing on the market, putting into service, or using AI systems that assess or predict the likelihood of a person committing criminal offences based solely on profiling or on assessing the personality traits and characteristics of a person. There is an exception for AI systems used to support human assessment of involvement of a person in a criminal activity, which is based on objective and verifiable facts directly linked to a criminal activity â€“ i.e. detection tools which are factual and supplement, but do not supplant, human decision making. This prohibition aims to avoid the scenario whereby people are treated as guilty for crimes they have not (yet) committed â€“ as illustrated in the film Minority Report . It is tied to human dignity as laid down in article 1 of the Charter of Fundamental Rights. 19 1 2 3 4 5 6 7 8 9 10 \n\nThe Commission guidelines on prohibited practices emphasise that the prohibition can extend to private entities if they act with public authority or assist law enforcement. For instance, a private company analysing data for law enforcement might face prohibition if specific criteria are met. The Commission guidelines also suggest that retrospective human assessments of AI system evaluations can fall outside the scope under certain conditions. This is informed by CJEU case law, which underscores the importance of human review to ensure that AI-driven decisions are based on objective criteria and are non-discriminatory, thus extending beyond the initial exemption in the AI Act. \n\nArticle 5(1)(e) Facial recognition databases \n\nThe fifth prohibited practice is the placing on the market, putting into service for the specific purpose, or use of AI systems to create or expand facial recognition databases through untargeted scraping of facial images from the internet or CCTV footage. Recital 43 considers this practice to add to the feeling of mass surveillance and that it can lead to gross violations of fundamental rights, including the right to privacy. This may be a response to the investigations by supervisory authorities into Clearview AI. The Commission guidelines on prohibited practices regarding facial recognition databases clarify several key points. Such databases can be temporary, centralised, or decentralised, and they fall under Article 5(1)(e), if they can be used for facial recognition, regardless of their primary purpose. Targeted scraping, such as collecting images of specific individuals or using reverse image searches, is allowed, but combining it with untargeted scraping is prohibited. The prohibition does not cover untargeted scraping of other biometric data, like voice samples, or databases not used for recognition, such as those for AI model training without identifying individuals. \n\nArticle 5(1)(f) Inference of emotions in working life and education \n\nThe sixth prohibited practice is the placing on the market, putting into service for this specific purpose, or use of AI systems to infer emotions in workplace or schools, except for safety or medical reasons such as systems intended for therapeutical use. The guidelines clarify that the definition of both the school and workplace should be interpreted widely and in the case of workplace use they should also cover the selection and hiring phases of recruitment. The exception for the safety or medical reasons on the other hand is to be interpreted narrowly. For example, systems measuring burnout or depression in the workplace would not be exempted. Recital 18 distinguishes between emotions or intentions such as happiness, sadness, anger etc. It explains that the notion does not include physical states, such as pain or fatigue (so, systems used in detecting the state of fatigue of professional pilots or drivers for the purpose of preventing accidents would not be affected). It also does not include detection of readily apparent expressions such as a frown or a smile, or gestures such as the movement of hands, arms or head, or characteristics of a personâ€™s voice, such as a raised voice or whispering. However, the guidelines still do not clarify the meaning of â€œintentionâ€ which are also covered by the definition of emotion recognition systems. The AI Act has a defined term of â€œemotion recognition systemâ€ , which means an â€œ AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of biometric data â€. Curiously, article 5(1)(f) does not use this term, and refers to any use of AI systems to infer emotions (i.e. without the requirement that this should be derived from biometric data). However, the Commissionâ€™s guidelines clarified that Article 5(1)(f) should be read as referring to the emotion recognition systems as the defined term under the Act. They further clarified that nonbiometric emotion recognition systems (e.g. text-based) are not prohibited provided they are not used in conjunction with biometric data such as keystroke analysis. The Act references the inaccuracy of biometric emotion recognition systems and their intrusive nature in settings where there is an imbalance of power (such as workplace and schools) as the reason for the prohibition in such settings. However, the AI Act does not explain why it considers non-biometric emotion recognition systems as less intrusive or more accurate than biometric systems. 20 1 2 3 4 5 6 7 8 9 10 \n\nArticle 5(1)(g) Biometric categorisation \n\nThe seventh prohibition is on the use of biometric categorisation systems that categorise individuals based on their biometric data to deduce or infer certain (not all) special category data under the GDPR, namely: race, political opinions, trade union membership, religious or political beliefs, sex life or sexual orientation. Special category data under the GDPR that are not covered in the prohibition are inferences of ethnic origin, health, and genetic data. However, inferring such types of data would likely fall under the high-risk category under Annex III. Additionally, the prohibition does not cover labelling or filtering of lawfully acquired biometric datasets or categorising of biometric data by law enforcement (e.g. sorting of images according to hair colour or eye colour by law enforcement to search for suspects). However as recital 54 suggests that AI systems intended to be used for biometric categorisation according to sensitive attributes or special category data under the GDPR, in so far as they are not prohibited the AI Act, should be classified as high-risk and the guidelines also state that most AI systems that fall under an exception from a prohibition listed in Article 5 AI Act will qualify as high-risk this would suggest that the exempted labelling and filtering systems would fall under the high-risk category. Recital 16 clarifies that biometric categorisation systems do not include purely ancillary features which are linked to another commercial service, where the feature cannot, for objective technical reasons, be used without the main service, and where this is not a circumvention mechanism to evade AI Act rules (e.g. retail try before you buy filters, or social media filters). The guidelines also clarify that the scope of biometric categorisation excludes categorisation according to clothes or accessories, such as scarfs or crosses, or social media activity. \n\nArticle 5(1)(h) Real-time remote biometric \n\nidentification in public spaces \n\nThe eighth and last prohibition is the use \n\nof real-time remote biometric identification systems (â€œRBIâ€) in publicly accessible spaces for law enforcement purposes. RBI systems are AI systems for the purpose of identifying natural persons, without their involvement, typically at a distance, by comparing biometric data with that contained in a reference database. Real-time systems include those where there is a short delay in the comparison. The AI Act does not define how much time amounts to â€œsignificant \n\ndelayâ€ . However, the guidelines suggest that this would likely be the case for when the person is likely to have left the place where the biometric data was taken and not allow for a quick reaction from the law enforcement. Biometric systems used for verification (i.e. confirming that someone is who they claim to be, to access a service, a device, or to have security access to premises) are distinguished from RBI and so not covered by this prohibition (recital 15). The guidelines clarify that the distinction between the identification and verification comes from the active involvement of the individual in the process which may have minor impact on fundamental rights of natural persons. For active involvement, however, it is not sufficient that persons are informed about the presence of cameras, but they need to step actively and consciously in front of a camera that is installed in a way fostering active participation. The AI Act allows (but does not require) member states to permit use of RBI for law enforcement purposes in limited situations where the use of RBI is strictly necessary for: \n\nâ€¢ targeted searches for specific victims of abduction, human trafficking, or sexual exploitation as well as searching for \n\nmissing persons; \n\nâ€¢ the prevention of a specific, substantial, and imminent threat to the life or physical safety, \n\nor a genuine and present or foreseeable threat \n\nof terrorist attack; or \n\nâ€¢ the localisation or identification of a person suspected of having committed a criminal offence, conducting a criminal investigation, prosecution or executing a criminal penalty for serious offences â€“ being those referred to in Annex II and punishable in the Member State concerned by a prison sentence for a maximum period of at least four years. The exemptions only permit RBI used to confirm the identity of the specifically targeted individual. In addition, use of RBI should consider the nature of the situation, in particular the seriousness, probability, and scale of the harm that would be caused if the system were not used, against the consequences of use on the rights and freedoms of the persons concerned. 21 1 2 3 4 5 6 7 8 9 10 \n\nFurther, protections include the need to complete a fundamental rights assessment, registration of the system in an EU database in line with article 49, and prior authorisation of each use case by judicial or administrative authority (subject to urgency measures). In addition, each use of RBI in publicly accessible spaces must be notified to the relevant market surveillance authority and the national data protection authority. The national authorities must then report to the European Commission which, \n\nin turn, prepares an annual state of the nation \n\nreport on usage of RBI in accordance with these provisions. \n\n# To whom do the prohibitions apply? \n\nAs set out in Chapter 2, the AI Act distinguishes between different actors involved in AI systems, attributing specific responsibilities based on their role in relation to the AI model or system. This method ensures that those who have the most influence over the development and implementation of AI technologies adhere to the highest standards. However, the rules on prohibited practices are operator-agnostic. In other words, they apply universally, independent of the specific role of the actor (i.e. whether they are involved in the provision, development, deployment, distribution, or use of AI systems engaging in prohibited practices). This wide-ranging application highlights the Actâ€™s dedication to stopping practices that could infringe on fundamental rights or present intolerable risks, emphasising a comprehensive approach to regulation that covers all types of interaction with harmful AI technologies. \n\n# Enforcement and fines \n\nWhen a practice is prohibited, the AI system in question may not be used in the EU. In the case of an infringement, competent authorities may issue a fine of up to 7% of the total worldwide annual turnover of the offender for the preceding financial year or 35 million EUR, whichever is higher. National market surveillance authorities will be responsible for ensuring compliance with the AI Actâ€™s provisions regarding prohibited AI systems. They will report to the European Commission annually about use of prohibited practices that occurred during the year and about the measures they have taken. 22 1 2 3 4 5 6 7 8 9 10 \n\n# Where can I find this? \n\nSubliminal, manipulative or deceptive techniques  article 5(1)(a)  recitals 28 & 29 \n\nExploitation of vulnerabilities  article 5(1)(b)  recitals 28 & 29 \n\nSocial scoring  article 5(1)(c)  recital 31 \n\nProfiling for criminal risk assessment  article 5(1)(d)  recital 42 \n\nFacial recognition database  article 5(1)(e)  recital 43 \n\nInference of emotions in working life and education  article 5(1)(f)  recitals 44 - 45 \n\nBiometric categorisation  article 5(1)(g)  recital 30 \n\nReal-time remote biometric identification in public spaces  article 5(1)(h)  recitals 32 - 41 \n\nOther useful resources \n\nâ€¢ Commission guidelines on prohibited artificial intelligence practices established by Regulation (EU 2024/1689 (AI Act) \n\nâ€¢ ETHICS GUIDELINES FOR TRUSTWORTHY AI: High-Level Expert Group on Artificial Intelligence (2019) \n\nâ€¢ EDPB Guidelines on Processing Personal Data Through Video Devices \n\nâ€¢ EDPB Guidelines on Use of Facial Recognition Technology In The Area of Law Enforcement \n\nâ€¢ EDPB Guidelines on Automated Decision Making and Profiling \n\nâ€¢ EDPB-EDPS Joint Opinion On The Proposal For The Artificial Intelligence Act \n\nâ€¢ EDPB guidelines on Deceptive Design Patterns in Social Media \n\nâ€¢ Guidelines on dark patterns from the Finnish Market Authority 23 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ AI systems fall within the scope of â€œhigh-riskâ€ if \n\nthey are intended to be used as: \n\nâ€” products, or safety components of products, which must undergo third-party conformity assessment pursuant to the legislation covered by Annex I; or \n\nâ€” for one of the purposes described in \n\nAnnex III. \n\nâ€¢ Providers, deployers, importers, distributors and suppliers to providers of high-risk AI systems have obligations under the AI Act. Market parties can have multiple roles in parallel and need to comply with multiple sets of obligations simultaneously. \n\nâ€¢ Providers of high-risk AI systems have the heaviest compliance burden and need to carry out a conformity assessment before the system can be placed on the market or put into service. \n\nâ€¢ Itâ€™s possible to become the provider of a \n\nhigh-risk AI system (e.g. by placing your own name/trademark on the system, making a substantial modification, or using the system for different purposes than intended by the original provider). Determine your role in the value chain (provider, deployer, importer, distributor, or third-party supplier) and review the corresponding obligations. Determine whether the AI system falls within the scope of high-risk as meant \n\nin article 6, in conjunction with Annexes I and III. \n\n# To do list At a glance \n\nCHAPTER 4 \n\n# High-risk AI systems 24 1 2 3 4 5 6 7 8 9 10 \n\n# Classification of an AI system as a high-risk AI system \n\nâ€¢ marine equipment \n\nâ€¢ rail systems \n\nâ€¢ motor vehicles and their trailers \n\nâ€¢ unmanned aircraft Note that the legislation in Annex I covers the categories above, but can also cover related products. For example, the Machinery Regulation covers lifting accessories and removable mechanical transmission devices as well as machinery. Itâ€™s also the core regulation for robotics, another steadily growing area of AI adoption for which the AI Act and its high-risk requirements will become highly relevant. Safety components fulfil a safety function for a product, where their failure or malfunction would endanger the health and safety of persons or property. You should make an assessment pursuant to the applicable product safety regulation in Annex I to see whether the AI system would have to undergo third-party conformity assessment pursuant to that legislation. For example, in the Medical Device Regulation, medical devices in class IIa and higher are subject to the third-party conformity procedure. If an AI-system qualifies as a safety component of such a medical device, or if it constitutes such a medical device itself, it is a high-risk AI system pursuant to the AI Act. Some of the legislation covered in Annex I also uses terms such as â€œhigh-riskâ€ and â€œmedium-riskâ€ .However, these categories are independent from the classification as high risk under the AI Act. For example, under applicable product safety legislation a product can be classed as â€œmedium-riskâ€ , but if the product has to to undergo third-party conformity assessment, then an AI system that is a safety component of that product, or that itself constitutes such a product, will be high-risk under the AI Act. \n\nCategory B: Annex III systems \n\nThe stand-alone list of high-risk systems currently contains: \n\nâ€¢ Biometrics: remote biometric identification of individuals, biometric categorisation of individuals and/or emotion recognition \n\nof individuals. The main part of the AI Act regulates high-risk \n\nAI systems. These are AI systems that can have \n\na significant harmful impact on the health, \n\nsafety and fundamental rights of persons in \n\nthe EU. There are two main categories of high-risk AI systems: a.  systems which are intended to be used as safety components of products or systems, or which are themselves products or systems, falling within the scope of Union harmonisation legislation listed in Annex I, if required to undergo a third-party conformity assessment pursuant to this legislation; and b.  systems whose intended purpose falls within the scope of the use cases set out in Annex III of the AI Act. \n\nCategory A: Annex I systems \n\nRegarding the first category (a), the product safety legislation listed in Annex I covers the following categories: \n\nâ€¢ machinery \n\nâ€¢ toys \n\nâ€¢ recreational craft and personal watercraft \n\nâ€¢ lifts/elevators \n\nâ€¢ equipment and protective systems for potentially explosive atmospheres \n\nâ€¢ radio equipment \n\nâ€¢ pressure equipment \n\nâ€¢ cableway installations \n\nâ€¢ personal protective equipment \n\nâ€¢ appliances burning gaseous fuels, \n\nmedical devices \n\nâ€¢ in vitro diagnostic medical devices \n\nâ€¢ civil aviation \n\nâ€¢ 2/3-wheel vehicles \n\nâ€¢ agricultural and forestry vehicles 25 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ Management and operation of critical infrastructure: to directly protect physical integrity or health and safety of individuals and property in relation to the management and operation of critical digital infrastructure (e.g., internet exchange points, DNS services, TLD registries, cloud computing services, data centres, content delivery networks, trust service providers, electronic communication networks or services), or in the supply of water, gas, heating, or electricity. \n\nâ€¢ Education and vocational training : decision-making in education and vocational training (e.g. selection, evaluation, assessment and monitoring of students or individuals applying to be students). \n\nâ€¢ Recruitment and HR : decision-making in recruitment and HR (e.g. selection, evaluation, assessment, promotion, termination, task allocation and monitoring of employees and/ or other workers and/or applicants). \n\nâ€¢ Essential services: evaluating the (continued) eligibility of individuals for public assistance benefits (e.g. healthcare services, social security allowances, disability benefits); evaluating creditworthiness of individuals \n\nor establishing their credit score (with the exception of the detection of financial fraud); risk assessment and pricing in relation to individuals in the case of life and health insurance; and evaluating and classifying emergency calls or making decisions in relation to dispatching or prioritisation of the dispatching of emergency first response services (e.g. police, firefighters, medical aid); and emergency healthcare patient triage. \n\nâ€¢ Crime analytics: assessment by/on behalf of/ in support of law enforcement authorities: (i) of the risk of individuals of becoming a victim or (re-)offender; (ii) of personality traits and characteristics; (iii) of past criminal behaviour of individuals or groups; or (iv) consisting of profiling of persons, in the course of the detection, investigation or prosecution of criminal offences. \n\nâ€¢ Evidence gathering and evaluation: \n\nevaluation of reliability of evidence during the investigation or prosecution of criminal offences, or in the course of applications for asylum, visa or residence permits, or with regard to associated complaints; use of polygraphs or similar tools by/on behalf of/ in support of law enforcement authorities or authorities conducting migration, asylum and/ or border control. \n\nâ€¢ Immigrant identification, migration risk and \n\nmigration application assessment: detecting, recognising or identifying individuals (with the exception of verification of travel documents) in the context of migration, asylum or border control management; assessment of risk (e.g. security risk, risk of irregular migration or health risk) posed by individuals who intend to enter or have entered the territory of an EU country and examination of applications for asylum, visa or residence permits and for associated complaints. \n\nâ€¢ Administration of justice: assisting judicial authorities or alternative dispute resolution institutions in researching and interpreting facts and the law and in applying the law to facts. \n\nâ€¢ Democratic processes: influencing the outcome of an election or referendum or voting behaviour of individuals. Note that Annex III may be amended by the Commission (article 7). The intended purpose is defined in article 3(12) as: â€œ the use for which an AI system is intended by the provider, including the \n\nspecific context and conditions of use, as specified in the information supplied by \n\nthe provider in the instructions for use, promotional or sales materials and statements, as well as in the technical documentation.â€ \n\nExceptions: not sufficiently high-risk \n\nArticle 6(3) provides that AI systems whose intended purpose falls within the scope of Annex III, so that they would (absent this provision be high-risk) shall nonetheless not be considered as high-risk if they do not pose a significant risk of harm to the health, safety or fundamental rights of natural persons. The article mentions four criteria. The exemption can be relied upon if one or more of these criteria are fulfilled (article 6(3) and recital 53): \n\nâ€¢ the AI system is intended to perform a narrow procedural task; \n\nâ€” Example: a system which transforms unstructured data into structured data or a system which detects duplicates of documents 26 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ the AI system is intended to improve the result of a previously completed human activity; \n\nâ€” Example: a system which improves the professional tone or academic style of language used in already drafted documents \n\nâ€¢ the AI system is intended to detect decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment, without proper human review; or \n\nâ€” Example: a system which checks flags inconsistencies or anomalies in the grades applied by a teacher, when compared with an existing grading pattern for that teacher \n\nâ€¢ the AI system is intended to perform a preparatory task to an assessment relevant for the purpose of the use cases listed in Annex III \n\nâ€” Example: a system for translating documents. The exception does not apply if the AI system involves profiling of natural persons within the meaning of article 4(4) of Regulation (EU) 2016/679 (GDPR) or article 3 (4) of Directive (EU) 2016/680 (Data Protection Enforcement Directive) or article 3, (5) of Regulation (EU) 2018/1725 (Data Protection for EU institutions) (recital 53). Companies deciding to make use of this exemption should note that they carry the burden of proof as to whether the system is high-risk. The assessment under article 6(3) must be documented before the system is placed on the market or put into service and the system must be registered (articles 49(2) and 6(4)). Providers of such systems must provide this documentation to national competent authorities on request. The Commission will provide guidelines specifying the practical implementation of article 6, including a comprehensive list of practical examples of high-risk and non-high-risk use cases of AI systems (article 6(5)). It may also adopt delegated acts adding to or modifying the criteria for article 6(3). The guidelines are expected to be published within six months after entry into force of the AI Act. \n\n# Obligations for providers of high-risk AI systems \n\nThe AI Act provides a detailed list of obligations for providers and deployers of high-risk AI systems as follows in Chapter III, Sections 2, 3 and 4: \n\nObligations for providers on high-risk AI systems Requirements of Section 2 Ensure compliance with requirements of Section 2 (see below). \n\nName of provider and contact information \n\nIndicate on the system (or, if not possible, on its packaging or accompanying documentation) the name of the provider or its brand and its contact information. \n\nQuality management system \n\nHave a quality management system complying with article 17. (Article 17 provides a detailed list of aspects of the system to be documented through policies, procedures and instructions). \n\nDocumentation \n\nKeep the documentation referred to in article 18. The documentation \n\nwill include: \n\nâ€¢ technical documentation (article 11) \n\nâ€¢ documentation concerning the quality management system (article 17) \n\nâ€¢ documentation concerning changes approved by notified bodies, \n\nwhere applicable \n\nâ€¢ decisions and other documents issued by notified bodies, where applicable \n\nâ€¢ the EU declaration of conformity (article 47). 27 1 2 3 4 5 6 7 8 9 10 \n\nLogs \n\nIf the system is under their control, keep logs automatically generated by the system (article 19). Such logs must be kept for a period appropriate to the intended purpose of the high-risk AI system. The period should be at least six months (unless any personal data protection provisions state otherwise). \n\nConformity Assessment \n\nEnsure that the system undergoes the relevant conformity assessment procedure in article 43, prior to being placed on the market or put into service (see below). \n\nDeclaration of conformity \n\nDraw up an EU declaration of conformity (article 47). See below. \n\nCE marking \n\nAffix the CE marking to the high-risk AI system (or, if not possible, on its packaging or accompanying documentation). The CE marking will confirm the conformity of the high-risk AI system with the AI Act as per article 48. See below. \n\nRegistration obligation \n\nComply with EU Database registration obligations (article 49(I)). See below. \n\nCorrective actions / provision of information \n\nIn cases where the system is not in conformity with the AI Act, take the necessary corrective actions, or withdraw, disable, or recall it. Where the system presents a risk to safety, or the fundamental rights \n\nof persons, inform the competent market surveillance authorities \n\nand, where applicable, the notified body that issued a certificate for \n\nthat system (article 79). \n\nDemonstration of conformity \n\nUpon a reasoned request of a national competent authority, demonstrate the conformity of the system with the requirements set out in Section 2 (see above), providing all necessary information and documentation. The duties relating to cooperation with competent authorities are set out in more detail in article 21. Any information shared with a national competent authority shall be treated as confidential. \n\nAccessibility requirements \n\nEnsure the high-risk AI system complies with accessibility requirements in accordance with: \n\nâ€¢ Directive (EU) 2016/2102 (on the accessibility of the websites and mobile applications of public sector bodies); and \n\nâ€¢ Directive (EU) 2019/882 (on the accessibility requirements for products and services). 28 1 2 3 4 5 6 7 8 9 10 \n\n# Harmonised standards and conformity assessment procedure for providers of high-risk AI systems \n\nHarmonised standards \n\nHarmonised standards will be published in the Official Journal of the European Union. If the AI system complies with these standards, there will be a presumption of conformity with the requirements for high-risk AI systems in Chapter III, Section 2 (article 40(1). Harmonised standards are highly relevant in practice. Under traditional product safety laws, â€˜manufacturersâ€™ usually follow them to demonstrate compliance with product safety law requirements. This will be similar under the AI Act. The European Commission issued a (draft) \n\nstandardisation request in accordance with article 40(2) to standardisation bodies CEN/CELENEC, requesting these bodies to draft harmonised standards covering the requirements of Chapter III, Section 2 by 30 April 2025. \n\nConformity assessment procedure \n\nThe conformity assessment procedure for high-risk AI systems under article 43 requires providers to demonstrate compliance with the requirements for high-risk AI systems in Section 2 of Chapter III (overview below). \n\nAnnex III high-risk AI systems \n\nHere, the AI Act outlines two primary procedures for conformity assessment. Most providers of high-risk AI systems in Annex III (i.e. those referred to in points 2 to 8 of Annex III), must follow the internal control procedure specified in Annex VI, without involving a notified body. Providers of high-risk AI systems listed in point 1 of Annex III (biometrics), who have applied harmonised standards or common specifications, as referenced in articles 40 and 41 must also follow the internal control procedure sufficient. However, for providers of high-risk biometric systems who have not done this theinvolvement of a notified body is required. \n\nAnnex I high-risk AI systems \n\nIf a high-risk AI system falls under Union harmonisation legislation listed in Section A of Annex I, the conformity assessment procedures from those legal acts apply. The high-risk AI system requirements of Section 2 in Chapter III are integrated into this assessment, and specific provisions of Annex VII also apply. Notified bodies under these legal acts must comply with certain requirements of the AI Act, to ensure consistent oversight. \n\nNew conformity assessments for substantial \n\nmodifications \n\nSubstantial modifications to high-risk AI systems necessitate a new conformity assessment. However, changes that form part of the systemâ€™s predetermined learning process do not count as substantial modifications. \n\nRequirements for high-risk AI systems Focus on Articles 8-15; requirements for high-risk AI systems Compliance with the requirements (Article 8) \n\nArticle 8 emphasises that high-risk AI systems must meet technical and organisational requirements (articles 9-15) throughout their life cycle, considering the intended use and the status of the technology. Itâ€™s crucial to prioritise requirements impacting humans and if suitable trade-offs are not found, the AI system should not be deployed. \n\nRisk management (Article 9) \n\nArticle 9 requires providers to establish a risk management system. This is an ongoing process to identify, analyse, and mitigate foreseeable risks, including designing risk reduction measures, implementing controls, and providing user information and training. The measures taken must be documented and high-risk AI systems tested at appropriate stages to ensure consistent performance. 29 1 2 3 4 5 6 7 8 9 10 \n\nData governance (Article 10) \n\nRobust data governance is a critical component of the technical and organisational requirements for high-risk AI systems. High-quality, representative, and to the best extent possible error-free and complete training, validation, and testing datasets are required to ensure proper functioning and safety of the system. Providers must also take measures to mitigate biases in datasets that could lead to prohibited discrimination, including by processing special categories of personal data under specific conditions. Certified third-party services can be employed for data integrity verification and to demonstrate compliance with the AI Actâ€™s data governance requirements. \n\nTechnical documentation and record keeping (Articles 11 and 12) \n\nArticles 11 and 12 necessitate detailed technical documentation and record-keeping logs throughout the systemâ€™s lifecycle. Providers must prepare this before deployment and regularly update it. It should cover all aspects of the system, including its characteristics, algorithms, data, training, testing, validation, and risk management. High-risk AI systems should also automatically record usage logs to provide traceability and identify potential risks or needed modifications. \n\nTransparency and provision of information (Article 13) \n\nArticle 13 mandates clear, comprehensive instructions for deployers of high-risk AI systems. These instructions should enable deployers to understand and use the systemâ€™s outputs correctly. The systemâ€™s decision-making must be understandable, and details on its identity, characteristics, limitations, purpose, accuracy, risks, capabilities, oversight, maintenance, and expected lifespan must be provided. All documentation should be tailored to the needs and knowledge level of the intended deployers. \n\nHuman oversight (Article 14) \n\nHuman oversight measures must prevent or minimise risks to health, safety, and rights. These measures must be proportionate to the systemâ€™s risks and level of autonomy. Human operators should also be able to override the system if necessary. Oversight can be achieved through: \n\nâ€¢ Built-in system constraints and responsiveness to human operators. \n\nâ€¢ Provider-identified measures for deployers to help them make informed, autonomous decisions. \n\nâ€¢ Oversight approaches can include human-in-the-loop, human-on-the-loop, or human-in-command, depending on the applicationâ€™s risks. \n\nAccuracy, robustness and cybersecurity (Article 15) \n\nArticle 15 mandates that high-risk AI systems must achieve suitable accuracy, robustness, and cybersecurity levels. Accuracy measures include minimising prediction errors, robustness measures ensure systems can handle errors and inconsistencies. Lastly, cybersecurity measures shall protect against unauthorised system alterations in which case compliance can be demonstrated through the EU Cyber Resilience Act for relevant AI systems subject to the EU Cyber Resilience Act. 30 1 2 3 4 5 6 7 8 9 10 \n\n# Obligations for deployers of high-risk AI systems \n\nThe AI Act provides for obligations for deployers of high-risk AI systems (article 26): \n\nTechnical and organisational measures \n\nDeployers must take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems. \n\nHuman oversight \n\nDeployers must assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support. \n\nInput data \n\nWhere the deployer exercises control over input data, that deployer must ensure that the input data is relevant and sufficiently representative. In other words, this principle states the deployerâ€™s responsibility as to the quality of the input data. \n\nMonitoring high-risk AI system \n\nDeployers must monitor the operation of the high-risk AI system based on the instructions for use. Deployers must inform providers in accordance with article 72 relating to post-marketing activities. If the deployer identifies a risk per article 79(1) it will immediately inform the provider, and then the importer or distributor and the relevant market surveillance authorities and suspend the use of that system. If a serious incident is identified, deployers must also immediately inform the provider, and then the importer or distributor and the relevant market surveillance authorities of that incident. \n\nLogs \n\nDeployers of high-risk AI systems must keep logs automatically generated by that high-risk AI system where these logs are under their control, for a period appropriate to the intended purpose of the high-risk AI system. This period is at least six months, unless provided otherwise in applicable Union or national law, in particular on the protection of personal data. \n\nInformation to the workersâ€™ representatives \n\nDeployers who are employers must inform workersâ€™ representatives and the affected workers that they will be subject to the use of the high-risk AI system. \n\nPublic authority deployers \n\nDeployers of high-risk AI systems who are public authorities, or Union institutions, bodies, offices or agencies must comply with the EU Database registration obligations under article 49. \n\nData protection impact assessment \n\nIf deployers of high-risk AI systems are required to perform a data protection impact assessment under article 35 of Regulation (EU) 2016/679 (GDPR) or article 27 of Directive (EU) 2016/680 (Data Protection Enforcement Directive), they must make use of the information provided by the provider under article 13 of the AI Act. \n\nInvestigation for criminal offences â€“ high-\n\nrisk AI system for post-remote biometric \n\nidentification \n\nWithout prejudice to Directive (EU) 2016/680 (Data Protection Enforcement Directive), in the framework of an investigation for the targeted search of a person suspected or convicted of having committed a criminal offence, someone who wishes to deploy a high-risk AI system for post-remote biometric identification must request an authorisation for this use, ex-ante, or without undue delay and no later than 48 hours, from a judicial authority or an administrative authority. \n\nFundamental rights impact assessment for high-risk AI systems \n\nPrior to deploying a high-risk AI system referred to in article 6(2) (i.e. high-risk AI systems detailed in Annex III of AI Act), deployers that are: I.  bodies governed by public law, or II.  private entities providing public services, and in each case are III.  deployers of high-risk AI systems intended \n\nto be used 31 1 2 3 4 5 6 7 8 9 10 \n\na.  to evaluate the creditworthiness of natural persons or establish their credit score (apart from AI systems used for the purpose of detecting financial fraud), and b.  for risk assessment and pricing in relation to natural persons in the case of life and health insurance must perform an assessment of the impact of the use of the system on fundamental rights (FRIA) . There is an exception for high-risk AI systems relating to critical infrastructure. The assessment consists of: \n\nâ€¢ a description of the deployerâ€™s processes in which the high-risk AI system will be used in line with its intended purpose; \n\nâ€¢ a description of the time period within which, and the frequency with which, each high-risk AI system is intended to be used; \n\nâ€¢ the categories of natural persons and groups likely to be affected by its use in the specific context; \n\nâ€¢ the specific risks of harm likely to have an impact on the categories of natural persons or groups of persons identified pursuant to point above, considering the information given by the provider pursuant to article 13; \n\nâ€¢ a description of the implementation of human oversight measures, according to the instructions for use; and \n\nâ€¢ the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal governance and complaint mechanisms. \n\n# Obligations for other parties in connection with high-risk AI systems \n\nMost obligations regarding high-risk systems in the AI Act are directed at providers and deployers. However, there are also a limited set of obligations for other parties: namely, importers and distributors of high-risk AI systems, and suppliers of any systems, tools, services, components or processes which are used or integrated in high-risk AI systems. Examples of services by suppliers include model (re)training, testing and evaluation and integration into software (recital 88). The obligations do not apply to suppliers that offer the relevant product or service under a free and open-source licence (article 25(4)). Additionally, \n\nit is possible for parties other than the original \n\nprovider of an AI system to be assigned the role of provider of a high-risk AI system by the AI Act. 32 1 2 3 4 5 6 7 8 9 10 \n\nImporters (article 23) Distributors (article 24) Suppliers (article 25) \n\nVerification: before placing the system on the market, verifying that the provider \n\nhas genuinely: \n\nâ€¢ carried out the conformity assessment procedure; \n\nâ€¢ drawn up the technical documentation; \n\nâ€¢ affixed the CE marking and has attached the EU declaration of conformity; \n\nand \n\nâ€¢ appointed an authorised \n\nrepresentative. \n\nVerification: before making the system available on the market, verifying that: \n\nâ€¢ it bears the CE marking; \n\nâ€¢ it is accompanied by a copy of the EU declaration of conformity and instructions for use; and \n\nâ€¢ the provider and the importer, as applicable, have complied with their respective obligations. \n\nProvide assistance: by written agreement, specifying the necessary information, capabilities, technical access and other assistance based on the generally acknowledged \n\nstate of the art, in order to \n\nenable the provider of the high-risk AI system to fully comply with their obligation. The AI Office/Commission may also develop and recommend voluntary model contractual terms between providers of high-risk AI systems and their third-party suppliers (article 25(4)) and recital 90). \n\nRisk flagging: inform the provider, the authorised representative and the market surveillance authority when the system presents a risk 1 to \n\nhealth, safety or fundamental rights of persons. \n\nRisk flagging : not make the system available when the distributor considers or has reason to consider that the system is not in conformity with the requirements set out in Section 2, until the system has been brought into conformity, and where the system presents a risk to health, safety or fundamental rights of persons, immediately inform the provider or the importer of the system and the competent authorities, giving details, in particular, of the non-compliance and of any corrective actions taken. \n\nCare: ensure that storage or \n\ntransport conditions do not jeopardise compliance with the requirements in Section 2. \n\nCare: ensure that storage or \n\ntransport conditions do not jeopardise compliance with the requirements in Section 2. 1.  Risk here means: â€œhaving the potential to affect adversely health and safety of persons in general, health and safety (...) to a  \n\n> degree which goes beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the product concerned, including the duration of use and, where applicable, its putting into service, installation and maintenance requirementsâ€ (article 79(1) AI Act in conjunction with Article 3(19) of Regulation (EU) 2019/1020 (Market surveillance regulation).\n\nObligations for importers, distributors and suppliers \n\nArticles 23, 24 and 25 set out the obligations for importers, distributors and suppliers: 33 1 2 3 4 5 6 7 8 9 10 \n\nImporters (article 23) Distributors (article 24) Suppliers (article 25) Cooperation with authorities: upon a \n\nreasoned request, provide competent authorities with all necessary information/ documentation, including technical documentation, to demonstrate conformity of the system and cooperate with these authorities in any action they take in relation to the system. \n\nCooperation with authorities: upon a reasoned \n\nrequest, provide competent authorities with all necessary information/documentation \n\nregarding their obligations \n\nin the rows above to demonstrate the conformity of that system, and cooperate with these authorities in any action they take in relation to the system. \n\nRecord keeping: keep, for a period of ten years after the system has been placed on the market/put into service, a copy of: the certificate issued by the notified body (in the event of third-party conformity assessment), the instructions for use and the EU declaration of conformity. \n\nContact details: indicate name, registered trade name or registered trademark and the address at which the importer can be contacted on the system and its packaging or accompanying documentation. \n\nCorrective actions: take the corrective actions necessary to bring the system into conformity, where the distributor considers or has reason to consider the system not to be in conformity with the requirements set out in Section 2, or withdraw or recall the system, or ensure that the provider, the importer or any relevant operator, as appropriate, takes those corrective actions. 34 1 2 3 4 5 6 7 8 9 10 \n\nBecoming a provider of someone elseâ€™s (high-risk) AI system \n\nArticle 25(1) provides that a person will be considered the provider of a high-risk AI system, even if that person was not originally the provider of the AI system, when that person: \n\nâ€¢ places their name or trademark on a high-risk AI system which is already placed on the market or put into service; \n\nâ€¢ makes a substantial modification 2 to an \n\nexisting high-risk AI system in such a way that it remains high-risk; and/or \n\nâ€¢ modifies the intended purpose of an AI system of an AI system which is not currently high-risk so that it becomes high-risk. If any of these three situations occur, the original provider will no longer be considered the provider of the (new or newly used) AI-system. One situation which often occurs in practice \n\nthat could lead to such switching of provider roles is the deployment of a general-purpose \n\nAI system by a deployer in a way that falls \n\nwithin the high-risk category as set out in article 6 (and Annexes I and III). As such, if a person deploys a general-purpose AI system in a high-risk way, that deployer assumes the responsibilities of a provider. The new provider will assume all the obligations of a provider of a high-risk AI system. The original provider is obliged to closely cooperate with the new provider and make available the necessary information and provide reasonably expected technical access and other assistance to the new provider to bring the system into conformity with the AI Act (article 25(2)). If, however, that original provider had â€clearly specifiedâ€ that the AI system was not to be changed into a high-risk AI system (article 25(2)) or â€œexpressly excluded the change of the AI system into a high-risk AI systemâ€ (recital 86), for example by prohibiting deployment for high-risk purposes in the applicable contract(s), then that original provider is not obligated to do 2.  A â€˜substantial modificationâ€™ is defined in article 3(23) as â€œa change to an AI system after its placing on the market or putting into service which is not foreseen or planned in the initial conformity assessment carried out by the provider and as a result of which \n\n> the compliance of the AI system with the requirements set out in Chapter III, Section 2 is affected or results in a modification to the\n> intended purpose for which the AI system has been assessedâ€ . The Commission will provide further guidelines on the practical implementation of the provisions related to substantial modification (Article 96(1)(c)). Recital 84 also provides that provisions established in certain Union harmonisation legislation based on the New Legislative Framework, such as the Medical Device Regulation, should continue to apply. For example, article 16(2) of the Medical Device Regulation provides that certain changes should not be modifications of a device that could affect its compliance with the applicable requirements, and these provisions should continue to apply to high-risk AI systems which are medical devices within the meaning of the Medical Device Regulation.\n\nthis. If high-risk deployment is not prohibited, then the co-operation obligation applies, but is without prejudice to the need to observe and protect intellectual property rights, confidential business information and trade secrets (article 25(5)). As such, the original provider does not have to help to the extent that it compromises their own intellectual property rights or trade secrets (recital 88). The Commission will provide guidelines on the application of the requirements and obligations referred to in this article 25 (article 96(1)(a)). 35 1 2 3 4 5 6 7 8 9 10 \n\nScope of high-risk systems article 6, Annexes I \n\nand III \n\nrecitals 46-63 \n\nRequirements for providers of high-risk AI systems articles 8-22, 43, \n\n47-49 \n\nrecitals 64-83, \n\n123-128, 147, 131 \n\nRequirements for deployers of high-risk AI systems article 26, 27 recitals 91-96 \n\nRequirements for importers of high-risk AI systems article 23 recitals 83 \n\nRequirements for distributors of high-risk AI systems article 24 recitals 83 \n\nRequirement for third-party suppliers to high-risk systems \n\narticle 25 recitals 83-90 \n\nStandards article 40, 41 recital 121 \n\nConformity assessment procedure article 28 recital 149 \n\n# Where can I find this? 36 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ General-purpose AI models are versatile AI components demonstrating immense generality in the tasks they can handle, particularly encompassing current generative AI models. \n\nâ€¢ Fine-tuning and modification of general-purpose AI models may result in new general-purpose AI models. \n\nâ€¢ Providers of general-purpose AI models are tasked with a number of transparency obligations both towards the AI Office and competent authorities as well as towards AI systems providers intending to integrate their AI systems with general-purpose AI models. \n\nâ€¢ General purpose AI models that pose systemic risks, i.e., the most versatile and powerful models to date, are under heightened evaluation, transparency, security, risk assessment and incident management obligations. The classification procedure for general-purpose AI models with systemic risk should be a key area of focus for general-purpose AI models providers. \n\nâ€¢ The development and publication of codes of practice will help general-purpose AI models providers identify specific technical and organisational measures to implement in order to comply with their obligations. \n\nâ€¢ Provisions regarding general-purpose AI models will apply from 2 August 2025. For providers of general-purpose AI models: undertake a thorough governance review and make necessary adjustments to ensure compliance â€“ the obligations for providers of general-purpose AI models are among the strictest in the AI Act. For providers of general-purpose AI models: conduct a comprehensive legal IP assessment â€“ regulations for general-purpose AI models are heavily intertwined with IP laws, particularly regarding the copyright policy and the various training data obligations. For providers of general-purpose AI models: continuously and closely \n\nmonitor the thresholds for â€œsystemic risk,â€ \n\nas these may be adjusted over time via delegated acts. For providers of general-purpose AI models: keep an eye out for the development and publication of codes of practice, which will include specific and technical details on how to comply with the obligations for general-purpose model providers in practice. Sign up to our Connected newsletter and keep up with the latest developments here! \n\nFamiliarise yourself with the concepts of general-purpose AI models, general-purpose AI systems, AI systems, and high-risk AI systems â€“ and their relation to each other. This understanding is crucial for assessing which systems your company uses or markets and for making informed legal evaluations. \n\n# To do list At a glance \n\nCHAPTER 5 \n\n# General-purpose AI models 37 1 2 3 4 5 6 7 8 9 10 \n\n# Background and relevance of general-purpose AI models \n\nOne of the most prominent debates in the legislative process of the AI Act revolved around the regulation of general-purpose AI. The first draft of the AI Act (the Commissionâ€™s proposal of April 2021) was based on the understanding that each AI system is created for a specific purpose, and that this purpose can be associated with a specific risk potential. This classification did not have in mind foundation models which are trained on broad data such that it can be applied across a wide range of use cases. These AI models did not fit into the risk-based scheme of the first draft of the AI Act. The categorisation had to be expanded to include a new category that took into account the specific capabilities and dangers of such models. In the summer of 2023, the â€œfoundation modelâ€ (later \n\nrenamed general-purpose AI) was added to the then-current draft of the AI Act. The AI Actâ€™s chapter on the regulation of general-purpose AI models holds significant importance for two main reasons: \n\nâ€¢ firstly, it addresses generative AI, a subset of AI that is currently opening up the most intriguing new opportunities in the business environment and encompasses the majority of corporate use cases; and \n\nâ€¢ secondly, the requirements for general-purpose AI under the AI Act, alongside those for high-risk AI systems, are the most demanding in the AI Act, necessitating the utmost diligence in corporate implementation. This significance is only somewhat diminished by the fact that all requirements are directed solely at providers, not deployers. \n\n# Terminology and general-purpose AI value chain \n\nGeneral-purpose AI models and general-purpose AI systems \n\nArticle 3(63) outlines the characteristics of a general-purpose AI model, emphasising its versatility and competence across various tasks. Recital 98 highlights two key indicators: 1.  having at least a billion parameters; and 2.  being trained with a large amount of data using self-supervision. These models are distinguished by their ability to integrate into and function within diverse downstream systems or applications. Typically, general-purpose AI models undergo extensive training with large datasets, often utilising methods like self-supervision at scale. Recital 99 further specifies that large generative AI Models, such as LLMs or Diffusion Models, are typical examples of general-purpose AI models. Recital 97 clarifies that while general-purpose \n\nAI models are crucial components of AI systems, they are not AI systems themselves. Additional elements, such as user interfaces, are needed to transform general-purpose AI models into fully operational AI systems. A general-purpose AI system is an AI system built upon a general-purpose AI model, maintaining its versatility across various tasks (article 3(66) and recital 100). To clarify with an example, a system that solely performs translations would likely not qualify as a general-purpose AI system. \n\nGeneral-purpose AI systems and high-risk AI systems \n\nRecital 85 emphasises that general-purpose AI systems, due to their versatility, may function as high-risk AI systems or as components within them. Providers of general-purpose AI systems must collaborate closely with providers of high-risk AI systems to ensure compliance with the AI Act and to distribute responsibilities fairly along the AI value chain (see Chapter 4 for more on high-risk systems). 38 1 2 3 4 5 6 7 8 9 10 \n\nModification and fine-tuning of general-\n\npurpose AI models \n\nModifying or fine-tuning a general-purpose AI model, where a new specialised training data set is fed into the model to achieve better performance for specific tasks, does not transform it into a general-purpose AI system; it remains an abstract model without an interface. Instead, such actions create a modified or fine-tuned general-purpose AI model. Recital 97 and recital 109 specify that a provider who modifies or fine-tunes a general-purpose AI model has limited obligations related only to the changes made, including providing technical documentation or a summary of the training data used. \n\n# Obligations for providers of general-purpose AI models \n\nA provider of a general-purpose AI model that places such a model on the market, or integrates it with its own AI system and places it on the market or puts it into service, is obliged to: a.  prepare and maintain up-to-date technical documentation containing i.a. a description of the model and information on its development process (including training, testing and validation) for the purpose of making it available to the AI Office and competent authorities (article 53(1)(a)) â€“a list of the minimum information required is provided in Annex XI; b.  prepare, maintain up-to-date and make certain information and documentation available to downstream AI systems providers (i.e. those who wish to integrate their AI systems with the general-purpose AI model) so that they can understand the modelâ€™s characteristics and comply with their own obligations (article 53(1)(b)) â€“ a list of the minimum information required is provided in Annex XII; providers are allowed to balance the information they share against their need to protect confidential business information and trade secrets; c.  establish a policy to comply with the EU regulations on copyright and related rights (article 53(1)(c)), taking into account, i.a., the right to opt-out of text and data mining as provided for in article 4(3) of Directive (EU) 2019/790 -on copyright and related rights in the Digital Single Market (the AI Act does not specify other matters that have to be addressed in the policy); d.  prepare and publicly share a comprehensive summary on the data used for training the model (article 53(1)(d)) â€“ the AI Office is tasked with providing a template for this purpose; as the recital 107 explains the summary should allow interested parties to exercise their rights by, for example, listing main data collections, databases or data archives used; e.  cooperate with the relevant authorities when they exercise the powers granted to them under AI Act (article 53(3)); and f.  if the provider is established outside the EU: appoint an authorised representative in the EU (article 54(1)). If a provider releases a general-purpose AI model under a free and open-source licence and makes relevant information publicly available, it is not obliged to fulfil the requirements listed in a-b and f above â€“ unless the general-purpose AI model is qualified as presenting a systemic risk (article 53(2) and article 54(6)). \n\n# General-purpose AI models with systemic risk \n\nQualification criteria \n\nThe AI Act introduces specific heightened obligations for general-purpose AI models \n\npresenting â€œsystemic risksâ€ , e.g. reasonably foreseeable negative effects relating to major accidents, disruption of critical sectors, serious consequences to public health and safety, public and economic security, democratic processes, the dissemination of false or discriminatory content, etc. (recital 110). According to article 51(1) of the AI Act, a general-purpose AI model is classified as a general-purpose AI model with systemic risk if it meets one of these two conditions: (a) it has â€œ high impact capabilities â€ evaluated on the basis of technical tools and methodologies, or (b) is designated by the Commission as having capabilities or impact equivalent to those set out in point (a) having regard to the criteria set out in Annex XIII of the AI Act. These criteria notably 39 1 2 3 4 5 6 7 8 9 10 \n\ninclude the number of parameters of the model, the quality or size of the data set, the amount of computation used for training, the modelâ€™s impact on the European market, the number of registered users the EU. In addition, a model is presumed to have â€œ high impact capabilities â€ if it is trained with more than 10^25 floating point operations, i.e., massive computing powers (article 51(2)). At the time of this Guide, only a handful of Large Language Models seem to meet this threshold. Article 52 of the AI Act sets out the classification procedure. Most notably, providers of general-purpose AI models which meet the systemic risk classification conditions must notify the Commission without delay, and at the latest within two weeks after that requirement is \n\nmet or it becomes known that it will be met. Providers may present arguments to demonstrate that their models do not pose systemic risks despite meeting the requirements. Should such arguments be rejected by the Commission, the concerned models will be considered as presenting systemic risks. Upon â€œreasoned requestâ€ of \n\na provider, the Commission may decide to reassess the classification (article 52(5)). A list of general-purpose AI models with systemic risk will be published and updated by the Commission (article 52(6). \n\nObligations for providers of general-purpose AI models with systemic risk \n\nIn addition to the general requirements applicable to all general-purpose AI models providers, the AI Act imposes additional heightened obligations on providers of general-purpose AI models with systemic risk (articles 53(1) and 55(1)). These obligations apply prior to the modelsâ€™ placing on the market and throughout their entire lifecycle, and relate to: \n\nâ€¢ models evaluation; \n\nâ€¢ assessment and mitigation of systemic risks; \n\nâ€¢ incident management and reporting; \n\nâ€¢ increased level of cybersecurity protection; \n\nand \n\nâ€¢ extended technical documentation. 40 1 2 3 4 5 6 7 8 9 10 \n\nThe AI Act classifies AI systems by risk level, with increased transparency demands for high-risk categories. Transparency is required for high-risk AI systems before they are placed on the market or put into service. See Chapter 4 of this guide for more details regarding the transparency requirements for high-risk AI systems. Additionally, the AI Act mandates transparency requirements under article 50 for specific \n\ntypes of products, requiring that adequate information be provided to individuals, by \n\neither providers or deployers. \n\nâ€¢ Disclaimers: providers of AI systems intended to interact directly with individualsâ€™ need to design and develop them, so that the individuals will be informed about the fact that they are interacting with an AI system. \n\nâ€¢ Marking requirement: providers of \n\nAI systems must mark AI-generated \n\ncontent (audio, images, videos, text) in \n\na way that distinguishes it from human-generated content. \n\nâ€¢ Deepfake marking: AI-generated content (images, audio, video) that resembles real entities and could mislead people into believing it is authentic must be labelled. \n\nâ€¢ Emotion recognition system/ biometric categorisation system: deployers of AI-systems should make individuals aware of the operation of these systems. The AI Actâ€™s transparency obligations collate \n\nwith the other regulatory framework in the EU. \n\nIn particular, there is some overlap between \n\nthe transparency requirements of the GDPR \n\nand the AI Act, although the latter is more technical in nature. Deepfakes: label as â€˜Deepfakeâ€™ in a clear and distinguishable manner to disclose their artificial creation or manipulation. Implement Marking: ensure AI-generated content is marked in a machine-readable format. \n\nFor providers For deployers \n\n# To do list At a glance \n\n> CHAPTER 6\n\n# Transparency \n\nImplement Disclaimers: ensure proper disclaimers are added to AI systems intended to interact directly with individuals. Emotion recognition system/ biometric categorisation system: make individuals aware that such a system is operating. 41 1 2 3 4 5 6 7 8 9 10 \n\n# General transparency obligations \n\nâ€¢ Form: in practice, providers can design a disclaimer in different forms (e.g. as an avatar, icon or interface), as long as it provides clear information that the individual is interacting with an AI system. \n\nExemptions \n\nâ€¢ Obvious cases: if, considering the circumstances and the context of the AI system use, it is obvious for an individual who is reasonably well-informed, observant and circumspect that they are interacting with an AI system, then the system is exempt from this transparency requirement. \n\nâ€¢ Legal use: AI systems that are permitted by law for use in detecting, preventing, investigating, or prosecuting criminal \n\nactivities, that are subject to appropriate safeguards for the rights and freedoms of third parties, are also exempt from these transparency requirements, unless those systems are available for the public to report \n\na criminal offence. \n\nMarking of AI-Generated Content (article 50(2) AI Act) \n\nArticle 50(2) of the AI Act mandates that providers of AI systems, including general-purpose AI systems, must appropriately mark synthetic content such as audio, images, videos, or text. Recital 133 explains the rationale: with AI technology advancing, AI-generated synthetic content is becoming increasingly indistinguishable from human-generated content, posing the risk of misinformation, manipulation, fraud, impersonation, and consumer deception. \n\nThe Marking Obligation \n\nâ€¢ Marking : only providers of AI systems are required to mark AI-generated content. This requirement does not extend to deployers or other users of the content. \n\nâ€¢ Format : the output must be marked in a machine-readable format to indicate that it is artificially generated or manipulated. The AI Act acknowledges the importance of transparency in the use of AI systems. Individuals \n\nshould be enabled to understand the AI \n\nsystemâ€™s design and use, and there should be accountability for decisions made by companies and public authorities. Transparency is also essential for creating public trust in AI systems and ensuring their responsible deployment. Transparency also enhances the broader concept of â€˜AI literacyâ€™, developing awareness about the opportunities and risks of AI and the possible harm it can cause. Such awareness should especially be developed amongst: \n\nâ€¢ individuals concerned, giving them a better understanding of their rights in the context \n\nof AI, and \n\nâ€¢ deployers, allowing them to deploy AI systems in an informed way. Providers, and in certain circumstances deployers as well, have their own transparency requirements. The AI Act classifies AI systems by risk level, with increasing transparency demands for higher risk categories. The transparency requirements for specific types of products are described below. \n\nProvider Obligations: \n\nChatbots (article 50(1) AI Act) \n\nArticle 50(1) of the AI Act mandates that providers of AI systems need to ensure that such systems intended to interact directly with individuals are designed and developed such that the individuals concerned are informed that they are interacting with an AI system. \n\nâ€¢ Target audience: when implementing that obligation, the provider should identify not only \n\nthe intended but also the broader potential \n\ntarget audience to whom the disclaimer may be displayed. The characteristics of individuals belonging to vulnerable groups due to their age or disability should be taken into account, to the extent the AI system is also intended to interact with those groups. The intended or potential target audience has a significant impact on accessibility considerations. 42 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ Technical standards : the markings should be effective, interoperable, robust, and reliable. Providers need to consider the type of content, implementation costs, and current technical standards. \n\nMarking methods :\n\nâ€” Watermarks: visible watermarks can be easily implemented â€“ but also removed with basic editing tools, whereas invisible watermarks require specialised software for detection and removal. \n\nâ€” Metadata: this provides information \n\nabout the fileâ€™s creation and origin but \n\ncan be easily altered or removed with file editing tools. \n\nâ€” Algorithmic fingerprints: AI models leave unique traces or anomalies in the content they generate. For instance, AI-generated images might have minor distortions in textures or patterns, and AI-created audio files could display unnatural pauses or \n\ntonal shifts. \n\nâ€” Cryptographic signatures: digital \n\nsignatures embedded using cryptographic methods, such as a cryptographic hash \n\nthat verifies content authenticity. Even minor changes in the data result in a different hash, ensuring easy verification \n\nof alterations. Numerous tools and initiatives exist to manage and detect AI-generated content. Certain platforms use deepfake detection software that analyses algorithmic patterns and embedded metadata, while others rely on metadata and cryptographic hashes to authenticate the source of the content. For example, platforms might use voice analysis tools to detect synthetic audio, or employ blockchain technology to track the origin of and modifications to digital art. \n\nExemptions \n\nâ€¢ Editorial assistance: AI systems that mainly provide support for routine editing tasks or do not significantly change the original input data are exempt from the marking obligation. \n\nâ€¢ Legal use: AI systems that are authorised for use in detecting, preventing, investigating, or prosecuting criminal activities are also exempt from the marking requirement. \n\nDeployer obligations: \n\nEmotion recognition/ biometric categorisation systems (article 50(3) AI Act) \n\nArticle 50(3) of the AI Act sets forth specific transparency requirements for deployers of: \n\nâ€¢ Emotion recognition systems: AI systems used for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data, e.g., non-verbal signs such as facial expression. \n\nor \n\nâ€¢ Biometric categorisation systems: AI \n\nsystems used for the purpose of assigning natural persons to specific categories on the basis of their biometric data. Such specific categories can relate, e.g., to aspects such as sex, age, hair colour, eye colour, tattoos, personal traits, ethnic origin, personal preferences and interests. See Chapter 4 of this guide for more details on when the use of emotion recognition systems or biometric categorisation systems is prohibited. When these systems are allowed, deployers must inform the natural persons exposed to them about the use of the system. In particular, individuals should be notified when they are exposed to AI systems that, by processing their biometric data, can identify or infer their emotions or intentions or assign them to specific categories. \n\nExemptions \n\nâ€¢ Legal use: AI systems that are permitted for use in detecting, preventing or investigating criminal activities that are subject to \n\nappropriate safeguards for the rights and \n\nfreedoms of third parties and in accordance with the Union law, are exempt from these requirements. \n\nâ€¢ Biometric categorisation systems of ancillary use: AI systems whose use is ancillary to another commercial service and strictly necessary for objective technical reasons are exempt from these requirements. At present, there are no definitive guidelines on the scope of information that should be provided. Deployers, when using these systems, process personal data in accordance with GDPR and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable, apart from the requirements on 43 1 2 3 4 5 6 7 8 9 10 \n\nthe legal basis of the processing. This means that these regulations also constitute separate transparency obligations for deployers acting as controllers. In such cases, individuals should nevertheless be informed about the processing of their data as required under Article 13 and 14 GDPR. In relation to any automated processing, controllers are expected to additionally explain the logic behind their decision-making. In the case of an AI system, this might be provided as part of an explainability statement â€“ a document providing a non-technical explanation of i.a. why the organisation uses AI, how AI was developed, and how it operates and is used. \n\nDeepfakes (article 50(4) AI Act) \n\nArticle 50(4) of the AI Act sets forth specific labelling requirements for content known as \n\nâ€œDeepfakesâ€ . These obligations are crucial for ensuring transparency when AI systems are used to generate or manipulate content. \n\nDefinition of Deepfakes (article 3(60) AI Act) \n\nDeployers using AI to create content that: \n\nâ€¢ generates or manipulates images, audio, \n\nor video; \n\nâ€¢ significantly resembles real people, objects, places, entities, or events; and \n\nâ€¢ could mislead a person into believing the content is authentic or truthful. Examples of Deepfakes: \n\nâ€¢ Deepfake video calls mimicking company executives to trick employees into transferring large sums of money. \n\nâ€¢ AI-generated audio of politicians misleading voters about election dates via robocalls. \n\nâ€¢ Deepfake video ads impersonating political figures to manipulate public opinion on social media. \n\nâ€¢ Fake Zoom interviews using deepfake technology to impersonate high-profile individuals. \n\nâ€¢ Digital avatars delivering fabricated news reports to deceive viewers. \n\nLabelling requirements \n\nThe AI Act mandates that any content generated or manipulated by AI systems must be clearly and distinguishably labelled to disclose its artificial creation or manipulation. This requirement aims to ensure transparency and prevent the public from being misled by such content. At present, there are no definitive guidelines on how content should be labelled. This issue is likely to be addressed in future Codes of Conduct. Techniques such as watermarks, metadata identifications, fingerprints or other methods should be employed to indicate the contentâ€™s artificial nature (see recital 133). It is crucial that these labels are easily, instantly and constantly visible to the audience. For instance, in the case of videos, pre-roll labels or persistent watermarks may be used to meet these requirements effectively. \n\nExemptions \n\nThere are certain reliefs and exceptions to the labelling requirements under article 50(4) AI Act: \n\nâ€¢ The transparency requirements are more relaxed for artistic, creative, satirical, fictional, or similar works. Examples of such works include AI-generated movies or parodies, digital art exhibits, and AI-generated music videos. In these instances, the obligation is to disclose the AI involvement in a manner that does not disrupt the viewerâ€™s experience. This can be achieved through subtle watermarks, brief audio disclaimers, or notes in the description texts on digital platforms. \n\nâ€¢ The obligation to label AI-generated content does not apply if the AI systemâ€™s use is legally authorised for the purposes of detecting, preventing, investigating or prosecuting criminal offences. \n\nâ€¢ The labelling obligation may not apply if the AI-generated content has undergone human review or editorial control, with a natural or legal person holding editorial responsibility for the publication. This means that, if a human has reviewed and approved the AI-generated content, ensuring its accuracy and integrity, the stringent labelling requirements may be relaxed. This exception recognises the role of human oversight in maintaining the quality and reliability of AI-generated content. 44 1 2 3 4 5 6 7 8 9 10 \n\n# Transparency obligations for high-risk AI systems \n\nArticle 50(6) explains that the transparency \n\nobligations outlined here operate alongside \n\nother regulatory requirements. They neither replace nor reduce the obligations specified in Chapter III or other transparency requirements under EU or national legislation. See Chapter 4 of this guide for more details. \n\n# Timing and format \n\nAll the information required to meet the transparency obligations under article 50 must be provided to the individuals concerned: \n\nâ€¢ in a clear and distinguishable manner; \n\nâ€¢ by no later than the time of their first interaction or exposure to the AI system; and \n\nâ€¢ in conformity with the applicable accessibility requirements. The accessibility requirement means that the information should be accessible to diverse audiences, including individuals with disabilities. In practice, this may imply that, depending \n\non the circumstances, disclaimers or other marking methods will have to be displayed not only in written form but also in aural and (audio) visual form. Another aspect to be taken into account is that the individual should be provided with an amount of information that is clear and adequate but not overwhelming. \n\n# Transparency obligations at the national level and codes of practice \n\nThe transparency obligations outlined in article 50(1)-(4) AI Act are designed to coexist with \n\nother regulatory requirements, according to article 50(6) AI Act. They neither replace nor diminish the requirements set forth in \n\nChapter III or other transparency mandates under Union or national law. The AI Office is responsible for promoting and facilitating the development of codes of practice to support the effective implementation of the transparency obligations under article 50(1)-(4) AI Act at the EU level, under article 50(7) AI Act. These codes are intended to clarify the methods for detecting and labelling AI-generated content, to enhance cooperation throughout the \n\nvalue chain, and to ensure that the public \n\ncan clearly distinguish between content \n\ncreated by humans, and content generated \n\nby AI (recital 135). \n\n# Relationship with other regulatory frameworks \n\nâ€¢ The AI Actâ€™s marking obligations under article 50(2)-(4) support the Digital Services Actâ€™s (DSA) requirements for very large online platforms (VLOP) and search engines (VLOS) \n\nto identify and mitigate the risks associated with the dissemination of deepfakes (article 33 et seq. DSA). If the AI provider is separate from the VLOP or VLOS, these markings enable the platforms to recognise AI-generated content more efficiently. Conversely, if a VLOP or VLOS is also the AI provider, their DSA obligations are further detailed and enhanced by the AI Act. \n\nâ€¢ The transparency regulations for deepfakes will correlate with the European guidelines on misleading advertising (see Unfair Commercial Practices Directive) as well as national criminal provisions on deepfakes. \n\nâ€¢ The AI Actâ€™s transparency obligations also support and supplement the transparency requirements under Regulation (EU) 2016/679. However, the GDPR transparency requirements apply if personal data is processed when using AI technologies at all different stages of the AI lifecycle (e.g. when developing, testing or deploying AI technologies), and apply to controllers. Developers and providers of AI tools will not always be acting in such a role. In such case they may still be obliged to provide specific information to controllers to enable the latter to meet their obligations. 45 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ The AI Act enables the establishment of â€œAI regulatory sandboxesâ€ to provide a controlled environment in which to test innovative AI systems for a limited period before they are placed on the market. \n\nâ€¢ This regime is intended to encourage AI providers (or potential providers) to experiment with new and innovative products under supervision by regulators. There are specific incentives aimed at encouraging participation by SMEs and start-ups. \n\nâ€¢ Each Member State must establish at least one AI regulatory sandbox by 2 August 2026, although this can be done in co-operation with other Member States. \n\nâ€¢ The Commission is expected to adopt implementing acts to set out detailed arrangements for the establishment, operation and supervision of AI \n\nregulatory sandboxes. \n\nâ€¢ The AI Act also provides for â€œreal-worldâ€ \n\ntesting of AI systems, both inside and outside of regulatory sandboxes, subject to certain conditions to protect participants. \n\nâ€¢ The regimes relating to AI regulatory sandboxes and real-world testing are intended to be harmonise across the Union. However, there is the potential for divergent approaches at a national level, leading to a possibility of \n\nâ€œforum shoppingâ€ by providers. You should think about the countries in which you would like to test your AI services/products. Although the AI Act intends to establish a harmonised regime, there may be national differences which make some Member States more appropriate for you than others. Once you decide to participate in an AI regulatory sandbox, you will need to prepare a sandbox plan and follow the guidelines and supervision provided by the relevant national competent authority. If you decide to conduct real-world tests, you will also need to prepare a testing plan and seek approval from the relevant market surveillance authority. When you successfully complete an \n\nAI regulatory sandbox process, you \n\nshould obtain an exit report from the relevant national competent authority. This may be useful to accelerate the conformity assessment process for \n\nyour AI product/service. Participation in AI regulatory sandboxes and real-world testing is voluntary. AI providers should familiarise themselves with the relevant provisions of the AI Act if they intend to participate in a sandbox or real-world tests and should look out for further announcements and guidance on these topics, including detailed arrangements for AI regulatory \n\nsandboxes to be specified by the Commission in due course. \n\n# To do list At a glance \n\n> CHAPTER 7\n\n# AI regulatory sandboxes 46 1 2 3 4 5 6 7 8 9 10 \n\n# AI regulatory sandboxes \n\nThe AI Act enables the creation of â€œregulatory sandboxesâ€ to provide a controlled environment in which to test innovative AI systems for a limited period before they are placed on the market or otherwise put into service. The objectives of the AI regulatory sandbox regime include: \n\nâ€¢ fostering AI innovation while ensuring innovative AI systems comply with the AI Act; \n\nâ€¢ enhancing legal certainty for innovators; \n\nâ€¢ enhancing national competent authority understanding of the opportunities, risks and the impacts of AI use; \n\nâ€¢ supporting cooperation and the sharing of best practices; and \n\nâ€¢ accelerating access to markets, including by removing barriers for SMEs and start-ups. \n\nWhat is a regulatory sandbox under the AI Act? \n\nThe AI Act defines an â€œAI regulatory sandboxâ€ as: \n\nâ€œa controlled framework set up by a \n\ncompetent authority which offers providers \n\nor prospective providers of AI systems the possibility to develop, train, validate and test, where appropriate in real-world conditions, an innovative AI system, pursuant to a sandbox plan for a limited time under regulatory supervision. â€\n\nAI regulatory sandboxes can be established in physical, digital or hybrid form and may accommodate physical as well as digital products. \n\nObligation on Member States to establish AI regulatory sandboxes \n\nThe obligation to establish AI regulatory sandboxes rests with the Member States \n\nand their national competent authorities (see Chapter 8 for more on these). Each Member \n\nState must establish at least one AI regulatory sandbox by 2 August 2026. However, Member States can choose to either (i) establish one \n\nor more AI regulatory sandboxes at national level; (ii) jointly establish a sandbox with the national competent authorities of one or more other Member States or (iii) participate in an existing sandbox. National competent authorities establishing AI regulatory sandboxes should cooperate with other relevant national competent authorities where appropriate and may also involve other actors within the AI ecosystem. The EU Data Protection Supervisor may also establish an AI regulatory sandbox for European Union institutions, bodies, offices and agencies. A list of planned and existing sandboxes will \n\nbe made publicly available by the AI Office. \n\nThe Commission also intends to develop a \n\nsingle interface containing relevant information relating to AI regulatory sandboxes to allow stakeholders to: \n\nâ€¢ interact with AI regulatory sandboxes; \n\nâ€¢ raise enquiries with national competent authorities; and \n\nâ€¢ seek non-binding guidance on the \n\nconformity of innovative AI products, \n\nservices or business models. \n\nWho can participate in AI regulatory sandboxes? \n\nThe sandbox regime is aimed at providers (or prospective providers) of AI systems, although applications can be submitted in partnership with deployers and other relevant third parties. There are specific provisions which are designed to encourage participation by SMEs and start-ups, including: \n\nâ€¢ access to sandboxes should generally be free of charge for SMEs and start-ups; \n\nâ€¢ priority access for SMEs and start-ups with a registered office or branch in the EU; and \n\nâ€¢ SMEs and start-ups should have access to guidance on the implementation of the AI Act and other value-added services. 47 1 2 3 4 5 6 7 8 9 10 \n\nLiability \n\nProviders and prospective providers participating in an AI regulatory sandbox (including SMEs and start-ups) will remain liable for any harm inflicted on third parties as a result of the experimentation taking place in the sandbox. However, administrative fines will not be imposed on prospective providers if: \n\nâ€¢ they observe the relevant sandbox plan and the terms and conditions for their participation; and \n\nâ€¢ follow (in good faith) any guidance given by the national competent authority. \n\nImplementation of the sandbox regime \n\nIn order to avoid fragmentation across the EU, the Commission intends to adopt implementing acts specifying the detailed arrangements for the establishment, operation and supervision of AI regulatory sandboxes, including common principles on: \n\nâ€¢ eligibility and selection criteria for participation; \n\nâ€¢ procedures for the application, participation, monitoring, exiting from and termination \n\nof sandboxes; and \n\nâ€¢ the terms and conditions applicable \n\nto participants. These implementing acts are intended to ensure that AI regulatory sandboxes: \n\nâ€¢ are open to any provider who meets fair and transparent eligibility criteria; \n\nâ€¢ allow broad and equal access and keep up with demand for participation; \n\nâ€¢ facilitate the development of tools and infrastructure for testing and explaining dimensions of AI systems relevant for regulatory learning, such as accuracy, robustness and cybersecurity, as well as measures to mitigate risks to fundamental rights and society at large; \n\nâ€¢ facilitate the involvement of relevant actors within the AI ecosystem (e.g. notified bodies \n\nand standardisation organisations, testing \n\nand experimentation facilities, research and experimentation labs and European Digital Innovation Hubs), and also that participation in an AI regulatory sandbox is uniformly recognised (and carries the same legal effects) across the EU. \n\nNational competent authority obligations \n\nNational competent authorities must: \n\nâ€¢ allocate sufficient resources to ensure their sandbox regime complies with the requirements of the AI Act; \n\nâ€¢ provide guidance to sandbox participants on how to fulfil the requirements of the AI Act; \n\nâ€¢ provide participants with an exit report detailing the activities carried out in the sandbox, results and learning outcomes, which can later be used to demonstrate compliance with the AI Act through the conformity assessment process or relevant market surveillance activities; and \n\nâ€¢ provide annual reports to the AI Office and the Board (see Chapter 8 for more on these), identifying best practices, incidents and lessons learnt. National competent authorities will retain supervisory powers in relation to sandbox activities, including the ability to suspend or terminate activities carried out within a sandbox where it is necessary to address significant risks to fundamental rights or health and safety. \n\nProcessing of personal data within sandboxes \n\nPersonal data which has been lawfully collected for other purposes can be used in an AI regulatory sandbox subject to compliance with various conditions set out in the AI Act (all of which must be met for the relevant processing activities to be permitted). Some of the key conditions include: \n\nâ€¢ the relevant AI system being deployed in the sandbox must be aimed at safeguarding substantial public interest (e.g. public health, energy sustainability, safety of critical infrastructure); \n\nâ€¢ use of the personal data must be necessary and could not be substituted with anonymised or synthetic data; \n\nâ€¢ the personal data must be handled in a separate and protected environment and must be subject to appropriate technical and organisational measures; and 48 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ a detailed description of the process and \n\nrationale behind the training, testing and \n\nvalidation of the AI system is retained, together with the testing results. \n\n# Real-world testing of AI systems \n\nThe AI Act also enables the testing of AI systems in â€œreal-world conditionsâ€, subject to certain conditions. The AI Act defines â€œtesting in real-world conditionsâ€ as follows: \n\nâ€œthe temporary testing of an AI system for its intended purpose in real-world conditions outside a laboratory or otherwise simulated environment, with a view to gathering reliable and robust data and to assessing and verifying the conformity of the AI system with the requirements of [the AI Act] â€. Such real-world testing will not qualify as placing the relevant AI system on the market or putting it into service, provided that the relevant requirements of the AI Act are complied with. (See Chapter 2 for more on these concepts). The AI Act primarily focusses on real-world testing of high-risk AI systems outside of AI regulatory sandboxes. However, the AI Act also contemplates the possibility of AI systems (whether high-risk or not) being subject to real-world testing within the framework of an AI regulatory sandbox, under the supervision of a national competent authority. In both scenarios, the real-world testing must comply with various conditions set out in the AI Act (all of which must be met for the testing to be permitted, although there is greater flexibility where the testing is conducted within a sandbox). Some of the key conditions include: \n\nâ€¢ the proposed real-word tests have been approved by the relevant market surveillance authority and registered in the EU database for high-risk AI systems; \n\nâ€¢ the provider conducting the testing is \n\nestablished in the EU (or has appointed \n\na legal representative established in \n\nthe EU); \n\nâ€¢ testing is limited to a maximum of 6 months (which can be extended for an additional 6 months, although this requirement can be derogated from in relation to real-world testing within a sandbox environment); \n\nâ€¢ participants in the real-world testing are properly protected â€“ they must give informed consent, outcomes must be reversible (or capable of being disregarded) and they must be able to withdraw at any time; and \n\nâ€¢ market surveillance authorities can conduct unannounced inspections on the conduct of real-world tests. Providers and prospective providers will be liable for any damage caused in the course of their real-world testing. \n\nIs there a risk of â€œforum shoppingâ€ in relation to participation in sandboxes and real-world testing? \n\nAlthough the AI Act aims to harmonise the regimes relating to AI regulatory sandboxes and real-world testing across the EU, industry representatives and stakeholders will no doubt closely monitor their development and may elect participate in sandboxes and/or real-world testing in jurisdictions which are perceived to have the most industry-friendly approach (including in how liability relating to participation in sandboxes or real-world testing \n\nis determined). 49 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ The AI Act puts in place a post-market monitoring, reporting and information \n\nsharing process. \n\nâ€¢ Most obligations are on providers of high-risk AI systems who have to have post-market monitoring systems and procedures to report serious incidents. \n\nâ€¢ The serious incident reporting obligations can also sometimes apply to deployers. \n\nâ€¢ The timelines for reporting can be immediate. \n\nâ€¢ Reports need to be made to market surveillance authorities in Member States where the incident occurred; reporting to multiple authorities may therefore be needed. \n\nâ€¢ There is a multi-pronged approach to enforcement: \n\nâ€” The European Data Protection Supervisor handles EU institutions etc. \n\nâ€” The European Commission handles providers of general-purpose AI models. \n\nâ€” Competent authorities in each Member State are otherwise responsible. \n\nâ€¢ Sanctions are tiered, by reference to \n\nthe seriousness of the provision that has \n\nbeen infringed. \n\nâ€¢ Affected persons have a right to explanation of individual decision-making. Providers of high-risk systems should: \n\nâ€¢ Consider if they are already subject to other equivalent obligations; if so, check if you have double reporting obligations or not. \n\nâ€¢ Ensure quality management systems include serious incident reporting procedures. \n\nâ€¢ Ensure these procedures establish the nature of the serious incident (death, serious harm to health, violation of fundamental rights etc) and if they \n\nare widespread. \n\nâ€¢ Identify to whom you would have \n\nto report. Deployers of high-risk systems should: \n\nâ€¢ Develop stand-by procedures so they can report if needed. Providers of high-risk AI systems should: \n\nâ€¢ Watch for the European Commission template post-market monitoring plan, to be adopted by 2 February 2026. \n\nâ€¢ Prepare and implement a post-market monitoring plan. \n\nâ€¢ If already subject to existing post-market monitoring obligations, or a regulated financial services provider, consider if you can integrate your AI Act obligations into these systems. \n\n# To do list At a glance \n\nCHAPTER 8 \n\n# Enforcement and governance 50 1 2 3 4 5 6 7 8 9 10 \n\n> CHAPTER 8\n\nProviders and developers should: \n\nâ€¢ Check for the European Commission guidance due by 2 August 2025. \n\nâ€¢ Keep this under review as it will be \n\nre-assessed. Operators of non-high-risk AI systems \n\nshould: \n\nâ€¢ Ensure they comply with all existing product safety legislation. Providers of general-purpose AI models \n\nshould: \n\nâ€¢ Look out for, and consider responding to the consultation on, the European Commission implementing act relating to the arrangements for enforcement by the European Commission. All organisations in the AI value chain \n\nshould: \n\nâ€¢ Look out for, and consider responding to consultations on, rules relating to enforcement adopted at Member State level. \n\nâ€¢ Note the requirement to cooperate with market surveillance authorities where there is sufficient reason to consider that an AI system presents a risk. \n\nâ€¢ Note that disclosure of training, validation and testing data sets and source code might have to be disclosed. Deployers of high-risk systems should: \n\nâ€¢ Ensure they are able to provide clear and meaningful explanations as to the AIâ€™s decision-making procedure. \n\n# To do list 51 1 2 3 4 5 6 7 8 9 10 \n\n# Overview \n\nThe AI Act outlines a governance framework that provides for the implementation and supervision of both the ex ante requirements for AI systems and ex post surveillance and enforcement. The former is described in preceding chapters. The latter is the subject of this chapter, together with a description of the governance structure. The enforcement regime addresses two types of risk: risks to product safety, and risks to fundamental rights. In relation to the former, the AI Act builds upon existing product safety legislation and is mostly enforced by national market surveillance authorities. Where risks \n\nto fundamental rights are identified, the \n\nmarket surveillance authorities shall inform \n\nand fully cooperate with the relevant national public authorities or bodies protecting fundamental rights. Consistent with the risk-based approach in the AI Act, a multi-layered enforcement structure with different regimes applying to AI systems with varying risks is provided. For high-risk AI systems, the AI Act mandates, firstly, post-market monitoring obligations and, secondly, a requirement to report serious incidents. The serious incident reporting obligations can also sometimes apply to deployers, who should therefore also be aware of them. The marketing surveillance authorities can require operators to take all appropriate measures to ensure that AI systems do not present a risk and, where necessary, can demand the withdrawal of a product or AI system from the market. Very significant fines for non-compliance with the terms of the AI Act can also be levied. For general-purpose AI models, the European Commission has exclusive powers to supervise and enforce the obligations in the AI Act. The governance structure in the AI Act provides for the setting up of new institutional bodies at both the EU level (the AI Office, the European AI Board, the Advisory Forum and the Scientific Panel) and national level (notifying authorities and market surveillance authorities) and the roles and competencies of each of them are outlined. The coordination between these bodies will be key to the effective implementation and enforcement of the AI Act. Topics addressed in this chapter are as follows: \n\nâ€¢ Post-marketing obligations \n\nâ€¢ Market surveillance authorities \n\nâ€¢ Procedures for enforcement \n\nâ€¢ Authorities protecting fundamental rights \n\nâ€¢ General-purpose AI models \n\nâ€¢ Penalties \n\nâ€¢ Remedies for third parties \n\nâ€¢ Governance \n\n# Post-marketing obligations \n\nPost-market monitoring system for high-risk AI systems \n\nSince AI systems have the ability to adapt and continue to learn after they are placed on the market, it is important to monitor their performance once they are put on the market. Recital 155 explains that the aim of the post-market monitoring system is to ensure that providers of high-risk AI systems can consider experience from use of the system, so as to ensure ongoing compliance and improvement of the system. Providers of high-risk AI systems must include a post-market monitoring plan as part of the technical documentation that they draw up before they put the system on the market (articles 72(3) and 11(1)). This plan must be in line with the European Commission template, to be adopted by 2 February 2026. The post-marketing obligations will ensure that any need to immediately apply any necessary corrective or preventative actions are identified (article 3(25)). Article 72 provides that the post-market monitoring system (and the documentation of the system) must be proportionate to the nature of the AI technology and the risks of the systems. This system must actively and systematically collect, document, and analyse relevant data throughout the AI systemâ€™s lifetime, so as to allow the provider to evaluate continuous compliance. The data could be provided by deployers, or 52 1 2 3 4 5 6 7 8 9 10 \n\nby others (although sensitive operational data from law enforcement authority deployers is excluded). Where relevant, the system should also include analysis of interactions with other AI systems, including devices and software. Providers of certain types of high-risk AI systems, who already have post-market monitoring systems in place, can integrate their \n\nobligations under the AI Act into those \n\nexisting systems, provided this achieves an equivalent level of protection. This is the case for high-risk AI systems covered by Union harmonisation legislation listed in Section A \n\nof Annex I (i.e. including certain machinery, toys and medical devices). Itâ€™s also the case for financial institutions who are subject to requirements under Union financial services law regarding their internal governance, arrangements or processes, where these institutions place on the market high-risk AI systems listed in Annex III point 5 (in particular, evaluation of creditworthiness or for risk assessment and pricing in relation to life and health insurance) (article 72(4)). \n\nReporting of information on serious incidents for high-risk AI systems \n\nProviders of high-risk AI systems must report \n\nâ€œserious incidentsâ€ and the providerâ€™s quality management system must contain procedures relating to this (article 17(1)(i)). Ordinarily, deployers of high-risk AI systems must report serious incidents to the provider. However, if the deployer cannot reach the provider, then the serious incident reporting obligations of article 73 apply directly to the deployer (article 26(5)). Accordingly, deployers should also be aware of these provisions. The European Commission is to issue guidance for providers on incident reporting by 2 August 2025 and must keep this under regular review. Serious incidents are defined at article 3(49) and mean an incident or malfunctioning of an AI system that directly or indirectly causes: \n\nâ€¢ death, or serious harm to a personâ€™s health; \n\nâ€¢ serious and irreversible disruption to management or operation of critical infrastructure; \n\nâ€¢ violation of Union laws protecting fundamental rights; or \n\nâ€¢ serious harm to property or the environment. Serious incidents must be reported within set timelines, as set out below. If necessary, the provider or deployer may submit an initial report, which can be completed later (article 73(5)). \n\nSituation Period \n\nWidespread infringement \n\nOr \n\nSerious incident involving critical infrastructure Immediately â‰¤ than 2 days after awareness of the incident \n\nDeath of a person â‰¤ 10 days after awareness of the serious incident; or Immediately after establishing or suspecting a causal relationship between the serious incident and the AI system if earlier Other situations (i.e. serious harm to health, fundamental rights violations, serious harm \n\nto property or environment â€“ unless these \n\nare widespread) â‰¤ 15 days after awareness of the serious incident; or Immediately after the provider has established a causal link, or the reasonable likelihood \n\nof a link, between the AI system and the \n\nserious incident 53 1 2 3 4 5 6 7 8 9 10 \n\nAfter reporting, the provider must promptly conduct necessary investigations, including a risk assessment and corrective actions. The provider must not do anything that would alter the AI system in a way that may affect any subsequent evaluation of the cause of the incident before it has informed the competent authorities. Reports of serious incidents have to be made to the market surveillance authorities of the Member States where the incident occurred (article 73(1)). It follows that if a serious incident affects multiple Member States or affects multiple sectors so that there are multiple market surveillance authorities within a Member State, then multiple reports will need to be made. The market surveillance authority must take appropriate measures (which can include withdrawal or recall of the product) within seven days of receiving the notification and must also immediately notify the European Commission of any serious incident, whether or not they have taken action (article 73(8/11)). \n\nNon-high-risk AI systems \n\nAI systems relating to products that are not high-risk nevertheless must be safe when placed on the market or put into service. Regulation (EU) 2023/988 on general product safety and Regulation (EU) 2019/1020 on market surveillance and compliance of products apply to all AI systems governed by the AI Act, but these two Regulations provide the safety net for non-high-risk products (recital 166 and article 74(1)). Regulation (EU) 2019/1020 requires all operators to inform the relevant market surveillance authority when they have reason to believe that a product presents a risk under article 3(19) (see definition below). To the list of risks in article 3(19), the AI Act has added risks to fundamental rights of persons (article 79(1)). â€œProduct presenting a risk â€ means a product having the potential to affect adversely health and safety of persons in general, health and safety in the workplace, protection of consumers, the environment, public security and other public interests, protected by the applicable Union harmonisation legislation, to a degree which goes beyond that considered reasonable and acceptable in relation to its intended purpose or under the normal or reasonably foreseeable conditions of use of the product concerned, including the duration of use and, where applicable, its putting into service, installation and maintenance requirements. \n\n# Market surveillance authorities \n\nMember States play a key role as the enforcement of the AI Act will often require a local presence. Member States must each designate at least one market surveillance authority and one, if there is more than one, of these authorities must be set as a single point of contact vis-Ã -vis the public and other counterparts at Member State and Union level. The Member State shall notify the European Commission of the single point of contact and the European Commission will make a list of them available to the public (recital 153 and article 70(1/2)). The Member States have until 2 August 2025 to comply with these provisions (article 113(b)). \n\nWhich entities are to be designated market surveillance authorities? \n\nMember States have some flexibility in designating market surveillance authorities; they can either establish a new body dedicated to enforcing the AI Act or integrate the requirements of the AI Act into the framework of an existing body already responsible for market surveillance under the Union harmonisation laws listed in Section A of Annex I or the existing bodies regulating financial or credit institutions regulated by Union law (article 74(3/6/7)). However, for high-risk systems in the area of biometrics, law enforcement, migration, asylum and border control management and the administration of justice, Member States must designate either the national Data Protection Authority established by Regulation 54 1 2 3 4 5 6 7 8 9 10 \n\n(EU) 2016/679 or the supervisory authority designated under Directive (EU) 2016/680 \n\n(article 74(8)). Where AI systems relate to products already covered by the Union harmonisation legislation listed in Section A of Annex I and where such legal acts already provide for procedures ensuring an equivalent level of protection and having the same objective as the AI Act, the sectoral procedures shall apply instead of the national level enforcement procedures set out in articles 79 to 83 (see below under the heading â€˜Procedures for enforcementâ€™). In this instance, dual reporting of serious incidents is not required and providers report under those other laws (article 73(9) and 73(10)). These exceptions specifically apply to: \n\nâ€¢ Annex III-type high-risk AI systems, where the provider is subject to Union law that establishes reporting obligations equivalent to those set out in the AI Act. Such equivalence may - for example â€“ exist for critical infrastructure, which is covered by cybersecurity regulations that contain stand-alone incident reporting obligations that might be considered equivalent to those under the AI Act. However, it may not always be clear whether reporting obligations under other Union laws are considered equivalent to the reporting obligations under the AI Act; and \n\nâ€¢ high-risk AI systems that are safety components of devices, or are themselves devices, covered by Regulations (EU) 2017/745 on medical devices and (EU) 2017/746 on in vitro diagnostic medical devices. These both contain reporting obligations, according to which serious incidents must be reported to the competent authorities if they entail (a) the \n\ndeath of a patient, user or other person, (b) the \n\ntemporary or permanent serious deterioration \n\nof a patientâ€™s, userâ€™s or other personâ€™s state of \n\nhealth, or (c) a serious public health threat. However, in both instances, if the infringement relates to a violation of fundamental rights, it must still be notified under the AI Act and the relevant market surveillance authority must inform the national fundamental rights authority/ authorities. For AI systems used by Union institutions, agencies, offices, and bodies (with the exception of the Court of Justice of the European Union acting in its judicial capacity), the European Data Protection Supervisor will be the market surveillance authority (article 74(9)). \n\nPowers of the market surveillance authorities \n\nThe market surveillance authorities have all the broad enforcement powers set out in Regulation (EU) 2019/1020 in addition to further powers granted by the AI Act. For example, they have the power to: \n\nâ€¢ make operators disclose relevant documents, data and information on compliance. The AI Act adds that providers of high-risk AI systems may be compelled to disclose: \n\nâ€” training, validation and testing data sets used for the development of high-risk AI systems, including, where appropriate and subject to security safeguards, through application programming interfaces (API) or other relevant technical means and tools enabling remote access (article 74(12)); and \n\nâ€” where the testing or auditing procedures and verifications based on the data and documentation provided by the provider have been exhausted or proved insufficient, the source code if it is necessary to assess the conformity of a high-risk AI system with the requirements set out in chapter III, Section 2 (article 74(13)); \n\nâ€¢ make unannounced on-site inspections and make test purchases (article 74(5)); \n\nâ€¢ conduct investigations (engaging with the European Commission where high-risk AI systems are found to present a serious risk across two or more Member States) (article 74(11)); \n\nâ€¢ require operators to take appropriate actions to bring instances of non-compliance to an end, both formal non-compliance (article 83) and to eliminate a risk (articles 79-82); \n\nâ€¢ take appropriate measures where an operator fails to take corrective action or where the non-compliance persists, including withdrawal or recall (articles 73(8), 79-83); and \n\nâ€¢ impose penalties (articles 99-101). The market surveillance authorities shall also ensure that testing in real world conditions is in accordance with the AI Act (see Chapter 7). They have the power to require the provider or deployer to modify the testing or suspend or terminate it (article 76(3)). 55 1 2 3 4 5 6 7 8 9 10 \n\nHandling of confidential information \n\nAny information or documentation obtained by market surveillance authorities shall be treated in accordance with the confidentiality obligations set out in article 78. The provisions in article 78 also apply to the European Commission, the authorities protecting fundamental rights and natural and legal persons involved in the application of the AI Act. Such persons shall carry out their tasks in a manner which not only protects confidential information and trade secrets, but also protects intellectual property rights and the rights in source code, public and national security interests and classified information. These provisions shall apply from 2 August 2025. \n\n# Procedures for enforcement \n\nAs already noted, the following procedures do not apply where there exists already harmonising legislation providing an equivalent level of protection and having the same objective as the AI Act. \n\nAI systems presenting a risk (articles 79 and 81) \n\nWhere a market surveillance authority has sufficient reason to consider an AI system presents a risk (see definition above), it must carry out an evaluation as to whether the AI system is compliant with the AI Act. If it does not comply, the market surveillance authority shall without undue delay notify the relevant operator and require them to take all appropriate corrective actions to bring the AI system into compliance or to withdraw the AI system from the market, or to recall it. The market surveillance authority shall state how long the operator has to comply, but it will be no longer than 15 working days. If operator does not take adequate corrective action by the end of the specified period, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system being made available on its national market or put into service, to withdraw the product or the standalone AI system from that market or to recall it. The market surveillance authority must inform the operator of the grounds on which its decision is based. Where the non-compliance is not restricted to its national territory, the market surveillance authority shall inform the European Commission and the other Member States without undue delay of the results of the evaluation and of the actions which it has required the operator to take and the provisional measures which it has taken if the operator has not complied. The provisional measures shall be deemed justified if no objection has been raised by either a market surveillance authority of a Member State or by the European Commission within three months (reduced to 30 days in the event of non-compliance with the prohibitions referred to in article 5). However, if objections are raised, the European Commission shall consult with the market surveillance authority and the operator or operators and, within six months (or 60 days for an article 5 issue), decide whether the provisional measure is justified. If it is, all Member States shall ensure that they take appropriate restrictive measures in respect of the AI system concerned, such as requiring withdrawal from their market. If it is not, the provisional measure will be withdrawn. These provisions are without prejudice to the procedural rights of the operator set out in article 18 of Regulation (EU) 2019/1020, including the right to be heard. \n\nAI systems classified by the provider as non-\n\nhigh-risk (article 80) \n\nIf the market surveillance authorities have sufficient reason to consider an AI system classified by the provider as non-high-risk under article 6(3) is indeed high-risk, it must carry out an evaluation. The procedure to be followed is very much as described above, but article 80 specifically refers to the ability to fine the relevant provider. In exercising their power to monitor the application of article 80, market surveillance authorities may take into account the information stored in the EU database of \n\nhigh-risk AI systems (see below under the heading â€˜Governance at Union Level: Role of \n\nthe European Commissionâ€™). 56 1 2 3 4 5 6 7 8 9 10 \n\nCompliant AI systems which present a risk (article 82) \n\nIf a market surveillance authority finds that a high-risk AI system complies with the AI Act, but it nevertheless presents a risk to the health or safety of persons, to fundamental rights, or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that it no longer does so. \n\nFormal non-compliance (article 83) \n\nWhere a market surveillance authority finds that, for example, a CE marking has not been affixed where it should, no authorised representative has been appointed or technical documentation is not available, it shall require the relevant provider to correct the matter within a prescribed period. If the non-compliance persists, then the market surveillance authority shall take appropriate and proportionate measures to restrict or prohibit the high-risk AI system being made available on the market or to ensure that it is recalled or withdrawn from the market without delay. \n\n# Authorities protecting fundamental rights \n\nIn addition to identifying market surveillance authorities, by 2 November 2024, each Member State must identify the public authorities or bodies supervising and enforcing the obligations under Union law protecting fundamental rights, including the right to non-discrimination, in relation to the use of high-risk AI systems referred to in Annex III and shall notify them to the European Commission. Where market surveillance authorities identify risks to fundamental rights they must notify the relevant national public authority supervising their protection. These bodies have the power to request and access any documentation created or maintained under the AI Act when access to that documentation is necessary for effectively fulfilling their mandates. The relevant public authority or body shall inform the market surveillance authority of the Member State concerned of any such request and, where the documentation proves insufficient may request the market surveillance authority to organise testing of the high-risk AI system through technical means (article 77). \n\n# General-purpose AI models \n\nThe European Commission is the sole authority responsible for supervising and enforcing obligations on providers of general-purpose AI models. The rationale behind this is to benefit from centralised expertise and synergies at Union level (article 88). In practice, however, the AI Office (see below under the heading â€˜Governanceâ€™) will carry out all necessary actions to monitor the effective implementation of the AI Act with regard to general-purpose AI models, provided that the organisational powers of the European Commission and the division of competences between Member States and the Union are not affected. The AI Office can investigate possible breaches of the rules by providers of general-purpose AI models on its own initiative, following the results of its monitoring activities, or at a request from market surveillance authorities. It has the powers of a market surveillance authority for AI systems which are based on a general-purpose AI model, where the model and system are developed by the same provider. Market surveillance authorities must cooperate with the AI Office to carry out compliance evaluations if a market surveillance authority considers that a general-purpose AI system (that can be used by deployers for at least one high-risk purpose) is non-compliant with the AI Act. Market surveillance authorities can request the AI Office to provide information related to general-purpose AI models, where the market surveillance authority is unable to access that information (and as a result is unable to conclude its investigation into a high-risk system) (article 75). \n\n# Penalties \n\nAny person, which fails to comply with the AI Act - whether a natural or legal person, a public authority or an EU or national institution - can be sanctioned for non-compliance. The provisions on penalties under the AI Act exceed even those provided for in the GDPR (which are up to EUR 57 1 2 3 4 5 6 7 8 9 10 \n\n20,000,000 or 4% of annual worldwide turnover). The maximum fine was revised throughout the legislative process but was ultimately set at EUR 35,000,000 or 7% of annual worldwide turnover. Fines can be imposed by national authorities, the European Data Protection Supervisor, or the European Commission. The European Data Protection Supervisor can impose fines on Union institutions, agencies and bodies. The European Commission can impose fines on providers of general-purpose AI models. National authorities can impose fines on other operators. The AI Act has a tiered approach to penalties, as shown below. \n\nGrounds of infringement EU bodies All other persons \n\nPenalties imposed \n\nby EDPS Penalties imposed by national authorities (unless GPAI models, in which case imposed by the European Commission). For sanctioned persons which are undertakings, the penalties are capped at the higher of the %-based amount or the figure below. If the undertaking is an SME, they are capped at the lower amount. For other sanctioned persons, the specified figure is the cap. Supplying incorrect, incomplete or misleading information to notified \n\nbodies or national \n\ncompetent authorities. â‰¤ â‚¬750,000 (article 100(3)) â‰¤ 1% total worldwide annual turnover in preceding year; or â‰¤ â‚¬7,500,000 (article 99(5)) \n\nObligations relating to \n\nhigh-risk AI systems. â‰¤ 3% of total worldwide annual turnover in preceding year; or â‰¤ â‚¬15,000,000 (article 99(4) for high-risk AI systems; article 101(1) for general-purpose AI models) \n\nObligations relating to \n\nproviders of general -purpose AI models. \n\nObligations relating to \n\nprohibited practices. â‰¤ â‚¬1,500,000 (article 100(2)) â‰¤ 7% of total worldwide annual turnover in preceding year; or â‰¤ â‚¬35,000,000 (article 99(3)) Curiously, there appear to be no penalties for failure to comply with the AI literacy obligations at article 4. \n\nPenalties and fines imposed by national \n\nauthorities \n\nIt is the responsibility of Member States to provide for effective, proportionate, and dissuasive sanctions. These measures may include both monetary and non-monetary measures or warnings. They must be notified to the European Commission by the date of entry into application (article 99(1/2)). Penalties are to be imposed on a case-by-case basis. The competent national authority should consider all relevant circumstances of the specific situation, with due regard to the nature, gravity, and duration of the infringement and its consequences, as well as the size of the provider (article 99(7)). 58 1 2 3 4 5 6 7 8 9 10 \n\nEnforcement at Member State level must be subject to appropriate procedural safeguards, including effective judicial remedies. \n\nFines on Union institutions, bodies, offices \n\nand agencies \n\nThe European Data Protection Supervisor has the power to impose fines on Union institutions, agencies and bodies. Before adopting a decision on a fine, the EDPS should communicate its preliminary findings to the Union institution and give it an opportunity to be heard. The fine is not to affect the effective operation of the institution and the funds collected by the imposition of fines are to be contributed to the general budget of the EU. \n\nFines on providers of general-purpose AI models \n\nThe European Commission may impose fines on providers of general-purpose AI models for infringements (article 101). Unlike the other provisions on penalties and fines in chapter XII, which apply from 2 August 2025, article 101 does not apply until 2 August 2026. The European Commission will publish an implementing act with details on arrangements and procedural safeguards for proceedings. When imposing a fixed amount or periodic penalty payment, the European Commission should take due account of the nature, gravity and duration of the infringement, and the principles of proportionality and appropriateness. Before adopting a decision on a fine, the European Commission should communicate its preliminary findings to the provider of the general-purpose AI model and give it an opportunity to be heard. The imposition of a fine must be subject to appropriate procedural safeguards, including judicial review before the Court of Justice of the European Union. The CJEU may cancel, reduce or increase the amount of a fine imposed. \n\n# Remedies for third parties \n\nComplaint to a market surveillance authority (article 85) \n\nUnion and Member State law already provide some effective remedies for natural and legal persons whose rights and freedoms are adversely affected by the use of AI systems. Notwithstanding, the AI Act introduces a new complaints mechanism. It mandates that any natural or legal person may submit a complaint to the competent market surveillance authority if it has grounds for believing there has been an infringement of the AI Act. Compare: Under the GDPR, a data subject has the right to lodge a complaint with a supervisory authority about an alleged infringement if the data subject believes that the processing of personal data relating to him or her violates rights under the GDPR. In contrast, a complaint lodged under the AI Act may concern not only an infringement of the rights of the complainant, but also compliance with the AI Act as a whole. In addition, under the GDPR a remedy can be filed only by the data subjects; under the AI Act, a complaint can also be filed by a legal person. \n\nRight to explanation of individual decision-making (article 86) \n\nUnder the AI Act, any affected person is entitled to receive â€œclear and meaningfulâ€ explanations \n\nfrom the deployer concerning decisions made by high-risk AI systems (except for critical infrastructure systems). These explanations must clarify the decision-making procedure used and the main elements of the decision made by the AI system (article 86). The right can be invoked if: \n\nâ€¢ a deployerâ€™s decision is mainly based on the output of high-risk AI systems; and \n\nâ€¢ that decision has legal effects or similarly significant effects on an affected person that adversely affect his or her health, safety or fundamental rights. Compare: The right to an explanation under the AI Act aligns with a controllerâ€™s obligation under the GDPR concerning automated decision-making processes (article 22 GDPR). Under the GDPR, the controller must provide the data 59 1 2 3 4 5 6 7 8 9 10 \n\nsubject with meaningful information on the logic and significance of the consequences of such processing. Article 86 of the AI Act complements the data subjectâ€™s right to an explanation under the GDPR; it is more specific to AI as it requires the deployer to explain the role of the AI system in the decision. In addition, the AI Act grants this right to all affected persons who can also be legal persons. National data protection \n\nauthorities under the GDPR are still the \n\ncompetent authorities to enforce the controllerâ€™s obligation to provide information when it comes to automated decision-making involving personal data processing, regardless of what authority is competent to enforce article 86 of the AI Act. \n\nProtection for whistleblowers (article 87) \n\nDirective (EU) 2019/1937 on the protection of persons who report breaches of Union law applies to the reporting of infringements of the AI Act. \n\nDownstream providersâ€™ complaint (article 89) \n\nThe AI Act enables complaints by downstream providers (deployers of general-purpose AI systems) about possible violations of the rules set out in the Act. Complaints can be made to the AI Office and must be well-substantiated. They should include \n\nat least: \n\nâ€¢ details of the provider of the general-purpose AI model that is the subject of the complaint, and its point of contact; \n\nâ€¢ a description of the relevant facts, together with the provisions that have been breached; \n\nâ€¢ the reasons why the complainant believes there has been an infringement; and \n\nâ€¢ any other information that the requesting downstream provider deems relevant, including, where appropriate, information gathered at its own initiative. The possibility for downstream providers to make such complaints enables the AI Office to effectively oversee the enforcement of the AI Act. \n\n# Governance \n\nThe governance structure has been established to coordinate and support the application of the AI Act. Its aim is to build capabilities at both Union and national levels, integrate stakeholders, and ensure trustworthy and constructive cooperation. \n\nGovernance at Union Level: role of the European Commission \n\nThe European Commission is tasked by the AI Act with many responsibilities including developing and implementing delegated acts, developing and publishing guidelines, setting standards and best practice and making binding decisions to implement the AI Act effectively. In practice, these tasks will be carried out by the AI Office (part of the administrative structure of the Directorate-General for Communication Networks, Content and Technology) in its role of supporting the European Commission. One of the tasks that the European Commission, in collaboration with the Member States, must perform is set out in chapter VIII of the AI Act. The European Commission must set up and maintain an EU database for high-risk AI systems referred to in article 6(2) and AI systems that are not considered as high-risk pursuant to article 6(3). The database will contain: \n\nâ€¢ the data listed in Sections A and B of Annex VIII entered into the EU database by the provider or the authorised representative; and \n\nâ€¢ the data listed in Section C of Annex VIII entered into the EU database by the deployer who is, or who acts on behalf of, a public authority, agency or body. The data will be available to the public (with the exception of data relating to AI systems in the areas of law enforcement, migration, asylum and boarder control management). 60 1 2 3 4 5 6 7 8 9 10 \n\nThe supranational bodies set up by the AI Act \n\nRole of the AI Office Actions \n\nThe AI Office was established by the European Commission by its decision of 24 January 2024 (C/2024/1459). The AI Officeâ€™s function is to oversee the advancements in AI models, including as regards general-purpose AI models, the interaction with the scientific community, and to play a key role in investigations and testing, enforcement and to have a global vocation (recital 5 of the decision). The AI Office may involve independent experts to carry out evaluations on its behalf. The AI Office must establish systems and procedures to manage and prevent potential conflicts of interest and must develop Union expertise and capabilities in the field of AI. The AI Office has a role in the surveillance \n\nand control of general-purpose AI systems (article 75). Monitoring and enforcement: Monitor compliance and implementation of obligations for providers of general-purpose AI models. Investigation: Investigate infringements by requesting documentation and information, conducting evaluations and requesting measures from providers of general-purpose \n\nAI models. Risk management: Request appropriate measures, including risk mitigation, in cases of identified systemic risks, as well as restricting market availability, withdrawing or recalling \n\nthe model. \n\nCoordination and support: Support national \n\nauthorities in creating AI regulatory sandboxes and facilitate cooperation and information-sharing and encourage and facilitate the creation of codes of conduct. Coordinate joint investigations by market surveillance authorities and the European Commission. Advice: Issue recommendations and written opinions to the European Commission and the Board regarding codes of conduct, codes of practice and guidelines. \n\nRole of the European Artificial Intelligence Role of the European Artificial Intelligence \n\nBoard (The Board) Board (The Board) Actions Actions \n\nThe Board comprises representatives from each The Board comprises representatives from each Member State and is tasked with advising and Member State and is tasked with advising and assisting the European Commission and the assisting the European Commission and the Member States on the consistent and effective Member States on the consistent and effective application of the AI Act. Additionally, the Board application of the AI Act. Additionally, the Board issues guidelines and recommendations (articles issues guidelines and recommendations (articles 65 and 66). 65 and 66). Representatives are appointed for a term of three Representatives are appointed for a term of three years, renewable once. They may be individuals years, renewable once. They may be individuals from public entities with expertise from public entities with expertise \n\nin AI and the authority to facilitate national-level in AI and the authority to facilitate national-level coordination. The Board is chaired by one of coordination. The Board is chaired by one of \n\nits representatives. its representatives. Coordination and cooperation: Among Coordination and cooperation: Among national competent authorities and Union national competent authorities and Union institutions, bodies, offices and agencies, as institutions, bodies, offices and agencies, as well as relevant Union expert groups and well as relevant Union expert groups and networks. networks. Expertise sharing: Collect and share technical Expertise sharing: Collect and share technical and regulatory expertise, best practices and and regulatory expertise, best practices and guidance documents. guidance documents. Advice and recommendations: Provide Advice and recommendations: Provide advice on the implementation of the AI Act, advice on the implementation of the AI Act, in particular as regards the enforcement of in particular as regards the enforcement of rules on general-purpose AI models, issue rules on general-purpose AI models, issue recommendations and written opinions (at recommendations and written opinions (at the request of the European Commission or the request of the European Commission or on its own initiative). on its own initiative). 61 1 2 3 4 5 6 7 8 9 10 \n\nRole of the European Artificial Intelligence Role of the European Artificial Intelligence \n\nBoard (The Board) Board (The Board) Actions Actions \n\nThe Board must establish two dedicated standing The Board must establish two dedicated standing \n\nsubgroups: subgroups: \n\nâ€¢ The standing subgroup for notifying authorities provides a  platform for cooperation and exchange on issues related to notified bodies \n\nâ€¢ The standing subgroup for market surveillance acts as the administrative cooperation group (ADCO) for the AI Act. The Board may establish other standing or The Board may establish other standing or temporary subgroups as appropriate for the temporary subgroups as appropriate for the purpose of examining specific issues. purpose of examining specific issues. The European Data Protection Supervisor and The European Data Protection Supervisor and the AI Office attend the Boardâ€™s meetings as the AI Office attend the Boardâ€™s meetings as observers. Other national and Union authorities, observers. Other national and Union authorities, bodies, or experts or representatives of the bodies, or experts or representatives of the advisory forum may be invited on a case-by-case advisory forum may be invited on a case-by-case basis. basis. Harmonisation: Standardise administrative Harmonisation: Standardise administrative practices and facilitate the development practices and facilitate the development of common criteria and a  shared of common criteria and a  shared understanding. understanding. Public awareness on AI: Work towards AI Public awareness on AI: Work towards AI literacy, public awareness and understanding literacy, public awareness and understanding of the benefits, risks, safeguards and rights of the benefits, risks, safeguards and rights \n\nand obligations in relation to the use of AI and obligations in relation to the use of AI \n\nsystems. systems. International Cooperation: Advise the International Cooperation: Advise the European Commission in relation to European Commission in relation to international matters on AI and cooperate international matters on AI and cooperate with competent authorities of third countries with competent authorities of third countries and with international organisations. and with international organisations. \n\nRole of the Advisory Forum Actions \n\nThe Advisory Forum has been created to ensure the involvement of stakeholders in the implementation and application of the AI Act (article 67). Members are appointed by the European Commission and represent a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society, and academia with recognised expertise in the field of AI. Members are appointed for a term of two years, which may be extended up to four years. They elect two co-chairs from among the members for a term of two years, renewable once. The Fundamental Rights Agency (FRA), the European Union Agency for Cybersecurity (ENISA), the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC), and the European Telecommunications Standards Institute (ETSI) shall be permanent members of the Advisory Forum. The Advisory Forum may establish standing or temporary sub-groups as appropriate for examining specific questions. The Advisory Forum meets at least twice a  year and may invite experts and other stakeholders to its meetings. Advice and technical expertise: Provide advice to the Board and the European Commission. Prepare opinions, recommendations, and written contributions upon request. Consultancy group: The European Commission has to consult the Forum when preparing a standardisation request or drafting common specifications as referred to in article 41. \n\nAnnual report: Prepare and publish an annual \n\nreport on its activities. 62 1 2 3 4 5 6 7 8 9 10 \n\nRole of the scientific panel of independent \n\nexpert Actions \n\nThe scientific panel is created to integrate the scientific community in supporting the European Commissionâ€™s enforcement activities (article 68). Experts are selected by the European Commission based on their current scientific or technical expertise in AI. The number of experts is determined by the European Commission, in consultation with \n\nthe Board, based on the required expertise \n\nneeds, ensuring fair gender and geographical representation. To provide the scientific panel with the necessary information for performing its tasks, a mechanism should be established allowing the panel to request the European Commission to obtain documentation or information from a provider. An implementing act will define how the scientific panel and its members can issue alerts and request assistance from the AI Office. Support the AI Office in the implementation and enforcement as regards general-purpose AI models and system: \n\nâ€¢ Alert the AI Office of possible systemic risks. \n\nâ€¢ Develop tools and methodologies for evaluating capabilities. \n\nâ€¢ Advise on the classification including systemic risk. \n\nâ€¢ Contribute to the development of tools and templates. \n\nâ€¢ Support market surveillance authorities: At their request including with regard to cross-border market surveillance activities. \n\nâ€¢ Assist in the Union safeguard procedure pursuant article 81. Support Member States with their enforcement activities upon demand: \n\nâ€¢ Member States may be required to pay fees for the advice and support provided by the scientific panel. \n\nâ€¢ The implementing act referred to in article 68(1) will define the fees and recoverable costs. 63 1 2 3 4 5 6 7 8 9 10 \n\nGovernance at national level: national competent authorities \n\nMember States play a crucial role in the application and enforcement of the AI Act. To ensure effective application, harmonisation, and coordination within the Union and among Member States, each Member State must designate at least one notifying authority and one market surveillance authority. Together, they constitute the national competent authorities. For AI systems used by Union institutions, agencies, offices, and bodies, the European Data Protection Supervisor will be the supervisory authority. \n\nRole of the notifying authority(ies) Actions \n\nThis authority is responsible for establishing and applying the framework for conformity assessment bodies (article 28). The authority must have an adequate number of competent personnel with the necessary expertise in fields such as information technology, AI, and law, including the supervision of fundamental rights. Notifying authorities must avoid any conflict of interest with conformity assessment bodies, ensuring the objectivity and impartiality of their activities. In particular, the decision to notify a conformity assessment body must not be made by the person who assessed the conformity assessment body. Setting up and carrying out procedures: Establish and execute necessary procedures for the assessment, designation, notification, and monitoring of conformity assessment bodies. Develop these procedures in cooperation with the notifying authorities of other Member States. Advice and guidance: Provide guidance and advice on the implementation of the AI Act, considering the input from the Board and the European Commission, and consulting national competent authorities under other Union laws, if applicable. Activity and service restrictions: \n\nâ€¢ Must not offer or provide any activities performed by conformity assessment bodies. \n\nâ€¢ Must not offer consultancy services on a commercial or competitive basis. 64 1 2 3 4 5 6 7 8 9 10 \n\n# Where can I find this? \n\nGovernance: Chapter VII  recitals 148-154, 163,179 \n\nEU Database: Chapter VIII  recital 131 \n\nEnforcement: Chapters IX and XII  recitals 162-164 and 168-172 \n\nRole of the market surveillance authority(ies) Actions \n\nResponsible for carrying out the activities and taking the measures pursuant to Regulation (EU) 2019/1020 (market surveillance and compliance of products) on market surveillance and compliance of products. One of the market surveillance authorities \n\nwill be designated by each Member State as \n\nthe single point of contact for the public and other counterparts at both Member State and Union levels. The European Data Protection Supervisor will act as the market surveillance authority for Union institutions, agencies, and bodies under the AI Act. Market surveillance authorities for high-risk AI systems in biometrics, used for law enforcement, migration, asylum, border control, justice, and democratic processes, should have strong investigative and corrective powers. This includes access to all personal data and necessary information for their task. Member States must facilitate coordination between market surveillance authorities and other relevant national authorities. Many of task and responsibilities of the market surveillance authorities are described above, but in addition they have the following tasks and responsibilities assigned to them: \n\nâ€¢ Authorisation for high-risk AI systems: Member States can temporarily authorise specific high-risk AI systems to be placed on the market or put into service in their territory for exceptional reasons of public security, health, environmental protection, or key infrastructure, pending conformity assessments (article 46). \n\nâ€¢ Annual reporting: to the European \n\nCommission and national competition authorities on surveillance activities and prohibited practices including: (i) any information identified that is of potential interest for the application of competition law; (ii) use of any prohibited practices; and (iii) measures taken in relation to those practices. \n\nâ€¢ Advice and guidance: Provide guidance and advice on the implementation of the AI Act, considering the input from the Board and the European Commission, and consulting national competent authorities under other Union laws, if applicable. 65 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ The AI Act entered into force on 1 August 2024. \n\nâ€¢ Most provisions are set to apply from 2 August 2026, and others are being phased in over a period of six to 36 months from the date of entry into force. \n\nâ€¢ The European Commission will develop delegated and implementing acts, guidelines, codes of conduct and standards. These initiatives are aimed at providing practical guidance, ethical principles and technical specifications related to the AI Act, with the goal of ensuring the effective implementation of the legislation. \n\nâ€¢ The Commission also sent, in July 2024, an updated version of its proposed AI Liability Directive to both the European Parliament and the Council for consideration. \n\nâ€¢ Bird & Birdâ€™s AI experts are equipped to \n\nmonitor the forthcoming initiatives expected under AI Act and help you navigate the different processes and requirements. All actors dealing with AI systems should actively monitor the development of the legislative and non-legislative initiatives outlined in this chapter. \n\n# To do list At a glance \n\nCHAPTER 9 \n\n# AI Act: Whatâ€™s Next 66 1 2 3 4 5 6 7 8 9 10 \n\n# AI Act: Whatâ€™s Next \n\nThis chapter provides an overview of the application deadlines of the AI Act and the forthcoming initiatives expected under the Regulation. The EU institutions regard the \n\nAI Act as a new form of â€œliving regulationâ€ that \n\nwill be supplemented on an ongoing basis via secondary legislation and other initiatives, in \n\nan effort to keep pace with technological advances. Over the coming months, the AI Act envisions the adoption of a range of delegated and implementing acts, guidance documents, codes of conduct, codes of practice and standardisation requests. These initiatives \n\nare designed to provide practical guidance, ethical principles and technical specifications regarding the Regulation, with the aim of ensuring effective implementation. The requirements laid down in such documents will greatly shape the effective implementation of 1.  Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) Text with EEA relevance, OJ L, 2024/1689, 12.7.2024. the AI Act and the ability of actors to comply with its obligations. All actors dealing with AI systems would therefore be advised to actively monitor the work of the Commission in developing the legislative and non-legislative initiatives mentioned in this chapter. Bird & Birdâ€™s Regulatory and Public Affairs team is equipped to monitor the forthcoming initiatives expected under AI Act and help you navigate the different processes and requirements. \n\n# AI Act application deadlines \n\nFollowing its publication in the EU Official Journal 1 on 12 July 2024, the AI Act entered into force on 1 August 2024. The relevant dates of application are set out below. \n\n12 July 2024 The AI Act was published in the Official Journal of the EU, triggering the dates for specific provisions in the Regulation becoming applicable. \n\n2 February 2025 Prohibited practices ban applies (Chapter II). AI literacy rules apply (article 4). \n\n2 May 2025 Codes of practice for general-purpose AI must be ready (article 56 (9)). \n\n2 August 2025 National authorities designated (Chapter III Section 4). Obligations for General-Purpose AI (GPAI) (Chapter V). Governance (at EU and national level) (Chapter VII). Confidentiality and penalties (other than in relation to gen-AI) \n\n(Chapter XII). \n\n2 August 2026 Start of application of all other provisions of the EU AI Act (unless a later date applies below). 67 1 2 3 4 5 6 7 8 9 10 \n\nBetween 1 August 2024 and 2 August 2027, the European Commission is expected to adopt various documents to implement the Regulation. These comprise delegated and implementing acts, guidance documents, codes of conduct, codes of practice and standardisation requests. Apart from a few exceptions, there are no specific set deadlines for the publication of these initiatives by the Commission. Nonetheless, it is assumed that the Commission will aim to adopt such documents ahead of the application deadlines of the respective provisions. \n\n# Delegated acts \n\nSeveral provisions will be the subject of delegated acts, to be adopted by the Commission to specify obligations and operational implementation. Article 97 grants the power to adopt delegated acts to the Commission for a five-year period that started on 1 August 2024. The Commission must report on this delegation nine months before the end of the period. This period is automatically extended for another five years unless the European Parliament or the Council opposes it three months before the end of each period. As mentioned above, there are no specific set deadlines for the adoption of such delegated acts. However, it must be presumed that their adoption will precede the application deadlines for the related provisions in the AI Act (see article 113). 2.  Inter-institutional Agreement between the European Parliament, the Council of the European Union and the European Commission on Better Law-Making, OJ L 123, 12.5.2016. Pursuant to article 97(4), before adopting a delegated act, the Commission will have to carry out public consultations during its preparatory work and will also consult with the relevant Expert Groups (composed of Member States experts). Once adopted, the Commission must notify the European Parliament and the Council simultaneously. A delegated act only enters into force if neither the European Parliament nor the Council objects within three months of notification, extendable by another three months if needed. The European Parliament or the Council can revoke this power at any time, but this will not affect the validity of existing delegated acts. In accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making 2, the \n\nCommission will have to ensure that the European Parliament and the Council receive all documents at the same time as Member Statesâ€™ experts, and the Parliament and Councilâ€™s experts should systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts. The AI Act foresees the adoption of the following delegated acts where the Commission considers this to be necessary: \n\nâ€¢ Article 6(6/7) : amend article 6(3) by adding new conditions to those laid down in paragraph 3, by modifying or by deleting them if there is concrete and reliable evidence of the existence of AI systems that should not fall \n\nunder Annex III or that should not fall under \n\nthe conditions of article 6(3); \n\n2 August 2027 High-risk categories listed in Annex I). General purpose AI models placed on the market before 2 August 2025 (article 111). \n\n2 August 2030 High-risk AI systems (other than those listed below), which have been placed on the market or put into service before 2 August 2026 and which are intended to be used by public authorities (article 111). \n\n31 December 2030 Components of large-scale IT systems listed in Annex X, which have \n\nbeen placed on the market or put into service before 2 August 2027 \n\n(article 111). 68 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ Article 7(1/3) : amend Annex III, by adding, modifying or removing use-cases of high-risk \n\nAI systems; \n\nâ€¢ Article 11(3) : amend Annex IV, where necessary, to ensure that, in light of technical progress, the technical documentation provides all the information necessary to assess the compliance of the system; \n\nâ€¢ Article 43(5) : amend Annexes VI and VII by updating them in light of technical progress; \n\nâ€¢ Article 45(6) : amend article 43(1/2) in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to third-party conformity assessments; \n\nâ€¢ Article 47(5) : amend Annex V by updating the content of the EU declaration of conformity set out in that Annex, in order to introduce elements that become necessary in light of technical progress; \n\nâ€¢ Article 51(3) : amend the thresholds for systemic general-purpose AI models listed in article 51(1/2) as well as to supplement benchmarks and indicators in light of evolving technological developments, such as algorithmic improvements or increased hardware efficiency, when necessary, for these thresholds to reflect the state of the art; \n\nâ€¢ Article 52(4) : amend Annex XIII by \n\nspecifying and updating the criteria for systemic general-purpose AI models; \n\nâ€¢ Article 53(5) : detail measurement and calculation methodologies with a view to allowing for comparable and verifiable documentation to facilitate compliance with Annex XI; and \n\nâ€¢ Article 53(6) : amend Annexes XI and XII in light of evolving technological development. \n\n# Implementing acts \n\nArticle 98(2) of the AI Act confers on the European Commission the power to adopt implementing acts in accordance with Regulation 182/2011 3. Implementing acts  aim to create 3.  Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 February 2011 laying down the rules and general principles concerning mechanisms for control by Member States of the Commissionâ€™s exercise of implementing powers, OJ L 55, 28.2.2011. uniform conditions for the implementation of a specific legislative act, if and when this is necessary. With respect to the drafting of the implementing acts, the Commission will be assisted by a â€œComitologyâ€ Committee comprising Member State experts. As is the case for delegated acts, the timeline for adoption of the expected implementing acts is not specified in the text, except for the foreseen implementing act referred to in article 72(3), which is due by 2 February 2026. Therefore, it should be presumed that the relevant implementing acts will be adopted ahead of the application deadlines for the related provisions in the AI Act (see above and article 113). The AI Act foresees the adoption of the following implementing acts, where the Commission deems it necessary to: \n\nâ€¢ Article 37(2) : suspend, restrict or withdraw the designation of notified bodies when the Member State fails to take the necessary corrective measures; \n\nâ€¢ Article 41(1/4/6) : establish, in consultation with the â€œAdvisory Forumâ€ referred to in \n\narticle 67, common specifications for the requirements for high-risk AI systems or \n\nfor the obligations for general-purpose AI \n\nmodels set out in Chapter V, Sections 2 and 3. When a reference to a harmonised standard is published in the Official Journal of the European Union, which covers the same requirements set out in Section 2 of this Chapter III, the Commission shall repeal the implementing act referred to in article 41(1). Where a Member State considers that a common specification does not entirely meet the requirements set out in Section 2 of this Chapter III, the Commission shall assess that information and, if appropriate, amend the implementing act referred to in article 41(1); \n\nâ€¢ Article 50(7) : approve codes of practice drawn up to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content, in accordance with the procedure laid down in article 56(6). If the code of practice is not adequate, the Commission may adopt an implementing act to lay down a set of common rules for the implementation of the transparency 69 1 2 3 4 5 6 7 8 9 10 \n\nobligations for providers and deployers of certain AI systems of article 50; \n\nâ€¢ Article 56(6) : approve a code of practice for general-purpose AI models and give it a general validity within the Union. If, by 2 August 2025, a code of practice cannot be finalised, or if the AI Office deems it is not adequate, the Commission may provide, by means of implementing acts, common rules for the implementation of the obligations provided for in articles 53 and 55, including the issues set out in article 56(2); \n\nâ€¢ Article 58(1) : specify the detailed arrangements for the establishment, development, implementation, operation and supervision of the AI regulatory sandboxes; \n\nâ€¢ Article 60(1) : specify the detailed elements of the real-world testing plan for providers of high-risk AI systems; \n\nâ€¢ Article 68(1) : make provisions on the establishment of a scientific panel of \n\nindependent experts (the â€œscientific panelâ€ )\n\nintended to support the enforcement activities of the AI Act; \n\nâ€¢ Article 72(3) : publish, by 2 February 2026, an implementing act laying down detailed provisions establishing a template for the post-market monitoring plan for providers of high-risk AI systems and the list of elements to be included in the plan; \n\nâ€¢ Article 92(6) : set out the detailed \n\narrangements and the conditions for the AI Office of general-purpose AI models evaluations, including the detailed arrangements for involving independent experts, and the procedure for the selection thereof; and \n\nâ€¢ Article 101(6) : lay down detailed arrangements and procedural safeguards for proceedings in view of the possible fines on providers of general-purpose AI models. \n\n# Commission Guidelines \n\nâ€œCommission Guidelinesâ€ are explanatory documents produced by the Commission services to provide practical and informal guidance about how particular provisions of the AI Act should be applied. The AI Act foresees the adoption of the following Commission Guidelines: \n\nâ€¢ Article 6(5) : after consulting the European Artificial Intelligence Board, and no later than 2 February 2026, specifying the practical implementation of article 6, including a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk; \n\nâ€¢ Article 63(1) : on the elements of the quality management system which may be complied with in a simplified manner considering the needs of microenterprises, without affecting the level of protection or the need for compliance with the requirements in respect of high-risk AI systems (no set deadline for these guidelines); \n\nâ€¢ Article 73(7) : to facilitate compliance with \n\nthe reporting obligations of serious incident. The guidance has to be adopted by 2 August 2025, and will have to be assessed regularly \n\nby the Commission; \n\nâ€¢ Article 96 : on the practical implementation of this Regulation. There is no set deadline for the development of these guidelines. However, the related provisions apply from 2 August 2026. In particular, the Commission is to develop guidelines on: \n\nâ€” the application of the requirements and obligations referred to in articles 8 to 15 and in article 25; \n\nâ€” the prohibited practices referred to in \n\narticle 5; \n\nâ€” the practical implementation of the provisions related to substantial modification; \n\nâ€” the practical implementation of transparency obligations laid down in article 50; \n\nâ€” detailed information on the relationship of the AI Act with the EU harmonisation legislation listed in Annex I, as well as with other relevant EU laws, including as regards consistency in their enforcement; and \n\nâ€” the application of the definition of an AI system as set out in article 3, point (1). 70 1 2 3 4 5 6 7 8 9 10 \n\n# Codes of conduct and practice \n\nCodes of conduct \n\nCodes of conduct are documents of a voluntary nature that establish ethical guidelines and principles for the development and use of AI in certain conditions. They are also intended to foster the development of AI policies within organisations for the voluntary application of specific AI Act obligations. The AI Act calls for the adoption of the following codes of conduct: \n\nâ€¢ Recital 20 and article 4 : voluntary codes of conduct to advance AI literacy among persons dealing with the development, operation and use of AI. \n\nâ€” While there is no set deadline for the \n\ndevelopment of voluntary codes of practice to advance AI literacy, the related provisions on AI literacy in Article 4 will apply from 2 February 2025. \n\nâ€¢ Recital 165 and article 95 : codes of conduct intended to foster the voluntary application to AI systems of some or all the mandatory requirements applicable to high-risk AI systems. \n\nThese are adapted in light of the intended \n\npurpose of the systems and the lower risk involved, and take into account the available technical solutions and industry best practices such as model and data cards: \n\nâ€” to ensure that the voluntary codes of conduct are effective, they should be based on clear objectives and key performance indicators to measure the achievement of those objectives; \n\nâ€” they should also be developed in an inclusive way, as appropriate, with the involvement of relevant stakeholders such as business and civil society organisations, academia, research organisations, trade unions and consumer protection organisations; and \n\nâ€” while there is no set deadline for the development of voluntary codes of practice intended to foster the application to AI systems of some or all the mandatory requirements applicable to high-risk AI systems, the related provisions included in Article 95 will apply from 2 February 2026. By 2 August 2028 and every three years thereafter, the Commission is due to evaluate the impact and effectiveness of such voluntary codes of conduct. \n\nCodes of practice \n\nCodes of practice represent a central tool for proper compliance with specific obligations under the AI Act. In particular, one code of practice will detail the AI Act rules for providers of general-purpose AI models and general-purpose AI models with systemic risks. Another code of practice will focus on the detection and labelling of artificially generated or manipulated content. Organisations should be able to rely on codes of practice to demonstrate compliance with the relevant obligations, which is known as a \n\nâ€œpresumption of conformityâ€ .Specifically, the AI Act calls on the European Commissionâ€™s AI Office to facilitate the drawing up of the following codes of practice together with all interested stakeholders: \n\nâ€¢ Article 50(7): codes of practice at EU level to facilitate the effective implementation of the obligations in article 50(2/4), regarding the detection and labelling of artificially generated or manipulated content. The Commission may adopt implementing acts to approve those codes of practice. While there is no set deadline for the development of voluntary codes of practice to facilitate the effective implementation of the obligations in article 50(2/4), the related provisions included in Article 50 will apply from 2 February 2026. \n\nâ€¢ Article 56(1/3) : by 2 May 2025, codes of practice for general-purpose AI models. These will duly take into account international approaches as well as a diverse set of perspectives, by collaborating with relevant national competent authorities and, where appropriate, by consulting with civil society organisations and other relevant stakeholders and experts. These include the â€œScientific Panelâ€ of independent \n\nexperts established under the AI Act. By 2 August 2028 and every three years thereafter, the Commission will have to evaluate the impact and effectiveness of voluntary codes of practice. On 30 July 2024, the European AI Office opened \n\na call for expressions of interest to participate in the drawing-up of the first general-purpose AI Code of Practice. Interested parties could  express 71 1 2 3 4 5 6 7 8 9 10 \n\ntheir interest in participating by  25 August  2024. According to the Commission, this Code will be prepared by means of an iterative drafting process by April 2025, nine months from the AI Actâ€™s entry into force on 1 August 2024. The Code of Practice will facilitate the proper application of the rules of the AI Act for general-purpose AI models. The Commission may decide to approve the Code of Practice and give it a general validity within the European Union by means of an implementing act, pursuant to article 56(6).  If the Code of Practice is not deemed adequate, the Commission will provide common rules for the implementation of the relevant obligations. In addition, on 30 July 2024, the AI Office launched a consultation on trustworthy general-purpose AI models under the AI Act, specifically regarding the template for the summary of the content used for the training of the general-purpose AI models and the accompanying guidance. The deadline for responses was \n\n10 September 2024. \n\n# Standards \n\nInitial standardisation work \n\nThe process of drafting European standards in support of the AI Act started well before the adoption of the AI Act, with the Commissionâ€™s \n\nproposal on harmonised rules on artificial intelligence adopted as the Commission \n\nImplementing Decision C(2023)3215 on \n\n22 May 2023. This Implementing Decision requested the European Committee for Standardisation (CEN) and the European Committee for Electrotechnical \n\nStandardisation (CENELEC) to draft the \n\nfollowing new European standards or European standardisation deliverables on AI by 30 April 2025: \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on risk management systems for AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on governance and quality of datasets used to build \n\nAI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on record keeping through logging capabilities by AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on transparency and information provisions for users of AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on human oversight of AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on accuracy specifications for AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on robustness specifications for AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on cybersecurity specifications for AI systems; \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on quality management systems for providers of AI systems, including post-market monitoring processes; and \n\nâ€¢ European standard(s) and/or European standardisation deliverable(s) on conformity assessment for AI systems. \n\nThis standardisation request to CEN and \n\nCENELEC was made pursuant to action 63 of the European Commission 2022 â€œAnnual Union Work Programme for European standardisationâ€ with the aim of ensuring that AI systems are safe and trustworthy. \n\nFor the drafting of these standards, CEN \n\nand CENELEC have set up a specific joint technical committee named â€œCEN-CENELEC JTC 21 \n\nArtificial Intelligenceâ€ . CEN and CENELEC are \n\nalso collaborating on the drafting with the European Telecommunications Standards \n\nInstitute (ETSI) , an independent, not-for-profit, standardisation organisation in the field of information and communication. \n\nAI Act standardisation request \n\nArticle 40(2) of the AI Act calls on the European Commission to present, without undue delay \n\nafter the entry into force of the Regulation ,\n\nstandardisation requests for harmonised EU AI standards covering: \n\nâ€¢ all requirements set out in Section 2 of Chapter III of the AI Act; and 72 1 2 3 4 5 6 7 8 9 10 \n\nâ€¢ as applicable, standardisation requests covering obligations set out in Chapter V, Sections 2 and 3 of the AI Act. These requests revise the requests included \n\nin Commission Implementing Decision \n\nC(2023)3215 . This was also anticipated in the Commissionâ€™s Standardisation Work Programme \n\nfor 2024 published in February 2024. Indeed, Action 15 of the Work Programme calls for a \n\nâ€œrevision of the standardisation request in support \n\nof Union policy on artificial intelligenceâ€ , thereby calling for the revision of the Commission Decision in view of the final AI \n\nAct text. According to article 40(2) of the AI Act, the standardisation requests should also ask for deliverables on reporting and documentation processes to improve AI systemsâ€™ resource performance. Such requests could include reducing the consumption of energy and of other resources by high-risk AI systems during their lifecycle and the energy-efficient development of general-purpose AI models. The Commission should draft the requests after consulting with the European Artificial Intelligence Board and relevant stakeholders, including the Advisory Forum of stakeholders established under the AI Act. In addition, when issuing standardisation requests to the relevant European standardisation organisations, the Commission should specify that standards have to be clear and consistent. This prerequisite includes standards developed in the various sectors for products covered by the existing EU harmonisation legislation listed in Annex I. \n\nThey are aimed at ensuring that high-risk AI systems or general-purpose AI models placed on the market or put into service in the EU meet the relevant requirements or obligations laid down in the AI Act. By 2 August 2028 and every four years thereafter, the Commission will have to submit a report to review the progress made regarding the development of standardisation deliverables on the energy-efficient development of general-purpose AI models. In this context, the Commission will also be required to assess the need for further measures or actions, including binding measures or actions. The report will have to be submitted to the European Parliament and to the Council and made public. \n\n# Liability \n\nCommission amends proposal to align with AI Act \n\nFinally, it is worth noting that at the end of July 2024, the European Commission sent an updated version of its proposal adapting non-contractual civil liability rules to artificial intelligence (AI Liability Directive or AILD) to both the European Parliament and the Council. This proposal, which was first tabled by the Commission in September 2022, aims to address the risks generated by specific uses of AI through a set of rules focusing on respect of fundamental rights and safety. \n\nThe current changes are designed to align the \n\nAI Liability Directive proposal with the \n\ncompleted AI Act. It is notable that the new proposal amends article 4 regarding the increased potential responsibility of companies deploying AI systems. These deployers would now be presumed liable for damage caused if they â€œdid not monitor the operation of the AI system or, where appropriate, suspend [its] useâ€ or did not use â€œsufficiently representativeâ€ input data. The European Parliamentâ€™s lead draftsperson \n\n(â€œrapporteurâ€ ) for this file, the German Christian-democratic MEP Axel Voss, had previously requested the European Parliamentary Research Service to conduct an â€œalternative impact assessmentâ€ to evaluate whether the AILD is \n\nstill necessary in view of adoption of the AI Act. While the future of the proposed AI Liability Directive remains uncertain, it may proceed in \n\na reduced form. 73 1 2 3 4 5 6 7 8 9 10 \n\n# AI Guide Contributors \n\nAnne-Sophie Lampe \n\nPartner \n\n+33142686333 \n\nanne-sophie.lampe@twobirds.com \n\nBenoit Van Asbroeck \n\nOf Counsel \n\n+3222826067 \n\nbenoit.van.asbroeck@twobirds.com \n\nTobias BrÃ¤utigam \n\nPartner \n\n+358962266758 \n\ntobias.brautigam@twobirds.com As a market-leading law firm for technology, ranked Tier 1 for AI (first ranking of its kind within the European legal directory community) and TMT by Legal 500 in 12 jurisdictions and Band 1 for global multi-jurisdictional TMT by Chambers, we distinguish ourselves through our deep understanding of the technical intricacies involved in AI technology development and deployment. This expertise enables us to effectively collaborate with developers and commercial teams, speaking their language and asking the right questions from the outset. Our international AI group comprises over 120 experts , covering virtually every intersection where this transformative technology meets law and regulation. From handling ground-breaking IP litigation and guiding clients through complex regulatory changes to implementing effective governance frameworks and innovating commercial and contractual arrangements. If you have any questions about the content, please get in touch with any of the contributors below or your usual Bird & Bird contact. You can also find out more about the latest AI developments in our AI Hub \n\nPaolo Sasdelli \n\nRegulatory and Public Affairs Advisor \n\n+3222826076 \n\npaolo.sasdelli@twobirds.com \n\nBelgium Finland France Germany \n\nCen Zhang \n\nSenior Associate \n\n+33142686000 \n\ncen.zhang@twobirds.com \n\nDr. Miriam Ballhausen \n\nPartner \n\n+4940460636000 \n\nmiriam.ballhausen@twobirds.com \n\nFrancine Cunningham \n\nRegulatory and Public Affairs Director \n\n+3222826056 \n\nfrancine.cunningham@twobirds.com \n\nOliver Belitz \n\nCounsel \n\n+4969742226000 \n\noliver.belitz@twobirds.com \n\nDr. Nils LÃ¶lfing \n\nCounsel \n\n+4921120056000 \n\nnils.loelfing@twobirds.com 74 1 2 3 4 5 6 7 8 9 10 \n\nMarta Kwiatkowska-Cylke \n\nCounsel \n\n+48225837964 \n\nmarta.kwiatkowska-cylke@ twobirds.com \n\nGian Marco Rinaldi \n\nCounsel \n\n+390230356071 \n\ngianmarco.rinaldi@twobirds.com \n\nDr. Maria Jurek \n\nSenior Associate \n\n+48225837839 \n\nmaria.jurek@twobirds.com \n\nItaly Poland \n\nAleksandra Cywinska \n\nSenior Associate \n\n+48225837875 \n\naleksandra.cywinska@twobirds.com \n\nAleksandra Mizerska \n\nLawyer \n\n+48225837900 \n\naleksandra.mizerska@twobirds.com \n\nAndrzej Stelmachowski \n\nAssociate \n\n+48225837977 \n\nandrzej.stelmachowski \n\n@twobirds.com \n\nIzabela Kowalczuk-Pakula \n\nPartner \n\n+48225837932 \n\nizabela.kowalczuk-pakula@ twobirds.com \n\nPawel Lipski \n\nPartner \n\n+48225837991 \n\npawel.lipski@twobirds.com \n\nDr. Simon Hembt \n\nSenior Associate \n\n+4969742226000 \n\nsimon.hembt@twobirds.com \n\nThe Netherlands \n\nFeyo Sickinghe \n\nOf Counsel \n\n+31703538904 \n\nfeyo.sickinghe@twobirds.com \n\nSpain \n\nJoaquÃ­n MuÃ±oz \n\nPartner \n\n+34917906007 \n\njoaquin.munoz@twobirds.com \n\nUnited Kingdom \n\nAlex Jameson \n\nSenior Associate \n\n+442078507139 \n\nalex.jameson@twobirds.com \n\nGermany 75 1 2 3 4 5 6 7 8 9 10 \n\nLiz McAuliffe \n\nAssociate \n\n+442074156787 \n\nliz.mcauliffe@twobirds.com \n\nNora Santalu \n\nAssociate \n\n+442079826513 \n\nnora.santalu@twobirds.com \n\nUnited Kingdom \n\nIan Edwards \n\nPartner \n\n+442079056377 \n\nian.edwards@twobirds.com \n\nKaterina Tassi \n\nSenior Associate \n\n+442074156066 \n\nkaterina.tassi@twobirds.com \n\nKatharine Stephens \n\nPartner \n\n+442074156104 \n\nkatharine.stephens@ twobirds.com \n\nWill Bryson \n\nSenior Associate \n\n+442074156746 \n\nwill.bryson@twobirds.com \n\nToby Bond \n\nPartner \n\n+442074156718 \n\ntoby.bond@twobirds.com \n\nRuth Boardman \n\nPartner \n\n+442074156018 \n\nruth.boardman@twobirds.com 9 10 \n\n# twobirds.com \n\nThe information given in this document concerning technical legal or professional subject matter is for guidance only and does not constitute legal or professional advice. Always consult a suitably qualified lawyer on any specific legal problem or matter. Bird & Bird assumes no responsibility for such information contained in this document and disclaims all liability in respect of such information. This document is confidential. Bird & Bird is, unless otherwise stated, the owner of copyright of this document and its contents. No part of this document may be published, distributed, extracted, re-utilised, or reproduced in any material form. Bird & Bird is an international legal practice comprising Bird & Bird LLP and its affiliated and associated businesses. Bird & Bird LLP is a limited liability partnership, registered in England and Wales with registered number OC340318 and is authorised and regulated by the Solicitors Regulation Authority (SRA) with SRA ID497264. Its registered office and principal place of business is at 12 New Fetter Lane, London EC4A 1JP. A list of members of Bird & Bird LLP and of any non-members who are designated as partners, and of their respective professional qualifications, is open to inspection at that address.", "fetched_at_utc": "2026-02-08T19:07:36Z", "sha256": "d085ee7a46ea39634d2d38eef3a2aa413dcb9d798e771bed5c427fa8f6263dc3", "meta": {"file_name": "European Union Artificial Intelligence Act - Bird & Bird.pdf", "file_size": 1313072, "relative_path": "pdfs\\European Union Artificial Intelligence Act - Bird & Bird.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 46141}}}
{"doc_id": "pdf-pdfs-fbpml-organisationbp-v1-0-0-17-30-1a8bfb7dcffa", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_OrganisationBP_V1.0.0-17-30.pdf", "title": "FBPML_OrganisationBP_V1.0.0-17-30", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder. \n\n17 FBPML Organisation Best Practices v1.0.0. \n\n# Section 4. Data Governance \n\nObjective \n\nTo ensure the integrity, normalisation, fairness and non-discrimination of Projet and/or Model data. \n\n4. Data Governance \n\nControl:  Aim: \n\n4.1.  Data Governance \n\nPolicy \n\nA Policy and Guide, which promotes good \n\nData Governance in Product and Model \n\ndesign, development, and implementation \n\nought to be derived by Data Science \n\nManagers and approved by the Managerial \n\nCommittee. If a generic Data Governance \n\nPolicy already exists, the above should be \n\nintegrated accordingly. \n\nTo (a) ensure the integrity, \n\nnormalisation, fairness and non-\n\ndiscrimination of Product and/or \n\nModel data; and (b) provide clear \n\nOrganisation guidance to Products \n\non how to warrant data integrity, \n\nnormalisation, fairness and non-\n\ndiscrimination. \n\n4.2.  Data Governance \n\nProcedures \n\nA set of Procedures to operationalise \n\nthe Data Governance Policy should be \n\ndeveloped and implemented within \n\nProducts in light of Product Definitions, \n\nthe Product Risk Classification Portfolio, \n\nand Product Lifecycle and Workflow \n\nDescriptions. \n\nTo ensure the Data Governance of \n\nProducts and Models. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n18 FBPML Organisation Best Practices v1.0.0. \n\n# Section 5. Product and Model \n\n# Oversight & Management \n\nObjective \n\nTo (a) identify possible risks for protected classes of persons, animals and the natural environment; and (b) \n\nminimise the unequal distribution of Products and Models errors to prevent reinforcing and/or deriving social \n\ninequalities and/or ills. \n\n5.1 Fairness & Non-Discrimination \n\nControl:  Aim: \n\n5.1.1.  Fairness Policy  A Policy and Guide, which promotes \n\nProduct Fairness in - (a) Product \n\nDefinitions and Product design, \n\ndevelopment, and implementation; (b) \n\ndata processing; and (c) Model design, \n\ndevelopment, training, and output \n\nought to be derived by Data Science \n\nManagers and approved by the Managerial \n\nCommittee. \n\nTo ensure the Fairness of Machine \n\nLearning. \n\nTo provide clear Organisation \n\nguidance to Products on how \n\nto - (a) identify biases in Product \n\ndata and Models; (b) take \n\nremedial action against identified \n\nbiases; (c) identify and reduce \n\nasymmetric error rates between \n\nsubpopulations; and (d) implement \n\ndesign and processes to avoid and \n\ncircumvent risks that cannot be \n\nsolved by purely technical means. \n\n5.1.2.  Product Fairness \n\nProcedures \n\nA set of Procedures to operationalise the \n\nFairness Policy should be developed and \n\nimplemented within Products in light of \n\nProduct Definitions, the Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure the Fairness of Products \n\nand Models. \n\n5.1.3.  Fairness \n\nAssessments \n\nProducts should regularly complete \n\nFairness assessments according \n\nto Product Lifecycle and Workflow \n\nDescriptions to the extent that is \n\nreasonably practical. Assessment \n\nfindings ought to be documented by \n\nthe Product Team and reviewed by Data \n\nScience Managers and, when relevant, the \n\nManagement Committee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, Product \n\nFairness and non-discrimination \n\nat regular intervals within the \n\nProduct Lifecycle and Workflow \n\nDescription. \n\n5.1.4.  Review of the \n\nFairness Policy \n\nThe Fairness Policy should be reviewed \n\nperiodically, or if significant changes \n\noccur, by Data Science Managers to \n\nensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Fairness Policy \n\nis kept up-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n19 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\n5.1.5.  Review of \n\nProduct Fairness \n\nProcedures \n\nProduct Fairness Procedures should be \n\nreviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product Fairness \n\nProcedures are kept up-to-date. \n\nObjective \n\nTo ensure Data Quality and prevent unintentional effects, changes and/or deviations in Products and Models \n\noutputs associated with poor Product data. \n\n5.2 Data Quality \n\nControl:  Aim: \n\n5.2.1.  Data Quality Policy  A Policy and Guide, which describes \n\nthe assessment and remediation of an \n\nOrganisationâ€™s Data Quality. Chapters \n\nrelating to (a) Product Definitions and \n\nProduct design, development, and \n\nimplementation; (b) data processing; and \n\n(c) Model design, development, training, \n\nand output ought to be derived and \n\nincluded by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Data Quality of \n\nMachine Learning. \n\nTo provide Product Teams with \n\nreliable guidance on how to - (a) \n\nidentify Data Quality risks; (b) take \n\nremedial actions against identified \n\nData Quality risks; and (c) take \n\nsteps to account for Data Quality \n\nrisks and associated issues. \n\n5.2.2.  Product Data \n\nQuality Procedures \n\nA set of Procedures to operationalise the \n\nData Quality Policy should be developed \n\nand implemented within Products in light \n\nof Product Definitions, the Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions . \n\nTo ensure the Data Quality of \n\nProducts and Models. \n\n5.2.3.  Data Quality \n\nAssessments \n\nProducts should regularly complete Data \n\nQuality assessments according to Product \n\nLifecycle and Workflow Descriptions to \n\nthe extent that is reasonably practical. \n\nAssessment findings ought to be \n\ndocumented by the Product Team and \n\nreviewed by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo analyse, test, and report the \n\nrisks identified with, and measures \n\ntaken to ensure, Product Data \n\nQuality at regular intervals within \n\nthe Product Lifecycle and Workflow \n\nDescription. \n\n5.2.4.  Review of the Data \n\nQuality Policy \n\nThe Data Quality Policy should be reviewed \n\nperiodically, or if significant changes \n\noccur, by Data Science Managers to \n\nensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Data Quality \n\nPolicy is kept up-to-date. \n\n5.2.5.  Review of Product \n\nData Quality \n\nProcedures \n\nProduct Data Quality Procedures should \n\nbe reviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product Data Quality \n\nProcedures are kept up-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n20 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo (a) ensure that Product data and Models are representative of, and accurately specified for, target \n\nenvironments as far as is reasonably practical; and (b) guard against unintentional Products and Models \n\nbehaviours and outputs as far as is reasonably practical. \n\n5.3 Representativeness & Specification \n\nControl:  Aim: \n\n5.3.1.  Representativeness & \n\nSpecification Policy \n\nA Policy and Guide, which promotes \n\nRepresentativeness and Specification \n\nin - (a) Product Definitions and Product \n\ndesign, development, and implementation; \n\n(b) data processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the \n\nRepresentativeness and \n\nSpecification of Machine \n\nLearning. \n\nTo provide Product Teams with \n\nreliable guidance on how to -\n\n(a) identify Representativeness \n\nand Specification risks; (b) \n\nredress misspecification; and (c) \n\nremedy mis-, under- or over-\n\nrepresentation risks. \n\n5.3.2.  Product \n\nRepresentativeness \n\n& Specification \n\nProcedures \n\nA set of Procedures to operationalise the \n\nRepresentativeness & Specification Policy \n\nshould be developed and implemented \n\nwithin Products in light of Product \n\nDefinitions, the Product Risk Classification \n\nPortfolio, and the Product Lifecycle and \n\nWorkflow Descriptions. \n\nTo ensure the \n\nRepresentativeness and \n\nSpecification of Products and \n\nModels. \n\n5.3.3.  Representativeness \n\n& Specification \n\nAssessments \n\nProducts should regularly complete \n\nRepresentativeness & Specification \n\nassessments according to Product \n\nLifecycle and Workflow Descriptions to \n\nthe extent that is reasonably practical. \n\nAssessment findings ought to be \n\ndocumented by the Product Team and \n\nreviewed by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Representativeness and \n\nSpecification at regular intervals \n\nwithin the Product Lifecycle and \n\nWorkflow Description. \n\n5.3.4.  Review of the \n\nRepresentativeness & \n\nSpecification Policy \n\nThe Representativeness & Specification \n\nPolicy should be reviewed periodically, \n\nor if significant changes occur, by Data \n\nScience Managers to ensure its continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that the \n\nRepresentativeness & \n\nSpecification Policy is kept up-\n\nto-date. \n\n5.3.5.  Review of Product \n\nRepresentativeness \n\n& Specification \n\nProcedures \n\nProduct Representativeness & \n\nSpecification Procedures should be \n\nreviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that \n\nRepresentativeness & \n\nSpecification Procedures are \n\nkept up-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n21 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo warrant Model outcomes and prevent unintentional Model behaviour a priori under operational conditions as \n\nfar as is reasonably practical. \n\n5.4 Performance Robustness \n\nControl:  Aim: \n\n5.4.1.  Performance \n\nRobustness Policy \n\nA Policy and Guide, which promotes \n\nProduct Performance Robustness in - (a) \n\nProduct Definitions and Product design, \n\ndevelopment, and implementation; (b) \n\ndata processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Performance and \n\nRobustness of Machine Learning. \n\nTo provide Product Teams with \n\nreliable guidance on how to -\n\n(a) test, control, and improve \n\nPerformance Robustness under \n\noperational conditions before \n\ngoing live; and (b) assess and \n\ncontrol Performance Robustness \n\nrisks concerning unexpected \n\nconditions. \n\n5.4.2.  Product \n\nPerformance \n\nRobustness \n\nProcedures \n\nA set of Procedures to operationalise the \n\nPerformance Robustness Policy should \n\nbe developed and implemented within \n\nProducts in light of Product Definitions, \n\nthe Product Risk Classification Portfolio, \n\nand the Product Lifecycle and Workflow \n\nDescriptions . \n\nTo ensure the Performance \n\nRobustness of Products and \n\nModels. \n\n5.4.3.  Performance \n\nRobustness \n\nAssessments \n\nProducts should regularly complete \n\nPerformance Robustness Assessments \n\naccording to Product Lifecycle and \n\nWorkflow Descriptions to the extent \n\nthat is reasonably practical. Assessment \n\nfindings ought to be documented by \n\nthe Product Team and reviewed by Data \n\nScience Managers and, when relevant, the \n\nManagement Committee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Performance Robustness \n\nat regular intervals within the \n\nProduct Lifecycle and Workflow \n\nDescription. \n\n5.4.4.  Review of the \n\nPerformance \n\nRobustness Policy \n\nThe Performance Robustness Policy should \n\nbe reviewed periodically, or if significant \n\nchanges occur, by Data Science Managers \n\nto ensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Performance \n\nRobustness Policy is kept up-to-\n\ndate. \n\n5.4.5.  Review of Product \n\nPerformance \n\nRobustness \n\nProcedures \n\nProduct Performance Robustness \n\nProcedures should be reviewed \n\nperiodically, or if significant changes \n\noccur, by the Product Team to ensure their \n\ncontinued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that Performance \n\nRobustness Procedures are kept \n\nup-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n22 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo ensure that Products and Models remain within acceptable operational bounds. \n\n5.5 Monitoring & Maintenance \n\nControl:  Aim: \n\n5.5.1.  Monitoring & \n\nMaintenance Policy \n\nA Policy and Guide, which promotes \n\nProduct monitoring and maintenance \n\nin - (a) Product Definitions and Product \n\ndesign, development, and implementation; \n\n(b) data processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the monitoring and \n\nmaintenance of Machine Learning. \n\nTo provide Product Teams with \n\nreliable guidance on how to -\n\n(a) define, monitor and maintain \n\nacceptable operating bounds, \n\nincluding, inter alia, guarding \n\nagainst model drift; (b) define \n\nand review alert conditions \n\nand severity; and (c) create \n\nscenario playbooks regarding \n\nresponsibility, escalation, roll-\n\nback and resolution. \n\n5.5.2.  Product Monitoring \n\n& Maintenance \n\nProcedures \n\nA set of Procedures to operationalise the \n\nMonitoring & Maintenance Policy should \n\nbe developed and implemented within \n\nProducts in light of Product Definitions, \n\nthe Product Risk Classification Portfolio, \n\nand the Product Lifecycle and Workflow \n\nDescriptions. \n\nTo ensure the monitoring and \n\nmaintenance of Products and \n\nModels. \n\n5.5.3.  Monitoring & \n\nMaintenance \n\nAssessments \n\nProducts should regularly complete \n\nMonitoring & Maintenance assessments \n\naccording to Product Lifecycle and \n\nWorkflow Descriptions to the extent \n\nthat is reasonably practical. Assessment \n\nfindings ought to be documented by \n\nthe Product Team and reviewed by Data \n\nScience Managers and, when relevant, the \n\nManagement Committee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct monitoring and \n\nmaintenance at regular intervals \n\nwithin the Product Lifecycle and \n\nWorkflow Description. \n\n5.5.4.  Review of the \n\nMonitoring & \n\nMaintenance Policy \n\nThe Monitoring & Maintenance Policy \n\nshould be reviewed periodically, or \n\nif significant changes occur, by Data \n\nScience Managers to ensure its continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that the Monitoring & \n\nMaintenance Policy is kept up-to-\n\ndate. \n\n5.5.5.  Review of Product \n\nMonitoring & \n\nMaintenance \n\nProcedures \n\nProduct Monitoring & Maintenance \n\nProcedures should be reviewed \n\nperiodically, or if significant changes \n\noccur, by the Product Team to ensure their \n\ncontinued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that Monitoring & \n\nMaintenance Procedures are kept \n\nup-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n23 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo ensure Products and Models functions and outputs are explainable and justifiable as far as is practically \n\nreasonable. \n\n5.6 Explainability \n\nControl:  Aim: \n\n5.6.1.  Explainability \n\nPolicy \n\nA Policy and Guide, which promotes \n\nProduct Explainability in - (a) Product \n\nDefinitions and Product design, \n\ndevelopment, and implementation; (b) \n\ndata processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Explainability of \n\nMachine Learning. \n\nTo provide clear Organisation \n\nguidance to Products on how \n\nto - (a) ensure the justifiability \n\nof individual Model predictions \n\nand decisions; (b) maintain and \n\npromote Product and Model \n\ninner workings amongst Product \n\nTeams, Business Stakeholders, \n\nOrganisation Stakeholders \n\nand end-consumers; and (c) \n\nprovide Model transparency for \n\nauthorities, Special Interest \n\nGroups and/or the Public. \n\n5.6.2.  Product \n\nExplainability \n\nProcedures \n\nA set of Procedures to operationalise the \n\nExplainability Policy should be developed \n\nand implemented within Products in light \n\nof Product Definitions, the Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure the Explainability of \n\nProducts and Models. \n\n5.6.3.  Explainability \n\nAssessments \n\nProducts should regularly complete \n\nExplainability assessments according \n\nto Product Lifecycle and Workflow \n\nDescriptions to the extent that is \n\nreasonably practical. Assessment \n\nfindings ought to be documented by \n\nthe Product Team and reviewed by Data \n\nScience Managers and, when relevant, the \n\nManagement Committee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Explainability at \n\nregular intervals within the \n\nProduct Lifecycle and Workflow \n\nDescription. \n\n5.6.4.  Review of the \n\nExplainability \n\nPolicy \n\nThe Explainability Policy should be \n\nreviewed periodically, or if significant \n\nchanges occur, by Data Science Managers \n\nto ensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Explainability \n\nPolicy is kept up-to-date. \n\n5.6.5.  Review of Product \n\nExplainability \n\nProcedures \n\nProduct Explainability Procedures should \n\nbe reviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product \n\nExplainability Procedures are kept \n\nup-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n24 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo (a) prevent adversarial actions against, and encourage graceful failures for, Products and/or Models; (b) avert \n\nmalicious extraction of Models, data and/or intellectual property; (c) prevent Model based physical or irreparable \n\nharms; and (d) prevent erosion of trust in outputs or methods. \n\n5.7 Safety & Security \n\nControl:  Aim: \n\n5.7.1.  Safety & Security \n\nPolicy \n\nA Policy and Guide, which promotes \n\nProduct Safety & Security in - (a) \n\nProduct Definitions and Product design, \n\ndevelopment, and implementation; (b) \n\ndata processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Safety of Machine \n\nLearning. \n\nTo provide clear Organisation \n\nguidance to Products on how \n\nto identify and guard against \n\nProduct vulnerabilities from \n\nAdversarial Actions. \n\n5.7.2.  Product Safety \n\nProcedures \n\nA set of Procedures to operationalise the \n\nSafety Policy should be developed and \n\nimplemented within Products in light of \n\nProduct Definitions, the Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions . \n\nTo ensure the Safety of Products \n\nand Models. \n\n5.7.3.  Safety \n\nAssessments \n\nProducts should regularly complete \n\nSafety assessments according to Product \n\nLifecycle and Workflow Descriptions to \n\nthe extent that is reasonably practical. \n\nAssessment findings ought to be \n\ndocumented by the Product Team and \n\nreviewed by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Safety at regular intervals \n\nwithin the Product Lifecycle and \n\nWorkflow Description. \n\n5.7.4.  Review of the \n\nSafety Policy \n\nThe Safety Policy should be reviewed \n\nperiodically, or if significant changes \n\noccur, by Data Science Managers to ensure \n\nits continued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that the Safety Policy is \n\nkept up-to-date. \n\n5.7.5.  Review of Product \n\nSafety Procedures \n\nProduct Safety Procedures should be \n\nreviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product Safety \n\nProcedures are kept up-to-date. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n25 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo ensure (a) building desirable solutions; (b) human control over Products and Models; and (c) that individuals \n\naffected by Products and Models outputs can obtain redress. \n\nControl:  Aim: \n\n5.8.1.  Human-centric \n\nDesign & Redress \n\nPolicy \n\nA Policy and Guide, which promotes \n\nProduct Human-Centric Design & Redress \n\nin - (a) Product Definitions and Product \n\ndesign, development, and implementation; \n\n(b) data processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Human-Centric \n\nDesign & Redress of Machine \n\nLearning. \n\nTo provide clear Organisation \n\nguidance to Products on how to -\n\n(a) adds value and desirability of \n\nyour product for end users; \n\n(b) assess and implement human-\n\nin-control requirements; and \n\n(c) assess and implement human-\n\ncentric remediation and redress. \n\n5.8.2.  Product Human-\n\nCentric Design \n\n& Redress \n\nProcedures \n\nA set of Procedures to operationalise the \n\nHuman-Centric Design & Redress Policy \n\nshould be developed and implemented \n\nwithin Products in light of Product \n\nDefinitions, the Product Risk Classification \n\nPortfolio, and the Product Lifecycle and \n\nWorkflow Descriptions. \n\nTo ensure the Human-Centric \n\nDesign & Redress of Products and \n\nModels. \n\n5.8.3.  Human-Centric \n\nDesign & Redress \n\nAssessments \n\nProducts should regularly complete \n\nHuman-Centric Design & Redress \n\nassessments according to Product \n\nLifecycle and Workflow Descriptions to \n\nthe extent that is reasonably practical. \n\nAssessment findings ought to be \n\ndocumented by the Product Team and \n\nreviewed by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Human-Centric Design \n\n& Redress at regular intervals \n\nwithin the Product Lifecycle and \n\nWorkflow Description. \n\n5.8.4.  Review of the \n\nHuman-Centric \n\nDesign & Redress \n\nPolicy \n\nThe Human-Centric Design Policy & \n\nRedress should be reviewed periodically, \n\nor if significant changes occur, by Data \n\nScience Managers to ensure its continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that the Human-Centric \n\nDesign & Redress Policy is kept \n\nup-to-date. \n\n5.8.5.  Review of Product \n\nHuman-Centric \n\nDesign & Redress \n\nProcedures \n\nProduct Human-centric Design & \n\nRedress Procedures should be reviewed \n\nperiodically, or if significant changes \n\noccur, by the Product Team to ensure their \n\ncontinued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that Product Human-\n\nCentric Design & Redress \n\nProcedures are kept up-to-date. \n\n5.8 Human-Centric Design & Redress Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n26 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo prevent (in)direct adverse social and environmental effects as a consequence of interactions amongst \n\nProducts, Models, the Organisation, and the Public. \n\nControl:  Aim: \n\n5.9.1.  Systemic Stability \n\nPolicy \n\nA Policy and Guide, which promotes \n\nawareness and control of systemic effects \n\nand interactions in - (a) Product Definitions \n\nand Product design, development, and \n\nimplementation; (b) data processing; and \n\n(c) Model design, development, training, \n\nand output ought to be derived by Data \n\nScience Managers and approved by the \n\nManagerial Committee. \n\nTo ensure the Systemic Stability \n\nof Machine Learning. \n\nTo provide clear Organisation \n\nguidance to Products on how to \n\nidentify, analyse and prevent risks \n\nderived from (higher-order and/or \n\nhighly complex) relations between \n\nProducts, Models, Product design, \n\nOrganisation processes and \n\nsociety at large. \n\n5.9.2.  Product Systemic \n\nStability \n\nProcedures \n\nA set of Procedures to operationalise \n\nthe Systemic Stability Policy should be \n\ndeveloped and implemented within \n\nProducts in light of Product Definitions, \n\nthe Product Risk Classification Portfolio, \n\nand the Product Lifecycle and Workflow \n\nDescriptions. \n\nTo ensure the Systemic Stability \n\nof Products. \n\n5.9.3.  Systemic Stability \n\nAssessments \n\nProducts should regularly complete \n\nSystemic Stability according to Product \n\nLifecycle and Workflow Descriptions to \n\nthe extent that is reasonably practical. \n\nAssessment findings ought to be \n\ndocumented by the Product Team and \n\nreviewed by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo analyse, test, and report \n\nthe risks identified with, and \n\nmeasures taken to ensure, \n\nProduct Systemic Stability at \n\nregular intervals within the \n\nProduct Lifecycle and Workflow \n\nDescription. \n\n5.9.4.  Review of the \n\nSystemic Stability \n\nPolicy \n\nSystemic Stability should be reviewed \n\nperiodically, or if significant changes \n\noccur, by Data Science Managers to ensure \n\nits continued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that the Systemic \n\nStability Policy is kept up-to-date. \n\n5.9.5.  Review of \n\nSystemic Stability \n\nProcedures \n\nProduct Systemic Stability should be \n\nreviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product Systemic \n\nStability Procedures are kept up-\n\nto-date. \n\n5.9 Systemic Stability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n27 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo ensure the clear and complete Traceability of Products, Models and their assets (inclusive of, inter alia, data, \n\ncode, artifacts, output, and documentation) for as long as is reasonably practical. \n\nObjective \n\nTo ensure that Product decision-making is done in a clear, informed, unbiased and collaborative manner with a \n\ndiversity of Organisation opinions, when relevant. \n\nControl:  Aim: \n\n5.10.1.  Product \n\nTraceability Policy \n\nA Policy and Guide, which promotes \n\nProduct Traceability during and after - (a) \n\nProduct Definitions and Product design, \n\ndevelopment, and implementation; (b) \n\ndata processing; and (c) Model design, \n\ndevelopment, training, and output ought to \n\nbe derived by Data Science Managers and \n\napproved by the Managerial Committee. \n\nTo ensure the Product Traceability \n\nof Machine Learning. \n\nTo provide clear Organisation \n\nguidance to Products on how to \n\nmanage Product Traceability. \n\n5.10.2.  Product \n\nTraceability \n\nProcedures \n\nA set of Procedures to operationalise the \n\nTraceability Policy should be developed \n\nand implemented within Products in light \n\nof Product Definitions, the Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure the Product Traceability \n\nof Products. \n\n5.10.3.  Review of \n\nthe Product \n\nTraceability Policy \n\nThe Product Traceability should be \n\nreviewed periodically, or if significant \n\nchanges occur, by Data Science Managers \n\nto ensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Product \n\nTraceability Policy is kept up-to-\n\ndate. \n\n5.10.4.  Review of Product \n\nTraceability \n\nProcedures \n\nProduct Product Traceability should be \n\nreviewed periodically, or if significant \n\nchanges occur, by the Product Team to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Product Product \n\nTraceability Procedures are kept \n\nup-to-date. \n\nControl:  Aim: \n\n5.11.1.  Product \n\nDecision-Making \n\nPolicy \n\nA Policy which promotes Product decision-\n\nmaking clarity in - (a) Product Definitions \n\nand Product design, development, and \n\nimplementation; (b) data processing; \n\n(c) Model design, development, training, \n\nand output; and (d) consideration for the \n\nProduct Risk Classification Portfolio ought \n\nto be derived by Data Science Managers \n\nand approved by the Managerial Committee. \n\nTo ensure Product decisions - (a) are \n\nbased on all available information, \n\ninclusive of those derived from \n\nPolicies and Procedures of this \n\ndocument; (b) follow from and are \n\naligned with the Product Lifecycle \n\nand Workflow Description; and (c) \n\nare made with reasonable care to \n\navoid cognitive bias. \n\n5.10 Product Traceability \n\n5.11 Product Decision-Making Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n28 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\n5.11.2.  Product \n\nDecision-Making \n\nProcedures \n\nA set of Procedures to operationalise the \n\nDecision-Making Policy within Products \n\nshould be developed and implemented, \n\ninclusive of consultations with Business \n\nStakeholders. \n\nTo ensure clear, informed, unbiased \n\nand collaborative decision-making \n\nin Products. \n\n5.11.3.  Product \n\nDecision-Making \n\nDiversity \n\nA diversity of Organisation stakeholder and \n\nBusiness Stakeholder opinions and input \n\nshould be obtained, considered and, when \n\nrelevant, weighed when making material \n\nProduct decisions \n\nTo ensure a diversity of opinions \n\nwhen making material Product \n\ndecisions. \n\n5.11.4.  Product \n\nDecision-Making \n\nDocumentation \n\nProduct decisions should be documented, \n\nindexed, stored and, when relevant, \n\nreviewed. \n\nTo ensure that Product decisions \n\nare documented and indexed to \n\nwarrant their effective management \n\nand oversight. \n\nObjective \n\nTo ensure that Products have sufficient capacity and capabilities to meet Product Definitions. \n\nControl:  Aim: \n\n5.12.1.  Financial \n\nResource \n\nAllocation \n\nSufficient financial resources ought \n\nto be allocated to Products based on \n\ntheir Product Definitions, Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure that Products have \n\nsufficient financial resources to \n\nallow for their success. \n\n5.12.2.  Human Resource \n\nAllocation \n\nSufficient human resources ought \n\nto be allocated to Products based on \n\ntheir Product Definitions, Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure that Products have \n\nsufficient human resources to allow \n\nfor their success. \n\n5.12.3.  Assets Allocation  Sufficient Assets ought to be allocated \n\nto Products based on their Product \n\nDefinitions, Product Risk Classification \n\nPortfolio, and the Product Lifecycle and \n\nWorkflow Descriptions. \n\nTo ensure that Products have \n\nsufficient Assets to allow for their \n\nsuccess. \n\n5.12.4.  Software \n\nAllocation \n\nSufficient Software ought to be allocated \n\nto Products based on their Product \n\nDefinitions, Product Risk Classification \n\nPortfolio, and the Product Lifecycle and \n\nWorkflow Descriptions. \n\nTo ensure that Products have \n\nsufficient Software to allow for their \n\nsuccess. \n\n5.12.5.  Knowledge & \n\nDevelopment \n\nProduct Teams should receive sufficient \n\ntraining and development based on \n\nthe Product Definitions, Product Risk \n\nClassification Portfolio, and the Product \n\nLifecycle and Workflow Descriptions. \n\nTo ensure that Product Teams have \n\nsufficient training and development \n\nto allow for their success. \n\n5.12 Product Capabilities Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n29 FBPML Organisation Best Practices v1.0.0. \n\n5. Product and Model Oversight & Management - FBPML Organisation Best Practices v1.0.0. \n\n5.12.6.  Review of \n\nProduct \n\nCapabilities \n\nProduct resource allocation ought to be \n\nperiodically reviewed, or if significant \n\nchanges occur, by Data Science Managers, \n\nin consultation with Product Owners, to \n\nensure their continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that Products resources \n\nare sufficiently maintained. \n\nObjective \n\nTo promote the documentation and recording of Product, Product Team and employees tasks, deliverables and \n\nprogress. \n\nControl:  Aim: \n\n5.13.1.  Product Record  A clear and detailed Product record should \n\nbe continually kept of Product design, \n\ndevelopment, implementation, deliverables, \n\nprogress and employee tasks. \n\nTo ensure that a clear Product \n\nrecord is kept to warrant effective \n\noversight, management and \n\naccountability. \n\n5.13.2.  Employee and \n\nProduct Team \n\nDocumentation \n\nEmployee and Product Team processes \n\nought to promote the documentation of \n\nProduct tasks, discussions and deliverables \n\nwhen relevant and as much as is reasonably \n\npractical. \n\nTo ensure that sufficient \n\ndocumentation is kept to warrant \n\neffective oversight, management \n\nand accountability. \n\n5.13.3.  Log of User \n\nAccess \n\nA formal log of user access rights to \n\nProducts should be maintained and \n\nreviewed at regular intervals by Product \n\nOwners. \n\nTo ensure the management, \n\nintegrity and review of user access. \n\n5.13.4.  Event Logs  Event logs of Product user activities, \n\nexceptions, and faults should be produced, \n\nkept and regularly reviewed by Product \n\nOwners. \n\nTo formally index and manage \n\nuser activities to maintain Product \n\noversight and integrity. \n\n5.13 Product Record Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n30 FBPML Organisation Best Practices v1.0.0. \n\n# Section 6. Product Validation \n\nTo autonomously and impartially validate Products, Models and their outputs. \n\nObjective \n\nControl:  Aim: \n\n6.1.  Validation \n\nDepartment \n\nThe Organisation ought to have a \n\ndepartment that is independent of Data \n\nScience Managers and Product Teams \n\nwho validate Products. \n\nTo validate Product compliance, \n\nethics, and performance. \n\n6.2.  Validation Policy  A Policy detailing the responsibilities \n\nof the Validation Department and the \n\nrequirements for Products to be Validated \n\nought to be derived by the Managerial \n\nCommittee. \n\nTo ensure the validity of \n\nOrganisation Machine Learning. \n\n6.3.  Validation \n\nProcedures \n\nThe Validation Department should develop \n\nand implement a set of Procedures to \n\noperationalise the Validation Policy within \n\nthe Organisation. \n\nTo validate Product compliance, \n\nEthics, and performance within the \n\nOrganisation. \n\n6.4.  Review of the \n\nValidation Policy \n\nThe Validation Policy should be reviewed \n\nperiodically, or if significant changes \n\noccur, by the Managerial Committee \n\nto ensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Validation Policy \n\nis kept up-to-date. \n\n6.5.  Review of Product \n\nValidation \n\nProcedures \n\nProduct Validation Procedures should \n\nbe reviewed periodically, or if significant \n\nchanges occur, by the Validation \n\nDepartment to ensure their continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that Product Validation \n\nProcedures are kept up-to-date.", "fetched_at_utc": "2026-02-08T19:07:38Z", "sha256": "1a8bfb7dcffa71d485b7784adf9aed85d38640996ac030621c115ef35f820c90", "meta": {"file_name": "FBPML_OrganisationBP_V1.0.0-17-30.pdf", "file_size": 378097, "relative_path": "pdfs\\FBPML_OrganisationBP_V1.0.0-17-30.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 8566}}}
{"doc_id": "pdf-pdfs-fbpml-organisationbp-v1-0-0-38-40-030bf251474f", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_OrganisationBP_V1.0.0-38-40.pdf", "title": "FBPML_OrganisationBP_V1.0.0-38-40", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder. \n\n38 FBPML Organisation Best Practices v1.0.0. \n\n# Section 11. Third Party Contracts \n\n# Management \n\nTo ensure the integrity and implementation of Policies and Procedures in third-party contracts. \n\nObjective \n\nControl:  Aim: \n\n11.1.  Policies and \n\nProcedures \n\nfor Third-Party \n\nContracts \n\nProcedures ought to be designed and \n\nimplemented to ensure that Policies and \n\nProcedures are legally enforceable in \n\nrelevant third-party contracts. \n\nTo ensure that Policies and \n\nProcedures are implemented by \n\nthird-party contractors. \n\n11.2.  Compliance \n\nin Third-Party \n\nContracts \n\nProcedures should be implemented to \n\nensure that Policies and Procedures are \n\ncomplied with to agreed-upon standards \n\nin relevant third-party contracts. \n\nTo ensure that Policies and \n\nProcedures are complied with by \n\nthird-party contractors. \n\n11.3.  Monitoring, Review \n\nand Auditing \n\nof Third-Party \n\nContracts \n\nThe Organisation should regularly monitor, \n\nreview and audit third party contracts \n\nto warrant third-party compliance with \n\nPolicies and Procedures. \n\nTo ensure effective oversight of \n\nthird-party contractors. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n39 FBPML Organisation Best Practices v1.0.0. \n\n# Section 12. Ethics & \n\n# Transparency Management \n\nTo ensure that Products are transparent and ethical. \n\nObjective \n\nControl:  Aim: \n\n12.1.  Ethics Policy  An ethics policy, which promotes Ethical Practices \n\nin Machine Learning, ought to be derived by Data \n\nScience Managers and approved by the Management \n\nCommittee and Ethics Committee. The Ethics Policy \n\nmust be communicated to the Public. \n\nTo ensure that Machine \n\nLearning is designed, \n\ndeveloped and implemented \n\nin accordance with the \n\nEthical Practices. \n\n12.2.  Review of the \n\nEthics Policy \n\nThe Ethics Policy should be reviewed periodically, \n\nor if significant changes occur, by Data Science \n\nManagers to ensure its continued effectiveness, \n\nsuitability, and accuracy. \n\nTo ensure that the Ethics \n\nPolicy is kept up-to-date. \n\n12.3.  Transparency \n\nPolicy \n\nA Public Interest and Transparency Policy, which \n\npromotes Public engagement, regulator engagement, \n\nand Transparency in Machine Learning, ought to be \n\nderived by Data Science Managers and approved by \n\nthe Management Committee and Ethics Committee. \n\nThe Transparency Policy must be communicated to \n\nthe Public. \n\nTo ensure that Machine \n\nLearning is made \n\ntransparent to the Public and \n\nis designed, developed and \n\nimplemented in accordance \n\nwith the Public Interest. \n\n12.4.  Review of the \n\nTransparency \n\nPolicy \n\nThe Transparency Policy should be reviewed \n\nperiodically, or if significant changes occur, by \n\nData Science Managers to ensure its continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that the \n\nTransparency Policy is kept \n\nup-to-date. \n\n12.5.  Speaking-Out \n\nPolicy \n\nA policy, which promotes Product Teams and/ \n\nor Product Teams members to speak-out against \n\nunethical practices, ought to be derived by Data \n\nScience Managers and approved by the Management \n\nCommittee and Ethics Committee. \n\nTo ensure that Product \n\nTeams and/or Product Teams \n\nmembers have a safe space \n\nto voice their concerns about \n\nMachine Learning and/or \n\nProducts practices and/or \n\ndecisions. \n\n12.6.  Review of the \n\nSpeaking-Out \n\nPolicy \n\nThe Speaking-Out Policy should be reviewed \n\nperiodically, or if significant changes occur, by \n\nData Science Managers to ensure its continued \n\neffectiveness, suitability, and accuracy. \n\nTo ensure that the Speaking-\n\nOut Policy is kept up-to-date. \n\n12.7.  Contact with \n\nAuthorities \n\nAppropriate contact with relevant sector authorities \n\nregarding Products, and their implementation, should \n\nbe maintained. Products and Product Teams ought \n\nto work in close collaboration with relevant sector \n\nauthorities in a collaborative and bona fide manner. \n\nTo ensure awareness and \n\noversight of Products by \n\nauthorities. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder. \n\n40 FBPML Organisation Best Practices v1.0.0. \n\n# Section 13. Compliance, Auditing & \n\n# Legal Management and Oversight \n\nTo ensure that Policies and Procedures are designed, developed and implemented in accordance with the law, \n\nindustry best practices, contractual obligations, and/or regulatory Guidelines. \n\nObjective \n\nControl:  Aim: \n\n13.1.  Identification of \n\nLaws, Regulations and \n\nContracts \n\nRelevant laws, regulations and contracts \n\nconcerning Machine Learning ought to be \n\nidentified and documented. \n\nTo ensure that relevant \n\nMachine Learning laws, \n\nregulations and contracts \n\nhave been identified and \n\ndocumented. \n\n13.2.  Response to Laws, \n\nRegulations and \n\nContracts \n\nProcedures should be implemented to \n\nincorporate relevant laws, regulations and \n\ncontracts into Policies and Procedures. \n\nTo ensure that relevant \n\nMachine Learning laws, \n\nregulations and contracts are \n\nimplemented and abided by. \n\n13.3.  Privacy and \n\nProtection of \n\nPersonally Identifiable \n\nInformation \n\nProcedures \n\nProcedures should be implemented to \n\nensure the privacy and protection of \n\npersonally identifiable information in \n\nProducts as required in law, regulations \n\nand/or contracts. \n\nTo ensure that relevant data \n\nprotection and privacy laws, \n\nregulations and contracts are \n\nimplemented and abided by. \n\n13.4.  Intellectual Property \n\nRights Procedures \n\nProcedures should be implemented \n\nto protect intellectual property rights \n\nand the use of proprietary products in \n\nProducts as required in law, regulations \n\nand/or contracts. \n\nTo ensure that relevant \n\nintellectual property rights \n\nlaws, regulations and contracts \n\nare implemented and abided \n\nby. \n\n13.5.  Internal Audit \n\nof Policies and \n\nProcedures \n\nDefined organisational employees should \n\naudit the implementation of Policies and \n\nProcedures within the Organisation. \n\nTo ensure the quality and \n\nintegrity of implemented \n\nPolicies and Procedures.", "fetched_at_utc": "2026-02-08T19:07:40Z", "sha256": "030bf251474f2fb439bcdc19ac0189618f6a9aa19003db9a3e6fbb1a949225ec", "meta": {"file_name": "FBPML_OrganisationBP_V1.0.0-38-40.pdf", "file_size": 143814, "relative_path": "pdfs\\FBPML_OrganisationBP_V1.0.0-38-40.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 1436}}}
{"doc_id": "pdf-pdfs-fbpml-organisationbp-v1-0-0-7-15-fcfbd902d81c", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_OrganisationBP_V1.0.0-7-15.pdf", "title": "FBPML_OrganisationBP_V1.0.0-7-15", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n7FBPML Organisation Best Practices v1.0.0. \n\n1.1.  Adversarial Action  means actions characterised by mala fide (malicious) intent and/or bad \n\nfaith. \n\n1.2.  Assessment  means the action or process of making a series of determinations and \n\njudgments after taking deliberate steps to test, measure and collectively \n\ndeliberate the objects of concern and their outcomes. \n\n1.3.  Assets  means information technology hardware that concerns Products Machine \n\nLearning. \n\n1.4.  Business Stakeholders  means the departments and/or teams within the Organisation who do \n\nnot conduct data science and/or technical Machine Learning, but have a \n\nmaterial interest in Products Machine Learning. \n\n1.5.  Corporate Governance \n\nPrinciples \n\nmean the structure of rules, practices and processes used to direct and \n\nmanage a company in terms of industry recognised and published legal \n\nguidelines. \n\n1.6.  Data Governance  means the systems of governance and/or management over data assets \n\nand/or processes within an Organisation. \n\n1.7.  Data Quality  means the calibre of qualitative or quantitative data. \n\n1.8.  Data Science  means an interdisciplinary field that uses scientific methods, processes, \n\nalgorithms and computational systems to extract knowledge and insights \n\nfrom structured and/or unstructured data. \n\n1.9.  Domain  means the societal and/or commercial environment within which the \n\nProduct will be and/or is operationalised. \n\n1.10.  Ethical Practices  means the ethical principles, values and/or practices that are \n\nencapsulated and promoted in an â€˜artificial intelligenceâ€™ ethics guideline \n\nand/or framework, such as (a) The Asilomar AI Principles (Asilomar AI \n\nPrinciples, 2017), (b) The Montreal Declaration for Responsible AI (Montreal \n\nDeclaration, 2017), (c) The Ethically Aligned Design: A Vision for Prioritizing \n\nHuman Well-being with Autonomous and Intelligent Systems (IEEE, 2017), \n\nand/or (d) any other analogous guideline and/or framework. \n\n1.11.  Ethics Committee  means the committee within the Organisation charged with managing and/ \n\nor directing organisation Ethical Practices. \n\n1.12.  Executive Management  means the managerial team at the highest level of management within the \n\nOrganisation. \n\n1.13.  Explainability  means the property of Models and Model outcomes to be interpreted and/ \n\nor explained by humans in a comprehensible manner. \n\nAs used in this Best Practice Guideline, the following terms shall have the following meanings where capitalised. \n\nAll references to the singular shall include references to the plural, where applicable, and vice versa. Any terms \n\nnot defined or capitalised in this Best Practice Guideline shall hold their plain text meaning as cited in English and \n\ndata science. \n\n# Section 1. Definitions Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n8FBPML Organisation Best Practices v1.0.0. \n\n1. Definitions - FBPML Organisation Best Practices v1.0.0. \n\n1.14.  Fairness & Non-\n\nDiscrimination \n\nmeans the property of Models and Model outcomes to be free from bias \n\nagainst protected classes. \n\n1.15.  Best Practice Guideline  means this document. \n\n1.16.  Guide  means an established and clearly documented series of actions or \n\nprocess(es) conducted in a certain order or manner to achieve particular \n\noutcomes. \n\n1.17.  Human-Centric Design \n\n& Redress \n\nmeans orienting Products and/or Models to focus on humans and their \n\nenvironments through promoting human and/or environment centric \n\nvalues and resources for redress. \n\n1.18.  Incident  means the occurrence of a technical event that affects the integrity of a \n\nProduct and/or Model. \n\n1.19.  Machine Learning  means the use and development of computer systems and Models that \n\nare able to learn and adapt with minimal explicit human instructions by \n\nusing algorithms and statistical modelling to analyse, draw inferences, and \n\nderive outputs from data. \n\n1.20.  Model  means Machine Learning algorithms and data processing designed, \n\ndeveloped, trained and implemented to achieve set outputs, inclusive of \n\ndatasets used for said purposes unless otherwise stated. \n\n1.21.  Organisation  means the concerned juristic entity designing, developing and/or \n\nimplementing Machine Learning. \n\n1.22.  Performance \n\nRobustness \n\nmeans the propensity of Products and/or Models to retain their desired \n\nperformance over diverse and wide operational conditions. \n\n1.23.  Policy  means a documented course of normative actions or set of principles \n\nadopted to achieve a particular outcome. \n\n1.24.  Procedure  means an established and defined series of actions or process(es) \n\nconducted in a certain order or manner to achieve a particular outcome. \n\n1.25.  Product  means the collective and broad process of design, development, \n\nimplementation and operationalisation of Models, and associated \n\nprocesses, to execute and achieve Product Definitions, inclusive of, inter \n\nalia, the integration of such operations and/or Models into organisation \n\nproducts, software and/or systems. \n\n1.26.  Product Team  means the collective group of Organisation employees directly charged \n\nwith designing, developing and/or implementing the Product. \n\n1.27.  Product Lifecycle  means the collective phases of Products from initiation to termination \n\n- such as design, exploration, experimentation, development, \n\nimplementation, operationalisation, and decommissioning - and their \n\nmutual iterations. \n\n1.28.  Product Owner  means the employee charged with (a) managing and maximising the \n\nvalue of the Product and its Product Team; and (b) engaging with various \n\nBusiness Stakeholders concerning the Product and its Product Definitions. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n9FBPML Organisation Best Practices v1.0.0. \n\n1. Definitions - FBPML Organisation Best Practices v1.0.0. \n\n1.29.  Public  means society at large. \n\n1.30.  Public Interest  means the welfare or well-being of the Public. \n\n1.31.  Representativeness  means the degree to which datasets and Models reflect the true \n\ndistribution and conditions of Subjects, Subject populations, and/or \n\nDomains. \n\n1.32.  Safety & Security  means (a) the resilience of Products and/or Models against malicious and/ \n\nor negligent activities that result in Organisational loss of control over \n\nconcerned Products and/or Models; and (b) real Product Domain based \n\nphysical harms that result through Products and/or Models applications \n\n1.33.  Social Corporate \n\nResponsibilities \n\nmeans the structure of rules, practices and processes used to direct and \n\nmanage a company in terms of industry recognised and published legal \n\nguidelines to positively contribute to economic, environmental and social \n\nprogress. \n\n1.34.  Software  means information technology software that concerns Products Machine \n\nLearning. \n\n1.35.  Special Interest Groups  means a specific body politic, or a particular collective of citizens, who can \n\nreasonably be determined to have a material interest in the Product. \n\n1.36.  Specification  means the accuracy, completeness and exactness of Products, Models \n\nand/or datasets in reflecting Product Definitions, Product Domains and/ \n\nor Product Subjects, either in their design and development and/or \n\noperationalisation. \n\n1.37.  Subjects  means the entities and/or objects that are represented as data points in \n\ndatasets and/or Models, and who may be the subject of Product and/or \n\nModel outcomes. \n\n1.38.  Systemic Stability  means the stability of Organisation, Domain, society and environments as \n\na collective ecosystem. \n\n1.39.  Traceability  means the ability to trace, recount, and reproduce Product outcomes, \n\nreports, intermediate products, and other artifacts, inclusive of Models, \n\ndatasets and codebases. \n\n1.40.  Transparency  means the provision of an informed target audiences understanding of \n\nOrganisation and/or Products Machine Learning, and their workings, based \n\non documented Organisation information. \n\n1.41.  Workflows  means the coordinated and standardised sequences of employee work \n\nactivities, processes, and tasks. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder. \n\n10 FBPML Organisation Best Practices v1.0.0. \n\n# Part A \n\n# Organisation Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n11 FBPML Organisation Best Practices v1.0.0. \n\n# Section 2. Managerial Oversight \n\n# & Management \n\nObjective \n\nTo ensure managerial direction and support for Products in accordance with Organisation strategies, business \n\nrequirements, Corporate Governance Principles, Social Corporate Responsibilities, legal regulations and Ethical \n\nPractices. \n\n2.1 Management Direction for Machine Learning \n\nControl:  Aim: \n\n2.1.1.  Management \n\nCommittee \n\nA managerial committee ought to be \n\nestablished to (a) oversee Organisation \n\nMachine Learning and Products; and \n\n(b) warrant their effective alignment in \n\naccordance with Organisation strategies, \n\nbusiness requirements, Corporate \n\nGovernance Principles, Social Corporate \n\nResponsibilities, legal regulations and \n\nEthical Practices. \n\nTo ensure clear managerial \n\nresponsibility, oversight and \n\ncustody of Organisation Machine \n\nLearning and Products. \n\n11.1.2.  Management \n\nCommittee \n\nDiversity \n\nThe Management Committee ought to \n\nhold a diversity of members from differing \n\nOrganisation departments, including \n\nExecutive Management, legal, finance, \n\noperations, public communications as \n\nwell as Data Science. \n\nTo (a) ensure the diversity of \n\nmanagerial opinions and oversight \n\nof Organisation Machine Learning \n\nand Products; and (b) foster \n\nOrganisation buy-in for Machine \n\nLearning and Products. \n\n11.1.3.  Managerial \n\nOversight \n\nProcedures \n\nThe Management Committee should \n\nestablish appropriate Procedures to \n\nwarrant managerial oversight and \n\ngovernance of Organisation Products, \n\ninclusive of the appointment of Data \n\nScience Managers. \n\nTo ensure the operationalisation \n\nof the oversight and management \n\nof Organisation Machine Learning \n\nand Products by the Management \n\nCommittee. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n12 FBPML Organisation Best Practices v1.0.0. \n\n# Section 3. Internal Organisation \n\n# Management & Oversight \n\nObjective \n\nTo establish managerial Procedures to control and oversee the design, development and implementation of \n\nProducts. \n\n3.1 Internal Organisation \n\nControl:  Aim: \n\n3.1.1.  Data Science \n\nManagers \n\nThe Management Committee should \n\nappoint Data Science Managers to oversee \n\nProducts and warrant their effective \n\nalignment in accordance with the \n\ndirectives of the Management Committee, \n\nPolicies, and, more broadly, Organisation \n\nstrategies, business requirements, \n\nCorporate Governance Principles, \n\nSocial Corporate Responsibilities, legal \n\nregulations and Ethical Practices. \n\nTo ensure the clear management, \n\noversight, ownership and custody \n\nof Products. \n\n3.1.2.  Data Science \n\nManagers Products \n\nOwnership and \n\nCustody \n\nThe Management Committee ought to \n\ndefine and allocate to Data Science \n\nManagers Products. \n\nTo ensure clear managerial \n\noversight, ownership and custody \n\nof Products. \n\n3.1.3.  Data Science \n\nManagers \n\nSegregation of \n\nDuties \n\nConflicting duties and areas of \n\nresponsibility of Data Science Managers \n\nshould be segregated to reduce \n\nopportunities for the unauthorised and/or \n\nunintentional modification and/or misuse \n\nof Products. \n\nTo reduce the threat of Product \n\nabuse, misuse and/or mala fide \n\nactions by Data Science Managers. \n\n3.1.4.  Product Owners  Data Science Managers ought to appoint \n\nProduct Owners to (a) oversee specific \n\nProducts and Product Teams; and (b) \n\nwarrant their effective management in \n\naccordance with the directives of Data \n\nScience Managers, the Management \n\nCommittee, and Organisation Policies. \n\nTo ensure the clear management, \n\noversight, ownership and custody \n\nof a Product and its Product Team. \n\n3.1.5.  Product Owners \n\nOwnership and \n\nCustody \n\nData Science Managers ought to define \n\nand allocate to designated Product \n\nOwners Products and Product Teams. \n\nTo ensure clear managerial \n\noversight, ownership and custody \n\nof a Product and its Product Team. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n13 FBPML Organisation Best Practices v1.0.0. \n\n3. Internal Organisation Management & Oversight - FBPML Organisation Best Practices v1.0.0. \n\n3.1.6.  Product Owners \n\nSegregation of \n\nDuties \n\nConflicting duties and areas of \n\nresponsibility of Product Owners should \n\nbe segregated to reduce opportunities \n\nfor the unauthorised and/or unintentional \n\nmodification and/or misuse of a Product. \n\nTo reduce the threat of Product \n\nabuse, misuse and/or mala fide \n\nactions by Product Owners. \n\n3.1.7.  Product Teams  Data Science Managers, in consultation \n\nwith Product Owners, should define and \n\nallocate Products to designated Product \n\nTeams. \n\nTo ensure clear Product ownership \n\nand custody. \n\n3.1.8.  Product Definitions  Data Science Managers, Product Owners, \n\nBusiness Stakeholders and, when relevant, \n\nProduct employees ought to collectively \n\ndocument and define clear Product \n\ndefinitions, aims, internal deliverables and \n\noutcomes. \n\nTo ensure Products have clear \n\nscopes to warrant (a) their \n\neffective oversight, management \n\nand execution, as well as (b) to \n\nallow for the accurate evaluation of \n\nProduct risks and controls. \n\n3.1.9.  Approval of \n\nProduct Definitions \n\nThe Management Committee should \n\nreview and approve Product Definitions. \n\nTo ensure managerial oversight of \n\nProducts scopes. \n\n3.1.10.  Product Definitions \n\nReview \n\nProduct Definitions ought to be reviewed \n\nperiodically, or if significant changes \n\noccur, by Data Science Managers, Product \n\nOwners, Business Stakeholders and, when \n\nrelevant, Product employees. \n\nTo ensure that Product Definitions \n\nare kept up-to-date to ensure \n\ntheir continued effectiveness, \n\nsuitability, and accuracy. \n\n3.1.11.  Product Risk \n\nClassification \n\nPolicy \n\nA Policy and Guide, which standarises \n\nthe approaches to assessing Product \n\nrisks, ought to be derived by Data Science \n\nManagers and approved by the Managerial \n\nCommittee. \n\nTo ensure that (a) clear guidelines \n\nexist on how to evaluate and \n\ndetermine Product based-risks for \n\nsubsequent evaluation in Product \n\nRisk Portfolios; and (b) Products \n\nare assigned risk-appropriate \n\nmandatory minimum capacity and \n\noversight. \n\n3.1.12.  Product Risk \n\nClassification \n\nPortfolio \n\nData Science Managers, Product Owners, \n\nBusiness Stakeholders and, when \n\nrelevant, Product employees ought to \n\ncollectively document and interrogate \n\n(a) Product Definitions and (b) Product \n\ndesign, development and implementation \n\nto identify Product based-risks and assign \n\nProduct risk values and classifications. \n\nTo ensure Products have clear \n\nrisk portfolios to warrant (a) their \n\neffective oversight, management \n\nand execution, as well as (b) to \n\nallow for the accurate evaluation of \n\nProduct risks and controls. \n\n3.1.13.  Approval of \n\nProduct Risk \n\nClassification \n\nPortfolio \n\nThe Management Committee should \n\nreview and approve Product Risk \n\nPortfolios. \n\nTo ensure managerial oversight of \n\nProducts risks. \n\n3.1.14.  Product Product \n\nRisk Classification \n\nPortfolio Review \n\nThe Product Risk Classification Portfolio \n\nought to be continuously reviewed and \n\ndeveloped by Data Science Managers, \n\nProduct Owners, Business Stakeholders \n\nand, when relevant, Product employees. \n\nTo ensure that Product Risk \n\nPortfolios are kept up-to-date \n\nto ensure their continued \n\neffectiveness, suitability, and \n\naccuracy. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n14 \n\n3. Internal Organisation Management & Oversight - FBPML Organisation Best Practices v1.0.0. \n\nObjective \n\nTo establish Procedures to control the design, development and implementation of Products. \n\n3.2 Product Management \n\nControl:  Aim: \n\n3.2.1.  Product Lifecycle \n\nGuide \n\nData Science Managers and, when \n\nrelevant, Product Owners should derive \n\na clear Product Lifecycle Guide for the \n\nOrganisation. \n\nTo ensure a clear organisational \n\nProduct Lifecycle Guide to warrant \n\nthe effective management and \n\noversight of Machine Learning. \n\n3.2.2.  Product Lifecycle \n\nand Workflow \n\nDescriptions \n\nHaving consideration for the Product \n\nLifecycle Policy, Product Definitions, and \n\nthe Product Risk Classification Portfolio, \n\nProduct workflows ought to be derived, \n\ndeveloped, and documented by Data \n\nScience Managers, Product Owners and, \n\nwhen relevant, Product employees for \n\neach Product. \n\nTo ensure clear Lifecycle and \n\nWorkflows for Products to warrant \n\ntheir effective management and \n\noversight. \n\n3.2.3.  Reviewed of \n\nProduct Lifecycle \n\nGuide \n\nThe Product Lifecycle Guide should be \n\nreviewed and approved by Data Science \n\nManagers and, when relevant, the \n\nManagement Committee. \n\nTo ensure managerial oversight of \n\nthe Product Lifecycle Guide. \n\n3.2.4.  Reviewed of \n\nProduct Lifecycle \n\nand Workflow \n\nDescription \n\nProduct Lifecycle and Workflow \n\nDescriptions should be reviewed and \n\napproved by Data Science Managers \n\nand, when relevant, the Management \n\nCommittee. \n\nTo ensure managerial oversight of \n\nProduct Lifecycle and Workflow \n\nDescriptions. \n\n3.2.5.  Product Lifecycle \n\nand Workflow \n\nProcedures \n\nEach Product ought to derive, develop \n\nand implement a set of Procedures to \n\noperationalise Product Lifecycle and \n\nWorkflow Descriptions. \n\nTo ensure the operationalisation \n\nof Product Lifecycle and Workflow \n\nDescriptions. \n\n3.2.6.  Reviewed of \n\nProduct Lifecycle \n\nand Workflow \n\nProcedures \n\nThe Product Lifecycle and Workflow \n\nProcedures should be reviewed \n\nperiodically, or if significant changes \n\noccur, by the Product Team to ensure their \n\ncontinued effectiveness, suitability, and \n\naccuracy. \n\nTo ensure that Product Product \n\nLifecycle and Workflow Procedures \n\nare kept up-to-date. \n\n3.2.7.  Product Employee \n\nRoles and \n\nResponsibilities \n\nData Science Managers and Product \n\nOwners ought to define and allocate \n\nto Product employees defined \n\nresponsibilities and roles in terms \n\nof Product Lifecycle and Workflow \n\nDescriptions. \n\nTo establish clear employee \n\nresponsibilities and custodies in \n\nterms of Product Lifecycle and \n\nWorkflow Descriptions. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n15 \n\n3. Internal Organisation Management & Oversight - FBPML Organisation Best Practices v1.0.0. \n\n3.2.8.  Data Science \n\nManagers Reports \n\nFrequent reports detailing Product \n\nprogress, changes and risks ought to be \n\nmade to the Management Committee \n\nby Data Science Managers and, \n\nsubsequently, reviewed timeously. \n\nTo ensure the clear communication \n\nand management of Product \n\ndeliverables to the Management \n\nCommittee. \n\n3.2.9.  Product Owners \n\nReports \n\nFrequent reports detailing Product \n\nprogress, changes and risks ought to be \n\nmade to the Data Science Managers and \n\nBusiness Stakeholders by Product Owners \n\nand, subsequently, reviewed timeously. \n\nTo ensure the clear communication \n\nand management of Product \n\ndeliverables to Data Science \n\nManagers and Business \n\nStakeholders.", "fetched_at_utc": "2026-02-08T19:07:50Z", "sha256": "fcfbd902d81c60c193627bc07bacd52f3cc20de5f68f6d6794dc1679dbdb0dfc", "meta": {"file_name": "FBPML_OrganisationBP_V1.0.0-7-15.pdf", "file_size": 228824, "relative_path": "pdfs\\FBPML_OrganisationBP_V1.0.0-7-15.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4653}}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-13-30-166d9a6e0506", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-13-30.pdf", "title": "FBPML_TechnicalBP_V1.0.0-13-30", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n13 FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n2.1.  Product Team \n\nComposition \n\nDocument and define a clear diversity \n\nof Product Team roles and expertises \n\nneeded for the Product, inclusive of, \n\namongst other things, engineers, data \n\nscientists, Product Managers, and user \n\nexperience experts. Once established, \n\nrecruit accordingly. \n\nTo (a) assemble a robust team \n\nfor Product and/or Model design, \n\ndevelopment and deployment; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n2.2.  Product Team \n\nRoles \n\nDocument and allocate clear Product \n\nTeam roles and expectations for Product \n\nTeam members, including expectations \n\nfor, and the structure of, intra-Product \n\nTeam collaboration and overlapping \n\nresponsibilities. \n\nTo (a) ensure that Product Team \n\nroles are clearly defined; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n2.3.  Product Team \n\nStrengths and \n\nSkills Analysis \n\nDocument and assess the range of \n\nProduct Team member skills and \n\ninterests. Attempt to match member skills \n\nand interests to appropriate Product Team \n\nRoles as much as is practically possible. \n\nTo (a) ensure Product Team skill \n\nalignment and continued interest; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n2.4.  Product \n\nManagement \n\nDocument and allocate a clear Product \n\nManagement role and duties to Product \n\nManagers, inclusive of ensuring that \n\nProduct Managers have suitable Product \n\noversight, a clear understanding of \n\nProduct Team dynamics, and a contextual \n\nunderstanding of the Product and its \n\noperationalisation. \n\nPlease see Section 3 of the Organisation \n\nBest Practices Guideline for further \n\ncontext. \n\nTo (a) ensure that Product Manager \n\nroles are clearly defined; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nObjective: \n\nTo (a) ensure a balanced Product Team composition that fosters close collaboration and enhances a diversity of \n\nskills; and (b) to promote Product Team coordination and understanding through thorough team organization. \n\n# Section 2. Team Composition \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n14 FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n3.1.  Industry Context  Incorporate regulations, standards, and norms \n\nthat reflect industry values, boundaries, and \n\nconstraints during each phase of Product design and \n\ndeployment. Document and define clear qualitative \n\nmetrics and counter-metrics in Product & Outcome \n\nDefinitions, Data & Model Metrics and Acceptance \n\nCriteria Metrics, as relevant, as discussed in Section \n\n4 - Problem Mapping; Section 5 - Model Decision-\n\nMaking. \n\nTo (a) assemble a \n\nrobust team for Product \n\nand/or Model design, \n\ndevelopment and \n\ndeployment; and (b) \n\nhighlight associated risks \n\nthat might occur in the \n\nProduct Lifecycle. \n\n3.2.  Deployment \n\nContext \n\nIncorporate an understanding of the technical and \n\ninfrastructure aspects of the deployed Product \n\ninto the Product design process. Ensure that \n\ninfrastructure, integration, and scaling requirements \n\nand limitations are considered during the Problem \n\nMapping and Planning phases and document and \n\ndefine clear requirements for the Organisation \n\nCapacity Analysis, Product Scaling Analysis, Product \n\nIntegration Strategy, Product Risk Analysis, Testing \n\n- Automation Analysis, and POC-to-Production \n\nAnalysis, as discussed in Section 4 - Problem \n\nMapping; Section 6 - Management & Monitoring; \n\nSection 8 - Testing. \n\n3.3.  Societal Context  Research and consider the on and off platform \n\neffects of Product deployment on end users, their \n\ncommunities, and societies during each phase \n\nof Product design and deployment. Ensure that \n\nbehavioral shifts, power balance, and cultural \n\nconcerns are considered during the Problem Mapping \n\nand Planning phases, and that these provide input \n\nfor the Problem Statement & Solution Mapping, \n\nOutcome Definition, Product & Outcome Definitions \n\nData & Model Metrics, Product Risk Analysis, User \n\nExperience Mapping, Model Type - Best Fit Analysis, \n\nAcceptance Criteria, Privacy, Testing Participants, \n\nand Accuracy Perception, as discussed in Section \n\n4 - Problem Mapping; Section 7 - Privacy; Section 8 -\n\nTesting; Section 9 - Managing Expectations. \n\nObjective: \n\nTo ensure the Product Teamâ€™s continual access to a deep understanding of the various external contexts that \n\naffect the successful design and deployment of the Product. \n\n# Section 3. Context \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n15 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo determine and define an appropriate, feasible and solvable business problem through consideration of several \n\ninteracting analyses. \n\nControl:  Aim: \n\n4.1.  Problem Statement \n\n& Solution Mapping \n\nDocument and define clear problem \n\nstatements in terms of (i) User \n\nneeds, (ii) Organisation problem, \n\nand/or (iii) Organization opportunity. \n\nSubsequently, document and define \n\nclear solutions to the problem \n\nstatements, inclusive of the \n\ncontextual needs and/or variants of \n\nthe problem statements and/or their \n\nsolutions. \n\nTo ensure Products have clear scopes \n\nto warrant (a) their effective oversight, \n\nmanagement and execution, as well as \n\n(b) allow for the accurate evaluation of \n\nProduct risks and controls. \n\n4.2.  Data Capacity \n\nAnalysis \n\nMap and document the state of the \n\ndata delivery pipeline and available \n\ndatabases required to support the \n\nproblem statements and solutions. \n\nTo (a) ensure that the data pipeline is \n\nsufficient to support Product(s) and \n\nenable the desired Outcomes; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n4.3.  Product Definitions  Document and define clear Product \n\ndefinitions, aims, requirements and \n\ninternal deliverables having regard \n\nfor the above Problem Statement & \n\nSolution Mapping analysis, inclusive of \n\nsubsequent iterations thereof. \n\nTo ensure Product(s) have clear scope \n\nto warrant (a) their effective oversight, \n\nmanagement and execution, as well as \n\n(b) allow for the accurate evaluation of \n\nProduct risks and controls. \n\n4.4.  Outcomes \n\nDefinitions \n\nDocument, delineate, and define clear \n\nProduct Outcomes and Outcomes \n\ndeliveries based on the above \n\nProduct Definitions and the Problem \n\nStatement & Solution Mapping \n\nanalysis, inclusive of subsequent \n\niterations thereof. \n\nTo ensure Product(s) have clear scopes \n\nto warrant (a) their effective oversight, \n\nmanagement and execution, as well as \n\n(b) allow for the accurate evaluation of \n\nProduct risks and controls. \n\n4.5.  Product & Outcome \n\nDefinitions Data & \n\nModel Metrics \n\nDocument and define the above \n\nProduct and Outcome Definitions in \n\nterms of clear Model and data metrics. \n\nTo ensure Product(s) have clear scopes \n\nto warrant (a) their effective oversight, \n\nmanagement and execution, as well as \n\n(b) to allow for the accurate evaluation \n\nof Product risks and controls. \n\n# Section 4. Problem Mapping \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n16 FBPML Technical Best Practices v1.0.0. \n\n4. Problem Mapping - FBPML Technical Best Practices v1.0.0. \n\n4.6.  Organisation \n\nCapacity Analysis \n\nDocument and assess whether \n\nthe organisation has the requisite \n\ncapacity to achieve the above \n\nProduct Outcome and Product Metric \n\nDefinitions given the Product Team \n\nComposition and Product Team \n\nStrengths and Skills and Data Capacity \n\nAnalyses. If constraints detected, \n\nreiterate formulations of Product \n\nand/or Outcome Definitions to \n\naccommodate organisation capacity. \n\nTo (a) ensure that the Organization \n\nhas sufficient capacity to support \n\nProduct(s) and enable desired \n\nOutcomes; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n4.7.  Product Scaling \n\nAnalysis \n\nDocument and assess the estimated \n\ndegree to which the Product can \n\nbe feasibly scaled within Product \n\nDomains and the Organisation, having \n\nconsideration for the Organisation \n\nCapacity Analysis. \n\nTo (a) ensure that the Organization \n\nhas sufficient capacity to support \n\nthe Product(s) and enable the desired \n\nOutcomes as the Product scales; and \n\n(b) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n4.8.  Product Integration \n\nStrategy \n\nDocument and assess the processes \n\nneeded to integrate and scale \n\nthe Product into organisational \n\nstructures based on the Organisation \n\nCapacity and Product Scaling \n\nAnalyses. If constraints detected and/ \n\nor integration appears unfeasible, \n\nreiterate formulations of Product \n\nand/or Outcome Definitions and/ \n\nor review the Organisation Capacity \n\nand/or Product Scaling Analyses to \n\naccommodate a practical Product \n\nIntegration Strategy. \n\nTo (a) ensure the Product and Outcome \n\nDefinitions can be achieved within the \n\nbounds of the Organisation Capacity \n\nand Product Scaling Analyses; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n4.9.  Product Risk \n\nAnalysis \n\nDocument and assess the estimated \n\nrisks associated with Product design, \n\ndevelopment, implementation, and \n\noperation, inclusive of considerations \n\nfrom the Product Scaling Analysis, and \n\nthe Product Integration Strategy. \n\nTo (a) ensure Products have clear risk \n\nportfolios to warrant (i) their effective \n\noversight, management and execution, \n\nas well as (ii) to allow for the accurate \n\nevaluation of Product risks and \n\ncontrols; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n17 FBPML Technical Best Practices v1.0.0. \n\n4. Problem Mapping - FBPML Technical Best Practices v1.0.0. \n\n4.10.  Product Cost \n\nAnalysis \n\nCollaborate with Finance and \n\npurchasing to document and assess \n\nthe estimated costs associated \n\nwith Product design, development, \n\nimplementation, and operation, \n\ninclusive of considerations from the \n\nProduct Scaling Analysis, the Product \n\nIntegration Strategy, and the Product \n\nRisk Analysis. \n\nTo (a) ensure a realistic project \n\nbudget is provided; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n4.11.  User Experience \n\nMapping \n\nDocument and assess the user \n\nexperience and the desired \n\nexperience for various user groups, \n\nwhen interacting with the Product (e.g. \n\nusing Normanâ€™s Usability Heuristics). \n\nConsider mitigation strategies for \n\npossible negative impacts on and off \n\nplatform. If gaps in user experience \n\nare detected or a need for process \n\nredesign or behavioral changes are \n\nuncovered reiterate formulations of \n\nOutcome Definition, as discussed \n\nin Section 4 - Problem Mapping, \n\nOrganisation Capacity Analysis, \n\nand Product Integration Strategy \n\nto accommodate an effective user \n\nexperience. \n\nTo (a) ensure an effective user \n\nexperience; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n18 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo determine the most desirable and feasible model to achieve the desired Product Outcomes through \n\nconsideration of several interacting analyses. \n\nControl:  Aim: \n\n5.1.  Model Type - Metric \n\nFit Analysis \n\nDocument and assess the Model \n\nrequirements needed to meet \n\nthe Product Definitions, Outcome \n\nDefinitions, and Product & Outcome \n\nDefinitions Data & Model Metrics, \n\nas discussed in Section 4 - Problem \n\nMapping. \n\nTo ensure that chosen Model(s) meet \n\nthe requirements of the Product \n\nDefinitions, Outcome Definitions, and \n\nProduct & Outcome Definitions Data & \n\nModel Metrics. \n\n5.2.  Model Type - Risk \n\nAnalysis \n\nDocument and assess Model \n\nrequirements needed to meet the \n\nExplainability Requirements and \n\nProduct Risk Analysis, as discussed in \n\nSection 16 - Explainability; Section 4 -\n\nProblem Mapping. \n\nTo ensure that chosen Model(s) meet \n\nthe requirements of the Explainability \n\nRequirements and Product Risk \n\nAnalysis. \n\n5.3.  Model Type -\n\nOrganisation \n\nAnalysis \n\nDocument and assess the \n\ncompatibility of potential Models with \n\nthe Organisation Capacity Analysis, \n\nProduct Scaling Analysis, and Product \n\nIntegration Strategy, as discussed in \n\nSection 4 - Problem Mapping, given \n\ntechnical considerations. \n\nTo ensure that chosen Model(s) meet \n\nthe requirements of the Organisation \n\nCapacity Analysis, Product Scaling \n\nAnalysis, and Product Integration \n\nStrategy. \n\n5.4.  Model Type - Best \n\nFit Analysis \n\nDocument and assess the most \n\nappropriate Models that best meet the \n\nrequirements of, and which produces \n\nthe most favorable outcome given the \n\ntrade-offs between, the Model Type \n\n- Metric Fit, Risk and Organization \n\nAnalyses. \n\nTo (a) ensure that the most appropriate \n\nModel(s) are chosen; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n5.5.  Acceptance \n\nCriteria - Metrics \n\nDocument and define the desired \n\nperformance for an acceptable Model \n\nin terms of clear Model and data \n\nmetrics that are written from the end \n\nuser's perspective. \n\nTo (a) determine the metrics and \n\ndesired performance for an acceptable \n\nModel; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n# Section 5. Model Decision-Making \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n19 FBPML Technical Best Practices v1.0.0. \n\n5. Model Decision-Making - FBPML Technical Best Practices v1.0.0. \n\n5.6.  Acceptance \n\nCriteria -\n\nAccuracy, Bias, and \n\nFairness \n\nDocument and define clear, narrow \n\naccuracy goals and metrics that \n\nmanage the tradeoff of accuracy \n\nand explainability. Document and \n\ndefine the Model requirements \n\nneeded to meet the Fairness & Non-\n\nDiscrimination goals, as discussed \n\nmore thoroughly and technically \n\nin Section 11 - Fairness & Non-\n\nDiscrimination. \n\nTo (a) ensure appropriate accuracy, \n\nbias and fairness metrics for Model(s); \n\nand (b) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n5.7.  Acceptance \n\nCriteria -\n\nError Rate Analysis \n\nConsider the Societal and Industry \n\nContexts in determining the \n\nacceptable method for error \n\nmeasurement, as discussed in Section \n\n4 - Problem Mapping. Document and \n\ndefine the acceptable error types and \n\nrates for the Product as required by \n\nRepresentativeness & Specification, \n\nas discussed more thoroughly \n\nand technically in Section 13 -\n\nRepresentativeness & Specification. \n\nAnalyze any potential tension between \n\nachievable and acceptable error rates \n\nand determine whether that tension \n\ncan be resolved. \n\nTo (a) ensure appropriate error type \n\nand rate metrics for Model(s); and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n5.8.  Acceptance \n\nCriteria -\n\nKey Business \n\nMetrics / Targeted \n\nMetrics \n\nDocument and define the key business \n\nmetrics (KPIs) as determined in \n\nProblem Statement & Solution \n\nMapping, as discussed in Section \n\n4 - Problem Mapping, and translate \n\nthem into metrics that can be tracked \n\nwithin the framework of chosen \n\nModel(s), or into proxy metrics if direct \n\ntracking is not feasible. \n\nTo (a) ensure appropriate business \n\nmetrics for Model(s); and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n5.9.  Technical \n\nConsiderations \n\nDocument and assess technical issues \n\nthat should be considered during the \n\nModel selection process. \n\nTo (a) ensure that technical issues are \n\nconsidered when selecting Models; \n\nand (b) highlight associated risks that \n\nmight occur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n20 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure an effective and auditable Product Lifecycle. \n\nControl:  Aim: \n\n6.1.  Product \n\nRequirements \n\nDraft and document clear Product requirements. \n\nReview Model Type - Metrics and Acceptance \n\nCriteria, as discussed in Section 4 - Problem \n\nMapping, to ensure alignment. Regularly review \n\nProduct & Outcome Definitions Data & Model \n\nMetrics and User Experience Mapping, as \n\ndiscussed in Section 4 - Problem Mapping, and \n\nupdate as necessary \n\nTo (a) ensure current, clear, \n\nand actionable Product \n\nrequirements; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n6.2.  Product \n\nRoadmap and \n\nPipeline \n\nDevelop and document a Product roadmap \n\nand pipeline that enable the experience \n\nenvisioned in the User Experience Mapping, as \n\ndiscussed in Section 4 - Problem Mapping, and \n\ninclude the following sections: Schedule and \n\nmilestones, tasks and deliverables, limitations \n\nand exclusions (scope), initial prioritization, and \n\nmethods for determining future priority. \n\nTo (a) ensure a clear, \n\nactionable, and prioritized \n\nProduct roadmap and pipeline; \n\nand (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n6.3.  Experimentation \n\nConstraints \n\nDevelop and document a method for evaluating \n\nthe quality of predictions. Develop and \n\ndocument criteria for determining when to stop \n\nthe experimentation process. \n\nTo (a) develop processes to \n\nensure a balance between \n\neffectiveness and efficiency in \n\nthe experimentation cycle; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n6.4.  Behavioral \n\nChange Analysis \n\n- Process \n\nChanges \n\nResearch and assess the business processes \n\nthat will be affected by the new Product and/ \n\nor the infrastructure changes that enable the \n\nProduct. Review the User Experience Mapping, \n\nData Capacity Analysis, and Organisation \n\nCapacity Analysis, as discussed in Section 4 -\n\nProblem Mapping, and reformulate as necessary. \n\nDevelop and document a plan to retrain affected \n\nparties as necessary and mitigate business \n\ndisruptions as much as feasible. \n\nTo (a) determine business \n\nprocesses that may be affected \n\nby the project and create a plan \n\nto retrain or mitigate impacts \n\nas necessary; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n# Section 6. Management & \n\n# Monitoring \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n21 FBPML Technical Best Practices v1.0.0. \n\n6. Management & Monitoring - FBPML Technical Best Practices v1.0.0. \n\n6.5.  Behavioral \n\nChange Analysis \n\n- Social (Off-\n\nPlatform) \n\nResearch and document ways in which the \n\nProduct can be abused or negatively impact \n\ncustomers, end users, or the broader society. \n\nDevelop and document a plan to mitigate \n\nnegative impacts as much as feasible. Develop \n\nand document counter metrics to assess \n\nwhether users or the model are â€˜gamingâ€™ the \n\nsystem. \n\nTo (a) determine (i) negative \n\nproduct uses, (ii) negative \n\nproduct impacts (iii) negative \n\nuser or model behaviors \n\nand create a plan to counter \n\nbehaviors or mitigate impacts \n\nas necessary; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n6.6.  Resource \n\nAssessment \n\nDocument the processes, tools, and staffing \n\nthat are required for every phase of the \n\nproject, including the Data Capacity Analysis, \n\nOrganisation Capacity Analysis, Product \n\nScaling Analysis, and Product Cost Analysis, \n\nas discussed in Section 4 - Problem Mapping, \n\nbefore starting each phase of the project and \n\nupdate as necessary. \n\nTo (a) ensure adequate \n\nresources and funding during \n\nevery phase of the Product \n\nLifecycle; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n6.7.  POC-to-\n\nProduction \n\nChecklist \n\nDocument and define a POC-to-Production \n\nChecklist that details the existing system \n\nmodifications, and new system builds, required \n\nfor integrating the Product into Organisation \n\ninfrastructure and incorporating additional data \n\nsources. If gaps in organisational capacity are \n\ndetected, reiterate formulations of Organisation \n\nCapacity Analysis, as discussed in Section 4 -\n\nProblem Mapping, as necessary. \n\nTo (a) ensure sufficient planning \n\nfor Product development and \n\nproduction; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n6.8.  Update Schedule  Document and define a POC-to-Production \n\nChecklist that details the existing system \n\nmodifications, and new system builds, required \n\nfor integrating the Product into Organisation \n\ninfrastructure and incorporating additional data \n\nsources. If gaps in organisational capacity are \n\ndetected, reiterate formulations of Organisation \n\nCapacity Analysis, as discussed in Section 4 -\n\nProblem Mapping, as necessary. \n\nTo (a) ensure the Product \n\nand its related software are \n\nupdated and upgraded regularly \n\nand that the schedule for \n\nsaid updates are coordinated \n\nwith information technology \n\ndepartment(s); and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n22 FBPML Technical Best Practices v1.0.0. \n\n6. Management & Monitoring - FBPML Technical Best Practices v1.0.0. \n\n6.9.  Project Records  Develop and document a process for preserving \n\ndata of the information considered when making \n\nsignificant product decisions. Include any \n\nmethods for standardized experiment tracking \n\nand artifact capturing that are developed \n\nby Data Science and Engineering. Develop \n\na continuously maintained and consistently \n\navailable repository for Product Requirements \n\nand any data related to their updates. \n\nTo (a) maintain a historical \n\nrecord of Product and data \n\nand ensure that all iterations \n\nof Product Requirements \n\nare continuously available to \n\nStakeholders; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n6.10.  Project Records \n\n- Stakeholder \n\nSign-offs \n\nDevelop a standard Stakeholder Sign-off \n\nDocument to be utilized (i) after the finalization \n\nof the following documents and analyses: \n\nProblem Statement & Solution Mapping, \n\nOutcomes Definition, Product & Outcome \n\nDefinitions, Product Integration Strategy, Model \n\nType - Best Fit Analysis, Acceptance Criteria -\n\nKey Business Metrics/Targeted Metrics, Testing \n\nDesign and Scheduling Framework, Resource \n\nAssessment, as discussed in Section 4 - Problem \n\nMapping, Section 5 - Model Decision-Making; \n\nand (ii) at Project Checkpoints, as discussed in \n\nSection 10 - Project Checkpoints. \n\nTo (a) ensure stakeholder \n\nbuy-in; and (b) provide an \n\nauditable record of project and \n\nstakeholder expectations at \n\nevery major project decision-\n\npoint. \n\n6.11.  Custody  Develop a system for documenting the chain \n\nof custody for Product(s) and the data, \n\nmicroservices, and applications that it is built on \n\nand with, that indicates: i) provenance ii) control \n\niii) transfer, iv) analysis, and v) transformation. \n\nTo (a) ensure that the building \n\nblocks of the Product can be \n\ntraced back to their origins; (b) \n\nallow for undesirable changes \n\nto be reverted; and (c) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n23 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo determine the most appropriate and feasible privacy-preserving techniques for the Product. \n\nControl:  Aim: \n\n7.1.  Decentralization \n\nMethod Analysis \n\nConsider the appropriateness of utilizing \n\nmethods for distributing data or training across \n\ndecentralized devices, services, or storage. \n\nWhen analyzing federated learning methods, \n\nconsider Data Capacity Analysis, Product \n\nIntegration Strategy, Product Traceability, and \n\nFairness & Non-Discrimination, as discussed \n\nmore thoroughly in Section 4 - Problem Mapping; \n\nSection 21 - Product Traceability; and Section 11 \n\n- Fairness & Non-Discrimination. When analyzing \n\ndifferential privacy methods, consider Data \n\nQuality - Noise, as discussed more thoroughly in \n\nSection 12 - Data Quality. \n\nTo (a) ensure appropriate \n\nprivacy-preserving techniques \n\nthat are aligned with chosen \n\nModels; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n7.2.  Cryptographic \n\nMethods Analysis \n\nConsider the appropriateness of utilizing \n\nmethods for encrypting all or various parts of \n\nthe data and/or Model pipeline. When analyzing \n\nhomomorphic encryption methods, consider \n\nProduct Integration Strategy and Product \n\nScaling Analysis, as discussed more thoroughly \n\nin Section 4 - Problem Mapping. Additionally, \n\nconsider -\n\n(a) whether the types of operations and \n\ncalculations that can be performed meet the \n\nrequirements of Model Type - Best Fit Analysis, \n\nas discussed more thoroughly in Section 5 -\n\nModel Decision-Making; and/or \n\n(b) whether the encrypted Model processing \n\nspeed is acceptable with consideration for real \n\nworld robustness and direct user interaction, \n\nas discussed more thoroughly in Section 14 -\n\nPerformance Robustness. \n\nTo (a) ensure appropriate \n\nprivacy-preserving techniques \n\nthat are aligned with chosen \n\nModels; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n# Section 7. Privacy \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n24 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure (a) that robust, effective, and efficient strategies, methodologies and schedules are developed for \n\ntesting the Product; and (b) clear maintenance metrics and/or phases to warrant continued Product alignment \n\nwith chosen metrics and performance goals. \n\nControl:  Aim: \n\n8.1.  Testing Design \n\nand Scheduling \n\nFramework \n\nDocument and define a testing design and \n\nschedule that does not artificially constrain \n\nthe testing process. Incorporate the Feedback \n\nLoop Analysis in the testing design. Review the \n\nAutomation Analysis in determining what level \n\nof automation is appropriate for each stage of \n\ntesting. Ensure the individuals chosen through \n\nthe Testing Participant Identification process \n\nare involved at the earliest stages of the testing \n\nschedule as practical. Post Product deployment, \n\ndocument and define a framework and process \n\nfor testing and selecting variations of the \n\nproduction Model. \n\nTo (a) ensure a robust and \n\nfeasible testing design and \n\nscheduling framework that \n\nallows for effective Product \n\noptimization; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n8.2.  Testing \n\nParticipant \n\nIdentification \n\nDocument and define a process for identifying \n\ntest participants as required by User Experience \n\nMapping and Societal Context, as discussed in \n\nSection 4 - Problem Mapping; and Section 3 -\n\nContext. Determine a framework for ensuring \n\nthat testing participants are intentionally \n\ndiverse across use cases, user types and roles, \n\nand internal and external Stakeholders. \n\nTo (a) ensure testing for user \n\nimpact and participant pool \n\ndiversity; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n# Section 8. Testing \n\nFBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n25 FBPML Technical Best Practices v1.0.0. \n\n8. Testing - FBPML Technical Best Practices v1.0.0. \n\n8.3.  Automation \n\nAnalysis \n\nDetermine and define data, Model, and \n\ncomponent integration validations that can be \n\nreasonably automated. Assess and define any \n\nprocesses during the development, deployment, \n\nor maintenance phases that could benefit \n\nfrom integrating automation into the testing \n\ninfrastructure. Be sure to review -\n\n(a) the Organisation Capacity Analysis, as \n\ndiscussed in Section 4 - Problem Mapping, while \n\ndetermining the feasibility of automating the \n\nidentified processes; and/or \n\n(b) the Industry, Deployment, and Societal \n\nContexts, as discussed in Section 3 - Context, to \n\nuncover any gaps or misalignment raised by the \n\nautomation of any identified process. \n\nTo (a) identify suitable and \n\neffective areas for incorporating \n\ntesting automation within \n\nthe Product development, \n\ndeployment, and maintenance \n\nphases; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n8.4.  Feedback Loop  Document and define a feedback loop that \n\nenables monitoring of stability, performance, \n\nand operations metrics, and counter-metrics, \n\nas required by Performance Robustness, \n\nMonitoring and Maintenance, and Systemic \n\nStability, as discussed more thoroughly in \n\nSection 14 - Performance Robustness; Section \n\n15 - Monitoring & Maintenance; and Section 20 \n\n- Systemic Stability. Develop and incorporate a \n\nmethod for flagging bias and for issue reporting. \n\nDocument and define a process for real-time \n\nsharing of testing participant feedback with \n\nthe development and maintenance teams. \n\nIncorporate the Feedback Loop in the Testing \n\nDesign and Scheduling Framework to ensure \n\nthat the features the Model is utilizing are \n\nacceptable for the application during the \n\ndevelopment, deployment, and maintenance \n\nphases. \n\nTo (a) ensure robust and \n\nresponsive feedback loop \n\nmeasures that enable monitoring \n\nof necessary metrics and \n\neffectively integrate into the \n\nTesting Design and Scheduling \n\nFramework; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n26 FBPML Technical Best Practices v1.0.0. \n\nFBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo effectively set and communicate realistic Product expectations to Stakeholders and obtain their buy-in. \n\nControl:  Aim: \n\n9.1.  Performance  Product management should attempt to set \n\nrealistic Product performance expectations \n\nfor Stakeholders through periodic stakeholder \n\ndiscussions on the following issues: (i) limited \n\nindustry understanding of what tasks are \n\ndifficult for the Product; (ii) difficulty of \n\ndetermining what type of modifications -\n\nnetwork design, input features, or training data \n\n- will create the greatest Product improvement; \n\nand (iii) Model improvement can stall \n\nsignificantly while experimenting with different \n\nvariable modifications. \n\nTo (a) effectively communicate \n\nrealistic Product performance \n\nexpectations throughout the \n\ndevelopment process; and (b) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. \n\n9.2.  Timeframe  Product management should set expectations \n\nfor long-term investment in the Product for \n\nStakeholders, specifically focusing on: (i) the \n\nunpredictability of Product improvement; (ii) \n\nProduct difficulties are traditionally hard to \n\ndiagnose as they are often caused by subtle \n\nissues of intersecting inputs; and (iii) it is \n\npossible for the Product to completely stall with \n\nabsolutely no discernible improvement in spite \n\nof significant time and effort. \n\nTo (a) effectively set Stakeholder \n\nexpectations regarding the \n\ndifficulty of locking down \n\nProduct timelines; and (b) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. \n\n9.3.  Accuracy \n\nperception \n\nThe Product Team should work to ensure that \n\nthe solution will be accurate enough to meet a \n\nvariety of different Stakeholdersâ€™ expectations, \n\nrecognizing that each group of Stakeholders will \n\nhave different views on what is â€˜accurateâ€™ based \n\non their interaction with the Product. Product \n\nmanagement should set and communicate \n\nexpectations in-line with the achievable level of \n\naccuracy for each user group. \n\nTo (a) effectively communicate \n\nachievable accuracy levels, \n\nconsidering individual \n\nStakeholder accuracy \n\npreferences; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n# Section 9. Managing Expectations Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n27 FBPML Technical Best Practices v1.0.0. \n\n9. Managing Expectations - FBPML Technical Best Practices v1.0.0. \n\n9.4.  POC-to-\n\nProduction \n\nThe Product Team should effectively \n\ncommunicate that infrastructure is often the \n\ndetermining factor for the success of the POC-\n\nto-Production transition and rely heavily on \n\nthe POC-to-Production Checklist, as discussed \n\nin Section 6 - Management & Monitoring, to \n\nset and align Stakeholder expectations of the \n\ntransition process. The Product Team should set \n\nthe expectation, before beginning the transition \n\nprocess, that novel problems will likely arise \n\nduring the transition that may significantly \n\naffect the timeline and costs. The Product Team \n\nshould be on alert for integration issues arising \n\nclose to the final release of the solution, which \n\nthe Product Manager should communicate to \n\nrelevant Stakeholders, along with progress \n\nupdates, at a progressively more frequent \n\ncadence. \n\nTo (a) uncover and communicate \n\nissues that may delay the \n\ntransition of the solution from \n\nPOC-to-Production or make that \n\ntransition less feasible; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n9.5.  Production \n\nCosts \n\nReview and analyze the finalized POC budget to \n\ndetermine a realistic Product implementation \n\nbudget. Review the Product Cost Analysis as \n\ndiscussed in Section 4 - Problem Mapping to \n\nensure its continued accuracy and reformulate \n\nas necessary. The Product Team should \n\neffectively communicate to Stakeholders that \n\nthe budget for implementation will likely be \n\nin-line or more expensive than the cost to get \n\nthrough POC. \n\nTo (a) ensure realistic \n\nexpectations for a sufficient \n\nProduct implementation budget \n\nare communicated; and (b) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n28 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure that necessary factors are considered at key decision points. \n\nControl:  Aim: \n\n10.1.  Machine \n\nLearning \n\nAppropriate \n\nTool Analysis \n\nThe Product Team should work cross-functionally with \n\nStakeholders to define and document a Machine Learning \n\nchecklist that considers the following areas, amongst \n\nother things: \n\na.  Is there a different approach that will generate a \n\ngreater return more quickly; \n\nb.  Given the results of the Data Capacity Analysis, \n\ndoes the Organisation have enough secure, non-\n\ndiscriminatory, representative, high quality data for \n\nevery stage of the process; \n\nc.  Can the problem be solved by simple rules; \n\nd.  Does the Product solution require emotional \n\nintelligence or empathy; \n\ne.  Does the Product solution need to be fully \n\ninterpretable or explainable; \n\nf.  Given the results of the Organisation Capacity \n\nAnalysis, does the Organization have the people, \n\nprocesses, and tools necessary to productize the end \n\nproduct; \n\ng.  Can the consequences of Product failure be easily \n\nfixed or mitigated; and/or \n\nh.  What other non-technical solutions can be used \n\nto augment the Product and its offering and/or, \n\nmore directly, whether Machine Learning is the best \n\nsolution for the Product at hand. \n\nTo (a) ensure that \n\nMachine Learning is the \n\nappropriate method \n\nfor solving the chosen \n\nproblem; and (b) highlight \n\nassociated risks that \n\nmight occur in the \n\nProduct Lifecycle. \n\n# Section 10. Project Checkpoints Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n29 FBPML Technical Best Practices v1.0.0. \n\n10. Project Checkpoints- FBPML Technical Best Practices v1.0.0. \n\n10.2  Data Buy v. \n\nBuild Analysis \n\nThe Product Team should work cross-functionally with \n\nrelevant Stakeholders to define and document a Buy v. \n\nBuild checklist that considers the following areas: \n\na.  Does the Organisation have enough data for every \n\nstage of the process (training, POC, production) \n\nand for every purpose (replacing stale/flawed data, \n\nmeasuring success); \n\nb.  Does the Organisation have the right type of data for \n\nevery stage of the process (training, POC, production) \n\nand for every purpose (replacing stale/flawed data, \n\nmeasuring success); \n\nc.  Is bought data secure and free of privacy concerns; \n\nd.  Is the bias in the bought data limited, mitigatable, or \n\nremovable; \n\ne.  Given the results of the Data Quality Analysis, does \n\nthe Organisation have quality data and are datasets \n\ncomplete; \n\nf.  Given the Product Team Composition, does the \n\nOrganisation have the staffing and expertise to clean, \n\nprepare, and maintain internal data; and/or \n\ng.  Given the Data Capacity Analysis, is the necessary \n\ndata easily and readily available internally. \n\nTo (a) ensure that the \n\nOrganisationâ€™s decision \n\nto either purchase data \n\nor utilize in-house data \n\nis appropriate based on \n\nOrganisation capacity \n\nand/or constraints; and \n\n(b) highlight associated \n\nrisks that might occur in \n\nthe Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n30 FBPML Technical Best Practices v1.0.0. \n\n10. Project Checkpoints- FBPML Technical Best Practices v1.0.0. \n\n10.3  Model Buy v. \n\nBuild Analysis \n\nThe Product Team should work cross-functionally with \n\nrelevant Stakeholders to define and document a Buy v. \n\nBuild checklist that considers the following areas: \n\na.  Is the scope of the Product manageable, given the \n\nresults of the Organisation Capacity Analysis; \n\nb.  Can bought Models be used for other Products (eg. \n\ntransfer learning); \n\nc.  Does the Organisation have the in-house expertise \n\nrequired to acquire and label the training data, given \n\nthe Product Team Composition; \n\nd.  How much would it cost to acquire a properly labeled \n\ntraining dataset; \n\ne.  Given the Product Team Composition, does the \n\nOrganisation have the in-house expertise required to \n\nretrain Models, if necessary; \n\nf.  How important is Model customization and, if so, can \n\nbought Models be customised; \n\ng.  Are the Acceptance Criteria - Accuracy, Bias, and \n\nFairness requirements for bought Models feasible \n\ngiven the timeline, Product Team Composition, and \n\nOrganisation Capacity Analysis; and/or \n\nh.  What are the usage limits and costs for pre-trained \n\nModels. \n\nTo (a) ensure that the \n\nOrganisationâ€™s decision \n\nto either purchase or \n\nbuild the Models is \n\nappropriate based on \n\nOrganisation capacity \n\nand/or constraints; and \n\n(b) highlight associated \n\nrisks that might occur in \n\nthe Product Lifecycle. \n\n10.4  POC-to-\n\nProduction \n\nGo/No-Go \n\nAnalysis \n\nThe Product Team should work cross-functionally with \n\nrelevant Stakeholders to define and document a Go/No-\n\nGo checklist that considers qualitative and quantitative \n\nfactors in the following areas: \n\na.  Can POC-to-Production Checklist be adequately \n\naddressed; \n\nb.  Is the Product Cost Analysis still feasible; \n\nc.  Does the Product Team have approval for a Product \n\nmaintenance budget; \n\nd.  Are the updates, upgrades, and add-ons to the data \n\ninfrastructure near completion; \n\ne.  What is the state of customer process reconstruction \n\nand end-user training; \n\nf.  Has the failsafe, rollback, or emergency shutdown \n\nplan been completed and approved; and/or \n\ng.  Have the communication and mitigation plans in case \n\nof failsafe, rollback, or emergency shutdown been \n\ncompleted and approved. \n\nTo (a) ensure that the \n\nsolution should be \n\ndeployed in production \n\nand/or Product Domains; \n\nand (b) highlight \n\nassociated risks that \n\nmight occur in the \n\nProduct Lifecycle.", "fetched_at_utc": "2026-02-08T19:07:53Z", "sha256": "166d9a6e0506c0fa81e351c96dd63f920d7bd6700137808dad36fb7be7041508", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-13-30.pdf", "file_size": 368603, "relative_path": "pdfs\\FBPML_TechnicalBP_V1.0.0-13-30.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 9618}}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-32-62-12585dab1747", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-32-62.pdf", "title": "FBPML_TechnicalBP_V1.0.0-32-62", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n32 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) identify and mitigate risk of disproportionately unfavorable Outcomes for protected (Sub)populations; and \n\n(b) minimise the unequal distribution of Product and Model errors to prevent reinforcing and/or deriving social \n\ninequalities and/or ills, and (c) promote compliance with existing anti-discrimination laws and statutes. \n\nWhat do we mean when we refer to Fairness? \n\nFairness is a complex socio-technical challenge for which there is no single generic definition. Broadly speaking -\n\nFairness is about identifying bias in a machine learning Model or Product and mitigating discrimination with \n\nrespect to sensitive, and (usually) legally protected attributes such as ethnicity, gender, age, religion, disability, \n\nor sexual orientation. \n\nAlgorithmic discrimination can take many forms and may occur unintentionally. Machine learning Products might \n\nunfairly allocate opportunities, resources, or information, and they might fail to provide the same quality of \n\nservice to some people as they do to others. \n\nThe conversation about fairness distinguishes between group fairness and individual fairness measures. Group \n\nfairness ensures some form of statistical parity (e.g. equal calibration, equal false positive/negative rate) across \n\nprotected groups. Individual fairness requires that individuals who are similar with respect to the predictive task \n\nbe assigned similar outcomes regardless of the sensitive attribute. \n\nWhy is Fairness relevant? \n\nMachine learning Products are increasingly used to inform high-stakes decisions that impact peopleâ€™s lives.It is \n\ntherefore important that ML-driven decisions do not reflect discriminatory behavior toward certain populations. \n\nIt is the responsibility of data science practitioners and business leaders to design machine learning Products \n\nthat minimizes bias and promotes inclusive representation. \n\nSome business leaders express concerns about a potential increase in the risk of reputational damage and legal \n\nallegations in case of discriminatory â€˜black boxâ€™ Models. AI fairness can substantially reduce these concerns. \n\nAnother reason for taking AI fairness seriously is the development of regulatory frameworks for AI. For example, \n\nthe European Commission published a white paper on AI in 2020, which was followed in 2021 by a regulatory \n\nframework proposal for AI in the European Union. \n\nHow to apply Fairness? \n\nFairness should be considered throughout the product lifecycle. Given that AI systems are usually designed \n\nto evolve with experience, fairness should be closely monitored during deployment as well as during product \n\ndevelopment. The Technical Best Practices Guidelines provide detailed guidance into implementing fairness in \n\nyour AI products. \n\n# Section 11. Fairness & Non-\n\n# Discrimination Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n33 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) identify and mitigate risk of disproportionately unfavorable Outcomes for protected (Sub)populations; and \n\n(b) minimise the unequal distribution of Product and Model errors to prevent reinforcing and/or deriving social \n\ninequalities and/or ills, and (c) promote compliance with existing anti-discrimination laws and statutes. \n\n11.1 Product Definitions \n\nControl:  Aim: \n\n11.1.1.  (Sub)populations \n\nDefinition \n\nDefine (Sub)populations that are subject \n\nto Fairness concern, with input from \n\nDomain and/or legal experts when \n\nrelevant. \n\nTo (a) ensure that vulnerable \n\nand affected populations are \n\nappropriately identified in all \n\nsubsequent Fairness testing and \n\nModel build; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.1.2.  (Sub)population \n\nData \n\nGather data on (Sub)population \n\nmembership. If a proxy approach is used, \n\nensure the performance of the proxy is \n\nadequate in this context. \n\nTo (a) facilitate Fairness testing \n\npre- and post-Model deployment; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.3.  (Sub)population \n\nOutcome \n\nPerceptions \n\nDocument and assess whether scored \n\n(Sub)populations would view Model \n\nOutcomes as favorable or not, using \n\ninput from subject matter experts and \n\nstakeholders in affected (Sub)populations. \n\nDocument and assess any divergent views \n\namongst (Sub)populations. \n\nTo (a) ensure uniformity in (Sub) \n\npopulation outcome perception, \n\nif applicable; (b) highlight \n\nOutcome effects for different \n\n(Sub)populations; and (c) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.1.4.  Erroneous \n\nOutcome \n\nConsequence \n\nEstimation \n\nDivergence \n\nDocument and assess the results \n\nof erroneous (false positive & false \n\nnegative) outcome consequences, both \n\nreal and perceived, specifically in terms \n\nof divergence between relevant (Sub) \n\npopulations. If material divergence \n\npresent, take measures to harmonise \n\nOutcome perceptions and/or mitigate \n\nerroneous Outcome consequences in \n\nModel design, exploration, development, \n\nand production. \n\nTo (a) ensure uniformity in \n\nerroneous Outcomes for (Sub) \n\npopulations; (b) highlight outcome \n\neffects for different (Sub) \n\npopulations; and (c) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.1.5.  Positive Outcome \n\nSpread \n\nDocument and assess the degree to \n\nwhich Model positive outcomes can be \n\ndistributed to non-scored (Sub)population, \n\nwhen contextually appropriate. If present, \n\ntake measures to promote Model Outcome \n\ndistribution in Model design, exploration, \n\ndevelopment, and production. \n\nTo (a) ensure the non-prejudicial \n\nspread of positive Model Outcomes; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n34 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\n11.1.6.  Enduring Bias \n\nEstimation \n\nDocument and assess whether exclusions \n\nfrom Product usage might perpetuate \n\npre-existing societal inequalities between \n\n(Sub)populations. If present, take \n\nmeasures to mitigate societal inequalities \n\nperpetuation in Model design, exploration, \n\ndevelopment, and production. \n\nTo (a) ensure the non-prejudicial \n\nspread of Model Outcomes; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.7.  Appropriate \n\nFairness Metrics \n\nConsult Domain experts to inform \n\nwhich Fairness metrics are contextually \n\nmost appropriate for the Model when \n\nconducting Fairness testing. \n\nTo (a) ensure that fairness testing \n\nand subsequent Model changes (i) \n\nresult in outcome changes which \n\nare relevant for (Sub)populations; \n\nand/or (ii) are consistent with \n\nregulatory guidance and context-\n\nspecific best practices; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.8.  Model Implications  Document and assess the downside risks \n\nof Model misclassification/inaccuracy \n\nfor modeled populations. Use the relative \n\nseverity of these risks to inform the \n\nchoice of Fairness metrics. \n\nTo (a) ensure that improving in the \n\nchosen Fairness metrics achieves \n\nthe greatest Fairness in Model \n\ndecisioning after deployed; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.1.9.  Fairness Testing \n\nApproach \n\nDocument and assess the Fairness testing \n\nmethodologies that will be applied to \n\nModel and/or candidate Models, along with \n\nany applicable thresholds for statistical/ \n\npractical significance, acceptable \n\nperformance loss tolerance, amongst \n\nother metrics. \n\nTo (a) prevent Fairness testing \n\nmethodology and associated \n\nthresholds change during \n\nModel review; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\nObjective \n\nTo identify and control for Fairness and Non-Discrimination risks based on the available datasets. \n\n11.2 Exploraition \n\nControl:  Aim: \n\n11.2.1.  (Sub)population Data \n\nAccess \n\nKeep separate Model development \n\ndata and (Sub)population membership \n\ndata (if applicable Regulations allow \n\nthe possession and processing of such \n\nin the first place), especially if the use \n\nof (Sub)population data in the Model is \n\nprohibited or would introduce fairness \n\nconcerns. \n\nTo (a) guarantee that (Sub) \n\npopulation membership data does \n\nnot inadvertently leak into a Model \n\nduring development. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n35 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\n11.2.2.  Univariate \n\nAssessments \n\nDocument and perform univariate \n\nassessments of relationship between \n\n(Sub)populations and Model input \n\nFeatures, including appropriate \n\ncorrelation statistics. \n\nTo (a) identify input Feature trends \n\nassociated with (Sub)populations; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.2.3.  Prohibited Data \n\nSources \n\nDevelop and maintain an index of data \n\nsources or features that should not be \n\nmade available or utilized because of \n\nthe risks of harming (Sub)populations, \n\nspecifically Protected Classes. \n\nTo (a) prohibit the actioning of data \n\nsources that will disproportionately \n\nprejudice (Sub)populations; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.2.4.  Data \n\nRepresentativeness \n\nEnsure the membership rates of (Sub) \n\npopulations in Model development data \n\nalign with expectations and that data is \n\nrepresentative of Domain populations. \n\nTo (a) guarantee that Model \n\nperformance and Fairness testing \n\nduring model development will \n\nprovide a consistent picture \n\nof Model performance after \n\ndeployment; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.2.5.  (Sub)population \n\nProxies and \n\nRelationships \n\nDocument and assess the relationship \n\nbetween potential input Features and \n\n(membership of) (Sub)populations \n\nof interest based on, amongst other \n\nthings, (i) reviews with diverse Domain \n\nexperts, (ii) explicit encoding of (Sub) \n\npopulation membership, (iii) correlation \n\nanalyses, (iv) visualization methods. If \n\nrelationships exist, the concerned input \n\nFeatures should be excluded from Model \n\ndatasets, unless a convincing case can \n\nbe made that an (adapted version of) the \n\ninput Feature will not adversely affect \n\nany (Sub)populations, and document this. \n\nTo (a) prevent Model decisions \n\nbased directly or indirectly on \n\nprotected attributes or protected \n\nclass membership; (b) reduce \n\nthe risk of Model bias against \n\nrelevant (Sub)populations; (c) \n\nunderstand any differences in \n\ndata distributions across (Sub) \n\npopulations before development \n\nbegins; and (c) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\nAssociated Controls  Review the following controls with particular attention in the context of bias \n\nand fairness with respect to protected (Sub)populations: \n\nSection 12.2.2. - Missing and Bad Data Assessment. \n\nSection 13.2.4. - Selection Function; which is concerned with accurate \n\nrepresentation of (Sub)populations. \n\nSection 13.3.1. - 13.3.4.; which are concerned with the choice and definition of \n\nthe Target Feature. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n36 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n11.3.1.  Explainability (xAI) \n\n(Sub)population \n\nOutcomes \n\nKeep separate Model development data \n\nand (Sub)population membership data \n\n(if applicable Regulations allow the \n\npossession and processing of such in the \n\nfirst place), especially if the use of (Sub) \n\npopulation data in the Model is prohibited \n\nor would introduce fairness concerns. \n\nTo (a) guarantee that (Sub) \n\npopulation membership data does \n\nnot inadvertently leak into a Model \n\nduring development. \n\n11.3.2.  Model Architecture \n\nand Interpretability \n\nChoose Model architecture that maximizes \n\ninterpretability and identification \n\nof causes of unfairness. Consider \n\ndifferent methodologies within the \n\nsame Model architecture (ex. monotonic \n\nXGBoost, explainable neural networks). \n\nEvaluate whether Product Aims can be \n\naccomplished with a more interpretable \n\nModel. \n\nTo (a) provide information that can \n\nguide Model-builders; (b) ensure \n\nthat Model decisions are made in \n\nline with expectations; (c) allow \n\nProduct Subjects and/or End Users \n\nto understand why they received \n\ncorresponding Outcomes; (d) help \n\ninform the causes of Fairness \n\nissues if issues are detected; \n\nand (e) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.3.  Fairness Testing of \n\nOutcomes \n\nFocus fairness testing initially on \n\noutcomes that are immediately \n\nexperienced by (Sub)populations. For \n\nexample, if a model uses a series of \n\nsub-Models to generate a score and \n\na threshold is applied to that score to \n\ndetermine an Outcome, focus on Fairness \n\nissues related to that Outcome. If issues \n\nare identified, then diagnose the issue \n\nby moving â€œup-the-chainâ€ and testing the \n\nModel score and sub-Models. \n\nTo (a) ensure that the testing \n\nperformed best reflects what will \n\nhappen when Models are deployed \n\nin the real world; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.4.  Disparate Impact \n\nTesting \n\nIf applicable, test Model(s) for disparate \n\nimpact. Evaluate whether Model(s) predict \n\na Positive Outcome at the same rate \n\nacross (Sub)populations. \n\nTo (a) ensure that (Sub)population \n\nmembers are receiving the Positive \n\nOutcome as often as their peers; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.5.  Equalized \n\nOpportunity \n\nTesting \n\nIf applicable, test Model(s) for equalized \n\nopportunity. Evaluate whether Model(s) \n\npredict a Positive Outcome for (Sub) \n\npopulation members that are actually in \n\nthe positive class at the same rates as \n\nacross (Sub)populations. \n\nTo (a) ensure that (Sub)population \n\nmembers who should receive the \n\nPositive Outcome are receiving the \n\nPositive Outcome as often as their \n\npeers; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\nObjective \n\nTo minimise the unequal distribution of Product and Model errors for (Sub)populations during Model development \n\nin the most appropriate manner. \n\n11.3. Development Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n37 FBPML Technical Best Practices v1.0.0. \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\n11.3.6.  Equalized Odds \n\nTesting \n\nIf applicable, test Model(s) for equalized \n\nodds. Evaluate whether Model(s) predict \n\na Positive & Negative Outcome for (Sub) \n\npopulation members that are actually in \n\nthe positive & negative class respectively \n\nat the same rates across (Sub)populations. \n\nTo (a) ensure that (i) protected \n\n(Sub)populations who should \n\nreceive the Positive Outcome are \n\nreceiving the Positive Outcome as \n\noften as other (Sub)populations, \n\nand (ii) protected (Sub)populations \n\nwho should not receive the Positive \n\nOutcome are not receiving the \n\nPositive Outcome as often as other \n\n(Sub)populations; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.7.  Conditional \n\nStatistical Parity \n\nTesting \n\nIf applicable, test Model(s) for conditional \n\nstatistical parity. Evaluate whether \n\nModel(s) predict a Positive Outcome at \n\nthe same rate across (Sub)populations \n\ngiven some predefined set of â€œlegitimate \n\nexplanatory factorsâ€. \n\nTo (a) ensure that (Sub)populations \n\nmembers are receiving the Positive \n\nOutcome just as often as (Sub) \n\npopulations with similar underlying \n\ncharacteristics; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.8.  Calibration Testing \n\nAcross (Sub) \n\npopulations \n\nIf applicable, test Model(s) for calibration. \n\nEvaluate whether (Sub)populations \n\nmembers with the same predicted \n\nOutcome have an equal probability of \n\nactually being in the positive class. \n\nTo (a) ensure that Subpopulations \n\neach have the same likelihood of \n\ndeserving the Positive Outcome \n\nfor a given Model prediction; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.9.  Differential Validity \n\nTesting \n\nIf applicable, test Model(s) for differential \n\nvalidity. Evaluate whether Model \n\nperformance varies meaningfully by (Sub) \n\npopulation, with a special focus on any \n\ngroups that are underrepresented in \n\nmodelling data. \n\nTo (a) ensure that the Modelâ€™s \n\npredictive abilities arenâ€™t isolated in \n\nor concentrated to (Sub)population \n\nmembers; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.3.10.  Feature Selection \n\nFairness Review \n\nEvaluate the impact of removing or \n\nmodifying potentially problematic input \n\nFeatures on Fairness metrics and Model \n\nquality. \n\nTo (a) assess whether more fair \n\nalternative Models can be made \n\nthat fulfill Model objectives; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.3.11.  Modeling \n\nMethodology \n\nFairness Review \n\nEvaluate the impact of changing Modelling \n\nmethodology choices (f.e. algorithm, \n\nsegmentation, hyperparameters, etc.) on \n\nFairness metrics and Model quality. \n\nTo (a) assess whether more fair \n\nalternative Models can be made \n\nthat fulfill the Model objectives; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n38 FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo maintain operationalised Fairness at the level established during Model Development. \n\n11.4 Production \n\n11. Fairness and Non-Discrimination - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n11.4.1.  Domain Population \n\nStability \n\nContinually assess the stability of the \n\nDomain population being scored, both in \n\nterms of its composition relative to the \n\nModel development population, and the \n\nquality of the Model by class. \n\nTo (a) ensure the continued \n\naccuracy of Fairness tests \n\nand metrics; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n11.4.2.  Fairness Testing \n\nSchedule \n\nDefine a policy for timing of re-\n\nassessment of Model fairness that \n\nincludes re-testing at regular intervals \n\nand/or established trigger events (e.g. \n\nany modifications to Model inputs or \n\nstructure, changes to the composition of \n\nthe modeled population, impactful policy \n\nchanges). \n\nTo (a) detect issues with Model \n\nFairness that may not have existed \n\nduring pre-deployment of the \n\nModel; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n11.4.3.  Input Data \n\nTransparency \n\nEnsure that Product Subjects have the \n\nability to observe attributes relied on \n\nin the modeling decision and correct \n\ninaccuracy. Collect data around this \n\nprocess and use it to identify issues in the \n\ndata sourcing/aggregation pipeline. \n\nTo (a) ensure that the Model is \n\nmaking decisions on accurate \n\ndata; (b) learn whether there are \n\nproblems with Modelâ€™s data assets; \n\nand (c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n11.4.4.  Feature Attribution  Ensure that Product Subjects can \n\nunderstand why the Model made the \n\ndecision it did, or how the Model output \n\ncontributed to the decision. Ideally, an \n\nunderstanding would include which \n\nfeatures were most important in the \n\ndecision and give some guidance as \n\nto how the subject could improve in \n\nthe eyes of the Model. (See Section 13 -\n\nRepresentativeness & Specification for \n\nfurther information.) \n\nTo (a) ensure that Product Subjects \n\n(i) have some level of trust/ \n\nunderstanding in the Model that \n\naffect them and (ii) feel that they \n\nhave agency over the process \n\nand that Model Outcomes are not \n\narbitrary. \n\n11.4.5.  Product Subject \n\nAppeal Process \n\nIncorporate a â€œright of appealâ€ procedure \n\ninto the Modelâ€™s deployment, where \n\nProduct Subjects can request a human \n\nreview of the modeling decision. Collect \n\ndata around this process and use it to \n\ninform Model design choices. \n\nTo (a) ensure that Product Subjects \n\nare, at a minimum, made aware of \n\nthe results of Model decisions; and \n\n(b) allow inaccurate predictions to \n\nbe corrected. \n\n11.4.6.  Feature attribution \n\nMonitoring \n\nAs part of regularly scheduled review, or \n\nmore frequently, monitor any changes in \n\nfeature attribution or other explainable \n\nmetric by sub-population. (See Section \n\n15 - Monitoring & Maintenance for further \n\ninformation. \n\nTo (a) detect reasons for changes \n\nin Model performance, as well \n\nas any changes earlier in the \n\ndata pipeline; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n39 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure Data Quality and prevent unintentional effects, changes and/or deviations in Product and Model \n\noutputs associated with poor Product data. \n\nWhat is Data Quality? \n\nLike many other concepts in Machine learning and data science, Data quality is something without a single and \n\nwidely accepted definition. Nonetheless, we think of -\n\nData Quality as data which is fit for use for its intended purpose and satisfies business, system and technical \n\nrequirements. \n\nIn technical terms, data quality can be a measure of its completeness, accuracy, consistency, reliability and \n\nwhether it is up-to-date. \n\nData integrity is sometimes used interchangeably with data quality. However, data integrity is a broader concept \n\nthan data quality and can encompass data quality, data governance and data protection. \n\nWhy is Data quality important? \n\nIt is not difficult for all stakeholders involved in a Project to agree that good data quality is of prime importance. \n\nBad data quality means a business or an organization may not have a good grasp on whether they are successful \n\nin meeting prior set objectives or not. Bad data quality results in poor analytical solutions, wrong insights and \n\nconclusions. This translates into inadequate response to market opportunities, an inability to timely react to \n\ncustomersâ€™ requests, increased costs, and last, but not least, potential shortcomings in meeting compliance \n\nrequirements. In short, poor data results in poor products and poor decisions. This is undesirable. \n\nThe How of Data quality \n\nData quality is something that needs to be addressed throughout the product lifecycle, not only in the early \n\nstages of it, and not in any stage in isolation. \n\n# Section 12. Data Quality Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n40 FBPML Technical Best Practices v1.0.0. \n\n12. Data Quality - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo determine if the quality of the data shall be sufficient, or can be made sufficient, to achieve the Product \n\nDefinitions. \n\n12.1 Exploration \n\nControl:  Aim: \n\n12.1.1.  Data Definitions  Document and ensure all subtleties \n\nof definitions of all data dimensions \n\nare clear, inclusive of but not limited \n\nto gathering methods, allowed values, \n\ncollection frequency, etc. If not, acquire \n\nsuch knowledge, or discard the dimension. \n\nTo (a) assess and prevent \n\nunjustified assumptions about the \n\nmeaning of a data dimension or its \n\nvalues; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n12.1.2.  Data Modeling  Document and ensure all relationships \n\nbetween (the fields of) different datasets \n\nare clear, in the light of their Data \n\nDefinitions. (See Section 12.1.1 - Data \n\nDefinitions for further information.) If \n\nthis â€œData Modelâ€ is not clear or available, \n\ncreate it, or discard the datasets. \n\nTo (a) prevent the creation and/or \n\ncombination of invalid datasets; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n12.1.3.  Missing and Bad \n\nData Assessment \n\nDocument and assess (a) the occurrence \n\nrates and (b) co-variances of missing \n\nvalues and nonsensical values throughout \n\nthe Model data. If either is significant, \n\ninvestigate causes and consider \n\ndiscarding affected data dimension(s) \n\nor commit dedicated research and \n\ndevelopment to mitigating measures for \n\naffected data dimension(s). (See Section \n\n12.3.1. - Live Data Quality for further \n\ninformation.) \n\nTo assess (a) the risk of low quality \n\ndata introducing bias to Model \n\ndata and/or Outcomes; and (b) \n\nwhether Model dataset(s) quality is \n\nsufficient for Product Definitions; \n\nand (c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n12.1.4.  Data Veracity \n\nUncertainty & \n\nPrecision \n\nDocument and assess the veracity and \n\nprecision of data. If compromised, \n\nuncertain and/or unknown, document and \n\nassess (i) the causes and sources hereof \n\nand (ii) statistical accuracy .Incorporate \n\nappropriate statistical handling \n\nprocedures, such as calibration, and \n\nappropriate control mechanisms in Model, \n\nor discard the data dimension. \n\nTo assess (a) the risk of low quality \n\ndata introducing bias to Model data \n\nand/or outcomes; (b) a priori the \n\nplausibly achievable performance; \n\n(c) whether the Model dataset(s) \n\nquality is sufficient for Product \n\nDefinitions; and (d) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n41 FBPML Technical Best Practices v1.0.0. \n\n12. Data Quality - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo determine if Model performance is affected or biased due to data quality issues. \n\n12.2 Development \n\nControl:  Aim: \n\n12.2.1.  Missing and Bad \n\nData Handling \n\nDocument and assess how missing and \n\nnonsensical data (a) are handled in the \n\nModel, through datapoint exclusion or \n\ndata imputation; (b) affect the Selection \n\nFunction through datapoint removal; (c) \n\naffect Model performance and Fairness for \n\nsubpopulations through data imputation. \n\nIf (Sub)populations are unequally affected, \n\ntake additional measures to increase \n\ndata quality and/or improve Model \n\nresilience. Consult Domain experts during \n\nassessment and mitigation. \n\nTo (a) prevent introducing bias to \n\nModel Outcomes due to low quality \n\ndata; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. \n\n12.2.2.  Error - Quality \n\nCorrelation \n\nDocument and assess whether low-quality \n\ndatapoints (those with low-confidence, \n\nuncertain, nonsensical, missing and/or \n\nimputed attributes) correlate with high \n\n(rates of) error, and how this affects \n\n(Sub)populations. If so, take additional \n\nmeasures to increase data quality and/or \n\nimprove Model performance for specific \n\n(Sub)populations. \n\nTo (a) prevent introducing bias \n\nto Model Outcomes due to low \n\nquality data; (b) whether the Model \n\ndataset(s) quality is sufficient \n\nfor Product Definition(s); and \n\n(c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nObjective \n\nTo ensure the quality of incoming data to the Product during operations. \n\n12.3 Production \n\nControl:  Aim: \n\n12.3.1.  Live Data Quality  Document and assess whether live \n\nincoming data with low quality (low-\n\nconfidence, uncertain, nonsensical, \n\nmissing and/or imputed attributes) can be \n\nhandled appropriately by the Model on the \n\nper-Data Subject level. If not, implement \n\nadditional measures, and/or re-assess \n\nvalidity of Product Definition(s) in view \n\nof non-applicability to low quality live \n\nsubsets. \n\nTo (a) assess and control that all \n\nProduct Subjects can be supported \n\nappropriately by the live Product; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n42 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) ensure that Product data and Model(s) are representative of, and accurately specified for, Product Domain \n\nas far as is reasonably practical; and (b) guard against unintentional Product and Model behaviour and Outcomes \n\nas far as is reasonably practical. \n\nWhat is Representativeness and Specification? \n\nRepresentativeness is a concept that is often used in statistics and machine learning with regards to the data \n\nwe use to train a Model. A representative data sample is a set from a larger statistical population that adequately \n\nreplicates the larger group according to a characteristic or quality under study. Put less metaphorically -\n\nRepresentativeness means the ability of the Model and its data to adequately replicate and represent that \n\ncharacteristics of its operational environment. \n\nIt should not be confused with representation learning (also known as feature learning) in machine learning. \n\nThe latter refers to a set of techniques for automatically detecting feature patterns and in fact replaces manual \n\nfeature engineering. \n\nSpecification is a less known term. In our context -\n\nSpecification refers to ensuring the appropriate degrees of freedom in the Model. \n\nFor example, we have selected the appropriate cost function for the problem at hand, the target variable is \n\nappropriate and not a proxy for what we are really interested in measuring, etc. It is like representativeness \n\nbut for the Model, and not the data. Unlike the performance robustness section, many of the controls here will \n\nbe difficult to precisely measure quantitatively. However, we should still try to consider as many scenarios as \n\npossible and minimize all risks stemming from not addressing them rigorously. \n\nWhy is Representativeness and Specification important? \n\nIf the data is not representative with relation to the goal of the Product, it will not serve us well. It will result \n\nin poor performing Models when deployed, and it will inherently contain bias (not in the fairness and non-\n\ndiscrimination sense but in relation to sampling). This can lead to misleading conclusions and unrealistic \n\nassumptions and expectations. Correct specifications on the other hand relates to selecting appropriate and \n\nrigorous features, selection function, and target, etc. This ensures that the Model we develop is rigorous, robust \n\nand has a properly specified number of parameters. \n\nThe How of Representativeness and Specification \n\nRepresentativeness and Specification is something that needs to be addressed throughout the product lifecycle, \n\nnot only in the early stages of it, and not in any stage in isolation. \n\n# Section 13. Representativeness \n\n# & Specification Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n43 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) ensure the pragmatic formulation and accurate specification of Product Definition(s); (b) minimise Model \n\nsimplifications, assumptions and ambiguities; and (c) ensure adequate vigil of the non-reducible ones throughout \n\nthe Product Lifecycle. \n\n13.1 Product Definitions \n\nControl:  Aim: \n\n13.1.1.  R&S Product \n\nDefinition(s) \n\nAssessment \n\nDocument and assess whether recorded \n\nProduct Definition(s) are complete, \n\nunambiguous and representative of \n\nintended Product Outcomes. If they are \n\nnot, refine them as much as is reasonably \n\npractical. \n\nTo (a) enable reliable execution of \n\nall further research, development \n\nand assessments; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.2.  Product \n\nAssumptions \n\nDocument and assess Product \n\nassumptions, the likelihood of their \n\nappropriateness, their continued validity, \n\nand inherent risks. \n\nTo (a) detect, mitigate and review \n\nProduct assumptions and their \n\ninherent risks; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.3.  Product \n\nSimplifications \n\nDocument and assess Product \n\nsimplifications, the likelihood of their \n\nappropriateness, and their inherent risks. \n\nTo (a) detect, mitigate and review \n\nProduct simplifications and their \n\ninherent risks; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.4.  Product Limits  Document and assess the limitations \n\nof the Productâ€™s application and the \n\napplicability of Product Definitions. \n\nTo (a) detect and review Model \n\nlimitations in light of (i) Model \n\nassumptions and (ii) Model \n\nsimplifications; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.1.5.  R&S Problem \n\nDefinition Review \n\nR&S Product Definition(s) ought to be \n\nreviewed continually, specifically when \n\nsignificant Model changes occur. \n\nTo ensure that R&S Product \n\nDefinition(s) are kept up-to-\n\ndate to ensure their continued \n\neffectiveness, suitability, and \n\naccuracy; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n44 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n13.2.1.  Data Subjectivity  Document and assess whether the \n\nModel dataset(s) contain subjective \n\ncomponents. If subjective components \n\nare present, take measures to handle or \n\navoid subjectivity risks in Product and/ \n\nor Model design as much as is reasonably \n\npractical. \n\nTo (a) assess and control for the \n\naccuracy of the specification \n\nof Model inputs, manipulations, \n\nOutcomes, and interpretations \n\nto ensure the unambiguous \n\napplicability of Model(s) in Product \n\nDomain(s); and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\n13.2.2.  Heterogeneous \n\nVariable \n\nSimplification \n\nDocument and assess whether Model \n\ndatasets contain, or Model components \n\nproduce, simplified input Features that \n\nrepresent inherently heterogeneous \n\nconcepts in Product Domains. If \n\nsimplified, take measures to reflect the \n\nheterogeneity of Product Domains as \n\nmuch as is reasonably practical. \n\nTo (a) detect, review and \n\ncontrol for the simplification of \n\nheterogeneous input Variables; \n\n(b) prevent generalization and \n\nspurious correlation; and (c) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. \n\n13.2.3.  Hidden Variables  Document and assess whether \n\nModel datasets are missing, or Model \n\ncomponents hide relevant attributes of \n\nProduct Subjects or systemic Variables \n\nwith respect to Product Domains. If \n\nhidden, obtain additional data and/ \n\nor account for the hidden Variables in \n\nmodelling as much as is reasonably \n\npractical. \n\nTo (a) assess and control for hidden \n\ncorrelations and causal relations in \n\nModel datasets and Variables and/ \n\nor risks of relations being spurious, \n\nambiguous and/or confounding; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n13.2.4.  Selection Function  Document and assess the propensity \n\nof subpopulations and subpopulation \n\nmembers to be (accurately) recorded in \n\nModel datasets, with particular care for \n\n(i) unrecorded individuals, (ii) Protected \n\nClasses, and (iii) survivorship effects. \n\nIncorporate the Selection Function \n\nin Model development and evaluation \n\nin particular during Fairness & Non-\n\nDiscrimination, Performance Robustness \n\ncontrols. \n\nTo (a) assess and control for the \n\naccuracy of Model and Model \n\ndatasets in representing (Sub) \n\npopulations; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\nObjective \n\nTo (a) ensure that Model dataset(s) correspond to the Product Definition in sufficient detail, completeness and \n\nwithout material unambiguity; and (b) to identify associated risks in order to ensure an adequate vigil throughout \n\nthe Product Lifecycle. \n\n13.2 Exploration Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n45 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\n13.2.5.  Feature \n\nConstraints \n\nEvaluate whether any constraints should \n\nbe applied to input Features, such as \n\nmonotonicity or constraints on input \n\nFeature interactions in consultation with \n\nDomain experts. If determined, utilise \n\nidentified constraints. \n\nTo (a) ensure that (i) Model \n\nOutcomes are maximally \n\ninterpretable and (ii) Model \n\nbehavior for individual Model \n\nSubjects is consistent with Domain \n\nexperience; and (b) highlight \n\nassociated risks that might occur \n\nin the Product Lifecycle. \n\nControl:  Aim: \n\n13.3.1.  Target \n\nSubjectivity \n\nDocument and assess whether the Target \n\nFeature(s) objectively represent Product \n\nDomain(s). If subjective, consider refining \n\nProduct Definition(s), choosing a different \n\nTarget Feature, or taking measures \n\nto promote the objectivity of Product \n\nOutcomes. \n\nTo (a) ensure that Product Outcomes \n\nare representative of subpopulations \n\nand applications, and are not \n\nmisinterpreted; (b) ensure that Models \n\nare optimized only and precisely \n\naccording to Product Definitions; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n13.3.2.  Target Proxies  Document and assess whether the \n\nTarget Feature(s) are proxies for the true \n\nTarget(s) of Interest in Product Domain(s). \n\nIf Target Features are proxies, take \n\nmeasures to ensure and review non-\n\ndivergence of Product Outcomes with \n\nregard to Product Definitions. \n\nTo (a) ensure that Product Outcomes \n\nare representative of subpopulations \n\nand applications, and are not \n\nmisinterpreted; (b) ensure that Models \n\nare optimized only and precisely \n\naccording to Product Definitions; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n13.3.3.  Target Proxy \n\nvs. True Target \n\nof Interest \n\nContrasting \n\nIf the Target Feature is a proxy (i) \n\ndocument and assess whether the \n\ntrue Target(s) of Interest correlate with \n\nprotected attributes and classes, including \n\nthrough hidden systemic Variables as \n\nmuch as is reasonably practical; and (ii) \n\ndocument and assess whether the true \n\nTarget(s) of Interest and the proxy Target \n\nFeature(s) correlate differently with the \n\nModel datasets. If true, take measures to \n\nmitigate this as much as is reasonably \n\npractical. \n\nTo (a) ensure that the Model design \n\nis oriented to the true Target(s) of \n\nInterest; and (b) highlight associated \n\nrisks that might occur in the lack \n\nthereof in the Product Lifecycle. \n\nObjective \n\nTo (a) ensure that Model design is sufficiently specified to represent Product Domain(s) and the Product \n\nDefinition(s) as much as is reasonably practical; and (b) minimise the risks of (i) adverse effects from the Modelâ€™s \n\noptimisation leading to unintended loopholes and local optima, and (ii) mis-balancing competing optimisation \n\nrequirements in Model design and development. \n\n13.3 Development Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n46 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\n13.3.4.  Heterogeneous \n\nTarget Variable \n\nSimplification \n\nDocument and assess whether the \n\nTarget Feature is a simplification of, or \n\ncontains a subset of, true Target(s) of \n\nInterest. If true, consider refining Product \n\nDefinitions, recovering the heterogeneity, \n\nor failing that, take measures to mitigate \n\nand review this as much as is reasonably \n\npractical. \n\nIdem Section 11.3.1-2; and to (a) detect \n\nand control for risks of generalization \n\nand spurious correlation creation. \n\n13.3.5.  Cost Function \n\nSpecification & \n\nOptimisation \n\nDocument and assess the risk propensity \n\nfor - (i) optimizing for subset of \n\nobjectives to the detriment of other \n\nProduct objectives, (ii) optimizing for \n\nOutcomes that are unintended and/or \n\nnot aligned with any Product objectives, \n\n(iii) feedback loops (when containing \n\nnested optimization loops), and (iv) \n\nModel confinement in adverse or less-\n\nthan-optimal parameter or solution \n\nspace - through Model cost function \n\nand optimisation procedures during the \n\nProduct Lifecycle. If risks occur, take \n\nmeasures to mitigate them as much as is \n\nreasonably practical. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n13.3.6.  Importance \n\nWeighting \n\nDocument and assess whether Model \n\ndata points are weighted by design or as \n\ncollateral effect. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle \n\n13.3.7.  Asymmetric \n\nError Weights \n\nDocument and assess whether Model \n\nerrors, and error rates, are weighted \n\nasymmetrically in the Model. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle \n\n13.3.8.  Feature \n\nWeighting \n\nDocument and assess whether Model \n\nfeatures are weighted by design or as \n\ncollateral effect. \n\nTo (a) ensure the adequate \n\noptimisation of Product Definitions \n\nthrough an assessment of the cost \n\nfunction and optimization procedure; \n\n(b) to respect the boundary conditions \n\nand requirements set by the Product \n\nDefinitions; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n47 FBPML Technical Best Practices v1.0.0. \n\n13. Representativeness & Specification - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n13.4.1.  Asymmetric \n\nError Costs \n\nDocument and assess whether Product \n\nDomain(s) costs produced by different \n\nModel errors types are accounted for in \n\nProduct implementation and application \n\nin software and processes. If not, take \n\nmeasures to ensure that they are. \n\nTo (a) ensure that Product \n\nDomain(s) and Product Subjects \n\nconsequences are accurately \n\nconsidered when implementing \n\nProduct outcomes; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n13.4.2.  Output \n\nInterpretation(s) \n\nDocument and assess whether Product \n\nOutcomes can be clearly, completely \n\nand unambiguously interpreted by the \n\nnon-technical parties and whether these \n\ninterpretations remain representative \n\nof Product Definition(s) and Model inner \n\nworkings. If not, take measures to ensure \n\nthat they are as much as is reasonably \n\npractical. \n\nTo (a) prevent (i) misinterpretation \n\nof Product Outcomes, (ii) the \n\napplication of Products in contexts \n\nand/or to Subjects for which \n\ntheir appropriateness and/or \n\nquality is unconfirmed, unknown, \n\nand/or unsatisfactory, (iii) the \n\nintentional and/or unintentional \n\nmisuse of Product components \n\nand Outcomes; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nObjective \n\nTo ensure that the Implementation of the Product and Model(s) align with and represent Product Definition(s) and \n\nProduct Domain(s). \n\n13.4 Production \n\n13.3.9.  Output \n\nInterpretation(s) \n\nDocument and assess whether the \n\ninterpretation of the Model Outcomes are \n\nclearly, completely and unambiguously \n\ndefined. If not, take measures to promote \n\nOutcome interpretation(s) clarity and \n\ncompleteness as much as is reasonably \n\npractical. \n\nTo (a) guard against the \n\nmisinterpretation and/or \n\nmisapplication of Model Outcomes; \n\nand (b) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n13.3.10.  Time-dependent \n\nData Modeling \n\nDocument and assess whether all time-\n\ndependent aspects of data generation \n\n(including but not limited to gathering, \n\ncalibration, cleaning, and annotation), data \n\nmodeling and data usage are specified \n\nand incorporated in Model design and \n\nProduct Definition(s). \n\nTo (a) prevent data leakage and other \n\nforms of â€œtime travelingâ€ information \n\nleading to inaccurate representations \n\nof the data and/or Data Subjects; and \n\n(b) highlight associated risks that \n\nmight occur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n48 FBPML Technical Best Practices v1.0.0. \n\n# Section 14. Performance \n\n# Robustness \n\nObjective: \n\nTo warrant Model Outcomes and prevent unintentional Model behaviour a priori under operational conditions as \n\nfar as is reasonably practical. \n\nWhat is Performance Robustness? \n\nModel robustness is the property of an algorithm that, when tested on a training sample and on a similar testing \n\nsample, the performance is similar. In other words, a robust model is one for which the testing error is similar to \n\nthe training error. Performance robustness takes into account prospective scenarios where (one of more) inputs \n\nor assumptions are (drastically) changed due to unforeseen circumstances and, in light of these, the ability of the \n\nModel to to still consistently generate accurate output. Put more holistically -\n\nPerformance Robustness means the ability of the Model to generate consistent, accurate results across different \n\nsampling tests and in light of changes in operational circumstances. \n\nWhy do we need Performance Robustness? \n\nA Model that is not robust will hopefully not end up being used and deployed. Good performance in training, \n\nbut significantly worse performance when tested on real data, is one of the reasons many proof-of-concepts \n\ndo not end up being utilized. A Model which is not robust will inevitably deteriorate over time. Its predictions \n\nand recommendations will deviate from the ground truth and the end users will lose trust in the Model and may \n\nstop utilizing it altogether. This is the optimistic case. More worrisome is when users of the Model continue to \n\nuse a poor performing Model and are unaware of its poor accuracy or precision, but still take it into account \n\nwhen making (important) judgement calls. In scenarios where there is no human-in-the-loop, detecting poor \n\nperformance robustness can be even more difficult and time costly. This will result in more unknown harm, which \n\nis naturally hard to detect and determine. So, it is clear that it is in everyoneâ€™s interest to ensure the Modelâ€™s \n\nperformance robustness. \n\nHow to ensure Performance Robustness? \n\nThough performance robustness needs to be of a certain level to even consider deploying the Model, it is \n\nsomething that needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not \n\nin any stage in isolation. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n49 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo prevent performance loss due to Product Definition changes. \n\nObjective \n\nTo prevent performance loss due to (a) data and/or data definition instability; (b) volatile data elements; and/or (c) \n\nprospective increases in scale. \n\n14.1 Product Definitions \n\n14.2 Exploration \n\nControl:  Aim: \n\n14.1.1.  Product \n\nDefinition(s) \n\nStability \n\nDocument and assess the stability of historic \n\nand prospective Product Definition(s) and \n\nProduct Aim(s). If unstable, take measures \n\nto redefine or, failing that, to correct for or \n\nmitigate as much as is reasonably practical. \n\nTo (a) ensure that Product \n\nDefinition(s) and Models remain \n\nstable and up-to-date in light of \n\nProduct Domain Stability; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n14.1.2.  Product Domain \n\nStability \n\nDocument and assess the stability of \n\nhistoric and prospective Product Domain(s). \n\nIf unstable, revise Product Definition(s) \n\naccordingly to ensure Product consistency \n\nand stability. \n\nTo (a) ensure that Product \n\nDefinition(s) and Models remain \n\nstable and up-to-date in light of \n\nProduct Domain Stability; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nControl:  Aim: \n\n14.2.1.  Data Drift \n\nAssessment \n\nDocument and assess historic and \n\nprospective changes in data distribution, \n\ninclusive of missing and nonsensical data. If \n\ndata drift is apparent and/or expected in the \n\nfuture, implement mitigating measures as \n\nmuch as is reasonably practical. \n\nTo (a) assess and promote the \n\nstability of data distributions (data \n\ndrift); (b) determine the need for \n\ndata distributions monitoring, \n\nrisk-based mitigation strategies \n\nand responses, drift resistance \n\nand adaptation simulations and \n\noptimization, and data distribution \n\ncalibration; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.2.2.  Data Definition \n\nTemporal \n\nStability \n\nDocument and assess - both technically \n\nand conceptually - historic and prospective \n\nchanges of each data dimension definition. \n\nIf unstable, consider refining Product \n\nDefinitions and/or limiting usage of unstable \n\ndata dimensions. \n\nTo (a) assess and control for the \n\nneed for Model design adaptation \n\nbased on data definition stability; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n50 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\n14.2.3.  Outlier \n\nOccurrence \n\nRates \n\nDocument and assess outliers, their causes, \n\nand occurrence rates as a function of their \n\nlocation in data space. If numerous and \n\npersistent, include mitigating measures in \n\nModel design accordingly. \n\nTo (a) identify outliers and \n\nassess the need for Model design \n\nadaptation; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.2.4.  Selection \n\nFunction \n\nTemporal \n\nStability \n\nDocument and assess the historic and \n\nprospective behaviour of Selection \n\nFunction(s) of Model data. (See Section \n\n13.2.4. - Selection Function for more \n\ninformation.) If unstable, take measures \n\nto account for past and future changes, \n\nand/or promote the consistency and \n\nrepresentativeness of Model datasets and \n\ndata gathering as much as is reasonably \n\npractical. \n\nTo (a) assess and control for \n\nhard-to-measure changes to the \n\nrelation between Model datasets \n\nand Product Domain(s); (b) identify \n\nthe risk of hard-to-diagnose Model \n\nperformance degradation and \n\nbias throughout Product Lifecycle \n\n(to be controlled by 14.3.6); and \n\n(c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n14.2.5.  Data Generating \n\nProcess \n\nTemporal \n\nStability \n\nDocument and assess the historic and \n\nprospective behaviour of data generating \n\nprocesses, and their influence on the \n\nSelection Function. If unstable, take \n\nmeasures to account for past and future \n\nchanges and/or promote the stability and \n\nconsistency of data generation processes as \n\nmuch as is reasonably practical. \n\nTo (a) assess and control for \n\nhard-to-measure changes to the \n\nrelation between Model datasets \n\nand Product Domain(s); (b) identify \n\nthe risk of hard-to-diagnose Model \n\nperformance degradation and \n\nbias throughout Product Lifecycle \n\n(to be controlled by 14.3.6); and \n\n(c) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle \n\nObjective \n\nTo characterize, determine and control for Model performance variation, risks and robustness under live \n\nconditions a priori and throughout the Product Lifecycle. \n\n14.3 Development \n\nControl:  Aim: \n\n14.3.1.  Target Feature \n\nDefinition \n\nStability \n\nDocument and assess - both technically \n\nand conceptually - the historic and \n\nprospective stability of the Target Feature \n\ndefinition. If unstable, consider refining \n\nProduct Definitions and/or choosing a \n\ndifferent Target Feature. \n\nTo (a) assess the need for Model design \n\nand Product Definition adaptation \n\nbased on Target Feature definition \n\nstability; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n51 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\n14.3.2.  Blind \n\nPerformance \n\nValidation \n\nDocument and validate that Model \n\nPerformance can always be reproduced \n\non never-before-seen hold-out data-\n\nsubsets and prove that these hold-out \n\ndata-subsets are never used to guide \n\nModel and Product design choices by \n\ncomparing Model performance on the \n\nhold-out dataset. If performance cannot \n\nbe reproduced on never-before-seen \n\nhold-out data-subset, take measures to \n\nimprove robustness and Model fitting as \n\nmuch as is reasonably practical. \n\nTo (a) ensure Model performance \n\nrobustness against insufficient \n\ngeneralization capabilities on live data \n\n(such as overfitting); and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.3.3.  Error \n\nDistributions \n\nDocument and assess error and/or \n\nresidual distributions along as many \n\ndimensions and/or subsets as is \n\npractically feasible. If distributions are \n\ntoo broad and/or too unequal between \n\nsubsets, improve Model(s). \n\nTo (a) assess and control for \n\nperformance influence of data \n\npoints and/or groups; (b) assess \n\nand control for the distribution of \n\nerrors to influence - (i) performance \n\nrobustness as a function of data drift, \n\n(ii) the systematic performance of \n\nminority data-subsets, and (iii) the \n\nrisks of unacceptable errors and/or \n\ncatastrophic failure; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.3.4.  Output Edge \n\nCases \n\nDocument and assess the causes, \n\noccurrence probabilities, overall \n\nperformance impact of Edge Cases \n\noutput by Model(s), inclusive of on Model \n\ntraining and design. If their influence \n\nis significant, improve model design. If \n\noccurrence is high, increase Model, code \n\nand data quality control. \n\nTo (a) assess and control for the \n\nimpact of Output Edge Cases on Model \n\ndesign, bugs and performance; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n14.3.5.  Performance \n\nRoot Cause \n\nAnalysis \n\nDocument and assess Model performance \n\nRoot Cause Analysis as well as its \n\ntesting method. If Root Cause Analysis \n\nis ineffective, simplify Model and/or \n\nincrease diagnostics like logging and \n\ntracking. \n\nTo (a) assess and control for Model \n\nperformance changes and assist \n\nin Model design, development, and \n\ndebugging; (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n14.3.6.  Model Drift \n\n& Model \n\nRobustness \n\nSimulations \n\nDocument and perform simulations of \n\nModel training and retraining cycles, \n\nusing historic and synthetic data. \n\nDocument and assess the effects of \n\ntemporal changes to, amongst other \n\nthings, the Selection Function, Data \n\nGenerating Process and Data Drift \n\non the drift in performance and error \n\ndistributions of said simulations. If Model \n\ndrift is apparent, document and perform \n\nfurther simulations for Model drift \n\nresponse optimization, and/or consider \n\nrefining Product Definitions. \n\nTo (a) assess and control for Model \n\npropensity for Model drift; (b) \n\ndetermine the robustness of Model \n\nperformance as a function of data \n\nchanges; (c) determine appropriate \n\nProduct response to drift; and (d) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n52 FBPML Technical Best Practices v1.0.0. \n\n14. Performance Robustness - FBPML Technical Best Practices v1.0.0. \n\n14.3.7.  Catastrophic \n\nFailures \n\nDocument and assess the prevalence of \n\npredictions with High Confidence Values, \n\nbut large Evaluation Errors. If apparent, \n\nimprove Model to avoid these, and/or \n\nimplement processes to mitigate these as \n\nmuch as is reasonably practical. \n\nTo (a) assess the propensity of the \n\nModel for catastrophic failures; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n14.3.8.  Performance \n\nUncertainty \n\nand Sensitivity \n\nAnalysis \n\nDocument and assess the probability \n\ndistribution of the model performance \n\nusing cross-validation, statistical \n\nand simulation techniques under - (a) \n\nthe assumption that the distribution \n\nof training and validation data is \n\nrepresentative of the distribution of \n\nlive data; and (b) multiple realistic \n\nvariations to the Model data due to both \n\nstatistical and contextual causes. If Model \n\nperformance variation is high, improve \n\nModel and/or take measures to mitigate \n\nperformance variation impact. \n\nTo (a) assess and control for the \n\nrange of expected values of Model \n\nperformance under both constant \n\nand changing conditions; (b) assess \n\nand control for whether trained model \n\nperformance is consistent with these \n\nranges; (c) identify main sources of \n\nuncertainty and variation for further \n\ncontrol; and (d) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n14.3.9.  Outlier Handling  Document and assess the effect of \n\nvarious outlier handling procedures on \n\n(a) Performance Robustness and (b) \n\nRepresentativeness & Specification. \n\nEnsure that only procedures are \n\nimplemented that positively affect both. \n\nTo (a) ensure that outlier removal \n\nis not used to heedlessly improve \n\ntest-time performance only and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\nObjective \n\nTo ensure the future satisfaction of Product Definition(s) through the technical and functional implementation of \n\nthe Product Model(s) and systems. \n\n14.4 Production \n\nControl:  Aim: \n\n14.4.1.  Real World \n\nRobustness \n\nDocument and assess potential future \n\nchange in the applied effects of the Product, \n\nsuch as through diminishing returns and/ \n\nor psychological effects. If significant \n\nchange or decrease is expected, consider \n\nrefining Product Definitions and/or develop \n\nprocedures for mitigation. \n\nTo (a) assess and control for the \n\nvariation in applied effects of the \n\nProduct on Product Definition(s) \n\nand performance; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n14.4.2.  Performance \n\nStress Testing \n\nPerform and document experiments \n\ndesigned to attempt to induce failures in the \n\nProduct and/or Model, for example, but not \n\nlimited to, by supplying large quantities of or \n\nunusual data to the training or inferencing \n\nphases. \n\nTo (a) identify and control for \n\nrisks associated with operational \n\nscenarioâ€™s outside of regimes \n\nencountered during Model \n\ndevelopment. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n53 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure that Products and Models remain within acceptable operational bounds. \n\nWhat is Monitoring and maintenance? \n\nMachine learning -\n\nMonitoring refers to the processes of tracking and analysing the performance of a model over time and once it is \n\ndeployed in production \n\nIt provides early warning signals for performance issues. Maintenance is closely related to monitoring but is a \n\nmore actionable concept. \n\nMaintenance relates to the activities we need to perform upon detecting or suspecting possible deterioration in \n\nthe performance of the model. \n\nThough itâ€™s a process closely related to Models in production, note that maintenance and monitoring steps need \n\nto be designed and addressed in early stages of the Product Lifecycle too. \n\nWhy is Monitoring and maintenance important? \n\nMonitoring and maintenance is not only important but it is a â€˜must haveâ€™ for any Product that is deployed in a \n\nproduction environment. Over time, the â€˜liveâ€™ data will differ in small or significant ways from the historical data \n\nused to train the Model. Trends and preferences will change too. The way certain data sources are measured and \n\ncoded will also change over time: new data sources are added, while others become unavailable. Therefore, we \n\nneed to continuously, real-time monitor the Models that are deployed. A Model that is not maintained or updated \n\nover time eventually deteriorates, makes errors and could lead to a loss of trust and varying degrees of harm (if \n\nthe domain in question is a high-stakes decision domain). \n\nThe How of Monitoring and maintenance \n\nModel monitoring and maintenance though most commonly discussed in the deployment phase, is something \n\nthat needs to be addressed throughout the product lifecycle, not only in the early stages of it, and not in any stage \n\nin isolation. \n\n# Section 15. Monitoring & \n\n# Maintenance Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n54 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) track Model performance in production; and (b) ensure desired Model performance. \n\nObjective \n\nTo (a) define robust Product and/or Model monitoring requirements, inclusive of concerns related to Features and \n\nskews of the data; and (b) ensure the continued monitoring of Products and/or Models throughout their lifecycles. \n\n15.1 Product Definitions \n\n15.2 Exploration \n\nControl:  Aim: \n\n15.1.1.  Monitoring \n\nObjectives \n\nBased on Product Definition(s), document \n\nand assess Product and Model monitoring \n\nobjectives, inclusive of which Product \n\nand/or Model elements need close \n\nmonitoring attention, such as Model \n\ndata and code. Document and assess \n\nthe associated risks of failing to achieve \n\nModel and/or Product Monitoring \n\nObjectives. \n\nTo (a) define Product and Model \n\nmonitoring objectives; and (b) \n\nhighlight associated risks for failed \n\nmonitoring. \n\n15.1.2.  Monitoring Risks  Document and assess the associated risks \n\nof failing to achieve Monitoring Objectives. \n\nTo (a) define Product and Model \n\nmonitoring risks. \n\nControl:  Aim: \n\n15.2.1.  Data Source \n\nMismatch: \n\nTraining & \n\nProduction Data \n\nDefine and deploy methods to detect \n\nthe degree to which data sources \n\nand Features, in Model training and \n\nproduction data, match one another. If \n\nmismatch is detected, take measures to \n\nensure that data sources and Features \n\nare adequately matched in both Model \n\ntraining and production data. \n\nTo (a) reduce nonsensical predictions \n\nof the Model due to (i) missing data, (ii) \n\nlack of data incorporated, or (iii) data \n\nmeasurement scaling, encoding and/or \n\nmeaning; (b) to reduce the discrepancy \n\nbetween training and production data; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n15.2.2.  Data \n\nDefinitions and \n\nMeasurements: \n\nTraining & \n\nProduction Data \n\nDefine and deploy methods by which to \n\ndetect the degree to which data sources \n\nin Model training and production have \n\nthe same definitions and measurement \n\nscales . \n\nTo (a) reduce nonsensical predictions \n\nof the Model due to (i) missing data, (ii) \n\nlack of data incorporated, or (iii) data \n\nmeasurement scaling, encoding and/or \n\nmeaning; (b) to reduce the discrepancy \n\nbetween training and production data; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\n15.2.3.  Data \n\nDependencies \n\nand Upstream \n\nChanges \n\nDerive and implement change \n\nassessments for changes in data due \n\nto - (i) one or multiple internal or external \n\nsources (partial) updates, (ii) substantial \n\nsource change, and/or (iii) changes in \n\ndata production and/or delivery. \n\nTo (a) reduce nonsensical predictions \n\nof the Model due to (i) missing data, (ii) \n\nlack of data incorporated, or (iii) data \n\nmeasurement scaling, encoding and/or \n\nmeaning; (b) to reduce the discrepancy \n\nbetween training and production data; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n55 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\n15.2.4.  Data Drift \n\nDetection \n\nDefine and deploy monitoring metrics \n\nand thresholds for detecting sudden \n\nand/or gradual, short term and/or long \n\nterm changes in data distributions, \n\ngiving priority to those that can detect \n\npast observed changes. (See Section \n\n12.2.1- Missing and Bad Data Handling \n\nfor further information). Document and \n\nassess distribution families, statistical \n\nmoments, similarity measures, trends \n\nand seasonalities. \n\nTo (a) prevent predictions from \n\ndiverging from training data and/ \n\nor Product Definitions by assessing \n\nwhether production data is \n\nrepresentative of older data; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n15.2.5.  Product and/or \n\nProduct Domain \n\nChanges: Trends \n\nand Preferences \n\nDefine and deploy (a) monitoring \n\nmethods for detecting changes in \n\nProduct Domain(s) and/or Product \n\nDefinition(s); and (b) timeframes and/ \n\nor contextual triggers for reassessment \n\nof Product Domain(s) and Product \n\nDefinition(s) continued stability. \n\nTo (a) ensure Models capture accurate, \n\nrelevant, and current trends and \n\npreferences in Product Domain(s); (b) \n\nreduce Model â€˜blind spotsâ€™ and better \n\ncapture malicious events/attempts; \n\nand (c) highlight associated risks that \n\nmight occur in the Product Lifecycle. \n\nObjective \n\nTo (a) create metrics for (i) Model performance and (ii) Model performance deterioration; and (b) ensure the \n\ncontinued monitoring of Products and/or Models throughout their lifecycles. \n\n15.3 Development \n\nControl:  Aim: \n\n15.3.1.  Model \n\nPerformance \n\nDeterioration \n\nThresholds \n\nDocument, assess, and set thresholds \n\nfor Model performance deterioration in \n\nconsultation with Stakeholders. \n\nTo (a) ensure clear guidelines \n\nand indices of Model failure and \n\nperformance deterioration; (b) \n\nreduce the risk of unacknowledged \n\nModel failure and performance \n\ndeterioration; (c) reduce the \n\nlikelihood of Model decay, ensure \n\nrobustness and good performance \n\nin terms of selected metrics \n\nand scenarios; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n15.3.2.  Product \n\nContextual \n\nIndicators: \n\nModel \n\nPerformance \n\nDeterioration \n\nDocument, assess, and set Product and \n\nProduct Domain specific indicators of Model \n\nperformance deterioration, inclusive of \n\ntechnical and non-technical indicators. \n\nTo (a) ensure clear guidelines \n\nand indices of Model failure and \n\nperformance deterioration; (b) \n\nreduce the risk of unacknowledged \n\nModel failure and performance \n\ndeterioration; (c) reduce the \n\nlikelihood of Model decay, ensure \n\nrobustness and good performance \n\nin terms of selected metrics \n\nand scenarios; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n56 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\n15.3.3.  Reactive Model \n\nMaintenance \n\nIndicators \n\nDocument, assess, and set thresholds for \n\nModel failure and reactive maintenance \n\nTo (a) ensure clear guidelines \n\nand indices of Model failure and \n\nperformance deterioration; (b) \n\nreduce the risk of unacknowledged \n\nModel failure and performance \n\ndeterioration; (c) reduce the \n\nlikelihood of Model decay, ensure \n\nrobustness and good performance \n\nin terms of selected metrics \n\nand scenarios; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n15.3.4.  Awareness of \n\nfeedback loops \n\nDefine and deploy as far as is reasonably \n\npractical (a) methods to detect whether \n\nfeedback loops are occuring, and/or (b) \n\ntechnical and non-technical warning \n\nindicators for increased risk of the same. \n\nAs per Section 17 - Security: to \n\nprevent (in)direct adverse social \n\nand environmental effects as a \n\nconsequence of self-reinforcing \n\ninteractions with the Model(s); \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\nObjective \n\nTo (a) identify operational maintenance metrics; and (b) ensure timely update, re-train and re-deployment of \n\nModel(s). \n\n15.4 Production \n\nControl:  Aim: \n\n15.4.1.  Operational \n\nPerformance \n\nThresholds \n\nDefine and set metrics and tolerance \n\nintervals for operational performance of \n\nModels and Products, such as, amongst \n\nother things, latencies, memory size, CPU \n\nand GPU usage. \n\nTo (a) prevent unavailable and \n\nunreliable service; (b) enable quick \n\ndetection of bugs in the code; (c) \n\nensure smooth integration of the \n\nModel with the rest of the systems; \n\nand (d) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n15.4.2.  Continuous \n\nDelivery of \n\nMetrics: Model \n\nPerformance \n\nContinuously report on and record \n\nmetrics about Model performance, \n\npredictions, errors, Features, and \n\nassociated performance metrics to \n\nrelevant Stakeholders (as decided upon \n\nin Section 13.2 _- Representativeness & \n\nSpecification: Exploration and Section \n\n13.3 - Representativeness & Specification: \n\nDevelopment). \n\nTo (a) enable rapid identification of \n\nModel decay, and/or red flags and \n\nbugs in Model and/or data pipelines; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n57 FBPML Technical Best Practices v1.0.0. \n\n15. Monitoring & Maintenance - FBPML Technical Best Practices v1.0.0. \n\n15.4.3.  Model Decay & \n\nData Updates \n\nOperationalise procedures to mitigate Data \n\nDrift and/or Model decay (as described in \n\nSection 14.2 _- Performance Robustness: \n\nExploration and Section 14.3 - Performance \n\nRobustness: Development). \n\nTo (a) ensure timely implementation \n\nof any changes required in data \n\nand/or Modelling pipelines; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n15.4.4.  Model Re-\n\ntraining \n\nOperationalise procedures on how Model \n\nre-training ought to be conducted as well \n\nas approached, inclusive of, amongst other \n\nthings, -\n\n(1) when will (i) a new Model be deployed, \n\nand/or (ii) a Model with the same \n\nhyperparameters but trained on new data; \n\nand/or \n\n(2) when operationalizing re-trained \n\nModels ought they be run in parallel with \n\nolder Models and/or do to gracefully \n\ndecommission older Models. \n\nTo (a) ensure timely implementation \n\nof any changes required in data \n\nand/or Modelling pipelines; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n15.4.5.  Create \n\nContingency \n\nPlans \n\nDevelop and put in place contingency plans \n\nin case of technical failures and out-of-\n\nbounds behaviour based on (a) bounds and \n\nthreshold set in other controls; and (b) risk \n\nassessment of failure modes. \n\nTo (a) prevent adverse effects \n\nfrom failures and unexpected \n\nbehaviour by providing clear \n\ninstructions on roll-back, mitigation \n\nand remediation; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n58 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure Model functions and outputs are explainable and justifiable as far as is practically reasonable in \n\norder to (a) foster explainability for Stakeholders, (b) promote Model trust, (c) facilitate Model debugging and \n\nunderstanding, and (d) promote compliance with existing laws and statutes. \n\nWhat do we mean when we refer to Explainability? \n\nThere is not one agreed-upon definition of explainability but the working definition we have adopted is that \n\nExplainability refers to making the behavior and decisions of a complex machine learning model understandable \n\nto humans. \n\nClosely related to the concept of explainability is interpretability. Interpretability refers to the degree to which a \n\nhuman can inherently understand the cause of a Modelâ€™s decision. In other words, interpretability relates to using \n\nModels that are transparent and can be inherently understood by humans; while explainability concerns making \n\ncomplex, non-transparent models understandable to humans. Many researchers and practitioners use the terms \n\ninterchangeably. \n\nTransparency is another closely related concept to explainability and interpretability. It is the broadest of the \n\nthree. Transparency refers to the openness of the workings and/or processes and/or features of data, Models \n\nand the overall project (the Product). Transparency can be both comprehensible or incomprehensible depending \n\non its content. Transparency does not necessarily mean comprehension: this is important and why it differs \n\nfrom explainability and interpretability. Again, transparency just refers to the openness of the workings and/or \n\nprocesses and/or features of data, Models and the overall project - whether technical or not. \n\nWhy is Explainability relevant? \n\nWhen we talk about machine learning used for high-stakes decisions, there is a strong agreement that it is \n\nextremely important for the public and for machine learning practitioners to understand the inner workings and \n\ndecision-making of Models. This is because through such understandings, we can ensure that machine learning \n\nis done fairly or, rather, that it does not generate unfair or harmful consequences. To put it more simply, we can \n\nensure human oversight and correction over machine learning operations. Explainability also is very important \n\nfor promoting trust and social acceptance of machine learning. People do not often trust and accept things they \n\ndo not understand. Through explainability, we can help people understand machine learning and, in turn, trust it. \n\nHow to apply Explainability? \n\nIn order to generate thorough and thoughtful explainability, it must be considered continuously throughout all \n\nstages of the product lifecycle. This means that explainability must be addressed at the (a) Product Definition(s), \n\n(b) Exploration, (c) Development and (d) Production stages of machine learning operations. \n\n# Section 16. Explainability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n59 FBPML Technical Best Practices v1.0.0. \n\n16. Explainability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo (a) ensure the transparency of Product Definitions; (b) foster multi-stakeholder buy-in through explanations; \n\nand (c) reduce ethical risks in Product Definition(s) decision-making and Model Runs. \n\n16.1 Product Definitions \n\nControl:  Aim: \n\n16.1.1.  Explainability \n\nAims \n\nHaving consideration for (a) Product \n\nDefinition(s), (b) the explanations and/or \n\ntransparency sought, (c) the Model adopted, \n\nand (d) datasets used, document and assess \n\nthe explainability aims of the Model. \n\nTo (a) clearly document the \n\nexplainability and transparency \n\naims of the Model; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.1.2.  Explainability \n\nStakeholder \n\nDocument and assess the internal and \n\nexternal Stakeholders affected by the Model. \n\nTo identify the Model explainability \n\nStakeholders; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.1.3.  Explainability \n\nRisks \n\nAssessment \n\nDocument and assess the individual risks \n\nof failing to provide model explainability, \n\ninclusive of a legal liability and Explainability \n\nStakeholders mistrust. \n\nTo identify the risks of failing \n\nto provide Model explainability; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n16.1.4.  Legal \n\nRequirements \n\nfor \n\nInterpretability \n\nDocument and assess any specific \n\nlegal requirements for Explainability in \n\nconsultation with legal experts. \n\nTo (a) ensure that minimum \n\nstandards for explainability are \n\nmet and legal risk is addressed; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n16.1.5.  Explainability \n\nRequirements \n\nDocument and assess the explainability \n\nand transparency requirements and \n\nlevels in light of (a) Explainability Aims, \n\n(b) Explainability Stakeholders, and (c) \n\nExplainability Risks, taking care that the \n\nelicitation of said requirements involves \n\nappropriate guidance, education and \n\nunderstanding of Stakeholders. \n\nTo (a) clearly document the \n\nexplainability requirements of the \n\nModel; and (b) highlight associated \n\nrisks that might occur in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n60 FBPML Technical Best Practices v1.0.0. \n\n16. Explainability - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n16.2.1.  Stakeholder \n\nAppraisal \n\nDocument and conduct (a) ad-hoc \n\ninterviews, (b) structured surveys and/ \n\nor (c) workshops with Explainability \n\nStakeholders about their Model and \n\nProduct concerns and literacy. \n\nTo (a) generate Explainability \n\nStakeholders analytics in order to map \n\nModel explainability requirements and \n\ndemands; and (b) highlight associated \n\nrisks that might occur in the Product \n\nLifecycle. \n\n16.2.2.  Stakeholder \n\nAppraisal \n\nAnalysis \n\nDocument, analyse and map the \n\noutcomes of the Stakeholder Appraisal \n\nagainst the Explainability Aims and \n\nExplainability Risks. \n\nTo (a) map and analyse Model \n\nexplainability requirements and \n\ndemands in light of the needs of \n\nExplainability Stakeholders; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n16.2.3.  Explainability \n\nMatrix \n\nDocument, assess, and derive a matrix \n\nevaluating and ranking the metrics and/ \n\nor criteria of explanations needed for \n\nbased on the (a) Stakeholder Appraisal \n\nAnalyse, (b) Explainability Aims, (c) \n\nExplainability Risks, and (d) Explainability \n\nRequirements, inclusive of explanations \n\naccuracy, fidelity, consistency, stability, \n\ncomprehensibility, certainty, and \n\nrepresentativeness. \n\nTo (a) derive a clear matrix from \n\nwhich to assess Model explainability \n\nrequirements; and (b) highlight \n\nassociated risks that might occur in the \n\nProduct Lifecycle. \n\n16.2.4.  Explainability \n\nFeature \n\nSelection \n\nDocument and analyse the degree of \n\nFeature explainability needed in light of \n\nthe Explainability Matrix. \n\nTo (a) identify the requisite degree of \n\nFeature explainability needed; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle, such \n\nas later stage Model retraining due to \n\nfeature ambiguity. \n\n16.2.5.  Explainability \n\nModelling \n\nMapping & \n\nAnalysis \n\nDocument and analyse the technical \n\nneeds of Model explainability in light \n\nof the Explainability Matrix, inclusive \n\nof considerations of global vs. local \n\nexplainability and/or pre-modelling \n\nexplainability, modelling explainability, \n\nand post-hoc modelling explainability \n\nTo (a) identify the technical needs \n\nof the Explainability Matrix; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\n16.2.6.  Explanation \n\nFrequency \n\n& Delivery \n\nAssessment \n\nDocument and assess the frequency, \n\nmost suitable and practically reasonable \n\nmethods of communicating Model \n\nexplainability in light of the Explainability \n\nMatrix and Stakeholder Appraisal \n\nAnalysis. \n\nTo (a) identify the most appropriate \n\nmethod of communicating Model \n\nexplainability in order to promote \n\nexplainability comprehension; and (b) \n\nhighlight associated risks that might \n\noccur in the Product Lifecycle. \n\nObjective \n\nTo identify and document Model explainability and transparency requirements, inclusive of Stakeholder needs. \n\n16.2 Exploration Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n61 FBPML Technical Best Practices v1.0.0. \n\n16. Explainability - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n16.3.1.  Explainability \n\nFeature \n\nSelection \n\nAssessment \n\nConduct a Feature analysis of the \n\nExplainability Feature Selection in order to \n\nremove correlated and dependent Features. \n\nTo (a) interrogate the assumption \n\nof zero Feature dependency in \n\nexplainability modelling; (b) prevent \n\nmisleading Model explainability \n\nand transparency; and (c) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.3.2.  Global \n\nExplainability \n\nModel Run \n\nDocument and run as many types of global \n\nexplainability Models as is reasonably \n\npractical, such as Feature importances, \n\nFeature interactions, global surrogate \n\nModels, perturbation-based techniques \n\nor gradient-based techniques. When \n\nthere is doubt about the stability of the \n\ntechniques being used, test their quality \n\nthrough alternative parameterizations or by \n\ncomparing across techniques. \n\nTo (a) generate global explainability \n\nof the model; (b) help promote \n\nmodel debugging; (c) ensure \n\nexplainability fidelity and stability \n\nthrough numerous explainability \n\nmodel runs; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.3.3.  Local \n\nExplainability \n\nModel Run \n\nDocument and run as many types of local \n\nexplainability Models as is reasonably \n\npractical, such as perturbation-based \n\ntechniques or gradient-based techniques \n\nor, for more specific examples, Local \n\nInterpretable Model-Agnostic Explanations \n\n(LIME), SHAP values, Anchor explanations \n\namongst others. When there is doubt \n\nabout the stability of the techniques being \n\nused, test their quality through alternative \n\nparameterizations or by comparing across \n\ntechniques. \n\nTo (a) generate global explainability \n\nof the model; (b) help promote \n\nmodel debugging; (c) ensure \n\nexplainability fidelity and stability \n\nthrough numerous explainability \n\nmodel runs; and (d) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n16.3.4.  Visual \n\nExplanations \n\nAssessment \n\nDevelop visual aids to present and represent \n\nModel explainability and transparency \n\ninsights, such as Tabular Graphics, Partial \n\nDependency Plots, Individual Conditional \n\nExpectations, and/or Accumulated Local \n\nEffects plot. \n\nTo promote explainability \n\ncomprehension. \n\n16.3.5.  Example-based \n\nand Contrastive \n\nExplanations \n\nAssessment \n\nDevelop example-based and contrastive \n\nexplanations to present and represent \n\nModel explainability insights, such as the \n\nunderlying distribution of the data or select \n\nparticular instances. \n\nTo promote explainability \n\ncomprehension, such as of \n\ncomplex data distributions and/ \n\nor datasets for Explainability \n\naudiences. \n\nObjective \n\nTo ensure that Model design represents the explainability requirements and demands of transparency aims as \n\nmuch as is reasonably practical. \n\n16.3 Development Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n62 FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo monitor and track the performance of the explanations and trigger when any of the explainability approaches \n\nneed to be re-trained. \n\n16.4 Production \n\nControl:  Aim: \n\n16.4.1.  Explainability \n\nModel \n\nThresholds \n\nSet clear performance thresholds and \n\nlimitations for explainability Model(s). \n\nTo (a) define parameters for \n\nthe continued suitability and \n\nperformance of explainability \n\nModel(s); and (b) highlight \n\nassociated risks. \n\n16.4.2.  Explainability \n\nModel Review & \n\nMonitoring \n\nPeriodically, or when significant Model \n\nchanges occur, review implemented \n\nexplainability Model(s) in light of \n\nExplainability Model Thresholds. \n\nTo (a) ensure the continued \n\nsuitability and performance of \n\nexplainability Model(s) and their \n\nexplanations; and (b) highlight \n\nassociated risks. \n\n16.4.3.  Explanation \n\nTracking & \n\nMonitoring \n\nDocument and conduct (a) ad-hoc \n\ninterviews, (b) structured surveys, and/or (c) \n\nworkshops with Explainability Stakeholders \n\non explanations provided and adjust \n\noutcomes in Section 14 - Performance \n\nRobustness accordingly. \n\nTo ensure the continued \n\neffectiveness and suitability of \n\nprovided Model explanations.", "fetched_at_utc": "2026-02-08T19:07:57Z", "sha256": "12585dab1747c66e4581210aea88c74799e2f45c82c2f1f703f702da76123171", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-32-62.pdf", "file_size": 945946, "relative_path": "pdfs\\FBPML_TechnicalBP_V1.0.0-32-62.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 20487}}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-63-87-d95e143273c4", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-63-87.pdf", "title": "FBPML_TechnicalBP_V1.0.0-63-87", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n63 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) prevent adversarial actions against, and encourage graceful failures for, Products and/or Models; (b) \n\navert malicious extraction of Models, data and/or intellectual property; (c) prevent Model based physical and/or \n\nirreparable harms; and (d) prevent erosion of trust in Outputs or methods. \n\nWhat do we mean when we refer to Security? \n\nSecurity is broadly defined as the state of being free from danger or threat. Building on this definition, within the \n\ncontext of machine learning -\n\nSecurity refers to the state of ensuring that machine learning Products and/or Models are free from adversarial \n\ndanger, threat or attacks. \n\nAdversarial danger, threat or attacks are understood as the malicious intent to negatively impact machine \n\nlearning Productsâ€™ and/or Modelsâ€™ functionality and/or metrics without organisation consent, whether threatened \n\nor actualised. If an organisation does consent to any such activity, this is - rather - a form of penetration testing \n\nand/or security analysis, as opposed to an adversarial danger, threat or attack. \n\nWhy is Security relevant? \n\nMachine learning Product and/or Model security is imperative to ensure operational robust performance. Without \n\nthe ability to secure the Productâ€™s and/or Modelâ€™s integrity from adversarial danger, threat or attack, malicious \n\nthird parties can use an organisationâ€™s Products and Models to either unlawfully enrich themselves or, more \n\nseriously, cause operational environment harms, including death and/or destruction. These are intolerable risks \n\nas they undermine organisation, societal and machine learning trust and confidence. \n\nHow to apply Security? \n\nIn order to generate thorough and thoughtful security, it must be considered continuously throughout all \n\nstages of the product lifecycle. This means that security must be addressed at the (a) Product Definition(s), (b) \n\nExploration & Development, (c) Production and (d) Confidence & Trust stages of machine learning operations. \n\n# Section 17. Security Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n64 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo identify and control for Adversarial risks and motives based on Product Definition, characterized by adversary \n\ngoals. \n\n17.1 Product Definitions \n\nControl:  Aim: \n\n17.1.1.  Exfiltration \n\nAttacks \n\nDocument and assess whether the data \n\nemployed and gathered by the Product, and \n\nthe intellectual property generated possess \n\nvalue for potential adversarial actors. \n\nTo (a) identify the risks associated \n\nwith (i) Product Subject physical, \n\nfinancial, social and psychological \n\nwellbeing, and (ii) Organization \n\nfinancial wellbeing; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n17.1.2.  Evasion Attacks  Document and assess whether Product \n\nSubjects gain advantage from evading \n\nand/or manipulating the Product Outputs. \n\nDocument and assess whether adversarial \n\nactors stand to gain advantage in \n\nmanipulating Product Subject by evading \n\nand/or manipulating Product Output. \n\nTo (a) identify the risks associated \n\nwith Product Output manipulation \n\nin regard to malicious and \n\nnefarious motives; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n17.1.3.  Targeted \n\nSabotage \n\nDocument and assess whether adversarial \n\nactors can cause harm to specific targeted \n\nProduct Subjects by manipulating Product \n\nOutputs. \n\nDocument and assess whether \n\nadversarial actors can cause \n\nharm to specific targeted Product \n\nSubjects by manipulating Product \n\nOutputs. \n\n17.1.4.  Performance \n\nDegradation \n\nAttack \n\nDocument and assess whether a malicious \n\nperformance degradation for a specific (Sub) \n\npopulation can cause harm to that (Sub) \n\npopulation. Document and assess whether \n\ngeneral performance degradation can cause \n\nharm to society, Product Subjects, the \n\nOrganization, the Domain and/or the field of \n\nMachine Learning. \n\nTo (a) identify the risks in \n\nregard to (i) Product Subjectsâ€™ \n\nphysical, financial, social and \n\npsychological wellbeing, (ii) the \n\nOrganizationâ€™s financial and \n\nreputational wellbeing, (iii) society-\n\nwide environmental, social and \n\neconomic wellbeing, and (iv) the \n\nDomainsâ€™ reputational wellbeing; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n65 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\n17.2.1.  Data Poisoning \n\nAssessment \n\nDocument and assess the ease and extent with \n\nwhich adversarial actors may influence training \n\ndata through manipulating and/or introducing \n\n- (i) raw data; (ii) annotation processes; (iii) \n\nnew data points; (iv) data gathering systems \n\n(like sensors); (v) metadata; and/or (vi) multiple \n\ncomponents thereof simultaneously. If this \n\nconstitutes an elevated risk, document, assess \n\nand implement measurements that can be taken \n\nto detect and/or prevent the above manipulation \n\nof training data. \n\nTo (a) prevent adversarial \n\nactors from seeding \n\nsusceptibility to Evasion \n\nAttacks, Targeted Sabotage \n\nand Performance Degradation \n\nAttacks by way of (i) \n\nintroducing hard to detect \n\ntriggers, (ii) increasing \n\nnoise, and/or (iii) occluding \n\nor otherwise degrading \n\ninformation content; and (b) \n\nhighlight associated risks that \n\nmight occur in the Product \n\nLifecycle. \n\n17.2.2.  Public Datasets  Employ public datasets whose characteristics \n\nand Error Rates are well known as a benchmark \n\nand/or make the Product evaluation results \n\npublic. \n\nTo (a) increase the probability \n\nof detection adversarial \n\nattacks, such as Data \n\nPoisoning, by enabling \n\ncomparison with and by public \n\nresources; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.3.  Data Exfiltration \n\nSusceptibility \n\nDocument and assess the susceptibility of the \n\nModel to data Exfiltration Attacks through - (i) \n\nthe leakage of (parts of) input data through \n\nModel Output; (ii) Model memorization of training \n\ndata that may be exposed through Model output; \n\n(iii) the inclusion by design of (some) training \n\ndata in stored Model artifacts; and/or (iv) \n\nrepeated querying of the Model. \n\nTo (a) warrant and control the \n\nrisk of Model data theft; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.2.4.  Model \n\nExfiltration \n\nSusceptibility \n\nDocument and assess the susceptibility of \n\nModels to Exfiltration Attacks with the aim of \n\nobtaining a copy, or approximation of, the Model \n\nor other Organization intellectual property, \n\nthrough repeated querying of the Model and \n\nanalysing the obtained results and confidence \n\nscores. \n\nTo (a) warrant and control the \n\nrisk of Model and intellectual \n\nproperty theft; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\nObjective \n\nTo identify and control for Adversarial Risks based on and originating in Model properties and/or Model data \n\nproperties. \n\n17.2. Exploration & Development \n\nControl:  Aim: Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n66 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\n17.2.5.  Exfiltration \n\nDefence \n\nTo reduce susceptibility of Exfiltration Attacks, \n\n(a) make Exfiltration Attacks computationally \n\nexpensive; (b) remove as much as possible \n\ninformation from Model Output; (c) add noise \n\nto Model Outputs through techniques such \n\nas differential privacy; (d) limit querying \n\npossibilities in volume and/or scope; and/or (e) \n\nchange Model architecture. \n\nTo (a) warrant and control the \n\nrisk of Exfiltration Attacks; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.2.6.  Adversarial \n\nInput \n\nSusceptibility \n\nDocument and assess the susceptibility \n\nof Models to be effectively influenced by \n\nmanipulated (inferencing) input. Reduce \n\nthis susceptibility by (a) increasing the \n\nrepresentational robustness (f.e. through \n\nmore complete embeddings or latent space \n\nrepresentation); and/or (b) applying robust \n\ntransformations (possibly cryptographic) and \n\ncleaning. \n\nTo (a) warrant the control of the \n\nrisk of Evasion and Sabotage \n\nAttacks, including Adversarial \n\nExamples; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.7.  Filtering \n\nSusceptibility \n\nIf sufficient potential motive has been \n\ndetermined for adversarial attack, document \n\nand assess the specific susceptibility of the \n\npre-processing filtering procedures of Models \n\nbeing evaded by tailored inputs, based on the \n\ninformation available to an adversarial attacker \n\nabout these procedures; in addition to the \n\ngeneral Susceptibility Assessment. Increase the \n\nrobustness of this filtering as far as practically \n\nfeasible. \n\nTo (a) warrant the control of the \n\nrisk of Evasion and Sabotage \n\nAttacks, including Adversarial \n\nExamples; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.8.  Training \n\nSusceptibility \n\nIf sufficient potential motives have been \n\ndetermined for adversarial attack, document \n\nand assess the specific susceptibility of Model \n\ntraining to attack through the manipulation of (a) \n\nthe partitioning of train, validation and test sets, \n\nand/or (b) Modelsâ€™ hyperparameters; in addition \n\nto the general Susceptibility Assessment. \n\nImplement more strict access control on \n\nproduction-grade training and hyperparameter \n\noptimization procedures. \n\nTo (a) warrant the control of \n\nthe risk of Evasion, Sabotage \n\nand Performance Degradation \n\nAttacks; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.9.  Adversarial \n\nExample \n\nSusceptibility \n\nIf sufficient potential motives have been \n\ndetermined for adversarial attack, document and \n\nassess the specific susceptibility of Models to \n\nAdversarial Examples by considering - (a) sparse \n\nor empty regions of the input space, and/or (b) \n\nModel architectures; in addition to the general \n\nSusceptibility Assessment. Document and \n\nimplement specific protective measures, such \n\nas but not limited to adversarial training. \n\nTo (a) warrant the control of \n\nthe risk of Evasion Attacks, \n\nspecifically Adversarial \n\nExamples; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n67 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\n17.2.10.  Adversarial \n\nDefence \n\nIf sufficient potential motive and susceptibility \n\nto adversarial attacks have been determined, \n\nimplement as far as reasonably practical \n\n- (a) data testing methods for detection of \n\noutside influence on input and Output Data; \n\n(b) reproducibility; (c) increase redundancy \n\nby incorporating multimodal input; and/or (d) \n\nperiodic resets or cleaning of Models and data. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.11.  General \n\nSusceptibility -\n\nInformation \n\nDocument, assess and control the general \n\nsusceptibility to attack due to information \n\nobtainable by attackers, by considering (a) \n\nsensitivity to input noise and/or noise as \n\na protective measure; (b) the amount of \n\ninformation an adversarial actor may obtain \n\nfrom over-extensive logging; and/or (c) whether \n\nproviding confidence scores as Output is \n\nbeneficial to adversarial actors. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.12.  General \n\nSusceptibility -\n\nExploitability \n\nDocument, assess and control the general \n\nModel susceptibility to attack due to exploitable \n\nproperties of Models, considering (a) overfit \n\nor highly sensitivity Models and Model \n\nhyperparameters are easier to attack; (b) an \n\nover-reliance on gradient methods that make \n\nModels more predictable and inspectable; (c) \n\nModels may be pushed past their applicability \n\nboundaries if input is not validated; and (d) non-\n\nrandom random number generators might be \n\nreplaced by cryptographically secure random \n\nnumber generators. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.13.  General \n\nSusceptibility -\n\nDetection \n\nDocument, assess and control the capability to \n\ndetect attacks through the ability to understand \n\nwhen Model behaviour is anomalous by (a) \n\ndecreasing Model opaqueness, and/or (b) \n\nincreasing Model robustness. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. \n\n17.2.14.  Open Source \n\nand Transfer \n\nLearning \n\nVulnerability \n\nDocument the correspondence between \n\npotential attack motives and attack \n\nsusceptibility posed by using, re-using or \n\nemploying for transfer learning open source \n\nModels, Model weights, and/or Model parameters \n\nthrough - (a) maliciously inserted behaviour and/ \n\nor code (â€œtrojansâ€), (b) the ability of an adversarial \n\nactor to investigate and attack open source \n\nModels unhindered; and (c) improper (re-)use. \n\nConsider using non-open source Models or \n\nmaking significant changes aimed at reducing \n\nsusceptibility. \n\nTo (a) warrant and control the \n\nrisk of Adversarial Attacks \n\nin general; and (b) highlight \n\nassociated risks that might \n\noccur in the Product Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n68 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo identify and control for Adversarial Risks based on and/or originating in Models production. \n\n17.3 Production \n\nControl:  Aim: \n\n17.3.1.  IT Security  Traditional IT security practices are referred \n\nto. Areas of particular importance to ML-\n\nbased systems include - (a) backdoor access \n\nto the Product, in particular the components \n\nvulnerable to attack risk as identified in \n\nother controls; (b) remote host servers \n\nvulnerability; (c) hardened and isolated \n\nsystems; (d) malicious insiders (e)man-in-\n\nthe-middle attacks; and/or (f) denial-of-\n\nservice. \n\nTraditional IT security practices \n\nare referred to. Areas of particular \n\nimportance to ML-based systems \n\ninclude - (a) backdoor access \n\nto the Product, in particular \n\nthe components vulnerable to \n\nattack risk as identified in other \n\ncontrols; (b) remote host servers \n\nvulnerability; (c) hardened and \n\nisolated systems; (d) malicious \n\ninsiders (e)man-in-the-middle \n\nattacks; and/or (f) denial-of-\n\nservice. \n\n17.3.2.  Periodic Review \n\nand Validation \n\nIf motive and risk for Adversarial Attack is \n\nhigh, perform more stringent and frequent \n\nreview and validation activities. \n\nTo (a) warrant and control the risk \n\nof Adversarial Attacks in general \n\nby increasing detection probability \n\nand fixing vulnerabilities quickly; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.3.3.  input and Output \n\nVulnerability \n\nDocument and assess the vulnerability of \n\nthe Product and related systems to direct \n\nmanipulation of inputs and Outputs. \n\nDirect Output manipulation if possible is the \n\nmost straightforward, simplest, cheapest \n\nand hardest to detect attack \n\nTo (a) create redundancy with input \n\nand inferencing hyperparameter \n\nsusceptibility; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\n17.3.4.  Defense \n\nStrength \n\nAssessment \n\nDocument and assess the strength and \n\nweaknesses of all layers of defense against \n\nattacks and identify the weakest links. \n\nTo (a) build defense in depth; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n69 \n\n17. Security - FBPML Technical Best Practices v1.0.0. \n\nControl:  Aim: \n\n17.4.1.  Trust Erosion  Document and assess the potential impact \n\non trust from adversarial and defacement \n\nattacks, and establish a strategy to mitigate \n\ntrust erosion in case of successful attacks. \n\nTo (a) prevent erosion of trust in \n\nProduct Outputs, the Product, the \n\nOrganization, and/or Domains from \n\npreventing beneficial Products \n\nand technologies to be employed; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.4.2.  Confidence  Document and assess the degree of over-\n\nand under-confidence in the Product output \n\nby Product Team, Stakeholder(s) and End \n\nUsers. Encourage an appropriate level of \n\nconfidence through education and self-\n\nreflection. \n\nNote: Underconfidence will lead to users \n\nover-ruling the Product in unexpected ways. \n\nOverconfidence leads to lower scrutiny and \n\ntherefore lowers the chance of detection \n\nand prevention of attacks. \n\nTo (a) balance the risk of \n\ncompromising Product effects \n\nagainst reduced vigilance; and \n\n(b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n17.4.3.  Warning Fatigue  Document and assess the frequency of \n\nwarnings and alerts provided to Product \n\noperators, maintainers, and Product \n\nSubjects, and refine the thresholds and \n\nprocesses such that no over-exposure to \n\nalerts can lead to systematic ignoring of \n\nalerts. \n\nTo (a) prevent an overexposure \n\nto alerts that can lead to ignoring \n\nserious defects and incidents, \n\ncausing harm; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nObjective \n\nTo identify and control for Adversarial Risks based on and/or originating in Product trust and confidence. \n\n17.4 Confidence & Trust Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n70 \n\nFBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo (a) prevent Model-based physical and/or irreparable harms; and (b) identify and mitigate risks due to Product \n\nfailures, including Model failures, IT failures, and process failures. \n\nWhat do we mean when we refer to Safety? \n\nWhen referring to safety in the context of machine learning we mean-\n\nSafety means the protection of the operational environment - and its subjects - from negative physical and/or \n\nother harms that might result from machine learning Products and/or Models, either directly or indirectly. \n\nPut slightly differently, when we discuss safety we are not talking about the safety of machine learning Products \n\nand Models (called, rather, security), but, instead, the operational environment within which machine learning \n\nProducts and Models operate. Specifically, the harms and risks that machine learning Products and Models might \n\ncause for these environments and their subjects. For example, an autonomous vehicle crashing and causing \n\ninjury, death, or destruction. \n\nWhy is Safety important? \n\nMachine learning Product and Model safety is imperative to ensure the integrity of the operational environment. \n\nWithout such safety, machine learning Products and Models can cause grave operational environment harms, \n\nsuch as physical injury or, at worst, death. These are intolerable risks as they undermine organisation, societal \n\nand machine learning trust and confidence. Moreover, they cause irreparable real damage in the real world. \n\nHow to apply Safety? \n\nIn order to generate thorough and thoughtful safety, it must be considered continuously throughout all stages of \n\nthe product lifecycle. This means that safety must be addressed at the (a) Product Definition(s), (b) Exploration, \n\n(c) Development and (d) Production stages of machine learning operations. \n\n# Section 18. Safety Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n71 \n\n18. Safety - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo establish the appropriate safety-oriented attitudes based on first principles and Product Definitions. \n\nObjective \n\nTo start the process of identifying,specifying and controlling (potential) risks and failures modes of the Model(s) \n\nand Product based on research and exploration, and sustain this process throughout the Product Lifecycle. \n\n18.1 Product Definitions \n\n18.2 Exploration \n\nControl:  Aim: \n\n18.1.1.  Physical and \n\nIrreparable \n\nHarm Risk \n\nDocument and assess whether any \n\nlikely failure modes can cause physical \n\nand/or irreparable harm, based on the \n\nProduct Definitions. If such is the case, \n\nwarrant increased oversight and attention \n\nthroughout the Product Lifecycle to risks \n\nand controls in general and from this section \n\nin particular. \n\nTo warrant the necessary amount of \n\ncontrol and resources throughout \n\nthe Product Lifecycle with regard \n\nto preventing and mitigating \n\nsignificant threats to individualsâ€™ \n\nphysical, financial, social, and \n\npsychological well being. \n\n18.1.2.  Domain-first \n\nHumble Culture \n\nDocument and establish tenents for Product \n\nTeam culture to promote risk awareness \n\nand prevent blind spots, inclusive of (a) put \n\nDomain expertise central; (b) never assume \n\nonly positive effects; (c) admit uncertainty \n\nwhen assessing impacts. \n\nTo promote risk awareness and \n\nprevent blindspots in analysing \n\nfailure modes and other safety \n\nrelated controls and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nControl:  Aim: \n\n18.2.1.  Forecast Failure \n\nModes \n\n(a) Document and assess continuously \n\nthroughout the Product Lifecycle all \n\npotential failure modes that can be \n\nidentified through - (i) researching past \n\nfailures; and/or (ii) interrogating all \n\ncomponents or product and context with \n\nan open mind; (b) rank identified failure \n\nmodes according to likelihood and severity; \n\n(c) prepare for and mitigate these risks as \n\nfar as is reasonably practical in order of risk \n\nthroughout the Product Lifecycle. \n\nTo (a) reduce harmful \n\nconsequences of failures through \n\nanticipation and preparation; \n\nand (b) highlight associated risks \n\nthat might occur in the Product \n\nLifecycle. \n\n18.2.2.  Prediction \n\nLimits \n\nDocument and assess with a diversity of \n\nStakeholders the real limitations on the \n\nProduct and Model Outcomes that ought be \n\nstrictly enforced in order to prevent physical \n\nand/or irreparable harm and/or other Failure \n\nModes. \n\nTo prevent Model and Product \n\nOutcomes from violating clear, \n\nfixed and safe operating bounds. \n\n18.2.3.  Surprise Diary  Document continuously throughout the \n\nProduct Lifecycle all surprising findings and \n\noccurrences. \n\nTo discover and subsequently \n\ncontrol for previously unknown or \n\nunanticipated failures modes. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n72 \n\n18. Safety - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo control Safety risks and failure modes based on testing and controlling Model and Product technical \n\ncomponents. \n\nObjective \n\nTo control for Safety risks and failure modes and prevent physical and/or irreparable harm by performing \n\nassessments and implementing measures at the systemic and organisational level. \n\n18.3 Development \n\n18.4 Production \n\nControl:  Aim: \n\n18.3.1.  General \n\nIT Testing \n\nPractices \n\nAdhere to all traditional IT/Software Testing \n\nbest practices. \n\nTo warrant and control the risk of \n\nfailures due to code, software and \n\nother IT mistakes in general. \n\n18.3.2.  Testing by \n\nDomain Experts \n\nDocument and perform testing of the \n\nModel(s) and Product by Domain experts. \n\nTo warrant that Product and \n\nProduct Safety are tested against \n\nthe most relevant requirements and \n\nprevent blind spots caused by lack \n\nof multidisciplinarity. \n\n18.3.3.  Algorithm \n\nBenchmarking \n\nDocument and perform benchmark testing \n\nof Models and Model code against well-\n\nknown, trusted and/or simpler Models/code. \n\nTo warrant the correct \n\nimplementation of Models and \n\ncode, and safeguard reproducibility \n\nin general. \n\nControl:  Aim: \n\n18.4.1.  System Failure \n\nPropagation \n\nDocument and assess how failures in \n\nModels and Product components propagate \n\nto other components and other systems, \n\nand what damage they may cause there. \n\nIncorporate such information in Failure \n\nMode risk assessments and implementation \n\nof Graceful Failures and Kill Switches. \n\nTo (a) prevent blind spots and \n\ncascading failures and (b) provide \n\nessential input for creating \n\nmitigation measures with a \n\nminimum of uncontrolled side-\n\neffects. \n\n18.4.2.  Graceful Failure  Document and assess whether (i) Model \n\nerrors, (ii) Model failures, (iii) Product \n\nfailures, (iv) IT failures, and/or (v) process \n\nand implementation failures - whether \n\ncaused by attack or not - can result in \n\nphysical or irreparable harm to humans, \n\nsociety and/or the environment. If present, \n\nmitigate these risks by implementing \n\ntechnological and/or process measures that \n\nmake these failures graceful. \n\nTo identify risks and mitigating \n\nmeasures throughout the Product \n\nLifecycle with regard to significant \n\nthreats to individualsâ€™ physical, \n\nfinancial, social and psychological \n\nwellbeing. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n73 \n\n18. Safety - FBPML Technical Best Practices v1.0.0. \n\n18.4.3.  Kill Switch  Document and implement Kill Switches \n\naccording to the findings of all previous \n\ncontrols, taking into account (a) instructions \n\nand procedures for engaging the Kill Switch; \n\n(b) who is/are responsible for engaging the \n\nKill Switch; (c) what impacts the engagement \n\nof the Kill Switch has on users, other parts of \n\nthe Product and other systems. \n\nDocument and implement Kill \n\nSwitches according to the findings \n\nof all previous controls, taking \n\ninto account (a) instructions and \n\nprocedures for engaging the Kill \n\nSwitch; (b) who is/are responsible \n\nfor engaging the Kill Switch; (c) \n\nwhat impacts the engagement of \n\nthe Kill Switch has on users, other \n\nparts of the Product and other \n\nsystems. \n\n18.4.4.  Safety Stress \n\nTesting \n\nDocument and perform scenario-based \n\nstress testing of Product in Domain, Society \n\nand Environmental contexts, for realistic but \n\nrare high-impact scenarios, recording the \n\nProductâ€™s reaction to and influence on the \n\nDomain, Society and Environment. \n\nTo control and prepare for worst-\n\ncase scenarios in the context \n\nof rapid and/or large changes \n\nin Domain, in Society or in \n\nEnvironment. \n\n18.4.5.  Product Incident \n\nResponse \n\nDocument and prepare Product-specific \n\nimplementation of the Organisation Incident \n\nResponse Plan insofar as that does not cover \n\nthe Productâ€™s specific risks, if appropriate. \n\nTo (a) control for and contain \n\nProduct Incidents; (b) minimize \n\nharms stemming from Product \n\nIncidents; (c) repair harms caused \n\nby Product Incidents; and (d) \n\nincorporate lessons learned. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n74 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure (a) building desirable solutions; (b) human control over Products and Models; and (c) that individuals \n\naffected by Product and Model outputs can obtain redress. \n\nWhat is Human-centric Design? \n\nHuman-centric (or also called human-centered) Design is a creative approach to problem-solving, by involving \n\nthe human perspective in all steps of problem solving. \n\nIn the context of machine learning and the Model framework, Human-centric Design makes you stay focused on \n\nthe user when designing with ML, therefore building desirable solutions for your target users. Moreover, it also \n\nensures that the right stakeholders are involved throughout the whole design and development process and helps \n\nto properly identify the right opportunity areas. Lastly, human-centric design encompasses the extent to which \n\nhumans have control over the Model and its output as well as the degree to which humans can obtain redress, if \n\nthey are affected by the Model. \n\nWhy Human-centric Design? \n\nIncorporating Human-centric Design in the Product is vital. It ensures the Model is not built in isolation but \n\nis integrated with other problems and, most of all, that it helps solve the right questions. Having the right \n\nStakeholders (beyond the technical teams) involved in the whole Model lifecycle translates to higher trust levels \n\nin the Model, increases the rate of adoption, as well as results in more human-friendly and creative solutions. Not \n\nhaving the human-centric part of the Model will inevitably result in an inferior Model - and one which very likely \n\nend up on a â€˜shelfâ€™ and, therefore, not be applied in practice. \n\nHow to ensure Human-centric Design? \n\nHuman-centric Design is something that needs to be addressed throughout the product lifecycle, not only in the \n\nearly stages of it, and not in any stage in isolation. \n\n# Section 19. Human-Centred Design Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n75 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo discover and gain insight so that the Product and Model(s) will solve the right problems, designed for human \n\nneeds and values, before building it. \n\nObjective \n\nTo (a) cluster, (b) find insights and (c) define the right opportunity area, ensuring to focus on the right questions to \n\nsolve in preparation for the development and production phase. \n\n19.1 Product Definitions \n\n19.2 Exploration \n\nControl:  Aim: \n\n19.1.1.  Human Centered \n\nMachine \n\nLearning \n\nIncorporate the human (non-technical) \n\nperspective in your (technical) process of \n\nexploration, development and production \n\nby applying user research, design thinking, \n\nprototyping and rapid feedback, and human \n\nfactors when defining a usable product or \n\nmodel. \n\nTo (a) ensure that Product(s) and \n\nModel(s) are not only feasible and \n\nviable, but also align with a human \n\nneeds; and (b) highlight associated \n\nrisks failing such. \n\n19.1.2.  UX (or user) \n\nresearch \n\nFocus on understanding user behaviours, \n\nneeds, and motivations through observation \n\ntechniques, task analysis, user interviews, \n\nand other research methodologies. \n\nTo prevent (a) a focus on technology \n\nfrom overshadowing a focus on \n\nproblem solving; and (b) cognitive \n\nbiases from adverse influence \n\nProduct and Model design. \n\n19.1.3.  Design for \n\nHuman values \n\nInclude activities for (a) the identification \n\nof societal values, (b) deciding on a moral \n\ndeliberation approach (e.g. through \n\nalgorithms, user control or regulation), and \n\n(c) methods to link values to formal system \n\nrequirements (e.g. value sensitive design \n\n(VSD) mapping). \n\nTo reflect societal concerns about \n\nthe ethics of AI, and ensure that AI \n\nsystems are developed responsibly, \n\nincorporating social, ethical values \n\nand ensuring that systems will \n\nuphold human values. The moral \n\nquality of a technology depends on \n\nits consequences. \n\nControl:  Aim: \n\n19.2.1.  Design Thinking  Ensure an iterative development process by \n\n(a) empathize: research your usersâ€™ needs, \n\n(b) define: state your usersâ€™ most important \n\nneeds and problems to solve, (c) ideate: \n\nchallenge assumptions and create ideas, (d) \n\nprototype: start to create solutions and (e) \n\ntest: gather user feedback early and often. \n\nTo let data scientists organise and \n\nstrategise their next steps in the \n\nexploratory phase. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n76 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\n19.2.2.  Ethical \n\nassessment \n\nDiscuss with your team to what extend \n\n(a) the AI product actively or passively \n\ndiscriminates against groups of people \n\nin a harmful way; (b) everyone involved in \n\nthe development and use of the AI product \n\nunderstands, accepts and is able to \n\nexercise their rights and responsibilities; \n\n(c) the intended users of an AI product \n\ncan meaningfully understand the purpose \n\nof the product, how it works, and (where \n\napplicable) how specific decisions were \n\nmade. \n\nDiscuss with your team to what \n\nextend (a) the AI product actively \n\nor passively discriminates against \n\ngroups of people in a harmful \n\nway; (b) everyone involved in the \n\ndevelopment and use of the AI \n\nproduct understands, accepts \n\nand is able to exercise their \n\nrights and responsibilities; (c) the \n\nintended users of an AI product \n\ncan meaningfully understand the \n\npurpose of the product, how it \n\nworks, and (where applicable) how \n\nspecific decisions were made. \n\n19.2.3.  Estimating the \n\nvalue vs effort \n\nof possible \n\nopportunity \n\nareas \n\nExplore the details of what mental Models \n\nand expectations people might bring when \n\ninteracting with an ML system as well as \n\nwhat data would be needed for that system. \n\nE.g. an Impact Matrix. \n\nTo reveal the automatic \n\nassumptions people will bring to an \n\nML-powered product, to be used \n\nas prompts for a product team \n\ndiscussion or as stimuli in user \n\nresearch. (See also Section 4.11 -\n\nUser Experience Mapping.) \n\nObjective \n\nTo (a) ensure rapid iteration and targeted feedback from relevant Stakeholders, allowing a larger range of \n\npossible solutions to be considered in the selection process. (b) Increase the creativity and options considered, \n\nwhile avoiding avoiding personal biases and/or pigeon-holing a solution. \n\n19.3 Development \n\nControl:  Aim: \n\n19.3.1.  Prototyping  1: Focus on quick and minimum viable \n\nprototypes that offer enough tangibility \n\nto find out whether they solve the initial \n\nproblem or answer the initial question. \n\nDocument how test participants react and \n\nwhat assumptions they make when they \n\nâ€œuseâ€ your mockup. \n\n2: Design a so-called â€˜Wizard of Ozâ€™ test; have \n\nparticipants interact with what they believe \n\nto be an autonomous system, but which is \n\nactually being controlled by a human (usually \n\na team member) \n\nTo gain early feedback (without \n\nhaving to actually have build an \n\nML product) needed to (a) adjust \n\nor pivot your Products(s) and/or \n\nModel(s) thus ensuring business \n\nviability; and/or (b) assess the cost \n\nand benefits of potential features \n\nwith more validity than using \n\ndummy examples or conceptual \n\ndescriptions. \n\n19.3.2.  Cost weighing of \n\nfalse positives \n\nand false \n\nnegatives \n\nWhile all errors are equal to an ML system, \n\nnot all errors are equal to all people. Discuss \n\nwith your team how mistakes of your ML \n\nmodel might affect the userâ€™s experience of \n\nthe product. \n\nto avoid sensitive decisions being \n\ntaken (a) autonomously; or (b) \n\nwithout human consideration. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n77 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\n19.3.3.  Visual \n\nStorytelling \n\nFocus on explanatory analysis over \n\nexploratory analysis, taking the mental \n\nmodels of your target audience in account. \n\nTo avoid uninformed decisions \n\nabout your product or model by \n\nnon-technical stakeholders, when \n\npresenting complex analysis, \n\nmodels, and findings. \n\n19.3.4.  Preventative \n\nProcess Design \n\nDocument and assess whether high-risk \n\nand/or high-impact Model (sub)problems \n\nor dilemmas that are present in the Product \n\n(as determined from following the Best \n\nPractices Framework) can be mitigated or \n\navoided by applying non-Model process \n\nand implementation solutions. If non-Model \n\nsolutions are not applied, document the \n\nreasons for this, document the sustained \n\npresence of these risks and implement \n\nappropriate incident response measures. \n\nTo (a) prevent high-risk and/or \n\nhigh-impact Model (sub)problems \n\nor dilemmas through non-Model \n\nprocess and implementation \n\nsolutions; and (b) highlight \n\nassociated risks that might occur in \n\nthe Product Lifecycle. \n\nObjective \n\nTo ensure (a) delivering a user-friendly product, (b) increasing the adoption rate of your product or model, \n\nfocussing on (dis-)trust as main fundamental risk of ML models with (non-technical) end users \n\n19.4 Production \n\nControl:  Aim: \n\n19.4.1.  Trust; increased \n\nby design \n\nAllow for users to develop systems \n\nheuristics (ease of use) via design patterns \n\nwhile at the same time facilitate a detailed \n\nunderstanding to those who value the \n\nâ€˜intelligentâ€™ technology used. (See Section \n\n19.4.2 -Design for Human Error; Section \n\n19.4.3 - Algorithmic transparency; and \n\nSection 19.4.4 - Progressive disclosure for \n\nfurther information.) \n\nTo avoid (a) that the user does not \n\ntrust the outcome, and will act \n\ncounter to the design, causing at \n\nbest inefficiencies and at worst \n\nserious harms, or (b) that -trusting \n\nan application will do what we think \n\nit will do- an user can confirm their \n\ntrust is justified. \n\n19.4.2.  Design for \n\nHuman Error \n\n(a) Understand the causes of error and \n\ndesign to minimise those causes; (b) \n\nDo sensibility checks. Does the action \n\npass the â€œcommon senseâ€ test (e.g. is the \n\nnumber is correct? - 10.000g or 10.000kg) \n\n(c) Make it possible to reverse actions - to \n\nâ€œundoâ€ them - or make it harder to do what \n\ncannot be reversed (eg. add constraints \n\nto block errors - either change the color \n\nto red or mention â€œDo you want to delete \n\nthis file? Are you sure?â€). (d) make it easier \n\nfor people to discover the errors that do \n\noccur, and make them easier to correct \n\nTo (a) increase trust between the \n\nend user and the model; (b) minimize \n\nthe opportunities for errors while \n\nalso mitigating the consequences. \n\nIncrease the trust users have with \n\nyour product by design for deliberate \n\nmis-use of your model (making your \n\nmodel or product â€œidiot-proofâ€) so \n\nusers are (a) able to insert data to \n\ncompare the model outcome with \n\ntheir own expected outcome which \n\nwill increase their trust, or (b) users \n\nable to test the limitations of your \n\nproduct or model -via fake or highly \n\nunlikely data- without breaking your \n\nproduct or model. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n78 \n\n19. Human-Centred Design - FBPML Technical Best Practices v1.0.0. \n\n19.4.3.  Algorithmic \n\ntransparency \n\nAssess the appropriate system heuristics \n\n(eg. ease of use), document all factors that \n\ninfluence the algorithmic decisions, and \n\nuse them as a design tool to make them \n\nvisible, or transparent, to users who use or \n\nare affected by the ML systems. \n\nTo (a) increase trust between the end \n\nuser and the model; (b) increase end-\n\nuser control; (c) improve acceptance \n\nrate of tool; (d) promote user \n\nlearning with complex data; and (e) \n\nenable oversight by developers. \n\n19.4.4.  Progressive \n\ndisclosure \n\nAt the point where the end-user interacts \n\nwith the Product outcomes, show them \n\nonly the initial features and/or information \n\nnecessary at that point in the interaction \n\n(thus initially hiding more advanced \n\ninterface controls). Show the secondary \n\nfeatures and/or information only when the \n\nuser requests it (show less, provide more-\n\nprinciple). \n\nTo greatly reduce unwanted \n\ncomplexity for the end-user and thus \n\npreventing (a) end-user non-adoption \n\nor misunderstanding and (b) ensuring \n\nan increased feeling of trust by the \n\nusers. \n\n19.4.5.  Human in the \n\nloop (HITL) \n\nEmbed human interaction with machine \n\nlearning systems to be able to label \n\nor correct inaccuracies in machine \n\npredictions. \n\nTo avoid the risk of the Product \n\napplying a materially detrimental or \n\ncatastrophic Product Outcome to \n\na Product Subject without human \n\nintervention. \n\n19.4.6.  Remediation  Document, assess and implement in \n\nthe Model(s), Product and Organization \n\nprocesses, requirements for enabling \n\nProduct Subjects to challenge and obtain \n\nredress for Product Outcomes applied to \n\nthem. \n\nTo ensure detrimental Product \n\nOutcomes are easily reverted when \n\nappropriate. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n79 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo prevent (in)direct adverse social and environmental effects as a consequence of interactions amongst \n\nProducts, Models, the Organisation, and the Public. \n\nWhat is Systemic Stability? \n\nModel stability is a relatively popular notion. It is usually centered at putting a bound at the Modelâ€™s generalization \n\nerror. \n\nSystemic Stability refers to the robustness of the Model (or lack thereof) stemming from the interaction between \n\nthe Model, Organization, environment and the broader public (society at large). \n\nThere are numerous potential risks that can emerge in this interaction. Many of them can impact the stability of \n\nthe Model - beyond the context of traditional performance robustness or deterioration of the Model over time. \n\nAnother way to think of it is as the extent to which the Model and/or its building blocks are susceptible to chain \n\neffects and self-reinforcing interactions between the Model, Organization, environment and society. \n\nWhy Systemic stability? \n\nSystemic stability forces one to think beyond the traditional definitions of Model stability and its potential \n\ncauses and consequences. Systemic stability ensures that we consider the effect on the Model and society \n\ndue to the interaction between the Model, the Organization, the environment and society. This means thinking \n\nabout susceptibility to feedback loops, self-fulfilling prophecies and how either of them may impact the data \n\nor the Model and its output. It, therefore, reduces risks related to deteriorated performance and minimises the \n\npropagation of undesirable biases. \n\nHow to ensure Systemic stability? \n\nIn order to ensure systemic stability, it must be considered continuously throughout all stages of the product \n\nlifecycle. This means that systemic stability must be addressed at the (a) Product Definition(s), (b) Exploration, (c) \n\nDevelopment and (d) Production stages of machine learning operations. \n\n# Section 20. System Stability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n80 \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct Definition(s). \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct exploration. \n\n20.1 Product Definitions \n\n20.2 Exploration \n\nControl:  Aim: \n\n20.1.1.  Product \n\nAssumption \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in invalidating \n\nProduct Assumptions. If so, attempt to \n\nredefine Product Assumptions to warrant \n\ntheir longevity. \n\nTo (a) prevent unpredictable social \n\nand/or environmental behaviour \n\nthrough Product Outcomes; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. \n\nControl:  Aim: \n\n20.2.1.  Selection \n\nFunction \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in invalidating \n\nProduct Assumptions. If so, attempt to \n\nredefine Product Assumptions to warrant \n\ntheir longevity. \n\nTo (a) prevent unpredictable social \n\nand/or environmental behaviour \n\nthrough Product Outcomes; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.2.2.  Data Definition \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes \n\nto the Selection Function, and whether \n\nthis is a self-reinforcing interaction. If \n\ntrue, attempt to mitigate or stabilize \n\nassociated effects through refining \n\nProduct Definition(s) and/or improving \n\nModel design and/or Product and process \n\nimplementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n81 \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. \n\n20.2.3.  Data Generating \n\nProcess \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes to \n\nthe Product data definitions, and whether \n\nthis is a self-reinforcing interaction. If \n\ntrue, attempt to mitigate or stabilize \n\nassociated effects through refining \n\nProduct Definition(s) and/or improving \n\nModel design and/or Product and process \n\nimplementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.2.4.  Data \n\nDistributions \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes to \n\nthe data generating process, and whether \n\nthis is a self-reinforcing interaction. If \n\ntrue, attempt to mitigate or stabilize \n\nassociated effects through refining \n\nProduct Definition(s) and/or improving \n\nModel design and/or Product and process \n\nimplementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.2.5.  Hidden Variable \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in the creation \n\nof new hidden Variables in the system. If \n\ntrue, record the new Variable during data \n\ngathering, or prevent the creation of the \n\nnew Variable through improved Product \n\nDefinition(s) and implementation. \n\nTo (a) determine and prevent Product \n\nand/or Model risk in - (i) progressively \n\nstrengthening biases (from encoded \n\nassumptions and definitions to \n\ndatasets to algorithms chosen); (ii) \n\nprogressively reinforcing Model errors \n\nand/or Product generalizations; (iii) \n\nprogressively losing sensitivity to \n\ndata and/or Domain changes; (iv) \n\nsuffering from self-reinforcing and/ \n\nor exponential run-away effects; \n\n(b) determine and prevent risks of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n82 \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct development. \n\n20.3 Development \n\nControl:  Aim: \n\n20.3.1.  Target Feature \n\nDefinition \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs will result in changes to the \n\nTarget Feature definition. If true, attempt to \n\nmitigate associated effects through refining \n\nProduct Output and/or Model design and/or \n\ndevelopment. \n\nTo (a) determine and prevent risk of \n\nunpredictable behaviour once the \n\nProduct outcomes are applied; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.3.2.  Optimization \n\nFeedback Loop \n\nSusceptibility \n\nDocument and assess whether the cost \n\nfunction and/or optimization algorithm \n\nexhibits a feedback loop behaviour that \n\nincludes the gathering of data that has been \n\ninfluenced by previous Model iterations, and \n\nwhether this behaviour is self-reinforcing \n\nor self-limiting. If true, attempt to mitigate \n\nassociated effects through refining \n\nProduct Output and/or Model design and/or \n\ndevelopment. \n\nIdem Section 20.2.1- Selection \n\nFunction Susceptibility \n\nObjective \n\nTo investigate and mitigate unforeseen social and environmental chain effects and/or risks caused through \n\nProduct application. \n\n20.4 Production \n\nControl:  Aim: \n\n20.4.1.  Self-fulfilling \n\nProphecies \n\nDocument and assess whether applying \n\nProduct Outputs will result in change to \n\nProduct inputs, dependencies and/or \n\nDomain(s) (other than those mentioned in \n\ncontrols elsewhere) and whether this is a \n\nself-reinforcing interaction. If true, attempt \n\nto mitigate associated effects through \n\nrefining Product Output and/or Model design \n\nand/or development. \n\nIdem Section 20.2.1- Selection \n\nFunction Susceptibility \n\n20.4.2.  Hidden Variable \n\nDependencies \n\nDocument and assess whether the effect \n\nof applying Product Outputs depends \n\non Hidden Variables. If true, control for \n\nHidden Variables, for example through \n\nmarginalization and/or by deriving indicators \n\nfor Hidden Variables influence. \n\nTo (a) prevent diverging effects on \n\nseemingly similar individuals or \n\ndatapoints; (b) prevent or detect \n\nhigh-risk interactions; and (c) \n\nhighlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n83 \n\n20.4.3.  Society \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs results in potentially \n\nharmful societal or environmental changes, \n\nand research the possible knock-on effects \n\nas far as reasonably practical. \n\nTo (a) identify and prevent both \n\ndirect and indirect adverse effects \n\non society and the environment; \n\n(b) determine if there is a risk of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.4.4.  Domain \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs results in changes to \n\napplication Domain(s), and research \n\nthe possible knock-on effects as far as \n\nreasonably practical. \n\nTo (a) identify and prevent both \n\ndirect and indirect adverse \n\neffects on Product Domain(s); \n\n(b) determine if there is a risk of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20.4.5.  Other \n\nOrganisation \n\nProducts \n\nSusceptibility \n\nDocument and assess whether applying \n\nProduct Outputs result in changes to inputs, \n\ndependencies and/or context for other \n\nOrganisation Products. If true, attempt to \n\nmitigate associated effects through refining \n\nProduct Output and/or Model design and/or \n\ndevelopment. \n\nTo (a) identify and prevent both \n\ndirect and indirect adverse \n\neffects on the Organisation or \n\nother Organisation Products; (b) \n\ndetermine if there is a risk of \n\nunpredictable behaviour once the \n\nProduct Outcomes are applied; and \n\n(c) highlight associated risks in the \n\nProduct Lifecycle. \n\n20. System Stability - FBPML Technical Best Practices v1.0.0. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n84 FBPML Technical Best Practices v1.0.0. \n\nObjective: \n\nTo ensure the clear and complete Traceability of Products, Models and their assets (inclusive of, amongst other \n\nthings, data, code, artifacts, output, and documentation) for as long as is reasonably practical. \n\nWhat do we mean when we refer to Product Traceability? \n\nProduct Traceability refers to the ability to identify, track and trace elements of the Product as it is designed, \n\ndeveloped, and implemented. \n\nAlternatively put, Product Traceability, is the ability to trace and track all Product elements and decisions \n\nthroughout the Product Lifecyle. It is the identification, indexing, storage, and management of each unique \n\nProduct element. \n\nWhy is Product Traceability relevant? \n\nThrough Product Traceability, each element of the Product can be easily identified and, thereafter, re-examined, \n\nand amended. This allows for greater Product accountability and transparency as, through this process, each \n\nProduct element and its developers can be identified. \n\nHow to apply Product Traceability? \n\nIn order to generate thorough and thoughtful Product Traceability, it must be considered continuously throughout \n\nall stages of the Product Lifecycle. This means that Product Traceability must be addressed at the (a) Product \n\nDefinition(s), (b) Exploration, (c) Development and (d) Production stages of Machine Learning Operations. \n\n# Section 21. Product Traceability Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n85 \n\n21. Product Traceability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo document and maintain an overview of the requirements necessary to complete the Product and the \n\ninterdependencies in the Product design phase. \n\nObjective \n\nTo document the impact analysis of each requirement. \n\n21.1 Product Definitions \n\n21.2 Exploration \n\nControl:  Aim: \n\n21.3.1.  Document \n\nStorage \n\nDefine a single fixed storage solution for all \n\nreports, documents, and other traceability \n\nfiles. \n\nTo (a) prevent the usage and \n\ndissemination of outdated and/ \n\nor incorrect files; (b) prevent the \n\nhaphazards storage of Product \n\nreports, documents and/or files; \n\nand (c) highlight associated risks in \n\nthe Product Lifecycle. \n\n21.3.2.  Version Control \n\nof Documents \n\nEnsure that document changes are tracked \n\nwhen changes are made. Subsequent \n\nversions ought to list version number, \n\nauthor, date of change, and short \n\ndescription of the changes made. \n\nTo (a) track changes to any and all \n\ndocuments; (b) ensure everyone \n\nis using the same and latest \n\ndocument version; and (c) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.3.  Architectural \n\nRequirements \n\nDocument \n\nDocument which information technology \n\nresources are necessary for each element \n\nof the Product to provide a necessary \n\noverview of system requirements and \n\ncost distribution. Document the reasons \n\neach resource was chosen along with \n\njustifications. \n\nTo (a) provide clear documentation \n\nof which system resources are \n\nused, where they are used, why \n\nthey are used, and costs; and (b) \n\nhighlight associated risks in the \n\nProduct Lifecycle. \n\nControl:  Aim: \n\n21.2.1.  Document \n\nImpact Analysis \n\nof Requirements \n\nDocument and complete an impact analysis \n\non the resources and design of the Product \n\nthat can result in technical debt. \n\nTo (a) avoid Product failures due \n\nto unresolved technical debt by \n\ndocumenting potential sources of \n\nfriction and the solutions; and (b) \n\nhighlight associated risks in the \n\nProduct Lifecycle. \n\n21.2.2.  Resource \n\nTraceability \n\nMatrix \n\nProvide and keep up to date a clear view of \n\nthe relationships and interdependencies \n\nbetween resources in a documented matrix. \n\nTo (a) document and show resource \n\ncoverage for each use case; and \n\n(b) highlight associated risks in the \n\nProduct Lifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n86 \n\n21. Product Traceability - FBPML Technical Best Practices v1.0.0. \n\n21.2.3.  Design \n\nTraceability \n\nMatrix \n\nProvide and keep up to date a clear view of \n\nthe relationships and interdependencies \n\nbetween designs and interactions thereof in \n\na documented matrix. \n\nTo (a) document design and \n\nexecution status; (b) clearly trace \n\ncurrent work and what can be \n\npursued next; and (c) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.2.4.  Results \n\nReproducibility \n\nLogs \n\nThroughout the entire Product Lifecycle, \n\nwhenever a Product component - inclusive \n\nof Models, experiments, analyses, \n\ntransformation, and evaluations - are run, all \n\nparameters, hyperparameters and results \n\nought to be logged and/or tracked, including \n\nunique identifier(s) for runs, artifacts, code \n\nand environments. \n\nTo (a) enable Absolute \n\nReproducibility; (b) validate Models \n\nand Outcomes through enablement \n\nof analysis of logs, run comparisons \n\nand reproducibility. \n\nObjective \n\nTo document and maintain the status of each product and the testing results. Ensure 100% test coverage. \n\nPrevent inconsistencies between project elements and prevent feature creep. \n\n21.3 Development \n\nControl:  Aim: \n\n21.3.1.  Backlog  Ensure that an effective backlog is \n\nmaintained to track work items and serve as \n\na historical representation and timeline of \n\ncompleted features and velocity. \n\nTo (a) ensure a comprehensive \n\nbreakdown of Features and \n\ntasks necessary to achieve full \n\nproduct functionality; (b) provide \n\nhighly readable coarse-grained \n\nversioning; and (c) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.2.  Documentation \n\nfor Technical \n\nContributors \n\nMaintain technical documentation that \n\nenables all current and future contributors \n\nto efficaciously and safely develop and \n\nmaintain the Product, including such \n\ninformation as description of each file, the \n\nworkflow, author, environments, accrued \n\ntechnical debt. \n\nTo (a) maintain Product technical \n\nintegrity by ensuring safe \n\ncontribution and maintenance \n\npractices; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.3.  Version Control \n\nof Code \n\nMaintain uninterrupted version control \n\nsystems and practices of all code used by, in \n\nand during the Product and its Lifecycle. \n\nTo (a) maintain Product technical \n\nintegrity by ensuring safe \n\ncontribution and maintenance \n\npractices; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n87 \n\n21.3.4.  Docstrings and \n\nCode Comments \n\nDocument in each function the author of \n\ncode, purpose of code, input, Output, and \n\nimprovements to be made. Document the \n\nsource of inputs and potentially a short \n\nbusiness description of data used. \n\nTo (a) ensure Model clarity as to \n\ntechnical progress; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21.3.5.  Project Status \n\nReports \n\nEnsure that all status reports and similar \n\ncommunications to Management and \n\nStakeholders are stored and maintained, \n\ninclusive of team updates, reports to the \n\nProduct Manager, and Stakeholder reports \n\nby request. \n\nTo (a) maintain a formal written \n\nrecord of decisions, progress and \n\ncontext evolution; and (b) highlight \n\nassociated risks in the Product \n\nLifecycle. \n\n21. Product Traceability - FBPML Technical Best Practices v1.0.0. \n\nObjective \n\nTo document the observed impact of updates to the product. Document product runs and their input for \n\nreproducibility. \n\n21.4 Production \n\nControl:  Aim: \n\n21.4.1.  Version control \n\nthrough CI/CD \n\nMaintain distinct production versions \n\nto easily revert or roll back to a working \n\nprevious Product, if production issues arise. \n\nProperly set up CI/CD enables easy redeploy \n\nof any artifact and version. \n\nTo (a) provide functional Product \n\nto users at all times; (b) seamlessly \n\nredeploy Product versions if \n\nneeded; and (c) highlight associated \n\nrisks in the Product Lifecycle. \n\n21.4.2.  Data Lineage \n\nManifest \n\nUtilise a data lake for production data, \n\nintermediate results, and end results. Each \n\nstep should be documented in a manifest \n\nthat is passed from one step of the process \n\nto the next and always accompanies stored \n\ndata and results. \n\nTo (a) create a structured way for \n\ntracing where data has been, what \n\nwas done to it, and results; and (b) \n\nhighlight associated risks in the \n\nProduct Lifecycle.", "fetched_at_utc": "2026-02-08T19:08:01Z", "sha256": "d95e143273c46729361d8e462f63055d85030182d8f6e0d3a1fc96c16703e2b4", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-63-87.pdf", "file_size": 1007448, "relative_path": "pdfs\\FBPML_TechnicalBP_V1.0.0-63-87.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 14691}}}
{"doc_id": "pdf-pdfs-fbpml-technicalbp-v1-0-0-7-9-a2d2073e84c2", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\FBPML_TechnicalBP_V1.0.0-7-9.pdf", "title": "FBPML_TechnicalBP_V1.0.0-7-9", "text": "Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n7FBPML Technical Best Practices v1.0.0. \n\n1.1.  Absolute Reproducibility  means a guarantee that any and all results, outputs, outcomes, artifacts, \n\netc can be exactly reproduced under any circumstances. \n\n1.2.  Best Practice Guideline  means this document. \n\n1.3.  Confidence Value  means a measure of a Modelâ€™s self-reported certainty that the given Output \n\nis correct. \n\n1.4.  Data Generating \n\nProcess \n\nmeans the process, through physical and digital means, by which Records \n\nof data are created (usually representing events, objects or persons). \n\n1.5.  Data Science  means an interdisciplinary field that uses scientific methods, processes, \n\nalgorithms and computational systems to extract knowledge and insights \n\nfrom structural and/or unstructured data. \n\n1.6.  Domain  means the societal and/or commercial environment within which the \n\nProduct will be and/or is operationalised. \n\n1.7.  Edge Case  means an outlier in the space of both input Features and Model Outputs. \n\n1.8.  Error Rate  means the frequency of occurrence of errors in the (Sub)population \n\nrelative to the size of the (Sub)population \n\n1.9.  Evaluation Error  means the difference between the ground truth and a Modelâ€™s prediction or \n\noutput. \n\n1.10.  Fairness & Non-\n\nDiscrimination \n\nmeans the property of Models and Model outcomes to be free from bias \n\nagainst Protected Classes. \n\n1.11.  Features  mean the different attributes of datapoints as recorded in the data. \n\n1.12.  Hidden Variable  means an attribute of a datapoint or an attribute of a system that \n\nhas a causal relation to other attributes, but is itself not measured or \n\nunmeasurable. \n\n1.13.  Human-Centric Design \n\n& Redress \n\nmeans orienting Products and/or Models to focus on humans and their \n\nenvironments through promoting human and/or environment centric \n\nvalues and allowing for redress. \n\n1.14.  Implementation  means every aspect of the Product and Model(s) insertion of and/or \n\napplication to Organisation systems, infrastructure, processes and culture \n\nand Domains and Society. \n\n1.15.  Incident  means the occurrence of a technical event that affects the integrity of a \n\nProduct and/or Model. \n\n1.16.  Label  means the Feature that represents the (supposed) ground-truth values \n\ncorresponding to the Target Variable. \n\nAs used in this Best Practice Guideline, the following terms shall have the following meanings where capitalised. \n\nAll references to the singular shall include references to the plural, where applicable, and vice versa. Any terms \n\nnot defined or capitalised in this Best Practice Guideline shall hold their plain text meaning as cited in English and \n\ndata science. \n\n# Section 1. Definitions Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n8FBPML Technical Best Practices v1.0.0. \n\n1. Definitions - FBPML Technical Best Practices v1.0.0. \n\n1.17.  Machine Learning  means the use and development of computer systems and Models that \n\nare able to learn and adapt with minimal explicit human instructions by \n\nusing algorithms and statistical modelling to analyse, draw inferences, and \n\nderive outputs from data. \n\n1.18.  Model  means Machine Learning algorithms and data processing designed, \n\ndeveloped, trained and implemented to achieve set outputs, inclusive of \n\ndatasets used for said purposes unless otherwise stated. \n\n1.19.  Organisation  means the concerned juristic entity designing, developing and/or \n\nimplementing Machine Learning. \n\n1.20.  Outcome  means the resultant effect of applying Models and/or Products. \n\n1.21.  Output  means that which Models produce, typically (but not exclusively) \n\npredictions or decisions. \n\n1.22.  Performance \n\nRobustness \n\nmeans the propensity of Products and/or Models to retain their desired \n\nperformance over diverse and wide operational conditions. \n\n1.23.  Product  means the collective and broad process of design, development, \n\nimplementation and operationalisation of Models, and associated \n\nprocesses, to execute and achieve Product Definition(s), inclusive of, \n\namongst other things, the integration of such operations and/or Models \n\ninto organisation products, software and/or systems. \n\n1.24.  Product Manager  means either a Design Owner and/or Run Owner as identified in the \n\nOrganisation Best Practice Guideline in Sections 3.1.4. & 3.1.7. respectively. \n\n1.25.  Product Team  means the collective group of Organisation employees directly charged \n\nwith designing, developing and/or implementing the Product. \n\n1.26.  Product Subjects  means the entities and/or objects that are represented as data points in \n\ndatasets and/or Models, and who may be the subject of Product and/or \n\nModel outcomes. \n\n1.27.  Project Lifecycle  means the collective phases of Products from initiation to termination \n\n- such as design, exploration, experimentation, development, \n\nimplementation, operationalisation, and decommissioning - and their \n\nmutual iterations. \n\n1.28.  Protected Classes  mean (Sub)populations of Product Subjects, typically persons, that are \n\nprotected by law, regulation, policy or based on Product Definition(s) \n\n1.29.  Root Cause Analysis  means the activity and/or report of the investigation into the primary \n\ncausal reasons for the existence of some behaviour (usually an error or \n\ndeviation). \n\n1.30.  Safety  means real Product Domain based physical harms that result through \n\nProducts and/or Models applications. \n\n1.31.  Security  means the resilience of Products and/or Models against malicious and/ \n\nor negligent activities that result in Organisational loss of control over \n\nconcerned Products and/or Models. Notice: This document and its content has been licensed under the  Creative Commons Attribution license  by the  Foundation (Stichting) for Best Practices in Machine Learning  (kvk number: \n\n> 82610363). Any subsequent and/or other use, copying and/or adaptation of this document or its content must abide by the appropriate licensing terms & conditions as reflected thereunder.\n\n9FBPML Technical Best Practices v1.0.0. \n\n1. Definitions - FBPML Technical Best Practices v1.0.0. \n\n1.32.  Selection Function  means a (where possible mathematical) description of the probability or \n\nproportion of all real Subjects that might potentially be recorded in the \n\ndataset that are actually recorded in a dataset. \n\n1.33.  Stakeholders  mean the department(s) and/or team(s) within the Organisation who do \n\nnot conduct data science and/or technical Machine Learning, but have a \n\nmaterial interest in Product Machine Learning. \n\n1.34.  (Sub)population  means any group of persons, animals, or any other entities represented \n\nby a piece of data , that is part of a larger (potential) dataset and \n\ncharacterized by any (combination of) attributes. The importance of (Sub) \n\npopulations is particularly high when some (Sub)populations are vulnerable \n\nor protected (Protected Classes). \n\n1.35.  Systemic Stability  means the stability of Organisation, Domain, society and environment as a \n\ncollective ecosystem. \n\n1.36.  Target of Interest  means the fundamental concept that the Product is truly interested in \n\nwhen all is said and done, even if it is something that is not (objectively) \n\nmeasureable. \n\n1.37.  Target Variable  means the Variable which a Model is made to predict and/or output. \n\n1.38.  Traceability  means the ability to trace, recount, and reproduce Product outcomes, \n\nreports, intermediate products, and other artifacts, inclusive of Models, \n\ndatasets and codebases. \n\n1.39.  Variables  mean the different attributes of subjects or systems which may or may not \n\nbe measured.", "fetched_at_utc": "2026-02-08T19:08:03Z", "sha256": "a2d2073e84c287ca7fc7f721b93d8bf9a5d57a19d9762573a085ef82c292fe7a", "meta": {"file_name": "FBPML_TechnicalBP_V1.0.0-7-9.pdf", "file_size": 115002, "relative_path": "pdfs\\FBPML_TechnicalBP_V1.0.0-7-9.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 1809}}}
{"doc_id": "pdf-pdfs-general-purpose-ai-model-compliance-guide-part-1-oliver-patel-8739a1b95698", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel.pdf", "title": "General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1) \n\nhttps://oliverpatel.substack.com/p/general-purpose-ai-model-compliance  2/22 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! \n\nFollow me on LinkedIn for more frequent updates. It has been a consequential few weeks for the EU AI Act's provisions on general-purpose AI (GPAI) models. Now that we've all had some time to (hopefully) take a deep breath, relax, and take stock, it is the ideal moment for a deep-dive on GPAI model compliance. The next three editions of Enterprise AI Governance will provide a comprehensive, practical, and actionable GPAI Model Compliance Guide. It is targeted at AI governance, legal, and compliance practitioners working for firms that develop, fine-tune, modify, deploy, or otherwise use GPAI models. That is quickly encompassing all of us.   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 3/22\n\nMy goal is to simplify these complex obligations and highlight what they mean for your business. \n\nNote: this series assumes familiarity with the EU AI Act, its core concepts, and the topic of general-purpose AI and foundation models more broadly. \n\nPart 1's focus (this edition) is on Obligations for Providers of GPAI Models . It covers: \n\nâœ… What is a GPAI model? \n\nâœ… What do providers of GPAI models need to do? \n\nâœ… What about open-source GPAI models? \n\nâœ… What about legacy GPAI models (released before 2 August 2025)? \n\nâœ… How does the GPAI Code of Practice fit in? \n\nâœ… How and when will the GPAI model provisions be enforced? And here is a glimpse of what's coming in parts 2 and 3. Part 2 will focus on GPAI Models with Systemic Risk , covering: \n\nâœ… What is a GPAI model with systemic risk? \n\nâœ… How are such models classified and what are the exemptions? \n\nâœ… What do providers of GPAI models with systemic risk need to do?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 4/22\n\nâœ… How do the compliance obligations differ? \n\nâœ… Deep dive on the GPAI Code of Practice: Safety and Security Chapter And Part 3 wraps up the series with the question on everyone's lips: am I a GPAI model provider!? The focus here will be on 'Downstream Actors': Modification, Deployment, and Use of GPAI Models :\n\nâœ… Who exactly is a provider of a GPAI model? \n\nâœ… How do you become the provider of a GPAI model you are modifying? \n\nâœ… What is a 'downstream provider' and how do you become one? \n\nâœ… What is a general-purpose AI system? \n\nâœ… What if you integrate a GPAI model into a high-risk AI system? \n\nâœ… How should you assess GPAI model providers and choose which models to use? By the end of the series, you will have a thorough understanding of precisely what your organisation needs to do to achieve compliance. My analysis represents a simplified breakdown of the following official legal and regulatory documents, which are referenced throughout. \n\nEU AI Act full text European Commission Guidelines for Providers of GPAI Models   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 5/22\n\nGPAI Code of Practice \n\nTransparency Chapter Model Documentation Form Copyright Chapter Safety and Security Chapter \n\nTemplate for publishing GPAI Model Training Data Summary \n\nThe aforementioned Code of Practice, Guidelines, and Training Data Template were all published in July 2025. And the Code of Practice was formally approved by the EU on 1 August 2025, one day before the GPAI model provisions became applicable on 2 August 2025. Whether or not you are a provider of a GPAI model, and thus subject to the below obligations, depends on whether or not the model you are developing and placing on the market is in fact a GPAI model. The AI Act defines a GPAI model as: \n\nWhat is a GPAI model?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 6/22\n\n\"An AI model, including where such an AI model is trained with a large amount of data using self-supervision at scale , that displays significant generality and is capable of competently performing a wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated into a variety of downstream systems or applications , except AI models that are used for research, development or prototyping activities before they are placed on the market\" .The corresponding AI Act recital says that models with at least 1 billion parameters, trained in this way, should be considered to be GPAI models. In its recent guidelines, the European Commission acknowledged that the above definition is fairly general and lacks specific, objective criteria that organisations can use to determine whether or not their models constitute GPAI. Plugging this gap, the Commission has now provided specific criteria for GPAI model classification. This criteria is based on the amount of computational resource used to train the model. Specifically, if a model's training compute is greater than 10^23 floating-point operations (FLOP) and it can generate language (either text or audio), or generate image or video based on text inputs, then it should be considered a GPAI model.   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 7/22\n\nThe Commission's position is that there is a direct correlation between how much computational resource is used to train the modelâ€”which in itself is linked to the size of the model and the volume of training and pre-training dataâ€”and the general-purpose capabilities of the model. However, it is acknowledged that there could be exceptional instances where a model trained with this amount of computational resource that generates text would not meet the legal definition of a GPAI model (e.g., if it did not display significant generality in its capabilities). Also, a model which does not meet this indicative criteria may also meet the legal definition of a GPAI model. The key message is that organisations developing and fine-tuning large AI models at scale need to track the computational resource they use in a standardised and automated way. Providers of GPAI models are organisations or entities that develop such AI models and place them on the market or otherwise make them available. \n\nWhat do providers of GPAI models need to do?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 8/22\n\nPart 3 of this series will cover exactly who is a provider of a GPAI model and how to know if you are one. For now, here are some examples the European Commission provides that constitute a provider placing a GPAI model on the market. In all instances, company A is the GPAI model provider. Company A develops a GPAI model and makes it available via a software library, repository, API, or cloud computing service. Company A commissions company B to develop a GPAI model on its behalf, and company A makes it available via a software library, repository, API, or cloud computing service. Company A develops a GPAI model and uploads it to an online repository hosted and managed by company C. There are a suite of obligations that apply to providers of all GPAI models, as well as additional obligations that also apply to providers of GPAI models with systemic risk. Part 2 of this series will cover, in detail, the obligations for providers of GPAI models with systemic risk. They are left out here. Below is a summary of the four main sets of obligations that apply to providers of all \n\nGPAI models: \n\n1. Providers of GPAI models must develop, maintain, and keep up-to-date   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 9/22\n\ncomprehensive technical documentation which provides information about the GPAI model, how it was developed, and how it should be used. The Transparency Chapter of the GPAI Code of Practice provides extensive detail about how this can be done. It is accompanied by a Model Documentation Form consisting of 42 metadata attributes across 8 categories. This builds on the elements set out in Annex XI and XII of the AI Act. The attributes include training time and computation, model size, energy consumption, data collection and curation methods, and input and output modalities. Some of this information must be shared with the European Commission's AI Office and/or national regulators (on request), and some must be proactively shared with 'downstream providers', which are organisations that integrate GPAI models into an AI system. Some documentation is required for all GPAI models, whereas certain additional documentation is only required for GPAI models with systemic risk. \n\n2. Providers of GPAI models must produce and make publicly available a detailed summary of the content and data used to train the GPAI model. \n\nThe AI Act requires this training data summary to be \"sufficiently detailed\" and structured in line with a template provided by the AI Office. That template was   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 10/22\n\npublished by the European Commission in July 2025, alongside explanatory guidance. The template carefully attempts to balance protecting trade secrets and confidentiality with promoting transparency, data protection, and copyright protection. In a nutshell, the training data summary should cover the amount and type of training data used, as well as the use of: publicly available datasets; private non-publicly available datasets; data crawled and scraped from online sources; user data; and synthetic data. Importantly, the template does not require providers to publish or disclose details about the specific data and works used to train GPAI models, as this would \"go beyond the requirement to provide a 'summary'\" .\n\n3. Providers of GPAI models must implement a policy to comply with EU copyright and intellectual property law.   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 11/22\n\nGiven the large volume of copyright protected material typically used to train GPAI models, this obligation is significant. It compels providers to consider and document how they will comply with EU copyright law in such contexts. The AI Act provides minimal detail about how to do this. However, it does stipulate that \"state-of-the-art technologies\" must be used to promote copyright compliance in the context of developing GPAI models. The Copyright Chapter of the GPAI Code of Practice provides more detail, outlining six practical measures for organisations to implement. \n\n4. Providers of GPAI models must cooperate with the European Commission and regulatory authorities and appoint an EU-based authorised representative if they are based outside of the EU. \n\nThe AI Act has extraterritorial effect. In this context, it means that if a provider places their GPAI model on the market or makes it available in the EU, then that provider must comply with the AI Act, irrespective of where the provider is established or operating from. In such extraterritorial scenarios, the appointment of an authorised representative is important, to enable effective cooperation and correspondence with the AI Office and any other relevant authorities.   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 12/22\n\nThe AI Act exempts providers of open-source GPAI models from the first and fourth set of obligations. Namely, they do not need to develop, maintain, and keep up-to-date comprehensive technical documentation referenced in point 1 above. And they do not need to appoint an authorised representative (if based outside the EU), as referenced in point 4. However, providers of open-source GPAI models are obliged to comply with points 2 and 3. This means they must implement a copyright compliance policy and publish the training data summary. Furthermore, providers of open-source GPAI models with systemic risk are not exempt from any of these obligations. This means that they must comply with the obligations for both all GPAI models and the additional obligations for GPAI models with systemic risk. The European Commission's guidelines provide clarity about what exactly constitutes an open-source GPAI modelâ€”a hotly contested topic. Here is a summary of the recent guidance: \n\nWhat about open-source GPAI models?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 13/22\n\nTo qualify as open-source, a GPAI model must be released under a free and open-source licence that permits unrestricted access, use, modification, and distribution without payment or significant limitations. All model parameters, architecture, and usage information must be publicly available to ensure usability. A licence is not considered open-source if it includes restrictions like non-commercial use only, prohibitions on distribution, usage limits based on scale, or requirements for separate commercial licences. Essentially, most usage restrictions, aside from those justifiably designed to protect users or other groups, or ensure appropriate attribution, would likely disqualify a GPAI model from open-source classification. Monetisation generally also disqualifies a model from open-source exemptions if payment is required for core access, use, or essential, indistinguishably linked support, or if it's exclusively hosted on paid platforms. However, optional paid services that don't hinder free use are permissible. \n\nWhat about 'legacy' GPAI models (released before 2 August 2025)?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 14/22\n\nIrrespective of when a GPAI model was developed and placed on the market, providers of that model must eventually comply with the AI Act's obligations. This means that the full suite of obligations, including publishing training data summaries, implementing a copyright compliance policy, and maintaining technical documentation, applies to both 'legacy' and 'new' GPAI models. The only difference is the applicable date of those obligations. For the 'legacy' GPAI models, placed on the market before 2 August 2025, providers have until 2 August 2027 to comply with the aforementioned provisions. For GPAI models released after 2 August 2025, there is no longer a grace period. The practical implication of this is that virtually all GPAI models that have already been released and made available in the EU will have to retrospectively become compliant. This represents a practical and operational headache for AI developers, especially given how recently the Code of Practice and guidelines about how to comply were released. The European Commission acknowledges that this is going to be challenging, noting that \"the AI Office is dedicated to supporting providers in taking the necessary steps to comply with their obligations by 2 August 2027\" .However, the Commission confirmed that providers of 'legacy' GPAI models will not have to conduct \"retraining or unlearning of models\" where this is not possible or   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 15/22\n\nwhere it would cause undue burden. The Commission accepts that information about historical training data may not always be available and encourages providers to be open about this. The GPAI Code of Practice was published by the European Commission on 10 July 2025 and was approved by the EU on 1 August 2025. The AI Act mandated the Commission to work with external experts to develop the Code of Practice, to assist GPAI model providers in their compliance journey. It is a voluntary tool which helps providers comply with the full suite of obligations for GPAI models. Although organisations are not obliged to sign and implement the Code of Practice, doing so provides a standardised and endorsed approach for regulatory compliance. This offers more legal certainty regarding the validity of an organisation's approach. However, you can still be compliant, via adequate alternative means, even if you do not sign up to and follow the Code of Practice. \n\nHow does the GPAI Code of Practice fit in?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 16/22\n\nThe Code of Practice consists of three chapters: 1) Transparency, 2) Copyright, and 3) Safety and Security. Each chapter contains several 'measures'. These are practical steps and actions that signatory organisations commit to taking and implementing to achieve GPAI model compliance. Organisations are able to sign up to specific chapters, or the entire Code. Thus far, 26 organisations have signed up in full and xAI (Grok's creator) has signed up to the Safety and Security Chapter only. Meta announced that it will not sign up. The Transparency Chapter focuses on the technical documentation which GPAI model providers are obliged to produce, maintain, and share with key stakeholders. It includes the 'Model Documentation Form' template, which providers can use to ensure they gather and document all the information which they are required to maintain and potentially share with the AI Office, national regulators, and downstream providers. The Copyright Chapter focuses on how organisations can comply with the obligation to implement an EU copyright compliance policy. This includes measures for compliant web crawling and scraping and mitigating the risk of copyright-infringing outputs via technical guardrails. Finally, the Safety and Security Chapter focuses exclusively on the obligations for providers of GPAI models with systemic risk. It consists of ten commitments, all relating to GPAI model risk identification, management, mitigation, treatment, monitoring,   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 17/22\n\nownership, and accountability. The European Commission strongly encourages providers to sign the Code of Practice, noting its various benefits and calling it a \"straightforward way of demonstrating compliance\" . The Commission even indicates that it will be more trusting of signatory organisations, due to their transparent approach. It noted that providers who do not sign up may be \"subject to a larger number of requests for information and requests for access to conduct model evaluations throughout the entire model lifecycle because the AI Office will have less of an understanding of how they are ensuring compliance with their obligations\" .In future, the GPAI Code of Practice may be superseded by harmonised technical standards. These remain under development. The most important thing to understand is that the obligations relating to providers of GPAI models are enforced by the European Commission's AI Office, not member state regulators. This is in contrast to the rest of the AI Act's provisions. With respect to enforcement actions, the AI Office has the following powers: \n\nHow and when will the GPAI model provisions be enforced?   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 18/22\n\nRequest information from providers. Conduct evaluations of GPAI models. Request the implementation of measures and mitigations. Recall GPAI models from the market. Impose financial penalties of up to 3% of global annual turnover or â‚¬15m (whichever is higher). Because the AI Office is charged with enforcement, the European Commission's guidelines on GPAI models carry significant weight, even though they are not legally binding. In those guidelines, the Commission explained that it expects to forge close working relationships with GPAI model providers. It encourages \"close informal cooperation with providers during the training of their GPAI models\" , as well as \"proactive reporting by providers of GPAI models with systemic risk\" . Given the various notification, reporting, and transparency obligations, there will be a lot of back and forth between providers and the AI Office. For those that signed the Code of Practice, the AI Office's focus will be on monitoring their adherence to its measures. Expect slightly more leniency and goodwill. For those that opted not to, the focus will be on in-depth assessments and investigations as to how they are nonetheless compliant.   \n\n> 1/3/26, 5:22 PM General-Purpose AI Model Compliance Guide (Part 1)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance 19/22\n\nIn terms of timelines for compliance and enforcement, the obligations for providers of GPAI models became applicable on 2 August 2025. This means that organisations are legally obliged to be compliant today. However, this is only for GPAI models that were placed on the market from 2 August 2025 onwards. As discussed above, for â€˜legacyâ€™ GPAI models placed on the market before then (i.e., the vast majority of GPAI models available today), providers have until 2 August 2027 to comply. Although the obligations have been applicable since 2 August 2025, the European Commission, including its AI Office, does not have any enforcement powers until 2August 2026 . This means there will be no enforcement proceedings or fines for at least one year from now. However, this does not change the fact that organisations are obliged to work on compliance from today. Any critical gaps in the first year may be taken into account in future enforcement actions or rulings. Irrespective of the provision, and whether or not enforcement is managed by the AI Office or member state regulators, the Court of Justice of the European Union (CJEU) always has the final say on AI Act interpretation. Over the years, it will be interesting to monitor and learn from the inevitable GPAI model case law that will pile up. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.", "fetched_at_utc": "2026-02-08T19:08:05Z", "sha256": "8739a1b95698e309a5954196a355fe3350ff51e3731d281c6d3173b04e6e889c", "meta": {"file_name": "General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel.pdf", "file_size": 755365, "relative_path": "pdfs\\General-Purpose AI Model Compliance Guide - Part 1 - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4887}}}
{"doc_id": "pdf-pdfs-general-purpose-ai-model-compliance-guide-part-2-oliver-patel-4fda32651a33", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel.pdf", "title": "General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel, author and creator of Enterprise AI Governance .\n\n1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2) \n\nhttps://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46  2/24 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! \n\nFollow me on LinkedIn for more frequent updates. Welcome to Part 2 of the General-Purpose AI (GPAI) Model Compliance Guide. This 3-part series is posted exclusively on Enterprise AI Governance. To all subscribers and new readers, thanks for supporting the newsletter! This weekâ€™s edition provides a comprehensive yet accessible overview of the EU AI Actâ€™s provisions for GPAI Models with Systemic Risk .You will learn: \n\nâœ… What is a GPAI model with systemic risk? \n\nâœ… What are the notification and exception procedures for providers of GPAI models   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 3/24\n\nwith systemic risk? \n\nâœ… What are the compliance obligations for providers of GPAI models with systemic risk? \n\nâœ… What exactly is a â€œsystemic riskâ€? \n\nâœ… Deep dive on the GPAI Code of Practice: Safety and Security Chapter \n\nIf you havenâ€™t read Part 1, you should check it out here and read it first. It provides a detailed breakdown of the Obligations for Providers of GPAI Models , including what the core obligations for all GPAI models are, how and when these obligations will be enforced by the AI Office, as well as specific considerations for open-source models and legacy GPAI models (released before 2 August 2025). All this is essential background information that is necessary to fully understand the compliance implications for GPAI models with systemic risk. Part 3, coming next week, will cover the knotty issue of 'Downstream Actors': Modification, Deployment, and Use of GPAI Models . This will address the important question of who exactly is a provider of a GPAI modelâ€”it could be you! \n\nNote: this series assumes familiarity with the EU AI Act, its core concepts, and the topic of general-purpose AI and foundation models more broadly. Also, I was not involved in the multi-stakeholder process of drafting and developing the EUâ€™s GPAI Code of Practice. Finally, none of this should be taken as legal advice. Always consult a legal professional.   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 4/24\n\nThe following official sources have been used to create this guide: \n\nEU AI Act full text European Commission Guidelines for Providers of GPAI Models GPAI Code of Practice \n\nTransparency Chapter Model Documentation Form Copyright Chapter Safety and Security Chapter \n\nTemplate for publishing GPAI Model Training Data Summary \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nThere are two broad categories of GPAI models that the AI Act regulates. These are: \n\nWhat is a GPAI model with systemic risk?   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 5/24\n\n1. GPAI models \n\n2. GPAI models with systemic risk As explained in Part 1 of this series, GPAI models â€œare trained with a large amount of data using self-supervision at scale [â€¦ ] display significant generality and are capable of performing a wide range of distinct tasksâ€. The European Commissionâ€™s recent guidelines also clarify that if an AI modelâ€™s training compute exceeds 10^23 floating-point operations, and it can generate language (either text or audio), or generate image or video based on text inputs, then it should be considered a GPAI model. An AI model is classified as a GPAI model with systemic risk if it meets the above definition and criteria and also has â€œ high impact capabilities â€. The AI Act defines this as capabilities that â€œ match or exceed the capabilities recorded in the most advanced GPAI models â€. If the cumulative amount of computation used to train a GPAI model exceeds 10^25 floating-point operations, then it is, by default, presumed to have â€œhigh impact capabilitiesâ€ and thus classified as a GPAI model with systemic risk. By relying on this compute threshold, the EUâ€™s position is that there is a direct correlation between how much computational resource is used to train an AI model and both the general-purpose capabilities of the model and the level of risk that it poses.   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 6/24\n\nThis means that the larger an AI model is (e.g., in terms of number of parameters) and the more data that is used to train it (e.g., in terms of different examples), the more likely it is to be classified as a GPAI model with systemic risk. Whilst the European Commission acknowledges that â€œ training compute is an imperfect proxy for generality and capabilities â€, it argues that it is â€œ the most suitable approach at presentâ€ .However, the European Commission is empowered to amend this compute threshold, or introduce an entirely new indicator, via a delegated act. This means it can do so independently, without reopening and amending the AI Act itself. However, the European Parliament and the Council (i.e., the member states) have a right to object to any such changes. Interestingly, it is possible for a GPAI model to be classified as a GPAI model with systemic risk even if it does not meet the 10^25 floating-point operations compute threshold. This would require the European Commission to determine that it nonetheless has high impact capabilities, despite the lower cumulative amount of compute used to train it. In making such a decisionâ€”that would prove controversial due to the impact on the impacted providerâ€”the European Commission would consider factors like the size of the model, input and output modalities, benchmark and evaluation results, model   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 7/24\n\nautonomy level, and the number of end-users. Ultimately, it would have to prove that its capabilities match or exceed those of the most advanced GPAI models, despite the fact that less compute was used to train it. The key point for enterprises is that they must carefully forecast, measure, track, and record their estimates of the amount of computational resource used to develop, train, modify, and fine-tune GPAI models, in order to determine what compliance obligations they may have to adhere to. When estimating and measuring compute levels, the European Commissionâ€™s guidance is that providers should â€œ as a general rule, account for all compute that contributed or will contribute to the modelâ€™s capabilities â€. This even includes the compute expended to generate synthetic data for training, even if not all the synthetic data was eventually used to train the GPAI model. Once an organisation knows that a GPAI model it has developed (or is in the process of developing) meets the threshold for training compute (which means it is classified \n\nWhat are the notification and exception procedures for providers of GPAI models with systemic risk?   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 8/24\n\nas a GPAI model with systemic risk), it must notify the European Commission of this as soon as possible, and within two weeks at the latest. In some cases, this notification will be required before the overall training process is completed (e.g., if the threshold is exceeded mid-training run). This notification should include both the precise computation amount as well as a detailed explanation of how this has been estimated. In its guidelines, the European Commission recommends that â€œ providers should estimate the cumulative amount of training compute that they will use â€ before the training process begins. If their pre-training estimate surpasses the systemic risk threshold, they should inform the Commission of this. Zooming out, this notification procedure enables the European Commissionâ€™s AI Office to fulfill its role as the regulator overseeing and enforcing the AI Actâ€™s provisions on GPAI models. It will also promote transparency, as the European Commission will publish a list of all GPAI models with systemic risk that are in scope of the AI Act. Finally, it is possible for a provider of a GPAI model that is by default classified as a GPAI model with systemic risk to secure an exception. To do this, the provider must demonstrate that its GPAI model does not have â€œhigh impact capabilitiesâ€ and therefore does not pose systemic risks and should not be classified as such, despite surpassing the cumulative compute for training threshold of 10^25 floating-point operations.   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 9/24\n\nProviders can do this by pointing to evidence like benchmark and evaluation results, especially if these demonstrate a capability gap between their model and the most advanced AI models. It is important to note that providers cannot get out of the GPAI model with systemic risk classification merely by implementing robust controls and safeguards which mitigate the systemic risk. To secure an exception, they must convince the European Commission, with cold, hard evidence, that the model genuinely does not have high impact capabilities. The AI Act describes this as an â€œexceptionalâ€ scenario, requiring European Commission approval. In such instances, the burden of proof will be on the provider. Given the extensive additional compliance obligations for providers of GPAI models with systemic risk (as compared to GPAI models), the question of which GPAI models are and are not classified as posing systemic risk is significant. Precisely what these additional compliance obligations are is explained below. \n\nWhat are the compliance obligations for providers of GPAI models with systemic risk?   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 10/24\n\nPart 1 of this series provides a detailed breakdown of the compliance obligations for providers of GPAI models. In summary, the four core obligations for GPAI models are: \n\n1. Develop, maintain, and keep up-to-date comprehensive technical documentation. \n\n2. Produce and make publicly available a detailed summary of the content and data used to train the GPAI model. \n\n3. Implement a policy to comply with EU copyright and intellectual property law. \n\n4. Cooperate with the European Commission and regulatory authorities and appoint an EU-based authorised representative (if based outside of the EU). Providers of open-source GPAI models are exempt from obligations 1 and 4. This means they still need to publish a training data summary and implement a copyright compliance policy. Providers of GPAI models with systemic risk must comply with all of the above obligations. Also, providers of open-source GPAI models with systemic risk are not exempt from any of the above obligations. This means that sufficiently advanced and capable open-source AI models are treated the same, from an AI Act compliance perspective, as proprietary models. In other words, providers of any GPAI model with systemic risk, that is placed on the market or made available in the EU, irrespective of whether it is open-source, must   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 11/24\n\ncomply with all the above obligations (for providers of GPAI models), as well as the additional obligations for providers of GPAI models with systemic risk. There are four core additional obligations that only apply to providers of GPAI models with systemic risk. These are: \n\n1. Perform model evaluation using state of the art tools and protocols. This includes conducting adversarial testing to enable the identification and mitigation of â€œsystemic risks â€. \n\n2. Assess and mitigate potential systemic risks that may stem from the development, deployment, or use of the GPAI model with systemic risk. \n\n3. Track, document, and report information about serious incidents and any corrective measures to address them. \n\n4. Ensure an â€œadequate level of cybersecurity protectionâ€ for both the GPAI model with systemic risk and the physical infrastructure of the model. These obligations reflect the fact that EU lawmakers deem it both appropriate and necessary for the most advanced and capable foundation models to be subject to rigorous governanceâ€”including stringent safety and security testing and evaluation procedures, the implementation of technical guardrails and safeguards to mitigate risk, continuous monitoring and oversight, and documented accountability and risk ownershipâ€”due to the widespread use of these models and the distinct possibility   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 12/24\n\nthat this use could lead to significant negative impact. However, the text of the AI Act itself does not provide much detail about how to approach and implement the above four obligations. That is why there is a GPAI Code of Practice. The Code provides a detailed, standardised, and step-by-step compliance framework for GPAI model providers. The GPAI Code of Practice: Safety and Security Chapter , which is most relevant for these obligations, is analysed below. But first, we explore the definition of â€œsystemic riskâ€, which is at the heart of these obligations. The overarching purpose of the above obligations is for providers to uncover and mitigate the systemic risks which their most capable and advanced GPAI models pose. This includes reducing the likelihood of these risks materialising and reducing their impact if they do materialise. This raises an important question for GPAI model providers: what exactly is a â€œsystemic riskâ€? \n\nWhat exactly is a â€œsystemic riskâ€?   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 13/24\n\nThe GPAI Code of Practic e builds on the AI Act by providing additional detail regarding precisely how providers should define, identify, and evaluate the systemic risks that their GPAI models could pose. It classifies the following four risks as systemic risks. This means that if any providers that are also signatories identify any of these risks, they must be classified and treated as systemic risks: \n\nChemical, biological, radiological, and nuclear (CBRN) , e.g., a GPAI model that makes it easier or otherwise enables the design, development, and use of CBRN-related weapons or materials. \n\nLoss of control , e.g., a GPAI model that autonomously self-replicates and creates new, more advanced AI models, without human awareness or control. \n\nCyber offence , e.g., a GPAI model that can be used to significantly lower barriers to entry for scaling cyber attacks. \n\nHarmful manipulation , e.g., a GPAI model that targets large populations of people and uses deceptive techniques to promote harmful or destructive behaviour. Recital 110 of the AI Act complements this, by providing an illustrative list of examples of systemic risks:   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 14/24\n\nMajor accidents. Disruptions of critical sectors. Serious consequences to public health and safety. Negative effects of democratic processes. Negative effects on public or economic security. The dissemination of illegal, false, or discriminatory content. More broadly the GPAI Code of Practice clarifies the essential characteristics of a systemic risk, to further enable their identification. This clarification is based on the formal definition of systemic risk provided in the AI Act. The three essential characteristics of a systemic risk are: \n\n1. The risk is directly related to the GPAI modelâ€™s high-impact capabilities. \n\n2. The risk has a significant impact on the EU due to its reach or due to the actual or potential negative impact on public health, safety, public security, fundamental rights, or society as a whole. \n\n3. The impact can spread widely, at scale, through connected AI systems and the AI and industry ecosystem more broadly. The EUâ€™s view is that as model capabilities and model reach increase, so do the potential systemic risks. Recital 110 of the AI Act also highlights that such systemic   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 15/24\n\nrisks can arise due to various factors and causes, including (but not limited to): Model misuse Model reliability Model fairness Model security Model autonomy level Tool access Model modalities Release and distribution mechanisms Potential to remove model guardrails The detailed information provided in the AI Act and the Code of Practice is sufficient to enable providers of GPAI models with systemic risk to fulfil their obligation of identifying, evaluating, and mitigating the specific systemic risks their GPAI models may pose. \n\nDeep dive on the \n\nGPAI Code of Practice: Safety and Security Chapter   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 16/24\n\nThe final section of this article summarises and analyses the key elements of the GPAI Code of Practice: Safety and Security Chapter. \n\nThis Chapter focuses exclusively on the specific obligations for providers of GPAI models with systemic risk. It provides a comprehensive and standardised set of commitments and measures that signatory organisations will implement, in order to adhere to the four compliance obligations for GPAI models with systemic risk (detailed above). For a dedicated explainer on the GPAI Code of Practice, check out this previous post \n\non Enterprise AI Governance. The most important things to know about the GPAI Code of Practice are that i) it was approved by the EU on 1 August 2025, ii) it is a voluntary resource which helps providers comply with the full suite of obligations for GPAI models, and iii) it consists of three chapters: 1) Transparency, 2) Copyright, and 3) Safety and Security. The European Commission strongly encourages providers to sign the Code of Practice and even indicated that it will be more trusting of signatory organisations. However, the Code itself is not law. The Safety and Security Chapter, which is by far the most detailed of the three chapters, consists of ten commitments. All commitments directly relate to GPAI model with systemic risk risk identification, management, mitigation, treatment, monitoring,   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 17/24\n\nownership, and accountability. Below is a detailed breakdown of each of the ten commitments and a summary of the most important measures that signatory organisations have committed to implementing.   \n\n> 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2)\n> https://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46 18/24\n\n1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2) \n\nhttps://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46  19/24 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2) \n\nhttps://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46  20/24 1/3/26, 5:23 PM General-Purpose AI Model Compliance Guide (Part 2) \n\nhttps://oliverpatel.substack.com/p/general-purpose-ai-model-compliance-e46  21/24", "fetched_at_utc": "2026-02-08T19:08:07Z", "sha256": "4fda32651a331c4eec7607653ab4e15672769ef3bbf9f1079f5c3060cf180226", "meta": {"file_name": "General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel.pdf", "file_size": 1723762, "relative_path": "pdfs\\General-Purpose AI Model Compliance Guide - Part 2 - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4371}}}
{"doc_id": "pdf-pdfs-how-could-the-eu-ai-act-change-oliver-patel-0ecedae03efa", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\How could the EU AI Act change - Oliver Patel.pdf", "title": "How could the EU AI Act change - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel \n\nhttps://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change  2/23 On Wednesday 19 November 2025, the European Commission unveiled its Digital Omnibus Package, proposing targeted yet impactful amendments to the EU AI Act. This article distils what could change, why it matters for enterprise AI governance practitioners, and what to watch as trilogue negotiations begin. \n\nIf you value my work and want to learn more about the EU AI Act and AI governance implementation, sign up to secure a 25% discount for my forthcoming book, \n\nFundamentals of AI Governance (2026). On Wednesday 19 November 2025, the European Commission (henceforth the Commission) announced proposed changes to the AI Act. These changes are presented as â€œinnovation-friendly AI rulesâ€ that will â€œreduce compliance costs for businessesâ€. It did so by publishing a proposal for a new regulation. The purpose of this proposed regulation is to â€œsimplifyâ€ the AI Act with targeted yet meaningful amendments. This is part of the Commissionâ€™s broader â€œDigital Packageâ€, which is a major programme of work aiming to â€œsimplify EU digital rules and boost innovationâ€. The \n\nWhat are the most important proposed EU AI Act changes?   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 3/23\n\nDigital Packageâ€”which encompasses the â€œDigital Omnibus Regulationâ€â€”includes proposals to amend flagship digital laws like the AI Act, the GDPR, the ePrivacy Directive, the Data Act, and the NIS 2 Directive. Specifically, the Commission simultaneously published proposals for two regulations (so itâ€™s not really an â€œomnibusâ€ anymore): \n\nProposal for Regulation on simplification of AI rules (which covers the AI Act amendments); and \n\nProposal for Regulation on simplification of the digital legislation (which covers the amendments to the other EU digital laws mentioned above). This article explains and analyses the six most important AI Act amendments that enterprise AI governance professionals need to understand. These are: \n\n1. Timeline changes for high-risk AI system compliance. \n\n2. Timeline changes for transparency-requiring AI system compliance. \n\n3. Limiting registration in the public EU database for high-risk AI systems. \n\n4. Softening of the AI literacy obligation. \n\n5. Expanding the scope of the European AI Officeâ€™s regulatory powers. \n\n6. Proportionality for small mid-cap (SMC) enterprises.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 4/23\n\nFor each of these six proposed amendments, I explain what is in the law today, what changes are being proposed, and what the impact of these changes would be. Earlier this week, I published an article on Enterprise AI Governance that explains how we got to this point and why the EU is now doing this. It provides a detailed account of the background context to AI Act simplification, highlighting how the â€˜Draghi reportâ€™â€”which argued that digital regulatory burdens are impeding European growth and competitivenessâ€”has influenced the Commissionâ€™s proposals. It also outlines three important caveats on the EUâ€™s legislative process that are worth repeating: This merely represents the proposal of one EU institution (the Commission). Such amendments of EU law require formal approval from both the European Parliament and the EU member states via the Council of the EU (the Council). Therefore, this proposal will now be followed by lengthy and potentially fraught trilogue negotiations between the Commission, European Parliament, and Council. Finally, it is impossible to predict what the final legislative text will consist of, how long the negotiation and approval process will take, and whether approval to amend the AI Act will ultimately be agreed on and enacted.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 5/23\n\nScope of this article: this is not an exhaustive analysis of the entire AI Act simplification proposal and it does not cover every proposed amendment in the Commissionâ€™s 65-page document. Rather, it focuses on the six proposed changes that would be most consequential (if passed) for enterprises implementing AI governance. Also, it intentionally does not cover the proposed changes to the GDPR, nor the AI Act amendments that are directly related to the processing of personal data (e.g., use of sensitive personal data for bias mitigation), as this topic will be addressed in a future article on Enterprise AI Governance. \n\nDisclaimer: this article is not intended to be legal advice and must not be relied upon or used in that way. Always consult a qualified legal professional. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nWhat is in law today? \n\n1. Timeline changes for high-risk AI system compliance   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 6/23\n\nThe key provisions relating to high-risk AI systems apply from 2 August 2026. This means that from 2 August 2026, unless there is a change in the law, providers and deployers must adhere to the obligations and requirements for high-risk AI systems and can be subject to investigations and penalties for non-compliance. However, this applicable date only applies to high-risk AI systems listed in Annex III (e.g., education, employment, administration of justice etc.) that are placed on the market or put into service from 2 August 2026 onwards. For such Annex III high-risk AI systems that were placed on the market or put into service before 2 August 2026, providers and deployers are only subject to AI Act obligations and requirements if, from that date onwards, there is a significant change in design or intended purpose of the AI system. Furthermore, for high-risk AI systems that are products, or safety components of products, regulated by specific EU product safety laws listed in Annex I, the applicable date is 2 August 2027. \n\nWhat changes are being proposed? \n\nIf you are asked â€œwhen do the compliance obligations for high-risk AI systems apply?â€ ,your answer now has to be â€œit dependsâ€. Given the nature of the proposed amendments, there are various potential scenarios. The Commission is seeking to link the applicability of high-risk AI system provisions   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 7/23\n\nwith the availability of technical standards and associated support tools. However, if these artefacts are not approved and available within a certain timeframe, there is a backstop date, which represents the latest applicable date. Under this proposal there are, broadly speaking, three potential scenarios for when most of the provisions relating to high-risk AI systems may apply (covering high-risk AI system classification, development requirements, and obligations of providers, deployers, and other parties): \n\nScenario 1. If technical standards and associated support tools for high-risk AI system compliance are finalised and approved by the Commission, then the applicable compliance date will be six months after this approval (for high-risk AI systems listed in Annex III) and 12 months after this approval (for high-risk AI systems that are products, or safety components of products, regulated by an EU law listed in Annex I). \n\nScenario 2. However, if technical standards and associated support tools for high-risk AI system compliance are not finalised or approved by the Commission in time (i.e., before the dates below), then the applicable compliance dates will be 2 December 2027 (for high-risk AI systems listed in Annex III) and 2 August 2028 (for high-risk AI systems that are products, or safety components of products, regulated by an EU law listed in Annex I).   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 8/23\n\nScenario 3. Given that the applicable date in law today is 2 August 2026, if these amendments are not approved and enacted before this date, then the provisions relating to high-risk AI systems will, technically speaking, apply from then. This creates timeline pressure to get these changes approved within the next few months. \n\nWhat impact would these changes have? \n\nTo clarify, 2 December 2027 and 2 August 2028 are the backstop dates for high-risk AI system compliance. To reinforce this point, the Commission has explained that the grace period will be up to sixteen months (referring to the time between 2 August 2026 and 2 December 2027). This means that if technical standards come too late (i.e., after 2 June 2027, which is six months before 2 December 2027) these backstop dates will apply. These amendments shine the spotlight on the ongoing work being led by CEN/CENELEC to agree and publish technical standards. They also highlight the importance that the Commission places on these artefacts to support organisations and facilitate compliance. However, even if the applicable date may be delayedâ€”giving providers and deployers more time to prepareâ€”this extra time has been achieved at the expense of certainty, with organisations now not knowing what the applicable date will be.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 9/23\n\nWhat is in law today? \n\nArticle 50 of the AI Act outlines transparency obligations for providers and deployers of certain AI systems. Article 50 covers obligations relating to disclosure, informing end users about the use of AI, labelling certain deep fake content, and detectability of AI system outputs. Specifically, Article 50(2) stipulates that: \n\nâ€œproviders of AI systems, including general-purpose AI systems, that generate synthetic audio, image, video, or text content shall ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulatedâ€. \n\nCurrently, this specific obligation applies from 2 August 2026. This compliance date applies to all AI systems, irrespective of whether they are placed on the market or put into service before or after 2 August 2026. \n\nWhat changes are being proposed? \n\n2. Timeline changes for transparency-requiring AI system compliance   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 10/23\n\nThe Commission proposes to push back the applicable date for this specific transparency obligation to 2 February 2027 for providers of AI systems that have been placed on the market before 2 August 2026. This proposed six month delay to the applicable date only applies to obligation stipulated in Article 50(2) (on AI system output machine readability and detectability) and not the other transparency obligations outlined in Article 50. \n\nWhat impact would these changes have? \n\nThis change would give providers of AI systems that have already been placed on the market or put into service, or that will be before 2 August 2026, and that generate synthetic audio, image, video, or text content (i.e., most generative AI systems), an additional six months to ensure that these AI systems are developed in such a way that ensures the outputs they generate are detectable as AI-generated. Although this is a relatively short delay, it is nonetheless an acknowledgement by the Commission of the technical and engineering challenges providers face in developing or modifying their AI systems to adhere to this obligation. However, any AI systems placed on the market on or after 2 August 2026 will have to comply with this obligation from their release date. \n\n3. Limiting registration in the EU public database for high-risk AI   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 11/23\n\nWhat is in law today? \n\nAnnex III of the AI Act lists eight categories of high-risk AI system, including law enforcement (#6), education and vocational training (#3), and employment, workersâ€™ management and access to self-employment (#5). However, there are classification rules which mean that just because an AI system is intended for use or used in one of these domains does not necessarily mean it is a high-risk AI system. AI systems listed in Annex III are not considered high-risk if it is demonstrated that they do not pose significant risk of harm to health, safety, or fundamental rights. For example, if the AI system does not materially influence decisions or is only used for a narrow procedural task, the provider is entitled to demonstrate, based on a documented assessment, that it is not a high-risk AI system. This derogation, including the conditions to fulfil it, is outlined in Article 6(3) and only applies to AI systems listed in Annex III. Providers must register high-risk AI systems listed in Annex III in the EU public database for high-risk AI systems, before those AI systems are placed on the market or put into service. Interestingly, this registration obligation also includes AI systems that \n\nsystems   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 12/23\n\nthe provider has concluded are not high-risk via the derogation procedure outlined in Article 6(3). \n\nWhat changes are being proposed? \n\nThe Commission proposes to limit the scope of this registration obligation so that it no longer applies to AI systems that providers have concluded are not high-risk via the Article 6(3) derogation procedure. Simply put, where a provider has assessed and documented that an AI system used in an Annex III domain is not high-risk, the provider will not have to register that AI system in the EU public database for high-risk AI systems. However, although providers can make this assessment independently and do not require any external approval (e.g., from the AI Office or market surveillance authority), providers will still be obliged to share the documentation of the assessment, containing the justification and supporting evidence, upon request from a regulator. \n\nWhat impact would this have? \n\nThis may seem like a subtle change at first glance, but it would be consequential for organisations using AI at scale.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 13/23\n\nMost enterprise AI governance practitioners likely raised their eyebrows when they realised that every AI system used in a high-risk domain, including AI systems that are not high-risk due to their use for mere assistive, procedural, or preparatory tasks, would have to be registered. This will be difficult (or perhaps near impossible) to keep track of and implement, due to the increasingly ubiquitous use of AI to support and augment workflows across virtually all domains of enterprise activity. Indeed, the Commission describes this current registration obligation as a â€œdisproportionate compliance burdenâ€. Therefore, the most obvious impacts of this change would likely be far fewer AI systems registered in the EU public database for high-risk AI systems and reduced administrative overheads for organisations developing and deploying AI systems. However, it would also reduce public transparency regarding which AI systems providers deem not to be high-risk and how they have made such determinations. This could incentivise some providers to take a more expansive approach to interpreting Article 6(3) and determining what is not a high-risk AI system, as they may reasonably judge that the risk of doing so (and being penalised for getting it wrong) is lower with significantly less public scrutiny. \n\n4. Softening of the AI literacy obligation   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 14/23\n\nWhat is in law today? \n\nUnder Article 4 of the AI Act providers and deployers of AI systems are obliged to implement â€œAI literacyâ€. This is one of the most important aspects of the law, because it has contributed to many organisations in the EU and further afield rolling out AI training and upskilling initiatives for their workforce. Specifically, Article 4 requires organisations to ensure that â€œstaff and other persons dealing with the operation and use of AI systemsâ€ have a â€œsufficient level of AI literacyâ€. In practice, given that all staff in modern organisations can use AI systems (e.g., ChatGPT or Gemini), a reasonable interpretation of Article 4 is that all of these staff should receive some form of AI-focused training. The AI literacy obligation has been applicable since February 2025. However, there are no enforcement penalties for non-compliance with it. Although non-compliance could be taken into account during enforcement investigations or proceedings relating to other aspects of non-compliance. \n\nWhat changes are being proposed? \n\nThe Commission proposes to remove the obligation for providers and deployers to implement AI literacy. Rather than providers and deployers being legally required to ensure their staff operating and using AI systems have sufficient levels of AI literacy,   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 15/23\n\nthe Commission and Member States will be required to foster AI literacy and \n\nâ€œencourage providers and deployers of AI systems to take measures to ensure a sufficient level of AI literacyâ€ . The Commission has alluded to the fact that the ambiguity of the current â€œunspecified obligationâ€ has caused issues for businessesâ€”especially smaller firms. \n\nWhat impact would this have? \n\nThis change would be significant because it would remove the broad and expansive legal obligation for companies to implement AI literacy. However, human oversight of high-risk AI systems must still be assigned to staff with sufficient training and competence. Therefore, ensuring AI literacy is still required in that context. Moreover, it will be practically impossible for any organisation to comply with the AI Actâ€”or to manage AI risks, implement AI at scale, and maximise the value of AIâ€” without educational and training initiatives focused on AI. Therefore, forward-thinking enterprises are unlikely to abandon their AI literacy programmes because it is no longer a legal requirement. However, certain initiatives may be scaled back, deprioritised, or change in focus or scope.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 16/23\n\nWhat is in law today? \n\nHere is a simplified summary of the (rather complex) AI Act governance regime: There are governance and regulatory bodies at both the EU and member state level. At the EU level, the most important bodies are the European AI Office (which is part of the Commission) and the European AI Board. At the member state level, the most important bodies are the market surveillance authorities. They are responsible for monitoring, investigations, and enforcement of the AI Act. There will potentially be several in each EU member state. The AI Office is responsible for overseeing and enforcing the provisions on general-purpose AI models, whereas the market surveillance authorities are responsible for overseeing and enforcing the provisions on AI systems (e.g., high-risk and transparency-requiring AI systems), as well as most other AI Act provisions. \n\nWhat changes are being proposed? \n\n5. Expanding the scope of the European AI Officeâ€™s regulatory powers   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 17/23\n\nThe Commission proposes to â€œcentralise oversight over a large number of AI systems built on general-purpose AI modelsâ€ when the same provider develops both the general-purpose AI model and the AI system. The proposed amendments to Article 75 would render the AI Office as the body responsible for monitoring and supervising compliance of AI systems that leverage general-purpose AI models. However, this would only apply when the general-purpose AI model and the AI system are developed and placed on the market or put into service by the same provider. In such scenarios, the AI Office would be â€œexclusively competentâ€, which means that the market surveillance authorities in the respective EU member states would no longer have a supervisory role. The AI Office would also have â€œall the powers of a market surveillance authorityâ€. This expansion in scope of the AI Officeâ€™s responsibilities does not apply to high-risk AI systems covered by an Annex I EU product safety law. Therefore, this change primarily impacts high-risk AI systems listed in Annex III and transparency-requiring AI systems regulated by Article 50 (where such AI systems leverage general-purpose AI models). Finally, under this proposal, the AI Office would also have exclusive competence as the regulator of â€œAI systems that constitute or that are integrated into a designated very large online platform or very large online search engineâ€ (as defined and regulated by the Digital Services Act).   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 18/23\n\nWhat impact would this have? \n\nThe implied rationale behind this change is that the Commission does not think it makes sense for the AI Office to be responsible for overseeing providers of general-purpose AI models but not the AI systems developed, made available, and put into service by those same providers. These changes would make the AI Office the supervisory authority for many of the most widely used AI systems worldwide. This is because most mainstream generative AI platforms, such as ChatGPT, Gemini, Claude, Grok, and Microsoft Copilot, are AI systems built on general-purpose AI models, with the same organisation being the provider of both the AI model and the AI system. Therefore, this would represent a meaningful increase in the relevance and prominence of the AI Office for AI Act oversight and enforcement, and it would also enable the AI Office to pursue investigations and enforcement action relevant for both general-purpose AI model and AI system compliance in a coordinated manner. This would also mean that certain AI system providers would not be subject to regulatory investigations and enforcement action across multiple EU member states, which is possible if the law isnâ€™t amended.   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 19/23\n\nThe AI Act provides an element of flexibility and proportionality for micro, small, and medium-size enterprises (SMEs), including start-ups. For example, the compliance penalties which SMEs can face are capped in the following way: 35 million EUR or 7% of total worldwide annual turnover (whichever is lower). 15 million EUR or 3% of total worldwide annual turnover (whichever is lower). 7.5 million EUR or 1% of total worldwide annual turnover (whichever is lower). This contrasts with the â€œwhichever is higherâ€ penalty logic that applies for all other businesses (i.e., those which are not SMEs). In practice, this means that many non-SME businesses could face potential penalties into the billions of euros, whereas penalties for SMEs will always be capped as per the above. Other ways in which the AI Act seeks to ease compliance burdens for SMEs include allowing SME providers of high-risk AI systems to provide the required technical documentation in a simplified way and providing SMEs with free access to AI regulatory sandboxes. \n\n6. Proportionality for small mid-cap (SMC) enterprises   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 20/23\n\nWhat changes are being proposed? \n\nThe first proposed change is to add legal definitions of SME and small mid-cap enterprise (SMC) to the AI Act. These are: SME: an enterprise which employs fewer than 250 people and which has an annual turnover not exceeding 50 million EUR, and/or an annual balance sheet total not exceeding 43 million EUR. SMC: an enterprise which employs fewer than 750 people and which has an annual turnover not exceeding 150m EUR or an annual balance sheet total not exceeding 129m EUR. The second and more significant proposed change is to extend the flexibility and proportionality penalties afforded to SMEs to SMCs also. This means that SMCs would benefit from the same capped enforcement penalty regime as SMEs, significantly reducing their total potential penalty exposure in certain circumstances. SMCs that are providers of high-risk AI systems would also be able to provide the required technical documentation in a simplified manner. \n\nWhat impact would this have?   \n\n> 1/3/26, 5:25 PM How could the EU AI Act change? - by Oliver Patel\n> https://oliverpatel.substack.com/p/how-could-the-eu-ai-act-change 21/23\n\nCore threads from the Draghi report are woven throughout this proposal. The Commission will be hoping that easing regulatory compliance burdens and softening the enforcement environment for a larger pool of companies will make it easier for EU digital start-ups and scale-ups to grow, innovate, and compete internationally. Indeed, the Draghi report argued that â€œregulatory burdensâ€ are particularly damaging for digital sector SMEs trying to rapidly scale up. \n\nThanks for reading! Subscribe below for weekly updates from Enterprise AI Governance. \n\n> 12 Likes âˆ™ 2 Restacks\n\nDiscussion about this post \n\nWrite a comment...", "fetched_at_utc": "2026-02-08T19:08:09Z", "sha256": "0ecedae03efafe388dc2480f5790ec48727b1ad86b24f1ab8e3fe20381d4f18c", "meta": {"file_name": "How could the EU AI Act change - Oliver Patel.pdf", "file_size": 802855, "relative_path": "pdfs\\How could the EU AI Act change - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 5622}}}
{"doc_id": "pdf-pdfs-initial-reflections-on-agentic-ai-governance-oliver-patel-67545ac8c4be", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Initial reflections on agentic AI governance - Oliver Patel.pdf", "title": "Initial reflections on agentic AI governance - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 4:48 PM Initial reflections on agentic AI governance \n\nhttps://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai  2/29 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! ICYMI: visit this page to download my free 20-page AI Usage Policy Playbook and register interest for my upcoming AI Usage Policy Bootcamp .This weekâ€™s edition is an essay on agentic AI governance . It covers: \n\nâœ… What is agentic AI? \n\nâœ… How agentic AI is used today and how it could be used in future? \n\nâœ… What novel risks does agentic AI pose? \n\nâœ… How do these risks challenge existing AI governance frameworks? \n\nâœ… What new policies, standards, and guardrails are required to address these challenges and mitigate risk? The key message is that urgent work is required, within the AI governance community, to refine and update our approach to AI governance and risk management in the   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 3/29\n\nagentic AI era. I am very keen for comments and feedback, to guide my thinking and work in this emerging field. However, I also donâ€™t want to waste anyoneâ€™s time. Therefore, if you donâ€™t want to read a 4,000-word essay on agentic AI governance, then this may not be for you. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week. \n\nThis essay outlines my initial reflections on agentic AI governance. The phrase â€˜initial reflectionsâ€™ is designed to serve as a health warning for all readers. Large language model (LLM) based AI agents are a relatively new technology. There are not many enterprise use cases in production, at least compared to generative AI and traditional machine learning. \n\n## Initial reflections on agentic AI governance   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 4/29\n\nFurthermore, there are not yet any laws, standards, frameworks, or guidelines which directly address or stipulate how the novel risks of agentic AI should be mitigated. Finally, not much has been researched or published on the topic of agentic AI ethics and governance. However, I will reference some of what has been written throughout this essay. Considering the above, it is reasonable for you to ask why I am writing and publishing this today. Well, the main reason is that agentic AI is comingâ€”whether the AI governance community is ready or not. As we should all appreciate by now, technology is not going to wait for us to figure things out. As a community, we had several years to discuss, align on, and codify the key ethical principles, policies, and standards for AI, before enterprise adoption of machine learning became mainstream. And, although our response and adaptation timelines were accelerated for generative AI, the launch of ChatGPT was a landmark moment that made it obvious there was pressing work for us to do.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 5/29\n\nWith agentic AI, the challenge is twofold. Not only is time in short supply, I also do not anticipate that there will be a watershed moment that highlights the urgency of agentic AI governance. It is, and will continue to, creep up on us and permeate our organisations. For this reason, there is a serious possibility that we may fail to identify and grasp the unique challenges and risks which agentic AI poses in time for the inevitable proliferation of use cases, adoption at scale, and democratisation. This could leave our organisations and stakeholders exposed, with an AI governance and risk management framework that is no longer fit for purpose in the agentic AI era. I hope that my â€˜initial reflectionsâ€™ essay can help to address this challenge. However, please take everything I say with a pinch of salt, as this is not a peer-reviewed paper, and there is a lot of work for me to do to fully wrap my head around this complex topic. You will perhaps be unsurprised to read that there is not yet an internationally agreed definition of agentic AI. Here are some industry definitions, to get us started: \n\nWhat is agentic AI?   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 6/29\n\nâ€œAI agents are software systems that use AI to pursue goals and complete tasks on behalf of usersâ€ Google \n\nâ€œAgentic AI systems can accomplish a specific goal with limited supervisionâ€ IBM \n\nâ€œAgentic AI systems act autonomously, make decisions, and adapt dynamically to complex environmentsâ€ Kieran Gilmurray \n\nCentral to all definitions of agentic AI is the concept of proactive and autonomous completion of tasks. In his new book Kieran Gilmurray outlines the three waves of AI: predictive AI, generative AI, and agentic AI. With traditional machine learning, models generate predictive outputs, like scores, classifications, and recommendations. With generative AI, models generate content, like text, video, or code. These predictive or generative AI outputs are typically provided to humans via a user interface and are then used by those humans to assist, augment, or optimise their work. With agentic AI systems, the work itself may no longer be done by the humans.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 7/29\n\nAgentic AI systems can autonomously develop plans, solve problems, retrieve data, leverage memory, use tools, and execute tasks in a range of other applications which they are integrated with. They do so by constantly processing inputs, learning from, and adapting to their environment, and proactively determining the best course of action to take. Hence the notion of â€˜agencyâ€™. AI agents are powered by LLMs and APIs. The LLM enables the system to process the initial instructions and mimic reasoning to break down complex problems into a series of smaller steps to be executed. The APIs enable integration with a range of other applications, tools, and databases, to retrieve data and make things happen. Because agents are LLM-powered, they are unpredictable and non-deterministic, in contrast to more traditional forms of software automation and workflows, like RPA; more on that below. As you can imagine, the potential spectrum of agentic AI use cases is limitless. However, there are not a huge number of AI agents in production today. This is still a \n\nWhat are the use cases for agentic AI?   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 8/29\n\nnovel (and by extension relatively unreliable and untested) technology. \n\nIBM has declared 2025 as the year of agentic exploration. Each organisation will likely witness a proliferation of PoCs and pilots this year, just like we have seen with generative AI over the past two and a half years. In this early wave of agentic AI use cases, there is an emphasis on assistive and productivity enhancing tasks, such as research, summarisation, and information retrieval. A survey by LangChain finds that the most popular agentic AI use cases today are research and summarisation, personal assistance and productivity, customer service, and code generation. In many cases, AI agents are already working behind the scenes, without our awareness, to improve the performance and effectiveness of the most widely used generative AI applications, like Perplexity and ChatGPT. For example, the current crop of â€˜deep researchâ€™ tools function by leveraging teams of AI agents which collaborate with each other to scour the internet and other data sources to retrieve, collate, assess, merge, and summarise relevant information, to augment and enhance the final AI-generated â€˜reportâ€™ or response which the user receives.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 9/29\n\nThe architecture of such applications has become much more sophisticated than one model receiving a prompt, performing inference, and generating a predictive output. Looking ahead, the thinking is that agentic and multi-agentic systems will be able to take on increasingly complex tasks and projects, such as managing customer service interactions, planning and booking holidays, planning, creating, and posting social media content, and managing investment portfolios. Before we get there, we need a bulletproof approach to agentic AI governance. Although there is currently ample hype (some of which is inevitably overblown), this is not an excuse to ignore or disregard this technological trend. Agentic AI poses unique risks, which the AI governance community cannot afford to overlook. The risks of AI stem primarily from the way the technology is used and the real-world impact this use can have. \n\nThe novel risks and challenges of agentic AI   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 10/29\n\nTherefore, even if agentic AI is based upon the same underlying technology as generative AI (i.e., LLMs), this does not mean it will be used in the same way. The deployment and use of agentic AI, and thus its impact on people, organisations, and society, will be markedly different to what has come before. Increasingly autonomous capability enables novel AI use cases, such as control of computers and automation of dynamic, data-intensive processes in sensitive areas like supply chain management and logistics planning. Furthermore, it is conceivable that, in future, knowledge workers will have access to their own personalised AI agent, to assist with all aspects of their work. Taken together, these examples represent a meaningful shift from how AI is used today. To illustrate the novel risks, I will focus on four themes of the utmost importance for agentic AI: \n\n1. â€˜Human out of the loopâ€™ \n\n2. Autonomous task execution and performance   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 11/29\n\n3. Adaptiveness and unpredictability \n\n4. Data, privacy, and cyber security In the agentic AI era, all AI risks are amplified. Virtually all existing AI risk themes, such as bias, transparency, copyright, explainability, alignment, sustainability, and labour market disruption remain as relevantâ€”if not more soâ€”than ever. However, my goal here is to focus on the most novel challenges posed by agentic AI. \n\n1. Human out of the loop \n\nIt is not an exaggeration to state that the purpose of agentic AI is to take the human out of the loop. Why plan and book your own holiday when an AI agent can do it for you? Why respond to all of your fans and followers across multiple social media platforms when an AI agent can take care of the correspondence? Why employ hundreds of call centre workers when an army of autonomous agents can do the job? However, this is in direct tension with the concept of human in the loop, which is a foundational pillar of AI governance. By delegating and outsourcing tasks to AI agents, humans may be freed to focus their   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 12/29\n\ntime and energy elsewhere. However, AI agents will also be trusted to take on increasingly important tasks, with diminishing human oversight. The key risk is that we become overly trusting of agentic AI systems and take the human out of the loop to a degree which becomes dangerous. In the quest for efficiency gains, we may underestimate the level of human oversight required for safe agentic deployment. This risk is especially pertinent at first, as we do not truly understand the limitations and capabilities of agentic AI systems. An associated challenge, discussed below, will be the complexity of refining and updating our approach to human oversight. This will require defining exactly when human review and approval is required before an action can be taken. Moreover, if the AI agent executes the action, it may become even harder to determine which human, or entity, should be held accountable for it. This will need to be codified at the outset of agentic development. \n\n2. Autonomous task execution and performance   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 13/29\n\nIf you thought AI hallucinations were bad, wait till you learn about â€˜cascading hallucinationsâ€™. OWASP describes this as when an â€œAI agent generates inaccurate information, which is then reinforced through its memory, tool use, or multi-agent interactions, amplifying misinformation across multiple decision-making stepsâ€. This can lead to self-reinforcing destructive behaviours and systemic failures in agentic AI performance. Agentic AI raises the stakes for the hallucination problem. An LLM hallucination is primarily a problem if the user naively fails to verify the accuracy of the output before relying on or using it. However, an LLM hallucination which directly informs and shapes the course of action taken by an AI agent (or team of agents) could have severe consequences, if that agent is being trusted to execute tasks in a high-risk domain. The more autonomous AI agents become, and the more we trust those agents to take over tasks and projects in sensitive areas, the greater the risk and negative impact of malfunction, error, and performance degradation. Although AI performance concerns are nothing new, without appropriate controls, such as human oversight and observability, there is a risk that the agents we begin to trust let us down, without us even realising at first.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 14/29\n\nFor example, if the AI agent is autonomously handling and responding to customer complaints, how many inappropriate interactions and unnecessary follow ups could occur before this is flagged and addressed? It is also critical that the appropriate agent performs the appropriate task. In the modern enterprise there will be many agents, working together and supervising each other in complex hierarchies, based on their pre-defined roles, permissions, and guardrails. Therefore, the reliable performance of agents towards the top of the hierarchy is critical for the performance and effectiveness of all the other agents. By taking the human out of the loop and allowing AI agents to autonomously execute tasks, it is undeniable that there are immense potential efficiency and productivity gains. However, with increased autonomy comes increased risk. There may be some domains where we simply cannot afford agentic AI mistakes. A broader question is whether we are building these exciting new tools on the relatively shaky foundation of a technology which was ultimately designed to predict the next word, rather than perform important tasks.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 15/29\n\nResearchers from Hugging Face have sounded the alarm bell and argued that fully autonomous agents should not be developed, due to the unacceptable potential risks resulting from system inaccuracy, privacy and security breaches, spread of false information, and loss of human control. \n\n3. Adaptiveness and unpredictability \n\nAI agents are unpredictable precisely because they are proactive. If we knew or could consistently guess what they were going to do, they would not have agency in any meaningful sense. LLMs are non-deterministic, which means models can generate different outputs in response to the same inputs. This leads to unexpected, unpredictable, and unreliable outputs. This will inevitably be reflected in the behaviour and performance of AI agents, which will rely on the effectiveness of LLMs and their ability to accurately predict the next word. Given the proactive and dynamic way in which AI agents respond and adapt to their environment, it could become virtually impossible to predict and anticipate how they will behave, and therefore the risks that could emerge. This makes AI risk assessment, and therefore AI risk mitigation, much more challenging than it is today. It also requires much more continuous and comprehensive monitoring   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 16/29\n\nof AI performance. It is hard enough to predict the risks of downstream general-purpose AI usage, let alone the potential behaviour and risks of autonomous agents, which are integrated with a range of other applications, and empowered to solve complex and open-ended problems in whichever way they see fit. \n\n4. Data, privacy and cyber security \n\nData, privacy and cyber security risks are nothing new for AI. However, these risks are exacerbated by agentic AI. Agentic AI systems could easily mine and retrieve data from sources which they were not supposed to have access to, or sources which are not permitted to be used for AI processing or text and data mining. This could include copyrighted material without an appropriate license, or sensitive personal data originally collected for a different, more narrow purpose. Furthermore, there is also the risk of AI agents disclosing and revealing data to people who were not authorised to have access to it. Agents will be performing and automating increasingly personalised tasks, such as booking medical appointments, whilst having access to and being trained on huge amounts of personal data, such as medical records.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 17/29\n\nThis elevates the risk of data breaches and information leakage. Therefore, encoding privacy by design and data governance guardrails will be a challenging but necessary part of agentic AI governance. Agentic AI systems will also become attack surfaces. Nefarious actors will undoubtedly attempt to take control of, and manipulate, the autonomous systems which themselves may control important spheres of business activity. There is a lot you can do if you control an agentic system which itself can control computers with access to sensitive data and applications. Also, in situations where AI agents are trusted to both generate and execute code, the risk of autonomously executed (and non-vetted) malicious code creeping in to production applications increases. In a recent report , OWASP outlines 15 unique threats which agentic AI systems are vulnerable to. This includes cascading hallucination attacks, resource overload, tool misuse, and rogue agents in multi-agent systems. â€˜Traditional' AI governance frameworks, such as the EU AI Act and NIST AI Risk Management Framework, were developed during a time when traditional machine \n\nPolicies, guardrails and controls to manage agentic AI risks   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 18/29\n\nlearning and then generative AI was prevalent. They do not directly address many of the novel risks and challenges of agentic AI, discussed above. Indeed, if the EU AI Act was being drafted today, I am certain that some of my below points would be directly addressed in the law. However, we cannot rely on, or wait for, the regulators to come and save us with new guidance or updated standards. They will, rightly so, expect industry to figure out how to develop and implement agentic AI in a manner which is safe, secure, and respects existing laws, like the EU AI Act. None of the risks highlighted above represent insurmountable problems. I have full faith in the ingenuity of the AI governance community to solve them. Below, I will sketch out the six most important considerations for AI governance professionals seeking to refine, update, and implement policies, guardrails, and controls, to meet the challenge of managing risk in the agentic AI era. This includes: \n\n1. Action permissions and thresholds \n\n2. Integrations and data access \n\n3. Hierarchy and approval matrixes \n\n4. Monitoring, observability, and orchestration \n\n5. Human oversight, accountability, and control   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 19/29\n\n6. Use cases and risk assessments \n\n1. Action permissions and thresholds \n\nConfiguring action permissions appropriately is an essential part of agentic AI governance. Autonomy is a spectrum. Just because an agent can do something does not mean we should let it. We can determine exactly which actions an agent can and cannot perform. The potential behaviour and permissions of agents can be restricted at the system and API level. If there are certain tasks which an agent could in theory perform in a given system or environment, or certain tools the agent could use, and we do not want the agent to do so, we can specify and encode these restrictions. This sounds simple enough at first. For example, in a financial context, we may not want the agent to execute or process any transaction, or make any decision, which carries a financial value over a certain amount. Similarly, we may want to restrict the ability of an agent to execute AI-generated code in certain applications.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 20/29\n\nWhat is more challenging is to define generally applicable policy principles, which can be used to determine, for any use case in any domain, what type of action permissions and restrictions we should impose, and what thresholds we should use to guide this. Some potential action permission threshold categories could be: Financial value Direct impact on people Number of people impacted Impact on patients Importance of the decision or action on the business Potential ethical risk (e.g., EU AI Act high-risk AI system categories) \n\n2. Integrations and data access \n\nOn a similar note, we can also determine and restrict which applications an agent is integrated with, as well as which data it has access to. As well as enabling privacy by design and data governance, this also supports the points raised above relating to access restrictions.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 21/29\n\nIf an agent is unable to access certain data and/or is not integrated with the application where a particular task is performed or tool is used, then it will be unable to use that data or tool to do something which we do not want it to do. Again, we will need to formulate generally applicable policy principles which can steer us in our assessment of which applications agents should and should not be integrated with, as well as which datasets should and should not augment their knowledge base. \n\n3. Hierarchy and approval matrixes \n\nIn the modern enterprise there will be countless agents working together, in complex hierarchies of agentic collaboration, supervision, and oversight. There will need to be a clearly defined RACI or matrix for AI agents, which outlines the roles, responsibilities, and segregation of duties. It is crucial that agent Y only performs tasks within its permitted duties and that agent X does the same. Agents towards the top of the hierarchy will be empowered to review, approve, authorise, and restrict the work of other agents. And agents lower down in the hierarchy should not be allowed to operate in a way which circumvents the authority of their superiors.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 22/29\n\nThis will require both complex engineering and architectural design, as well as a new conceptual framework for AI governance professionals to lean on. \n\n4. Monitoring, observability, and orchestration \n\nWe are moving from MLOps and LLMOps to AgentOps. In the â€˜old worldâ€™, MLOps is used to validate, test, and monitor model performance, including robustness and accuracy. This primarily focuses on the predictive outputs that are generated and how they perform across a range of key metrics. With AgentOps, the goal is to automate the monitoring, oversight, and orchestration of agentic and multi-agentic AI systems, so we can keep tabs on what actions they are performing, how they are behaving, which tools they are using, the impact this is having, and ultimately, whether we can trust them to keep working on our behalf. There should also be visibility as to whether any agents are operating contrary to their guardrails and permissions. Assessing and evaluating agentic AI performance also entails additional complexity, at least compared with traditional machine learning performance evaluation. This is because the actual tasks that agents perform are much more wide-ranging, varied, and hard to anticipate, given the proactive nature of agentic AI. Therefore, we   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 23/29\n\nwill need updated and rigorous performance and accuracy metrics, which can account for the variety of possible tasks and agent could perform. \n\n5. Human oversight, accountability and control \n\nWhat we mean by human oversight will also require a refresh. It will no longer make sense to mandate human review and approval of each AI-generated output, when the purpose of agentic AI is to take the human out of the loop, to automate processes and drive efficiencies. If the goal is AI autonomy, humans cannot review everything. However, this does not mean human oversight is no longer relevant. For example, human oversight could mean reviewing the ultimate output of an agentic AI system, such as a â€˜deep research report or generated code (as opposed to all the outputs generated and decisions taken to reach its conclusion and generate that final output). Human oversight could also mean having a human in the loop to review actions, tasks, and decisions which meet a certain threshold or risk level, which could be aligned to the action permissions and thresholds detailed above. We will need clearly defined   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 24/29\n\ntouch points for human oversight and review, and it will be more nuanced than what we have today. Finally, humans must always have the ability to override or shut down an agentic AI system, no matter how much autonomy we have empowered it with. According to a LangChain survey on agentic AI usage in 2025, very few companies are allowing agents to freely read, write, and delete data and information from the applications they are operating in and the databases they have access to. Rather, agents are given read-only permissions, with human approval required for significant actions. \n\n6. Use cases and risk assessments \n\nFinally, it is important to determine which agentic AI use cases should be off limits, at least for now. The EU AI Act serves as a useful starting point. AI agents should obviously not be used to for anything which constitutes a prohibited AI practice. Furthermore, I would also advise extreme caution in using agents to autonomously perform tasks, which can have a material impact on decision making or a process, in a domain relating to high-risk AI systems, such as recruitment, critical infrastructure safety management, or determining eligibility for welfare payments.   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 25/29\n\nFor one, there is no evidence that agentic AI systems can yet be trusted to perform to a high enough standard required for these sensitive domains. Furthermore, it will be challenging to comply with the AI Actâ€™s obligation for deployers to assign human oversight and the GDPRâ€™s restrictions on solely automated decision-making, whilst also leveraging autonomous agentic AI systems to automate decision-making in sensitive and high-risk domains. However, you will need to look beyond EU law in your work to determine what use cases are appropriate, inappropriate, and off limits for your organisation. Consider the fundamentals of what agents can and cannot do, as well as their strengths and weaknesses. Google, for example, highlights that agents struggle with and should not be used for tasks requiring empathy and emotional intelligence, complex human interactions, high-stakes ethical decision-making, and the navigation of unpredictable physical environments. Once you have figured this all out, you will also need to update your approach to risk assessments, as well as your supporting guidance and training. The key question which needs to be answered throughout is when is it safe to use agentic AI and when is it not?   \n\n> 1/3/26, 4:48 PM Initial reflections on agentic AI governance\n> https://oliverpatel.substack.com/p/initial-reflections-on-agentic-ai 26/29\n\n*The purpose of this essay is to highlight some of the novel risks and governance challenges of agentic AI. Whilst I am not proposing a complete overhaul of AI governance frameworks and policies, the considerations I have outlined above should serve as a starting point for refining and updating your organisationâ€™s approach to AI governance in the agentic AI era. If you have made it this far, I would greatly appreciate comments and feedback. Thank you! \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work.", "fetched_at_utc": "2026-02-08T19:08:12Z", "sha256": "67545ac8c4be65b85732e7ce73aa33544f3f1e9643db47caba4ba952cdca6db0", "meta": {"file_name": "Initial reflections on agentic AI governance - Oliver Patel.pdf", "file_size": 814142, "relative_path": "pdfs\\Initial reflections on agentic AI governance - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 6402}}}
{"doc_id": "pdf-pdfs-netherlands-ai-act-guide-3015a2b513d5", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Netherlands AI Act Guide.pdf", "title": "Netherlands AI Act Guide", "text": "AI Act Guide \n\n# Version 1.1 â€“ September 2025 AI Act Guide  | 2\n\nCover image: Wes Cockx & Google DeepMind / BetterImages of AI / AI large language models / CC-BY 4.0 AI Act Guide  | Guide to reading and disclaimer  3\n\nGuide to reading this document and disclaimer \n\nYou develop AI systems or are considering using them in your organisation. In that case, you may \n\nwell come into contact with the AI Act. This guide has been prepared as a tool to provide you with \n\neasy insight into the key aspects of the AI Act.  No rights may be derived from the content of this \n\nguide.  The legal text of the AI Act always takes precedence. \n\nPlease submit any feedback on this guide  to  ai-verordening@minezk.nl . Your feedback will be \n\nused to improve future editions of the guide. \n\nIf you are reading a paper version of the guide , visit  ondernemersplein.nl  for the latest version. \n\nThe website also provides references to the latest guidelines from the European Commission. At the \n\ntime of publication, guidelines have been issued regarding the definition of AI, prohibited AI and \n\nGeneral Purpose AI models. \n\n# The AI Act \n\nThe AI Act is an extensive legal document governing Artificial Intelligence (AI) for the entire European Union \n\n(EU). The AI Act contains rules for the responsible development and use of AI by businesses, government \n\nand other organisations. The aim of the regulation is to protect the safety, health and fundamental rights \n\nof natural persons. Application of the regulation means that organisations can be certain that the AI they \n\nuse is responsible and that they can enjoy the benefits and opportunities offered by AI. \n\nThe regulation will be introduced in phases and the majority will apply as from mid-2026 onwards. \n\nA number of AI systems have already been prohibited since February 2025. Given this situation, it is \n\nimportant that you make the necessary preparations. To help you in that process, this guide lists the \n\nmost important provisions from the AI Act. However, no rights may be derived from the information \n\ncontained in this document. Its sole purpose is to provide support. Click  here 1 for the complete text of \n\nthe regulation. \n\n## What does the AI Act mean for your organisation? \n\nDepending on the type of AI system and the use to which the organisation puts that system, requirements \n\nwill be imposed on its development and use. Whether requirements are imposed will among others depend \n\non the risk the AI system represents to safety, health and fundamental rights. Different requirements will \n\nbe imposed on organisations that develops an AI-system or has it developed than on organisations that \n\nmake use of AI. To find out what the AI Act means for your organisation, it is important to work through \n\nthe four steps listed below. These steps are explained further in this guide: \n\nStep 1 (Risk):  Is our (AI) system covered by one of the risk categories? \n\nStep 2 (AI):  Is our system â€˜AIâ€™ classified according to the AI Act? \n\nStep 3 (Role):  Are we the provider or deployer of the AI system? \n\nStep 4 (Obligations):  What obligations must we comply with? \n\nNote:  Many other guides and step-by-step plans start at step 2 (is our system â€˜AIâ€™) rather than step 1 \n\n(risk categories). After all, if an AI system does not qualify as â€˜AIâ€™ there are no requirements subject to the \n\nAI Act. However, even for systems which are not categorised as AI according to the AI Act, it is important \n\nto have a clear idea of the risks of the purposes for which they are used. That is why in this AI Act Guide, \n\nwe have chosen to start with the risk categories.  \n\n> 1https://eur-lex.europa.eu/legal-content/NL/TXT/?uri=CELEX:32024R1689\n\nAI Act Guide  | Step 1  4\n\n# Step 1. (Risk) Is our (AI) system covered by one \n\n# of the risk categories? \n\nAll AI systems are subject to the AI Act, but depending on the risk, different requirements are imposed \n\non different categories of system. The risk is determined by the intended application or the product for \n\nwhich the AI system is being developed, sold and used: \n\nâ€¢ Prohibited AI practices:  these AI systems may not be placed on the market, put into service for used \n\nfor certain practices. 2\n\nâ€¢ High-risk AI systems:  these AI systems must satisfy a number of requirements to mitigate the risks \n\nbefore they may be placed on the market or used. 3\n\nOther requirements will also apply to AI models and AI systems capable of performing certain tasks: \n\nâ€¢ General purpose AI models and systems : these models and systems will be subject to specific \n\ninformation requirements. In certain cases, other requirements must be complied with in order to \n\nmitigate risks. 4\n\nâ€¢ Generative AI and chatbots : these applications will be subject to specific transparency requirements \n\ndepending on whether the system is or is not a high-risk system. 5\n\nThe same AI system can sometimes be covered by multiple categories. A chatbot, for example, can be \n\ndeployed for a high-risk application and/or based on a general purpose AI model. AI systems not covered \n\nby any of the categories described above are not required to comply with the requirements from the \n\nAI  Act. Nevertheless, you must remember that AI systems and the development of AI systems may also \n\nbe required to comply with requirements from other regulations such as the General Data Protection \n\nRegulation (GDPR). \n\nTo determine whether you are required to comply with requirements from the AI Act, it is important to \n\nfirst identify the category that covers your AI system. Below we discuss the different risk categories in \n\nmore detail. \n\n## 1.1.  Prohibited AI systems \n\nCertain AI practices bring about an unacceptable risk for people and society. These practices have \n\ntherefore been prohibited since February 2025. This means that these systems may not be placed on \n\nthe market, put into service or used for these practices. These prohibitions apply both to providers and \n\ndeployers since 2 February 2025 (further explanation is provided under  Step 3. Are we the provider or \n\ndeployer of the AI system? on page 12 ).  \n\n> 2\n\nChapter II, Article 5 AI Act.  \n\n> 3\n\nChapter III, Article 6 through to 49 AI Act.  \n\n> 4\n\nChapter V, Article 51 through to 56 AI Act.  \n\n> 5\n\nChapter IV, Article 50 AI Act. Specific transparency requirements will also apply for emotion recognition and biometric \n\ncategorisation systems. As these systems are also high-risk AI systems, they will also have to adhere to the requirement for \n\nthese systems, as described in step 4.2. AI Act Guide  | Step 1  5\n\nProhibited AI systems 6\n\n1.  Systems intended to  manipulate human behaviour  with a view to restricting the free choice of \n\nindividuals and which can result in significant harm to those persons. \n\n2.  Systems which  exploit the vulnerabilities  of persons due to their age, disability or a specific social \n\nor economic situation and which are likely to cause significant harm to those persons. \n\n3.  Systems that draw up a point system of rewards and punishments based on social behaviour \n\nor personality traits, known as  social scoring , which could lead to detrimental or unfavourable \n\ntreatment. \n\n4.  Prohibition on systems for making  risk assessments to predict the risk of a person committing \n\na criminal offence , based solely on profiling or personality or other traits. \n\n5.  Systems which create or expand  facial recognition databases  through the untargeted  scraping \n\nof facial images from the internet or CCTV footage. \n\n6.  Systems for  emotion recognition  in the workplace and in education institutions, except where \n\nintended for medical or safety reasons. \n\n7.  Systems used for categorising  individual persons using biometric categorisation systems  in \n\ncertain sensitive categories such as race and sexual orientation. \n\n8.  The use of real-time remote biometric identification systems in publicly accessible spaces \n\nfor the purposes of law enforcement.  There are a number of exceptions in cases in which use \n\nis strictly necessary, for example when searching for specific victims of obduction, trafficking in \n\nhuman beings or missing persons. These applications are subject to additional guarantees. \n\n## 1.2.  High-risk AI systems \n\nHigh-risk AI systems may result in risks to health, safety or fundamental rights of natural persons, such \n\nas the right to privacy and the right not to be discriminated. At the same time, these systems can also \n\nhave positive impact on natural persons and organisations, if they are reliable and the risks are mitigated. \n\nAgainst that background, from August 2026 onwards, high-risk AI systems must comply with a variety \n\nof requirements before being placed on the market, used or put into service. This means that during the \n\ndevelopment of the system,  providers  must ensure that the system satisfies these requirements before \n\nit is first placed on the market or used. A professional party that uses the AI system subject to its personal \n\nresponsibility is considered a  deployer  (explained in more detail under  Step 3. Are we the provider or \n\ndeployer of the AI system? on page 12 ). \n\nDeployers are also subject to obligations with the aim of mitigating risks resulting from the specific use of \n\nthe system. There are two types of high-risk AI systems: \n\nâ€¢ High-risk products : AI systems that are directly or indirectly also subject to a selection of  existing \n\nproduct regulations  (see below). For example an AI system as a safety component of a lift or an AI \n\nsystem that in and of itself is a medical device. \n\nâ€¢ High-risk applications : AI systems developed and deployed for specific applications in â€˜high-risk \n\napplication areasâ€™. These are eight application areas for AI that range from AI for law enforcement to AI \n\nin education. Within those eight areas, around 30 different specific applications have been identified \n\nthat result in high risks, such as AI systems that support the deployment of emergency first response \n\nservices. \n\nThe product groups and application areas in which AI systems are categorised as high-risk appear in the \n\nfigures below. \n\nThe obligations for this category also apply to high-risk products as from 2 August 2027 and to high-risk \n\napplication areas as from 2 August 2026. The obligations are described under  4.2. High-risk AI on page 13 . \n\n> 6Article 5 AI Act, see also Commission Guidelines on prohibited artificial intelligence practices.\n\nAI Act Guide  | Step 1  6\n\nHigh-risk AI as (safety element of) existing products \n\nThese are products already regulated within the EU. A product is considered as representing a risk if \n\nin accordance with existing product regulations, third-party approval is required before the product \n\ncan be placed on the market (conformity assessment). If AI is a safety-related component of the risk \n\nproduct or if the risk product itself is an AI system, it is considered as high-risk AI. This applies to \n\nproducts covered by the following product legislation: 7\n\nâ€¢ Machines  (Directive 2006/42/EC) \n\nâ€¢ Toys  (Directive 2009/48/EC) \n\nâ€¢ Recreational craft  (Directive 2013/53/EU) \n\nâ€¢ Lifts  (Directive 2014/33/EU) \n\nâ€¢ Equipment and protective systems intended for use in potentially explosive atmospheres \n\n(Directive 2014/34/EU) \n\nâ€¢ Radio equipment  (Directive 2014/53/EU) \n\nâ€¢ Pressure equipment  (Directive 2014/68/EU) \n\nâ€¢ Cableway installations  (Regulation (EU) 2016/424) \n\nâ€¢ Personal protective equipment  (Regulation (EU) 2016/425) \n\nâ€¢ Appliances burning gaseous fuels  (Regulation (EU) 2016/425) \n\nâ€¢ Medical devices  (Regulation (EU) 2017/745) \n\nâ€¢ In-vitro diagnostic medical devices  (Regulation (EU) 2017/746) \n\nIn addition, the AI Act contains a further list of products also considered as high-risk AI, but which \n\nare not subject to any direct requirements under the AI Act. Nevertheless, at a later moment, the \n\nrequirements from the AI Act will be used to clarify the specific product legislation applicable to \n\nthese products. It is not yet known when this will take place, and it will differ from product to \n\nproduct. The products in question are subject to the following product legislation: 8\n\nâ€¢ Civil aviation security  (Regulation (EC) 300/2008 and Regulation (EU) 2018/1139) \n\nâ€¢ Two or three-wheeled vehicles and quadricycles  (Regulation (EU) 168/2013) \n\nâ€¢ Agricultural and forestry vehicles  (Regulation (EU) 167/2013) \n\nâ€¢ Marine equipment  (Directive 2014/90/EU) \n\nâ€¢ Interoperability of the railway system in the EU  (Directive (EU) 2016/797) \n\nâ€¢ Motor vehicles and trailers  (Regulation (EU) 2018/858 and Regulation (EU) 2019/2144) \n\nHigh-risk application areas \n\nAn AI system is within the scope of one of the high-risk application areas if the provider intended the \n\nuse of the AI system in one of these areas. In the documentation of the AI system, the provider must \n\nexplicitly state the purpose, including the instructions for use, advertising materials and any other \n\ntechnical documentation.  Note:  Even if the provider did not intend the AI system as being high-risk \n\nwhen it was placed on the market, it may still be that in practice, a deployer does use the system for \n\none of the high-risk application areas. In that case, the deployer is seen as the provider, and as such \n\nbecomes responsible for the requirements imposed on high-risk AI systems. See also  chapter 4.2 .\n\nThere are eight high-risk application areas. This does not mean that all AI systems covered by \n\nthe often abstractly described application areas are necessarily high-risk. A number of specific \n\napplications are listed for each area. 9\n\nTip:  First check whether your AI system is covered by one of the eight application areas and then \n\ndetermine whether your AI system is one of the AI systems described in that category. Only in that \n\ncase are you dealing with a high-risk AI system that must comply with the requirements.    \n\n> 7Article 6(1) and Annex I, Section A AI Act.\n> 8Article 2(2) and Annex I, Section B AI Act.\n> 9Annex III AI Act.\n\nAI Act Guide  | Step 1  7\n\n1.  Biometrics \n\nâ€¢ Remote biometric identification systems, unless the system is only used for verification. \n\nâ€¢ Systems used for biometric categorisation according to sensitive or protected attributes. \n\nâ€¢ Systems for emotion recognition. \n\n2.  Critical infrastructure \n\nâ€¢ Systems intended to be used as safety components for the management and operation of critical \n\ndigital infrastructure, road traffic or in the supply of water, gas, heating or electricity. \n\n3.  Education and vocational training \n\nâ€¢ Systems for admission to or allocation of (vocational) education. \n\nâ€¢ Systems for evaluating learning outcomes. \n\nâ€¢ Systems for assessing the level of (vocational) education. \n\nâ€¢ Systems for monitoring students during tests. \n\n4.  Employment, workersâ€™ management and access to self employment. \n\nâ€¢ Systems for the recruitment or selection of candidates. \n\nâ€¢ Systems to be used to make decisions affecting terms of work-related relationships, the allocation \n\nof tasks or the monitoring and evaluation of workers. \n\n5.  Essential private services and public services and benefits \n\nâ€¢ Systems for evaluating the eligibility to essential public assistance benefits and services. \n\nâ€¢ Systems for evaluating the creditworthiness or credit score of natural persons unless used for the \n\npurposes of detecting financial fraud. \n\nâ€¢ Systems for risk assessment and pricing in relation to life and health insurance. \n\nâ€¢ Systems for evaluating emergency calls and prioritising the dispatch of emergency first response \n\nservices and emergency health care patient triage systems. \n\n6.  Law enforcement \n\nâ€¢ Systems for law enforcement to assess the risk of a natural person becoming the victim of \n\ncriminal offences. \n\nâ€¢ Systems for law enforcement to be deployed as polygraphs or similar tools. \n\nâ€¢ Systems for law enforcement for evaluating the reliability of evidence. \n\nâ€¢ Systems for law enforcement for assessing or predicting the risk of a natural personal offending or \n\nto assess past criminal behaviour of natural persons or groups. \n\nâ€¢ Systems for law enforcement for the profiling of natural persons in the course of detection, \n\ninvestigation or prosecution of criminal offences. \n\n7.  Migration, asylum and border control \n\nâ€¢ Systems for public authorities to be used as polygraphs or similar tools. \n\nâ€¢ Systems for public authorities for assessing a security risk, the risk of irregular migration or a \n\nhealth risk upon entry into a country. \n\nâ€¢ Systems for public authorities to assist in the examination of applications for asylum, visa or \n\nresidence permit, including associated complaints. \n\nâ€¢ Systems for public authorities for detecting, recognising or identifying natural persons, with the \n\nexception of the verification of travel documents. \n\n8.  Administration of justice and democratic processes \n\nâ€¢ Systems to be used by a judicial authority to assist in applying the law and resolving disputes and \n\nresearching and interpreting facts and applying the law to a concrete set of facts. \n\nâ€¢ Systems for influencing the outcome of an election or referendum or the voting behaviour \n\nof natural persons, with the exception of tools used to support political campaigns from an \n\nadministrative or logistic point of view. AI Act Guide  | Step 1  8\n\nExceptions to high-risk application areas \n\nThere are a number of specific exceptions in which AI systems are covered by one of the application \n\nareas but which are not seen as high-risk AI. This applies where there is no significant risk to health, \n\nsafety or fundamental human rights. This for example applies if an AI system has  no significant \n\nimpact on the outcome of a decision,  for example because the system is intended for :10 \n\nâ€¢ Performing a narrow procedural task; \n\nâ€¢ Improving the result of a previously completed human activity; \n\nâ€¢ Detecting decision-making patterns or deviations from prior decision-making patterns and not \n\nmeant to replace or influence the previously completed human assessment; \n\nâ€¢ Performing a preparatory task to an assessment relevant to one of the high-risk application areas. \n\nIt should also be noted that an AI system used for profiling natural persons cannot make use of \n\nthis exception. If you have determined that your (non-profiling) AI system is subject to one of the \n\nexceptions, you must record this fact and register the AI system in the EU database for high-risk AI \n\nsystems. 11  At a later moment, the European Commission will draw up a list of examples to clarify \n\nwhat is and what is not covered by the exceptions. \n\n## 1.3.  General purpose AI models and AI systems \n\nAn AI model is an essential component of an AI system, but is not an AI system in and of itself. This \n\nrequires more elements, for example a user interface. 12 \n\nA general purpose AI model  (General Purpose AI) can successfully perform a wide range of different \n\ntasks and as such can be integrated in a variety of AI systems. These models are often trained on large \n\nvolumes of data using self-supervision techniques. 13 \n\nThe broad deployability of these models via specific AI systems means that they are used for a wide \n\nrange of applications. These can include high-risk applications. Due to the potential large impact of these \n\nmodels, from August 2025 onwards, they must comply with various requirements. \n\nIf an AI system is based on a general purpose AI model which itself can actually serve multiple purposes, \n\nthen it is a  general purpose AI system. 14 \n\nThe obligations applicable to this category apply from 2 August 2025 and are described under \n\n4.3. General  purpose AI models and systems on page 18 .     \n\n> 10 Article 6(3) AI Act.\n> 11 Article 6(4) AI Act.\n> 12 Consideration 97 AI Act.\n> 13 Article 3(63) AI Act.\n> 14 Article 3(66) AI Act.\n\nAI Act Guide  | Step 1  9\n\n## 1.4.  Generative AI and Chatbots \n\nCertain AI systems are subject to transparency obligations. 15  These are systems with which natural \n\npersons often interact directly. It must therefore be clear to these natural persons that they are \n\ninteracting with AI or that the content has been manipulated or generated. \n\nâ€¢ Systems used for generating audio, images, video or text ( generative AI ); \n\nâ€¢ Systems made for interaction ( chatbots ). \n\nThe obligations applicable to this category apply from 2 August 2026 and are described under \n\n4.4. Generative AI  and Chatbots on page 19 .\n\n## 1.5.  Other AI \n\nSee  4.5. Other AI on page 20  for more information on AI systems not covered by one of the risk \n\ncategories described above.  \n\n> 15 Article 50 AI Act.\n\nAI Act Guide  | Step 2  10 \n\n# Step 2. Is our system â€˜AIâ€™ classified according to \n\n# the AI Act? \n\nThe AI Act imposes regulations on AI systems. There are different ideas about what AI is and what is not \n\nAI. The AI Act offers the following definition, which is intended to demarcate the nature of AI as a product \n\non the market: \n\nâ€œAn AI system means a  machine-based system  that is designed to operate  with varying levels of autonomy  and \n\nthat may exhibit  adaptiveness  after deployment and that, for  explicit or implicit objectives, infers,  from the  input  it \n\nreceives, how to generate  outputs such as predictions, content, recommendations, or decisions  that can influence \n\nphysical or virtual environments .â€ 16 \n\nThese different elements can be present both in the development phase and in the use phase. What is \n\nthe meaning of the terms used in this definition? 17 \n\nâ€¢ Autonomy : This element is satisfied if autonomy is present in a system to a certain extent, even if very \n\nlimited. Systems without any form of autonomy are systems that only operate if human actions or \n\ninterventions are required for all actions by that system. \n\nâ€¢ Adaptiveness:  It is stated that a system â€˜mayâ€™ exhibit adaptiveness after deployment. Although \n\nadaptiveness is therefore not identified as a decisive element, its presence is an indication that the \n\nsystem is an AI system. \n\nâ€¢ It infers how to generate output on the basis of input (capacity to infer):  This relates not only to \n\ngenerating output during the â€˜use phaseâ€™ but also the capacity of an AI system to infer models and/or \n\nalgorithms from data during the development phase. \n\nWhat does this cover? 18 \n\nâ€¢ Systems that make use of  machine learning  in which the system learns how certain objectives can be \n\nachieved on the basis of data. Examples of machine learning are (un)supervised learning, self-super -\n\nvised learning, reinforcement learning and deep learning. \n\nâ€¢ Systems that make use of  knowledge and logic-based approaches  which make learning, reasoning or \n\nmodelling possible on the basis of determined knowledge or a symbolic representation of a task to be \n\nsolved. Examples of these approaches are knowledge representation, inductive (logic) programming, \n\nknowledge bases, inference and deductive engines, (symbolic) reasoning, expert systems and search \n\nand optimisation methods. \n\nWhat is  not  covered? \n\nâ€¢ Systems based on rules laid down exclusively by natural persons to conduct automatic actions. Some \n\nof these systems can to a certain extent derive how to generate output from input received, but are still \n\nbeyond the definition because they are only able to analyse patterns to a limited extent or are unable to \n\nautonomously adapt their output. Examples can be systems for improved mathematical optimisation, \n\nstandard data processing, systems based on classic heuristics and simple prediction systems. \n\nâ€¢ Systems designed to be used with full human intervention. These systems do not operate with a \n\nminimum level of autonomy.  \n\n> 16\n\nArticle 3(1) AI Act.  \n\n> 17\n\nCommission Guidelines on the definition of an artificial intelligence system.  \n\n> 18\n\nConsideration 12 AI Act. AI Act Guide  | Step 2  11 \n\nThe European Commission has issued a guideline to further clarify the definition of AI. You can find the \n\nlatest version on this guideline at  ondernemersplein.nl .\n\nIf your system is not considered AI under the AI Act but is covered by one of the risk categories, it is \n\nimportant to hold a discussion within your organisation about the extent to which the system still \n\nrepresents risks and to mitigate these risks by complying with (specific) requirements from the AI Act. \n\nSystems beyond the scope of the AI Act may nevertheless still have to comply with requirements from \n\nother legislation and regulations. AI Act Guide  | Step 3  12 \n\n# Step 3. Are we the provider or deployer of the \n\n# AI system? \n\nOnce you have determined the risk category that covers you AI system and whether your AI system is in \n\nfact subject to the AI Act, you must then determine whether you are the provider or deployer. \n\nâ€¢ Provider : a person or organisation that develops or commissions the development of an AI system or \n\nmodel and places it on the market or puts the AI system into service. 19 \n\nâ€¢ Deployer : a person or organisation using an AI system under its personal authority. This does not \n\ninclude non-professional use. 20 \n\nThe description of the requirements in step 4 describes for each risk category which obligations apply \n\nto providers and deployers. Each is required to comply with other obligations. The strictest obligations \n\napply to providers. \n\nNote:  As deployer, in certain cases you can also become provider of a high-risk AI system such that you \n\nare required to comply with the high-risk obligations for providers. 21  This is further explained in steps \n\n4.2. High-risk  AI on page 13  and  4.3. General purpose AI models and systems on page 18 .\n\nNote : the AI Act also includes other roles such as authorised representative, importer and distributor. 22 \n\nThe obligations governing these actors are not discussed in this guide.  \n\n> 19\n\nArticle 3(3) AI Act.  \n\n> 20\n\nArticle 3(4) AI Act.  \n\n> 21\n\nArticle 25 AI Act.  \n\n> 22\n\nArticle 3(5), (6) and (7), Articles 22, 23 and 24. AI Act Guide  | Step 4  13 \n\n# Step 4. What obligations must we comply with? \n\n## 4.1.  Prohibited AI practices \n\nThese AI practices result in unacceptable risk and have therefore been prohibited since 2 February  2025. \n\nThis means that AI systems cannot be placed on the market or used for these practices. These prohibitions \n\napply to both  providers  and  deployers. 23 \n\nThere are sharply demarcated exceptions to the prohibition on the use of real-time remote biometric \n\nidentification systems in publicly accessible spaces for the purposes of law enforcement, and the use of \n\nthose systems must be provided with a basis in national legislation. There are also additional guarantees \n\nrelating to the deployment of these systems. \n\n## 4.2.  High-risk AI \n\nThe majority of requirements from the AI Act will apply to high-risk AI systems.  Providers  must comply \n\nwith various obligations such as: 24 \n\nâ€¢ System for risk management; \n\nâ€¢ Data and data governance; \n\nâ€¢ Technical documentation; \n\nâ€¢ Record-keeping (logs); \n\nâ€¢ Transparency and information; \n\nâ€¢ Human oversight; \n\nâ€¢ Accuracy, robustness and cybersecurity; \n\nâ€¢ Quality management system; \n\nâ€¢ Monitoring. \n\nIf you as provider comply or believe you comply with all these obligations, you will be required to \n\nconduct a  conformity assessment . In certain cases you can conduct this assessment yourself, while \n\nin other cases it must be conducted by a third party on your behalf. 25  A future version of this guide will \n\nexplain in more detail when you are required to conduct which procedure. \n\nDeployers  must also comply with various obligations. Additional obligations apply to government \n\norganisations using AI systems. 26 \n\nEach obligation is explained in the figure below. These obligations will be further elaborated over the \n\ncoming years in European standards. Participation will be organised via the standardisation institutes of \n\nthe European Member States. In the Netherlands this is the  NEN 27 .\n\nNote:  In two cases, as deployer of a high-risk AI system, you yourself can become the provider of \n\nthat system: 28 \n\nâ€¢ When you as deployer place your own name or trademark on the high-risk system; \n\nâ€¢ When you as deployer make a substantial modification to the high-risk AI system that was not \n\nintended by the provider as a consequence of which the system no longer complies with the require -\n\nments or as a consequence of which the purpose of the system intended by the provider changes. This \n\nlatter situation arises if you make use of an AI system that was not intended for high-risk applications, \n\nfor any such applications.  \n\n> 23\n\nArticle 5 AI Act.  \n\n> 24\n\nArticle 16 and Section 2 of Chapter III (Articles 8-15) AI Act.  \n\n> 25\n\nArticle 43 AI Act.  \n\n> 26\n\nArticles 26 and 27 AI Act.  \n\n> 27\n\nhttps://www.nen.nl/ict/digitale-ethiek-en-veiligheid/ai . \n\n> 28\n\nArticle 25 AI Act. AI Act Guide  | Step 4  14 \n\n## Requirements for high-risk AI systems \n\n1.  System for risk management 29 \n\nVarious steps must be taken for this system: \n\nâ€¢ Identification and analysis of foreseeable risks the system can pose to health, safety or \n\nfundamental rights. \n\nâ€¢ Taking suitable risk management measures to ensure that the risks that remain following \n\nimplementation of these measures are acceptable. \n\nThe following points must be taken into account: \n\nâ€¢ The risks must be identified and dealt with before the AI system is placed on the market or used \n\nand subsequently continuously during the use of the AI system. \n\nâ€¢ Foreseeable abuse of the system must be taken into account. \n\nâ€¢ The context of the use, including the knowledge and experience of the deployer of such AI \n\nsystems or the fact that children and vulnerable groups may experience consequences of the \n\nAI system, must be taken into account. It may for example be necessary to offer training to the \n\npeople working with the AI system. \n\nâ€¢ The risk management measures must be tested to check that they are actually effective. \n\nThis must be carried out on the basis of benchmarks appropriate to the purpose for which the \n\nAI system is deployed. \n\nâ€¢ If a risk management system must also be established pursuant to existing product legislation, \n\nthis may be combined to form a single risk management system. \n\n2.  Data and data governance 30 \n\nDifferent requirements are imposed on the datasets used for training, validating and testing \n\nhigh-risk AI systems. \n\nâ€¢ Data management appropriate to the purpose of the AI system, including: \n\nâ€¢ The registration of the processes, including processes for data gathering and data processing; \n\nâ€¢ The recording of assumptions about the datasets; \n\nâ€¢ An assessment of the availability, quantity and suitability of the datasets including possible \n\nbiases that could have consequences for natural persons; \n\nâ€¢ Measures for tracing, preventing and mitigating biases; \n\nâ€¢ Tackling shortcomings in the datasets that may prevent compliance with the AI Act (for example \n\nmitigating risks according to the risk management system). \n\nâ€¢ For the purpose for which they are used, datasets must be sufficiently representative and as far \n\nas possible error-free. The context in which the AI system is to be used must also be taken into \n\naccount; for example the geographical or social context. \n\nâ€¢ Subject to a number of strict conditions, special categories of personal data (a term from the \n\nGeneral Data Protection Regulation) may be processed as a means of tackling bias.   \n\n> 29 Article 9 AI Act.\n> 30 Article 10 AI Act.\n\nAI Act Guide  | Step 4  15 \n\n3.  Technical documentation 31 \n\nThe technical documentation must demonstrate that the high-risk AI system complies with the \n\nrequirements laid down in the AI Act. The technical documentation must among others include: \n\nâ€¢ A general description of the AI system including the intended purpose of the system, the name of \n\nthe provider and instructions for use; \n\nâ€¢ A detailed description of the elements of the AI system and of the development process for that \n\nsystem, including the steps in development, the design choices, the expected output from the \n\nsystem, the risk management system and the datasets used; \n\nâ€¢ Detailed information about the monitoring, operation and control of the AI system, including the \n\ndegree of accuracy at individual and general level, risks, the system for evaluation during use and \n\nmeasures for monitoring and human oversight; \n\nâ€¢ An overview of the applicable standards; \n\nâ€¢ The EU conformity declaration (the CE mark). \n\nSME enterprises can record their technical documentation in a simplified manner. At a future \n\nmoment, the European Commission will issue a relevant form. \n\n4.  Record-keeping (logs) 32 \n\nAutomatic logs must be retained during the lifecycle of the AI system so that risks can be traced in a \n\ntimely manner and the operation of the system can be monitored. \n\nThese logs must be stored for at least six months. At least the following events must be recorded: \n\nâ€¢ The duration of each use of the AI system; \n\nâ€¢ The input data and the control of that data by the AI system (and the reference database); \n\nâ€¢ The identification of the natural persons involved in the verification of the results. \n\n5.  Transparency and information 33 \n\nThe provider of the AI system knows how the system operates and how it should be used. For that \n\nreason, the provider must ensure that the AI system is sufficiently transparent so that deployers \n\nunderstand how they can correctly make use of the output from the system. \n\nWith this in mind,  instructions for use  must be drawn up, that include at least the following points: \n\nâ€¢ Contact details; \n\nâ€¢ The purpose, characteristics, capacities and limitations of the performance of the AI system; \n\nâ€¢ The measures for human oversight. \n\n6.  Human oversight 34 \n\nHigh-risk AI systems must be designed by the provider in such a way that during use they can be \n\neffectively overseen by natural persons in order to mitigate the risks for natural persons. Human \n\noversight shall be context dependent â€“ the greater the risks, the stricter the oversight must be. \n\nThe measures for oversight may be technical in nature (for example a clear human-machine \n\ninterface), or measures that must be undertaken by the deployer (for example a compulsory course \n\nfor their personnel).     \n\n> 31 Article 11 AI Act.\n> 32 Article 12 AI Act.\n> 33 Article 13 AI Act.\n> 34 Article 14 AI Act.\n\nAI Act Guide  | Step 4  16 \n\nThe eventual objective of these measures is to ensure that the natural persons who use the \n\nAI system are capable of the following: \n\nâ€¢ They understand the capacities of the system and can monitor its functioning; \n\nâ€¢ They are aware of automation bias; \n\nâ€¢ They can correctly interpret and if necessary ignore or replace the output; \n\nâ€¢ They can halt the system. \n\n7.  Accuracy, robustness and cybersecurity 35 \n\nHigh-risk AI systems must offer an appropriate level of accuracy, robustness and cybersecurity. \n\nTo achieve this, benchmarks and measuring methods are developed by the European Commission. \n\nAt least the following measures must be mentioned: \n\nâ€¢ Technical and organisational measures to prevent errors that occur in interaction between the AI \n\nsystem and natural persons; \n\nâ€¢ Solutions for robustness such as backups or security measures in the event of defects; \n\nâ€¢ Removing or mitigating negative influencing of the system by limiting feedback loops; \n\nâ€¢ Cybersecurity that prevents unauthorised third-party access by tracing, responding to and dealing \n\nwith attacks. These are attacks aimed at data poisoning, model poisoning, adapting input or \n\nobtaining confidential data. \n\n8.  Quality management system 36 \n\nThe quality management system must ensure that the requirements from the AI Act are complied \n\nwith. How extensive the quality management system must be will depend on the size of the \n\norganisation. For example by documenting the following: \n\nâ€¢ A strategy for compliance; \n\nâ€¢ Techniques, procedures and measures for the design, development and quality assurance of the \n\nAI system; \n\nâ€¢ Whether standardisation is used; \n\nâ€¢ Systems and procedures for data management, risk management, monitoring, incident reporting \n\nand documentation. \n\n9.  Monitoring 37 \n\nAs soon as an AI system has been placed on the market or is in use, providers must monitor the \n\nsystem on the basis of use data, thereby determining whether the system continues to comply with \n\nthe requirements from the AI Act. For this purpose, providers must draw up a monitoring plan. \n\nIf the provider of a high-risk AI system discovers that the system no longer functions in compliance \n\nwith the AI Act, corrective measures must be taken immediately to correct the situation. This may \n\neven include recalling the system if necessary. The provider must also work alongside the deployer \n\nand duly inform the surveillance authorities. \n\nSerious incidents involving the AI system must be reported to the surveillance authorities. 38     \n\n> 35 Article 15 AI Act.\n> 36 Article 17 AI Act.\n> 37 Article 72 AI Act.\n> 38 Article 73 AI Act.\n\nAI Act Guide  | Step 4  17 \n\nOther requirements \n\nâ€¢ The registration of the high-risk AI system in the EU database. 39 \n\nâ€¢ The contact details of the provider must be registered with the AI system. 40 \n\nâ€¢ The technical documentation, documentation concerning the quality management system and \n\ndocumentation concerning the conformity assessment must be kept for 10 years. 41 \n\nObligations for deployers of high-risk AI systems \n\nNot only providers but also the deployers of high-risk AI systems are subject to obligations. After \n\nall they are the parties who control how the AI system is used in practice and as such have a major \n\nimpact on the risks that can occur. \n\nDeployers must: 42 \n\nâ€¢ Take appropriate technical and organisational measures to ensure that the high-risk AI system is \n\nused in accordance with the instructions for use; \n\nâ€¢ Assign human oversight to natural persons who have the necessary competence, training and \n\nauthority; \n\nâ€¢ Ensure that the input data is relevant and sufficiently representative, wherever possible; \n\nâ€¢ Monitor the operation of the AI system on the basis of the instructions for use; \n\nâ€¢ If the deployer has reason to believe that the system no longer complies with the requirements \n\nfrom the AI Act, the deployer must duly inform the provider and cease use of the system; \n\nâ€¢ Inform the provider and surveillance authorities of possible risks and serious incidents that have \n\noccurred; \n\nâ€¢ Keep the logbook under their control for at least six months; \n\nâ€¢ Inform worker representation if the AI system is to be deployed on the shop floor; \n\nâ€¢ Duly inform people if decisions are taken about natural persons using the high-risk AI system; \n\nâ€¢ If use is made of AI for emotion recognition or biometric categorisation, the natural persons in \n\nrespect of whom the system is used must be duly informed. \n\nSpecific obligations for government organisations as deployers  In addition to the obligations \n\ndescribed above, government organisations must comply with a number of additional obligations: \n\nâ€¢ Register use of a high-risk system in the EU database; 43 \n\nâ€¢ Assess the potential consequences for fundamental rights if the high-risk AI system is used \n\nwith a view to the specific context within which use takes place (a  fundamental rights impact \n\nassessment ). They will for example consider the duration of use, the processes within which \n\nthe system is used and the potential impact of use on the fundamental rights of natural persons \n\nand groups. Following identification of the risks, deployers must take measures for human \n\noversight and deal with possible risks. A report must also be submitted to the market surveillance \n\nauthorities unless an appeal can be made to an exception based on public safety or protection of \n\nhuman health. 44 \n\nNote:  This obligation also applies to private entities providing public services, the use of AI systems \n\nfor assessing the creditworthiness of natural persons and AI systems for risk assessments for life and \n\nhealth insurance.       \n\n> 39 Article 49 AI Act.\n> 40 Article 16(b) AI Act.\n> 41 Article 18 AI Act.\n> 42 Article 26 AI Act.\n> 43 Article 49(3) AI Act.\n> 44 Article 27 AI Act.\n\nAI Act Guide  | Step 4  18 \n\n## 4.3.  General purpose AI models and systems \n\nObligations for providers of general purpose AI models 45 \n\nGeneral purpose AI models can be integrated in all kinds of different AI systems. It is essential \n\nthat the providers of these AI systems know what the AI model is and is not capable of. Specific \n\nrequirements are also imposed on the training of these models because training often makes use of \n\nlarge datasets. The providers of these models must: \n\nâ€¢ Draw up technical documentation of the model including the training and testing process and the \n\nresults and evaluation; \n\nâ€¢ Draw up and keep up to date information and documentation for providers of AI systems \n\nwho intend to integrate the model in their AI system. The information must provide a good \n\nunderstanding of the capacities and limitations of the AI model and must enable the provider of \n\nthe AI system to comply with the obligations from the AI Act. \n\nâ€¢ Draw up a policy to ensure that they train the model without infringing the copyrights of natural \n\npersons and organisations; \n\nâ€¢ Draw up and publish a sufficiently detailed summary about the content used for training the AI model. \n\nProviders of open source models are not required to comply with the first two obligations (technical \n\ndocumentation and drawing up information for downstream providers). \n\nObligations for providers of general purpose AI models with systemic risks 46 \n\nIn certain cases, general purpose AI models can generate systemic risks. This applies if the model \n\nhas high impact capacity, for example due to the scope of the model or due to (potential) negative \n\nimpact on public health, safety, fundamental rights or society as a whole. This is at least assumed \n\nif at least 10 25  floating point operations (FLOPs) are used to train the model. On the basis of specific \n\ncriteria, the European Commission can also determine that the model has a similar major impact in \n\nsome other way. These models must: \n\nâ€¢ Comply with the obligations for general purpose AI models; \n\nâ€¢ Implement model evaluations to map out the systemic risks; \n\nâ€¢ Mitigate systemic risks; \n\nâ€¢ Record information about serious incidents and report those incidents to the AI Office; \n\nâ€¢ Ensure appropriate cybersecurity. \n\nNote : these obligations only apply to the largest AI models. \n\nProviders of these models with systemic risks cannot appeal to an exception for open source.   \n\n> 45 Article 53 AI Act.\n> 46 Article 55 AI Act.\n\nAI Act Guide  | Step 4  19 \n\nWhat rights do you have if you integrate a general purpose AI model in your (high-risk) AI system? \n\nAs indicated above, you must at least receive information and documentation to enable you to determine \n\nfor yourself how you can make use of the model in your AI system for the chosen purpose. If you include \n\nthe model in a high-risk AI system, as a provider you must then still comply with the obligations from the \n\nAI Act. \n\nHow should you deal with general purpose AI systems?  As indicated in  1.3. General purpose AI models \n\nand AI systems on page 8 , there are also AI systems that can serve multiple purposes. Take for example \n\nthe widely known AI chatbots.  Note:  If you deploy these systems for high-risk purposes, according to the \n\nAI Act you yourself become a provider of a high-risk AI system. 47  It is then up to you to comply with the \n\napplicable obligations. In this situation it is very difficult to comply with the obligations for a high-risk AI \n\nsystem, which means that you may run the risk of receiving a penalty. \n\n## 4.4.  Generative AI and Chatbots \n\nTo ensure that natural persons know whether they are talking to an AI system or are seeing content \n\ngenerated by AI, transparency obligations are imposed on generative AI and chatbots. \n\nRules for providers of chatbots 48 \n\nProviders of systems designed for direct interaction with natural persons must ensure that these \n\nnatural persons are informed that they are interacting with an AI system. \n\nRules for providers of generative AI 49 \n\nProviders of systems that generate audio, image, video or text content must ensure that the output \n\nis marked in a machine readable format so that the output can be detected as artificially generated \n\nor manipulated. \n\nRules for deployers of generative AI 50 \n\nDeployers of systems that generate audio, image or video content must ensure that it is clear that \n\nthe content is artificially generated or manipulated. This can for example be achieved by applying a \n\nwatermark. For creative, satirical, fictional or analogue work, this may be carried out in a way that \n\ndoes not ruin the work. \n\nA special regime applies to artificially generated text. Only in cases where texts are used with the \n\npurpose of â€˜informing the public on matters of public interestâ€™, the fact must be disclosed that the \n\ntext has been artificially generated or manipulated. If there is editorial review and responsibility, this \n\nneed not be carried out. \n\nRules for deployers of emotion recognition systems or systems for biometric categorisation 51 \n\nThe deployers of these AI systems must inform the natural persons exposed to these systems about \n\nhow the system works.      \n\n> 47 Article 25(1)(c) AI Act.\n> 48 Article 50(1) AI Act.\n> 49 Article 50(2) AI Act.\n> 50 Article 50(4) AI Act.\n> 51 Article 50(3) AI Act.\n\nAI Act Guide  | Step 4  20 \n\n## 4.5.  Other AI \n\nAI systems beyond the categories referred to above are not required to comply with requirements \n\naccording to the AI Act. \n\nBut note : if you as  deployer  make use of â€˜other categoryâ€™ AI systems for a high-risk application as \n\nreferred to in the AI Act (see  1.2. High-risk AI systems on page 5 ), it automatically becomes a high-risk AI \n\nsystem and you must comply with the requirements from the AI Act as a system  provider .52  \n\n> 52 Article 25(1)(c) AI Act.\n\nColofon \n\nDit is een uitgave van; \n\nMinisterie van Economische Zaken \n\nPostbus 20901  | 2500 EX Den Haag \n\nwww.rijksoverheid.nl", "fetched_at_utc": "2026-02-08T19:08:51Z", "sha256": "3015a2b513d55c3a2595ac227ceb79dfccae8b256d0a076b69168d4b000aad77", "meta": {"file_name": "Netherlands AI Act Guide.pdf", "file_size": 293902, "relative_path": "pdfs\\Netherlands AI Act Guide.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 9985}}}
{"doc_id": "pdf-pdfs-prohibited-ai-practices-oliver-patel-0436fda42825", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Prohibited AI practices - Oliver Patel.pdf", "title": "Prohibited AI practices - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:04 PM Prohibited AI practices: A compliance guide \n\nhttps://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance  2/18 This free newsletter delivers practical, actionable and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! For more frequent updates, be sure to follow me on LinkedIn .This weekâ€™s edition features: \n\nâœ… Top 5 practical tips for prohibited AI compliance \n\nâœ… What does the EU AI Act say on prohibited AI? \n\nâœ… The European Commissionâ€™s new Guidelines on prohibited AI (140 pages distilled) \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week. \n\nTop 5 practical tips for prohibited AI compliance   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 3/18\n\nMany readers of Enterprise AI Governance are experienced lawyers, compliance executives, and governance leaders seeking practical advice and actionable insights. Therefore, before providing an overview of the prohibited AI practices and the European Commissionâ€™s new Guidelines on the topic, I will frontload this weekâ€™s edition with my practical tips for meeting Article 5 of the AI Act head on. \n\n1. Deliver company-wide communications, awareness, and training campaigns on prohibited AI. AI governance leaders need to constantly remind themselves that although AI is everywhere, the AI Act is still a relatively niche topic. Do not assume that everyone already knows about prohibited AI, just because it is top of mind for us. Indeed, the legal concept of prohibited AI practices is new for us all. Although there are many uses of AI that would of course be illegal, the AI Act is the first EU law which explicitly prohibits specific AI use cases and systems. Therefore, invest ample time in rolling out dedicated training and communications campaigns, so that your organisation is fully aware of these new provisions, their scope, and practical impact. \n\n2. Determine whether prohibited in the EU means prohibited worldwide. And be prepared to justify your decision. A realistic operational scenario that could arise is a team or department in your organisation, based outside of the EU, wanting to use an AI system, which is, or could potentially be, prohibited under   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 4/18\n\nthe AI Act. The use of this AI system is confined to one specific non-EU location. No AI outputs will be used in the EU and no EU employees, customers and stakeholders will be involved in any way. It is feasible that this use case could be perfectly legal in the relevant jurisdiction. Furthermore, perhaps it is not even particularly problematic, from an ethical or cultural standpoint. This raises a key question for global organisations: should prohibited in the EU mean prohibited worldwide? And, if so, what is the justification for this? On the one hand, the EU has prohibited certain AI practices due to their morally egregious nature and/or the potential for harm. However, on the other hand, no jurisdiction has a monopoly on morality. \n\n3. Do not assume that there are no activities in your organisation that breach these provisions. Be wary of the unknown unknowns. You may have a well-established and embedded AI governance process, which captures all new AI projects, initiatives, and vendor applications. It may be that nothing has ever come through which relates to, or closely resembles, any of the prohibited AI practices. Unfortunately, this does not necessarily mean that no such development, deployment, or use of prohibited AI is occurring. Assumptions are dangerous and you donâ€™t know what you donâ€™t know. That is why dedicated training, compliance, and audit exercises are crucial.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 5/18\n\n4. Conduct dedicated compliance initiatives, such as assurance evaluations and audits. In large organisations, where it is difficult to keep track of and monitor everything, there is a need for bespoke compliance initiatives aimed at deterring, preventing, identifying, and halting AI activities which risk falling into the prohibited category. These initiatives are in addition to your BAU AI governance risk assessment processes. Ad hoc mechanisms, such as senior leadership attestation, assurance monitoring and evaluation, and internal and external audits, should all form part of your arsenal. Such initiatives will focus minds, hold leaders to account, and steer behavioural and cultural change in the right direction. \n\n5. Pay close attention to tools which may have emotional recognition capabilities, as there will be plenty of grey areas for companies. Every prohibited AI category has grey areas and exceptions, but perhaps none more so than emotional recognition. For most responsible organisations, it is reasonable to suggest that they are unlikely to develop, deploy, or use AI systems in most of the prohibited AI categories. Emotional recognition could be an outlier, as it is something that could be inadvertently or unintentionlly used, or turned on, in applications designed for recruitment, coaching, training, productivity, and employee and customer engagement or support. Furthermore, it may be that similar tools can lawfully be used to predict and monitor performance, sentiment, or behaviour, but close attention will need to be paid to ensure this does not cross over into emotional recognition. It is also not necessarily a capability which is   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 6/18\n\ndeemed socially, culturally, or morally unacceptable in every jurisdiction worldwide. \n\nBonus tip : without a robust AI governance framework and risk assessment process, which is based upon a solid policy foundation, sponsored by senior leadership, and embedded across the organisation, you will be much less likely to identify and prevent any prohibited AI practices. Always prioritise establishing the fundamental building blocks of AI governance, before turning your attention elsewhere. Article 5 of the EU AI Act may only take up three out of 144 pages, but it is probably the most consequential section. It outlines the AI practices that are now prohibited under EU law. The provisions on prohibited AI practices became applicable on 2nd February 2025, exactly 6 months after the AI Act entered into force on 1st August 2024. The prohibited AI practices are deemed to be particularly harmful, abusive, and contrary to EU values. Breaching these rules carries the largest potential enforcement \n\nWhat does the EU AI Act say on prohibited AI?   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 7/18\n\nfines of up to 7% of global annual turnover. This is a highly dissuasive figure which should focus minds in the boardroom. Below, I will provide a digestible summary of the eight categories of prohibited AI practice. For some of these, there are specific and limited exceptions, which must be carefully considered and reviewed on a case-by-case basis. For example, the practices of predictive policing and the use of emotion recognition systems are not outright prohibited in every scenario. Also, some of these categories proved highly contentious during the AI Actâ€™s legislative process and trilogue negotiations. In particular, the question of whether law enforcement authorities should be permitted to use AI systems for real-time remote biometric identification in public was totemic. The European Parliament argued for an outright prohibition, whereas member states wanted their authorities to retain such capabilities. A compromise between these two positions was eventually struck. Now the dust has settled, it is prohibited, under EU law, to sell, make available, or use AI systems for any of the practices listed below. \n\nCausing harm by deploying subliminal or deceptive techniques   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 8/18\n\nAI systems which deploy subliminal, manipulative, or deceptive techniques in order to distort the behaviour of an individual or group, in a way which causes, or is likely to cause, significant harm. This can include persuading or manipulating people to engage in unwanted behaviours. This practice is prohibited even if there was no intention to cause harm. \n\nCausing harm by exploiting vulnerabilities \n\nAI systems which exploit the vulnerabilities of an individual or group (e.g., age, disability, or socioeconomic status) to distort that individual or groupâ€™s behaviour in a way which causes, or is likely to cause, significant harm. This practice is also prohibited even if there was no intention to cause harm. \n\nSocial credit scoring systems \n\nAI systems which evaluate and score people based on social behaviour and personal characteristics, with the score leading to detrimental or unfavourable treatment which is disproportionate, unjustified, and/or unrelated to the context of the original data. Such AI systems can lead to discrimination, marginalisation, and social exclusion.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 9/18\n\nPredictive policing \n\nAI systems which are used to assess or predict the risk of an individual committing a criminal offence, where the assessment is based solely on automated profiling of their traits and characteristics. It is unlawful to use any of the following traits as the sole basis for assessing and predicting whether someone will commit a crime: nationality, birth location, residence location, number of children, level of debt etc. However, predictive policing AI systems are classified as high-risk (i.e., not prohibited), when they are not based solely on the type of automated profiling described above. \n\nEmotion recognition in the workplace and education \n\nAI systems which infer or detect emotions, based on biometric data (e.g., facial images, fingerprints, or physiological data) in the context of the workplace and educational institutions. This includes emotions like happiness, excitement, sadness, and anger. However, it does not include physical states like pain or fatigue. There are exemptions for emotional recognition systems used for medical or safety purposes. In such permissible scenarios, an emotion recognition system would be classified as high-risk.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 10/18\n\nCreating facial recognition databases via untargeted scraping \n\nAI systems which create or expand databases for facial recognition, via untargeted scraping of facial images from the internet or CCTV footage. \n\nBiometric categorisation to infer specific protected characteristics \n\nAI systems which categorise individuals based on biometric data, to infer protected characteristics like race, political opinions, trade union membership, religious beliefs, sexual orientation, or sex life. If biometric categorisation systems are used to infer characteristics or traits which are not protected, then those AI systems are classified as high-risk. \n\nLaw enforcement use of real-time biometric identification in public \n\nLaw enforcement use of â€™real-timeâ€™ remote biometric identification systems in public (e.g., facial recognition used to identify and stop potential suspects in public) is prohibited, apart from in very specific and narrowly defined scenarios. This prohibition is intended to safeguard privacy and prevent discrimination. Acceptable scenarios for leveraging AI in this way include searching for victims of serious crime, preventing imminent threats to life (e.g., terrorist attacks), or locating suspects or perpetrators of serious crimes (e.g., murder).   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 11/18\n\nHowever, independent judicial or administrative authorisation must be granted, before such AI systems are used. The use must only occur for a limited time period, with safeguards. National regulators and data protection authorities must be notified each time an AI system is used in this way. The European Commission will publish an annual report tracking and documenting these use cases. The development and placing on the market of AI systems intended for this purpose is, however, not prohibited. But there are prohibitions on the use of such systems. Last week, the European Commission published Draft Guidelines on prohibited AI practices. These shed light on how organisations can interpret, practically implement, and comply with these important provisions. The guidelines are also designed to assist regulatory authorities reviewing such cases, although those authorities are not legally obliged to take these guidelines into account. \n\nThe European Commissionâ€™s new Guidelines on prohibited AI   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 12/18\n\nAt 140 pagesâ€”which is not too dissimilar to the overall length of the EU AI Actâ€”we get a glimpse of the task ahead. To be facetious, if the EU keeps up its rate of 140 pages of guidance for every 3 pages of legal text, then AI governance professionals may have over 6700 pages of overall text which they will need to understand, in order to fully grasp this new law. On a more serious note, this highlights that over the coming months and years, there will be a huge amount of additional documentation and analysis, such as official guidelines, recommendations, opinions, standards, codes of practice, enforcement decisions, and court judgements, which we will all need to keep up with. Luckily, youâ€™ve come to the right place ðŸ˜‰ \n\nSummary of key points \n\nRegulatory authorities should use these guidelines in relevant cases and investigations. However, despite the various examples provided, there will always need to be a detailed, case-by-case assessment of the AI system in question. Although much enforcement of Article 5 will be actioned by regulators at the member state level, they must keep the Commission and other regulators informed regarding any cases with cross-border impact. Furthermore, regulatory authorities should strive for harmonisation on prohibited AI enforcement, via collaboration at the European AI Board.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 13/18\n\nThe prohibitions apply to any AI system, whether it has a narrow â€˜intended purposeâ€™ which is prohibitedâ€™, or whether it is a general-purpose AI system used in a manner which is prohibited. Deployers must therefore ensure that they do not use or customise general-purpose AI systems in a prohibited way. There are other scenarios, under EU law, where the use of an AI system could be prohibited, even if this is not explicitly stipulated in the AI Act. For example, there are other conceivable uses of AI which could breach the EUâ€™s Charter of Fundamental Rights. \n\nExample of potential prohibited scenarios for each category \n\nIn the guidelines, the European Commission provided some illustrative examples of what could be prohibited, across each category. I have provided a selection below. \n\nCausing harm by deploying subliminal or deceptive techniques \n\nAn AI companionship app imitates how humans talk, act, and respond. It uses human-like traits and emotional signals to affect how users feel and think. This can make people emotionally attached to the app, leading to addiction-like behavior. In some cases, this might cause serious problems, like suicidal thoughts or even harm to others.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 14/18\n\nCausing harm by exploiting vulnerabilities \n\nAn AI chatbot targets and radicalises socio-economically disadvantaged people, encouraging them to harm or injure other people, by tapping into their fears, vulnerabilities, and sense of social exclusion. \n\nSocial credit scoring systems \n\nA national labour agency uses AI to determine whether unemployed people should receive state employment benefits. As part of the scoring process, data is used and relied upon which is unrelated to the purpose of the evaluation, such as marital status, health problems, or addictions. \n\nPredictive policing \n\nA law enforcement authority uses AI to predict and determine that an individual is more likely to commit a terrorism offence, based solely on personal characteristics like their age, nationality, address, type of car, and marital status. \n\nEmotion recognition in the workplace and education \n\nInferring emotions from written text (e.g., sentiment analysis) is not emotion recognition. However, inferring emotions from keystrokes, facial expressions, body postures, and movement is emotional recognition, as it is based on biometric data.   \n\n> 1/3/26, 5:04 PM Prohibited AI practices: A compliance guide\n> https://oliverpatel.substack.com/p/prohibited-ai-practices-a-compliance 15/18\n\nCreating facial recognition databases via untargeted scraping \n\nA facial recognition company collects photos of people's faces using an automated tool that searches social media platforms. It gathers these images along with other data, such as the image's URL, location data, and the person's name. The company then processes the photos to extract facial features and converts them into mathematical data for easy storage and comparison. When someone uploads a new photo to their facial recognition system, it is checked for any matches in the facial database. \n\nBiometric categorisation to infer specific protected characteristics \n\nAn AI system which categorises individualsâ€™ social media profiles based on their assumed sexual orientation, which is predicted by analysing biometric data from their photos, is prohibited. \n\n## ICYMI: get the full 17-page report which compares AI laws in the EU, China and U.S.A ðŸ‘‡ðŸ¼", "fetched_at_utc": "2026-02-08T19:08:53Z", "sha256": "0436fda428252583733cac243d6f4faed26dd1fea581aff5727ec713479bdc02", "meta": {"file_name": "Prohibited AI practices - Oliver Patel.pdf", "file_size": 916744, "relative_path": "pdfs\\Prohibited AI practices - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 3789}}}
{"doc_id": "pdf-pdfs-proposal-digital-omnibus-c10497b01d38", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Proposal Digital Omnibus.pdf", "title": "Proposal Digital Omnibus", "text": "EN EN \n\nEUROPEAN \n\nCOMMISSION \n\nBrussels, 19.11.2025 \n\nCOM(2025) 836 final \n\n2025/0359 (COD) \n\nProposal for a \n\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL \n\namending Regulations (EU) 2024/1689 and (EU) 2018/1139 as regards the simplification \n\nof the implementation of harmonised rules on artificial intelligence (Digital Omnibus on \n\nAI) \n\n{SWD(2025) 836 final} \n\n(Text with EEA relevance) EN 1 EN \n\nEXPLANATORY MEMORANDUM \n\n1. CONTEXT OF THE PROPOSAL \n\nâ€¢ Reasons for and objectives of the proposal \n\nIn its Communication on a Simpler and Faster Europe ( 1), the Commission announced its commitment to an ambitious programme to promote forward-looking, innovative policies that strengthen the European Unionâ€™s (EU) competitiveness and lighten the regulatory burdens on people, businesses and administrations, while maintaining the highest standard in promoting its values. \n\nRegulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence (â€˜AI Actâ€™), which entered into force on 1 August 2024, establishes a single market for trustworthy and human-centric artificial intelligence (â€˜AIâ€™) across the EU. Its purpose is to promote innovation and the uptake of AI while ensuring a high level of protection for health, safety, and fundamental rights, including democracy and the rule of law. \n\nThe AI Actâ€™s entry into application occurs in stages, with all rules entering into application by 2 August 2027. The prohibitions on AI practices with unacceptable risks and the obligations for general-purpose AI models are already applicable. However, most provisions â€“ in particular those governing high-risk AI systems â€“ will only start to apply from 2 August 2026 or 2 August 2027. These provisions include detailed requirements for data governance, transparency, documentation, human oversight, and robustness, so as to ensure that AI systems placed on the EU market are safe, transparent, and reliable. \n\nThe Commission is committed to a clear, simple, and innovation-friendly implementation of the AI Act, as set out in the AI Continent Action Plan (2) and the Apply AI Strategy (3). Initiatives such as the General-Purpose AI Code of Practice, Commission guidelines and templates, the AI Pact and the launch of the AI Act Service Desk build clarity regarding the applicable rules and support for their application. In particular, the website through which the AI Act Service Desk is provided offers a single information platform ( 4) on all resources available to stakeholders to navigate the AI Act, including guidelines, national authorities and support initiatives, webinars, and harmonised standards. These efforts will continue, with further guidance and digital tools under preparation. \n\nBuilding on experience gained from the implementation of already applicable provisions, the Commission held a series of consultations, including a public consultation to identify potential challenges with implementing the AI Actâ€™s provisions, a call for evidence in preparation of the Digital Omnibus, a reality check allowing stakeholders to directly share their implementation experiences and an SME panel to identify their particular needs in the implementation of the AI Act.     \n\n> 1COM(2025) 47 final.\n> 2COM(2025)165 final.\n> 3COM(2025) 723 final.\n> 4https://ai-act-service-desk.ec.europa.eu/\n\n# EN 2 EN \n\nThese consultations reveal implementation challenges that could jeopardise the effective entry into application of key provisions of the AI Act. These include delays in designating national competent authorities and conformity assessment bodies, as well as a lack of harmonised standards for the AI Actâ€™s high-risk requirements, guidance, and compliance tools. Such delays risk significantly increasing the compliance costs for businesses and public authorities and slowing down innovation. \n\nTo address these challenges, the Commission is proposing targeted simplification measures \n\nto ensure timely, smooth, and proportionate implementation of certain of the AI Actâ€™s provisions. These include: \n\nâ€¢ linking the implementation timeline of high-risk rules to the availability of standards or other support tools; \n\nâ€¢ extending regulatory simplifications granted to small and medium-sized enterprises (SMEs) to small mid-caps (SMCs), including simplified technical documentation requirements and special consideration in the application of penalties; \n\nâ€¢ requiring the Commission and the Member States to foster AI literacy instead enforcing unspecified obligation on providers and deployers of AI systems in this respect, while training obligations for high-risk deployers remain; \n\nâ€¢ offering more flexibility in the post-market monitoring by removing aprescription of a harmonised post-market monitoring plan; \n\nâ€¢ reducing the registration burden for providers of AI systems that are used in high-risk areas but for which the provider has concluded that they are not high-risk as they are only used for narrow or procedural tasks; \n\nâ€¢ Centralising oversight over a large number of AI systems built on general-purpose AI models or embedded in very large online platforms and very large search engines with the AI Office; \n\nâ€¢ facilitating compliance with the data protection laws by allowing providers and deployers of all AI systems and models to process special categories of personal data for ensuring bias detection and correction, with the appropriate safeguards; \n\nâ€¢ a broader use of AI regulatory sandboxes and real-world testing , that will benefit European key industries such as the automotive industry, and facilitating an EU-level AI regulatory sandbox which the AI Office will set up as from 2028; \n\nâ€¢ targeted changes clarifying the interplay between the AI Act and other EU legislation and adjusting the AI Actâ€™s procedures to improve its overall implementation and operation. \n\nBeyond the legislative measures, the Commission is taking further measures to facilitate compliance with the AI Act and address the concerns raised by stakeholders. Further guidance is under preparation, focusing on offering clear and practical instructions to apply the AI Act in parallel with other EU legislation. This includes: \n\nâ€¢ Guidelines on the practical application of the high-risk classification; \n\nâ€¢ Guidelines on the practical application of the transparency requirements under Article 50 AI Act; \n\nâ€¢ Guidance on the reporting of serious incidents by providers of high-risk AI systems; \n\nâ€¢ Guidelines on the practical application of the high-risk requirements; EN 3 EN \n\nâ€¢ Guidelines on the practical application of the obligations for providers and deployers of high-risk AI systems; \n\nâ€¢ Guidelines with a template for the fundamental rights impact assessment; \n\nâ€¢ Guidelines on the practical application of rules for responsibilities along the AI value chain; \n\nâ€¢ Guidelines on the practical application of the provisions related to substantial modification; \n\nâ€¢ Guidelines on the post-market monitoring of high-risk AI systems; \n\nâ€¢ Gudelines on the elements of the quality management system which SMEs and SMCs may comply with in a simplified manner; \n\nâ€¢ Guidelines on the AI Actâ€™s interplay with other Union legislation, for example joint guidelines of the Commission and European Data Protection Board on the interplay of the AI Act and EU data protection law, guidelines on the interplay between the AI Act and the Cyber Resilience Act, and guidelines on the interplay between the AI Act and the Machinery Regulation; \n\nâ€¢ Guidelines on the competences and designation procedure for conformity assessment bodies to be designated under the AI Act. \n\nIn particular, stakeholder consultations reveal the need to offer guidance on the practical application of the AI Actâ€™s research exemptions under Article 2(6) and (8), including how they apply in sectoral contexts like in the pre-clinical research and product development in the field of medicinal products or medical devices, which the Commission will work on with priority. \n\nThese simplification efforts will help to ensure that the implementation of the AI Act is smooth, predictable, and innovation-friendly, enabling Europe to strengthen its position as the AI continent and to pursue an AI-first approach safely. \n\nâ€¢ Consistency with existing policy provisions in the policy area \n\nThe proposal is part of a broader Digital Package on Simplification composed of measures to reduce the administrative costs of compliance for businesses and administrations in the EU, which applies to several regulations of the EUâ€™s digital acquis without compromising the objectives of the underlying rules. The proposal builds on Regulation (EU) 2024/1689 and is aligned with existing policies to make the EU a global leader in AI, to make the EU an AI continent and to promote the uptake of human-centric and trustworthy a AI. \n\nâ€¢ Consistency with other Union policies \n\nThe proposal is part of a series of simplification packages. \n\n2. LEGAL BASIS, SUBSIDIARITY AND PROPORTIONALITY \n\nâ€¢ Legal basis \n\nThe legal basis for this proposal is Article 114 of the Treaty on the Functioning of the European Union (TFEU) in line with the original legal basis for the adoption of the legal acts which this proposal aims to amend. EN 4 EN \n\nâ€¢ Subsidiarity (for non-exclusive competence) \n\nRegulation (EU) 2024/1689 was adopted at EU level. Accordingly, amendments to that Regulation need to be made at EU level. \n\nâ€¢ Proportionality \n\nThe initiative does not go beyond what is necessary to achieve the objectives of simplification and burden reduction without lowering the protection of health, safety and fundamental rights. \n\nâ€¢ Choice of the instrument \n\nThe proposal amends Regulation (EU) 2024/1689 adopted by ordinary legislative procedure. Therefore, the amendments to that Regulation also must be adopted by regulation in accordance with the ordinary legislative procedure. \n\n3. RESULTS OF EX-POST EVALUATIONS, STAKEHOLDER CONSULTATIONS AND IMPACT ASSESSMENTS \n\nâ€¢ Ex-post evaluations/fitness checks of existing legislation \n\nThe proposal is accompanied by a Commission staff working document that provides a detailed overview of the impact of the proposed amendments to certain provisions of Regulation (EU) 2024/1689. It also provides an analysis of the positive impacts of the proposed measures. The analysis is based on existing data, information gathered through consultations and during a reality check and through written stakeholder feedback through a call for evidence. \n\nâ€¢ Stakeholder consultations \n\nSeveral consultations were carried out in the context of the proposal. They all complemented one another, addressing either different topical issues or stakeholder groups concerned by the initiative. \n\nIn the initial scoping phase of the Digital Package on Simplification, three public consultations and calls for evidence were published on the key strands of the proposal in the spring of 2025. A consultation was held on the Apply AI Strategy from 9 April to 4 June 2025 (5), another on the revision of the Cybersecurity Act from 11 April to 20 June 2025 ( 6), and finally another on the European Data Union Strategy from 23 May to 20 July 2025 ( 7). Each consultation included a questionnaire with a section (or at times multiple) on implementation and simplification concerns, directly related to the reflections on the Digital Package on Simplification. Taken together, 718 responses were received as part of this first consultation exercise.         \n\n> 5European Commission (2025) Call for evidence on the Apply AI Strategy . Available at: Apply AI\n> Strategy â€“ strengthening the AI continent\n> 6European Commission (2025) Call for evidence on the revision of the Cybersecurity Act. Available at:\n> The EU Cybersecurity Act\n> 7European Commission (2025) Call for evidence on the European Data Union Strategy . Available at:\n> European Data Union Strategy\n\n# EN 5 EN \n\nFrom 16 September to 14 October 2025, a call for evidence on the Digital Package on Simplification was further published ( 8). Its aim was to cover the whole scope of the initiative and give an opportunity to stakeholders to comment on a more targeted set of proposals in one go. A total of 513 responses were received, by a wide range of stakeholders. \n\nWith a view to raising awareness on the Digital Package on Simplification among small and medium-sized enterprises (SMEs), and collecting their feedback, a dedicated SME Panel was organised through the Enterprise Europe Network (EEN) between 4 September and 16 October 2025. The EEN is the worldâ€™s largest support network for SMEs and is implemented by the Commissionâ€™s European Innovation Council and SMEs Executive Agency (EISMEA). SME Panels are a way to consult stakeholders falling under this framework. SMEs have the opportunity to contribute their views to upcoming policy initiatives. In addition to the online written consultation (where 106 SMEsâ€™ responses were received), the Commission also presented the Digital Package on Simplification to SME associations part of the EEN, in a meeting that took place on 1 October 2025. \n\nA large number of bilateral meetings were organised by the Commission services with stakeholders in 2025, to address specific concerns. Discussions were also held with Member States. In addition to bilateral exchanges, specific agenda points on the Digital Simplification Package were discussed at Council Working Parties in June and September 2025, where the Commission presented the current situation and asked Member Statesâ€™ to express their views. \n\nOverall, stakeholder feedback converged on the need for a simplified application of some of the digital rules. Better coherence, and a focus on optimisation of compliance costs, was largely supported by a cross-section of stakeholders. Some differences in opinion were expressed regarding some of the more tailored measures. A more detailed overview of these stakeholder consultations, and how they were reflected in the proposal can be found in the staff working document accompanying the Digital Package on Simplification. \n\nâ€¢ Collection and use of expertise \n\nIn addition to the consultation outlined above, the Commission mainly relied on its own internal analysis for the purpose of this proposal. \n\nâ€¢ Impact assessment \n\nThe amendments put forward in the proposal are technical in nature. They are designed to ensure a more efficient implementation of rules that were already agreed at political level. There are no policy options that could meaningfully be tested and compared in an impact assessment report. \n\nThe accompanying staff working document examines the reasoning behind the amendments and outlines the views of stakeholders on the different measures. It also presents the costs savings and other types of impacts the proposal may entail. In many cases, it builds on the impact assessments that was originally carried out for the Regulation (EU) 2024/1689.   \n\n> 8European Commission (2025) Call for evidence on the digital package and omnibus . Available at:\n> Simplification â€“ digital package and omnibus\n\n# EN 6 EN \n\nThe staff working document therefore serves as a reference point to inform the European Parliament and the Councilâ€™s debate on the proposal, as well as the public, in a clear and engaged way. \n\nâ€¢ Regulatory fitness and simplification \n\nThe proposal aims to produce a significant reduction in administrative burden for businesses, national administrations, and the public at large. Initial estimates project possible savings of â‰ˆ\n\nEUR 297.2 to 433.2 million . Non-quantifiable benefits are also expected, notably due to a streamlined set of rules which will ease compliance and enforcement thereof. \n\nSMEs already benefit from regulatory privileges under Regulation (EU) 2024/1689. Some regulatory privileges already afforded to SMEs are extended to small mid-caps (SMCs). Since SMEs and SMCs are disproportionality more impacted by the compliance burden, it is expected that they will particularly benefit from these simplification measures. \n\nThe proposal is consistent with the Commissionâ€™s â€˜Digital Fitness Check for the digital rulebookâ€™, which aims to ensure properly aligned policy proposals with real-world digital environments (see Chapter 4 on Legislative and Financial Digital Statement). \n\nâ€¢ Fundamental rights \n\nRegulation (EU) 2024/1689 is expected to promote the protection of a number of fundamental rights and freedoms set out in the EU Charter of Fundamental Rights ( 9), as well as positively impacting the rights of a number of special groups ( 10 ). At the same time, the Regulation (EU) 2024/1689 imposes some restrictions on certain rights and freedoms (11 ), which are proportionate and limited to the minimum necessary. The proposal is not expected to modify the impact of the Regulation (EU) 2024/1689 on fundamental rights since the targeted nature of envisaged amendments do not affect the scope of the regulated AI systems or on the substantive requirements applicable to those systems. \n\n4. BUDGETARY IMPLICATIONS \n\nThe proposal amends the supervision and enforcement system of Regulation (EU) 2024/1689, whereby oversight over certain AI systems will be transferred to the Commissionâ€™s AI Office. In addition, to facilitate compliance by operators, the AI Office should set up an EU-level AI regulatory sandbox. To implement these new tasks, to the Commission will need the appropriate resources, which is estimated to stand at 53 FTE, of which 15 FTE can be covered    \n\n> 9In detail: the right to human dignity (Article 1), respect for private life and protection of personal data (Articles 7 and 8), non-discrimination (Article 21) and equality between women and men (Article 23), freedom of expression (Article 11) and freedom of assembly (Article 12), right to an effective remedy and to a fair trial, the rights of defence, and the presumption of innocence (Articles 47 and 48), right to a high level of environmental protection and the improvement of the quality of the environment (Article 37).\n> 10 In detail: workersâ€™ rights to fair and just working conditions (Article 31), a high level of consumer protection (Article 28), the rights of the child (Article 24) and the integration of persons with disabilities (Article 26).\n> 11 In detail: the freedom to conduct business (Article 16) and the freedom of art and science (Article 13).\n\n# EN 7 EN \n\nby internal redeployment. These implications have to be considered against the backdrop of reduced budgetary implications for the Member States which no longer have to ensure the oversight for those certain AI systems. A detailed overview of the costs involved in this transfer of competences is provided in the â€˜Legislative and Financial Digital Statementâ€™ accompanying this proposal. \n\n5. OTHER ELEMENTS \n\nâ€¢ Implementation plans and monitoring, evaluation and reporting arrangements \n\nThe Commission will monitor the implementation, application, and compliance with the new provisions. Furthermore, the Regulation that is amended by this proposal is regularly evaluated for its efficiency, effectiveness in reaching its objectives, relevance, coherence and value added in line with the EUâ€™s better regulation principles. This proposal does not require an implementation plan. \n\nâ€¢ Explanatory documents (for directives) \n\nNot applicable. \n\nâ€¢ Detailed explanation of the specific provisions of the proposal \n\nArticle 1 amends Regulation (EU) 2024/1689 (â€˜AI Actâ€™).  In particular, \n\nâ€¢ Paragraph 1 adds a reference to SMCs in the subject matter of the AI Act. \n\nâ€¢ Paragraph 2 is a technical change that is necessary to enable extending the real-world testing to high-risk AI systems embedded in products covered under Section B of Annex I AI Act. \n\nâ€¢ Paragraph 3 adds legal definitions for SME and SMC to the definitions in Article 3 of the AI Act. \n\nâ€¢ Paragraph 4 transforms the obligation for providers and deployers of AI systems with regards to AI literacy in Article 4 AI Act to an obligation on the Commission and the Member States to foster AI literacy. \n\nâ€¢ Paragraph 5 introduces a new Article 4a, replacing Article 10(5) AI Act, which provides a legal basis for providers and deployers of AI systems and AI models to exceptionally process special categories of personal data for the purpose of ensuring bias detection and correction under certain conditions. \n\nâ€¢ Paragraphs 6, 14 and 32 refer to the deletion of the obligation for providers to register AI systems in the EU database for high-risk systems under Annex III, where they have been exempted from classification as high-risk under Article 6(3) AI Act, because they are for instance only used for preparatory tasks. \n\nâ€¢ Paragraph 7 contains editorial follow-up changes to amendments made by paragraph 4. \n\nâ€¢ Paragraphs 8 and 9 extend existing regulatory privileges of the AI Act for SMEs to SMCs on technical documentation and putting in place a quality management system that takes into account their size. \n\nâ€¢ Paragraph 10 introduces a new procedure in Article 28 AI Act, whereby Member States are required to ensure that a conformity assessment body that applies for designation both under this Regulation and Union harmonization legislation listed in EN 8 EN \n\nSection A of Annex I AI Act shall be provided with the possibility to submit a single application and undergo a single assessment procedure to be designated. \n\nâ€¢ Paragraph 11 proposes to replace paragraph 4 of Article 29 AI Act which requires conformity assessment bodies to submit a single application in the cases to which reference is made in that paragraph. \n\nâ€¢ Paragraph 12 amends Article 30 AI Act by requiring conformity assessment bodies which apply to be designated as notified bodies to make that application in accordance with the codes, categories, and corresponding types of AI systems referred to in a new Annex XIV for the Commissionâ€™s New Approach Notified and Designated Organisations (â€˜NANDOâ€™) information system, and empowers the Commission to amend these codes, categories, and corresponding types in light of technological developments. \n\nâ€¢ Paragraph 13 clarifies the conformity assessment procedure laid down in Article 43 AI Act where a high-risk AI system is covered by Union harmonisation legislation listed under Section A of Annex I to the AI Act and where an AI system is classified as high-risk both under Annex I and Annex III to the AI Act. \n\nâ€¢ Paragraphs 15 and 16 remove the Commission empowerments in Articles 50 and 56 AI Act to adopt implementing acts to give codes of practice for general purpose AI models and transparency obligations for certain AI systems general validity in the Union. \n\nâ€¢ Paragraph 17 introduces amendments to the rules on AI regulatory sandboxes in Article 57 AI Act, inter alia, by providing the legal basis for the AI Office to introduce an AI regulatory sandbox on EU level for certain AI systems within its exclusive competence of supervision and require Member States to strengthen cross-border cooperation of their sandboxes. \n\nâ€¢ Paragraph 18 specifies the empowerment of the Commission to adopt implementing acts specifying the detailed arrangements for the establishment, development, implementation, operation, governance and supervision of AI regulatory sandboxes. \n\nâ€¢ Paragraph 19 introduces changes to the testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes as governed by Article 60 AI Act, inter alia extending this opportunity to high-risk AI systems covered by Section A of Annex I. \n\nâ€¢ Paragraph 20 creates an additional legal basis for interested Member States and the Commission, on voluntary basis, to enter into written agreements to test high-risk AI systems referred to in Section B of Annex I in real world-conditions. \n\nâ€¢ Paragraph 21 extends the derogation from micro-enterprises to SMEs to comply with certain elements of the quality management system required by Article 17 AI Act in \n\na simplified manner .\n\nâ€¢ Paragraph 22 removes an empowerment of the Commission in Article 69 AI Act to adopt an implementing act in relation to the reimbursement of experts of the scientific panel when called upon by Member States, to simplify the procedure. \n\nâ€¢ Paragraph 23 extends the focus of guidance which national authorities may provide from SMEs to SMCs. \n\nâ€¢ Paragraph 24 replaces the empowerment of the Commission in Article 72 AI Act to adopt an implementing act with regard to the post-market monitoring plan. EN 9 EN \n\nâ€¢ Paragraph 25 makes amendments to the supervision and enforcement of certain AI systems in Article 75 AI Act: \n\nâ€¢ Point (a) changes the heading. \n\nâ€¢ Point (b) reinforces the competence of the AI Office for the supervision and enforcement of certain AI systems, that are based on a general-purpose AI model, where the model and the system are provided by the same provider. At the same time, the provision clarifies that AI systems related to products covered under Annex I are not included in that supervision. Moreover, it is clarified that the supervision and enforcement of the compliance of AI systems embedded in designated very large online platforms or very large online search engines should fall under the competence of the AI Office. \n\nâ€¢ Point (c) introduces several new paragraphs, empowering the Commission to adopt implementing acts to define the enforcement powers and the procedures for the exercise of those powers of the AI Office, introducing a reference to Regulation (EU) 2019/1020 ensuring certain procedural safeguards apply to providers covered and empowering the Commission to carry out conformity assessments of AI systems within the scope of Article 75. \n\nâ€¢ Paragraph 26 amends Article 77 AI Act as regards the powers of authorities or bodies protecting fundamental rights and cooperation with market surveillance authorities. \n\nâ€¢ Paragraphs 27 and 28 extends provisions in Articles 95 and 96 that require that voluntary support tools should take into account the needs of SMEs to SMCs. \n\nâ€¢ Paragraph 29 extends existing regulatory privileges in Article 99 AI Act on penalties for SMEs to SMCs. \n\nâ€¢ Paragraph 30 contains amendments to Article 111 AI Act which result from amendments made in paragraph 30 and introduces a transitional period of 6 months for providers who need to retroactively include technical solutions in their generative AI systems, to make them machine readable and detectable as artificially generated or manipulated. \n\nâ€¢ Paragraph 31 introduces changes to the entry into application of certain provisions of the AI Act: \n\nâ€¢ For the obligations for high-risk AI systems in Chapter III, a mechanism is introduced that links the entry into application to the availability of measures in support of compliance with the AI Actâ€™s high-risk rules, such as harmonised standards, common specifications, and Commission guidelines. This availability will be confirmed by the Commission by decision, following which the rules for high-risk AI systems start to apply after an appropriate transition period. However, this flexibility should apply only for a limited time and a definite date by which the rules apply in any case should be set. Moreover, it is appropriate to distinguish between the two types of AI systems that classify as high-risk and extend a longer transition period to AI systems that classify as high-risk pursuant to Article 6(1) and Annex I to the AI Act. \n\nâ€¢ It is clarified that the amendments necessary to integrate the high-risk requirements into sectoral law listed in Section B of Annex I apply with the Digital Omnibusâ€™ entry into force. EN 10 EN \n\nâ€¢ Paragraph 33 is related to the change in paragraph 11 and introduces a new Annex XIV setting out codes, categories, and corresponding types of AI systems referred to in a new Annex XIV for the Commissionâ€™s New Approach Notified and Designated Organisations (â€˜NANDOâ€™) information system. \n\nArticle 2 makes amendments with regards to Regulation (EU) 2018/1139, to allow a smooth integration of the AI Actâ€™s high-risk requirements into that Regulation. \n\nArticle 3 provides the rule of entry into force and the binding nature of this Regulation. EN 11 EN \n\n2025/0359 (COD) \n\nProposal for a \n\nREGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL \n\namending Regulations (EU) 2024/1689 and (EU) 2018/1139 as regards the simplification of the implementation of harmonised rules on artificial intelligence (Digital Omnibus on AI) \n\n(Text with EEA relevance) \n\nTHE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION, \n\nHaving regard to the Treaty on the Functioning of the European Union, and in particular Article 114 thereof, \n\nHaving regard to the proposal from the European Commission, \n\nAfter transmission of the draft legislative act to the national parliaments, \n\nHaving regard to the opinion of the European Economic and Social Committee 1,\n\nHaving regard to the opinion of the Committee of the Regions 2,\n\nActing in accordance with the ordinary legislative procedure, \n\nWhereas: \n\n(1) Regulation (EU) 2024/1689 of the European Parliament and of the Council 3 lays down harmonised rules on artificial intelligence (AI) and aims to improve the functioning of the internal market, to promote the uptake of human-centric and trustworthy artificial intelligence, while ensuring a high level of protection of health, safety and fundamental rights, and supporting innovation. Regulation (EU) 2024/1689 entered into force on 1 August 2024. Its provisions enter into application in a staggered manner, with all rules entering into application by 2 August 2027. \n\n(2) The experience gathered in implementing the parts of Regulation (EU) 2024/1689 that have already entered into application can inform the implementation of those parts that are yet to apply. In this context, the delayed preparation of standards, which should provide technical solutions for providers of high-risk AI systems to ensure compliance with their obligations under that regulation, and the delayed establishment of    \n\n> 1OJ C , , p. .\n> 2OJ C , , p. .\n> 3Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).\n\n# EN 12 EN \n\nthe governance and the conformity assessment frameworks at national level result in a compliance burden that is heavier than expected. In addition, consultations of stakeholders have revealed the need for additional measures that facilitate and provide clarification on the implementation and compliance, without reducing the level of protection for health, safety and fundamental rights from AI-related risks that the rules of Regulation (EU) 2024/1689 seek to achieve. \n\n(3) Consequently, targeted amendments to Regulation (EU) 2024/1689 are necessary to address certain implementation challenges, with a view to the effective application of the relevant rules. \n\n(4) Enterprises outgrowing the micro, small and medium-sized enterprises (â€˜SMEâ€™) definition â€“ the â€˜small mid-cap enterprisesâ€™ (â€˜SMCsâ€™) â€“ play a vital role in the Unionâ€™s economy. Compared to SMEs, SMCs tend to demonstrate a higher pace of growth, and level of innovation and digitisation. Nevertheless, they face challenges similar to SMEs in relation to administrative burden, leading to a need for proportionality in the implementation of Regulation (EU) 2024/1689 and for targeted support. To enable the smooth transition of enterprises from SMEs into SMCs, it is important to address in a coherent manner the effect that regulation may have on their activity once those enterprises outgrow the segment of SMEs and are faced with rules that apply to large enterprises. Regulation (EU) 2024/1689 provides for several measures for small-scale providers, which should be extended to SMCs. In order to clarify the treatment of SMEs and SMCs in Regulation (EU) 2024/1689, it is necessary to introduce definitions for SMEs and SMCs, which should correspond to the definition set out in the Annex to Commission Recommendation 2003/361/EC 4 and Annex to Commission Recommendation 2025/3500/EC 5.\n\n(5) Article 4 of Regulation (EU) 2024/1689 currently imposes an obligation on all providers and deployers of AI systems to ensure AI literacy of their staff. AI literacy development starting from education and training and continuing in a lifelong learning manner is crucial to equip providers, deployers and other affected persons with the necessary notions to make informed decisions regarding AI systems deployment. However, experience shared by stakeholders reveals that a one-size-fits-all solution is not suitable for all types of providers and deployers in relation to the promotion of AI literacy, rendering such a horizontal obligation ineffective in achieving the objective pursued by this provision. Moreover, data indicate that imposing such an obligation creates an additional compliance burden, particularly for smaller enterprises, whereas AI literacy should be a strategic priority, regardless of regulatory obligations and potential sanctions. In light of that, Article 4 of Regulation (EU) 2024/1689 should be amended to require the Member States and the Commission, without prejudice to their respective competences, to individually, collectively and in cooperation with relevant stakeholders encourage providers and deployers to provide a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI   \n\n> 4Commission Recommendation of 6 May 2003 concerning the definition of micro, small and medium-sized enterprises (OJ L 124, 20.5.2003, pp. 36â€“41, ELI: http://data.europa.eu/eli/reco/2003/361/oj).\n> 5Commission Recommendation (EU) 2025/1099 of 21 May 2025 on the definition of small mid-cap enterprises (OJ L, 2025/1099, 28.5.2025, ELI: http://data.europa.eu/eli/reco/2025/1099/oj).\n\n# EN 13 EN \n\nsystems on their behalf, including through offering training opportunities, providing informational resources, and allowing exchange of good practices and other non-legally binding initiatives. The European Artificial Intelligence Board (â€˜Boardâ€™) will ensure recurrent exchange between the Commission and Member States on the topic, while the Apply AI Alliance will allow discussion with the wider community. This amendment is without prejudice to the broader measures taken by the Commission and the Member States to promote AI literacy and competences for the wider population, including learners, students, and citizens at different ages and in particular through education and training systems. \n\n(6) Bias detection and correction constitute a substantial public interest because they protect natural persons from biasesâ€™ adverse effects, including discrimination. Discrimination might result from the bias in AI models and AI systems other than high-risk AI systems for which of Regulation (EU) 2024/1689 already provides a legal basis authorising the processing of special categories of personal data under Article 9(2), point (g), of Regulation (EU) 2016/679 of the European Parliament and of the Council 6. Given that discrimination might result also from those other AI systems and models, it is therefore appropriate that Regulation (EU) 2024/1689 should provide for a legal basis for the processing of special categories of personal data also by providers and deployers of other AI systems and AI models as well as deployers of high-risk AI systems. The legal basis is established in compliance with Article 9(2), point (g) of Regulation (EU) 2016/679 Article 10(2), point (g) of Regulation (EU) 2018/1725 of the European Parliament and of the Council 7 and Article 10, point (a) of Directive (EU) 2016/680 of the European Parliament and of the Council 8 provides a legal basis allowing, where necessary for the detection and removal of bias, the processing of special categories of personal data by providers and deployers of all AI systems and models, subject to appropriate safeguards that complement Regulations (EU) 2016/679, Regulation (EU) 2018/1725 and Directive (EU) 2016/680, as applicable. \n\n(7) In order to ensure consistency, avoid duplication and minimise administrative burdens in relation to the procedure for designating notified bodies under Regulation (EU) 2024/1689, while maintaining the same level of scrutiny, a single application and a single assessment procedure should be available for new conformity assessment                        \n\n> 6Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1, ELI: http://data.europa.eu/eli/reg/2016/679/oj).\n> 7Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of natural persons with regard to the processing of personal data by the Union institutions, bodies, offices and agencies and on the free movement of such data, and repealing Regulation (EC) No 45/2001 and Decision No 1247/2002/EC (OJ L295, 21.11.2018, p. 39, ELI: http://data.europa.eu/eli/reg/2018/1725/oj).\n> 8Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA (OJ L119, 4.5.2016, pp. 89â€“131, ELI: http://data.europa.eu/eli/dir/2016/680/oj).\n\n# EN 14 EN \n\nbodies and notified bodies which are designated under the Union harmonisation legislation listed in Section A of Annex I to Regulation (EU) 2024/1689, such as under Regulations (EU) 2017/745 9 and (EU) 2017/746 10  of the European Parliament and of the Council, where such a procedure is established under that Union harmonisation legislation. The single application and assessment procedure aims at facilitating, supporting and expediting the designation procedure under Regulation (EU) 2024/1689, while ensuring compliance with the requirements applicable to notified bodies under that Regulation and the Union harmonisation legislation listed in Section A of Annex I thereto. \n\n(8) With a view to ensuring the smooth application and consistency of Regulation (EU) 2024/1689, amendments should be made to it. A technical correction to Article 43(3), first subparagraph, of Regulation (EU) 2024/1689 should be added to align the conformity assessment requirements with the requirements of providers of high-risk AI systems in Article 16 of that Regulation. Moreover, it should be clarified that where a provider of a high-risk AI system is subject to the conformity assessment procedure under Union harmonisation legislation listed in Section A of Annex I to Regulation (EU) 2024/1689, and the conformity assessment extends to compliance of the quality management system of that Regulation and of such Union harmonisation legislation, the provider should be able to include aspects related to quality management systems under that Regulation as part of the quality management systems under such Union harmonisation legislation, in line with Article 17(3) of Regulation (EU) 2024/1689. Article 43(3), second subparagraph, should be amended to clarify that notified bodies which have been notified under the Union harmonisation legislation listed in Section A of Annex I to Regulation (EU) 2024/1689 and which aim to assess high-risk AI systems covered by the Union harmonisation legislation listed in Section A of Annex I to that Regulation, should apply for the designation as a notified body under that Regulation within 18 months from [the entry into application of this Regulation]. This amendment is without prejudice to Article 28 of Regulation (EU) 2024/1689. Moreover, Regulation (EU) 2024/1689 should be amended to clarify that where a high-risk AI system is both covered by the Union harmonisation legislation listed in Section A of Annex I to Regulation (EU) 2024/1689 and falls within one of the use-cases listed in Annex III to that Regulation, the provider should follow the relevant conformity assessment procedure as required under that relevant harmonisation legislation. \n\n(9) To streamline compliance and reduce the associated costs, providers of AI systems should not be required to register AI systems referred to in Article 6(3) of Regulation (EU) 2024/1689 in the EU database pursuant to Article 49(2) of that Regulation. Given that such systems are not considered high-risk under certain conditions where   \n\n> 9Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1, ELI: http://data.europa.eu/eli/reg/2017/745/oj).\n> 10 Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro diagnostic medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176, ELI: http://data.europa.eu/eli/reg/2017/746/oj).\n\n# EN 15 EN \n\nthey do not pose significant risk of harm to the health, safety or fundamental rights of persons, imposing registration requirements would constitute a disproportionate compliance burden. Nevertheless, a provider who considers that an AI system falls under Article 6(3) remains obligated to document its assessment before that system is placed on the market or put into service. This assessment may be requested by national competent authorities. \n\n(10) Articles 57, 58 and 60 of Regulation (EU) 2024/1689 should be amended to strengthen further cooperation at Union level of AI regulatory sandboxes, foster clarity and consistency in the governance of AI regulatory sandboxes, and to extend the scope of real-world testing outside AI regulatory sandboxes to high-risk AI systems covered by the Union harmonisation legislation listed in Annex I to that Regulation. In particular, to allow procedural simplification, where applicable, in the projects supervised in the AI regulatory sandboxes that include also real-world testing, the real-world testing plan should be integrated in the sandbox plan agreed by the providers or prospective providers and the competent authority in a single document. In addition, it is appropriate to provide for the possibility of the AI Office to establish an AI regulatory sandbox at Union level for AI systems that are covered by Article 75(1) of Regulation (EU) 2024/1689. By leveraging these infrastructures and facilitating cross-border collaboration, coordination would be better streamlined and resources optimally utilised. \n\n(11) To foster innovation, it is also appropriate to extend the scope of real-world testing outside AI regulatory sandboxes in Article 60 of Regulation (EU) 2024/1689, currently applicable to high-risk AI systems listed in Annex III to that Regulation, and allow providers and prospective providers of high-risk AI systems covered by the Union harmonisation legislation listed in Annex I to that Regulation to also test such systems in real-world conditions. This is without prejudice to other Union or national law on the testing in real-world conditions of high-risk AI systems related to products covered by that Union harmonisation legislation. To address the specific situation of high-risk AI systems covered the Union harmonisation legislation listed in Section B of Annex I to that Regulation, it is necessary to allow the conclusion of voluntary agreements between the Commission and Member States to enable testing of such high-risk AI systems in real-world conditions. \n\n(12) Article 63 of Regulation (EU) 2024/1689 offers microenterprises who are providers of high-risk AI systems the possibility to benefit from a simplified way to comply with the obligation to establish a quality management system. With a view to facilitating compliance for more innovators, that possibility should be extended to all SMEs, including start-ups. \n\n(13) Article 69 of Regulation (EU) 2024/1689 should be amended to simplify the fee structure of the scientific panel. If Member States call upon the panelâ€™s expertise, the fees they may be required to pay the experts should be equivalent to the remuneration the Commission is obliged to pay in similar circumstances. Furthermore, to reduce the procedural complexity, Member States should be able to consult the experts of the scientific panel directly, without involvement of the Commission. \n\n(14) In order to strengthen the governance system for AI systems based on general-purpose AI models, it is necessary to clarify the role of the AI Office in monitoring and supervising compliance of such AI systems with Regulation (EU) 2024/1689, while excluding AI systems related to products covered by the Union harmonisation legislation listed in Annex I to that Regulation. While sectoral authorities continue to EN 16 EN \n\nremain responsible for the supervision of AI systems related to products covered by that Union harmonisation legislation, Article 75(1) Regulation (EU) 2024/1689 should be modified to bring all AI systems based on general-purpose AI models developed by the same provider within the scope of the AI Office's supervision. This does not include AI systems placed on the market, put into service or used by Union institutions, bodies, offices or agencies, which are under the supervision of the European Data Protection Supervisor pursuant to Article 74(9) of Regulation (EU) 2024/1689. To ensure effective supervision for those AI systems in accordance with the tasks and responsibilities assigned to market surveillance authorities under Regulation (EU) 2024/1689, the AI Office should be empowered to take the appropriate measures and decisions to adequately exercise its powers provided for in that Section and Regulation (EU) 2019/1020 of the European Parliament and of the Council 11 . Article 14 of Regulation (EU) 2019/1020 should apply mutatis mutandis. Furthermore, to ensure effective enforcement, the authorities involved in the application of Regulation (EU) 2024/1689 should cooperate actively in the exercise of those powers, in particular where enforcement actions need to be taken in the territory of a Member State. \n\n(15) Considering the existing supervisory and enforcement system under Regulation (EU) 2022/2065 of the European Parliament and of the Council 12 , it is appropriate to grant the Commission the powers of a competent market surveillance authority under Regulation (EU) 2024/1689 where an AI system qualifies as a very large online platform or a very large online search engine within the meaning of Regulation (EU) 2022/2065, or where it is embedded in such a platform or search engine. This should contribute to ensuring that the exercise of the Commissionâ€™s supervision and enforcement powers under Regulation (EU) 2024/1689 and Regulation (EU) 2022/2065, as well as those applicable to general-purpose AI models integrated into such platforms or search engines, are carried out in a coherent manner. In the case of AI systems embedded in or qualifying as a very large online platform or search engine, the first point of entry for the assessment of the AI systems are the risk assessment, mitigating measures and audit obligations prescribed by Articles 34, 35 and 37 of Regulation (EU) 2022/2065, without prejudice to the AI Officeâ€™s powers to investigate and enforce ex post non-compliance with the rules of this Regulation. In the context of the analysis of this risk assessment, mitigating measures and audits, the Commission services responsible for the enforcement of Regulation (EU) 2022/2065 may seek the opinion of the AI Office on the outcome of a potential earlier or parallel risk assessment carried out under this Regulation and the applicability of prohibitions under this Regulation. In addition, the AI Office and the competent national authorities under (EU) 2024/1689 should coordinate their enforcement efforts with the authorities              \n\n> 11 Regulation (EU) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market surveillance and compliance of products and amending Directive 2004/42/EC and Regulations (EC) No 765/2008 and (EU) No 305/2011 (OJ L169, 25.6.2019, p. 1, ELI: http://data.europa.eu/eli/reg/2019/1020/oj).\n> 12 Regulation (EU) 2022/2065 of the European Parliament and of the Council of 19 October 2022 on a Single Market For Digital Services and amending Directive 2000/31/EC (Digital Services Act) (OJ L 277, 27.10.2022, p. 1, ELI: http://data.europa.eu/eli/reg/2022/2065/oj).\n\n# EN 17 EN \n\ncompetent for the supervision and enforcement of Regulation (EU) 2022/2065, including the Commission, in order to ensure that the principles of loyal cooperation, proportionality and non bis in idem are respected, while information obtained under the respective other Regulation would be used for the purposes of supervision and enforcement of the other only provided the undertaking agrees. In particular, those authorities should exchange views regularly and take into account, in their respective areas of competence, any fines and penalties imposed on the same provider for the same conduct through a final decision in proceedings relating to an infringement of other Union or national rules, so as to ensure that the overall fines and penalties imposed are proportionate and correspond to the seriousness of the infringements committed. \n\n(16) To further operationalise the AI Officeâ€™s supervision and enforcement set out in Article 75(1) of Regulation (EU) 2024/1689, it is necessary to further define the which of the powers listed in Article 14 of Regulation (EU) 2019/1020 should be conferred upon the AI Office. The Commission should therefore be empowered to adopt implementing acts to specify those powers, including the ability to impose penalties, such as fines or other administrative sanctions, in accordance with the conditions and ceilings referred to in Article 99, and applicable procedures. This should ensure that the AI Office has the necessary tools to effectively monitor and supervise compliance with Regulation (EU) 2024/1689. \n\n(17) Additionally, it is essential to ensure that effective procedural safeguards apply to providers of AI systems subject to monitoring and supervision by the AI Office. To that end, the procedural rights provided for in Article 18 of Regulation (EU) 2019/1020 should apply mutatis mutandis to providers of AI systems, without prejudice to more specific procedural rights provided for in Regulation (EU) 2024/1689. \n\n(18) To enable access to Union market for AI systems which are under the supervision by the AI Office pursuant to Article 75 of Regulation (EU) 2024/1689 and subject to third party conformity assessment, the Commission should be enabled to carry out pre-market conformity assessments of those systems. \n\n(19) Article 77 and related provisions of Regulation (EU) 2024/1689 constitute an important governance mechanism, as they aim to enable authorities or bodies responsible for enforcing or supervising Union law intended to protect fundamental rights to fulfil their mandate under specific conditions and to foster cooperation with market surveillance authorities responsible for the supervision and enforcement of that Regulation. It is necessary to clarify the scope of such cooperation, as well as to clarify which public authorities or bodies benefit from it. With a view to reinforcing the cooperation, it should be clarified that requests to access information and documentation should be made to the competent market surveillance authority, which should respond to such requests, and that the involved authorities or bodies should have a mutual obligation to cooperate. \n\n(20) To allow sufficient time for providers of generative AI systems subject to the marking obligations laid down in Article 50(2) of Regulation (EU) 2024/1689 to adapt their practices within a reasonable time without disrupting the market, it is appropriate to introduce a transitional period of 6 months for providers who have already placed their systems on the market before the 2 August 2026. \n\n(21) To provide sufficient time for providers of high-risk AI systems and to clarify applicable rules to the AI systems already placed on the market or put into service EN 18 EN \n\nbefore the entry into application of relevant provisions of the Regulation (EU) 2024/1689, it is appropriate to clarify the application of a grace period provided in Article 111(2) of that Regulation. The grace period, for the purpose of Article 111(2), should apply to a type and model of AI systems already placed in the market. This means that if at least one individual unit of the high-risk AI system has been lawfully placed on the market or put into service before the date specified in Article 111(2), other individual units of the same type and model of high-risk AI system are subject to the grace period provided in Article 111(2) and thus may continue to be placed on the market, made available or put into service on the Union market without any additional obligations, requirements or the need for additional certification, as long as the design of that high-risk AI system remains unchanged. For the purposes of application of the grace period provided in Article 111(2), the decisive factor is the date on which the first unit of that type and model of high-risk AI system was placed on the market or put into service on the Union market for the first time. Any significant change to the design of that AI system after the date specified in Article 111(2) should trigger the obligation of the provider to comply fully with all relevant provisions of this Regulation applicable to high-risk AI systems, including the conformity assessment requirements. \n\n(22) Article 113 of Regulation (EU) 2024/1689 establishes the dates of entry into force and application of that Regulation, notably that the general date of application is 2 August 2026. For the obligations related to high-risk AI systems laid down in Sections 1, 2 and 3 of Chapter III of Regulation (EU) 2024/1689, the delayed availability of standards, common specifications, and alternative guidance and the delayed establishment of national competent authorities lead to challenges that jeopardise those obligationâ€™s effective entry into application and that risk to significantly increase implementation costs in a way that does not justify maintaining their initial date of application, namely 2 August 2026. Building on experience, it is appropriate to put in place a mechanism that links the entry into application to the availability of measures in support of compliance with Chapter III, which may include harmonised standards, common specifications, and Commission guidelines. This should be confirmed by the Commission by decision, following which the rules obligations for high-risk AI systems should apply after 6 months as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III and after 12 months as regards AI systems classified as high-risk pursuant to Article 6(1) and Annex I to Regulation (EU) 2024/1689. However, this flexibility should only be extended until 2 December 2027 as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III and until 2 August 2028 as regards AI systems classified as high-risk pursuant to Article 6(1) and Annex I to that Regulation, by which dates those rules should enter into application in any case. The distinction between the entry into application of the rules as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III and Article 6(1) and Annex I to that Regulation is consistent with the difference between the initial dates of application envisaged in Regulation (EU) 2024/1689 and aims to provide the necessary time for adaptation and implementation of the corresponding obligations. \n\n(23) In light of the objective to reduce implementation challenges for citizens, businesses and public administrations, it is essential that harmonised conditions for the implementation of certain rules are adopted only where strictly necessary. For that purpose, it is appropriate to remove certain empowerments bestowed on the Commission to adopt such harmonised conditions by means of implementing acts in cases where those conditions are not met. Regulation (EU) 2024/1689 should therefore EN 19 EN \n\nbe amended to remove the empowerments conferred on the Commission in Article 50(7), Article 56(6), and Article 72(3) thereof to adopt implementing acts. The removal of the empowerment to adopt a harmonised template for a post-market monitoring plan in Article 72(3) of Regulation (EU) 2024/1689 has as an additional benefit that it will offer more flexibility for providers of high-risk AI systems to put in place a system for post-market monitoring that is tailored to their organisation. At the same time, recognising the need to offer clarity how providers of high-risk AI systems are required to comply, the Commission should be required to publish guidance. \n\n(24) Conformity assessment of high-risk AI systems under Regulation (EU) 2024/1689 may require involvement of conformity assessment bodies. Only conformity assessment bodies that have been designated under that Regulation may carry out conformity assessments and only for the activities related to the categories and types of AI systems concerned. To enable the specification of the scope of the designation of conformity assessment bodies notified under Article 30 of Regulation (EU) 2024/1689, it is necessary to draw up a list of codes, categories, and corresponding types of AI systems. The list of codes should take into account whether the AI system is a component of a product or itself a product covered by the Union harmonisation legislation listed in Annex I (referred to as â€˜AIP codesâ€™, for AI systems covered by product legislation) or a system referred in Annex III of Regulation (EU) 2024/1689, which currently concerns only biometric AI systems referred to in point (1) of Annex III (referred to as â€˜AIB codesâ€™, for biometric AI systems). Both AIP codes and AIB codes are vertical codes. The AIP codes are reference codes to provide a link to the Union harmonisation legislation listed in Section A of Annex I of Regulation (EU) 2024/1689. The AIB codes are new codes specific to Regulation (EU) 2024/1689 to identify biometric AI systems referred in paragraph 1 of Annex III of that Regulation. The list of codes should also take into account specific types and underlying technologies of AI systems (referred to as â€˜AIH codesâ€™, for horizontal AI system codes). The AIH codes are new AI technology-specific codes and can be applied in conjunction with AIP or AIB vertical codes. The AIH codes cover AI systemsâ€™ underlying types and technologies. The list of codes, including three categories, should provide for a multi-dimensional typology of AI systems which ensures that conformity assessment bodies designated as notified bodies are fully competent for the AI systems they are required to assess. \n\n(25) Regulation (EU) 2018/1139 of the European Parliament and the Council 13  lays down common rules in the field of civil aviation. Article 108 of Regulation (EU) 2024/1689 sets out amendments to Regulation (EU) 2018/1139 to ensure that the Commission takes into account, on the basis of the technical and regulatory specificities of the civil aviation sector, and without interfering with existing governance, conformity              \n\n> 13 Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in the field of civil aviation and establishing a European Union Aviation Safety Agency, and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the European Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the Council and Council Regulation (EEC) No 3922/91(OJ L212, 22.8.2018, pp. 1â€“122, ELI: http://data.europa.eu/eli/reg/2018/1139/oj).\n\n# EN 20 EN \n\nassessment and enforcement mechanisms and authorities established therein, the mandatory requirements for high-risk AI systems laid down in Regulation (EU) 2024/1689 when adopting any relevant delegated or implementing acts on the basis of that act. A technical correction extending specific articles of Regulation (EU) 2018/1139 is necessary to ensure that those mandatory requirements for high-risk AI systems laid down in Regulation (EU) 2024/1689 are fully covered when adopting relevant delegated or implementing acts on the basis of Regulation (EU) 2018/1139. \n\n(26) In order to ensure legal certainty as soon as possible, with a view to the imminent general application of Regulation (EU) 2024/1689, this Regulation should enter into force as a matter of urgency, \n\nHAVE ADOPTED THIS REGULATION: \n\nArticle 1 \n\nAmendments to Regulation (EU) 2024/1689 \n\nRegulation (EU) 2024/1689 is amended as follows: \n\n(1) in Article 1(2), point (g) is replaced by the following: \n\nâ€™(g) measures to support innovation, with a particular focus on small mid-cap enterprises (SMCs) and small and medium-sized enterprises (SMEs), including start-ups.â€™; \n\n(2) in Article 2, paragraph 2 is replaced by the following: \n\nâ€˜2. For AI systems classified as high-risk AI systems in accordance with Article 6(1) related to products covered by the Union harmonisation legislation listed in Section B of Annex I, only Article 6(1), Article 60a, Articles 102 to 109 and Articles 111 and 112 shall apply. Article 57 shall apply only in so far as the requirements for high-risk AI systems under this Regulation have been integrated in that Union harmonisation legislation.; \n\n(3) in Article 3, the following points (14a) and (14b) are inserted: \n\nâ€˜(14a) micro, small and medium-sized enterprise (â€˜SMEâ€™) means a micro, small or medium-sized enterprise as defined in Article 2 of the Annex to Commission Recommendation 2003/361/EC; \n\n(14b) small mid-cap enterprise (â€˜SMCâ€™) means a small mid-cap enterprise as defined in point (2) of the Annex to Commission Recommendation (EU) 2025/1099â€™; \n\n(4) Article 4 is replaced by the following: \n\nâ€˜ Article 4 \n\nAI literacy \n\nâ€˜The Commission and Member States shall encourage providers and deployers of AI systems to take measures to ensure a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, level of education and training and the context the AI EN 21 EN \n\nsystems are to be used in, and considering the persons or groups of persons on whom the AI systems are to be used.â€™; \n\n(5) the following Article 4a is inserted in Chapter I: \n\nâ€˜Article 4a \n\nProcessing of special categories of personal data for bias detection and mitigation \n\n1. To the extent necessary to ensure bias detection and correction in relation to high-risk AI systems in accordance with Article 10 (2), points (f) and (g), of this Regulation, providers of such systems may exceptionally process special categories of personal data, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons. In addition to the safeguards set out in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable, all the following conditions shall be met in order for such processing to occur: \n\n(a) the bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic or anonymised data; \n\n(b) the special categories of personal data are subject to technical limitations on the re-use of the personal data, and state-of-the-art security and privacy-preserving measures, including pseudonymisation; \n\n(c) the special categories of personal data are subject to measures to ensure that the personal data processed are secured, protected, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse and ensure that only authorised persons have access to those personal data with appropriate confidentiality obligations; \n\n(d) the special categories of personal data are not transmitted, transferred or otherwise accessed by other parties; \n\n(e) the special categories of personal data are deleted once the bias has been corrected or the personal data has reached the end of its retention period, whichever comes first; \n\n(f) the records of processing activities pursuant to Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 include the reasons why the processing of special categories of personal data was necessary to detect and correct biases, and why that objective could not be achieved by processing other data. \n\n2. Paragraph 1 may apply to providers and deployers of other AI systems and models and deployers of high-risk AI systems where necessary and proportionate if the processing occurs for the purposes set out therein and provided that the conditions set out under the safeguards set out in this paragraph.; \n\n(6) in Article 6(4), paragraph 4 is replaced by the following: \n\nâ€˜4. A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessment before that system is placed on the market or put into service. Upon request of national competent authorities, the provider shall provide the documentation of the assessment.â€™; EN 22 EN \n\n(7) Article 10 is amended as follows: \n\n(a) paragraph 1 is replaced by the following: \n\nâ€˜1. High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2, 3 and 4 of this Article and in Article 4a(1) whenever such data sets are used.â€™; \n\n(b) paragraph 5 is deleted; \n\n(c) paragraph 6 is replaced by the following: \n\nâ€˜6. For the development of high-risk AI systems not using techniques involving the training of AI models, paragraphs 2, 3 and 4 of this Article and Article 4a(1) shall apply only to the testing data sets.â€™; \n\n(8) in Article 11(1), the second subparagraph is replaced by the following: \n\nâ€˜That technical documentation shall be drawn up in such a way as to demonstrate that the high-risk AI system complies with the requirements set out in this Section and to provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive form to assess the compliance of the AI system with those requirements. It shall contain, at a minimum, the elements set out in Annex IV. SMCs and SMEs, including start-ups, may provide the elements of the technical documentation specified in Annex IV in a simplified manner. To that end, the Commission shall establish asimplified technical documentation form targeted at the needs of SMCs and SMEs, including start-ups. Where an SMC or SME, including a start-up, opts to provide the information required in Annex IV in a simplified manner, it shall use the form referred to in this paragraph. Notified bodies shall accept the form for the purposes of the conformity assessment.â€™; \n\n(9) in Article 17, paragraph 2 is replaced by the following: \n\nâ€˜2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the size of the providerâ€™s organisation, in particular, if the provider is an SMC or an SME, including a start-up. Providers shall, in any event, respect the degree of rigour and the level of protection required to ensure the compliance of their high-risk AI systems with this Regulation.â€™; \n\n(10) in Article 28, the following paragraph 8 is added: \n\nâ€˜8. Notifying authorities designated under this Regulation responsible for AI systems covered by the Union harmonisation legislation listed in Section A of Annex I shall be established, organised and operated in such a way that ensures that the conformity assessment body that applies for designation both under this Regulation and the Union harmonisation legislation listed in Section A of Annex I shall be provided with the possibility to submit a single application and undergo a single assessment procedure to be designated under this Regulation and Union harmonisation legislation listed in Section A of Annex I, where the EN 23 EN \n\nrelevant Union harmonisation legislation provides for such single application and single assessment procedure. \n\nThe single application and single assessment procedure referred to in this paragraph shall also be made available to notified bodies already designated under the Union harmonisation legislation listed in Section A of Annex I, when those notified bodies apply for designation under this Regulation, provided that the relevant Union harmonisation legislation provides for such a procedure. \n\nThe single application and single assessment procedure shall avoid any unnecessary duplications, build on the existing procedures for designation under the Union harmonisation legislation listed in Section A of Annex I and ensure compliance with the requirements both relating to notified bodies under this Regulation and the relevant Union harmonisation legislation.â€™; \n\n(11) in Article 29, paragraph 4 is replaced by the following: \n\nâ€˜4. For notified bodies which are designated under any other Union harmonisation legislation, all documents and certificates linked to those designations may be used to support and expedite their designation procedure under this Regulation, as appropriate. \n\nNotified bodies, which are designated under any of the Union harmonisation legislation listed in Section A of Annex I and which apply for the single assessment referred to in Article 28(8), shall submit the single application for assessment to the notifying authority designated in accordance with that Union harmonisation legislation. \n\nThe notified body shall update the documentation referred to in paragraphs 2 \n\nand 3 of this Article whenever relevant changes occur, in order to enable the authority responsible for notified bodies to monitor and verify continuous compliance with all the requirements laid down in \n\nArticle 31. â€™; \n\n(12) in Article 30, paragraph 2 is replaced by the following: \n\nâ€˜2. Notifying authorities shall notify the Commission and the other Member States, based on the list of codes, categories, and corresponding types of AI systems referred to in Annex XIV, and using the electronic notification tool developed and managed by the Commission, of each conformity assessment body referred to in paragraph 1. \n\nThe Commission is empowered to adopt delegated acts in accordance with Article 97 to amend Annex XIV, in the light of technical progress, advances in knowledge or new scientific evidence by adding to the list of codes, categories, and corresponding types of AI systems a new code, a category or a type of AI system, withdrawing an existing code, category or a type of AI system from that list or moving a code or type of AI system from one category to another.â€™; \n\n(13) in Article 43, paragraph 3 is replaced by the following: \n\nâ€˜For high-risk AI systems covered by the Union harmonisation legislation listed in Section A of Annex I, the provider of the system shall follow the relevant conformity assessment procedure as required under the relevant EN 24 EN \n\nUnion harmonisation legislation. The requirements set out in Section 2 of this Chapter shall apply to those high-risk AI systems and shall be part of that assessment. Assessment of the quality management system set out in Article 17 and Annex VII shall also apply. \n\nFor the purposes of that conformity assessment, notified bodies which have been notified under the Union harmonisation legislation listed in Section A of Annex I shall have the power to assess the conformity of high-risk AI systems with the requirements set out in Section 2, provided that the compliance of those notified bodies with the requirements laid down in Article 31(4), (5), (10) and (11) has been assessed in the context of the notification procedure under the relevant Union harmonisation legislation. Without prejudice to Article 28, such notified bodies which have been notified under the Union harmonisation legislation in Section A of Annex I, shall apply for designation in accordance with Section 4 at the latest [18 months from the entry into application of this Regulation]. \n\nWhere Union harmonisation legislation listed in Section A of Annex I provides the product manufacturer with an option to opt out from a third-party conformity assessment, provided that that manufacturer has applied harmonised standards covering all the relevant requirements, that manufacturer may use that option only if it has also applied harmonised standards or, where applicable, common specifications referred to in Article 41, covering all requirements set out in Section 2 of this Chapter. \n\nWhere a high-risk AI system is both covered by the Union harmonisation legislation listed in Section A of Annex I and it falls within one of the categories listed in Annex III, the provider of the system shall follow the relevant conformity assessment procedure as required under the relevant Union harmonisation legislation listed in Section A of Annex I.â€™; \n\n(14) in Article 49, paragraph 2 is deleted; \n\n(15) in Article 50, paragraph 7 is replaced by the following: \n\nâ€˜7. The AI Office shall encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective implementation of the obligations regarding the detection, marking and labelling of artificially generated or manipulated content. The Commission may assess whether adherence to those codes of practice is adequate to ensure compliance with the obligation laid down in paragraph 2, in accordance with the procedure laid down in Article 56(6), first subparagraph. If it deems the code is not adequate, the Commission may adopt an implementing act specifying common rules for the implementation of those obligations in accordance with the examination procedure laid down in Article 98(2).â€™; \n\n(16) in Article 56(6), the first subparagraph is replaced by the following: \n\nâ€˜6. The Commission and the Board shall regularly monitor and evaluate the achievement of the objectives of the codes of practice by the participants and their contribution to the proper application of this Regulation. The Commission, taking utmost account of the opinion of the Board, shall assess whether the codes of practice cover the obligations provided for in Articles 53 and 55, and shall regularly monitor and evaluate the EN 25 EN \n\nachievement of their objectives. The Commission shall publish its assessment of the adequacy of the codes of practice.â€™; \n\n(17) Article 57 is amended as follows: \n\n(a) the following paragraph 3a is inserted: \n\nâ€˜The AI Office may also establish an AI regulatory sandbox at Union level for AI systems covered by Article 75(1). Such an AI regulatory sandbox shall be implemented in close cooperation with relevant competent authorities, in particular when Union legislation other than this Regulation is supervised in the AI regulatory sandbox, and shall provide priority access to SMEs.â€™; \n\n(b) paragraph 5 is replaced by the following: \n\nâ€˜5. AI regulatory sandboxes established under this Article shall provide for a controlled environment that fosters innovation and facilitates the development, training, testing and validation of innovative AI systems for a limited time before their being placed on the market or put into service pursuant to a specific sandbox plan agreed between the providers or prospective providers and the competent authority, ensuring that appropriate safeguards are in place. Such sandboxes may include testing in real world conditions supervised therein. When applicable, the sandbox plan shall incorporate in a single document the real-world testing plan.â€™; \n\n(c) paragraph 9, point (e) is replaced by the following: \n\nâ€˜(e) facilitating and accelerating access to the Union market for AI systems, in particular when provided by SMCs and SMEs, including start-ups.â€™; \n\n(d) paragraph 13 is replaced by the following: \n\nâ€™13. The AI regulatory sandboxes shall be designed and implemented in such a way that they facilitate cross-border cooperation between national competent authorities.â€™; \n\n(e) paragraph 14 is replaced by the following: \n\nâ€™14. National competent authorities shall coordinate their activities and cooperate within the framework of the Board. They shall support the joint establishment and operation of AI regulatory sandboxes, including in different sectors.â€™; \n\n(18) Article 58, paragraph 1, is replaced by the following: \n\nâ€˜1. In order to avoid fragmentation across the Union, the Commission shall adopt implementing acts specifying the detailed arrangements for the establishment, development, implementation, operation, governance, and supervision of the AI regulatory sandboxes. The implementing acts shall include common principles on the following issues: \n\n(a) eligibility and selection criteria for participation in the AI regulatory sandbox; \n\n(b) procedures for the application, participation, monitoring, exiting from and termination of the AI regulatory sandbox, including the sandbox plan and the exit report; EN 26 EN \n\n(c) the terms and conditions applicable to the participants; \n\n(d) the detailed rules applicable to the governance of AI regulatory sandboxes covered under Article 57, including as regards the exercise of the tasks of the competent authorities and the coordination and cooperation at national and EU level.â€™; \n\n(19) Article 60 is amended as follows: \n\n(a) in paragraph 1, the first subparagraph is replaced by the following: \n\nâ€˜Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers of high-risk AI systems listed in Annex III or covered by Union harmonisation legislation listed in Section A of Annex I, in accordance with this Article and the real-world testing plan referred to in this Article, without prejudice to the prohibitions under Article 5.â€™; \n\n(b) paragraph 2 is replaced by the following: \n\nâ€˜2. Providers or prospective providers may conduct testing of high-risk AI systems referred to in Annex III or covered by Union harmonisation legislation listed in Section A of Annex I in real world conditions at any time before the placing on the market or the putting into service of the AI system on their own or in partnership with one or more deployers or prospective deployers.â€™; \n\n(20) the following Article 60a is inserted: \n\nâ€˜Article 60a \n\nTesting of high-risk AI systems covered by Union harmonisation legislation listed in Section B of Annex I in real-world conditions outside AI regulatory sandboxes \n\n1. Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers of AI enabled products covered by Union harmonisation legislation listed in Section B of Annex I, in accordance with this Article and a voluntary real-world testing agreement, without prejudice to the prohibitions under Article 5. \n\n2. The voluntary real-world testing agreement referred to in paragraph 1 shall be concluded in writing between interested Member States and the Commission. It shall set the requirements for the testing of those AI-enabled products covered by Union harmonisation legislation listed in Section B of Annex I in real-world conditions. \n\n3. Member States, the Commission, market surveillance authorities and public authorities responsible for the management and operation of infrastructure and products covered by Union harmonisation legislation listed in Section B of Annex I shall cooperate closely with each other and in good faith, and shall remove any practical obstacles, including on procedural rules providing access to physical public infrastructure, where this is necessary, to successfully implement the voluntary real-world testing agreement and test AI-enabled products covered by Union harmonisation legislation listed in Section B of Annex. EN 27 EN \n\n4. The signatories of the voluntary real-world testing agreement, shall specify conditions of the testing in real world conditions and establish detailed elements of the real-world testing plan for AI systems covered by Union harmonisation legislation listed in Section B of Annex I. \n\n5. Article 60(2), (5) and (9) shall apply.â€™; \n\n(21) Article 63(1) is replaced by the following: \n\nâ€˜1. SMEs, including start-ups, may comply with certain elements of the quality management system required by Article 17 in a simplified manner. For that purpose, the Commission shall develop guidelines on the elements of the quality management system which may be complied with in a simplified manner considering the needs of SMEs, without affecting the level of protection or the need for compliance with the requirements in respect of high-risk AI systems.â€™; \n\n(22) Article 69 is amended as follows: \n\n(a) paragraph 2 is replaced by the following: \n\nâ€˜2. The Member States may be required to pay fees for the advice and support provided by the experts at a rate equivalent to the remuneration fees applicable to the Commission pursuant to the implementing act referred to in Article 68(1).â€™; \n\n(b) paragraph 3 is deleted. \n\n(23) in Article 70, paragraph 8 is replaced by the following: \n\nâ€˜8. National competent authorities may provide guidance and advice on the implementation of this Regulation, in particular to SMCs and SMEs, including start-ups, taking into account the guidance and advice of the Board and the Commission, as appropriate. Whenever national competent authorities intend to provide guidance and advice with regard to an AI system in areas covered by other Union law, the national competent authorities under that Union law shall be consulted, as appropriate.â€™; \n\n(24) in Article 72, paragraph 3 is replaced by the following: \n\nâ€˜3. The post-market monitoring system shall be based on a post-market monitoring plan. The post-market monitoring plan shall be part of the technical documentation referred to in Annex IV. The Commission shall adopt guidance on the post-market monitoring plan.â€™; \n\n(25) Article 75 is amended as follows: \n\n(a) the heading of Article 75 is replaced by the following: \n\nâ€˜Market surveillance and control of AI systems and mutual assistance â€™; \n\n(b) paragraph 1 is replaced by the following: \n\nâ€˜1. Where an AI system is based on a general-purpose AI model, with the exclusion of AI systems related to products covered by the Union harmonisation legislation listed in Annex I, and that model and that system are developed by the same provider, the AI Office shall be exclusively competent for the supervision and enforcement of that system with the obligations of this Regulation in accordance with the tasks and EN 28 EN \n\nresponsibilities assigned by it to market surveillance authorities. The AI Office shall also be exclusively competent for the supervision and enforcement of the obligations under this Regulation in relation to AI system that constitute or that are integrated into a designated very large online platform or very large online search engine within the meaning of Regulation (EU) 2022/2065. \n\nWhen exercising its tasks of supervision and enforcement under the first subparagraph, the AI Office shall have all the powers of a market surveillance authority provided for in this Section and in Regulation (EU) 2019/1020. The AI Office shall be empowered to take appropriate measures and decisions to adequately exercise its supervisory and enforcement powers. Article 14 of Regulation (EU) 2019/1020 shall apply mutatis mutandis. \n\nThe authorities involved in the application of this Regulation shall cooperate actively in the exercise of these powers, in particular where enforcement actions need to be taken in the territory of a Member State.â€™; \n\n(c) the following paragraphs 1a to 1c are inserted: \n\nâ€˜1a. The Commission shall adopt an implementing act to define the enforcement powers and the procedures for the exercise of those powers of the AI Office, including its ability to impose penalties, such as fines or other administrative sanctions, in accordance with the conditions and ceilings identified in Article 99, in relation to AI systems referenced to in paragraphs 1 and 1a of this Article that are found to be non-compliant with this Regulation, in the context of its monitoring and supervision tasks under this Article.â€™ \n\nâ€˜1b. Article 18 of Regulation (EU) 2019/1020 shall apply mutatis mutandis to providers of AI systems referred to in paragraph 1, without prejudice to more specific procedural rights provided for in this Regulation.â€™ \n\nâ€˜1c. The Commission shall organise and carry out pre-market conformity assessments and tests of AI systems referred to in paragraph 1 that are classified as high-risk and subject to third-party conformity assessment under Article 43 before such AI systems are placed on the market or put into service. These tests and assessments shall verify that the systems comply with the relevant requirements of this Regulation and may be placed on the market or put into service in the Union in accordance with this Regulation. The Commission may entrust the performance of these tests or assessments to notified bodies designated under this Regulation, in which case the notified body shall act on behalf of the Commission. Article 34(1) and (2) shall apply mutatis mutandis to the Commission when exercising its powers under this paragraph. \n\nThe fees for testing and assessment activities shall be levied on the provider of a high-risk AI system who has applied for third-party conformity assessment to the Commission. The costs related to the services entrusted by the Commission to the notified bodies in accordance with this Article shall be directly paid by the provider to the notified body.â€™; \n\n(26) Article 77 is amended as follows: \n\n(a) the heading is replaced by the following: EN 29 EN \n\nâ€˜Powers of authorities protecting fundamental rights and cooperation with market surveillance authorities â€™\n\n(b) paragraph 1 is replaced by the following: \n\nâ€˜1. National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights, including the right to non-discrimination, shall have the power to make a request and access any information or documentation created or maintained from the relevant market surveillance authority under this Regulation in accessible language and format where access to that information or documentation is necessary for effectively fulfilling their mandates within the limits of their jurisdiction.â€™; \n\n(c) the following paragraph 1a and 1b are inserted: \n\nâ€˜1a. Subject to the conditions specified in this Article, the market surveillance authority shall grant the relevant public authority or body referred to in paragraph 1 access to such information or documentation, including by requesting such information or documentation from the provider or the deployer, where necessary.â€™ \n\nâ€˜1b. Market surveillance authorities and public authorities or bodies referred to in paragraph 1 shall cooperate closely and provide each other with mutual assistance necessary for fulfilling their respective mandates, with a view to ensuring coherent application of this Regulation and Union law protecting fundamental rights and streamlining procedures. This shall include, in particular, exchange of information where necessary for the effective supervision or enforcement of this Regulation and the respective other Union legislation.â€™; \n\n(27) Article 95, paragraph 4 is replaced by the following: \n\nâ€˜4. The AI Office and the Member States shall take into account the specific interests and needs of SMCs and SMEs, including start-ups, when encouraging and facilitating the drawing up of codes of conduct.â€™; \n\n(28) in Article 96(1), the second subparagraph is replaced by the following: \n\nâ€˜When issuing such guidelines, the Commission shall pay particular attention to the needs of SMCs and SMEs including start-ups, of local public authorities and of the sectors most likely to be affected by this Regulation.â€™; \n\n(29) Article 99 is amended as follows: \n\n(a) paragraph 1 is replaced by the following: \n\nâ€˜1. In accordance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on penalties and other enforcement measures, which may also include warnings and non-monetary measures, applicable to infringements of this Regulation by operators, and shall take all measures necessary to ensure that they are properly and effectively implemented, thereby taking into account the guidelines issued by the Commission pursuant to Article 96. The penalties provided for shall be effective, proportionate and dissuasive. The Member States shall take into account the interests of SMCs and EN 30 EN \n\nSMEs, including start-ups, and their economic viability when imposing penalties.â€™; \n\n(b) paragraph 6 is replaced by the following: \n\nâ€˜6. In the case of SMCs and SMEs, including start-ups, each fine referred to in this Article shall be up to the percentages or amount referred to in paragraphs 3, 4 and 5, whichever thereof is lower.â€™; \n\n(30) Article 111 is amended as follows: \n\n(a) paragraph 2 is replaced by the following: \n\nâ€˜2. Without prejudice to the application of Article 5 as referred to in Article 113(3), third paragraph, point (a), this Regulation shall apply to operators of high-risk AI systems, other than the systems referred to in paragraph 1 of this Article, that have been placed on the market or put into service before the date of application of Chapter III and corresponding obligations referred to in Article 113, only if, as from that date, those systems are subject to significant changes in their designs. In any case, the providers and deployers of high-risk AI systems intended to be used by public authorities shall take the necessary steps to comply with the requirements and obligations laid down in this Regulation by 2 August 2030.â€™; \n\n(b) the following paragraph 4 is added: \n\nâ€˜4. Providers of AI systems, including general-purpose AI systems, generating synthetic audio, image, video or text content, that have been placed on the market before 2 August 2026 shall take the necessary steps in order to comply with Article 50(2) by 2 February 2027.â€™; \n\n(31) Article 113 is amended as follows: \n\n(a) in the third paragraph, point (d) is added: \n\nâ€˜(d) Chapter III, Sections 1, 2, and 3, shall apply following the adoption of a decision of the Commission confirming that adequate measures in support of compliance with Chapter III are available, from the following dates: \n\n(i) 6 months after the adoption of that decision as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III, and \n\n(ii) 12 months after the adoption of the decision as regards AI systems classified as high-risk pursuant to Article 6(1) and Annex I. \n\nIn the absence of the adoption of the decision within the meaning of subparagraph 1, or where the dates below are earlier than those that follow the adoption of that decision, Chapter III, Sections 1, 2, and 3, shall apply: \n\n(i) on 2 December 2027 as regards AI systems classified as high-risk pursuant to Article 6(2) and Annex III, and \n\n(ii) on 2 August 2028 as regards AI systems classified as high-risk pursuant to Article 6(1) and Annex I.â€™; \n\n(b) in the third paragraph, point (e) is added: EN 31 EN \n\nâ€˜ 3. Articles 102 to 110 shall apply from [the date of entry into application of this Regulation].â€™; \n\n(32) in Annex VIII, section B is deleted; \n\n(33) the following Annex XIV is added: \n\nâ€˜Annex XIV \n\nThe list of codes, categories and corresponding types of AI systems for the purpose of the notification procedure referred to in Article 30 specifying the scope of the designation as notified bodies \n\n1. Introduction \n\nConformity assessment of high-risk AI systems under this Regulation may require involvement of conformity assessment bodies. Only conformity assessment bodies that have been designated in accordance with this Regulation may carry out conformity assessments and only for the activities related to the types of AI systems concerned. The list of codes, categories, and corresponding types of AI systems sets the scope of the designation of conformity assessment bodies notified under Article 30 of this Regulation. \n\n2. List of Codes, categories, and corresponding AI systems \n\n1. AI systems subject to Annex I of the AI Act \n\nAIA Code \n\nAIP 0101  AI systems subject to Annex I.A.1. of the AI Act. \n\nAIP 0102  AI systems subject to Annex I.A.2. of the AI Act. \n\nAIP 0103  AI systems subject to Annex I.A.3. of the AI Act. \n\nAIP 0104  AI systems subject to Annex I.A.4. of the AI Act. \n\nAIP 0105  AI systems subject to Annex I.A.5. of the AI Act. \n\nAIP 0106  AI systems subject to Annex I.A.6. of the AI Act. \n\nAIP 0107  AI systems subject to Annex I.A.7. of the AI Act. \n\nAIP 0108  AI systems subject to Annex I.A.8. of the AI Act. \n\nAIP 0109  AI systems subject to Annex I.A.9. of the AI Act. \n\nAIP 0110  AI systems subject to Annex I.A.10. of the AI Act. \n\nAIP 0111  AI systems subject to Annex I.A.11. of the AI Act. \n\nAIP 0112  AI systems subject to Annex I.A.12. of the AI Act. \n\n2. AI systems subject to Annex III.1 of the AI Act EN 32 EN \n\nAIA Code \n\nAIB 0201  Remote biometric identification systems under Annex III.1.a. of the \n\nAI Act intended to be put into service by Union institutions, bodies, offices or agencies. \n\nAIB 0202  Biometric categorisation AI systems under Annex III.1.b. of the AI \n\nAct intended to be put into service by Union institutions, bodies, \n\noffices or agencies. \n\nAIB 0203  Emotion recognition AI systems under Annex III.1.c. of the AI Act \n\nintended to be put into service by Union institutions, bodies, offices or agencies. \n\nAIB 0204  Remote biometric identification systems under Annex III.1.a. of the \n\nAI Act intended to be put into service by law enforcement, \n\nimmigration or asylum authorities. \n\nAIB 0205  Biometric categorisation AI systems under Annex III.1.b. of the AI \n\nAct intended to be put into service by law enforcement, immigration or asylum authorities. \n\nAIB 0206  Emotion recognition AI systems under Annex III.1.c. of the AI Act \n\nintended to be put into service by law enforcement, immigration or \n\nasylum authorities. \n\nAIB 0207  Remote biometric identification systems under Annex III.1.a. of the \n\nAI Act (general). \n\nAIB 0208  Biometric categorisation AI systems under Annex III.1.b. of the AI \n\nAct (general). \n\nAIB 0209  Emotion recognition AI systems under Annex III.1.c. of the AI Act \n\n(general). \n\n3. AI technology-specific codes \n\na) Symbolic AI, expert systems and mathematical optimization \n\nAIA Code \n\nAIH 0101  Logic - and knowledge -based AI systems that infer from encoded \n\nknowledge or symbolic representation, expert systems \n\nAIH 0102  Logic-based AI systems, excluding basic data processing \n\nb) Machine learning, excluding GPAI and single modality generative AI \n\nAIA Code \n\nAIH 0201  AI systems that process structured data \n\nAIH 0202  AI systems that process signal and audio data \n\nAIH 0203  AI systems that process text data EN 33 EN \n\nAIH 0204  AI systems that process image and video \n\nAIH 0205  AI systems that learn from their environment, excluding agentic AI \n\nc) AI systems based on GPAI or single modality generative AI \n\nAIA Code \n\nAIH 0301  Single modality generative AI systems \n\nAIH 0302  Multimodal generative AI systems, including AI systems based on \n\nGPAI models \n\nd) Agentic AI \n\nAIA Code \n\nAIH 0401  Agentic AI \n\n3. Application for designation \n\nConformity assessment bodies shall use the lists of codes, categories and corresponding types of AI systems set out in this Annex when specifying the types of AI systems in the application for designation referred to in Article 29 of this Regulation.â€™. \n\nArticle 2 \n\nAmendments to Regulation (EU) 2018/1139 \n\nRegulation (EU) 2018/1139 is amended as follows: \n\n(1) in Article 27, the following paragraph is added: \n\nâ€˜3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council 14 , the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(2) in Article 31, the following paragraph is added:  \n\n> 14 Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/2024/1689/oj).\n\n# EN 34 EN \n\nâ€˜3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(3) in Article 32, the following paragraph is added: \n\nâ€˜3. When adopting delegated acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(4) in Article 36, the following paragraph is added: \n\nâ€˜3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(5) in Article 39 the following paragraph is added: \n\nâ€˜3. When adopting delegated acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(6) in Article 50, the following paragraph is added: \n\nâ€˜3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™; \n\n(7) in Article 53, the following paragraph is added: \n\nâ€˜3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.â€™. \n\nArticle 3 \n\nEntry into force and application \n\nThis Regulation shall enter into force on the third day following that of its publication in the \n\nOfficial Journal of the European Union .EN 35 EN \n\nThis Regulation shall be binding in its entirety and directly applicable in all Member States. \n\nDone at Brussels, \n\nFor the European Parliament For the Council \n\nThe President The President EN 1 EN \n\nLEGISLATIVE FINANCIAL AND DIGITAL STATEMENT \n\n1. FRAMEWORK OF THE PROPOSAL/INITIATIVE ................................................. 3 \n\n1.1. Title of the proposal/initiative ...................................................................................... 3 \n\n1.2. Policy area(s) concerned .............................................................................................. 3 \n\n1.3. Objective(s) .................................................................................................................. 3 \n\n1.3.1. General objective(s) ..................................................................................................... 3 \n\n1.3.2. Specific objective(s) ..................................................................................................... 3 \n\n1.3.3. Expected result(s) and impact ...................................................................................... 3 \n\n1.3.4. Indicators of performance ............................................................................................ 3 \n\n1.4. The proposal/initiative relates to: ................................................................................. 4 \n\n1.5. Grounds for the proposal/initiative .............................................................................. 4 \n\n1.5.1. Requirement(s) to be met in the short or long term including a detailed timeline for roll-out of the implementation of the initiative ............................................................ 4 \n\n1.5.2. Added value of EU involvement (it may result from different factors, e.g. coordination gains, legal certainty, greater effectiveness or complementarities). For the purposes of this section â€˜added value of EU involvementâ€™ is the value resulting from EU action, that is additional to the value that would have been otherwise created by Member States alone. ................................................................................. 4 \n\n1.5.3. Lessons learned from similar experiences in the past .................................................. 4 \n\n1.5.4. Compatibility with the multiannual financial framework and possible synergies with other appropriate instruments....................................................................................... 5 \n\n1.5.5. Assessment of the different available financing options, including scope for redeployment ................................................................................................................ 5 \n\n1.6. Duration of the proposal/initiative and of its financial impact .................................... 6 \n\n1.7. Method(s) of budget implementation planned ............................................................. 6 \n\n2. MANAGEMENT MEASURES .................................................................................. 8 \n\n2.1. Monitoring and reporting rules .................................................................................... 8 \n\n2.2. Management and control system(s) ............................................................................. 8 \n\n2.2.1. Justification of the budget implementation method(s), the funding implementation mechanism(s), the payment modalities and the control strategy proposed .................. 8 \n\n2.2.2. Information concerning the risks identified and the internal control system(s) set up to mitigate them............................................................................................................ 8 \n\n2.2.3. Estimation and justification of the cost-effectiveness of the controls (ratio between the control costs and the value of the related funds managed), and assessment of the expected levels of risk of error (at payment & at closure) ........................................... 8 \n\n2.3. Measures to prevent fraud and irregularities................................................................ 9 \n\n3. ESTIMATED FINANCIAL IMPACT OF THE PROPOSAL/INITIATIVE............ 10 \n\n3.1. Heading(s) of the multiannual financial framework and expenditure budget line(s) affected ....................................................................................................................... 10 EN 2 EN \n\n3.2. Estimated financial impact of the proposal on appropriations ................................... 12 \n\n3.2.1. Summary of estimated impact on operational appropriations.................................... 12 \n\n3.2.1.1. Appropriations from voted budget ............................................................................. 12 \n\n3.2.1.2. Appropriations from external assigned revenues ....................................................... 17 \n\n3.2.2. Estimated output funded from operational appropriations......................................... 22 \n\n3.2.3. Summary of estimated impact on administrative appropriations ............................... 24 \n\n3.2.3.1. Appropriations from voted budget .............................................................................. 24 \n\n3.2.3.2. Appropriations from external assigned revenues ....................................................... 24 \n\n3.2.3.3. Total appropriations ................................................................................................... 24 \n\n3.2.4. Estimated requirements of human resources.............................................................. 25 \n\n3.2.4.1. Financed from voted budget....................................................................................... 25 \n\n3.2.4.2. Financed from external assigned revenues ................................................................ 26 \n\n3.2.4.3. Total requirements of human resources ..................................................................... 26 \n\n3.2.5. Overview of estimated impact on digital technology-related investments ................ 28 \n\n3.2.6. Compatibility with the current multiannual financial framework.............................. 28 \n\n3.2.7. Third-party contributions ........................................................................................... 28 \n\n3.3. Estimated impact on revenue ..................................................................................... 29 \n\n4. DIGITAL DIMENSIONS .......................................................................................... 29 \n\n4.1. Requirements of digital relevance.............................................................................. 30 \n\n4.2. Data ............................................................................................................................ 30 \n\n4.3. Digital solutions ......................................................................................................... 31 \n\n4.4. Interoperability assessment ........................................................................................ 31 \n\n4.5. Measures to support digital implementation .............................................................. 32 EN 3 EN \n\n1. FRAMEWORK OF THE PROPOSAL/INITIATIVE \n\n1.1. Title of the proposal/initiative \n\nProposal for a Regulation of the European Parliament and of the Council amending \n\nRegulations (EU) 2024/1689 and (EU) 2018/1139 as regards the simplification of the \n\nimplementation of harmonised rules on artificial intelligence (Digital Omnibus on \n\nAI) \n\n1.2. Policy area(s) concerned \n\nCommunications Networks, Content and Technology; \n\nInternal Market, Industry, Entrepreneurship and SMEs \n\nThe budgetary impact concerns the new tasks entrusted with the AI Office. \n\n1.3. Objective(s) \n\n1.3.1. General objective(s) \n\n1. To strengthen the monitoring and supervision of certain categories of AI systems \n\nby the AI Office. \n\n2. To facilitate the development and testing at EU level of innovative AI systems \n\nunder strict regulatory oversight before these systems are placed on the market or \n\notherwise put into service. \n\n1.3.2. Specific objective(s) \n\nSpecific objective No 1 \n\nTo enhance governance and effective enforcement of the AI Act rules related to AI \n\nsystems by reinforcing the powers and procedures applicable as well as by providing \n\nfor new resources for the AI Office in charge of the enforcement. \n\nSpecific objective No 2 \n\nTo provide for the establishment of a sandbox at EU level, enabling cross border \n\nactivities and testing. \n\n1.3.3. Expected result(s) and impact \n\n> Specify the effects which the proposal/initiative should have on the beneficiaries/groups targeted.\n\nAI providers should benefit from a centralised level of governance and the access to \n\nan EU-level sandbox for certain categories of AI systems, avoiding duplication of \n\nprocedures and costs. \n\n1.3.4. Indicators of performance \n\n> Specify the indicators for monitoring progress and achievements.\n\nIndicator 1 \n\nNumber of AI systems falling under the scope of the monitoring and supervision \n\ntasks to be carried out by the AI Office. \n\nIndicator 2 \n\nNumber of providers and prospective providers requesting access to the sandbox at \n\nEU level. EN 4 EN \n\n1.4. The proposal/initiative relates to:  \n\n> ï‚¨\n\na new action  \n\n> ï‚¨\n\na new action following a pilot project / preparatory action 26  \n\n> ï¸\n\nthe extension of an existing action  \n\n> ï‚¨\n\na merger or redirection of one or more actions towards another/a new action \n\n1.5. Grounds for the proposal/initiative \n\n1.5.1. Requirement(s) to be met in the short or long term including a detailed timeline for roll-out of the implementation of the initiative \n\nThe additional elements relevant for the enhancement of the governance structure of \n\nthe AI Office should be in place before the entry into application of the provisions \n\napplicable to AI systems. \n\nThe first EU sandbox is expected to be operational in 2028, although some key \n\nsetting details should be established beforehand. \n\n1.5.2. Added value of EU involvement (it may result from different factors, e.g. coordination gains, legal certainty, greater effectiveness or complementarities). For the purposes of this section 'added value of EU involvement' is the value resulting from EU action, that is additional to the value that would have been otherwise created by Member States alone. \n\nThe AI Office will have the power to monitor and supervise the compliance of all AI \n\nsystems based on general-purpose AI (GPAI) models, where the model and the \n\nsystem are developed by the same provider, as well as all AI systems embedded in or \n\nconstituting very large online platforms or search engines, even if the system and \n\nGPAI model provider are different. The tasks that the AI Office would need to carry \n\nout for this vast range of AI systems include requesting full access to documentation, \n\ntraining/validation/testing datasets, and, when necessary, the source code of high-risk \n\nAI systems, supervising real-world testing, identifying and evaluating risks, dealing \n\nwith serious incidents, taking preventive and corrective measures while ensuring \n\ncooperation with national market surveillance authorities, dealing with AI systems \n\nclassified as not high-risk by the provider, dealing with complaints of non-\n\ncompliance, and imposing penalties. Moreover, to allow market access for AI \n\nsystems in the scope of this provision which are also subject to pre-market third-\n\nparty conformity assessment under the AI Act, the AI Office will be the responsible \n\nbody to carry out conformity assessments. All these actions require resources and a \n\nset of enforcement procedures to be developed and implemented, as well as the \n\nappropriate technical support to assess and evaluate systems. \n\nThe AI Officeâ€™s role in ensuring compliance would also involve ensuring synergies \n\nwith the evaluation of the GPAI models, which would strengthen the overall \n\nevaluations of models and systems provided by the same provider. This would enable \n\na more comprehensive understanding of the AI systems and their associated risks,  \n\n> 26 As referred to in Article 58(2), point (a) or (b) of the Financial Regulation.\n\n# EN 5 EN \n\nallowing for more effective monitoring and enforcement. The AI Office will also \n\nneed to consider the unique challenges posed by agentic AI, which can operate \n\nautonomously and make decisions that may have significant consequences, and \n\ndevelop strategies to address these risks in line with Commission policies. \n\nThe enhancement of the AI Officeâ€™s governance would bring numerous benefits to \n\nthe regulation of AI systems in the EU. One of the primary advantages is the \n\nconsistency and coherence it would ensure in the application of the AI Act across the \n\nEU. By having a single authority overseeing the implementation of the AI Act in \n\nrelation to certain categories of AI systems, the risk of conflicting interpretations and \n\ndecisions would be significantly reduced, providing clarity and certainty for \n\ncompanies operating in the EU. \n\nFurthermore, this would simplify the regulatory landscape for companies, as they \n\nwould only need to deal with one regulator, rather than multiple national authorities. \n\nThis would reduce the complexity and administrative burden associated with \n\nnavigating different regulatory frameworks, allowing companies to focus on \n\ninnovation and growth. The centralised approach would also enable the development \n\nwithin the Commission of specialised expertise in AI systems and GPAI models, \n\nenabling more effective monitoring and enforcement of the AI Act. \n\nThis approach would avoid diverging national enforcement actions on the AI systems \n\nconcerned that may lead to the fragmentation of the internal market and decrease \n\nlegal certainty for operators. This would also address the challenges faced by \n\nMember States in securing specialised resources to staff their authorities responsible \n\nfor implementing the AI Act and overseeing AI systems within their territories. By \n\ncentralizing market surveillance authoritiesâ€™ powers within the AI Office, this \n\nscenario would enable the AI Office to assume responsibility for evaluating and \n\nmonitoring complex AI systems provided by the same model provider, as well as AI \n\nsystems constituting or embedded into platforms, thereby alleviating the burden on \n\nnational authorities. This would leverage the AI Office's existing expertise in \n\nevaluating GPAI models and monitoring their compliance, creating a unique \n\nconcentration of specialized knowledge and capabilities. As a result, the AI Office \n\nwould be well-positioned to provide consistent and effective oversight, while also \n\nsupporting Member States in their efforts to implement the AI Act and ensure a \n\nharmonized regulatory environment across the EU. With the AI Office handling \n\nthese additional tasks, national authorities could focus more on their enforcement \n\nactions under the AI Act, allowing for a more efficient allocation of resources and a \n\nmore effective implementation of the AI Act across the EU. \n\n1.5.3. Lessons learned from similar experiences in the past \n\nThe European Commission's experience in enforcing the Digital Services Act (DSA) \n\nprovides valuable lessons that can be applied to the enforcement of the AI Act. In \n\nparticular, the establishment of a robust and transparent enforcement framework, \n\nwhich sets out clear procedures for investigating and addressing breaches of the DSA \n\nand the close cooperation with national authorities, to ensure that enforcement \n\nactions are coordinated and effective, represent relevant elements in this context. \n\nThe Commissionâ€™s experience with DSA enforcement has shown that this approach \n\ncan be effective in promoting compliance and protecting users' rights. For example, \n\nthe Commission has already taken action against several online platforms for \n\nbreaches of the DSA, and has worked with national authorities to develop guidance \n\nand best practices for compliance. EN 6 EN \n\nBy building on the lessons learned from DSA enforcement, the Commission can \n\ndevelop an effective enforcement framework for the AI Act that promotes \n\ncompliance, and supports the development of a trustworthy and innovative AI \n\necosystem in the EU. This will involve enhancing the AI Office enforcement role to \n\nduly monitor and supervision certain categories of AI systems, and working closely \n\nwith national authorities to ensure that the AI Act is enforced in a consistent and \n\neffective manner. \n\nThe possibility to provide for an EU-level sandbox should be seen as complementing \n\nthe sandboxes established at national level and should be implemented in a way to \n\nfacilitate cross-border cooperation between national competent authorities. \n\n1.5.4. Compatibility with the multiannual financial framework and possible synergies with other appropriate instruments \n\nThe amendments proposed to the AI Act within this initiative would result in a \n\nsignificant increase in the number of AI systems subject to the monitoring and \n\nsupervision of the AI Office, with a corresponding rise in the number of systems \n\npotentially eligible to participate in an EU-level sandbox. To effectively manage this \n\nexpansion, it is essential to strengthen the European regulatory and coordination \n\nfunction, as proposed in this initiative. This reinforcement would enable the AI \n\nOffice to efficiently oversee the growing number of AI systems, ensure compliance \n\nwith the regulatory framework, and provide a supportive environment for innovation \n\nand testing through the EU-level sandbox. \n\n1.5.5. Assessment of the different available financing options, including scope for redeployment \n\nThe AI Office will make an effort in order to redeploy part of the staff allocated but \n\ncould do it only partially (15 FTEs) as the staff is currently fully allocated to tasks \n\ndirectly linked to ensuring a timely and correct implementation of the AI Act. New \n\nresources will be needed (estimated in 38 additional FTEs) to efficiently exercise the \n\nnew enforcement tasks. \n\nIn particular, the AI Office plans to identify colleagues with legal and procedural \n\nexpertise who can take on part of the upcoming new enforcement tasks. At this stage, \n\nwe estimate that around 5 CAs with relevant profiles can be redeployed for this \n\npurpose. \n\nIn addition, the AI Office will make an effort to redeploy 5 officials. \n\nThe AI Office envisages to make fully operational the EU-level sandbox for AI \n\nsystems falling under its monitoring in 2028, which will make possible a\n\nredeployment of 3 CAs needed to set up and run the sandbox. This phased approach \n\nwould enable to ensure the full operational capacity of the sandbox by 2028, and in \n\nparticular will also give the AI Office the time to identify the most suitable staff to \n\ncover this task and ensure proper project management for facilitating the \n\ndevelopment, training, testing, and validation of innovative AI systems. \n\nIn addition, the AI Office will explore opportunities to expand the scope of IT tools \n\n(currently mostly in development or pre-launch phase) supporting the AI Act to also \n\ncover relevant new enforcement activities (i.e. case handling, AI system registry, \n\nmonitoring and reporting, exchange of information with authorities). 2 FTEs with IT \n\nand administrative profiles will be redeployed to manage these IT tools. This would \n\nhelp to partially cover the management needs related to the new tasks. EN 7 EN \n\nOverall, these redeployment efforts and synergies will help to address some of the \n\nstaffing needs for the new enforcement tasks, while additional resources will be \n\nnecessary to ensure the effective implementation of the AI Act. \n\nAdditional staff will be funded under DEP support, given that the objectives of the \n\nproposed amendments contribute directly to one key objective of Digital Europe â€“ \n\naccelerating AI development and deployment in Europe. EN 8 EN \n\n1.6. Duration of the proposal/initiative and of its financial impact \n\nï‚¨ limited duration \n\nâ€“ ï‚¨ in effect from [DD/MM]YYYY to [DD/MM]YYYY \n\nâ€“ ï‚¨ financial impact from YYYY to YYYY for commitment appropriations and from YYYY to YYYY for payment appropriations. \n\nï¸ unlimited duration \n\nâ€“ Implementation with a start-up period from 2026 to 2027, \n\nâ€“ followed by full-scale operation. \n\n1.7. Method(s) of budget implementation planned \n\nï¸ Direct management by the Commission \n\nâ€“ ï‚¨ by its departments, including by its staff in the Union delegations; \n\nâ€“ ï‚¨ by the executive agencies \n\nï‚¨ Shared management with the Member States \n\nï‚¨ Indirect management by entrusting budget implementation tasks to: \n\nâ€“ ï‚¨ third countries or the bodies they have designated \n\nâ€“ ï‚¨ international organisations and their agencies (to be specified) \n\nâ€“ ï‚¨ the European Investment Bank and the European Investment Fund \n\nâ€“ ï‚¨ bodies referred to in Articles 70 and 71 of the Financial Regulation \n\nâ€“ ï‚¨ public law bodies \n\nâ€“ ï‚¨ bodies governed by private law with a public service mission to the extent that they are provided with adequate financial guarantees \n\nâ€“ ï‚¨ bodies governed by the private law of a Member State that are entrusted with the implementation of a public-private partnership and that are provided with adequate financial guarantees \n\nâ€“ ï‚¨ bodies or persons entrusted with the implementation of specific actions in the common foreign and security policy pursuant to Title V of the Treaty on European Union, and identified in the relevant basic act \n\nâ€“ ï‚¨ï‚  bodies established in a Member State, governed by the private law of a Member State or Union law and eligible to be entrusted, in accordance with sector-specific rules, with the implementation of Union funds or budgetary guarantees, to the extent that such bodies are controlled by public law bodies or by bodies governed by private law with a public service mission, and are provided with adequate financial guarantees in the form of joint and several liability by the controlling bodies or equivalent financial guarantees and which may be, for each action, limited to the maximum amount of the Union support. EN 9 EN \n\n2. MANAGEMENT MEASURES \n\n2.1. Monitoring and reporting rules \n\nSpecify frequency and conditions. \n\nThe strengthened dispositions will be reviewed and evaluated with the entire AI Act \n\nin August 2029. The Commission will report on the findings of the evaluation to the \n\nEuropean Parliament, the Council and the European Economic and Social \n\nCommittee. \n\n2.2. Management and control system(s) \n\n2.2.1. Justification of the budget implementation method(s), the funding implementation mechanism(s), the payment modalities and the control strategy proposed \n\nThe regulation reinforces the European policy with regard to harmonised rules for \n\nthe provision of artificial intelligence systems in the internal market while ensuring \n\nthe respect of safety and fundamental rights. The simplified single supervision \n\nensures consistency for the cross-border application of the obligations under this \n\nRegulation. \n\nIn order to face these new tasks, it is necessary to appropriately resource the \n\nCommissionâ€™s services. The enforcement of the new regulation is estimated to \n\nrequire 53 FTE. \n\n2.2.2. Information concerning the risks identified and the internal control system(s) set up to mitigate them \n\nThe risks correspond to the standard risks of Commission operations and are \n\nadequately covered by existing standard risk minimising procedures. \n\n2.2.3. Estimation and justification of the cost-effectiveness of the controls (ratio between the control costs and the value of the related funds managed), and assessment of the expected levels of risk of error (at payment & at closure) \n\nFor the meeting expenditure, given the low value per transaction (e.g. refunding \n\ntravel costs for a delegate for a meeting), standard control procedures seem \n\nsufficient. \n\n2.3. Measures to prevent fraud and irregularities \n\nSpecify existing or envisaged prevention and protection measures, e.g. from the anti-fraud strategy. \n\nThe existing fraud prevention measures applicable to the Commission will cover the \n\nadditional appropriations necessary for this Regulation. EN 10 EN \n\n3. ESTIMATED FINANCIAL IMPACT OF THE PROPOSAL/INITIATIVE \n\n3.1. Heading(s) of the multiannual financial framework and expenditure budget line(s) affected \n\nâ€¢ Existing budget lines \n\nIn order of multiannual financial framework headings and budget lines. \n\n> Heading of multiannual financial framework\n\nBudget line  Type of expenditure  Contribution \n\nNumber  Diff./Non-diff. 27 \n\n> from EFTA countries\n> 28\n> from candidate countries and potential candidates\n> 29\n> From other third countries\n> other assigned revenue\n\n7 20 02 06 Administrative expenditure  Nondiff  No \n\n1 02 04 03 DEP Artificial Intelligence  Diff.  YES  NO  yes  NO \n\n1 02 01 30 01 Support expenditure for the Digital Europe programme  Nondiff  yes  yes  \n\n> 27\n\nDiff. = Differentiated appropriations / Non-diff. = Non-differentiated appropriations.  \n\n> 28\n\nEFTA: European Free Trade Association.  \n\n> 29\n\nCandidate countries and, where applicable, potential candidates from the Western Balkans. EN 11 EN \n\n3.2. Estimated financial impact of the proposal on appropriations \n\n3.2.1. Summary of estimated impact on operational appropriations \n\nâ€“ ï‚¨ The proposal/initiative does not require the use of operational appropriations \n\nâ€“ ï¸ The proposal/initiative requires the use of operational appropriations, as explained below \n\n3.2.1.1. Appropriations from voted budget \n\n[\n\n> EUR million (to three decimal places)\n\nHeading of multiannual financial framework  1\n\nDG: CNECT \n\nYear  Year  Year  Year  After 2027  TOTAL MFF 2021-2027 \n\n2024  2025  2026  2027  After 2027 \n\nBudget line 02 04 03   \n\n> Commitments (1a)\n\n0,500 30  0,500 31  1,000   \n\n> Payments (2a)\n\n0,500  0,500  1,000 \n\nAppropriations of an administrative nature financed from the envelope of specific programmes  \n\n> 30\n\nThis budget is already eamarked in the DEP WP 26-27 for the AI office  \n\n> 31\n\nThis budget is already eamarked in the DEP WP 26-27 for the AI office EN 12 EN \n\nBudget line 02 01 30 01  (3)  2,642 32  6,283  33  7,283  8,925 \n\nTOTAL appropriations \n\nfor DG CNECT \n\nCommitments  =1a+1b+3  3,142  6,783  7,283  9,925 \n\nPayments  =2a+2b+3  2,642  6,783  7,783  9,925 \n\nTOTAL \n\nYear  Year  Year  Year  After 2027  TOTAL MFF 2021-2027 \n\n2024  2025  2026  2027  After 2027 \n\nBudget line 02 04 03 \n\nCommitments  (1a)  0,500 34  0,500 35  1,000 \n\nPayments  (2a)  0,500  0,500  1,000 \n\nAppropriations of an administrative nature financed from the envelope of specific programmes  \n\n> 32\n\nThis budget corresponds to [48] additional FTEs for 6 months [(43 CAs and 5 SNEs)], the baseline being the staffing level agreed in the context of the 2026 budgetary procedure. The budget will be redeployed in the DEP admin envelope to cover the additional costs.  \n\n> 33\n\nThe amount will be redeployed from 02.0403 (SO2 artificial intelligence) in 2027, the request will be introduced in the 2027 budgetary procedure.  \n\n> 34\n\nThis budget is already earmarked in the DEP WP 26-27 for the AI Office.  \n\n> 35\n\nThis budget is already earmarked in the DEP WP 26-27 for the AI Office. EN 13 EN \n\nBudget line 02 01 30 01  (3)  2,642 36  6,283  37  7,283  8,925 \n\nTOTAL appropriations \n\nfor DG CNECT \n\nCommitments  =1a+1b+3  3,142  6,783  7,283  9,925 \n\nPayments  =2a+2b+3  2,642  6,783  7,783  9,925 \n\n]\n\n[\n\nHeading of multiannual financial framework  7 â€˜Administrative expenditureâ€™ \n\nDG: CNECT  Year  Year  Year  Year  TOTAL \n\nMFF 2021-2027 2024  2025  2026  2027 \n\nï‚Ÿ Human resources  0,940  0,940  1,880 \n\nï‚Ÿ Other administrative expenditure  0,025  0,025  0,050 \n\nTOTAL DG CNECT  Appropriations  0,965  0,965  1,930  \n\n> 36\n\nThis budget corresponds to 48 additional FTEs for 6 months (43 CAs and 5 SNEs), the baseline being the staffing level agreed in the context of the 2026 budgetary procedure. The budget will be redeployed in the DEP admin envelope to cover the additional costs.  \n\n> 37\n\nThe amount will be redeployed from 02.0403 (SO2 artificial intelligence) in 2027, the request will be introduced in the 2027 budgetary procedure. EN 14 EN \n\nTOTAL appropriations under HEADING 7 of the multiannual financial framework \n\n(Total commitments = Total payments) \n\n0,965  0,965  1,930 \n\nEUR million (to three decimal places) \n\nYear  Year  Year  Year  After 2027  TOTAL MFF 2021-2027 2024  2025  2026  2027 \n\nTOTAL appropriations under HEADINGS 1 to 7  Commitments  4,107  7,748  8,248  11,855 \n\nof the multiannual financial framework  Payments  3,607  7,748  8,748  11,855 \n\n]\n\n3.2.2. Estimated output funded from operational appropriations (not to be completed for decentralised agencies) \n\nCommitment appropriations in EUR million (to three decimal places) \n\nIndicate objectives and outputs \n\nïƒ²\n\nYear \n\n2024 \n\nYear \n\n2025 \n\nYear \n\n2026 \n\nYear \n\n2027 \n\nEnter as many years as necessary to show the duration of the impact (see Section1.6)  TOTAL \n\nOUTPUTS \n\nType 38  Avera ge cost \n\n> No\n\nCost \n\n> No\n\nCost \n\n> No\n\nCost \n\n> No\n\nCost \n\n> No\n\nCost \n\n> No\n\nCost \n\n> No\n\nCost  Total No \n\nTotal cost \n\nSPECIFIC OBJECTIVE No 1 39 â€¦ \n\n> 38\n\nOutputs are products and services to be supplied (e.g. number of student exchanges financed, number of km of roads built, etc.). EN 15 EN \n\n- Output \n\n- Output \n\n- Output \n\nSubtotal for specific objective No 1 \n\nSPECIFIC OBJECTIVE No 2 ... \n\n- Output \n\nSubtotal for specific objective No 2 \n\nTOTALS  \n\n> 39\n\nAs described in Section 1.3.2. â€˜Specific objective(s)â€™ EN 16 EN \n\n3.2.3. Summary of estimated impact on administrative appropriations \n\nâ€“ ï‚¨ The proposal/initiative does not require the use of appropriations of an administrative nature \n\nâ€“ ï¸ The proposal/initiative requires the use of appropriations of an administrative nature, as explained below \n\n3.2.3.1. Appropriations from voted budget \n\n[\n\nVOTED APPROPRIATIONS  Year  Year  Year  Year  TOTAL 2021 - 2027 \n\n2024  2025  2026  2027                   \n\n> HEADING 7\n> Human resources 0,940 0,940 1,880\n> Other administrative expenditure 0,025 0,025 0,050\n> Subtotal HEADING 7 0,965 0,965 1,930\n> Outside HEADING 7\n> Human resources 2,429 4,858 7,287\n> Other expenditure of an administrative nature 0,213 1,425 1,638\n> Subtotal outside HEADING 7 2,642 6,283 8,925\n\n]\n\nThe appropriations required for human resources and other expenditure of an administrative nature will be met by appropriations from the DG that are already assigned to management of the action and/or have been redeployed within the DG, together, if necessary, with any additional allocation which may be granted to the managing DG under the annual allocation procedure and in the light of budgetary constraints. \n\n3.2.4. Estimated requirements of human resources \n\nâ€“ ï‚¨ The proposal/initiative does not require the use of human resources \n\nâ€“ ï¸ The proposal/initiative requires the use of human resources, as explained below EN 17 EN \n\n3.2.4.1. Financed from voted budget \n\nEstimate to be expressed in full-time equivalent units (FTEs) \n\n[\n\nVOTED APPROPRIATIONS  Year  Year  Year  Year \n\n2024  2025  2026  2027 \n\nï‚Ÿ Establishment plan posts (officials and temporary staff) \n\n20 01 02 01 (Headquarters and Commissionâ€™s Representation Offices)  0 0 5 5\n\n20 01 02 03 (EU Delegations)  0 0 0 0\n\n01 01 01 01 (Indirect research)  0 0 0 0\n\n01 01 01 11 (Direct research)  0 0 0 0\n\nOther budget lines (specify)  0 0 0 0\n\nâ€¢ External staff (in FTEs) \n\n20 02 01 (AC, END from the â€˜global envelopeâ€™)  0 0 0 0\n\n20 02 03 (AC, AL, END and JPD in the EU Delegations)  0 0 0 0\n\nAdmin. Support line [XX.01.YY.YY] \n\n- at Headquarters  0 0 0 0\n\n- in EU Delegations  0 0 0 0\n\n01 01 01 02 (AC, END - Indirect research)  0 0 0 0\n\n01 01 01 12 (AC, END - Direct research)  0 0 0 0\n\nOther budget lines (specify) - Heading 7  0 0 0 0\n\nOther budget lines (02 01 30 01) - Outside Heading 7  0 0 48  48 \n\nTOTAL  0 0 53  53 \n\n]\n\nThe staff required to implement the proposal (in FTEs): \n\nTo be covered by current staff \n\nExceptional additional staff* EN 18 EN \n\navailable in the \n\nCommission services \n\nTo be financed under Heading 7 or Research \n\nTo be financed from BA line \n\nTo be financed from fees \n\nEstablishment plan posts \n\n5 N/A \n\nExternal staff (CA, SNEs, INT) \n\n10  38 \n\nDescription of tasks to be carried out by:  \n\n> Officials and temporary staff The strengthening of the central supervision by the AI Office will lead to a significant increase in the number of AI systems. These task cannot be carried out by the current staff levels, which are only sufficient for the current scope of supervision. External staff\n\n3.2.5. Overview of estimated impact on digital technology-related investments \n\nCompulsory: the best estimate of the digital technology-related investments entailed by the proposal/initiative should be included in the table below. \n\nExceptionally, when required for the implementation of the proposal/initiative, the appropriations under Heading 7 should be presented in the designated line. \n\nThe appropriations under Headings 1-6 should be reflected as â€˜Policy IT expenditure on operational programmesâ€™. This expenditure refers to the operational budget to be used to re-use/ buy/ develop IT platforms/ tools directly linked to the implementation of the initiative and their associated investments (e.g. licences, studies, data storage etc). The information provided in this table should be consistent with details presented under Section 4 â€˜Digital dimensionsâ€™. EN 19 EN \n\nTOTAL Digital and IT appropriations \n\nYear  Year  Year  Year  TOTAL MFF 2021 -2027 2024  2025  2026  2027 \n\n> HEADING 7\n\nIT expenditure (corporate)  0.000  0.000  0.000  0.000  0.000      \n\n> Subtotal HEADING 7 0.000 0.000 0.000 0.000 0.000\n> Outside HEADING 7\n\nPolicy IT expenditure on operational \n\nprogrammes  0.000  0.000  0.000  0.000  0.000           \n\n> Subtotal outside HEADING 7 0.000 0.000 0.000 0.000 0.000\n> TOTAL 0.000 0.000 0.000 0.000 0.000\n\n3.2.6. Compatibility with the current multiannual financial framework \n\nThe proposal/initiative: \n\nâ€“ ï¸ can be fully financed through redeployment within the relevant heading of the multiannual financial framework (MFF) \n\nThe amounts will be redeployed from 02.013001 support expenditure for the Digital Europe Programme for 2026 and from 02.0403 (SO2 artificial intelligence) for 2027. \n\nâ€“ ï‚¨ requires use of the unallocated margin under the relevant heading of the MFF and/or use of the special instruments as defined in the MFF Regulation \n\nâ€“ ï‚¨ requires a revision of the MFF \n\n3.2.7. Third-party contributions \n\nThe proposal/initiative: \n\nâ€“ ï¸ does not provide for co-financing by third parties \n\nâ€“ ï‚¨ provides for the co-financing by third parties estimated below: \n\nAppropriations in EUR million (to three decimal places) EN 20 EN \n\nYear \n\n2024 \n\nYear \n\n2025 \n\nYear \n\n2026 \n\nYear \n\n2027  Total \n\nSpecify the co-financing body \n\nTOTAL appropriations co-financed \n\n3.3. Estimated impact on revenue \n\nâ€“ ï¸ The proposal/initiative has no financial impact on revenue. \n\nâ€“ ï‚¨ The proposal/initiative has the following financial impact: \n\nâ€“ ï‚¨ on own resources \n\nâ€“ ï‚¨ on other revenue \n\nâ€“ ï‚¨ please indicate, if the revenue is assigned to expenditure lines \n\nEUR million (to three decimal places)         \n\n> Budget revenue line: Appropriations available for the current financial year\n> Impact of the proposal/initiative 40\n> Year 2024 Year 2025 Year 2026 Year 2027\n> Article â€¦â€¦â€¦â€¦.\n\nFor assigned revenue, specify the budget expenditure line(s) affected.  \n\n> 40\n\nAs regards traditional own resources (customs duties, sugar levies), the amounts indicated must be net amounts, i.e. gross amounts after deduction of 20% for collection costs. EN 21 EN \n\nOther remarks (e.g. method/formula used for calculating the impact on revenue or any other information). \n\n4. DIGITAL DIMENSIONS \n\n4.1. Requirements of digital relevance \n\nReference to the requirement  Requirement description  Actors affected or concerned by the requirement \n\nHigh-level Processes  Categories \n\nArticle 1(5)  Inserting Article 4a : Allowing providers and deployers of AI systems and AI models to exceptionally process special categories of personal data to the extent necessary for the purpose of ensuring bias detection and correction, subject to certain conditions. \n\nProviders and deployers of AI systems and AI models \n\nConcerned data subjects \n\nData processing  Data \n\nArticle 1(8)  Amending Article 11(1), second subparagraph : Relating to the technical documentation of high-risk AI systems that needs to be drawn up before that system is placed on the market or put into service. SMEs and SMCs are given certain regulatory privileges as concerns this provision of information. \n\nProviders of high-risk AI systems (including SMCs and SMEs) \n\nNational competent authorities \n\nNotified bodies \n\nEuropean Commission \n\nTechnical documentation \n\nData EN 22 EN \n\nArticle 1(10)  Amending Article 28, inserting paragraph (1a) : Conformity assessment bodies that apply for a designation may be offered the possibility to submit a single application and undergo a single assessment procedure. \n\nConformity assessment bodies \n\nNotifying authorities \n\nApplication submission \n\nData \n\nArticle 1(11)  Amending Article 29(4 ): Notified bodies which apply for a single assessment shall submit the single application to the notifying authority. The notified body shall update the documentation if relevant changes occur. \n\nNotified bodies \n\nNotifying authority \n\nApplication submission \n\nData \n\nArticle 1(16)  Amending Article 56(6) : The Commission shall publish its assessments on the adequacy of the codes of practice. \n\nEuropean Commission  Assessment publication \n\nData \n\nArticle 1(26)  Amending Article 77 :\n\nâ€¢ Paragraph 1 : National public authorities/bodies which supervise/enforce EU law obligations protecting fundamental rights may make a reasoned request and access any information/documentation from the relevant market surveillance authority \n\nâ€¢ Paragraph 1a : market surveillance authority shall grant access and, where needed, request the information from the provider/deployer \n\nâ€¢ Paragraph 1b : where necessary, the aforementioned market surveillance \n\nNational public authorities/bodies which supervise/enforce EU law obligations protecting fundamental rights \n\nMarket surveillance authorities \n\nProviders/deployers of AI systems \n\nInformation exchange \n\nData EN 23 EN \n\nauthorities and public \n\nauthorities/bodies shall exchange information. \n\n4.2. Data \n\nHigh-level description of the data in scope \n\nType of data  Reference to the requirement(s)  Standard and/or specification (if applicable) \n\nSpecial categories of personal data (where the processing is needed for bias detection/correction) \n\nArticle 1(5)  // \n\nTechnical documentation for high-risk AI systems  Article 1(8)  Technical documentation shall contain, at a minimum, the elements set out in Annex IV of the AI Act. The Commission shall establish a simplified technical documentation form targeted at SMCs and SMEs. \n\nApplications of conformity assessment bodies for designation \n\nArticle 1(10)  // \n\nApplications of a conformity assessment bodies for notification \n\nArticle 1(11)  The notified body shall update the relevant documentation whenever relevant changes occur. \n\nCommission assessment of the adequacy of the codes of practice \n\nArticle 1(16)  // EN 24 EN \n\nRequest for access to information on AI systems  Article 1(26)  // \n\nInformation or documentation requested by national public authorities/bodies which supervise/enforce obligations relating to fundamental rights \n\nArticle 1(26)  To be provided in accessible language and format. \n\nAlignment with the European Data Strategy \n\nExplanation of how the requirement(s) are aligned with the European Data Strategy \n\nArticle 1(4) establishes that the processing of special categories of personal data shall be subject to appropriate safeguards for fundamental rights and freedoms of natural persons. This is in alignment with Regulations (EU) 2016/679 (GDPR) and (EU) 2018/1725 (EUDPR). \n\nAlignment with the once-only principle \n\nExplanation of how the once-only principle has been considered and how the possibility to reuse existing data has been explored \n\nArticle 1(10) states that conformity assessment bodies may be provided the possibility to submit a single application and undergo a single assessment procedure. \n\nExplanation of how newly created data is findable, accessible, interoperable and reusable, and meets high-quality standards \n\nData flows \n\nHigh-level description of the data flows \n\nType of data  Reference(s) to the \n\nActors who provide the data \n\nActors who receive the data \n\nTrigger for the data exchange \n\nFrequency (if applicable) EN 25 EN \n\nrequirement(s) \n\nApplications of a conformity assessment bodies for notification \n\nArticle 1(11)  Notified bodies which are designated under Union harmonisation legislation listed in Section A of Annex I \n\nNotifying authority designated in accordance with Union harmonisation legislation listed in Section A of Annex I\n\nApplication being made for single assessment \n\n// \n\nCommission assessment of the adequacy of the codes of practice \n\nArticle 1(16)  European Commission \n\nGeneral Public  Performance of an assessment as regards the codes of practice \n\nRegularly \n\nRequest for access to information on AI systems \n\nArticle 1(26)  National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights \n\nMarket surveillance authority \n\nNational public authorities/bodies require the information in order to fulfil their mandates \n\n// \n\nInformation or documentation requested by national public \n\nArticle 1(26)  Market surveillance \n\nNational public authorities or \n\nSubmission of a reasoned request to \n\n// EN 26 EN \n\nauthorities/bodies which \n\nsupervise/enforce obligations relating to fundamental rights \n\nauthority  bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights \n\naccess information \n\nInformation or documentation requested by market surveillance authorities \n\nArticle 1(26)  Market surveillance authorities \n\nProviders/ deployers of AI systems \n\nMarket surveillance authority is in need of the information so as to answer to a request from national public authorities/bodies which supervise/enforce obligations relating to fundamental rights) \n\n// \n\nInformation exchanges as part of the cooperation of market surveillance authorities and public authorities/bodies which supervise/enforce obligations relating to fundamental rights \n\nArticle 1(26)  Market surveillance authorities \n\n/ Public authorities/bodies \n\nMarket surveillance authorities \n\n/ Public authorities/bodies \n\nInformation exchange need identified in the course of cooperation and mutual assistance \n\n// \n\n4.3. Digital solutions \n\nHigh-level description of digital solutions EN 27 EN \n\nDigital solution \n\nReference(s) to the requirement(s) \n\nMain mandated functionalities  Responsible body \n\nHow is accessibility catered for? \n\nHow is reusability considered? \n\nUse of AI technologies (if applicable) \n\nN.A. (the proposed amendments to the AI Act do not foresee the adoption of new digital solutions) \n\nFor each digital solution, explanation of how the digital solution complies with applicable digital policies and legislative enactments \n\nDigital Solution #1 \n\nDigital and/or sectorial policy (when these are applicable) \n\nExplanation on how it aligns \n\nAI Act \n\nEU Cybersecurity framework \n\neIDAS \n\nSingle Digital Gateway and IMI \n\nOthers EN 28 EN \n\n4.4. Interoperability assessment \n\nHigh-level description of the digital public service(s) affected by the requirements \n\nDigital public \n\nservice or category of digital public services \n\nDescription  Reference(s) to the requirement(s) \n\nInteroperable Europe \n\nSolution(s) \n\n(NOT APPLICABLE) \n\nOther interoperability solution(s) \n\nN.A. (the proposed amendments to the AI Act do not affect digital public services) \n\nImpact of the requirement(s) as per digital public service on cross-border interoperability \n\nDigital Public Service #1 \n\nAssessment  Measure(s)  Potential remaining barriers (if applicable) \n\nAlignment with existing digital and sectorial policies \n\nPlease list the applicable digital and sectorial policies identified EN 29 EN \n\nOrganisational measures for a smooth cross-border digital public services delivery \n\nPlease list the governance measures foreseen \n\nMeasures taken to ensure a shared understanding of the data \n\nPlease list such measures \n\nUse of commonly agreed open technical specifications and standards \n\nPlease list such measures \n\n4.5. Measures to support digital implementation \n\nHigh-level description of measures supporting digital implementation \n\nDescription of the measure  Reference(s) to the requirement(s) \n\nCommission role \n\n(if applicable) \n\nActors to be involved \n\n(if applicable) \n\nExpected timeline \n\n(if applicable) \n\nN.A.", "fetched_at_utc": "2026-02-08T19:08:57Z", "sha256": "c10497b01d3883e105e26b1e9471322ed5d0af402a71a3f5e4c3948ffb6a0fc8", "meta": {"file_name": "Proposal Digital Omnibus.pdf", "file_size": 731943, "relative_path": "pdfs\\Proposal Digital Omnibus.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 30507}}}
{"doc_id": "pdf-pdfs-singapore-governance-for-agentic-ai-d729cf2c58b9", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\Singapore - Governance for Agentic AI.pdf", "title": "Singapore - Governance for Agentic AI", "text": "MODEL AI GOVERNANCE \n\n# FRAMEWORK FOR AGENTIC AI \n\nVersion 1.0 | Published 22 January 2026 \n\nPublished on 19 January 2026 by: Contents \n\nExecutive Summary ................................ ................................ ................................ ........... 1\n\n1 Introduction to Agentic AI ................................ ................................ ........................... 3\n\n1.1 What is Agentic AI? ................................ ................................ ............................. 3\n\n1.1.1 Core components of an agent ................................ ................................ ................... 3\n\n1.1.2 Multi -agent setups ................................ ................................ ................................ .... 4\n\n1.1.3 How agent design affects the limits and capabilities of each agent ............................. 4\n\n1.2 Risks of Agentic AI ................................ ................................ ............................... 6\n\n1.2.1 Sources of risk ................................ ................................ ................................ .......... 6\n\n1.2.2 Types of risk ................................ ................................ ................................ ............. 7\n\n2 Model AI Governance Framework for Agentic AI ................................ ........................... 8\n\n2.1 Assess and bound the risks upfront ................................ ................................ ...... 9\n\n2.1.1 Determine suitable use cases for agent deployment ................................ .................. 9\n\n2.1.2 Bound risks through design by defining agents limits and permissions ...................... 11 \n\n2.2 Make humans meaningfully accountable ................................ ............................ 13 \n\n2.2.1 Clear allocation of responsibilities within and outside the organisation .................... 13 \n\n2.2.2 Design for meaningful human oversight ................................ ................................ ... 16 \n\n2.3 Implement technical controls and processes ................................ ...................... 18 \n\n2.3.1 During design and development, use technical controls ................................ ........... 18 \n\n2.3.2 Before deploying, test agents ................................ ................................ .................. 19 \n\n2.3.3 When deploying, continuously monitor and test ................................ ....................... 20 \n\n2.4 Enable end -user responsibility ................................ ................................ ........... 22 \n\n2.4.1 Different users, different needs ................................ ................................ ............... 22 \n\n2.4.2 Users who interact with agents ................................ ................................ ................ 23 \n\n2.4.3 Users who integrate agents into their work processes ................................ .............. 23 \n\nAnnex A: Further resources ................................ ................................ ............................... 25 \n\nAnnex B: Call for feedback and case studies ................................ ................................ ...... 27 1\n\n# Executive Summary \n\nAgentic AI is the next evolution of AI , holding transformative potential for users and businesses. \n\nCompared to generative AI, AI agents can take actions, adapt to new information, and interact with \n\nother agents and systems to complete tasks on behalf of humans. While use cases are rapidly \n\nevolving, agents are already transforming the workplace through coding assistants, customer \n\nservice agents, and automating enterprise productivity workflows. \n\nThese greater capabilities also bring forth new risks . Agentsâ€™ access to sensitive data and ability \n\nto make changes to their environment, such as updating a customer database or making a payment, \n\nare double -edged swords. As we move towards deploying multiple agents with complex interactions, \n\noutcomes also become more unpredictable. \n\nHumans must remain accountable and properly manage these risks. While existing governance \n\nprinciples for trusted AI such as transparency, accountability and fairness continue to apply, they \n\nneed to be translated in practice for agents. Meaningful human control and oversight need to be \n\nintegrated into the agentic AI lifecycle. Nevertheless, a balance needs to be struck as continuous \n\nhuman oversight over all agent workflows becomes impractical at scale .\n\nThe Model AI Governance Framework (MGF) for Agentic AI gives organisations a structured \n\noverview of the risks of agentic AI and emerging best practices in managing these risks. If risks \n\nare properly managed, organisations can adopt agentic AI with greater confidence. The MGF is \n\ntargeted at organisations looking to deploy agentic AI, whether by developing AI agents in -house or \n\nusing third -party agentic solutions. Building on our previous model governance frameworks, we have \n\noutlined key considerations for or ganisations in four areas when it comes to agents: \n\n1.  Assess and bound the risks upfront \n\nOrganisations should adapt their internal structures and processes to account for new risks \n\nfrom agents. Key to this is first understanding the risks posed by the agentâ€™s actions, which \n\ndepend on factors such as the scope of actions the agent can take, the reversibility of those \n\nactions, and the agentâ€™s level of autonomy. \n\nTo manage these risks early, organisations could limit the scope of impact of their agents by \n\ndesigning appropriate boundaries at the planning stage, such as limiting the agentâ€™s access \n\nto tools and external systems. They could also ensure that the agentâ€™s actions are traceable \n\nand controllable through establishing robust identity management and access controls for \n\nagents. \n\n2.  Make humans meaningfully accountable \n\nOnce the â€œgreen lightâ€ is given for agentic AI deployment, an organisation should take steps \n\nto ensure human accountability. However, the autonomy of agents may complicate \n\ntraditional responsibility assignments which are tied to static workflows. Multiple actors may \n\nalso be involved in different parts of the agent lifecycle, diffusing accountability. It is \n\ntherefore important to clearly define the responsibilities of different stakeholders, both 2\n\nwithin the organisation and with external vendors, while emphasising adaptive governance, \n\nso that the organisation is set up to quickly understand new developments and update its \n\napproach as the technology evolves. \n\nSpecifically, â€œhuman -in -the -loopâ€ has to be adapted to address automation bias, which has \n\nbecome a bigger concern with increasingly capable agents. This includes defining significant \n\ncheckpoints in the agentic workflow that require human approval, such as high -stakes or \n\nirreversible actions, and regularly auditing human oversight to check that it remains effective \n\nover time. \n\n3.  Implement technical controls and processes \n\nOrganisations should ensure the safe and reliable operationalisation of AI agents by \n\nimplementing technical measures across the agent lifecycle. During development, \n\norganisations should incorporate technical controls for new agentic components such as \n\nplanning, tools and still -maturing protocols , to address increased risks from these new \n\nattack surfaces. \n\nBefore deployment, organisations should test agents for baseline safety and reliability, \n\nincluding new dimensions such as overall execution accuracy, policy adherence, and tool \n\nuse. New testing approaches will be needed to evaluate agents. \n\nDuring and after deployment , as agents interact dynamically with their environment and not \n\nall risks can be anticipated upfront, it is recommended to gradually roll out agents alongside \n\ncontinuous monitoring after deployment. \n\n4.  Enable end -user responsibility \n\nTrustworthy deployment of agents does not rely solely on developers, but also on end -users \n\nusing them responsibly. To enable responsible use, as a baseline, users should be informed \n\nof the agentâ€™s range of actions, access to data, and the userâ€™s own responsibilities. \n\nOrganisations should consider laye ring on training to equip employees with the knowledge \n\nrequired to manage human -agent interactions and exercise effective oversight, while \n\nmaintaining their tradecraft and foundational skills. \n\nThis is a living document. We have worked with government agencies and leading companies to \n\ncollate current best practices, but this is a fast -developing space, and best practices will evolve. This \n\nframework will need to be continuously updated to keep pace with new developments. W e invite \n\nfeedback to refine the framework, and case studies demonstrating how the framework can be \n\napplied for responsible agentic deployment. 3\n\n# 1 Introduction to Agentic AI \n\n# 1.1 What is Agentic AI ?\n\nAgentic AI systems are systems that can plan across multiple steps to achieve specified \n\nobjectives, using AI agents .1 There is no consensus on what defines an agent, but there are certain \n\ncommon features â€“ agents usually possess some degree of independent planning and action taking \n\n(e.g. searching the web or creating files) over multiple steps to achieve a user -defined goal. 2\n\nIn this framework, we focus on agents built on language models, which are increasingly being \n\nadopted. Such agents use a small, large, or multimodal large language model (SLM, LLM, or MLLM) \n\nas its brain to make decisions and complete tasks. However, it is w orth noting that software agents \n\nare not a new concept and other types of agents exist, such as those which use deterministic rules, \n\nor other neural networks, to make decisions. 3\n\n# 1.1.1 Core components of an agent \n\nCore components of a simple agent 4\n\nAs agents are built on top of language models, it is helpful to start with the core components of \n\na simple LLM -based app. \n\n1.  Model : an SLM, LLM or MLLM that serves as the central reasoning and planning engine, or \n\nthe â€œbrainâ€ of the agent. It processes instructions, interprets user inputs, and generates \n\ncontextually appropriate responses.  \n\n> 1\n\nAdapted from C yber Security Agency of Singapore (CSA) , Draft Addendum on Securing Agentic AI . \n\n> 2\n\nSee  International AI Safety Report . \n\n> 3\n\nSee World Economic Forum (WEF) , AI Agents in Action: Foundations for Evaluation and Governance . \n\n> 4\n\nAdapted from GovTech Singapor e,  Agentic Risk & Capability Framework , CSA Singapore , Draft \n\nAddendum on Securing Agentic AI  and Anthropic , Building Effective Agents ). \n\nGuide decisions \n\nand actions \n\nRetrieve and store \n\ninfo for long -term \n\nInteract with \n\nexternal systems \n\nModel \n\nPlanning & \n\nReasoning \n\nInstructions  Tools Memory \n\n1\n\nv\n\n2\n\nv\n\n3\n\nv\n\n5\n\nv\n\n4\n\nv4\n\n2.  Instructions : Natural language commands that define an agent's role, capabilities, and \n\nbehavioural constraints e.g. a system prompt for an LLM. \n\n3.  Memory : Information that is stored and accessible to the LLM, either in short or long -term \n\nstorage. Sometimes added to allow the model to obtain information from previous user \n\ninteractions or external knowledge sources. \n\nAn agent uses the model, instructions and memory in similar ways as an LLM -based app. In \n\naddition, it has other components that enable it to complete more complex tasks: \n\n4.  Planning and reasoning : The model is usually trained to reason and plan, meaning that it \n\ncan output a series of steps needed for a task. \n\n5.  Tools: Tools enable the agent to take actions and interact with other systems, such as writing \n\nto files and databases, controlling devices, or performing transactions. The model calls tools \n\nto complete a task. \n\n6.  Protocols: This is a standardised way for agents to communicate with tools and other agents. \n\nFor example, the Model Context Protocol (MCP) has been developed for agents to \n\ncommunicate with tools, 5 whereas the Agent2Agent Protocol (A2A) defines a standard for \n\nagents to communicate with each other. 6\n\n# 1.1.2 Multi -agent setups \n\nIn an agentic system, it is common for multiple agents to be set up to work together. This can \n\nsometimes improve performance, by allowing each agent to specialise in a certain function or task \n\nand work in parallel.  7\n\nThree common design patterns for multi -agent systems are :8\n\nâ€¢ Sequential : Agents work one after another in a linear workflow. Each agentâ€™s output \n\nbecomes the next agentâ€™s input. \n\nâ€¢ Supervisor : One supervising agent coordinates specialised agents under it. \n\nâ€¢ Swarm : Agents work at the same time, handing off to another agent when needed \n\n# 1.1.3 How agent design affects the limits and capabilities of each agent \n\nWhile each agent may have the same core components, the design of each component can \n\nsignificantly affect what the agent can do . It is generally helpful to distinguish between two \n\nconcepts when considering what an agent can do: 9\n\nâ€¢ Action -space (or authority, capabilities): Range of actions the agent is permitted to take, \n\ndetermined by the tools it is allowed to use, transactions it can execute, etc.              \n\n> 5See Anthropic, Model Context Protocol .\n> 6See Google, Agent2Agent Protocol .\n> 7See LangChain ,Benchmarking Multi -Agent Architectures .\n> 8Adapted from AWS ,Multi -Agent Collaboration Patterns with Strands Agents and Amazon Nova .\n> 9See WEF, AI Agents in Action: Foundations for Evaluation and Governance .\n\n5\n\nâ€¢ Autonomy (or decision -making): Degree to which an agent can decide when and how to act \n\ntowards a goal, such as by defining the steps to be taken in a workflow. This can be \n\ndetermined by its instructions and level of human involvement. \n\n## Action -space \n\nAn agentâ€™s action -space mainly depends on the tools it has access to, which can affect: \n\nâ€¢ Systems it can access :\n\no Sandboxes only: Sandboxed tools (e.g. for code execution, data analysis) that cannot \n\naffect any other system \n\no Internal systems: Tools internal to the organisation, such as being able to search and \n\nupdate the organisationâ€™s databases \n\no External systems: Tools that enable the agent to access external services, such as \n\nretrieving and updating data through third -party pre -defined APIs. \n\nâ€¢ Actions it can take in relation to the system it can access :\n\no Read vs write: An agent may only be able to read and retrieve information from a \n\nsystem, rather than write to and modify data within the system. \n\nAn emerging modality of agentic AI is a computer use agent, whose primary tool is access to a \n\ncomputer and browser. This means that it can take any action that a human can take with a computer \n\nand browser without having to rely on specifically defined tool s and APIs. This significantly increases \n\nwhat the agent can access and do. \n\n## Autonomy \n\nAn agentâ€™s autonomy mainly depends on its instructions component and the level of human \n\ninvolvement in the agentic system. \n\nIn terms of instructions, an agent can be given differing level of instructions: \n\nâ€¢ Detailed instructions and SOP: An agent instructed to follow a detailed SOP to complete a \n\ntask would be limited in the decisions it can make at each stage. \n\nâ€¢ Using its own judgment: An agent instructed to use its own judgment to complete a task \n\nwould have more freedom to define its plan and workflow. \n\nAnother relevant factor is the level of human involvement. When interacting with an agent, a human \n\ncan be involved to different levels: 10 \n\nâ€¢ Agent proposes, human operates: The human directs and approves every step taken by an \n\nagent. \n\nâ€¢ Agent and human collaborate: The human and agent work together. The agent requires \n\nhuman approval at significant steps, such as before writing to a database or making a \n\npayment. However, the human can intervene anytime by taking over the agentâ€™s work or \n\npausing the agent and requesti ng a change.   \n\n> 10 See Knight First Amendment Institute at Columbia University, Levels of Autonomy for AI Agents .\n\n6\n\nâ€¢ Agent operates, human approves: The agent requires human approval only at critical steps \n\nor failures, such as deleting a database or making a payment above a predefined amount. \n\nâ€¢ Agent operat es, human observes: The agent does not require human approval as it \n\ncompletes its task, though its actions may be audited after the fact. \n\n# 1.2 Risks of Agentic AI \n\n# 1.2.1 Sources of risk \n\nThe new components of an agent constitute new sources of risks .11  The risks themselves are \n\nfamiliar â€“ fundamentally, agents are software systems built on LLMs. They inherit traditional \n\nsoftware vulnerabilities (such as SQL injection) and LLM -specific risks (such as hallucination, bias, \n\ndata leakage and adversarial prompt injections). 12 \n\nHowever, the risks can manifest differently through the different components. For example: \n\nâ€¢ Planning and reasoning: An agent can hallucinate and make a wrong plan to complete a \n\ntask. \n\nâ€¢ Tools: An agent can hallucinate by calling non -existent tools or calling tools with the wrong \n\ninput, or call ing tools in a biased manner. As tools connect the agent to external systems, \n\nprompt or code injections can also manipulate the agent to exfiltrate or otherwise \n\nmanipulate the data it has access to. \n\nâ€¢ Protocols: Finally, as new protocols emerge to handle agent communication, they can also \n\nbe poorly deployed or compromised e.g. an untrusted MCP server deployed with code to \n\nexfiltrate the userâ€™s data. \n\nAs components within an agent or multiple agents interact, risks can also arise at the system \n\nlevel. 13  For example: \n\nâ€¢ Cascading effect: A mistake by one agent can quickly escalate as its outputs are passed \n\nonto other agents. For example, in supply chain management, a hallucinated inventory figure \n\nfrom one agent could potentially cause downstream agents to reorder excessive or \n\ninsufficient stock. \n\nâ€¢ Unpredictable outcomes: Agents working together can also compete or coordinate in \n\nunintended ways. For example, in manufacturing, different agents may be involved in \n\nmanaging machines and inventory. While coordinating to meet production goals, the agents \n\nmight interact unpredict ably due to complex optimi sation algorithms and over or under -\n\nprioritise one resource or machine, leading to unexpected bottlenecks.        \n\n> 11 BCG highlighted examples of new risks from agents e.g. agents that optimize their own goals locally\n> may create instability across the system, flawed behaviour by one agent may spread to other agents\n> (see What Happens When AI Stops Asking Permission? )\n> 12 Adapted from CSA ,Draft Addendum on Securing Agentic AI .\n> 13 See WEF, AI Agents in Action: Foundations for Evaluation and Governance , which highlighted a new\n> class of failure modes, linked to potentially misaligned interactions in multi -agent systems e.g.\n> orchestration drift, semantic misalignment, interconnectedness and cascading effects.\n\n7\n\n# 1.2.2 Types of risk \n\nBecause agents take actions in the real world, when they malfunction , it can lead to harmful \n\nreal -world impact . Organisations should be aware of these negative outcomes :\n\nâ€¢ Erroneous actions : Incorrect actions such as an agent fixing appointments on the wrong \n\ndate or producing flawed code. The exact harmful outcome depends on the action in \n\nquestion, e.g. flawed code can lead to exploited security vulnerabilities, and wrong medical \n\nappointment s may affect a patientâ€™s health outcomes. \n\nâ€¢ Unauthorised actions : Actions taken by the agent outside its permitted scope or authority, \n\nsuch as taking an action without escalating it for human approval based on a company policy \n\nor standard operating procedure. \n\nâ€¢ Biased or unfair actions : Actions that lead to unfair outcomes, especially when dealing with \n\ngroups of different profiles and demographics, such as biased vendor selection in \n\nprocurement, disbursements of grants, and/or hiring decisions. \n\nâ€¢ Data breaches : Actions that lead to the exposure or manipulation of sensitive data. Such \n\ndata may be personally identifiable information or confidential information e.g. customer \n\ndetails, trade secrets, and/or internal communications. This can be due to a security breach, \n\nwhere attackers exploit agents to reveal private information, or an agent disclosing sensitive \n\ndata due to a failure to recognise it as sensitive. \n\nâ€¢ Disruption to connected systems : As agents interact with other systems, they can cause \n\ndisruption to connected systems when they are compromised or malfunction e.g. deleting a\n\nproduction codebase, or overwhelming external system s with requests. \n\nErroneous actions  Unauthorised \n\nactions \n\nBiased or unfair \n\nactions \n\nData breaches \n\nDisruption to \n\nconnected \n\nsystems 8\n\n# 2 Model AI Governance Framework for Agentic AI \n\nFour dimensions of the MGF for Agentic AI \n\nThe MGF for Agentic AI builds on the responsible AI practices for organisations set out in MGF \n\n(2020) 14  by highlighting emerging best practices to address new concerns from agentic AI. This is so \n\nthat organisations can develop and use agentic AI with the requisite knowledge and judgment .\n\nThe framework begins with helping organisations to assess and bound the risks upfront . It \n\nhighlights new risks that should be considered during risk assessment, and design considerations at \n\nthe planning stage to limit the potential scope of impact of the agents, as well as ensure that agents \n\nare traceable and controllable. \n\nWhile agents may act autonomously, human responsibility continues to apply. Once the â€œgreen lightâ€ \n\nis given to deploy agentic AI, an organisation should take immediate steps to mak e humans \n\nmeaningfully accountabl e. This includes clearly defining responsibility across multiple actors \n\nwithin and outside the organisation involved in the agent lifecycle; and taking measures to ensure \n\nthat human -in -the -loop remains effective over time notwithstanding automation bias. \n\nTo ensure safe and reliable operationalisation of agents, an organisation should adopt technical \n\ncontrols and processes across the AI lifecycle. During development, guardrails for new \n\ncomponents in AI agents such as planning and tools should be implemented. Before deployment, \n\nagents should be tested for baseline safety and reliability. After deployment, agents should be \n\ncon tinuously monitored as they interact dynamically with their environment. \n\nFinally, trustworthy deployment of agents does not rest solely on developers, but also on end -users. \n\nOrganisations are responsible for enabling end -user responsibility by equipping them with \n\nessential information to use agents appropriately and exercise effective oversight, while maintaining \n\ntheir tradecraft and foundational skills.    \n\n> 14 See Model AI Governance Framework (2 nd Ed) .\n\n1. Assess and bound \n\nthe risks upfront \n\n2. Make humans \n\nmeaningfully accountable \n\n3. Implement technical \n\ncontrols and processes \n\n4. Enable end -user \n\nresponsibility 9\n\n# 2.1 Assess and bound the risks upfront \n\nAgents bring new risks, especially in their access to sensitive data and ability to change their \n\nenvironment through action -taking. Their adaptive, autonomous and multi -step nature also \n\nincreases the potential for unexpected actions, emergent risks and cas cading impacts. \n\nOrganisations should consider these new dimensions as part of risk assessment, and limit the scope \n\nof impact of their agents by designing appropriate boundaries at an early stage. \n\nWhen planning for the use of agentic AI, organisations should consider: \n\nâ€¢ Determining suitable use cases for agent deployment by considering agent -specific \n\nfactors that can affect the likelihood and impact of the risk. \n\nâ€¢ Design choices to bound the risks upfront by applying limits on agentâ€™s access to tools and \n\nsystems and defining a robust identity and permissions framework. \n\n# 2.1.1 Determine suitable use cases for agent deployment \n\nRisk identification and assessment is the first step when considering if an agentic use case is \n\nsuitable for development or deployment . Risk is a function of likelihood (probability of the risk \n\nmanifesting) and impact (severity of impact if the risk manifests). \n\nThe following non -exhaustive factors affect the level of risk of an agentic use case: \n\nFactors affecting impact \n\nFactor  Description  Illustration \n\nDomain and use \n\ncase in which \n\nagent is being \n\ndeployed \n\nLevel of tolerance of error in the \n\ndomain and use case in which the \n\nagent is being deployed to \n\nAgent executing financial \n\ntransactions which require a high \n\ndegree of accuracy, vs agent that \n\nsummarises internal meetings \n\nAgentâ€™s access to \n\nsensitive data \n\nWhether the agent can access \n\nsensitive data, such as personal \n\ninformation or confidential data \n\nAgent that requires access to \n\npersonal customer data gives rise to \n\nthe risk of leaking such data, vs \n\nagent who only has access to \n\npublicly available information \n\nAgentâ€™s access to \n\nexternal systems \n\nWhether the agent can access \n\nexternal systems \n\nAgent that sends data to third -party \n\nAPIs can leak data to these third \n\nparties, or disrupt these systems by \n\nmaking too many requests, vs agent \n\nthat only has access to sandboxed \n\nor internal tools \n\nScope of agentâ€™s \n\nactions \n\nWhether an agent can only read \n\nfrom or modify the data and systems \n\nit has access to \n\nRead vs write: Agent that can only \n\nread from a database vs being able \n\nto write to it \n\nMany tools vs a few: Agent that can \n\nonly choose from a few pre -defined \n\ntools, vs an agent who has unlimited \n\naccess to a browser tool 10 \n\nReversibility of \n\nagentâ€™s actions \n\nIf the agent can modify data and \n\nsystems, whether such \n\nmodifications are easily reversed \n\nAgent that schedules meetings vs \n\nagent that sends email \n\ncommunications to external parties \n\nFactors affecting likelihood \n\nFactor  Description  Illustration \n\nAgentâ€™s level of \n\nautonomy \n\nWhether the agent can define the \n\nentire workflow or must follow a \n\nwell -defined procedure. \n\nA higher level of autonomy can \n\nresult in higher unpredictability, \n\nincreasing likelihood of error. \n\nAgent is provided with a SOP and \n\ninstructed to follow it when carrying \n\nout a task, vs agent is instructed to \n\nuse its best judgment to select and \n\nexecute every step \n\nTask complexity  How complex the task is, in relation \n\nto the number of steps required to \n\ncomplete it and the level of analysis \n\nrequired at each step. \n\nA higher level of complexity similarly \n\nincreases unpredictability and the \n\nlikelihood of error. \n\nAgent is required to extract key \n\naction points from a meeting \n\ntranscript, vs agent is tasked to \n\nfollow a nuanced data sharing policy \n\nwhen handling external requests for \n\ninformation \n\nAgentâ€™s access to \n\nexternal systems \n\nWhether the agent is exposed to \n\nexternal systems, and who \n\nmaintains these systems. \n\nA higher level of exposure makes the \n\nagent more vulnerable to prompt \n\ninjections and cyberattacks. \n\nAgent can only access an internal \n\nknowledge base which is \n\nmaintained by trusted internal \n\nteams, vs an agent who can access \n\nthe web containing untrusted data \n\nThreat modelling also makes risk assessment more \n\nrigorous by systematically identifying specific ways \n\nin which an attacker may take to compromise the \n\nsystem. Common security threats to agentic systems \n\ninclude memory poisoning, tool misuse, and privilege \n\ncompromise. 15  As agentic systems (especially multi -\n\nagent systems) can become very complex, it is often \n\nuseful to use a method called taint tracing to map out \n\nall the workflows and interactions to track how \n\nuntrusted data can move through the system. For more \n\ninformatio n on how to perform threat modelling and \n\ntaint tracing for agentic systems, organisations may \n\nrefer to  CSAâ€™s Draft Addendum on Securing Agentic AI .                \n\n> 15 For a more comprehensive coverage of potential security threats to agentic AI systems , see OWASP,\n> Agentic AI â€“Threats and Mitigations .\n> The relationship between threat modelling\n> and risk assessment\n> Threat modelling augments the risk assessment\n> process by generating contextualised threat\n> events with well -described sequence of actions,\n> activities and scenarios that the attacker may\n> take to compromise the system. With more\n> relevant threat events, risk a ssessments will be\n> more rigorous and robust, resulting in more\n> targeted controls and effective layered defence.\n> Since risk assessment is continuous, the threat\n> model should be regularly updated.\n> Adapted from CSA, Guide to Cyber Threat\n> Modelling\n\n11 \n\n# 2.1.2 Bound risks through design by defining agents limits and permissions \n\nHaving selected an appropriate agent use case, organisations can further bound the risks by \n\ndefining appropriate limits and permission policies for each agent. \n\n## Agent limits \n\nOrganisations should consider defining limits on: \n\nâ€¢ Agentâ€™s access to tools and systems: Define policies that give agents only the minimum \n\ntools and data access needed for it to complete its task. 16  For example, a coding assistant \n\nmay not require access to a web search tool, especially if it already has curated access to \n\nthe latest software documentation. \n\nâ€¢ Agentâ€™s autonomy: For process -driven tasks, SOPs and protocols are frequently used to \n\nimprove consistency and reduce unpredictability.  17  Define similar SOPs for agentic \n\nworkflows that an agent is constrained to follow, rather than giving the agent the freedom to \n\ndefine every step of the workflow. \n\nâ€¢ Agentâ€™s area of impact: Design mechanisms and procedures to take agents offline and limit \n\ntheir potential scope of impact when they malfunction. This can include running agents in \n\nself -contained environments with limited network and data access, particularly when they \n\nare carrying out high -risk tasks such as code execution. 18 \n\n## Agent identity \n\nIdentity management and access control is one of the key means in which organisations enable \n\ntraceability and accountability today for humans. As agents become more autonomous, identity \n\nmanagement has to be extended to agents as well to track individual agent behaviour and establish \n\nwho holds accountability for each agent. \n\nThis is an evolving space, and gaps exist today in terms of handling agent identity robustly . For \n\nexample, current authorisation systems typically have pre -defined, static scopes. However, to \n\noperate safely in more complex scenarios, agents require fine -grained permissions that may change \n\ndynamically depending on the context, risk levels, and tas k objectives. Current authentication \n\nsystems are also typically based on a single, unique individual. Such systems face difficulty in \n\nhandling complex agent setups, such as when agents act for multiple human users with different \n\npermissions, or recursive delegation scenarios where agents spin up multiple sub -agents. 19                    \n\n> 16 See PwC ,The rise â€“and risks â€“of agentic AI .\n> 17 Grab introduced anLLM agent framework leverag ing on Standard Operating Procedures (SOPs) to\n> guide AI -driven execution (see Introducing the SOP -driven LLM agent frameworks ).\n> 18 See McKinsey, Deploying agentic AI with safety and security: A playbook for technology leaders .\n> 19 For a more comprehensive treatment of how current identity systems may face challenges when\n> catering to agentic AI, see OpenID, Identity Management for Agentic AI .\n\n12 \n\nSolutions are being developed to address these issues, such as integrating well -established \n\nstandards like OAuth 2.0 into MCP. 20  The industry is also developing new standards and solutions for \n\nagents, such as decentralised identity management and dynamic access control. 21 \n\nIn the interim, organisations should consider these best practices to enable agent control and \n\ntraceability: \n\nâ€¢ Identification: An agent should have its own unique identity, such that it can identify itself to \n\nthe organisation, its human user, or other agents. However, an agentâ€™s identity may need to \n\nbe tied to a supervising agent, a human user, or an organisational department for \n\naccountability and tracking. Additionally, the different capacities in which an agent acts (e.g. \n\nindependently or on behalf of a specified human user) should also be recorded. \n\nâ€¢ Authorisation: An agent can have pre -defined permissions based on its role or the task at \n\nhand, or its permissions may be dynamically set by its authorising human user, or a \n\ncombination of both. As a rule of thumb, the human user should not be able to set \n\npermissions fo r the agent greater than what the human user is himself authorised to do. Such \n\ndelegations of authority should be clearly recorded.         \n\n> 20 See MCP specifications for Authentication support ,Authorisation support .\n> 21 See proposed framework for agentic identity by Cloud Security Alliance, Agentic AI Identity & Access\n> Management: A New Approach .\n> Evaluating the residual risks\n> Residual risk is the risk that remains after mitigation measures have been applied. It is important to note\n> that there will always be a level of risk remaining, even after efforts are taken to identify appropriate agentic\n> use cases and define limits on any agents, especially given how quickly agentic AI is evolving. Ultimately,\n> organisations should evaluate and determine if the residual risk for their agentic deployment is of a\n> tolerable level and can be accepted.\n\n13 \n\n# 2.2 Make humans meaningfully accountable \n\nThe organisations that deploy agents and the humans who oversee them remain accountable for the \n\nagentsâ€™ behaviours and actions. But it can be challenging to fulfil this accountability when agent \n\nactions emerge dynamically and adaptively from interactions instead of fixed logic. Multiple \n\nstakehol ders may also be involved in different parts of the agent lifecycle, diffusing accountability. \n\nFinally, automation bias, or the tendency to over -trust a n automated system , especially when it has \n\nperformed reliably in the past, becomes a bigger concern as humans supervise increasingly capable \n\nagents. \n\nTo address these challenges to human accountability, organisations should consider: \n\nâ€¢ Clear allocation of responsibilities within and outside the organisation , by establishing \n\nchains of accountability across the agent value chain and lifecycle, while emphasising \n\nadaptive governance, so that the organisation is set up to quickly understand new \n\ndevelopments and update their approach as the technology evolves. \n\nâ€¢ Measures to enable meaningful human oversight of agents , such as requiring human \n\napproval at significant checkpoints, auditing the effectiveness of human approvals, and \n\ncomplementing these measures with automated monitoring .\n\n# 2.2.1 Clear allocation of responsibilities within and outside the organisation \n\nAs deployers, organisations and humans remain accountable for the decisions and actions of \n\nagents. However, as with AI, the value chain for agentic AI involves multiple actors. Organisations \n\nshould consider the allocation of responsibility both within their organisation, and vis -Ã -vis other \n\norganisations along the value chain. \n\nSimplified agentic AI value chain 22    \n\n> 22 For a more comprehensive list of potential stakeholders involved in the agentic AI ecosystem, see CSA\n> and FAR.AI, Securing Agentic AI: A Discussion Paper .\n\nModel developers \n\nModels that agents \n\ncan be built on  Agentic AI system \n\nproviders \n\nProviding platforms to \n\nbuild agents on or full \n\nSaaS solutions \n\nDeploying \n\norganisation \n\nMay also develop \n\nagents in -house \n\nEnd users \n\nInteracts with \n\nand uses \n\nagents \n\nTooling providers \n\ne.g. MCP, APIs \n\nAllow agents to \n\nconnect to external \n\nsystems 14 \n\n## Within the organisation \n\nWithin the organisation, organisations should allocate responsibilities for different teams \n\nacross the agent lifecycle. While each organisation is structured differently, this is an illustration \n\nof how such responsibilities may be allocated across different teams: \n\nKey decision \n\nmakers \n\nProduct teams \n\nCybersecurity \n\nteams \n\nWho: Leaders who define strategic decisions and high -level policies \n\nfor the organisation e.g. board members, C -suite executives, \n\nmanaging directors, or department leaders. \n\nKey responsibilities can include: \n\nâ€¢ Setting high -level goals for use of agents \n\nâ€¢ Defining permitted operational use cases for agents, \n\nincluding limits on agentâ€™s data access \n\nâ€¢ Setting the overall governance approach, including risk \n\nmanagement frameworks and escalation processes \n\nWho: These roles oversee the translation of stakeholder needs or \n\nbusiness goals into a technical agentic solution e.g. Product \n\nManagers, UI / UX Designers, AI Engineers, Software Engineers \n\nKey responsibilities can include: \n\nâ€¢ Defining the design and requirements for agents, as well as \n\nany feature controls or phased rollouts \n\nâ€¢ Reliable implementation of agents i.e. development, pre -\n\ndeployment testing and post -deployment monitoring \n\nacross the agent lifecycle \n\nâ€¢ Educating users on responsible use of agentic product \n\nWho: These roles oversee the protection of agentic systems from \n\ncyber threats, by implementing and managing security measures, \n\nidentifying vulnerabilities, and responding to incidents e.g. Chief \n\nSecurity Officer, Cyber Security Specialist, Penetration Tester \n\nKey responsibilities can include: \n\nâ€¢ Defining baseline security guardrails and secure -by -design \n\ntemplates that technical teams should implement or adapt \n\nto the agentic system being deployed \n\nâ€¢ Conducting regular red teaming and threat modelling 15 \n\nUsers \n\n## Outside the organisation \n\nOrganisations may also need to work with external parties when deploying agents e.g. model \n\ndevelopers, agentic AI providers, or hosts of external MCP servers or tools. \n\nIn these cases, organisations should similarly ensure that there are measures in place to fulfil its \n\nown accountability. Some agent -specific considerations are: \n\nâ€¢ Clarify distribution of obligations in any terms and conditions or contracts between the \n\norganisation and the external party. In particular, organisations should consider provisions \n\nto address any security arrangements, performance guarantees, or data protection and \n\nconfidentiality. Where th ere are gaps, the organisation should reassess if the agentic \n\ndeployment meets its risk tolerance. \n\nâ€¢ Features to maintain security and control. Organisations should consider if the external \n\npartyâ€™s product offers features for the organisation to maintain a sufficient level of security \n\nor control. This includes strong authentication measures such as scoped API keys, per -agent \n\nidentity tokens, and robust observability such as the logging of tool calls and access history. \n\nWhere such features are lacking, organisations should consider alternative or in -house \n\nsolutions, or scoping down the agentic use case, suc h as restricting access to sensitive data. \n\n## End users \n\nOrganisations may deploy agents to users within or outside their organisation. In doing so, \n\norganisations should ensure that users are provided sufficient information to hold the organisation \n\naccountable, as well as any information relating to the userâ€™s own responsibilities. More information \n\ncan be found in  Enabling end -user responsibility  below. \n\nWho: Any individual who utilises the output of the agents to contribute to \n\nan organisational goal e.g. company employees making decisions or \n\nautomating workflows and practices. \n\nKey responsibilities can include: \n\nâ€¢ Ethical and responsible usage of agents \n\nâ€¢ Attending required training, complying with usage policies, \n\ntimely reporting of bugs or issues with agents \n\n> Developing internal capabilities for adaptive governance\n> All teams involved in the agentic AI lifecycle should also develop internal capabilities to\n> understand agentic AI. As the technology is quickly evolving, being aware of the improvements\n> and limitations of new agentic developments, such as new modalities li ke computer use agents,\n> or new evaluation frameworks for agents, allow organisations to quickly adapt their governance\n> approach to new developments.\n\n16 \n\n# 2.2.2 Design for meaningful human oversight \n\nSetting up a system for effective human supervision \n\nOrganisations should define significant checkpoints or action boundaries that require human \n\napproval , especially before sensitive actions are executed. This can include: 23 \n\nâ€¢ High -stakes actions and decisions e.g. editing of sensitive data, final decisions in high -risk \n\ndomains (such as healthcare or legal), actions that may trigger liability \n\nâ€¢ Irreversible actions e.g. permanently deleting data, sending communications, making \n\npayments \n\nâ€¢ Outlier or atypical behaviour e.g. when agent accesses a system or database outside of its \n\nwork scope, when agent selects a delivery route that is twice as long as the median distance \n\nâ€¢ User -defined . Agents may act on behalf of users who have different risk appetites. Beyond \n\norganisation -defined boundaries, users may be given the option to define their own \n\nboundaries e.g. requiring approval for purchases above a certain amount \n\nApart from considering when approvals are required, organisations should also consider what \n\nform approvals should take. These considerations include: \n\nâ€¢ Keep approval requests contextual and digestible. When asking humans for approval, \n\nkeep the request short and clear, instead of providing long logs or raw data that may be \n\nchallenging to decipher and understand. \n\nâ€¢ Consider the form of human input required. For straightforward actions such as accessing \n\na database, the human user can simply approve or reject. For more complex cases, such as \n\nreviewing an agentâ€™s plan before execution, it may be more productive for the human to edit \n\nthe plan before giving the ag ent the go -ahead. \n\nOrganisations should implement measures to ensure continued effectiveness of human \n\noversight , particularly as humans remain susceptible to alert fatigue and automation bias. These \n\nmeasures can include:  \n\n> 23 For further examples of where human involvement may be considered, see Partnership on AI,\n> Prioritising real -time failure detection in AI agents ).\n\nDefine significant \n\ncheckpoints or action \n\nboundaries that require \n\nhuman approval \n\nTrain humans to evaluate \n\nthese requests for approval \n\neffectively, and audit these \n\napprovals \n\nComplement this with \n\nautomated monitoring \n\nmechanisms and \n\npredefined alert thresholds 17 \n\nâ€¢ Training humans to identify common failure modes e.g. inconsistent agent reasoning, \n\nagents referring to outdated policies \n\nâ€¢ Regularly auditing the effectiveness of human oversight \n\nFinally, human oversight should be complemented with automated real -time monitoring to \n\nescalate any unexpected or anomalous behaviour . This can be done by implementing alerts for \n\ncertain logged events (e.g. attempted unauthorised access or multiple failed attempts to call a tool), \n\nusing data science techniques to identify anomalous agent trajectories, or using agents to monitor \n\nother ag ents. For more information, see  Continuous testing and monitoring  below. 18 \n\n# 2.3 Implement technical controls and processes \n\nThe agentic components that differentiate agents from simple LLM -based applications necessitate \n\nadditional controls during the key stages of the implementation lifecycle. \n\nOrganisations should consider: \n\nâ€¢ During design and development, design and implement technical controls . The new \n\ncomponents and capabilities of agents also necessitate new and tailored controls. \n\nDepending on the agent design, implement controls such as tool guardrails and plan \n\nreflections. Further, limit the agentâ€™s impact on the external environment by enf orcing least -\n\nprivilege access to tools and data. \n\nâ€¢ Pre -deployment, test agents for safety and security. As with all software, testing before \n\ndeployment ensures that the system behaves as expected. Specifically for agents, test for \n\nnew dimensions such as overall task execution, policy adherence and tool use accuracy, \n\nand test at different levels and across v aried datasets to capture the full spectrum of agent \n\nbehaviour. \n\nâ€¢ When deploying, gradually roll out agents and continuously monitor them in production. \n\nThe autonomous nature of agents and the changing environment makes it challenging to \n\naccount for and test all possible outcomes before deployment. Hence it is recommended to \n\nroll out agents gradually, supported with real -time monitoring post -deployment to ensure \n\nthat agents function safely. \n\n# 2.3.1 During design and development, use technical controls \n\nOrganisations should design and implement technical controls in the agentic AI system to \n\nmitigate identified risks. For agents specifically, in addition to baseline software and LLM controls, \n\nconsider adding controls for: \n\nâ€¢ New agentic components, such as planning and reasoning and tools \n\nâ€¢ Increased security concerns from the larger attack surface and new protocols \n\nFor illustration, these are some sample controls for agents. For a more comprehensive list, \n\norganisations can refer to CSAâ€™s  Draft Addendum on Securing Agentic AI  and GovTechâ€™s  Agentic Risk \n\nand Capability Framework .\n\nPlanning  â€¢ Prompt agent to reflect on whether its plan adheres to user instructions \n\nâ€¢ Prompt the agent to summarise its understanding and request clarification \n\nfrom the user before proceeding \n\nâ€¢ Log the agentâ€™s plan and reasoning for the user to evaluate and verify \n\nTools  â€¢ Configure tools to require strict input formats \n\nâ€¢ Apply the principle of least privilege to limit tools available to each agent, \n\nenforced through robust authentication and authorisation \n\nâ€¢ For data -related tools: \n\no Do not grant agent write access to tables in sensitive databases unless \n\nstrictly required 19 \n\no Configure agent to let user take over control when keying in sensitive \n\ndata (e.g. passwords, API keys) \n\nProtocols  â€¢ Use standardised protocols where applicable (e.g. agentic commerce \n\nprotocols when agent is handling a financial transaction) \n\nâ€¢ For MCP servers: \n\no Whitelist trusted servers and only allow agent to interact with servers \n\non that whitelist \n\no Sandbox any code execution \n\n# 2.3.2 Before deploying, test agents \n\nOrganisations should test agents for safety and security before deployment. This provides \n\nconfidence that the agents work as expected and controls are effective. Best practices on software \n\nand LLM testing are still relevant, such as unit and integration testing for software systems, as well \n\nas selecting representative datasets, an d useful metrics and evaluators for LLM testing. \n\nOrganisations can refer to previous guidance, such as the Starter Kit for testing of LLM -based apps \n\nfor safety and reliability. \n\nHowever, organisations should adapt their testing approaches for agents. Some considerations \n\ninclude: \n\nâ€¢ Testing for new risks: Beyond producing incorrect outputs, agents can take unsafe or \n\nunintended actions through tools. Organisations can consider testing for: 24 \n\no Overall task execution : Whether agent can complete task accurately \n\no Policy compliance : Whether an agent follows defined SOPs and routes for human \n\napproval when required \n\no Tool calling : Whether an agent calls the right tools, with the right permissions, with \n\nthe right inputs and in the right order \n\no Robustness : As agents are expected to react and adapt to real -world situations, test \n\nfor their response to errors and edge cases \n\nâ€¢ Testing entire agent workflows: Agents can take multiple steps in sequence without human \n\ninvolvement. Thus, beyond testing an agentâ€™s final output, agents should be tested across \n\ntheir entire workflow, including reasoning and tool calling. \n\nâ€¢ Testing agents individually and together: Beyond individual agents, testing should be \n\ncarried out at the multi -agent system level, to understand any emergent risks and behaviours \n\nwhen agents collaborate, such as competitive behaviours or the impact on other agents \n\nwhen one agent has been compromis ed. \n\nâ€¢ Testing in real or realistic environments: As agents may be expected to navigate real -world \n\nsituations, testing should occur in a properly configured execution environment that mirrors \n\nproduction as closely as possible, such as using tool integrations, external APIs, and \n\nsandboxes that behave as th ey would in deployment. However, organisations should    \n\n> 24 For an example of new agentic aspects to test for, see Microsoft Foundry, Agent evaluators .\n\n20 \n\ncalibrate the need for realism against the risk of prematurely allowing agents to access tools \n\nthat affect the real world. \n\nâ€¢ Testing repeatedly and across varied datasets: Agent behaviour is inherently stochastic \n\nand context -dependent. Testing should thus be done at scale and across varied datasets to \n\nobserve any unexpected low -probability behaviours, especially if they are high -impact. This \n\nrequires generating test dataset s that cover different conditions that agents may encounter \n\nand running these tests multiple times, including minor perturbations where needed. \n\nâ€¢ Evaluating test results at scale: Reliably evaluating test results at scale is a known \n\nchallenge for LLM testing. Agents add a further layer of complexity as their workflows can be \n\nlong and contain unstructured information that cannot be easily processed by humans or \n\nautomated scripts. Org anisations may consider using different evaluation methods for \n\ndifferent parts of the agentic workflow (e.g. deterministic tests for structured tool calls vs \n\nLLM or human evaluation for unstructured agent reasoning). Howev er, there is still a need to \n\nevaluate agents holistically, so that agent patterns across steps can be evaluated. Current \n\nindustry solutions thus include defining LLMs or agents to evaluate other agents. 25 \n\n# 2.3.3 When deploy ing , continuous ly monitor and test \n\nAs agents are adaptive and autonomous , organisations should consider mechanisms to respond to \n\nunexpected or emergent risks when deploying agents .\n\n## Gradual deployment of agents \n\nOrganisations should consider gradually rolling out agents into production to control the \n\namount of risk exposure . Such rollouts can be controlled based on :\n\nâ€¢ User s of agents e.g. rolling out to trained or experienced users first \n\nâ€¢ Tools and protocols available to agent e.g. restricting agents to more secure, whitelisted \n\nMCP servers first \n\nâ€¢ Systems exposed to agent e.g. using agents in lower -risk internal systems first \n\n## Continuous test ing and monitor ing \n\nOrganisations should continuously m onitor and log agent behaviour post -deployment , and \n\nes tablish reporting and failsafe mechanisms for agent failures or unexpected behaviours . This \n\nallows the organisation to :\n\nâ€¢ Intervene in real -time : When potential failures are detected, s top agent workflow and \n\nescalate to a human supervisor e.g. if agent attempts unauthorised access \n\nâ€¢ Debug when incidents happen : Logging and tracing each step of an agent workflow and \n\nagent -to -agent interactions help to identify points of failure \n\nâ€¢ Audit at regular intervals: This ensure s that the system is performing as expected .    \n\n> 25 For an example of agent evaluation solutions, see AWS Labs ,Agent Evaluation .\n\n21 \n\nMonitoring and observability are not new concept s, but agents introduce some challenges . As \n\nagents execute multiple actions at machine speed, organisations face the issue of extracting \n\nmeaningful insights from the voluminous logs generated by monitoring systems. This becomes more \n\ndifficult when high -risk anomalies are expected to be detected in real -time and surfaced as early as \n\npossible .\n\nKey considerations when setting up a monitoring system include: \n\nâ€¢ What to log : Organisations should determine their objectives for monitoring (e.g. real -time \n\nintervention, debugging , integration between components ) to identify what to log. In doing \n\nso, prioritise monitoring for high -risk activities such as updating database records or \n\nfinancial transactions. \n\nâ€¢ How to effectively monitor logs : Organisations can consider approaches such as: \n\no Defining alert thresholds: \n\nâ–ª Programmatic, threshold -based : Define alerts when agents trigger \n\nthresholds e.g. agent attempts unauthorised access or makes too many \n\nrepeated tool calls within a specified timeframe .\n\nâ–ª Outlier / anomaly detection : Use data science or deep learning technique s\n\nto process agent signals and identify anomalous behaviour that may indicate \n\nmalfunctions. \n\nâ–ª Agents monitoring other agents : Design agents to monitor other agents in \n\nreal -time, flagging any anomalies or inconsistencies. \n\no Defining specific interventions : For each alert type, consider what the level of \n\nintervention should be. Some degree of human review should be incorporated, \n\nproportionate to the risk level. For example, lower -priority alerts can be flagged for \n\nreview at a scheduled time, whereas higher -priority ones might require temporarily \n\nhalting agent execution until a human reviewer can assess. In the event of \n\ncatastrophic agent ic malfunction or compromis e, comm en surate measures such as \n\ntermination and fallback solutions should be considered .\n\nFinally, continuously test the agentic system even post -deployment to ensure that it works as \n\nexpected and is not affected by model drift or other changes in the environment. 22 \n\n# 2.4 Enabl e end -user responsibility \n\nUltimately, end users are the ones who use and rely on agents , and human accountability also \n\nextends to these users. Organisations should provide sufficient information to end users to \n\npromote trust and enable responsible use. \n\nOrganisations should consider: \n\nâ€¢ Transparency : Users should be informed of the agentsâ€™ capabilities (e.g. scope of agentâ€™s \n\naccess to userâ€™s data, actions the agent can take) and the contact points whom users can \n\nescalate to if the agent malfunctions .\n\nâ€¢ Education : Users should be educated on proper use and oversight of agents (e.g. training \n\nshould be provided on an agentâ€™s range of actions, common failure modes like hallucinations, \n\nusage policies for data), as well as the potential loss of trade craft i.e. as agents take over \n\nmore functions, basic operational knowledge could be eroded. Hence sufficient training \n\n(espe cially in areas where agents are prevalent) should be provided to ensure that humans \n\nretain core skills. \n\n# 2.4.1 Different users, different needs \n\nOrganisations should cater to different users with different information needs, to enable such \n\nusers to use AI responsibly. Broadly, there are two main archetypes of end -users â€“ those who \n\ninteract with agents, and those who integrate agents into their work processes or oversee them .\n\nUsers who interact with agents \n\ne.g. customer service, HR agents â€“\n\nmostly external -facing \n\nUsers who integrate agents into \n\ntheir work processes \n\ne.g. coding assistants, enterprise \n\nworkflows â€“ mostly internal -facing \n\nFocus on transparency  Layer on education and training 23 \n\n# 2.4.2 User s who interact with agents \n\nSuch users usually interact with agents that act on behalf of the organisation, e.g. customer \n\nservice or sales agents. These agents tend to be external facing , although they can also be deployed \n\nwithin the organisation e.g. a human resource agent that interacts with other users in the \n\norganisation. \n\nFor these users , focus on transparency . Organisations should share pertinent information to \n\nfoster trust and facilitate proper usage of agents. Such information can include: \n\nâ€¢ Userâ€™s responsibilities : Clearly define the userâ€™s responsibilities, such as asking the user to \n\ndouble -check all information provided by the agent. \n\nâ€¢ Interaction : Declare upfront that the user s are interacting with agent s.\n\nâ€¢ Agent sâ€™ range of actions an d decisions : Inform the user s on the range of actions and \n\ndecisions that the agent is authorised to perform and make .\n\nâ€¢ Data : Be clear on how user data is collected, stored, and used by the agents, in accordance \n\nwith the organi sation's data privacy policies. Where necessary, obtain explicit consent from \n\nusers before collecting or using their data for the agents. \n\nâ€¢ Human accountability and escalation : Provide user s with the respective human contact \n\npoint s who are responsible for the agent s, whom the user s can alert if the agent s malfunction \n\nor if they are dissatisfied with a decision. \n\n# 2.4.3 Users who integrate agents into their work processes \n\nSuch users typically utili se agents as part of their internal work flows e.g. coding assistants, \n\nautomation of enterprise processes . The agent acts for and on behalf of the user. \n\nFor these users , in addition to the information in the previous section , layer on education and \n\ntraining so that users can use the agents responsibly . Key aspects include education and training \n\non: \n\nâ€¢ Foundational knowledge on agents \n\no Relevant use cases , so that the user s understand how to best integrate the agents into \n\ntheir day -to -day work, and the scenarios under which the use of agent s should be \n\nrestricted (e.g. do not use an agent for confidential data) \n\no Instructing the agents e.g. general best practices in prompting, glossary of keywords to \n\nelicit specific responses \n\no Agentsâ€™ range of actions , so that the user is aware of their capabilities and potential \n\nimpact \n\nâ€¢ Effective oversight of agents \n\no Common agent failure modes , such as hallucinations, getting stuck in loops after errors ,\n\nso that the user can identify and flag out issues .\n\no Ongoing support , such as regular refreshers to update users on latest features and \n\ncommon user mistakes \n\nâ€¢ Potential impact on tradecraft 24 \n\no As agents take over entry level tasks, which typically serve as the training ground for new \n\nstaff, this could le ad to loss of basic operational knowledge for the users. \n\no Organisations should identify core capabilities of each job and provide s ufficient training \n\nand work exposure so that users retain foundational skills. 25 \n\n# Annex A: Further resources \n\n1.  Introduction to Agentic AI \n\nWhat is Agentic \n\nAI? \n\nâ€¢ AWS , Agentic AI Security Scoping Matrix: A framework for securing \n\nautonomous AI systems \n\nâ€¢ WEF , AI Agents in Action: Foundations for Evaluation and \n\nGovernance \n\nâ€¢ Anthropic,  Building effective agents \n\nâ€¢ IBM,  The 2026 Guide to AI Agents \n\nâ€¢ McKinsey , What is an AI agent? \n\nRisks of Agentic AI  â€¢ GovTech , Agentic Risk & Capability Framework \n\nâ€¢ CSA , Draft Addendum on Securing Agentic AI \n\nâ€¢ OWASP , Multi -Agentic System Threat Modelling Guide \n\nâ€¢ IBM,  AI agents: Opportunities, risks, and mitigations \n\nâ€¢ Infosys,  Agentic AI risks to the enterprise, and its mitigations \n\n2.  MGF for Agentic AI \n\nAssess and bound \n\nthe risks upfront \n\nAgentic governance in general \n\nâ€¢ EY , Building a risk framework for Agentic AI \n\nâ€¢ McKinsey , Deploying agentic AI with safety and security: A playbook \n\nfor technology leaders \n\nâ€¢ Bain,  Building the Foundation for Agentic AI \n\nâ€¢ OWASP,  State of Agentic AI Security and Governance 1.0 \n\nRisk assessment and threat modelling \n\nâ€¢ OWASP , Agentic AI â€“ Threats & Mitigations \n\nâ€¢ OWASP , Multi -Agentic System Threat Modelling Guide \n\nâ€¢ Cloud Security Alliance,  Agentic AI: Understanding Its Evolution, \n\nRisks, and Security Challenges \n\nâ€¢ EY,  Building a risk framework for Agentic AI \n\nAgent limits and agent identity \n\nâ€¢ Meta,  Agents Rule of Two: A Practical Approach to AI Agent Security \n\nâ€¢ OpenID , Identity Management for Agentic AI \n\nMake humans \n\nmeaningfully \n\naccountable \n\nAllocating responsibility within and outside an organisation \n\nâ€¢ Carnegie Mellon University,  The â€˜Whoâ€™, â€˜Whatâ€™, and â€˜Howâ€™ of \n\nResponsible AI Governance \n\nâ€¢ CSA and FAR.AI,  Securing Agentic AI: A Discussion Paper \n\nâ€¢ McKinsey,  Accountability by design in the agentic organization \n\nDesigning for meaningful human oversight \n\nâ€¢ Partnership on AI , Prioritizing real -time failure detection in AI agents 26 \n\nâ€¢ Permit.IO , Human -in -the -Loop for AI Agents: Best Practices, \n\nFrameworks, Use Cases, and Demo \n\nImplement \n\ntechnical controls \n\nand processes \n\nTechnical controls \n\nâ€¢ GovTech , Agentic Risk & Capability Framework \n\nâ€¢ CSA , Draft Addendum on Securing Agentic AI \n\nTesting and evaluation \n\nâ€¢ Microsoft , Microsoft Agent Evaluators \n\nâ€¢ AWS , AWS Agent Evaluation \n\nâ€¢ Anthropic,  Demystifying evals for AI agents \n\nâ€¢ IBM,  What is AI Agent Evaluation? \n\nMonitoring and observability \n\nâ€¢ Microsoft,  Top 5 agent observability best practices for reliable AI \n\nEnabling end -user \n\nresponsibility \n\nâ€¢ Zendesk , What is AI transparency? A comprehensive guide \n\nâ€¢ HR Brew , Salesforceâ€™s head of talent growth and development \n\nshares how the tech giant is training its 72,000 employees on \n\nagentic AI \n\nâ€¢ Harvard Business Review , The Perils of Using AI to Replace Entry -\n\nLevel Jobs 27 \n\n# Annex B: Call for feedback and case studies \n\nCall for feedback: This is a living document, and we invit e suggestions on how the framework can \n\nbe updated or refined. The following questions can be used as a guide: \n\nâ€¢ Introduction to Agentic AI : Are the descriptions of agentic AI systems accurate and \n\nsufficiently comprehensive for readers to obtain a clear overview of the governance \n\nchallenges of agentic AI ? Are there other risks that should be included? \n\nâ€¢ Proposed Model Governance Framework : Are the four dimensions of the framework \n\npractical and applicable? Are there any other dimensions that should be included? For \n\neach dimension, are there specific governance and technical challenges and best \n\npractices that should be included? \n\nCall for case studies: We also invite organisations to submit their own agentic governance \n\nexperiences as case studies on how specific aspects of the framework can be implemented, to serve \n\nas practical examples of responsible deploymen t that other organisations can refer to . Case studies \n\nshould ideally involve an organisationâ€™s deployment of an agentic use case that demonstrates one \n\nof the dimensions of the framework . While not exhaustive, we are specifically interested in case \n\nstudies that demonstra te good practices in :\n\nDimension  Example case studies \n\nAssess and bound \n\nthe risks upfront \n\nâ€¢ Defining use cases to reduce risk but maximise benefits of \n\nagents \n\nâ€¢ Defining limits on agentâ€™s autonomy through defined SOPs and \n\nworkflows \n\nâ€¢ Defining limits on agentâ€™s access to tools and systems \n\nâ€¢ How identity is implemented for agents, and how it interacts with \n\nhuman identities in an organisation \n\nMake humans \n\nmeaningfully \n\naccountable \n\nâ€¢ Allocating responsibility across the organisation for agentic \n\ndeployment \n\nâ€¢ Assessing when human approvals are required in an agentic use \n\ncase, and how requests for such approvals are implemented \n\nImplement technical \n\ncontrols and \n\nprocesses \n\nâ€¢ Designing and implementing technical controls for agents \n\nâ€¢ How agentic safety testing is carried out \n\nâ€¢ How monitoring and observability mechanisms are set up, \n\nincluding defining alert thresholds and processing large volumes \n\nof agent -related data \n\nEnable end -user \n\nresponsibility \n\nâ€¢ Making information available to internal and external \n\nstakeholders who interact with and use agents \n\nâ€¢ Training human overseers to exercise effective oversight \n\nFor an example of what a case study may look like , please refer to those in our previous  Model \n\nGovernance Framework for AI .\n\nPlease note that a ny feedback and case studies may be incorporated into an updated version of the \n\nframework, and contributors will be acknowledged accordingly . Please submit your feedback and \n\ncase studies at this link : https://go.gov.sg/mgfagentic -feedback .", "fetched_at_utc": "2026-02-08T19:09:02Z", "sha256": "d729cf2c58b90a9dcd0f9a35309e5fc837197030f026dd114a85ffac77296768", "meta": {"file_name": "Singapore - Governance for Agentic AI.pdf", "file_size": 1078313, "relative_path": "pdfs\\Singapore - Governance for Agentic AI.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 12742}}}
{"doc_id": "pdf-pdfs-the-language-of-trustworthy-ai-glossary-nist-ca7880ea8842", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The Language of Trustworthy AI Glossary - NIST.pdf", "title": "The Language of Trustworthy AI Glossary - NIST", "text": "Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\naccountability 1) relates to an allocated responsibility. The responsibility can be based on regulation or agreement or through assignment as part of delegation; 2) For systems, a property that ensures that actions of an entity can be traced uniquely to the entity; 3) In a governance context, the obligation of an individual or organization to account for its activities, for completion of a deliverable or task, accept the responsibility for those activities, deliverables or tasks, and to disclose the results in a transparent manner. ISO/IEC_TS_ 5723Ê¼2022(en) \"accountable\" (adjective vs. noun): answerable for actions, decisions, and performance ISO/IEC_TS_ 5723Ê¼2022(en) accuracy Closeness of computations or estimates to the exact or true values that the statistics were intended to measure. OECD A qualitative assessment of correctness or freedom from error. FDA_Glossary The measure of an instrument's capability to approach a true or absolute value. It is a function of precision and bias. FDA_Glossary The accuracy of a machine learning system is measured as the percentage of correct predictions or classifications made by the model over a specific data set. It is typically estimated using a test or \"hold out\" sample, other than the one(s) used to construct the model. Its complement, the error rate, is the proportion of incorrect predictions on the same data. Raynor measure of closeness of results of observations, computations, or estimates to the true values or the values accepted as being true ISO/IEC_TS_ 5723Ê¼2022(en) active learning A proposed method for modifying machine learning algorithms by allowing them to specify test regions to improve their accuracy. At any point, the algorithm can choose a new point x, observe the output and incorporate the new (x, y) pair into its training base. It has been applied to neural networks, prediction functions, and clustering functions. Raynor Active learning (also called â€œquery learning,â€ or sometimes â€œoptimal experimental designâ€ in the statistics literature) is a subfield of machine learning and, more generally, artificial intelligence. The key hypothesis is that, if the learning algorithm is allowed to choose the data from which it learnsâ€”to be â€œcurious,â€ if you willâ€”it will perform better with less training. settles_active _2009 the process of learning through activities and/or discussion in class, as opposed to passively listening to an expert. Freeman_et_a l_2014 active learning agent [a machine learning algorithm that can] decide what actions to take [with regards to its training data, in contrast to a passive learning agent, which is limited to a fixed policy]. Russell_and_N orvig passive learning agent activity Work that an organization performs using business processes; can be singular or compound. IEEE_Guide_I PA Set of cohesive tasks of a process. CSRC adaptive dynamic programming An adaptive dynamic programming (or ADP) agent takes advantage of the constraints among the utilities of states by learning the transition model that connects them and solving the corresponding Markov decision process using dynamic programming. Russell_and_N orvig A means of learning a model and a reward function from observations that then uses value or policy iteration to obtain the utilities or an optimal policy; makes optimal use of the local constraints on utilities of states imposed through the neighborhood structure of the environment. Russell_and_N orvig adaptive learning Updating predictive models online during their operation to react to concept drifts Gama,_Joao adversarial example Machine learning input sample formed by applying a small but intentionally worst-case perturbation ... to a clean example, such that the perturbed input causes a learned model to output an incorrect answer. NISTIR_8269_ Draft Samples generated from real samples with carefully designed imperceptible perturbations Zhang, _Yonggang adversarial perturbation adverse action notice A notification of i) a refusal to grant credit in substantially the amount or on substantially the terms requested in an application unless the creditor makes a counteroffer (to grant credit in a different amount or on other terms) and the applicant uses or expressly accepts the credit offered; ii) A termination of an account or an unfavorable change in the terms of an account that does not affect all or substantially all of a class of the creditor's accounts or iii) A refusal to increase the amount of credit available to an applicant who has made an application for an increase. ECOA adverse impact ratio privileged and unprivileged groups receiving different outcomes irrespective of the decision makerâ€™s intent and irrespective of the decision-making procedure. Quantified as the ratio: disparate impact ratio = ð‘ƒ ( ð‘¦Ì‚ (ð‘‹ ) = fav âˆ£âˆ£ ð‘ = unpr )/ ð‘ƒ ( ð‘¦Ì‚ (ð‘‹ )= fav âˆ£âˆ£ ð‘ = priv ) where ð‘ƒ (ð‘¦Ì‚ (ð‘‹ ) = fav) is the favorable label, ( ð‘ = priv) is the privileged group, and ( ð‘ = unpr) is the unprivileged group. Varshney, _Kush disparate impact ratio, relative risk ratio agile a development approach that delivers software in increments by following the principles of the Manifesto for Agile Software Development. Gartner A philosophy and methodology used to describe the continuous, iterative process to develop and deliver software and other digital technologies. User requirements and feedback inform incremental development and delivery by developers. NSCAI AI principles [An overarching concept, value, belief, or norm that guides AI development, testing, and deployment across the AI lifecycle. The OECD] identifies five complementary values-based principles for the responsible stewardship of trustworthy AI and calls on AI actors to promote and implement them: inclusive growth, sustainable development and well-being; human-centred values and fairness; transparency and explainability; robustness, security and safety; and accountability. OECD_CAI_re commendation algorithm A set of computational rules to be followed to solve a mathematical problem. More recently, the term has been adopted to refer to a process to be followed, often by a computer. Comptroller_O ffice precise rules for transforming specified inputs into specified outputs in a finite number of steps knuth_art_198 1algorithms are step-by-step procedures for solving problems. For concreteness, we can think of them simply as being computed programs, written in some precise computer languages garey_comput ers_1979 algorithmic aversion biased assessment of an algorithm which manifests in negative behaviours and attitudes towards the algorithm compared to a human agent. Ekaterina_et_ al_2020 alignment ensur[ing] that powerful AI is properly aligned with human values. ... The challenge of alignment has two parts. The first part is technical and focuses on how to formally encode values or principles in artificial agents so that they reliably do what they ought to do. ... The second part of the value alignment question is normative . It asks what values or principles, if any, we ought to encode in artificial agents. Gabriel_2020 amplification [an act of amplifying, which is] to make larger or greater (as in amount, importance, or intensity). Merriam-Webster_ampl ify Let [construct space] ð‘Œ â€² and [prediction space] ð‘Œ Ë† be categorical. Then, a model exhibits disparity amplification if \n\nð‘‘ tv ( ð‘Œ Ë† | ð‘ =0, ð‘Œ Ë† | ð‘ =1) > ð‘‘ tv ( ð‘Œ â€² | ð‘ =0, ð‘Œ â€² | ð‘ =1). dtv is the total variation distance defined as follows. Let ð‘Œ 0 and ð‘Œ 1 becategorical random variables with finite supports Y0 and Y1. Then,the total variation distance between ð‘Œ 0 and ð‘Œ 1 is ð‘‘ tv ( ð‘Œ 0, ð‘Œ 1) =12 Î£\n\nð‘¦ âˆˆY0 âˆªY1 Pr[ ð‘Œ 0= ð‘¦ ] âˆ’ Pr[ ð‘Œ 1= ð‘¦ ] .In the special case where ð‘Œ 0, ð‘Œ 1 âˆˆ {0, 1}, the total variation distancecan also be expressed as | Pr[ ð‘Œ 0=1] âˆ’ Pr[ ð‘Œ 1=1] |. yeom_avoiding _2021 analytics Analytics is the application of scientific & mathematical methods to the study & analysis of problems involving complex systems. There are three distinct types of analytics: * Descriptive Analytics gives insight into past events, using historical data. * Predictive Analytics provides insight on what will happen in the future. * Prescriptive Analytics helps with decision making by providing actionable advice. informs_analyt ics_2022 annotation Further documentation accompanying a requirement. IEEE_Soft_Vo cab [the act of] mak[ing] or furnish[ing] critical or explanatory notes or comment Merriam-Webster_anno tate anomaly Anything observed in the documentation or operation of a system that deviates from expectations based on previously verified system, software, or hardware products or reference documents. IEEE_Soft_Vo cab Condition that deviates from expectations, based on requirements specifications, design documents, user documents, or standards, or from someone's perceptions or experiences. SP800-160 anonymization The process in which individually identifiable data is altered in such a way that it no longer can be related back to a given individual. Among many techniques, there are three primary ways that data is anonymized. Suppression is the most basic version of anonymization and it simply removes some identifying values from data to reduce its identifiability. Generalization takes specific identifying values and makes them broader, such as changing a specific age (18) to an age range (18-24). Noise addition takes identifying values from a given data set and switches them with identifying values from another individual in that data set. Note that all of these processes will not guarantee that data is no longer identifiable and have to be performed in such a way that does not harm the usability of the data. IAPP_Privacy_ Glossary process that removes the association between the identifying dataset and the data subject CSRC anthropomorphism the attribution of distinctively human-like feelings, mental states, and behavioral characteristics to inanimate objects, animals, and in general to natural phenomena and supernatural entities Anthropomorp hism_in_AI_2 020 a particular human-like interpretation of existing physical features and behaviors that goes beyond what is directly observable Anthropomorp hism_in_AI_2 020 application A software program hosted by an information system. SP800-37 A hardware/software system implemented to satisfy a particular set of requirements. CSRC software or a program that is specific tothe solution of an application problem aime_measure ment_2022 citing ISO/IEC TR 24030 application programming interface (API) a software contract between the application and client, expressed as a collection of methods or functions. . . it defines the available functions you can execute; . . . the intermediary interface between the client and the application. Hands-On_Smart_Co ntract_Dev artificial intelligence (AI) system an engineered or machine-based system that can, for a given set of objectives, generate outputs such as predictions, recommendations, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy NIST AI RMF (Adapted from: OECD Recommendati on on AIÊ¼2019; ISO/IEC 22989Ê¼2022). artificial intelligence learning The ingestion of a corpus, application of semantic mapping, and relevant ontology of structured and/or unstructured data that yields inference and correlation leading to the creation of useful conclusive or predictive capabilities in a given knowledge domain. Strong AI learning also includes the capability of creating unique hypotheses, attributing data relevance, processing data relationships, and updating its own lines of inquiry to further the usefulness of its purpose. IEEE_Guide_I PA artificial narrow intelligence (ANI) [an AI system that] is designed to accomplish a specific problem-solving or reasoning task. OECD_Artifici al_Intelligence _in_Society weak intelligence; applied intelligence Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nartificial neural networks A computing system, made up of a number of simple, highly interconnected processing elements, which processes information by its dynamic state response to external inputs. Reznik,_Leon Definition 1. A directed graph is called an Artificial Neural Network (ANN) if it has x at least one start node (or Start Element; SE), x at least one end node (or End Element; EE), x at least one Processing Element (PE), x all the nodes used must be Processing Elements (PEs), except start nodes and end nodes, x a state variable ni associated with each node i, x a real valued weight wki associated with each link (ki) from node k to node i, x a real valued bias bi associated with each node i, x at least two of the multiple PEs connected in parallel, x a learning algorithm that helps to model the desired output for given input. x a flow on each link (ki) from node k to node i, that carries exactly the same flow which equals to nk caused by the output of node k , x each start node is connected to at least one end node, and each end node is connected to at least one start node, x no parallel edges (each link (ki) from node k to node i is unique). assessment Action of applying specific documented criteria to a specific software module, package or product for the purpose of determining acceptance or release of the software module, package or product. IEEE_Soft_Vo cab the action or an instance of making a judgment about something : the act of assessing something : APPRAISAL Merriam-Webster_asses sment attack Action targeting a learning system to cause malfunction. NISTIR_8269_ Draft Any kind of malicious activity that attempts to collect, disrupt, deny, degrade, or destroy information system resources or the information itself. CSRC attribute Property associated with a a set of real or abstract things that is some characteristic of interest. IEEE_Soft_Vo cab property or characteristic of an object that can be distinguished quantitatively or qualitatively by human or automated means aime_measure ment_2022, citing ISO/IEC TR 24029-1 audit Systematic, independent, documented process for obtaining records, statements of fact, or other relevant information and assessing them objectively, to determine the extent to which specified requirements are fulfilled. IEEE_Soft_Vo cab To conduct an independent review and examination of system records and activities in order to test the adequacy and effectiveness of data security and data integrity procedures, to ensure compliance with established policy and operational procedures, and to recommend any necessary changes. FDA_Glossary Independent examination of a software product, software process, or set of software processes to assess compliance with specifications, standards, contractual agreements, or other criteria NASA_Soft_St andards Independent review conducted to compare the various aspects of the laboratoryâ€™ s performance with a standard for that performance. Also defined as a systematic, independent and documented process for obtaining audit evidence and evaluating it objectively to determine the extent to which audit criteria are fulfilled. UNODC_Gloss ary_QA_GLP audit log A chronological record of system activities, including records of system accesses and operations performed in a given period. SP800-37 authenticity property that an entity is what it claims to be ISO/IEC_TS_ 5723Ê¼2022(en) automation Independent machine-managed choreography of the operation of one or more digital systems. IEEE_Guide_I PA conversion of processes or equipment to automatic operation, or the results of the conversion IEEE_Soft_Vo cab The system functions with no/little human operator involvement; however, the system performance is limited to the specific actions it has been designed to do. Typically these are well-defined tasks that have predetermined responses (i.e., simple rule-based responses). DOD_TEVV automation bias over-relying on the outputs of AI systems David_Leslie_ Morgan_Brigg sautonomic A monitor-analyze-plan-execute (MAPE) computer system capable of sensing environments, interpreting policy, accessing knowledge (data --- information ---knowledge), making decisions, and initiating dynamically assembled routines of choreographed activity to both complete a process and update the set of environmental variables that enables the autonomic system to self-manage its own operation and the processes it oversees. An autonomic system is identified by eight characteristics: a) Knows the resources to which it has access, what its capabilities and limitations are, and how and why it is connected to other systems. b) Is able to configure and reconfigure itself depending on the changing computing environment. c) Is able to optimize its performance to ensure the most efficient computing process. d) Is able to work around encountered problems either by repairing itself or routing functions away from the trouble. e) Is able to detect, identify, and protect itself against various types of attacks to maintain overall system security and integrity. f) Is able to adapt to its environment as it changes by interacting with neighboring systems and establishing communication protocols. g) Relies on open standards and requires access to proprietary environments to achieve full performance. h) Is able to anticipate the demand on its resources transparently to users. IEEE_Guide_I PA autonomous vehicle [an] automobile, bus, tractor, combine, boat, forklift, etc. . . . capable of sensing its environment and moving safely with little or no human input. Introduction_t o_Information _Systems autonomy A systemâ€™s level of independence from human involvement and ability to operate without human intervention. [Different AI systems have different levels of autonomy.] An autonomous system has a set of learning, adaptive and analytical capabilities to respond to situations that were not pre-programmed or anticipated (i.e., decision-based responses) prior to system deployment. Autonomous or semi-autonomous AI systems can be characterised as \"human-in-the-loop\", \"human-on-the-loop\", or \"human-out-of-the loop\" systems depending on their level of meaningful involvement of human beings. TTC6_Taxono my_Terminolo gy availability Ensuring timely and reliable access to and use of information. SP800-37 The property that data or information is accessible and usable upon demand by an authorized person. NIST_SP_800 property of being accessible and usable on demand by an authorized entity ISO/IEC_TS_ 5723Ê¼2022(en) back-testing A form of outcomes analysis that involves the comparison of actual outcomes with modeled forecasts during a development sample time period (in-sample back-testing) and during a sample period not used in model development (out-of-time back-testing), and at an observation frequency that matches the forecast horizon or performance window of the model. Comptroller_O ffice batched automation Process automation execution of intentionally segregated work processes that are able to be processed irrespective of their contextual placement within a service. IEEE_Guide_I PA benchmark Standard against which results can be measured or assessed; Procedure, problem, or test that can be used to compare systems or components to each other or to a standard. IEEE_Soft_Vo cab An alternative prediction or approach used to compare a modelâ€™s inputs and outputs to estimates from alternative internal or external data or models. Comptroller_O ffice The term benchmarking is used in machine learning (ML) to refer to the evaluationand comparison of ML methods regarding their ability to learn patterns in â€˜benchmarkâ€™datasets that have been applied as â€˜standardsâ€™. Benchmarking could be thought of simplyas a sanity check to confirm that a new method successfully runs as expected and canreliably find simple patterns that existing methods are known to identify. olson_pmlb_2 017 bias A systematic error. In the context of fairness, we are concerned with unwanted bias that places privileged groups at systematic advantage and unprivileged groups at systematic disadvantage. AI_Fairness_3 60 (computational bias) An effect which deprives a statistical result of representativeness by systematically distorting it, as distinct from a random error which may distort on any one occasion but balances out on the average. OECD (systemic bias) systematic difference in treatment of certain objects, people or groups in comparison to others measurement_ iso22989_2022 (mathematical) A point estimator \\theta_hat is said to be an unbiased estimator fo \\theta if E(\\theta_hat) = \\theta for every possible value of \\theta. If \\theta_hat is not unbiased, the difference E(\\theta_hat) - \\theta is called the bias of \\theta devore_probab ility_2004 bias mitigation algorithm A procedure for reducing unwanted bias in training data or models. AI_Fairness_3 60 bias testing As it relates to disparate impact, courts and regulators have utilized or considered as acceptable various statistical tests to evaluate evidence of disparate impact. Traditional methods of statistical bias testing look at differences in predictions across protected classes, such as race or sex. In particular, courts have looked to statistical significance testing to assess whether the challenged practice likely caused the disparity and was not the result of chance or a nondiscriminatory factor. SP1270 big data consists of extensive datasets primarily in the characteristics of volume, variety, velocity, and/or variability \u0000that require a scalable architecture for efficient storage, manipulation, and analysis NIST_1500 binning a technique of lumping small ranges of values together into categories, or \"bins,\" for the purpose of reducing the variability (removing some of the fine structure) in a data set. Pyle, _Dorian_Data _Preparation_ as_a_Process biometric data personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person, such as facial images or dactyloscopic data; GDPR an individualâ€™s physiological, biological, or behavioral characteristics, including information pertaining to an individualâ€™s deoxyribonucleic acid (DNA), that is used or is intended to be used singly or in combination with each other or with other identifying data, to establish individual identity. Biometric information includes, but is not limited to, imagery of the iris, retina, fingerprint, face, hand, palm, vein patterns, and voice recordings, from which an identifier template, such as a faceprint, a minutiae template, or a voiceprint, can be extracted, and keystroke patterns or rhythms, gait patterns or rhythms, and sleep, health, or exercise data that contain identifying information. CCPA A measurable physical characteristic or personal behavioral trait used to recognize the identity, or verify the claimed identity, of an applicant. Facial images, fingerprints, and iris scan samples are all examples of biometrics. SP800-12 personal data; processing boosting A machine learning technique that iteratively combines a set of simple and not very accurate classifiers (referred to as \"weak\" classifiers) into a classifier with high accuracy (a \"strong\" classifier) by upweighting the examples that the model is currently misclassifying aime_measure ment_2022, citing Machine Learning Glossary by Google breach The loss of control, compromise, unauthorized disclosure, unauthorized acquisition, or any similar occurrence where: a person other than an authorized user accesses or potentially accesses personally identifiable information; or an authorized user accesses personally identifiable information for another than authorized purpose. CSRC broad artificial intelligence (broad AI) Complex, computational, cognitive automation system capable of providing descriptive, predictive, prescriptive, and limited deductive analytics with relevance and accuracy exceeding human expertise in a broad, logically related set of knowledge domains. IEEE_Guide_I PA Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nbuilt-in test Equipment or software embedded in the operational components or systems, as opposed to external support units, which perform a test or sequence of tests to verify mechanical or electrical continuity of hardware, or the proper automatic sequencing, data processing, and readout of hardware or software systems. SP1011 bug-bounty Reward given to independent security researchers, penetrations testers, and white hat hackers for discovering exploitable software vulnerabilities and sharing this knowledge with the operator of a particular bug-bounty program (BBP). Kuehn, _Andreas business process A defined set of business activities that represent the steps or tasks required to achieve a business objective, including the flow and use of information, participants, and human or digital resources. IEEE_Guide_I PA business process management Discipline involving any combination of modeling, automation, execution, control, measurement and optimization of business activity flows, in support of enterprise goals, spanning systems, employees, customers, and partners within and beyond the enterprise boundaries. IEEE_Guide_I PA business rule Definition, constraint, dependency, or decision criteria that determine the method of execution of a task or tasks, or influences the order of execution of a task or tasks. Business rules assert control, or influence the behavior, of a business process within computing systems. IEEE_Guide_I PA calibration A comparison between a device under test and an established standard, such as UTC(NIST). When the calibration is finished, it should be possible to state the estimated time offset and/or frequency offset of the device under test with respect to the standard, as well as the measurement uncertainty. CSRC operation that, under specified conditions, in a first step, establishes a relation between the quantity values with measurement uncertainties provided by measurement standards and corresponding indications with associated measurement uncertainties and, in a second step, uses this information to establish a relation for obtaining a measurement result from an indication aime_measure ment_2022, citing ISO/IEC Guide 99 Set of operations that establish, under specified conditions, the relationship between values indicated by a measuring instrument or measuring system, or values represented by a material measure, and the corresponding known values of a measurand .UNODC_Gloss ary_QA_GLP capability measure of capacity and the ability of an entity, person or organization to achieve its objectives ISO/IEC_TS_ 5723Ê¼2022(en) case Single entry, single exit multiple way branch that defines a control expression, specifies the processing to be performed for each value of the control expression, and returns control in all instances to the statement immediately following the overall construct. IEEE_Soft_Vo cab chatbot Conversational agent that dialogues with its user (for example: empathic robots available to patients, or automated conversation services in customer relations). COE_AI_Gloss ary choreography An ordered sequence of system-to-system message exchanges between two or more participants. In choreography, there is no central controller, responsible entity, or observer of the process. IEEE_Guide_I PA classification When the output is one of a finite set of values (such as sunny, cloudy or rainy), the learning problem is called classification, and is called Boolean or binary classification if there are only two values. AIMA task of assigning collected data to target categories or classes. aime_measure ment_2022, citing ISO/IEC TR 24030 classifier A model that predicts categorical labels from features. AI_Fairness_3 60 clustering Detecting potentially useful clusters of input examples. AIMA The basic problem of clustering may be stated as follows: Given a set of data points, partition them into a set of groups which are as similar as possible. aggarwal_clust ering_2013 the tendency for items to be consistently grouped together in the course of recall. This grouping typically occurs for related items. It is readily apparent in memory tasks in which items from the same category, such as nonhuman animals, are recalled together. APA_clusterin gcognitive automation The identification, assessment, and application of available machine learning algorithms for the purpose of leveraging domain knowledge and reasoning to further automate the machine learning already present in a manner that may be thought of as cognitive. With cognitive automation, the system performs corrective actions driven by knowledge of the underlying analytics tool itself, iterates its own automation approaches and algorithms for more expansive or more thorough analysis, and is thereby able to fulfill its purpose. The automation of the cognitive process refines itself and dynamically generates novel hypotheses that it can likewise assess against its existing corpus and other information resources. IEEE_Guide_I PA cognitive computing Complex computational systems designed to â€” Sense (perceive the world and collect data); â€” Comprehend (analyze and understand the information collected); - Act (make informed decisions and provide guidance based on this analysis in an independent way); and â€” Adapt (adapt capabilities based on experience) in ways comparable to the human brain. IEEE_Guide_I PA column In the context of relational databases, a column is a set of data values, all of a single type, in a table. techopedia_co lumn_2022 computer vision The digital process of perceiving and learning visual tasks in order to interpret and understand the world through cameras and sensors. NSCAI An image understanding task that automatically builds a description not only of the image itself, but of the three dimensional scene that it depicts. NBSIR_82-2582 concept drift Use of a system outside the planned domain of application, and a common cause of performance gaps between laboratory settings and the real world. SP1270 an online supervised learning scenario when the relation between the input data and the target variable changes over time. Gama,_Joao Systems that classify or predict a concept (e.g., credit ratings or computer intrusion monitors) over time can suffer performance loss when the concept they are tracking changes. This is referred to as concept drift. This can either be a natural process that occurs without a reference to the system, or an active process, where others are reacting to the system (e.g., virus detection). Raynor confidentiality Data confidentiality is a property of data, usually resulting from legislative measures, which prevents it from unauthorized disclosure. OECD Preserving authorized restrictions on information access and disclosure, including means for protecting personal privacy and proprietary information. CSRC The property that data or information is not made available or disclosed to unauthorized persons or processes. NIST_SP_800 A property that information is not disclosed to users, processes, or devices unless they have been authorized to access the information. CISA confusion matrix A matrix showing the predicted and actual classifications. A confusion matrix is of size LxL, where L is the number of different label values Kohavi,_Ron consent â€˜Consentâ€™ of the data subject means any freely given, specific, informed and unambiguous indication of the data subject's wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her. GDPR â€œConsentâ€ means any freely given, specific, informed, and unambiguous indication of the consumerâ€™s wishes by which the consumer, or the consumerâ€™s legal guardian, a person who has power of attorney, or a person acting as a conservator for the consumer, including by a statement or by a clear affirmative action, signifies agreement to the processing of personal information relating to the consumer for a narrowly defined particular purpose. Acceptance of a general or broad terms of use, or similar document, that contains descriptions of personal information processing along with other, unrelated information, does not constitute consent. Hovering over, muting, pausing, or closing a given piece of content does not constitute consent. Likewise, agreement obtained through use of dark patterns does not constitute consent. CCPA personal data constituent system independent system that forms part of a system of systems (SoS) (note: Constituent systems can be part of one or more SoS. Each constituent system is a useful system by itself, having its own development, management, utilization, goals, and resources, but interacts within the SoS to provide the unique capability of the SoS). ISO/IEC_TS_ 5723Ê¼2022(en) constraint Specification of what may be contained in a data or metadata set in terms of the content or, for data only, in terms of the set of key combinations to which specific attributes (defined by the data structure) may be attached. OECD A limitation or implied requirement that constrains the design solution or implementation of the systems engineering process and is not changeable by the enterprise IEEE_Soft_Vo cab construct validity the degree to which the application of constructs to phenomena is warranted with respect to the research goals and questions. Wieringa, _Roel_J. Construct validation is involved whenever a test is to be interpreted as a measure of some attribute or quality which is not â€œoperationally defined.â€ The problem faced by the investigator is, â€œWhat constructs account for variance in test performance?â€ cronbach_con struct_1955 Established experimentally to demonstrate that a survey distinguishes between people who do and do not have certain characteristics. It is usually established experimentally. fink_survey_2 010 Establishing construct validity means demonstrating, in a variety of ways, that the measurements obtained from measurement model are both meaningful and useful. jacobs_measur ement_2023 content validity Refers to the extent to which a measure thoroughly and appropriately assesses the skills or characteristics it is intended to measure. fink_survey_2 010 the extent to which a test measures a representative sample of the subject matter or behavior under investigation. For example, if a test is designed to survey arithmetic skills at a third-grade level, content validity indicates how well it represents the range of arithmetic operations possible at that level. Modern approaches to determining content validity involve the use of exploratory factor analysis and other multivariate statistical procedures. APA_content_ validity context The context is the circumstances, purpose, and perspective under which an object is defined or used. OECD The immediate environment in which a function (or set of functions in a diagram) operates IEEE_Soft_Vo cab the interrelated conditions in which something exists or occurs. Merriam-Webster_cont ext contextual learning A computing system with sufficient knowledge regarding its purpose that it understands the source, relevance, and utility of data and inputs. IEEE_Guide_I PA context-of-use The Context of Use is the actual conditions under which a given artifact/software product is used, or will be used in a normal day to day working situation. interaction_co ntext_2023 comprises a combination of users, goals, tasks, resources, and the technical, physical and social, cultural and organizational environments in which a system, product or service is used[; ...] can include the interactions and interdependencies between the object of interest and other systems, products or services. ISO_9241-11Ê¼ 2018 controllability property of a system that allows a human or another external agent to intervene in the systemâ€™s functioning; such a system is heteronomous. ISO/IEC_TS_ 5723Ê¼2022(en) control class (control group) the set of observations in an experiment or prospective study that do not receive the experimental treatment(s). These observations serve (a) as a comparison point to evaluate the magnitude and significance of each experimental treatment, (b) as a reality check to compare the current observations with previous observation history, and (c) as a source of data for establishing the natural experimental error. nist_statistics _2012 controller â€˜Controllerâ€™ means the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law; GDPR personal data; processor Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\ncopilot An artificial intelligence powered software program designed to assist users with various tasks and automate features within compatible applications using advanced language models, machine-learning algorithms, and conversational interfaces to understand user requests and provide suggestions, summaries, and content generation in response. A product or service that provides assistance using, incorporating and/or based on artificial intelligence software and artificial intelligence software services corpus (corpora) A deliberately assembled collection of knowledge and data (structured and/or unstructured) believed to contain relevant information on a topic or topics to be used by software systems for which useful analysis, prediction, or outcome is being sought. IEEE_Guide_I PA correlation In its most general sense correlation denoted the interdependence between quantitative or qualitative data. In this sense it would include the association of dichotomised attributes and the contingency of multiply-classified attributes. OECD The correlation coefficient of two random variables y_1, and y_2, denoted \\rho (y_1,y_2) is: \\rho(y_1, y_2) = Cov(y_1, y_2)/\\sqrt{Var(y_1)*Var(y_2)} box_statistics _2005 counterfactual explanation Statements taking the form: Score p was returned because variables V had values (v1, v2,...) associated with them. If V instead had values (v1', v2',...) score p' would have been returned. wachter_coun terfactual_201 8counterfactual fairness A fairnessmetric that checks whether a classifier produces the same result for one individualas it does for another individual who is identical to the first, except withrespect to one or more sensitive attributes. Evaluating a classifier for counterfactualfairness is one method for surfacing potential sources of bias in a model aime_measure ment_2022, citing Machine Learning Glossary by Google Given a predictive problem with fairness considerations, where A, X and Y represent the protectedattributes, remaining attributes, and output of interest respectively, let us assume that we are given acausal model (U; V; F), where V = A \\cup X. We postulate the following criterion for predictors of Y .Definition 5 (Counterfactual fairness). Predictor ^Y is counterfactually fair if under any context X = x and A = a, P( ^Y_{A <- a} (U) = y | X = x; A = a) = P( ^Y_{A <- a')(U) = y | X = x;A = a); (1) for all y and for any value a' attainable by A. kusner_counte rfactual_2017 countermeasure Actions, devices, procedures, techniques, or other measures that reduce the vulnerability of a system. Synonymous with security controls and safeguards. SP800-37 Actions, devices, procedures, or techniques that meet or oppose (i.e., counters) a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken. GWUC safeguard; security control criterion validity compares responses to future performance or to those obtained from other, more well-established surveys. Criterion validity is made up two subcategories: predictive and concurrent. Predictive validity refers to the extent to which a survey measure forecasts future performance. A graduate school entry examination that predicts who will do well in graduate school has predictive validity. Concurrent validity is demonstrated when two assessments agree or a new measure is compared favorably with one that is already considered valid. fink_survey_2 010 an index of how well a test correlates with an established standard of comparison (i.e., a criterion). Criterion validity is divided into three types: predictive validity, concurrent validity, and retrospective validity. For example, if a measure of criminal behavior is valid, then it should be possible to use it to predict whether an individual (a) will be arrested in the future for a criminal violation, (b) is currently breaking the law, and (c) has a previous criminal record. APA_criterion _validity criterion-referenced validity; criterion-related validity crowdsource a type of participative online activity in which an individual, an institution, a non-profit organization, or company proposes to a group of individuals of varying knowledge, heterogeneity, and number, via a flexible open call, the voluntary undertaking of a task. Enrique customer The beneficiary of the execution of an automated task, process, or service. IEEE_Guide_I PA cybersecurity Prevention of damage to, protection of, and restoration of computers, electronic communications systems, electronic communications services, wire communication, and electronic communication, including information contained therein, to ensure its availability, integrity, authentication, confidentiality, and nonrepudiation. SP800-37 dark pattern â€œDark patternâ€ means a user interface designed or manipulated with the substantial effect of subverting or impairing user autonomy, decisionmaking, or choice, as further defined by regulation. CCPA data Characteristics or information, usually numerical, that are collected through observation. OECD re-interpretable representation of information ina formalized manner suitable for communication, interpretation or processing aime_measure ment_2022, citing ISO/IEC TR 24029-1 data analytics the process of applying graphical, statistical, or quantitative techniques to a set of observations or measurements in order to summarize it or to find general patterns. APA_data_ana lysis Data analysis is the process of transforming raw data into usable information, often presented in the form of a published analytical article, in order to add value to the statistical output. OECD data cleaning Data Cleaning is the process of identifying, correcting, or removing inaccurate or corrupt data records Ranschaert, _Erik data control management oversight of information policies for an organizationâ€™s information; observing and reporting on how processes are working and managing issues. Egnyte data dredging A statistical bias in which testing huge numbers of hypotheses of a dataset may appear to yield statistical significance even when the results are statistically nonsignificant. SP1270 statistical bias; p-hacking data drift The change in model input data that leads to model performance degradation. Microsoft_Azu re_documenta tion data-driven Data-driven decision making (DDD) refers to the practice of basing decisions on the analysis of data rather than purely on intuition. provost_data_ 2013 data fabric A data corpus, after the application of semantic mapping, relevant ontologies, and data seeding sufficient for artificial intelligence (AI) or machine learning algorithms to provide meaningful insight, prediction, and/or prescription. IEEE_Guide_I PA data fusion A process in which data, generated by multiple sensory sources, is integrated and/or correlated to create information, knowledge, and/or intelligence that may be displayed for user or be actionable to accomplish the tasks. SP1011 The process of combining data from multiple sources to produce more accurate, consistent, and concise information than that provided by any individual data source. Munir,_Arslan data governance A set of processes that ensures that data assets are formally managed throughout the enterprise. A data governance model establishes authority and management and decision making parameters related to the data produced or managed by the enterprise. CSRC refers to a system, including policies, people, practices, and technologies, necessary to ensure data management within an organization NIST_1500 data mining computational process that extracts patternsby analysing quantitative data from different perspectives and dimensions, categorizingthem, and summarizing potential relationships and impacts aime_measure ment_2022 citinig ISO/IEC 22989 the process of data analysis and information extraction from large amounts of datasets with machine learning, statistical approaches. and many others. Ranschaert, _Erik data point a discrete unit of information. TechTarget_da ta_point data preparation We define data preparation as the set of preprocessing operations performed in early stages of a data processing pipeline, i.e., data transformations at the structural and syntactical levels hameed_data_ 2020 data proxy Data that are closely related to and serve in place of data that are either unobservable or immeasurable. Comptroller_O ffice data quality degree to which the characteristics of data satisfy stated and implied needs when used under specified conditions IEEE_Soft_Vo cab The dimensions of the IMF definition of \"data quality\" are: - integrity; - methodological soundness; - accuracy and reliability; - serviceability; - accessibility. There are a number of prerequisites for quality. These comprise: - legal and institutional environment; - resources; - quality awareness. OECD data science Methodology for the synthesis of useful knowledge directly from data through a process of discovery or of hypothesis formulation and hypothesis testing. NIST_1500 Interdisciplinary science that uses statistics, algorithms, and other methods to extract meaningful and useful patterns from data setsâ€”sometimes known as â€œbig data.â€ Today, machine learning is often used in this field. Next to analysis of data, data science is also concerned with the capturing, preparation, and interpretation of data. AI_Ethics_Mar k_Coeckelberg hartificial intelligence (AI); machine learning (ML) data scientist A practitioner who has sufficient knowledge in the overlapping regimes of business needs, domain knowledge, analytical skills, and software and systems engineering to manage the end-to-end data processes in the analytics life cycle. NIST_1500 data seeding The intentional introduction of initial state conditions, influencing factors, and outcomes (both successful and unsuccessful) in a data fabric to create sufficient machine learning analysis signals to enable encouragement/discouragement to enrich deterministic relationships between data elements in a given information domain. IEEE_Guide_I PA data wrangling process by which the data required by an application is identified, extracted, cleaned and integrated, to yield a data set that is suitable for exploration and analysis. Furche,_Tim decision A conclusion reached after consideration of business rules and relevant data within a given process. IEEE_Guide_I PA Types of statements in which a choice between two or more possible outcomes controls which set of actions will result. IEEE_Soft_Vo cab decision point A point within a business process where the process flow can take one of several alternative paths, including recursive. IEEE_Guide_I PA decision tree Treeâ€structure resembling a flowchart, where every node represents a test to an attribute, each branch represents the possible outcomes of that test, and the leaves represent the class labels. Reznik,_Leon decision-making the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker. Every decision-making process produces a final choice, which may or may not prompt action. Wikipedia_Dec ision-making the cognitive process of choosing between two or more alternatives, ranging from the relatively clear cut (e.g., ordering a meal at a restaurant) to the complex (e.g., selecting a mate). Psychologists have adopted two converging strategies to understand decision making: (a) statistical analysis of multiple decisions involving complex tasks and (b) experimental manipulation of simple decisions, looking at the elements that recur within these decisions. APA_decision_ making Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\ndecision support system a computer program application used to improve a company's decision-making capabilities. It analyzes large amounts of data and presents an organization with the best possible options available[; they] bring together data and knowledge from different areas and sources to provide users with information beyond the usual reports and summaries. This is intended to help people make informed decisions. TechTarget_d ecision_suppo rt_system decommission the total or partial removal of existing components and their corresponding sub-components from Production and any relevant environment, minimizing risks and impacts, ensuring policy compliance, and maximizing the financial benefits (i. e., optimizing the cost reduction). IG1190M_AIOp s_Decommissi on_v1.0.0 deductive analytics Insights, reporting, and information answering the question, \"What would likely happen IFâ€¦?â€ Deductive analytics evaluates causes and outcomes of possible future events. IEEE_Guide_I PA deductive reasoning deep learning Deep learning is a broad family of techniques for machine learning in which hypotheses take the form of complex algebraic circuits with tunable connection strengths. The word â€œdeepâ€ refers to the fact that the circuits are typically organized into many layers, which means that computation paths from inputs to outputs have many steps. Deep learning is currently the most widely used approach for applications such as visual object recognition, machine translation, speech recognition, speech synthesis, and image synthesis; it also plays a significant role in reinforcement learning applications. Russell_and_N orvig A form of machine learning that uses neural networks with several layers of \"neurons\": simple interconnected processing units that interact. AI_Ethics_Mar k_Coeckelberg h[an approach to AI that allows] computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept defined through its relation to simpler concepts. By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all the knowledge that the computer needs. The hierarchy of concepts enables the computer to learn complicated concepts by building them out of simpler ones. If we draw a graph showing how these concepts are built on top of each other, the graph is deep, with many layers. deeplearningb ook_intro deepfake AI-generated or manipulated image, audio or video content that resembles existing persons, objects, places or other entities or events and would falsely appear to a person to be authentic or truthful. TTC6_Taxono my_Terminolo gy deletion Of an <X>, the action of destroying an instantiated <X>. IEEE_Soft_Vo cab denial-of-service The prevention of authorized access to resources or the delaying of time-critical operations. (Time-critical may be milliseconds or it maybe hours, depending upon the service provided). SP800-12 An attack that prevents or impairs the authorized use of information system resources or services. CISA when legitimate users are unable to access information systems, devices, or other network resources due to the actions of a malicious cyber threat actor. Services affected may include email, websites, online accounts (e.g., banking), or other services that rely on the affected computer or network. A denial-of-service condition is accomplished by flooding the targeted host or network with traffic until the target cannot respond or simply crashes, preventing access for legitimate users. DoS attacks can cost an organization both time and money while their resources and services are inaccessible. ST04-015 dependability <of an item> ability to perform as and when required (note 1Ê¼ includes availability, reliability, recoverability, maintainability, and maintenance support performance, and, in some cases, other characteristics such as durability, safety and security. Note 2Ê¼ used as a collective term for the time-related quality characteristics of an item). ISO/IEC_TS_ 5723Ê¼2022(en) deployment Phase of a project in which a system is put into operation and cutover issues are resolved IEEE_Soft_Vo cab descriptive analytics Insights, reporting, and information answering the question, â€œWhy did something happen?â€ Descriptive analytics determines information useful to understanding the cause(s) of an event(s). IEEE_Guide_I PA deterministic modelling [that] produces consistent outcomes for a given set of inputs, regardless of how many times the model is recalculated. The mathematical characteristics are known in this case. None of them is random, and each problem has just one set of specified values as well as one answer or solution. The unknown components in a deterministic model are external to the model. It deals with the definitive outcomes as opposed to random results and doesnâ€™t make allowances for error. Sourabh_Meht a_deterministi cdeterministic algorithm An algorithm that, given the same inputs, always produces the same outputs. CSRC developer A general term that includes developers or manufacturers of systems, system components, or system services; systems integrators; vendors; and product resellers. Development of systems, components, or services can occur internally within organizations or through external entities. SP800-37 Individual or organization that performs development activities (including requirements analysis, design, testing through acceptance) during the system or software lifeâ€cycle process. IEEE_Soft_Vo cab diagnostic analytics Insights, reporting, and information answering the question, â€œWhy did something happen?â€ Diagnostic analytics determines information useful to understanding the cause(s) of an event(s). IEEE_Guide_I PA diagnostics Pertaining to the detection and isolation of faults or failures IEEE_Software _Vocab differential privacy Differential privacy is a method for measuring how much information the output of a computation reveals about an individual. It is based on the randomised injection of \"noise\". Noise is a random alteration of data in a dataset so that values such as direct or indirect identifiers of individuals are harder to reveal. An important aspect of differential privacy is the concept of â€œepsilonâ€ or É›, which determines the level of added noise. Epsilon is also known as the â€œprivacy budgetâ€ or â€œprivacy parameterâ€. privacy-enhancing_tec hnologies For two datasets D and D' that differ in at most one element, a randomized algorithm $M$ guarantees \\emph{$(\\epsilon, \\delta)$-differential privacy} for any subset of the output $S$ if $M$ satisfies: \\begin{equation} Pr[M(D) \\in S] \\leq exp(\\epsilon)*Pr[M(D') \\in S] + \\delta \\end{equation} Furthermore, when $\\delta = 0$ an algorithm M is said to guarantee \\emph {$\\epsilon$-differential privacy} gong_different ial_2020 differential validity Differential validity states that the validities in two applicant populations are unequal, that is, pi != pa. hunter_differe ntial_1979 digital labor Digital automation of information technology systems and/or business processes that successfully delivers work output previously performed by human labor or new work output that would typically or alternatively have been performed by human labor. IEEE_Guide_I PA digital workforce The collective suite of automation technologies delivering existing or new work output as applied in a business; the manifestation of digital labor. IEEE_Guide_I PA dimension The dimension of an object is a topological measure of the size of its covering properties. Roughly speaking, it is the number of coordinates needed to specify a point on the object. wolfram_math _2022 Distinct components that a multidimensional construct encompasses IEEE_Soft_Vo cab dimension reduction Dimensionality reduction is the process of taking data in a high dimensional space and mapping it into a new space whose dimensionality is much smaller Shalev-Shwartz,_Shai disparate impact For Predictor Y and Sensitive Impact S. Definition 6.2 Disparate Impact (DI) = P[YË† = 1 | S != 1]/P[YË† = 1 | S = 1] friedler_comp arative_2019 disparate treatment Intentional discrimination, including (i) decisions explicitly based on protected characteristics; and (ii) intentional discrimination via proxy variables (e.g literacy tests for voting eligibility). Lipton, _Zachary distributional robustness Optimizing the predictive accuracy for a whole class of distributions instead of just a single target distribution. Meinshausen, _Nicolai diversity the practice of including the many communities, identities, races, ethnicities, backgrounds, abilities, cultures, and beliefs of the American people, including underserved communities. EO_DEIA_202 1inclusion documentation Collection of documents on a given subject; written or pictorial information describing, defining, specifying, reporting, or certifying activities, requirements, procedures, or results. IEEE_Soft_Vo cab domain Distinct scope, within which common characteristics are exhibited, common rules observed, and over which a distribution transparency is preserved. IEEE_Soft_Vo cab A set of elements, data, resources, and functions that share a commonality in combinations of: (1) roles supported, (2) rules governing their use, and (3) protection needs. SP800-160 <artificial intelligence> specific field of knowledgeor expertise aime_measure ment_2022, citing ISO/IEC 2382 domain expertise Domain expertise implies knowledge and understanding of the essential aspects of a specific field of inquiry. McCue_Collee ndomain shift Differences between the source and target domain data Stacke,_Karin distributional shift drinking your own champagne The practice in which tech workers use their own product consistently to see how well it works and where improvements can be made. kelley_dogfoo ding_2022 dogfooding, eating your own dogfood dynamic process The process in which one or more paths are defined and may be utilized based on the conditions present at the time of execution. IEEE_Guide_I PA edge case a problem or situation, especially in computer programming, that only happens at the highest or lowest end of a range of possible values or in extreme situations: cambridge_dic tionary_2022 embedding An embedding is a representation of a topological object, manifold, graph, field, etc. in a certain space in such a way that its connectivity or algebraic properties are preserved. For example, a field embedding preserves the algebraic structure of plus and times, an embedding of a topological space preserves open sets, and a graph embedding preserves connectivity. One space X is embedded in another space Y when the properties of Y restricted to X are the same as the properties of X. wolfram_math _2022 emulation The use of a data processing system to imitate another data processing system, so that the imitating system accepts the same data, executes the same programs, and achieves the same results as the imitated system. IEEE_Soft_Vo cab end event An activity, task, or output that describes or defines the conclusion of a process. IEEE_Guide_I PA engineer n. 3a: a designer or builder of engines; b: a person who is trained in or follows as a profession a branch of engineering; c: a person who carries through an enterprise by skillful or artful contrivance; 4Ê¼ a person who runs or supervises an engine or an apparatus. v. 1Ê¼ to lay out, construct, or manage as an engineer. \n\nMerriam-Webster_engi neer Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nensemble a machine learning paradigm where multiple models (often called â€œweak learnersâ€) are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models. Joseph_Rocca _Ensemble_m ethods environment Anything affecting a subject system or affected by a subject system through interactions with it, or anything sharing an interpretation of interactions with a subject system IEEE_Soft_Vo cab equality of odds (Equalized odds). We say that a predictor bY satisfies equalized odds with respect to protected attribute A and outcome Y, if bY and A are independent conditional on Y. hardt_equality _2016 The probability of a person in the positive class being correctly assigned a positive outcome and the probability of a person in a negative class being incorrectly assigned a positive outcome should both be the same for the protected and unprotected group members. In other words, the protected and unprotected groups should have equal rates for true positives and false positives. Mehrabi, _Ninareh equality of opportunity (Equal opportunity). We say that a binary predictor bY satisfies equal opportunity with respect to A and Y if Pr{bY = 1 | A = 0; Y = 1} = Pr{bY = 1 | A = 1; Y = 1}. hardt_equality _2016 The probability of a person in positive class being assigned to a positive outcome should be equal for both protected and unprotected group members. In other words, the protected and unprotected groups should have equal true positive rates. Mehrabi, _Ninareh error The difference between the observed value of an index and its â€œtrueâ€ value. Errors maybe random or systematic. Random errors are generally referred to as â€œerrorsâ€. Systematic errors are called â€œbiasesâ€. OECD Difference between a computed, observed, or measured value or condition and the true, specified, or theoretically correct value or condition. IEEE_Soft_Vo cab measured quantity value minus a reference quantity value aime_measure ment_2022, citing ISO/IEC Guide 99 error propagation the way in which uncertainties in the variables affect the uncertainty in the calculated results. Dorf_2018 propgation of uncertainty; proprgation of error ethics definition 1a: \"a set of moral principles : a theory or system of moral values\"; definition 1b: \"the principles of conduct governing an individual or a group\"; definition 1c: \"a consciousness of moral importance\"; definition 1d: \"a guiding philosophy\"; definition 2Ê¼ \"a set of moral issues or aspects (such as rightness)\"; definition 3Ê¼ \"the discipline dealing with what is good and bad and with moral duty and obligation\" Merriam-Webster_ethic n. 1. the branch of philosophy that investigates both the content of moral judgments (i.e., what is right and what is wrong) and their nature (i.e., whether such judgments should be considered objective or subjective). The study of the first type of question is sometimes termed normative ethics and that of the second metaethics. Also called moral philosophy. 2. the principles of morally right conduct accepted by a person or a group or considered appropriate to a specific field. In psychological research, for example, proper ethics requires that participants be treated fairly and without harm and that investigators report results and findings honestly. See code of ethics; professional ethics; research ethics. â€”ethical adj. APA_ethics ethics by design An approach to technology ethics and a key component of responsible innovation that aims to integrate ethics in the design and development stage of the technology. Sometimes formulated as \"embedding values in design.\" Similar terms are \"value-sensitive design\" and \"ethically aligned design.\" AI_Ethics_Mar k_Coeckelberg hevaluation (1) systematic determination of the extent to which an entity meets its specified criteria; (2) action that assesses the value of something aime_measrue ment_2022, citing ISO/IEC 24765 Test, Evaluation, Verification and Validation (TEVV) example definition 1Ê¼ \"one that serves as a pattern to be imitated or not to be imitated\"; definition 3Ê¼ \"one that is representative of all of a group or type\"; definition 4Ê¼ \"a parallel or closely similar case especially when serving as a precedent or model\"; definition 5Ê¼ \"an instance (such as a problem to be solved) serving to illustrate a rule or precept or to act as an exercise in the application of a rule\" Merriam-Webster_exam ple exception An event that occurs during the performance of the process that causes a diversion from the normal flow of the process. Exceptions are generated by an unanticipated event within a process due to an undefined or unknown input, undefined or unexpected outcome, or unforeseen sequencing of a task or event. IEEE_Guide_I PA execute To carry out a plan, a task command, or another instruction SP1011 To carry out an instruction, process, or computer program; directing, managing, performing, and accomplishing the project work, providing the deliverables, and providing work performance information. IEEE_Soft_Vo cab experiment a series of observations conducted under controlled conditions to study a relationship with the purpose of drawing causal inferences about that relationship. An experiment involves the manipulation of an independent variable, the measurement of a dependent variable, and the exposure of various participants to one or more of the conditions being studied. Random selection of participants and their random assignment to conditions also are necessary in experiments. apa_experime nt_2023 A study of a fundamental physical process by the use of one or more computer simulators. Like empirical experiments, input variables (factors) are systematically changed to assess their impact upon simulator outputs (responses). Unlike empirical experiments, the simulator responses are deterministic, and this has implications: Computer experiments can appropriately have their factors with intermediate levels and the scope, especially the number of runs, can be more ambitious. Further, modeling methods based on interpolators (especially kriging) emerge as a viable approach. Good practice is to use Latin hypercubes for computer experiments, and advanced nonparametric modeling methods such as kriging, neural networks, and multivariate adaptive regression splines (MARS) in the data analysis stage. Important applications of computer experimental methods are for determining process optima and for evaluating process tolerances. nist_statistics _2012 expert system A form of AI that attempts to replicate a human's expertise in an area, such as medical diagnosis. It combines a knowledge base with a set of hand-coded rules for applying that knowledge. Machine-learning techniques are increasingly replacing hand coding. Hutson, _Matthew Intelligent computer program that uses knowledge and inference procedures to solve problems that are difficult enough to require significant human expertise for their solution. Reznik,_Leon An expert system is an intelligent computer program that uses knowledge and inference procedures to solve problems that are difficult enough to require significant human expertise for their solution. OECD Computer system that provides for expertly solving problems in a given field or application area by drawing inferences from a knowledge base developed from human expertise. IEEE_Soft_Vo cab A computer system emulating the decision-making ability of a human expert through the use of reasoning, leveraging an encoding of domain-specific knowledge most commonly represented by sets of if-then rules rather than procedural code. The term â€œexpert systemâ€ was used largely during the 1970s and â€™80s amidst great enthusiasm about the power and promise of rule-based systems that relied on a â€œknowledge baseâ€ of domain-specific rules and rule-chaining procedures that map observations to conclusions or recommendations. NSCAI expertise The accumulation of specialized knowledge is often called expertise . Passive \n\nexpertise is a type of knowledge-based specialization that arises from experiences in life and one's position in a society or culture. Formal expertise is the result of a self-selection of a domain of knowledge that is mastered deliberately and for which there are clear benchmarks of success. Schneider_Mc Grew_in_Flan agan_McDono ugh_2018 explainability The ability to provide a human interpretable explanation for a machine learning prediction and produce insights about the causes of decisions, potentially to line up with human reasoning. NISTIR_8269_ Draft Within the context of AI, the extent to which AI decisioning processes and outcomes are reasonably understood. Comptroller_O ffice A characteristic of an AI system in which there is provision of accompanying evidence or reasons for system output in a manner that is meaningful or understandable to individual users (as well as to developers and auditors) and reflects the systemâ€™s process for generating the output (e.g., what alternatives were considered, but not proposed, and why not). NSCAI interpretability explainer Functionality for providing details on or causes for fairness metric results. AI_Fairness_3 60 explanation Systems deliver accompanying evidence or reason(s) for all outputs. NISTIR_8269_ Draft The explanation principle obligates AI systems to supply evidence, support, or reasoning for each output. NISTIR_8312 exploratory Exploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to 1. maximize insight into a data set; 2. uncover underlying structure; 3. extract important variables; 4. detect outliers and anomalies; 5. test underlying assumptions; 6. develop parsimonious models; and 7. determine optimal factor settings. nist_statistics _2012 external validity the extent to which the results of research or testing can be generalized beyond the sample that generated them. The more specialized the sample, the less likely will it be that the results are highly generalizable to other individuals, situations, and time periods. APA_external_ validity facial recognition (FR) Face recognition algorithms, however, have no built-in notion of a particular person. They are not built to identify particular people; instead they include a face detector followed by a feature extraction algorithm that converts one or more images of a person into a vector of values that relate to the identity of the person. The extractor typically consists of a neural network that has been trained on ID-labeled images available to the developer. In operations, they act as generic extractors of identity-related information from photos of persons they have usually never seen before. Recognition proceeds as a differential operator: Algorithms compare two feature vectors and emit a similarity score. This is a vendor-defined numeric value expressing how similar the parent faces are. It is compared to a threshold value to decide whether two samples are from, or represent, the same person or not. Thus, recognition is mediated by persistent identity information stored in a feature vector (or â€œtemplateâ€). NISTIR_8280 fairness metric A quantification of unwanted bias in training data or models. AI_Fairness_3 60 A mathematical definition of â€œfairnessâ€ that is measurable. Some commonly used fairness metrics include: equalized odds predictive parity counterfactual fairness demographic parity Many fairness metrics are mutually exclusive; see incompatibility of fairness metrics. google_glossar y_2023 false negative An example in which the predictive model mistakenly classifies an item as in the negative class. NSCAI an outcome where the model incorrectly predicts the negative class. google_dev_cl assification-true-false-positive-negative A false negative is denying an applicant who should be approved Varshney, _Kush 1. An instance in which a security tool intended to detect a particular threat fails to do so. 2. Incorrectly classifying malicious activity as benign. CSRC_false_n egative Type II error (in statistics) Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nfalse positive An example in which the model mistakenly classifies an item as in the positive class NSCAI an outcome where the model incorrectly predicts the positive class. google_dev_cl assification-true-false-positive-negative A false positive is approving an applicant who should be denied Varshney, _Kush 1. An alert that incorrectly indicates that a vulnerability is present. 2. An alert that incorrectly indicates that malicious activity is occurring. 3. An instance in which a security tool incorrectly classifies benign content as malicious. 4. Incorrectly classifying benign activity as malicious. 5. An erroneous acceptance of the hypothesis that a statistically significant event has been observed. This is also referred to as a type 1 error. This is also referred to as a type 1 error. When â€œhealth-testingâ€ the components of a device, it often refers to a declaration that a component has malfunctioned â€“ based on some statistical test(s) â€“ despite the fact that the component was actually working correctly. CSRC_false_p ositive Type I error (in statistics) fault tolerance The ability of a system or component to continue normal operation despite the presence of hardware or software faults SP1011 favorable label A label whose value corresponds to an outcome that provides an advantage to the recipient. The opposite is an unfavorable label. AI_Fairness_3 60 feature An attribute containing information for predicting the label. AI_Fairness_3 60 feature extraction a more general method in which one tries to develop a transformation of the input space onto the lowdimensional subspace that preserves most of the relevant information khalid_feature _2014 feature importance how important the feature was for the classification performance of the model; a measure of the individual contribution of the corresponding feature for a particular classifier, regardless of the shape (e.g., linear or nonlinear relationship) or direction of the feature effect saarela_featur e_2021 feature shift Unlike joint distribution shift detection, which cannot localize which features caused the shift, we define a new hypothesis test for each feature individually. NaÃ¯vely, the simplest test would be to check if the marginal distributions have changed for each feature (as explored by [25]); however, the marginal distribution would be easy for an adversary to simulate (e.g., by looping the sensor values from a previous day). Thus, marginal tests are not sufficient for our purpose. Therefore, we propose to use conditional distribution tests. More formally, our null and alternative hypothesis for the j-th feature is that its full conditional distribution (i.e., its distribution given all other features) has not shifted for all values of the other features. kulinski_featur e_2020 federated learning An approach to machine learning which addresses problems of data governance and privacy by training algorithms collaboratively without transferring the data to a central location. Each federated device trains on data locally and shares its local model parameters instead of sharing the training data. Different federated learning systems have different topologies that involve different ways of sharing parameters. TTC6_Taxono my_Terminolo gy feedback loop describes the process of leveraging the output of an AI system and corresponding end-user actions in order to retrain and improve models over time. The AI-generated output (predictions or recommendations) are compared against the final decision (for example, to perform work or not) and provides feedback to the model, allowing it to learn from its mistakes. C3. ai_feedback_l oop closed-loop learning fitting Fitting is the process of verifying whether the data item value is in the previously specified interval. OECD firmware Computer programs and data stored in hardware - typically in read-only memory (ROM) or programmable read-only memory (PROM) - such that the programs and data cannot be dynamically written or modified during execution of the programs. SP800-37 Combination of a hardware device and computer instructions or computer data that reside as read only software on the hardware device. IEEE_Soft_Vo cab Forecasting Estimate or prediction of conditions and events in the project's future based on information and knowledge available at the time of the forecast. The information is based on the project's past performance and expected future performance, and includes information that could impact the project in the future, such as estimate at completion and estimate to complete. IEEE_Soft_Vo cab fraud detection Monitoring the behavior of populations of users in order to estimate, detect, or avoid undesirable behavior. Kou,_Yufeng detecting and recognizing fraudulent activities as they enter systems and report them to a system manager. Behdad fully autonomous Accomplishes its assigned mission, within a defined scope, without human intervention while adapting to operational and environmental conditions SP1011 generative adversarial network (GAN) Generative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks. Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset. Brownlee, _Jason A pair of jointly trained neural networks that generates realistic new data and improves through competition. One net creates new examples (fake Picassos, say) as the other tries to detect the fakes. Hutson, _Matthew Generative adversarial networks (GANs) consist of two competing neural networksâ€”a generator network that tries to create fake outputs (such as pictures), and a discriminator network that tries to determine whether the outputs are real or fake. A major advantage of this structure is that GANs can learn from less data than other deep learning algorithms. CRS_AI An approach to training AI models useful for applications like data synthesis, augmentation, and compression where two neural networks are trained in tandem: one is designed to be a generative network (the forger) and the other a discriminative network (the forgery detector). The objective is for each network to train and better itself off the other, reducing the need for big labeled training data. NSCAI global A global explanation produces a model that approximates the non-interpretable model. NISTIR_8312_ Full governance The actions to ensure stakeholder needs, conditions, and options are evaluated to determine balanced, agreed-upon enterprise objectives; setting direction through prioritization and decision-making; and monitoring performance and ompliance against agreed-upon directions and objectives. AI governance may include policies on the nature of AI applications developed and deployed versus those limited or withheld. NSCAI A system of laws, policies, frameworks, practices and processes at international, national and organizational levels. AI governance helps various stakeholders implement, manage, oversee and regulate the development, deployment and use of AI technology. It also helps manage associated risks to ensure AI aligns with stakeholders' objectives, is developed and used responsibly and ethically, and complies with applicable legal and regulatory requirements. IAPP_Governa nce_Terms graph Diagram that represents the variation of a variable in comparison with that of one or more other variables. Diagram or other representation consisting of a finite set of nodes and internode connections called edges or arcs. IEEE_Soft_Vo cab A graph (sometimes called an undirected graph to distinguish it from a directed graph, or a simple graph to distinguish it from a multigraph) is a pair G = (V, E), where V is a set whose elements are called vertices (singular: vertex), and E is a set of paired vertices, whose elements are called edges (sometimes links or lines). wikipedia_gra ph_2023 graphical processing unit (GPU) A specialized chip capable of highly parallel processing. GPUs are well-suited for running machine learning and deep learning algorithms. GPUs were first developed for efficient parallel processing of arrays of values used in computer graphics. Modern-day GPUs are designed to be optimized for machine learning. NSCAI ground truth information provided by direct observation as opposed to information provided by inference Collins_Dictio nary_ground_ truth value of the target variable for a particular item of labelledinput data aime_measure ment_2022, citing ISO/IEC 22989 group fairness The goal of groups defined by protected attributes receiving similar treatments or outcomes. AI_Fairness_3 60 hacker Unauthorized user who attempts to or gains access to an information system. Reznik,_Leon Technically sophisticated computer enthusiast who uses his or her knowledge and means to gain unauthorized access to protected resources. IEEE_Soft_Vo cab hardware Physical equipment used to process, store, or transmit computer programs or data IEEE_Soft_Vo cab harm An undesired outcome [whose] cost exceeds some threshold[; ...] the key points in the definition of safety are that: costs have to be sufficiently high in some human sense for events to be harmful, and that safety involves reducing both the probability of expected harms and the possibility of unexpected harms. Engineering_s afety_in_mach ine_learning harmful bias Harmful bias can be either conscious or unconscious. Unconscious, also known as implicit bias, involves associations outside conscious awareness that lead to a negative evaluation of a person on the basis of characteristics such as race, gender, sexual orientation, or physical ability. Discrimination is behavior; discriminatory actions perpetrated by individuals or institutions refer to inequitable treatment of members of certain social groups that results in social advantages or disadvantages humphrey_add ressing_2020 human-assisted The type of human-robot-interaction that that refers to situations during which human interactions are needed at the level of detail of task plans, i.e., during the execution of a task SP1011 human-computer interaction (HCI) methods and approaches for designing and architecting user interfaces and the interactions between humans and computer (or information) technology. Poore_Lawren ce_ARLIS_202 3-01 human-cognitive bias Human-cognitive biases relate to how an individual or group perceives AI system information to make a decision or fill in missing information, or how humans think about purposes and functions of an AI system. Human biases are omnipresent in decision-making processes across the AI lifecycle and system use, including the design, implementation, operation, and maintenance of AI. NIST_AI_RMF _1.0 Systematic error in judgment and decision-making common to all human beings which can be due to cognitive limitations, motivational factors, and/or adaptations to natural environments. human-enabled machine learning Detection, correlation, and pattern recognition generated through machine-based observation of human operation of software systems capturing successful or unsuccessful operations to enable the creation of a useful predictive analytics capability. IEEE_Guide_I PA human-in-the-loop An AI system that requires human interaction. DOD_Modelin g_and_Simula tion_Glossary human-machine teaming (HMT) The ability of humans and AI systems to work together to undertake complex, evolving tasks in a variety of environments with seamless handoff both ways between human and AI team members. Areas of effort include developing effective policies for controlling human and machine initiatives, computing methods that ideally complement people, methods that optimize goals of teamwork, and designs that enhance human-AI interaction. NSCAI methods and approaches for coordinating the functions and actions of (semi) autonomous machine capabilities and human users, which are granted equal weighting. Poore_Lawren ce_ARLIS_202 3-01 human-AI teaming human-operator-intervention The need for human interaction in a normally fully autonomous behavior due to some extenuating circumstances. SP1011 Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nhuman subjects a living individual about whom an investigator (whether professional or student) conducting research: (i) Obtains information or biospecimens through intervention or interaction with the individual, and uses, studies, or analyzes the information or biospecimens; or (ii) Obtains, uses, studies, analyzes, or generates identifiable private information or identifiable biospecimens. 45_CFR_46_2 018_Requirem ents_ (2018_Commo n_Rule) participant human system integration (HSI) methods and approaches for testing and optimizing all human-related considerations from a â€œwhole-systemâ€ or â€œsystem-of-systemsâ€ level. Poore_Lawren ce_ARLIS_202 3-01 hyperparameters the parameters that are used to either configure a ML model (e.g., the penalty parameter C in a support vector machine, and the learning rate to train a neural network) or to specify the algorithm used to minimize the loss function (e.g., the activation function and optimizer types in a neural network, and the kernel type in a support vector machine). On_Hyperpara meter_Optimi zation hypothesis testing A term used generally to refer to testing significance when specific alternatives to the null hypothesis are considered. OECD impact assessment a risk management tool that seeks to ensure an organization has sufficiently considered a system's relative benefits and costs before implementation. In the context of AI, an impact assessment helps to answer a simple question: alongside this systemâ€™s intended use, for whom could it fail? Bipartisan_Poli cy_Center_im pact_assessme nts An evaluation process designed to identify, understand, document and mitigate the potential ethical, legal, economic and societal implications of an AI system in a specific use case. IAPP_Governa nce_Terms impersonation A malicious individual is able to impersonate a legitimate data subject to the data controller. The adversary forges a valid access request and goes through the identity verification enforced by the data controller. The data controller sends to the adversary the data of a legitimate data subject. Defeating impersonation is the primary objective of any authentication protocol. The result of this attack is a data breach (e.g. blaggers [sic] pretend to be someone they are not in order to wheedle out the information they are seeking obtaining information illegaly which they then sell for a specified price). Security_Analy sis_of_Subject _Access in-processing Techniques that modify the algorithms in order to mitigate bias during model training. Model training processes could incorporate changes to the objective (cost) function or impose a new optimization constraint. SP1270 Techniques that try to modify and change state-of-the-art learning algorithms to remove discrimination during the model training process. Mehrabi, _Ninareh in-processing algorithm A bias mitigation algorithm that is applied to a model during its training. AI_Fairness_3 60 incident a situation in which AI systems caused, or nearly caused, real-world harm. AI_Incident_D atabase the occurrence of a technical event that affects the integrity of a Product and/or Model. FBPML_Wiki an alleged harm or near harm event to people, property, or the environment where an AI system is implicated. AI_Incident_E ditors Adverse event(s) in a computer system or networks caused by a failure of a security mechanism, or an attempted or threatened breach of these mechanisms. Hasan,_Raza incident response a public official response to an incident ... from an entity (i.e. company, organization, individual) allegedly responsible for developing or deploying the AI or AI system involved in said incident. AIID_incident _response independence Of software quality assurance (SQA), situation in which SQA is free from technical, managerial, and financial influences, intentional or unintentional IEEE_Soft_Vo cab Two events are independent if the occurrence of one event does not affect the chances of the occurrence of the other event. The mathematical formulation of the independence of events A and B is the probability of the occurrence of both A and B being equal to the product of the probabilities of A and B (i.e., P(A and B) = P(A)P(B)) nist_800_2010 In simple terms, inclusion is getting the mix to work together. individual fairness The goal of similar individuals receiving similar treatments or outcomes. AI_Fairness_3 60 Give similar predictions to similar individuals Mehrabi, _Ninareh A fairness metricthat checks whether similar individuals are classified similarly aime_measure ment_2022 citing Machine Learning Glossary by Google inference The stage of ML in which a model is applied to a task. For example, a classifier model produces the classification of a test sample. NISTIR_8269_ Draft information input component One of the three components of a model. This component delivers assumptions and data to the model. Comptroller_O ffice information security preservation of confidentiality, integrity and availability of information; in addition, other properties, such as authenticity, accountability, non-repudiation, and reliability can also be involved. ISO/IEC_TS_ 5723Ê¼2022(en) input Data received from an external source IEEE_Soft_Vo cab insider attack Those who are within [an] organisation may have authorised access to vast amounts of sensitive company records that are essential for maintaining competitiveness and market position, and knowledge of information services and procedures that are crucial for daily operations. . . .[and] should an individual choose to act against the organisation, then with their privileged access and their extensive knowledge, they are well positioned to cause serious damage. IEEE_Caught_ in_the_Act in silico carrying out some experiment by means of a computer simulation World_Wide_ Words_In_sili co computer simulation testing instance Discrete, bounded thing with an intrinsic, immutable, and unique identity. Individual occurrence of a type IEEE_Soft_Vo cab A single object of the world from which a model will be learned, or on which a model will be used (e.g., for prediction). Kohavi,_Ron instance weight A numerical value that multiplies the contribution of a data point in a model. AI_Fairness_3 60 integrity Degree to which a system, product, or component prevents unauthorized access to, or modification of, computer programs or data. IEEE_Soft_Vo cab Guarding against improper information modification or destruction, and includes ensuring information non-repudiation and authenticity. CSRC The property whereby information, an information system, or a component of a system has not been modified or destroyed in an unauthorized manner. CISA <data> property whereby data have not been altered in an unauthorized manner since they were created, transmitted, or stored; <systems> property of accuracy and completeness ISO/IEC_TS_ 5723Ê¼2022(en) the quality of moral consistency, honesty, and truthfulness with oneself and others. APA_integrity intelligent process automation A preconfigured software instance that combines business rules, experience-based context determination logic, and decision criteria to initiate and execute multiple interrelated human and automated processes in a dynamic context. The goal is to complete the execution of a combination of processes, activities, and tasks in one or more unrelated software systems that deliver a result or service with minimal or no human intervention. IEEE_Guide_I PA interaction Action that takes place with the participation of the environment of the object. IEEE_Soft_Vo cab internal validity the degree to which a study or experiment is free from flaws in its internal structure and its results can therefore be taken to represent the true nature of the phenomenon. In other words, internal validity pertains to the soundness of results obtained within the controlled conditions of a particular study, specifically with respect to whether one can draw reasonable conclusions about cause-and-effect relationships among variables. APA_internal_ validity interoperability The ability of software or hardware systems or components to operate together successfully with minimal effort by end user SP1011 Degree to which two or more systems, products or components can exchange information and use the information that has been exchanged. IEEE_Soft_Vo cab The ability for tools to work together in execution, communication, and data exchange under specific conditions. NIST_1500 interpretability The ability to understand the value and accuracy of system output. Interpretability refers to the extent to which a cause and effect can be observed within a system or to which what is going to happen given a change in input or algorithmic parameters can be predicted. NSCAI The ability to explain or to present an ML modelâ€™s reasoning in understandable terms to a human aime_measure ment_2022, citing Machine Learning Glossary by Google explainability interpretable model An interpretable machine learning model obeys a domain-specific set of constraints to allow it (or its predictions, or the data) to be more easily understood by humans. These constraints can differ dramatically depending on the domain. rudin_interpre table_2022 intervenability the property that intervention is possible concerning all ongoing or planned privacy relevant data processing[; ...] the data subjects themselves should be able to intervene with regards to the processing of their own data ... [to ensure] that data subjects have the ability to control how their data is processed and by whom. Covert_et_al knowledge The sum of all information derived from diagnostic, descriptive, predictive, and prescriptive analytics embedded in or available to or from a cognitive computing system. IEEE_Guide_I PA <artificial intelligence> abstracted informationabout objects, events, concepts or rules, their relationships and properties, organizedfor goal-oriented systematic use aime_measure ment_2022, citinig ISO/IEC 22989 label A value corresponding to an outcome. AI_Fairness_3 60 target variable assigned to a sample aime_measure ment_2022, citing ISO/IEC 22989 label shift Under label shift, the label distribution p(y) might change but the class-conditional distributions p(x|y) do not. ... We work with the label shift assumption, i.e., ps(x|y) = pt(x|y) saurabh_label _2020 large language model (LLM) a class of language models that use deep-learning algorithms and are trained on extremely large textual datasets that can be multiple terabytes in size. LLMs can be classed into two types: generative or discriminatory. Generative LLMs are models that output text, such as the answer to a question or even writing an essay on a specific topic. They are typically unsupervised or semi-supervised learning models that predict what the response is for a given task. Discriminatory LLMs are supervised learning models that usually focus on classifying text, such as determining whether a text was made by a human or AI. AI_Assurance_ 2022 language model language model A language model is an approximative description that captures patterns and regularities present in natural language and is used for making assumptions on previously unseen language fragments. Gustavii,_Ebba large language model (LLM) learning A procedure in artificial intelligence by which an artificial intelligence program improves its performance by gaining knowledge. Dennis_Merca dal the acquisition of novel information, behaviors, or abilities after practice, observation, or other experiences, as evidenced by change in behavior, knowledge, or brain function. Learning involves consciously or nonconsciously attending to relevant aspects of incoming information, mentally organizing the information into a coherent cognitive representation, and integrating it with relevant existing knowledge activated from long-term memory. APA_learning Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nleast privilege The principle that a security architecture should be designed so that each entity is granted the minimum system resources and authorizations that the entity needs to perform its function. CSRC The security objective of granting users only those accesses they need to perform their official duties. SP-800-12 lemmatization the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Artasanchez_J oshi_AI_with_ Python in natural language processing[, ...] working with words according to their root lexical components Techopedia_le mmatization grouping together words with the same root or lemma but with different inflections or derivatives of meaning so they can be analyzed as one item. Techslang_lem matization linear model [a supervised learning algorithm that uses] a simple formula to find a best-fit line through a set of data points. dataiku_ML_a nd_linear_mo dels (linear) An operator L^~ is said to be linear if, for every pair of functions f and g and scalar t, L^~(f+g)=L^~f+L^~g and L^~(tf)=tL^~f. wolfram_math world_2022 local A local explanation explains a subset of decisions or is a per-decision explanation. NISTIR_8312_ Full localization Creation of a national or specific regional version of a product. IEEE_Soft_Vo cab logistic model (logistic equation) The continuous version of the logistic model is described by the differential equation (dN)/(dt)=(rN(K-N))/K, (1) where r is the Malthusian parameter (rate of maximum population growth) and K is the so-called carrying capacity (i.e., the maximum sustainable population). Dividing both sides by K and defining x=N/K then gives the differential equation (dx)/(dt)=rx(1-x), (2) which is known as the logistic equation and has solution x(t)=1/(1+(1/(x_0)-1)e^(-rt)). (3) The function x(t) is sometimes known as the sigmoid function. wolfram_math world_2022 machine learning A branch of Artificial Intelligence (AI) that focuses on the development of systems capable of learning from data to perform a task without being explicitly programmed to perform that task. Learning refers to the process of optimizing model parameters through computational techniques such that the model's behaviour is optimized for the training task. TTC6_Taxono my_Terminolo gy A subcategory of artificial intelligence; a method of designing a sequence of actions to solve a problem that optimizes automatically through experience and with limited or no human intervention. Comptroller_O ffice machine observation Machine detection and interpretation of relevant and meaningful events and conditions that impact operation of the computer system itself or other dependent mechanisms or processes essential to the purpose of the system. IEEE_Guide_I PA malware Hardware, firmware, or software that is intentionally included or inserted in a system for a harmful purpose. Reznik,_Leon Software that compromises the operation of a system by performing an unauthorized function or process. CISA trojan horse materiality Refers to the significance of a matter in relation to a set of financial or performance information. If a matter is material to the set of information, then it is likely to be of significance to a user of that information OECD McNamara fallacy presum[ing] that (A) quantitative models of reality are always more accurate than other models; (B) the quantitative measurements that can be made most easily must be the most relevant; and (C) factors other than those currently being used in quantitative metrics must either not exist or not have a significant influence on success. Also known as the quantitative fallacy. \n\nMcNamara_Fal lacy quantitative fallacy measurement (Quantitative) (1) act or process of assigning a number or category to an entity to describe an attribute of that entity; (2) assignment of numbers to objects in a systematic way to represent properties of the object; (3) use of a metric to assign a value (e.g., a number or category) from a scale to an attribute of an entity; (4) set of operations having the object of determining a value of a measure; (5) assignment of values and labels to aspects of software engineering work products, processes, and resources plus the models that are derived from them, whether these models are developed using statistical or other techniques; (6) figure, extent, or amount obtained by measuring aime_measure ment_2022, citing ISO/IEC 24765 (Qualitative) (1) a way of learning about social reality [...][that uses] approaches [...] to explore, describe, or explain social phenomen[a]; unpack the meaning people ascribe to activities, situations, events, or [artifacts]; build a depth of understanding about some aspect of social life; build \"thick descriptions\" (see Clifford Geertz, 1973) of people in naturalistic settings; explore new or underresearched areas; or make micro-macro links (illuminate connections between individuals-groups and institutional and/or cultural contexts). (2) [approaches that] can make visible and unpick the mechanisms which link particular variables, by looking at the explanations, or accounts, provided by those involved. Leavy_OHQR_ Intro Qualitative measurement engages research methods and techniques to provide information about the nature of phenomenon. Qualitative methods are designed for systematic collection, organization, description and interpretation of non-numeric (textual, verbal or visual) data (Hammarberg et. al, 2016). Qualitative measurement generally answers questions about why, for whom, when, and how something is (or is not) observed, whereas quantitative measurement answers questions about what is observed. Elements assessed using qualitative measurement may include contextual norms or meaning, socio-cultural dynamics, individual or collective beliefs, and complex multi-component interactions or interventions (Busetto et. al, 2020). Hammarberg_ 2016_Busetto_ 2020 Documentation of assumptions and methods used is a foundational element of qualitative measurement, as the choice of single or combined methods is made based on the phenomenon and its context (Russell & Gregory, 2003). When appropriately paired, qualitative and quantitative measurement can provide corroboration or elaboration, demonstrate use cases, and/or identify conditions for complementarity or contradiction (Brannen, 2005). Russell_2003_ Brannen_2005 measurement method generic description of a logical organization of operations used in a measurement aime_measure ment_2022, citing ISO/IEC Guide 99 logical sequence of operations, described generically, usedin quantifying an attribute with respect to a specified scale aime_measure ment_2022, citing ISO/IEC 24765 measurement model The initial confirmatory factory analysis (CFA) model that underlies the structural model [that] tests the adequacy (as indexed by model fit) of the specified relations whereby indicators are linked to their underlying construct. Little_2013 A statistical model that links unobservable theoretical constructs, operationalized as latent variables, and observable propertiesâ€”i.e., data about the world jackman_oxfor d_2008 measurability ability to assess an attribute of an entity against a metric (note 1Ê¼ \"measurable\" is the adjective form of \"measurability\") ISO/IEC_TS_ 5723Ê¼2022(en) membership inference given a machine learning model and a record, determining whether the record was used as part of the model's training dataset or not. metadata Metadata is data that defines and describes other data. OECD Data that describe other data. IEEE_Soft_Vo cab Data employed to annotate other data with descriptive information, possibly including their data descriptions, data about data ownership, access paths, access rights, and data volatility. metric defined measurement method and measurement scale ISO/IEC_TS_ 5723Ê¼2022(en) (1) quantitative measure of the degree to which a system, component, or process possesses a given attribute; (2) defined measurement method and the measurement scale; c.f., measure in this section above aime_measure ment_2022, citing ISO/IEC 24765 minimization (Part of the ICO framework for auditing AI) AI systems generally require large amounts of data. However, organisations must comply with the minimisation principle under data protection law if using personal data. This means ensuring that any personal data is adequate, relevant and limited to what is necessary for the purposes for which it is processed. [â€¦] The default approach of data scientists in designing and building AI systems will not necessarily take into account any data minimisation constraints. Organisations must therefore have in place risk management practices to ensure that data minimisation requirements, and all relevant minimisation techniques, are fully considered from the design phase, or, if AI systems are bought or operated by third parties, as part of the procurement process due diligence ICO_data_min imisation a data controller should limit the collection of personal information to what is directly relevant and necessary to accomplish a specified purpose. They should also retain the data only for as long as is necessary to fulfil that purpose. In other words, data controllers should collect only the personal data they really need, and should keep it only for as long as they need it. The data minimisation principle is expressed in Article 5(1)(c) of the GDPR and Article 4(1)(c) of Regulation (EU) 2018/1725, which provide that personal data must be \"adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed\". EDPS_data_mi nimization mixed methods In mixed methods, the researcher collects and analyzes both qualitative and quantitative data rigorously in response to research questions and hypotheses; integrates the two forms of data and their results; organizes these procedures into specific research designs that provide the logic and procedures for conducting the study; and frames these procedures within theory and philosophy. Creswell_Clark _mixed_meth ods research in which the inquirer or investigator collects and analyzes data, integrates the findings, and draws inferences using both qualitative and quantitative approaches or methods in a single study or a program of study. Lisa_M. _Given_SAGE MLOPS MLOps (machine learning operations) stands for the collection of techniques and tools for the deployment of ML models in production. symeonidis_M LOps_2022 model A function that takes features as input and predicts labels as output. AI_Fairness_3 60 A model is a formalised expression of a theory or the causal situation which is regarded as having generated observed data. In statistical analysis the model is generally expressed in symbols, that is to say in a mathematical form, but diagrammatic models are also found. The word has recently become very popular and possibly somewhat over-worked. OECD A core component of an AI system used to make inferences from inputs in order to produce outputs. A model characterizes an input-to-output transformation intended to perform a core computational task of the AI system (e.g., classifying an image, predicting the next word for a sequence, or selecting a robot's next action given its state and goals). TTC6_Taxono my_Terminolo gy A quantitative method, system, or approach that applies statistical, economic, financial, or mathematical theories, techniques, and assumptions to process input data into quantitative estimates. A model consists of three components: an information input component, which delivers assumptions and data to the model; a processing component, which transforms inputs into estimates; and a reporting component, which translates the estimates into useful business information. Comptroller_O ffice model assertion Model assertions are arbitrary functions over a modelâ€™s input and output that indicate when errors may be occurring Kang,_Daniel model card short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. [They] also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. Model_Cards_ for_Model_Re porting A brief document that discloses information about an AI model, like explanations about intended use, performance metrics and benchmarked evaluation in various conditions, such as across different cultures, demographics or race. IAPP_Governa nce_Terms model debugging Model debugging aims to diagnose a modelâ€™s failures. Jain_Saachi model decay Model decay depicts that the performance of the model is degrading over time Nayak,_Pragati model editing An area of research that aims to enable fast, data-efficient updates to a pre-trained base modelâ€™s behavior for only a small region of the domain, without damaging model performance on other inputs of interest Mitchell,_Eric model extraction Adversaries maliciously exploiting the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface. Chandrasekara n,_Varun model inversion; model stealing model governance Model Governance is the name for the overall internal framework of a firm or organization that controls the processes for Model Development, Model Validation and Model Usage, assign responsibilities and roles etc. open_risk_202 2model inventory in the context of Risk Management, [...] a database/[management information system] developed for the purpose of aggregating quantitative model related information that is in use by a firm or organization. ORM_model_i nventory model overlay Judgmental or qualitative adjustments to model inputs or outputs to compensate for model, data, or other known limitations. A model overlay is a type of override. Comptroller_O ffice model risk management model risk management encompasses governance and control mechanisms such as board and senior management oversight, policies and procedures, controls and compliance, and an appropriate incentive and organizational structure Fed_Reserve model suite A group of models that work together. Comptroller_O ffice Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nmodel training the phase in the data science development lifecycle where practitioners try to fit the best combination of weights and bias to a machine learning algorithm to minimize a loss function over the prediction range C3. ai_Model_Trai ning process to determine or to improve the parameters of a machine learning model, based on a machine learning algorithm, by using training data aime_measure ment_2022, citing ISO/IEC 22989 model validation the set of processes and activities intended to verify that models are performing as expected. yields. io_model_vali dation the set of principles, practices and organizational arrangements supporting a rigorous (audited) model development and validation cycle. Open_Risk_M anual_model_ validation monitoring Examination of the status of the activities of a supplier and of their results by the acquirer or a third party. IEEE_Soft_Vo cab Continual checking, supervising, critically observing or determining the status in order to identify change from the performance level required or expected. SP800-160 moral agency The capacity for moral action, reasoning, judgment, and decision making, as opposed to merely having moral consequences. AI_Ethics_Mar k_Coeckelberg hmoral patiency The moral standing of an entity in the sense of how that entity should be treated. AI_Ethics_Mar k_Coeckelberg hnaive Bayes The naive Bayes classifier is a Bayesian learning method that has been found to be useful in many practical applications. It is called \"naive\" because it incorporates the simplifying assumption that attribute values are conditionally independent, given the classification of the instance. The naive Bayes classifier applies to learning tasks where each instance x is described by a conjunction of attribute values and where the target function f(x) can take on any value from some finite set V. Mitchell,_Tom natural language processing The field concerned with machines capable of processing, analysing, and generating human language, either spoken, written or signed. TTC6_Taxono my_Terminolo gy neural network A model that, taking inspiration from the brain, is composed of layers (at least one of which is hidden) consisting of simple connected units or neurons followed by nonlinearities aime_measure ment_2022, citing Machine Learnign Glossary by Google nondiscrimination the practice of treating people, companies, countries, etc. in the same way as others in order to be fair: Cambridge Dictionary In the context of machine learning non-discrimination can be defined as follows: (1) people that are similar in terms non-protected characteristics should receive similar predictions, and (2) differences in predictions across groups of people can only be as large as justified by non-protected characteristics. Å½liobaitÄ—_Indr Ä—the practice of treating people, companies, countries, etc. in the same way as others in order to be fair Cambridge_Di ctionary_non-discrimination normal flow The intended flow of a process originating from a start event, continuing through all defined activities, and concluding successfully to its defined end event. IEEE_Guide_I PA normalization Conceptual procedure in database design that removes redundancy in a complex database by establishing dependencies and relationships between database entities. Normalization reduces storage requirements and avoids database inconsistencies. OECD The process of convertingan actual range of values into a standard range of values, typically âˆ’1to +1 or 0 to 1 aime_measure ment_2022, citing Machine Learning Glossary by Google objective evidence data supporting the existence or verity of something (note: can be obtained through observation, measurement, test, or other means). ISO/IEC_TS_ 5723Ê¼2022(en) observation a piece of information received online from users, sensors, or other knowledge sources poole_mackwo rth_observatio nthe careful, close examination of an object, process, or other phenomenon for the purpose of collecting data about it or drawing conclusions. APA_observati on offline learning implies ... a static dataset that [one] know[s] from the start and the parameters of [one's] machine learning algorithm are adjusted to the whole dataset at once often loading the whole dataset into memory or in batches. Ben_Auffarth_ 2021 online learning fitting [one's] model incrementally as the data flows in (streaming data). Ben_Auffarth_ 2021 ontology A set of concepts and categories in a subject area or knowledge domain that shows their properties and the relationships among them to enable interoperability among disparate elements and systems and specify interfaces to independent, knowledge-based services for the purpose of enabling certain kinds of automated reasoning. IEEE_Guide_I PA opacity The nature of some AI techniques whereby the inferential operations are complex, hidden, or otherwise opaque to their developers and end users in terms of providing an understanding of how classifications, recommendations, or actions are generated and what overall performance will be. NSCAI A description of some deep learning systems [that] take an input and provide an output, but the calculations that occur in between are not easy for humans to interpret. Hutson, _Matthew When one or more features of an AI system, such as processes, the provenance of datasets, functions, output or behaviour are unavailable or incomprehensible to all stakeholders â€“ usually an antonym for transparency. TTC6_Taxono my_Terminolo gy black box; unexplainable operationalization Putting AI systems or related concepts into use so they can be measured. operator A role assumed by the person performing remote control or teleoperation, semi-autonomous operations, or other human-in-the-loop types of operations SP1011 Individual or organization that performs the operations of a system. IEEE_Soft_Vo cab Individual or organization that performs the operations of a system. SP800-160 opt-in an individual makes an active affirmative indication of choice via a user interface signaling a desire to share their information with third parties. IAPP_Privacy_ Glossary privacy; consent; opt-out opt-out an individual makes an active affirmative indication of choice via a user interface signaling a desire not to share their information with third parties. IAPP_Privacy_ Glossary privacy; consent; opt-in outcome something that follows as a result or consequence merriam_webs ter_outcome outlier An outlier is a data point that is far from other points. Russell_and_N orvig An outlier is a data value that lies in the tail of the statistical distribution of a set of data values. OECD Values distant from mostother values. In machine learning, any of the following are outliers:â€¢ Weights with high absolute valuesâ€¢ Predicted values relatively far away from the actual valuesâ€¢ Input data whose values are more than roughly 3 standard deviations fromthe meanOutliers often cause problems in model training. Clipping is one way of managingoutliers aime_measure ment_2022 citing Machine Learning Glossary by Google output Data transmitted to an external destination IEEE_Soft_Vo cab Process by which an information processing system, or any of its parts, transfers data outside of that system or part IEEE_Soft_Vo cab overfitting Given a hypothesis space H, a hypothesis h element of H is said to overfit the training data if there exists some alternative hypothesis h' element of H, such that h has smaller error than h' over the training examples, but h' has a smaller error than h over the entire distribution of instance. Mitchell,_Tom package a folder with all the code and metadata needed to train and serve a machine learning model. about_ML_pa ckages parametric A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) Russell_and_N orvig parent process A process that may contain one or more sub-processes, activities, and tasks. IEEE_Guide_I PA parity Bit(s) used to determine whether a block of data has been altered. Rationale: Term has been replaced by the term â€œparity bitâ€. NIST_CSRC_p arity the quality or state of being equal or equivalent Merriam-Webster_parit yparticipation engag[ing] multiple stakeholders in deliberative processes in order to achieve consensus. Sloane_et_al_ 2020 participant A computer system, data, input, business rule, human intervention, and other contributor to the flow of a process. IEEE_Guide_I PA a living individual about whom an investigator (whether professional or student) conducting research: (i) Obtains information or biospecimens through intervention or interaction with the individual, and uses, studies, or analyzes the information or biospecimens; or (ii) Obtains, uses, studies, analyzes, or generates identifiable private information or identifiable biospecimens. 45_CFR_46_2 018_Requirem ents_ (2018_Commo n_Rule) human subject passive learning agent A passive learning agent has a fixed policy that determines its behavior. An active learning agent gets to decide what actions to take. Russell_and_N orvig active learning agent Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\npersonal data â€˜Personal dataâ€™ means any information relating to an identified or identifiable natural person (â€˜data subjectâ€™); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person. GDPR (1) â€œPersonal informationâ€ means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular consumer or household. Personal information includes, but is not limited to, the following if it identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular consumer or household: (A) Identifiers such as a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, social security number, driverâ€™s license number, passport number, or other similar identifiers. (B) Any personal information described in subdivision (e) of Section 1798.80. (C) Characteristics of protected classifications under California or federal law. (D) Commercial information, including records of personal property, products or services purchased, obtained, or considered, or other purchasing or consuming histories or tendencies. (E) Biometric information. (F) Internet or other electronic network activity information, including, but not limited to, browsing history, search history, and information regarding a consumerâ€™s interaction with an internet website application, or advertisement. (G) Geolocation data. (H) Audio, electronic, visual, thermal, olfactory, or similar information. (I) Professional or employment-related information. (J) Education information, defined as information that is not publicly available personally identifiable information as defined in the Family Educational Rights and Privacy Act (20 U.S.C. Sec. 1232g; 34 C.F.R. Part 99). (K) Inferences drawn from any of the information identified in this subdivision to create a profile about a consumer reflecting the consumerâ€™s preferences, characteristics, psychological trends, predispositions, behavior, attitudes, intelligence, abilities, and aptitudes. (L) Sensitive personal information. (2) â€œPersonal informationâ€ does not include publicly available information or lawfully obtained, truthful information that is a matter of public concern. For purposes of this paragraph, â€œpublicly availableâ€ means: information that is lawfully made available from federal, state, or local government records, or information that a business has a reasonable basis to believe is lawfully made available to the general public by the consumer or from widely distributed media; or information made available by a person to whom the consumer has disclosed the information if the consumer has not restricted the information to a specific audience. â€œPublicly availableâ€ does not mean biometric information collected by a business about a consumer without the consumerâ€™s knowledge. (3) â€œPersonal informationâ€ does not include consumer information that is deidentified or aggregate consumer information. CCPA policy The general principles by which a government is guided in its management of public affairs, or the legislature in its measures. This term, as applied to a law, ordinance, or rule of law, denotes its general purpose or tendency considered as directed to the POLICY law_policy_20 23 A policy defines the learning agentâ€™s way of behaving at a given time sutton_reinfor cement_2018 positionality the researcher's starting points and standpoints before and during inquiry, as well as the conditions shaping the research situation, process, and product. Charmaz_Hen wood reflexivity post-hoc explanation Post-hoc explainability targets models that are not readily interpretable by design by resorting to diverse means to enhance their interpretability, such as text explanations, visual explanations, local explanations, explanations by example, explanations by simplification and feature relevance explanations techniques. Each of these techniques covers one of the most common ways humans explain systems and processes by themselves. NISTIR_8312_ Full Post-hoc explainability targets models that are not readily inter- pretable by design by resorting to diverse means to enhance their in- terpretability, such as text explanations, visual explanations, local expla- nations, explanations by example, explanations by simplification and feature relevance explanations techniques. Each of these techniques covers one of the most common ways humans explain systems and processes by themselves. barredo_explai nable_2020 post-processing Typically performed with the help of a holdout dataset (data not used in the training of the model). Here, the learned model is treated as a black box and its predictions are altered by a function during the post-processing phase. The function is deduced from the performance of the black box model on the holdout dataset. SP1270 Performed after training by accessing a holdout set that was not involved during the training of the model. If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-processing can be used in which the labels assigned by the black-box model initially get reassigned based on a function during the post-processing phase. Mehrabi, _Ninareh Steps performed after a machine learning model has been run to adjust its output. This can include adjusting a model's outputs or using a holdout dataset â€” data not used in the training of the model â€” to create a function run on the model's predictions to improve fairness or meet business requirements. IAPP_Governa nce_Terms post-processing algorithm A bias mitigation algorithm that is applied to predicted labels. AI_Fairness_3 60 practical significance a conceptual framework for evaluating discrimination cases developed primarily on statistical evidence that is the subject of increasing interest and discussion by some in the equal employment opportunity (EEO) field. DOL_Practical _Significance statistical significance (often paired in contrast to this); substantive significance (synonym) pre-processing algorithm A bias mitigation algorithm that is applied to training data. AI_Fairness_3 60 precision A metric for classification models. Precision identifies the frequency with which a model was correct when classifying the positive class. NSCAI closeness of agreement between indications or measuredquantity values obtained by replicate measurements on the same or similarobjects under specified conditions aime_measure ment_2022, citing ISO/IEC Guide 99 A metric for classification models.Precision identifies the frequency with which a model was correct when predictingthe positive class. That is:Precision = True Positive/(True Positive + False Positive) aime_measure ment_2022, citing Machine Learning Glossary by Google Closeness of agreement between independent test results obtained under prescribed conditions. It is generally dependent on analyte concentration, and this dependence should be determined and documented. The measure of precision is usually expressed in terms of imprecision and computed as a standard deviation of the test results. Higher imprecision is reflected by a larger standard deviation. Independent test results means results obtained in a manner not influenced by any previous results on the same or similar material. Precision covers repeatability and reproducibility [19]. Alternatively, precision is a measure for the reproducibility of measurements within a set, that is, of the scatter or dispersion of a set about its central value. Precision depends only on the distribution of random errors and does not relate to the true value or specified value. UNODC_Gloss ary_QA_GLP prediction Forecasting quantitative or qualitative outputs through function approximation, applied on input data or measurements. NSCAI primary output of an AI system when provided with input data or information aime_measure ment_2022, citing ISO/IEC 22989 predictive analysis The organization of analyses of structured and unstructured data for inference and correlation that provides a useful predictive capability to new circumstances or data. IEEE_Guide_I PA predictive analytics Insights, reporting, and information answering the question, \"What is likely to happen?\" Predictive analytics support high confidence foretelling of future event (s). IEEE_Guide_I PA preprocessing Transforming the data so that the underlying discrimination is mitigated. This method can be used if a modeling pipeline is allowed to modify the training data. SP1270 prescriptive analytics Insights, reporting, and information answering the question, â€œWhat should I do about it?\" Prescriptive analytics determines information that provides high confidence actions necessary to recover from an event or fulfill a need. IEEE_Guide_I PA privacy freedom from intrusion into the private life or affairs of an individual ISO/IEC_TS_ 5723Ê¼2022(en) freedom from intrusion into the private lifeor affairs of an individual when that intrusion results from undue or illegalgathering and use of data about that individual aime_measure ment_2022, citing ISO/IEC TR 24029-1 privacy-by-design Embedding privacy measures and privacy enhancing technologies directly into the design of information technologies and systems. ENISA data-protection-by-design (def: https:/ /eur-lex.europa. eu/legal-content/EN/T XT/? uri=CELEX% 3A02016R0679 -20160504&qid= 1532348683434 )privacy-enhancing technology A coherent system of ICT (Information and Communications Technology) measures that protects privacy by eliminating or reducing personal data or by preventing unnecessary and/or undesired processing of personal data, all without losing the functionality of the information system. PET_Handboo kprivileged protected attribute A value of a protected attribute indicating a group that has historically been at systematic advantage. AI_Fairness_3 60 procedure Information item that presents an ordered series of steps to perform a process, activity, or task. IEEE_Soft_Vo cab process A sequence or flow of activities in an organization with the objective of carrying out work, which may include a set of activities, events, tasks, and decisions in a sequenced flow that adhere to finite execution semantics. Process levels will generally follow structure at the capability maturity model integration (CMMI) level. IEEE_Guide_I PA Set of interrelated or interacting activities that transforms inputs into outputs IEEE_Soft_Vo cab process flow The defined representation of the overall progression of how a process is intended to be performed, including all exceptions. IEEE_Guide_I PA Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nprocessing â€˜Processingâ€™ means any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction. GDPR â€œProcessingâ€ means any operation or set of operations that are performed on personal information or on sets of personal information, whether or not by automated means. CCPA personal data; processing processing environment the combination of software and hardware on which the Application runs. Law_Insider_p rocessing_envi ronment processor â€˜Processorâ€™ means a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller. GDPR â€œProcessingâ€ means any operation or set of operations that are performed on personal information or on sets of personal information, whether or not by automated means. CCPA personal data; processing; controller product manager a specialized product management professional whose job is to manage the planning, development, launch, and success of products/solutions powered by AI, machine learning, and deep learning technologies. productmanag erHQ_Josh_Fe chter product owner [person who is] focused on providing direction and prioritization for the cross-functional AI team, ensuring everyone remains focused on the overall vision and road map. This role is responsible for unifying individuals with diverse skills and backgrounds toward a common goal. Forbes_Tracy _Kemp productization [turning the best performing model] into an actual \"data product,\" ready to be used in live services. Towards_Prod uctizing profiling â€˜Profilingâ€™ means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person's performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements. GDPR â€œProfilingâ€ means any form of automated processing of personal information, as further defined by regulations pursuant to paragraph (16) of subdivision (a) of Section 1798.185, to evaluate certain personal aspects relating to a natural person and in particular to analyze or predict aspects concerning that natural personâ€™s performance at work, economic situation, health, personal preferences, interests, reliability, behavior, location, or movements. CCPA Measuring the characteristics of expected activity so that changes to it can be more easily identified. CSRC personal data; processing protected attribute An attribute that partitions a population into groups whose outcomes should have parity. Examples include race, gender, caste, and religion. Protected attributes are not universal, but are application specific. AI_Fairness_3 60 protected class [a feature] that may not be used as the basis for decisions [and] could be chosen because of legal mandates or because of organizational values. Some common protected [classes] include race, religion, national origin, gender, marital status, age, and socioeconomic status. MIT_Protecte d_Attributes A group of people with a common characteristic who are legally protected from [...] discrimination on the basis of that characteristic. Protected classes are created by both federal and state law. Practical_Law _protected_cl ass prototype A prototype is an original model constructed to include all the technical characteristics and performances of the new product. OECD provisioning The granting of access rights and executional privilege to an agent (human or machine) within an application(s) or system(s). IEEE_Guide_I PA proxy A variable that can stand in for another, usually not directly observable or measurable, variable. SP1270 pseudo-anonymization (pseudonymization) â€˜Pseudonymisationâ€™ means the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person; GDPR â€œPseudonymizeâ€ or â€œPseudonymizationâ€ means the processing of personal information in a manner that renders the personal information no longer attributable to a specific consumer without the use of additional information, provided that the additional information is kept separately and is subject to technical and organizational measures to ensure that the personal information is not attributed to an identified or identifiable consumer. CCPA A data management technique to strip identifiers linking data to an individual. NSCAI personal data; processing quality The totality of features and characteristics of a product or service that bear on its ability to satisfy stated or implied needs. OECD <data> degree to which the characteristics of data satisfy stated and implied needs when used under specified conditions; <system> degree to which a set of inherent characteristics of an object fulfils requirements (an object can be a product, process or service) ISO/IEC_TS_ 5723Ê¼2022(en) racialized A socio-political process by which groups are ascribed a racial identity, whether or not members of the group self-identify as such AAAS_AI_and _Bias_2022-09 ranking a type of machine learning that sorts data in a relevant order[; often used by companies] to optimize search and recommendations. DEV_ranking position, order, or standing within a group : RANK Merriam-Webster_ranki ng recall A metric for classification models; identifies the frequency with which a model correctly classifies the true positive items. NSCAI A metric for classification modelsthat answers the following question: Out of all the possible positive labels,how many did the model correctly identify? That is: Recall = True Positive/(True Positive + false Negative) aime_measure ment_2022, citing Machine Learning Glossary by Google recognition the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. Pattern_Recog nition_and_M achine_Learni ng a sense of awareness and familiarity experienced when one encounters people, events, or objects that have been encountered before or when one comes upon material that has been learned in the past. APA_recogniti on to transfer prior learning or past experience to current consciousness: that is, to retrieve and reproduce information; to remember. APA_recall recommendation system A software tool and techniques that provide suggestion based on the customer's taste to discover new appropriate thing for them by filtering personalized information based on the user's preferences from a large volume of information Das,_Debashis A subclass of information filtering system that seek to predict â€˜ratingâ€™ or â€˜preferenceâ€™ that a user would give to an item (such as music, books or movies) or social element (e.g. people or group) they had not yet considered, using a model built from the characteristics of an item (content based approaches) or the userâ€™s social environment (collaborative filtering approaches) Sharma,_Lalita rectification An individualâ€™s right to have personal data about them corrected or amended by a business or other organization if it is inaccurate. IAPP_Privacy_ Glossary red-team A group of people authorized and organized to emulate a potential adversaryâ€™s attack or exploitation capabilities against an enterpriseâ€™s security posture. The Red Teamâ€™s objective is to improve enterprise cybersecurity by demonstrating the impacts of successful attacks and by demonstrating what works for the defenders (i.e., the Blue Team) in an operational environment. Also known as Cyber Red Team. CSRC reference class A class which is intended to describe structure and behavior of object identifiers. Its instances, called references, are passed by-value and indirectly represent objects by substituting for some primitive reference. IGI_Global_ref erence_class reflexivity A form of critical thinking that prompts us to consider the â€˜whysâ€™ and â€˜howsâ€™ of research, critically questioning the utility, ethics, and value of what, whom, and how we study Jamieson_Gov aart_Pownall in qualitative research, the self-referential quality of a study in which the researcher reflects on the assumptions behind the study and especially the influence of his or her own motives, history, and biases on its conduct. APA_reflexivity positionality regression Regression is a process of predicting the value to a yes or no label provided it falls on a continuous spectrum of input values, subcategory of supervised learning. Ranschaert, _Erik the prediction of an exact value using a given set of data Saleh_Alkhalifa _ML_in_Biote ch reinforcement learning A method of training algorithms to make suitable actions by maximizing rewarded behavior over the course of its actions. This type of learning can take place in simulated environments, such as game-playing, which reduces the need for real-world data. NSCAI Reinforcement learning (RL) is a subset of machine learning that allows an artificial system (sometimes referred to as an agent) in a given environment to optimize its behaviour. Agents learn from feedback signals received as a result of their actions, such as rewards or punishments, with the aim of maximizing the received reward. Such signals are computed based on a given reward function, which constitutes an abstract representation of the system's goal. The goal could be, for example, to earn a high video game score or to minimize idle worker time in a factory TTC6_Taxono my_Terminolo gy reliability Reliability refers to the closeness of the initial estimated value(s) to the subsequent estimated values. OECD ability of an item to perform as required, without failure, for a given time interval, under given conditions. Note 1 to <system> definition: The time interval duration can be expressed in units appropriate to the item concerned (e.g. calendar time, operating cycles, distance run, etc.) and the units should always be clearly stated. Note 2 to <system> definition: Given conditions include aspects that affect reliability, such as: mode of operation, stress levels, environmental conditions, and maintenance. ISO/IEC_TS_ 5723Ê¼2022(en) property of consistentintended behaviour and results aime_measure ment_2022, citing ISO/IEC 22989 remediation The process of treating data by cleaning, organizing, and migrating it to a safe and secure environment for optimized usage is called data remediation. Generally [understood] as a process involving deleting unnecessary or unused data. However, the actual process . . . is very detailed and includes several steps, including replacing, updating, or modifying data along with cleaning it, organizing it, and getting rid of unnecessary data. \n\nCPO_Magazin e_Amar_Kana garaj reproducibility Closeness of the agreement between the results of measurements of the same measurand carried out under changed conditions of measurement. IEEE_Soft_Vo cab requirement something essential to the existence or occurrence of something else : CONDITION Merriam-Webster_requi rement residual Residuals are differences between the one-step-predicted output from the model and the measured output from the validation data set. Thus, residuals represent the portion of the validation data not explained by the model. MathWorks_R esidual resilience The ability to prepare for and adapt to changing conditions and withstand and recover rapidly from disruptions. Resilience includes the ability to withstand and recover from deliberate attacks, accidents, or naturally occurring threats or incidents. The ability of a system to adapt to and recover from adverse conditions. NISTIR_8269_ Draft <governance> ability to anticipate and adapt to, resist, or quickly recover from a potentially disruptive event, whether natural or man-made; <system> capability of a system to maintain its functions and structure in the face of internal and external change, and to degrade gracefully when this is necessary ISO/IEC_TS_ 5723Ê¼2022(en) ability of a system to recover operational conditionquickly following an incident aime_measure ment_2022, citing ISO/IEC 22989 responsible AI An AI system that aligns development and behavior to goals and values. This includes developing and fielding AI technology in a manner that is consistent with democratic values. NSCAI result The consequential outcome of completing a process. IEEE_Guide_I PA retention limit refers to the amount of information that is stored long-term, and can be measured in volume (the size of the total collected logs in bytes) and time (the number of months or years that logs are stored for). Industrial_Net work_Security _2011 Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nrisk The composite measure of an eventâ€™s probability of occurring and the magnitude or degree of the consequences of the corresponding event. The impacts, or consequences, of AI systems can be positive, negative, or both and can result in opportunities or threats (Adapted from: iso 31000Ê¼2018 ) NIST_AI_RMF _1.0 A measure of the extent to which an entity is threatened by a potential circumstance or event, and typically a function of: (i) the adverse impacts that would arise if the circumstance or event occurs; and (ii) the likelihood of occurrence. SP800-12 An uncertain event or condition that, if it occurs, has a positive or negative effect on a project's objectives IEEE_Soft_Vo cab effect of uncertainty on objectives ISO_IEC_3850 7risk control mechanisms at the design, implementation, and evaluation stages [that can be taken] into consideration when developing responsible AI for organizations that includes security risks (cyber intrusion risks, privacy risks, and open source software risk), economic risks (e.g., job displacement risks), and performance risks (e.g., risk of errors and bias and risk of black box, and risk of explainability). Toward_an_u nderstanding_ of_responsible _artificial_inte lligence_practi ces risk tolerance Risk tolerance refers to the organizationâ€™s or AI actorâ€™s ... readiness to bear the risk in order to achieve its objectives. Risk tolerance can be influenced by legal or regulatoryrequirements. NIST_AI_RMF _1.0 robotic desktop automation (RDA) The computer application that makes available to a human operator a suite of predefined activity choreography to complete the execution of processes, activities, transactions, and tasks in one or more unrelated software systems to deliver a result or service in the course of human-initiated or -managed workflow. IEEE_Guide_I PA robotic process automation (RPA) A preconfigured software instance that uses business rules and predefined activity choreography to complete the autonomous execution of a combination of processes, activities, transactions, and tasks in one or more unrelated software systems to deliver a result or service with human exception management. IEEE_Guide_I PA Software to help in the automation of tasks, especially those that are tedious and repetitive. NSCAI robust AI An AI system that is resilient in real-world settings, such as an object-recognition application that is robust to significant changes in lighting. The phrase also refers to resilience when it comes to adversarial attacks on AI components. NSCAI robustness ability of a system to maintain its level of performance under a variety of circumstances ISO/IEC_TS_ 5723Ê¼2022(en) The ability of a machine learning model/algorithm to maintain correct and reliable performance under different conditions (e.g., unseen, noisy, or adversarially manipulated data). NISTIR_8269_ Draft root-mean-square deviation (RMSD) of an estimator of a parameter[; ...] the square-root of the mean squared error (MSE) of the estimator. In symbols, if X is an estimator of the parameter t, then RMSE(X) = ( E( (Xâˆ’t)2 ) )Â½. The RMSE of an estimator is a measure of the expected error of the estimator. The units of RMSE are the same as the units of the estimator. Glossary_of_S tatistical_Ter ms a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed Wikipedia_RM SD root-mean-square error (RMSE) row describes a single entity or observation and the columns describe properties about that entity or observation. The more rows you have, the more examples from the problem domain that you have. Machine_Lear ning_Mastery _Jason_Brown lee safety property of a system such that it does not, under defined conditions, lead to a state in which human life, health, property, or the environment is endangered; [safety involves reducing both the probability of expected harms and the possibility of unexpected harms]. ISO/IEC_TS_ 5723Ê¼2022(en) freedom from risk which is not tolerable aime_measure ment_2022, citinig ISO/IEC TR 24029-1 scalability The ability to increase or decrease the computational resources required to execute a varying volume of tasks, processes, or services. IEEE_Guide_I PA score A continuous value output from a classifier. Applying a threshold to a score results in a predicted label. AI_Fairness_3 60 screen out Screen-out discrimination occurs when â€œa disability prevents a job applicant or employee from meetingâ€”or lowers their performance onâ€”a selection criterion, and the applicant or employee loses a job opportunity as a result.â€ EEOC_ADA_AI security resistance to intentional, unauthorized act(s) designed to cause harm or damage to a system ISO/IEC_TS_ 5723Ê¼2022(en) degree to which a product or system (3.38)protects information (3.20) and data (3.11) so that persons or other productsor systems have the degree of data access appropriate to their types and levelsof authorization aime_measure ment_2022, citing ISO/IEC TR 24029-1 segmentation The process of identifying homogeneous subgroups within a data table. Raynor self-aware system A computing platform imbued with sufficient knowledge and analytic capability to make useful conclusions about its inputs, its own processing, and the use of its output so that it is capable of self- judgment and improvement consistent with its purpose. IEEE_Guide_I PA self-diagnosis Ability of a system to adequately take measurement information from sensors, validate the data, and communicate the processes and results to other devices SP1011 self-healing system A computing system able to perceive that it is not operating correctly and, without human intervention, make the necessary adjustments to restore itself to normalcy. IEEE_Guide_I PA semantic mapping A strategic schema or framework of metadata labels applied to all data, data groups, data fields, data types, or data content used to introduce new or raw data into a corpus or data fabric to give machine learning algorithms direction for investigating known or potential relationships between data. A semantic map provides a structure for the introduction of new data, information, or knowledge IEEE_Guide_I PA sensitivity analysis A â€œwhat-ifâ€ type of analysis to determine the sensitivity of the outcomes to changes in parameters. If a small change in a parameter results in relatively large changes in the outcomes, the outcomes are said to be sensitive to that parameter. OECD sensory digitization The conversion of typically analog or human sensory perception (e.g., vision, speech) to a digital format useful for machine-to-human interaction or machine processing of traditionally analog sensory information [e.g., optical character recognition (OCR)]. IEEE_Guide_I PA service A collection of coordinated processes that takes one or more kinds of input, performs a value-added transformation, and creates an output that fulfills the needs of a customer [or shareholder]. IEEE_Guide_I PA signal detection theory a framework for interpreting data from experiments in which accuracy is measured. Signal_Detecti on_Theory shallow learning Techniques that separate the process of feature extraction from learning itself. Reznik,_Leon situational awareness Perception of elements in the system and/or environment and a comprehension of their meaning, which could include a projection of the future status of perceived elements and the uncertainty associated with that status. SP800-160 socio-technical system how humans interact with technology within the broader societal context NIST SP1270 system that includes a combination of technical and human or natural elements ISO/IEC_TS_ 5723Ê¼2022(en) software testing Activity in which a system or component is executed under specified conditions, the results are observed or recorded, and an evaluation is made of some aspect of the system or component. IEEE_Soft_Vo cab sparsity refers to a matrix of numbers that includes many zeros or values that will not significantly impact a calculation. Dave_Salvator _sparsity specification A document that specifies, in a complete, precise, verifiable manner, the requirements, design, behavior, or other characteristics of a system or component and often the procedures for determining whether these provisions have been satisfied. SP800-37 stakeholder Individual or organization having a right, share, claim, or interest in a system or in its possession of characteristics that meet their needs and expectations. An individual, group, or organization who may affect, be affected by, or perceive itself to be affected by a decision, activity, or outcome of a project. IEEE_Soft_Vo cab any individual, group, or organization that can affect, be affected by, or perceive itself to be affected by a decision or activity ISO/IEC_TS_ 5723Ê¼2022(en) standard deviation The most widely used measure of dispersion of a frequency distribution introduced by K. Pearson (1893). It is equal to the positive square root of the variance. The standard deviation should not be confused with the root mean square deviation. OECD start event An activity, task, or input that describes or defines the beginning of a process. IEEE_Guide_I PA statistical bias A systematic tendency for estimates or measurements to be above or below their true values. Statistical biases arise from systematic as opposed to random error. Statistical bias can occur in the absence of prejudice, partiality, or discriminatory intent. SP1270 statistical parity The independence between the protected attribute and the outcome of the decision rule Besse, _Philippe statistical significance When the probability of obtaining a statistic of a given size due strictly to random sampling error, or chance, is less than the selected alpha level [or the probability of a type I error]; also represents a rejection of the null hypothesis. Statistics_in_P lain_English refers to whether a relationship between two or more variables exists beyond a probability expected by chance The_SAGE_En cyclopedia_of _Communicati on_Research_ Methods statistics Numerical data relating to an aggregate of individuals; the science of collecting, analysing and interpreting such data OECD stereotype a set of cognitive generalizations (e.g., beliefs, expectations) about the qualities and characteristics of the members of a group or social category. Stereotypes, like schemas, simplify and expedite perceptions and judgments, but they are often exaggerated, negative rather than positive, and resistant to revision even when perceivers encounter individuals with qualities that are not congruent with the stereotype. APA_stereotyp eContemporary social psychology typically defines stereotypes as mental representations of a group and its members, and stereotyping as the cognitive activity of treating individual elements in terms of higher level categorial properties Augoustinos_ Walker_1998 stochastic The adjective â€œstochasticâ€ implies the presence of a random variable; e.g. stochastic variation is variation in which at least one of the elements is a variate and a stochastic process is one wherein the system incorporates an element of randomness as opposed to a deterministic system. OECD straight-through processing (STP) The successful execution of a service, process, or transaction performed entirely through traditional application platforms with predefined interfaces (i.e., application programming interfaces [APIs]). IEEE_Guide_I PA Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\nstrawperson a fallacious argument which irrelevantly attacks a position that appears similar to, but is actually different from, an opponent's position, and concludes that the opponent's real position has thereby been refuted. Hughes_Laver y_Critical_Thi nking stress test Type of performance efficiency testing conducted to evaluate a test item's behavior under conditions of loading above anticipated or specified capacity requirements, or of resource availability below minimum specified requirements IEEE_Soft_Vo cab structured data Data that has a predefined data model or is organized in a predefined way. NIST_1500 sub-process A subordinate process that can be included within a parent process. It can be present and/or repeated within other parent processes. IEEE_Guide_I PA supervised learning A type of machine learning in which the algorithm compares its outputs with the correct outputs during training. In unsupervised learning, the algorithm merely looks for patterns in a set of data. Hutson, _Matthew Algorithms, which develop a mathematical model from the input data and known desired outputs. Reznik,_Leon For a computer to process a set of data whose attributes have been divided into two groups and derive a relationship between the values of one and the values of the other. These two groups are sometimes called predictor and targets, respectively. In statistical terminology, they are called independent and dependent variables. Respectively. The learning Is \"supervised because the distinction between the predictors and the target variables is chosen by the investigator or some other outside agency. Raynor a general subset of machine learning in which data, like its associated labels, is used to train models that can learn or generalize from the data to make predictions, preferably with a high degree of certainty. Saleh_Alkhalifa _ML_in_Biote ch support vector machines A supervised machine learning model for data classification and regression analysis. One of the most used classifiers in machine learning. It optimizes the width of the gap between the points of separate categories in feature space. Ranschaert, _Erik system combination of interacting elements organized to achieve one or more stated purposes ISO/IEC_TS_ 5723Ê¼2022(en) systemic bias Systemic biases result from procedures and practices of particular institutions that operate in ways which result in certain social groups being advantaged or favored and others being disadvantaged or devalued. This need not be the result of any conscious prejudice or discrimination but rather of the majority following existing rules or norms. D. Chandler and R. Munday, A Dictionary of Media and Communicatio n. Oxford University Press, Jan. 2011, publication Title: A Dictionary of Media and Communicatio nsystem of systems set of systems and system elements that interact to provide a unique capability that none of the constituent systems can accomplish on its own (note: can be necessary to facilitate interaction of the constituent systems in the system of systems) ISO/IEC_TS_ 5723Ê¼2022(en) target a method for solving a problem that an AI algorithm parses its training data to find. Once an algorithm finds its target function, that function can be used to predict results (predictive analysis). The function can then be used to find output data related to inputs for real problems where, unlike training sets, outputs are not included. TechTarget_ta rget_function target variable, target value task The performance of a discrete activity with a defined start, stop, and outcome that cannot be broken down to a finer level of detail. IEEE_Guide_I PA Required, recommended, or permissible action, intended to contribute to the achievement of one or more outcomes of a process IEEE_Soft_Vo cab set of activities undertaken in order to achieve a specific goal aime_measure ment_2022, citing ISO/IEC TR 24030 taxonomy Taxonomy refers to classification according to presumed natural relationships among types and their subtypes. OECD technical control Security controls (i.e., safeguards or countermeasures) for an information system that are primarily implemented and executed by the information system through mechanisms contained in the hardware, software, or firmware components of the system. NIST_SP_800-30_Rev_1 technochauvinism The belief that technology is always the solution M. Broussard, Artificial Unintelligence: How Computers Misunderstand the World. MIT Press, 2018. techno-solutionism test Technical operation to determine one or more characteristics of or to evaluate the performance of a given product, material, equipment, organism, physical phenomenon, process or service according to a specified procedure .UNODC_Gloss ary_QA_GLP any activity aimed at evaluating an attribute or capability of a program or system and deteermining that it meets its required results. William_Hetze l(1) activity in which a system or component is executedunder specified conditions, the results are observed or recorded, and an evaluationis made of some aspect of the system or component; (2) to conduct anactivity as in (1); (3) set of one or more test cases and procedures. aime_measure ment_2022, citing ISO/IEC 24765 the process of executing a program with the intent of finding errors. The_Art_of_S oftware_Testi ng Test, Evaluation, Verification and Validation (TEVV) Test and Evaluation, Verification and Validation (TEVV) A framework for assessing, incorporating methods and metrics to determine that a technology or system satisfactorily meets its design specifications and requirements, and that it is sufficient for its intended use. NSCAI_Report third party an entity that is involved in some way in an interaction that is primarily between two other entities. [Please see note, especially regarding NIST CSRC terms that we might incorporate into this definition.] TechTarget_th ird_party three lines of defense Most financial institutions follow a three-lines-of-defense model, which separates front line groups, which are generally accountable for business risks (the First Line), from other risk oversight and independent challenge groups (the Second Line) and assurance (the Third Line) AIRS_Penn traceability Ability to trace the history, application or location of an entity by means of recorded identification. [\"Chain of custody\" is a related term.] Alternatively, traceability is a property of the result of a measurement or the value of a standard whereby it can be related with a stated uncertainty, to stated references, usually national or international standards, i.e. through an unbroken chain of comparisons. In this context, The standards referred to here are measurement standards rather than written standards. UNODC_Gloss ary_QA_GLP A characteristic of an AI system enabling a person to understand the technology, development processes, and operational capabilities (e.g., with transparent and auditable methodologies along with documented data sources and design procedures). NSCAI training data A dataset from which a model is learned. AI_Fairness_3 60 samples for training used to fit a machine learningmodel aime_measure ment_2022, citing ISO/IEC 22989 transaction Enactment of a process represented by a set of coordinated activities carried out by multiple systems and/or participants in accordance with defined relationships. This coordination leads to an intentional, consistent, and verifiable result across all participants. IEEE_Guide_I PA transfer learning A technique in machine learning in which an algorithm learns to perform one task, such as recognizing cars, and builds on that knowledge when learning a different but related task, such as recognizing cats. Hutson, _Matthew transformer A procedure that modifies a dataset. AI_Fairness_3 60 transparency <information> open, comprehensive, accessible, clear and understandable presentation of information; <systems> property of a system or process to imply openness and accountability ISO/IEC_TS_ 5723Ê¼2022(en) Understanding the working logic of the model. NISTIR_8269_ Draft <organization> property of an organization that appropriate activities and decisions are communicated to relevant stakeholders (3.5.13) in a comprehensive, accessible and understandable manner Note 1 to entry: Inappropriate communication of activities and decisions can violate security, privacy or confidentiality requirements. iso_22989_20 22 <system> property of a system that appropriate information about the system is made available to relevant stakeholders (3.5.13) Note 1 to entry: Appropriate information for system transparency can include aspects such as features, performance, limitations, components, procedures, measures, design goals, design choices and assumptions, data sources and labelling protocols. Note 2 to entry: Inappropriate disclosure of some aspects of a system can violate security, privacy or confidentiality requirements. iso_22989_20 22 true negative outcome where the model correctly predicts the negative class. google_dev_cl assification-true-false-positive-negative true positive an outcome where the model correctly predicts the positive class. google_dev_cl assification-true-false-positive-negative trust the system status in the mind of human beings based on their perception of and experience with the system; concerns the attitude that a person or technology will help achieve specific goals in a situation characterized by uncertainty and vulnerability. DOD_TEVV degree to which a user or other stakeholder has confidence that a product or system will behave as intended aime_measure ment_2022, citing ISO/IEC TR 24029-1 trustworthiness The degree to which an information system (including the information technology components that are used to build the system) can be expected to preserve the confidentiality, integrity, and availability of the information being processed, stored, or transmitted by the system across the full range of threats and individualsâ€™ privacy. SP800-37 Worthy of being trusted to fulfill whatever critical requirements may be needed for a particular component, subsystem, system, network, application, mission, enterprise, or other entity. SP800-160 ability to meet stakeholders' expectations in a verifiable way; an attribute that can be applied to services, products, technology, data and information as well as to organizations. ISO/IEC_TS_ 5723Ê¼2022(en) Terms Definition 1 Citation 1 [1] Definition 2 Citation 2 Definition 3 Citation 3 Definition 4 Citation 4 Definition 5 Citation 5 Related terms and synonyms [2] Legal definition applicable \n\ntrustworthy AI Characteristics of trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. NIST_AI_RMF _1.0 Trustworthy AI has three components: (1) it should be lawful, ensuring compliance with all applicable laws and regulations (2) it should be ethical, demonstrating respect for, and ensure adherence to, ethical principles and values and (3) it should be robust, both from a technical and social perspective, since, even with good intentions, AI systems can cause unintentional harm. Trustworthy AI concerns not only the trustworthiness of the AI system itself but also comprises the trustworthiness of all processes and actors that are part of the systemâ€™s life cycle. european_ethi cs_2019 Trustworthy AI has three components: (1) it should be lawful, ensuring compliance with all applicable laws and regulations (2) it should be ethical, demonstrating respect for, and ensure adherence to, ethical principles and values and (3) it should be robust, both from a technical and social perspective, since, even with good intentions, AI systems can cause unintentional harm. Characteristics of Trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. Trustworthy AI concerns not only the trustworthiness of the AI system itself but also comprises the trustworthiness of all processes and actors that are part of the AI systemâ€™s life cycle. Trustworthy AI is based on respect for human rights and democratic values. \n\n> TTC6_Taxonom y_Terminology\n\ntype I error The null hypothesis H0 is rejected, even though it is [true] berthold_guid e_2020 false positive rate james_statistic al_2014 type II error The null hypothesis H0 is accepted, even though it is [false] berthold_guid e_2020 true positive rate james_statistic al_2014 uncertainty Result of not having accurate or sufficient knowledge of a situation; state, even partial, of deficiency of information related to understanding or knowledge of an event, its consequence, or likelihood IEEE_Soft_Vo cab underfitting Underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data. Ranschaert, _Erik underrepresentation inadequately represented. (See note.) Merriam-Webster_unde rrepresented when members of discernible groups are not consistently present in representative bodies and among measures of well-being in numbers roughly proportionate to their numbers within the population. Encyclopedia. com_underrep resentation unexplainable impossibility of providing an explanation for certain decisions made by an intelligent system which is both 100% accurate and comprehensible. Roman_V. _Yampolskiy_ Unexplainabilit yblack box; opacity unstructured data Data that does not have a predefined data model or is not organized in a predefined way unsupervised learning A learning strategy that consists in observing and analyzing different entities and determining that some of their subsets can be grouped into certain classes, without any correctness test being performed on acquired knowledge through feedback from external knowledge sources. Note 1 to entry: Once a concept is formed, it is given a name that may be used in subsequent learning of other concepts. iso_2382_1997 usability extent to which a system product or service can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use (note 1Ê¼ The â€œspecifiedâ€ users, goals and context of use refer to the particular combination of users, goals and context of use for which usability is being considered; note 2Ê¼ used as a qualifier to refer to the design knowledge, competencies, activities and design attributes that contribute to usability, such as usability expertise, usability professional, usability engineering, usability method, usability evaluation, usability heuristic). [See also: ISO/IEC 9241-11 Ergonomic of Human-System Interaction â€” Part 11Ê¼ Usability: Definitions and Concepts. ISO, Geneva, Switzerland, 2018, https:/ /www.iso. org/standard/63500.html.] ISO/IEC_TS_ 5723Ê¼2022(en) usability testing refers to evaluating a product or service by testing it with representative users. Typically, during a test, participants will try to complete typical tasks while observers watch, listen and takes notes. The goal is to identify any usability problems, collect qualitative and quantitative data and determine the participant's satisfaction with the product. Usabilitygov user individual or group that interacts with a system or benefits from a system during its utilization IEEE_Soft_Vo cab A person, organization, or other entity which requests access to and uses the resources of a computer system or network. CSRC user-centered design the practice of the following principles, the active involvement of users for a clear understanding of user and task requirements, iterative design and evaluation, and a multi-disciplinary approach Vredenburg, _Karel Approach to system design and development that aims to make interactive systems more usable by focusing on the use of the system; applying human factors, ergonomics and usability knowledge and techniques. IEEE_Soft_Vo cab validation Confirmation by examination and provision of objective evidence that the particular requirements for a specific intended use are fulfilled. UNODC_Gloss ary_QA_GLP Confirmation, through the provision of objective evidence, that the requirements for a specific intended use or application have been fulfilled. IEEE_Soft_Vo cab provides objective evidence that the capability provided by the system complies with stakeholder performance requirements, achieving its use in its intended operational environment; answers the question, \"Is it the right solution to the problem?\" [C]onsists of evaluating the operational effectiveness, operational suitability, sustainability, and survivability of the system or system elements under operationally realistic conditions. DOD_TEVV A continuous monitoring of the process of compilation and of the results of this process. OECD Test and Evaluation, Verification, and Validation (TEVV) value sensitive design a theoretically grounded approach to the design of technology that accounts for human values in a principled and systematic manner throughout the design process. Friedman_et_ al_2017 variable A variable is a characteristic of a unit being observed that may assume more than one of a set of values to which a numerical measure or a category from a classification can be assigned. OECD Quantity or data item whose value can change IEEE_Soft_Vo cab variance The variance is the mean square deviation of the variable around the average value. It reflects the dispersion of the empirical values around its mean. OECD A quantifiable deviation, departure, or divergence away from a known baseline or expected value IEEE_Soft_Vo cab verifiable can be checked for correctness by a person or tool ISO/IEC_TS_ 5723Ê¼2022(en) provides evidence that the system or system element performs its intended functions and meets all performance requirements listed in the system performance specification and functional and allocated baselines; answers the question, \"Did you build the system correctly?\" DOD_TEVV Test and Evaluation, Verification and Validation (TEVV) word embedding a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. . . . A word embedding, trained on word co-occurrence in text corpora, represents each word (or common phrase) w as a d-dimensional word vector w~ 2 Rd. It serves as a dictionary of sorts for computer programs that would like to use word meaning. First, words with similar semantic meanings tend to have vectors that are close together. Second, the vector differences between words in embeddings have been shown to represent relationships between words. Bolukbasi_et_ al_Debiasing_ Word_Embedd ings ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nGDPR Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) https:/ /eur-lex.europa.eu/eli/reg/2016/679/oj CCPA California Consumer Privacy Act of 2018 https:/ /leginfo.legislature.ca.gov/faces/codes_displayText.xhtml?division=3.&part=4.&lawCode=CIV&title=1.81.5 AI_Incident_Database What is an AI incident? AI Incident Database AI Incident Database 2022 https:/ /incidentdatabase.ai/research/1-criteria Shubendhu_and_Vijay Applicability of Artificial Intelligence in Different Fields of Life Shubhendu, Shukla S. and Jaiswal Vijay International Journal of Scientific Engineering and Research (IJSER) 1 1 28-35 2013 https:/ /www.ijser.in/archives/v1i1/MDExMzA5MTU=.pdf Raynor Glossary of Computer System Software Development Terminology Raynor, William J., Jr. The International Dictionary of Artificial Intelligence 1999 https:/ /archive.org/details/internationaldic0000rayn AI_Fairness_360 Glossary AI Fairness 360 AI Fairness 360 https:/ /aif360.mybluemix.net/resources#glossary Mitchell,_Tom Machine Learning Mitchell, Tom Machine Learning 1997 http:/ /www.cs.cmu.edu/~tom/mlbook.html Brookings_Institution The Brookings glossary of AI and emerging technologies Allen, John R. and Darrell M. West Brookings Institution 2021 https:/ /www.brookings.edu/blog/techtank/2020/07/13/the-brookings-glossary-of-ai-and-emerging-technologies/ Brownlee,_Jason A Gentle Introduction to Generative Adversarial Networks (GANs) Brownlee, Jason Machine Learning Mastery 2019 https:/ /machinelearningmastery.com/what-are-generative-adversarial-networks-gans/ Pyle_and_San_JosÃ© An executiveâ€™s guide to machine learning Pyle, Dorian and Cristina San JosÃ© McKinsey Quarterly 2015 https:/ /www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/an-executives-guide-to-machine-learning Hutson,_Matthew AI Glossary: Artificial intelligence, in so many words Hutson, Matthew Science 357 6346 19 2017 https:/ /www.science.org/doi/10.1126/science.357.6346.19 FBPML_Wiki Definitions Foundation for Best Practices in Machine Learning FBPML Wiki https:/ /wiki.fbpml.org/wiki/Definitions IAPP_Privacy_Glossary Glossary of Privacy Terms https:/ /iapp.org/resources/glossary/ IAPP_Governance_Terms Glossary of Governance Terms Reznik,_Leon Introduction I.5 Glossary of Basic Terms Reznik, Leon Intelligent Security Systems: How Artificial Intelligence, Machine Learning and Data Science Work for and Against Computer Security xv-xxiv 2022 IEEE_Guide_IPA IEEE Guide for Terms and Concepts in Intelligent Process Automation IEEE Standards Association IEEE Guide for Terms and Concepts in Intelligent Process Automation Russell_and_Norvig Stuart Russell and Peter Norvig Artificial Intelligence: A Modern Approach (Fourth Edition) 2021 SP1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence Schwartz, Reva; Apostol Vassilev; Kristen Greene; Lori Perine; Andrew Burt; Patrick Hall NIST Special Publication 1270 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf SP1011 Autonomy Levels for Unmanned Systems (ALFUS) Framework Autonomy Levels for Unmanned Systems Working Group Participants NIST Special Publication 1011 2008 https:/ /www.nist.gov/system/files/documents/el/isd/ks/NISTSP_1011-I-2-0.pdf Gartner Gartner Glossary Gartner Group https:/ /www.gartner.com/en/glossary/all-terms Varshney,_Kush Trustworthy Machine Learning Varshney, Kush R. Munir,_Arslan Artificial Intelligence and Data Fusion at the Edge Munir, Arslan, Erik Blasch, Jisu Kwon, Joonho Kong, and Alexander Aved IEEE A&E SYSTEMS MAGAZINE 36 7 62-78 2021 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9475883 Wallace,_Brian Introduction to Artificial Intelligence for Security Professionals Wallace, Brian; Sepehr Akhavan-Masouleh; Andrew Davis; Mike Wojnowicz; John H. Brock 2017 http:/ /book.itep.ru/depository/AI/IntroductionToArtificialIntelligenceForSecurityProfessionals_Cylance.pdf NSCAI National Security Commission on Artificial Intelligence: The Final Report National Security Commission on Artificial Intelligence National Security Commission on Artificial Intelligence Final Report 2021 https:/ /www.nscai.gov/2021-final-report/ OECD Glossary of Statistical Terms Organisation for Economic Co-operation and Development 2007 https:/ /ec.europa.eu/eurostat/ramon/coded_files/OECD_glossary_stat_terms.pdf / https:/ /stats.oecd.org/glossary/ OECD_CAI_recommendati on Recommendation of the Council on Artificial Intelligence OECD OECD Legal Instruments 2019 https:/ /legalinstruments.oecd.org/en/instruments/oecd-legal-0449 NISTIR_8269_Draft A Taxonomy and Terminology of Adversarial Machine Learning Tabassi, Elham;Kevin J. Burns; Michael Hadjimichael; Andres D. Molina-Markham; Julian T. Sexton Draft NISTIR 8269 2019 https:/ /nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8269-draft.pdf SP800-37 Risk Management Framework for Information Systems and Organizations: A System Life Cycle Approach for Security and Privacy Joint Task Force Interagency Working Group NIST Special Publication 800-37 Revision 2 2 (revision 2) 2018 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-37r2.pdf IEEE_Soft_Vocab Systems and software engineering â€”Vocabulary ISO/IEC/IEEE 24765 2017 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8016712 Kohavi,_Ron Glossary of Terms: Special Issue on Applications of Machine Learning and the Knowledge Discovery Process Kohavi, Ron; Foster Provost Machine Learning 30 271-274 1998 http:/ /robotics.stanford.edu/~ronnyk/glossary.html Mitchell,_Tom Machine Learning Mitchell, Tom M. McGraw-Hill Science/Engineering/Math 1997 https:/ /www.cin.ufpe.br/~cavmj/Machine%20-%20Learning%20-%20Tom%20Mitchell.pdf Cyber_Guide Cyber Security Planning Guide Federal Communications Commision https:/ /www.fcc.gov/sites/default/files/cyberplanner.pdf CSRC Information Technology Laboratory Computer Security Resource Center Glossary NIST https:/ /csrc.nist.gov/glossary AIMA Artificial Inelligence: A Modern Approach Russell, Stuart; Peter Norvig Pearson 2010 https:/ /zoo.cs.yale.edu/classes/cs470/materials/aima2010.pdf Breiman_Leo Bagging Predictors Breiman, Leo Machine Learning 24 123-140 1996 https:/ /link.springer.com/content/pdf/10.1007/BF00058655.pdf NISTIR_8312 Four Principles of Explainable Artificial 3 Intelligence Phillips, P. Jonathon; Carina A. Hahn; Peter C. Fontana; David A. Broniatowski; Mark A. Przybocki Draft NISTIR 8312 2020 https:/ /nvlpubs.nist.gov/nistpubs/ir/2020/NIST.IR.8312-draft.pdf SP800-12 An Introductin to Information Security Nieles, Michael; Kelley Dempsey; Victoria Yan Pillitteri NIST SP 800-12 2017 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-12r1.pdf Steinhardt,_Jacob Certified Defenses for Data Poisoning Attacks Steinhardt_Jacob; Pang Wei Koh; Percy Liang 31st Conference on Neural Information Processing Systems 2017 https:/ /proceedings.neurips.cc/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf Ranschaert,_Erik Artificial Intelligence in Medical Imaging: Opportunities, Applications and Risks Ranschaert, Erik R.; Sergey Morozov; Paul R. Algra Springer 2019 https:/ /link.springer.com/content/pdf/10.1007/978-3-319-94878-2.pdf Blank,_Abagayle_Lee Computer Vision Machine Learning and Future-Oriented Ethics Blank, Abagayle Lee Seattle Pacific University https:/ /digitalcommons.spu.edu/cgi/viewcontent.cgi?article=1100&context=honorsprojects Crawford,_Kate The Trouble with Bias Crawford, Kate Neural Information Processing Systems, Long Beach 2017 https:/ /www.youtube.com/watch?v=fMym_BKWQzk COE_AI_Glossary Artificial Intelligence Glossary Council of Europe https:/ /www.coe.int/en/web/artificial-intelligence/glossary Kuehn,_Andreas Analyzing Bug Bounty Programs: An Institutional Perspective on the Economics of Software Vulnerabilities Kuehn, Andreas; Milton Mueller 2014 TPRC Conference Paper 2014 https:/ /papers.ssrn.com/sol3/papers.cfm?abstract_id=2418812 Kang,_Daniel Model Assertions for Monitoring and Improving ML Models Kang, Daniel; Deepti Raghavan; Peter Baili; Matei Zaharia 3rd MLSys Conference 2020 https:/ /arxiv.org/pdf/2003.01668.pdf MathWorks_Residual What Is Residual Analysis? MathWorks https:/ /www.mathworks.com/help/ident/ug/what-is-residual-analysis.html Nayak,_Pragati Concept Drift and Model Decay Detection using Machine Learning Algorithm Nayak, Pragati Aravind; Pavithra Sriganesh; Rakshitha K.M; Manoj Kumar M.V; Prashanth B S; Sneha H R 12th International Conference on Computing Communication and Networking Technologies (ICCCNT) 2021 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9580110 Hasan,_Raza Artificial Intelligence Based Model for Incident Response Hasan, Raza; Salman Mahmood; Akshyadeep Raghav; M. Asim Hasan 2011 International Conference on Information Management, Innovation Management and Industrial Engineering 2011 https:/ /ieeexplore.ieee.org/abstract/document/6114714 McCue_Colleen Data Mining and Predictive Analysis: Intelligence Gathering and Crime Analysis McCue, Colleen Butterworth-Heinemann 2007 https:/ /www.sciencedirect.com/topics/computer-science/domain-expertise#:~:text=2.1%20Domain%20Expertise,need%20to%20know%20your% 20stuff. Besse,_Philippe A Survey of Bias in Machine Learning Through the Prism of Statistical Parity Besse, Philippe; Eustasio del Barrio; Paula Gordaliza; Jean-Michel Loubes; Laurent Risser The American Statistician 76 2 188-198 2021 https:/ /www.tandfonline.com/doi/full/10.1080/00031305.2021.1952897 Muller,_Michael Designing Ground Truth and the Social Life of Labels Muller, Michael; Christine T. Wolf; Josh Andres; Michael Desmond; Narendra Nath Joshi; Zahra Ashktorab; Aabhas Sharma; Kristina Brimijoin; Qian Pan; Evelyn Duesterwald; Casey Dugan Proceedings of the 2021 CHI Conference on Human Factors in Computing System 1-16 2021 https:/ /doi.org/10.1145/3411764.3445402 Shalev-Shwartz,_Shai Understanding Machine Learning: From Theory to Algorithms Shalev-Shwartz, Shai; Shai Ben-David Cambridge Unversity Press 2014 https:/ /www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf SP800-160 Engineering Trustworthy Secure Systems Ross, Ron; Mark Winstead; Michael McEvilley NIST SP 800-160 2022 https:/ /nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-160v1r1.fpd.pdf Hard choices in artificial intelligence, Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz, https:/ /www.sciencedirect.com/science/article/pii/S0004370221001065?via% 3Dihub Khanna,_Anirudh A Study of Today's A.I. through Chatbots and Rediscovery of Machine Intelligence Khanna, Anirudh; Bishwajeet, Pandey; Kushagra, Vashishta; Kartik, Kalia; Bhale, Pradeepkumar; Teerath, Das International Journal of u-and e-Service, Science and Technology 8 7 277-284 2015 http:/ /article.nadiapub.com/IJUNESST/vol8_no7/28.pdf NBSIR_82-2582 An Overview of Computer Vision Gevarter, William B. NIST 82-2582 1982 https:/ /nvlpubs.nist.gov/nistpubs/Legacy/IR/nbsir82-2582.pdf Kou,_Yufeng Survey of fraud detection techniques Kou, Yufeng; Chang-Tien, Lu; Sirirat, Sirwongwattana; Yo-Ping, Huang IEEE International Conference on Networking, Sensing and Control 2 749-754 2004 https:/ /ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1297040 Merritt,_Linda Human capital management: More than HR with a new name. Merritt, Linda People and Strategy 30 2 14-16 2007 https:/ /www.proquest.com/docview/224596559?pq-origsite=gscholar&fromopenview=true CRS_AI Artificial Intelligence: Background, Selected Issues, and Policy Considerations Harris, Laurie A. Congressional Research Service 2021 https:/ /crsreports.congress.gov/product/pdf/R/R46795 IJMAE A Review of the Role of Marketing in Recruitment and Talent Acquisition. Alashmawy, Ahmad; Rashad, Yazdanifard International Journal of Management, Accounting and Economics 6 7 2019 https:/ /www.academia.edu/41261685/A_Review_of_the_Role_of_Marketing_in_Recruitment_and_Talent_Acquisition?from=cover_page Das,_Debashis A survey on recommendation system. Das, Debashis; Laxman, Sahoo; Sujoy, Datta. International Journal of Computer Applications 160 7 2017 https:/ /www.researchgate.net/profile/Debashis-Das-17/publication/313787463_A_Survey_on_Recommendation_System/links/5d7de0474585155f1e4de908/A-Survey-on-Recommendation-System.pdf Hyndman,_Rob Forecasting: principles and practice Hyndman, Rob J.; George, Athanasopoulos 14 2018 https:/ /books.google.com/books? hl=en&lr=&id=_bBhDwAAQBAJ&oi=fnd&pg=PA7&dq=Forecasting&ots=Tij_xmXIMJ&sig=c8LjAcmbLDC5QeH_xQno2l_gTr0#v=onepage&q=Forecasting&f=f alse Ustun,_Berk Actionable Recourse in Linear Classification Ustun, Berk; Spangher, Alexander; Liu, Yang Association of Computing Machinery Voight,_Paul The EU General Data Protection Regulation: A Practical Guide Voight,_Paul; von_dem_Bussche,_Axel Springer ECOA 12 CFR Part 1002 - Equal Credit Opportunity Act (Regulation B) Consumer Financial Protection Bureau Gill,_Navdeep A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrimination Testing Gill, Navdeep; Hall, Patrick; Montgomery, Kim; Schmidt, Nicholas MDPI ISO_IEC_38507 Information Technology - Governance of IT - Governance implications of the use of artificial intelligence by organizations ISO/IEC ISO 1 2022 https:/ /www.iso.org/obp/ui/#iso:std:iso-iecÊ¼38507Ê¼ed-1Ê¼v1Ê¼en Darnell_Coss_Hall The Future of Analytics Dan Darnell, Rafael Coss, Patrick Hall O'Reilly Media Inc. Ch. 4 2020 https:/ /www.oreilly.com/library/view/the-future-of/9781492091769/ch04.html Fed_Reserve Guidance on Model Risk Management Patrick M. Parkinson The Federal Reserve 2011 https:/ /www.federalreserve.gov/supervisionreg/srletters/sr1107a1.pdf Jennifer,_Hill Causal Inference: Overview Jennifer Hill, Elizabeth A. Stuart International Encyclopedia of the Social & Behavioral Sciences (Second Edition) 255-260 2015 https:/ /www.sciencedirect.com/science/article/pii/B9780080970868420957 AIRS_Penn Artificial Intelligence Risk & Governance AIRS The Warton School, University of Pennslyvania 2022 https:/ /ai.wharton.upenn.edu/artificial-intelligence-risk-governance/ ENISA Privacy by Design European Union Agency for Cybersecurity https:/ /www.enisa.europa.eu/topics/data-protection/privacy-by-design Furche,_Tim Data wrangling for big data: Challenges and opportunities. Furche, Tim; George, Gottlob; Leonid, Libkin; Giorgio, Orsi; Norman, Paton Advances in Database Technologyâ€”EDBT 2016Ê¼ Proceedings of the 19th International Conference on Extending Database Technology 473-478 2016 https:/ /www.research.manchester.ac.uk/portal/files/50447231/paper_94_1_.pdf Enrique Towards an integrated crowdsourcing definition. EstellÃ©s-Arolas, Enrique; Fernando, GonzÃ¡lez-LadrÃ³n-de-Guevara Journal of Information science 38 2 189-200 2012 https:/ /journals.sagepub.com/doi/full/10.1177/0165551512437638 Behdad Nature-inspired techniques in the context of fraud detection. Behdad, Mohammad; Luigi, Barone; Mohammed, Bennamoun; Tim, French. EEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42 6 1273-1290 2012 https:/ /ieeexplore.ieee.org/abstract/document/6392447?casa_token=-mPJh2Do05MAAAAA: c5Eyg4__i64XlBowloZbwwwKNe0KnqAQvjg5romygO6ymRKDl70np3DiDL0mipMXmdmeboDj1hGwa9I Woodward Biometrics: A look at facial recognition. Woodward Jr, John D., Christopher Horn, Julius Gatune, and Aryn Thomas RAND CORP SANTA MONICA CA 2003 https:/ /apps.dtic.mil/sti/citations/ADA414520 Sharma,_Lalita A survey of recommendation system: Research challenges. Sharma, Lalita; Anju, Gera International Journal of Engineering Trends and Technology (IJETT) 4 5 1989-1992 2013 https:/ /d1wqtxts1xzle7.cloudfront.net/38584474/IJETT-V4I5P132_1_-with-cover-page-v2.pdf? Expires=1656295756&Signature=QAnv0QzAEvSK2TSOMFygnk1AGUQrGWtz7PD7~bal6kX9SbiWAwd18feH3nB0kH4EQDA0rEPGrm3V9E38s4eY2C5i52NHA-jY0h2zBYAnLETt5PB1cAMjSZf4qaqNWZCPvQtOpaxTkhV6YB1MLDTmaFCAjKcqQfqm6WPQoax5VLh0hIbF-hlc8p5wXiW7fE9z~OeYpmTJld4doW94bG3OrDUY75EN9cptH-IHlldQRs~jF1POAkJUO5PL91PKVJatYAjeWQn7eNchB2TszrfnDbs6pjCyPXIIvi1WdnQzkKVN0F6N-CA~YtZ6yGKIxTjDuxdcJh0AtfD2cjxLDvNFCA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA Å½liobaitÄ—_IndrÄ— A survey on measuring indirect discrimination in machine learning Å½liobaitÄ—, IndrÄ— CoRR 2018 https:/ /arxiv.org/abs/1511.00148 Vorobeychik Adversarial machine learning Vorobeychik, Yevgeniy; Murat, Kantarcioglu Synthesis Lectures on Artificial Intelligence and Machine Learning 12 3 2018 https:/ /www.morganclaypool.com/doi/abs/10.2200/S00861ED1V01Y201806AIM039?casa_token=bcr3UzYRz6AAAAAA:M-sh4ANwQuRFYcXk18O4x_x6zu7Qq3P5ZC12MrCgTckeNFm9sXOCAkAFvHtMce7t1A3lpUt4MFA Zhang,_Yonggang Principal component adversarial example Zhang,_Yonggang; Xinmei, Tian; Ya, Li; Xinchao, Wang; Dacheng, Tao IEEE Transactions on Image Processing 29 4804-4815 2020 https:/ /ieeexplore.ieee.org/abstract/document/9018372?casa_token=MFIULJJsFBEAAAAA:GVCQa1Ccivt9ETPlYlBmLvK74iIDNzODP-6mo2B4o4nkZG69ORI-HliXK0iXb3GG-Q8csl2vxtBorQ NIST_SP_800 Sp 800-66 rev. 1. an introductory resource guide for implementing the health insurance portability and accountability act (hipaa) security rule Scholl, Matthew A.; Kevin M. Stine; Joan Hash; Pauline Bowen; L. Arnold Johnson; Carla Dancy Smith; Daniel I. Steinberg National Institute of Standards & Technology 2008 https:/ /dl.acm.org/doi/pdf/10.5555/2206281 ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nJain_Saachi Missingness Bias in Model Debugging Jain, Saachi; Hadi Salman; Eric Wong; Pengchuan Zhang; Vibhav Vineet; Sai Vemprala; Aleksander Madry ICLR 2022 https:/ /arxiv.org/abs/2204.08945 Mehrabi,_Ninareh A Survey on Bias and Fairness in Machine Learning Mehrabi, Ninareh ; Fred Morstatter; Nripsuta Saxena; Kristina Lerman; Aram Galstyan ACM Computing Surveys 54 6 1-35 2022 https:/ /dl.acm.org/doi/abs/10.1145/3457607 Lipton,_Zachary Does mitigating MLâ€™s impact disparity require treatment disparity? Lipton, Zachary C.; Alexandra Chouldechova; Julian McAuley 32nd Conference on Neural Information Processing Systems 2018 https:/ /doi.org/10.48550/arXiv.1711.07076 Gustavii,_Ebba A Swedish Grammar for Word Prediction Gustavii, Ebba; Eva Pettersson Uppsala University - Department of Linguistics 2003 https:/ /www.researchgate.net/publication/2838153_A_Swedish_Grammar_for_Word_Prediction/link/00b4951a5f2645ca02000000/download Comptroller_Office Comptroller's Handbook: Model Risk Management, Version 1.0 Office of the Comptroller of the Currency Comptroller's Handbook: Model Risk Management, Version 1.0 2021 https:/ /www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html CISA A Glossary of Common Cybersecurity Words and Phrases National Initiative for Cybersecurity Careers and Studies 2022 https:/ /niccs.cisa.gov/cybersecurity-career-resources/glossary#D GWUC Cyber Glossary The George Washington University Natural Security Archive - The Cyber Vault Project 2018 https:/ /nsarchive.gwu.edu/cyber-glossary-b SP-800-12 Sp 800-12. an introduction to computer security: The nist handbook. Guttman, Barbara; Edward A. Roback 1995 https:/ /dl.acm.org/doi/pdf/10.5555/2206203 Thomas_Edgar Research Methods for Cyber Security Thomas W. Edgar; David O. Manz 367-392 2017 https:/ /www.sciencedirect.com/science/article/pii/B9780128053492000157 Vredenburg,_Karel A survey of user-centered design practice Vredenburg, Karel; Ji-Ye, Ma; Paul W., Smith; Tom, Carey Proceedings of the SIGCHI conference on Human factors in computing systems 471-478 2002 https:/ /dl.acm.org/doi/abs/10.1145/503376.503460 ST04-015 Understanding Denial-of-Service Attacks CISA https:/ /www.cisa.gov/uscert/ncas/tips/ST04-015 Chandrasekaran,_Varun Chandrasekaran,_Varun; Kamalika, Chaudhuri; Irene, Giacomelli; Somesh, Jha; Songbai, Yan 29th USENIX Security Symposium (USENIX Security 20) 1309-1326 2020 https:/ /www.usenix.org/conference/usenixsecurity20/presentation/chandrasekaran FDA_Glossary Glossary of Computer System Software Development Terminology FDA 1995 https:/ /www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/inspection-guides/glossary-computer-system-software-development-terminology-895#_top Meinshausen,_Nicolai Causality from a distributional robustness point of view Meinshausen, Nicolai 2018 IEEE Data Science Workshop (DSW) 6-10 2018 https:/ /ieeexplore.ieee.org/abstract/document/8439889?casa_token=8tga5hcskjQAAAAA: PlpvmJtJNGKSNXiBdghGmy670MxN91PXc3ekZRVEXwOcLoJkh0sxDuWseVzr0EowGRCf8WR-eYLWeU4 Stacke,_Karin Measuring domain shift for deep learning in histopathology. Stacke, Karin; Gabriel, Eilertsen; Jonas, Unger; Claes, LundstrÃ¶m IEEE journal of biomedical and health informatics 25 2 325-336 2020 https:/ /ieeexplore.ieee.org/abstract/document/9234592?casa_token=RIQcYxte8lIAAAAA: 4t6IwNomEw95f3c1ir73BRReG7OzKecUzpVQS_Bk5zIEWA5R75uG-66g9irlblzDDVwu7ut4jAo2i_8 NASA_Soft_Standards Software Assurance and Software Safety Standard NASA Standard 2020 https:/ /standards.nasa.gov/standard/nasa/nasa-std-87398 PET_Handbook Handbook of privacy and privacy-enhancing technologies. Van Blarkom, G. W.; John J. Borking; JG Eddy Olk Privacy Incorporated Software Agent (PISA) Consortium, The Hague 198 2003 https:/ /andrewpatrick.ca/pisa/handbook/Handbook_Privacy_and_PET_final.pdf NIST_1500 NIST Big Data Interoperability Framework Wo L. Chang; Nancy Grady NIST 1 2019 https:/ /www.nist.gov/publications/nist-big-data-interoperability-framework-volume-1-definitions?pub_id=918927 Microsoft_Azure_documen tation_Detect_data_drift Detect data drift (preview) on datasets Microsoft Azure documentation 2022 https:/ /docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-datasets?tabs=python Egnyte Egnyte Egnyte Data Control: Definition and Benefits 2022 https:/ /www.egnyte.com/guides/governance/data-control IG1190M_AIOps_Decommis sion_v1.0.0 IG1190M AIOps Decommission v1.0.0 AI Operations IG1190M AIOps Decommission v1.0.0 2022 https:/ /www.tmforum.org/resources/reference/ig1190m-aiops-decommission-v1-0-0/ peak.ai AI decision making: the future of business intelligence Peak.AI AI decision making: the future of business intelligence 2022 https:/ /peak.ai/hub/blog/ai-decision-making-the-future-of-business-intelligence/ Hochreiter,_Sepp Toward a Broad AI Hochreiter, Sepp Communications of the ACM 65 4 56-57 2022 https:/ /cacm.acm.org/magazines/2022/4/259402-toward-a-broad-ai/fulltext Mitchell,_Eric Memory-Based Model Editing at Scale. Mitchell, Eric; Charles, Lin; Antoine, Bosselut; Christopher D., Manning; Chelsea, Finn International Conference on Machine Learning, PMLR 15817-15831 2022 https:/ /proceedings.mlr.press/v162/mitchell22a.html ISO/IEC_TS_5723Ê¼2022(en) ISO/IEC TS 5723Ê¼2022(en) Trustworthiness â€” Vocabulary ISO/IEC ISO/IEC 2022 https:/ /www.iso.org/obp/ui/#iso:std:iso-iec:tsÊ¼5723Ê¼ed-1Ê¼v1Ê¼en H20.ai_glossary H20.ai Glossary H20.ai H20.ai 2022 https:/ /docs.h2o.ai/h2o/latest-stable/h2o-docs/glossary.html TechTarget_Ivy_Wigmore AI Washing Ivy Wigmore TechTarget 2017 https:/ /www.techtarget.com/searchenterpriseai/definition/AI-washing Roman_V. _Yampolskiy_Unexplainabil ity Unexplainability and Incomprehensibility of Artificial Intelligence Roman V. Yampokskiy PhilArchive 2019 https:/ /philarchive.org/archive/YAMUAI Forbes_Kayvan_Alikhani Remember 'Cloud Washing'? It's Happening In RegTech Kayvan Alikhani Forbes 2019 https:/ /www.forbes.com/sites/forbestechcouncil/2019/10/14/remember-cloud-washing-its-happening-in-regtech/?sh=2a52bd3a796c Ripley,_Brian Introduction and Examples Brian D. Ripley Pattern Recognition and Neural Networks 6 1996 https:/ /archive.org/details/patternrecogniti0000ripl/page/6/mode/2up?q=training&view=theater C3.ai_Model_Training Model Training C3.ai C3.ai Glossary https:/ /c3.ai/glossary/data-science/model-training/ Pyle, _Dorian_Data_Preparation _as_a_Process Data Preparation as a Process Dorian Pyle Data Preparation for Data Mining 89-124 1999 https:/ /www.google.com/books/edition/Data_Preparation_for_Data_Mining/hhdVr9F-JfAC?hl=en&gbpv=1&dq=Binning%20is%20a% 20technique&pg=PA110&printsec=frontcover Wieringa,_Roel_J. Conceptual Frameworks Roel J. Wieringa Design Science Methodology for Information Systems and Software Engineering 73-92 2014 https:/ /www.google.com/books/edition/Design_Science_Methodology_for_Informati/xLKLBQAAQBAJ?hl=en&gbpv=1&dq=% 22Construct+validity+is+defined+by+Shadish+et+al+24+p+506+as+the+degree+to+which+inferences+from+phenomena+to+constructs+are+warranted% 22&pg=PA87&printsec=frontcover York, _Dan_Internet_Society What Is the Splinternet? And Why You Should Be Paying Attention Dan York Internet Society 2022 https:/ /www.internetsociety.org/blog/2022/03/what-is-the-splinternet-and-why-you-should-be-paying-attention/ Splinternets Key Definitions European Parliament, Directorate-General for Parliamentary Research Services, Perarnaud, C., Rossi, J., Musiani, F., et al. 'Splinternets': Addressing the Renewed Debate on Internet Fragmentation 2022 https:/ /op.europa.eu/en/publication-detail/-/publication/5a5bfaed-0d52-11ed-b11c-01aa75ed71a1/language-en Regularization_for_Deep_ Learning Regularization for Deep Learning Ian Goodfellow, Yoshua Bengio, and Aaron Courville Deep Learning 221-266 2016 https:/ /www.google.com/books/edition/Deep_Learning/omivDQAAQBAJ?hl=en&gbpv=1&dq=% 22Sparsity+in+this+context+refers+to+the+fact+that+some+parameters+have+an+optimal+value+of+zero%22&pg=PA229&printsec=frontcover Statistics_in_Plain_English Statistical Significance and Effect Size Timothy C. Urdan Statistics in Plain English 43-56 2001 https:/ /www.google.com/books/edition/Statistics_in_Plain_English/MwCK52n_wBEC?hl=en&gbpv=1&dq=% 22Statistical+significance+When+the+probability+of+obtaining+a+statistic+of+a+given+size+due+strictly+to%22&pg=PA55&printsec=frontcover The_SAGE_Encyclopedia_ of_Communication_Resear ch_Methods Relationships Between Variables Mike Allen, ed. The SAGE Encyclopedia of Communication Research Methods, Volume 1 2017 https:/ /www.google.com/books/edition/The_SAGE_Encyclopedia_of_Communication_R/4GFCDgAAQBAJ?hl=en&gbpv=1&dq=% 22Statistical+significance+refers+to+whether+a+relationship+between+two+or+more+variables+exists+beyond+a+probability+expected+by+chance% 22&pg=PA1413&printsec=frontcover The_Science_of_Algorithm ic_Trading_and_Portfolio_ Management Evaluation Robert Kissell The Science of Algorithmic Trading and Portfolio Management 2013 https:/ /www.google.com/books/edition/The_Science_of_Algorithmic_Trading_and_P/FKPND2zz9OoC?hl=en&gbpv=1&dq=% 22Back+testing+is+the+quantitative+evaluation+of+a+model%E2%80%99s+performance+both+from+a+statistical+and+trading+perspective% 22&pg=PA446&printsec=frontcover Introduction_to_Informati on_Systems Artificial Intelligence R. Kelly Rainer, Reiner R. Kelly, and Brad Prince Introduction to Information Systems: International Adaptation 410-450 2022 https:/ /www.google.com/books/edition/Introduction_to_Information_Systems/Y75VEAAAQBAJ?hl=en&gbpv=1&dq=% 22An+autonomous+vehicle+automobile+bus+tractor+combine+boat+forklift+etc+is+a+vehicle+capable+of+sensing+its+environment+and+moving+safely+wi th+little+or+no+human+input%22&pg=PA437&printsec=frontcover OED_snake_oil Snake oil Oxford English Dictionary Oxford English Dictionary 2022 https:/ /www-oed-com.proxygw.wrlc.org/view/Entry/95490133?redirectedFrom=%22snake+oil%22 Nick_Higham_1 What Is Rounding? Nick Higham Nick Higham 2020 https:/ /nhigham.com/2020/04/28/what-is-rounding/ Nick_Higham_2 What Is Stochastic Rounding? Nick Higham Nick Higham 2020 https:/ /nhigham.com/2020/07/07/what-is-stochastic-rounding/ pwc_Model_Risk_Manage ment_of_AI_and_ML_Syst ems Model Risk Management of AI and Machine Learning Systems PricewaterhouseCoopers Model Risk Management of AI and Machine Learning Systems 2020 https:/ /www.pwc.co.uk/data-analytics/documents/model-risk-management-of-ai-machine-learning-systems.pdf Toward_an_understanding _of_responsible_artificial_ intelligence_practices Toward an understanding of responsible artificial intelligence practices Yichuan Wang, Mengran Xiong, and Hossein G. T. Olya In: Bui, T.X., (ed.) Proceedings of the 53rd Hawaii International Conference on System Sciences. Hawaii International Conference on System Sciences (HICSS 2020), 20202-01-07 - 2020-01-10, Maui, Hawaii, USA. Hawaii International Conference on System Sciences (HICSS) 2020 https:/ /eprints.whiterose.ac.uk/162719/8/Toward%20an%20Understanding%20of%20Responsible%20Artificial%20Intelligence%20Practices.pdf Comparing_scores_and_re ason_codes Comparing scores and reason codes in credit scoring systems: NeuroDecision vs. unconstrained neural networks Equifax Comparing scores and reason codes in credit scoring systems: NeuroDecision vs. unconstrained neural networks 2020 https:/ /assets.equifax.com/marketing/US/assets/comparing_scores_whitepaper.pdf Machine_Learning_Interpr etability_with_H20_Driverl ess_AI Machine Learning Interpretability with H20 Driverless AI Patrick Hall, Navdeep Gill, Megan Kurka, and Wen Phan; edited by Angela Bartz Machine Learning Interpretability with H20 Driverless AI 2022 https:/ /docs.h2o.ai/driverless-ai/latest-stable/docs/booklets/MLIBooklet.pdf Pattern_Recognition_and_ Machine_Learning Introduction Christopher M. Bishop Pattern Recognition and Machine Learning 1-66 2006 DOD_TEVV Technology Investment Strategy 2015-2018 United States Department of Defense's Test and Evaluation, Verification and Validation (TEVV) Working Group Technology Investment Strategy 2015-2018 2015 https:/ /defenseinnovationmarketplace.dtic.mil/wp-content/uploads/2018/02/OSD_ATEVV_STRAT_DIST_A_SIGNED.pdf Fundamentals_of_Informat ion_Systems_Security Auditing, Testing, and Monitoring David Kim and Michael G. Solomon Fundamentals of Information Systems Security 216-250 2016 https:/ /www.google.com/books/edition/Fundamentals_of_Information_Systems_Secu/Yb4eDQAAQBAJ?hl=en&gbpv=1&dq=% 22Audit+logs+Defined+events+that+provide+additional+input+to+audit+activities%22&pg=PA233&printsec=frontcover AI_Ethics_Mark_Coeckelb ergh Glossary Mark Coeckelbergh AI Ethics 203-206 2020 https:/ /www.google.com/books/edition/AI_Ethics/Gs_XDwAAQBAJ?hl=en&gbpv=1&dq=%22Trustworthy+AI+Al+that+can+be+trusted+by+humans% 22&pg=PA206&printsec=frontcover Hands-On_Smart_Contract_Dev Fundamental Concepts of Blockchain Matt Zand, Xun (Brian) Wu, and Mark Anthony Morris Hands-On Smart Contract Development with Hyperledger Fabric V2 \n\nTechTarget_target_functio n target function TechTarget TechTarget 2018 https:/ /www.techtarget.com/whatis/definition/target-function IGI_Global_reference_clas s What is Reference Class IGI Global IGI Global https:/ /www.igi-global.com/dictionary/reference-class/35564 CPO_Magazine_Amar_Kan agaraj Data Remediation and Its Role in Data Security and Privacy CPO Magazine CPO Magazine 2022 https:/ /www.cpomagazine.com/data-protection/data-remediation-and-its-role-in-data-security-and-privacy/ DEV_ranking Machine learning (ML) applications: ranking DEV Community DEV Community 2022 https:/ /dev.to/mage_ai/machine-learning-ml-applications-ranking-238d productmanagerHQ_Josh_ Fechter What Does an AI Product Manager Do? Product Manager HQ Product Manager HQ https:/ /productmanagerhq.com/ai-product-manager/ Proxy_Discrimination Proxy Discrimination in the Age of Artificial Intelligence and Big Data Anya E. R. Prince; Daniel Schwarcz Iowa Law Review 105 3 2020 https:/ /ilr.law.uiowa.edu/print/volume-105-issue-3/proxy-discrimination-in-the-age-of-artificial-intelligence-and-big-data Forbes_Tracy_Kemp Four Skills Every Successful AI Product Owner Should Possess Tracy Kemp Forbes 2021 https:/ /www.forbes.com/sites/forbestechcouncil/2021/08/24/four-skills-every-successful-ai-product-owner-should-possess/?sh=65dbfe423d3d Merriam-Webster_underrepresented underrepresented Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/underrepresented HBR_Andrew_Burt_how_t o_ensure How to Ensure Your AI Doesnâ€™t Discriminate Andrew Burt Harvard Business Review https:/ /hbr.org/2020/08/how-to-ensure-your-ai-doesnt-discriminate Cadient_EEOC Understanding and Avoiding Adverse Impact in Employment Practices Michael Baysinger; Kristin Worrell Cadient https:/ /cadienttalent.com/resources/understanding-and-avoiding-adverse-impact Ben_Green_Yiling_Chen Algorithm-in-the-Loop Decision Making Ben Green; Yiling Chen The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20) https:/ /ojs.aaai.org/index.php/AAAI/article/view/7115 IT_Governance_Blog_Luke _Irwin Personal data vs Sensitive Data: Whatâ€™s the Difference? Luke Irwin IT Governance Blog https:/ /www.itgovernance.co.uk/blog/the-gdpr-do-you-know-the-difference-between-personal-data-and-sensitive-data OED_stereotype Stereotype Oxford English Dictionary Oxford English Dictionary https:/ /www-oed-com.proxygw.wrlc.org/view/Entry/189956?rskey=JmZ8YE&result=1#eid Machine_Learning_Master y_Jason_Brownlee Machine Learning Terminology from Statistics and Computer Science Jason Brownlee Machine Learning Mastery 2016 https:/ /machinelearningmastery.com/data-terminology-in-machine-learning/#:~:text=Row%3A%20A%20row%20describes%20a,problem%20domain% 20that%20you%20have. Banking_on_Your_Data_C hristopher_Gilliard Prepared Testimony and Statement for the Record of Christopher Gilliard, PhD. Hearing on \"Banking on Your Data: the Role of Big Data in Financial Services\" before the House Financial Services Committee Task Force on Financial Technology Christopher Gilliard Prepared Testimony and Statement for the Record of Christopher Gilliard, PhD. Hearing on \"Banking on Your Data: the Role of Big Data in Financial Services\" before the House Financial Services Committee Task Force on Financial Technology https:/ /www.congress.gov/116/meeting/house/110251/witnesses/HHRG-116-BA00-Wstate-GillardC-20191121.pdf ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nMerriam-Webster_pseudoscience pseudoscience Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/pseudoscience Cost_Management_ch15 Lean Accounting and Productivity Measurement Don R. Hansen; Maryanne M. Mowen; Dan L. Heitger Cost Management 2021 https:/ /www.google.com/books/edition/Cost_Management/HhQcEAAAQBAJ?hl=en&gbpv=1&dq=% 22Velocity+has+to+do+with+how+fast+a+product+can+be+delivered+to+the+market+and+quality+is+concerned+with+providing+a+nondefective+product+ with+the+desired+features+to+customers%22&pg=PA783&printsec=frontcover Towards_Productizing Towards Productizing AI/ML Models: An Industry Perspective from Data Scientists Filippo Lanubile, Fabio Calefato, Luigi Quaranta, Maddalena Amoruso, Fabio Fumarola, and Michele Filannino arXiv https:/ /arxiv.org/abs/2103.10548 The_Art_of_Software_Tes ting The Psychology and Economics of Program Testing Glenford J. Myers The Art of Software Testing 1979 https:/ /archive.org/details/artofsoftwaretes0000myer/page/4/mode/2up William_Hetzel An Introduction William C. Hetzel The Complete Guide to Software Testing , 2nd edition 1988 https:/ /archive.org/details/completeguidetos0000hetz/page/6/mode/2up?view=theater On_Hyperparameter_Opti mization On Hyperparameter Optimization of Machine Learning Algorithms: Theory and Practice Li Yang and Abdallah Shami arXiv https:/ /arxiv.org/pdf/2007.15745.pdf Security_Analysis_of_Subj ect_Access Security Analysis of Subject Access Request Procedures: How to Authenticate Data Subjects Safely When They Request for Their Data Coline Boniface, Imane Fouad, Nataliia Bielova, CÃ©dric Lauradoux, and Cristiana Santos \n\nPrivacy Technologies and Policy (7th Annual Privacy Forum, APF 2019, Rome, Italy, June 13â€“14, 2019, Proceedings) 2019 https:/ /www.google.com/books/edition/Privacy_Technologies_and_Policy/SW2cDwAAQBAJ?hl=en&gbpv=1&dq=% 22i+Impersonation+data+breach+A+malicious+individual+is+able+to+impersonate+a+legitimate+data+subject+to+the+data+controller% 22&pg=PA186&printsec=frontcover IEEE_Caught_in_the_Act Caught in the Act of an Insider Attack: Detection and Assessment of Insider Threat Philip A. Legg, Oliver Buckley, Michael Goldsmith, and Sadie Creese 2015 IEEE International Symposium on Technologies for Homeland Security (HST 2015 https:/ /ieeexplore-ieee-org.proxygw.wrlc.org/stamp/stamp.jsp?tp=&arnumber=7446229 Moradi_Samwald Post-hoc explanation of black-box classifiers using confident itemsets Milad Moradi; Matthias Samwald Expert Systems with Applications 165 2021 https:/ /reader.elsevier.com/reader/sd/pii/S0957417420307302? token=858E09A321B5727ECF889090AFB0B943768A26AFE15FB920B20176BD22F82449CE42841047FD99C51A01659C2ECE9695&originRegion=us-east-1&originCreation=20220918190335#b0035 Mind_on_Statistics Chapter 4 Jessica M. Utts and Robert F. Heckard Mind on Statistics (6th Edition) 2021 https:/ /www.google.com/books/edition/Mind_on_Statistics/npQMEAAAQBAJ?hl=en&gbpv=1&dq=% 22Practical+Versus+Statistical+Significance+Statistical+significance+does+not+necessarily+mean+that+the+relationship+between+the+two+variables+has+ practical+significance%22&pg=PA138&printsec=frontcover Jenna_Burrell How the machine 'thinks': Understanding opacity in machine learning algorithms Jenna Burrell Big Data & Society 1-12 2016 https:/ /journals.sagepub.com/doi/pdf/10.1177/2053951715622512 Hughes_Lavery_Critical_T hinking Glossary William Hughes and Jonathan Lavery Critical Thinking - Concise Edition 2015 https:/ /www.google.com/books/edition/Critical_Thinking_Concise_Edition/k0idCgAAQBAJ?hl=en&gbpv=1&dq=% 222+7+2+straw+man+a+fallacious+argument+which+irrelevantly+attacks+a+position+that+appears+similar+to+but+is+actually+different+from+an+opponen t%E2%80%99s+position+and+concludes+that+the+opponent%E2%80%99s+real+position+has+thereby+been+refuted%22&pg=PA271&printsec=frontcover NIST_SP_800-30_Rev_1 NIST Special Publication 800-30 Revision 1Ê¼ Guide for Conducting Risk Assessments NIST NIST Special Publication 800-30 Revision 1Ê¼ Guide for Conducting Risk Assessments 2012 https:/ /nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-30r1.pdf TechTarget_third_party third party TechTarget TechTarget Glossary 2014 https:/ /www.techtarget.com/whatis/definition/third-party Law_Insider_processing_e nvironment Processing Environment Insider Law https:/ /www.lawinsider.com/dictionary/processing-environment wachter_counterfactual_2 018 â€œCounterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR.â€ Wachter, S., B. D. M. Mittelstadt, and C. Russell. Harvard Journal of Law and Technology 31 2 2018 https:/ /ora.ox.ac.uk/objects/uuidÊ¼86dfcdac-10b5-4314-bbd1-08e6e78b9094 mills_study_2010 Proposed Internet Congestion Control Mechanisms. Mills, Kevin L, James J Filliben, Dong Yeon Cho, Edward Schwartz, and Daniel Genin. NIST SP 500-282. 2010 https:/ /doi.org/10.6028/NIST.SP.500-282 friedman_additive_2000 Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors) Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. The Annals of Statistics 28 2 337â€“407 2000 https:/ /doi.org/10.1214/aos/1016218223 kusner_counterfactual_201 7 Counterfactual Fairness Kusner, Matt J, Joshua Loftus, Chris Russell, and Ricardo Silva. Advances in Neural Information Processing Systems (NIPS) 2017 https:/ /proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html cronbach_construct_1955 Construct Validity in Psychological Tests. Cronbach, Lee J., and Paul E. Meehl. Psychological Bulletin 52 281â€“302 1955 https:/ /doi.org/10.1037/h0040957 fink_survey_2010 Survey Research Methods Fink, A. International Encyclopedia of Education (Third Edition) 152â€“160 2010 https:/ /doi.org/10.1016/B978-0-08-044894-7.00296-7 bordens_research_2010 Research Design and Methods: A Process Approach Kenneth S. Bordens, Bruce B. Abbott Book (Eighth Edition) 2011 https:/ /www.amazon.com/Research-Design-Methods-Process-Approach/dp/B008BLHYQ8 nist_statistics_2012 NIST/SEMATECH e-Handbook of Statistical Methods https:/ /doi.org/10.18434/M32189 symeonidis_MLOps_2022 MLOps - Definitions, Tools and Challenges Symeonidis, Georgios, Evangelos Nerantzis, Apostolos Kazakis, and George A. Papakostas In 2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC) 453â€“460 2022 https:/ /ieeexplore.ieee.org/document/9720902 hardt_equality_2016 Equality of Opportunity in Supervised Learning Hardt, Moritz, Eric Price, and Nati and Srebro Advances in Neural Information Processing Systems (NIPS) 3315-3323 2016 http:/ /papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf EEOC_Q&A_Employee_Sel ection Questions and Answers to Clarify and Provide a Common Interpretation of the Uniform Guidelines on Employee Selection Procedures Equal Employment Opportunity Commission Questions and Answers to Clarify and Provide a Common Interpretation of the Uniform Guidelines on Employee Selection Procedures https:/ /www.eeoc.gov/laws/guidance/questions-and-answers-clarify-and-provide-common-interpretation-uniform-guidelines Engineering_safety_in_ma chine_learning Engineering safety in machine learning Kush R. Varshney Information Theory and Applications Workshop (ITA), 2016 2016 https:/ /ieeexplore-ieee-org.proxygw.wrlc.org/document/7888195 DOD_Modeling_and_Simul ation_Glossary DoD Modeling and Simulation (M&S) Glossary United States Department of Defense DoD Modeling and Simulation (M&S) Glossary 1998 https:/ /web.archive.org/web/20070710104756/http:/ /www.dtic.mil/whs/directives/corres/pdf/500059m.pdf Model_Cards_for_Model_ Reporting Model Cards for Model Reporting Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru arXiv https:/ /arxiv.org/abs/1810.03993 David_Leslie_Morgan_Brig gs Explaining Decisions Made with AI: A Workbook (Use Case 1Êµ AI-Assisted Recruitment Tool) David Leslie; Morgan Briggs \n\nExplaining Decisions Made with AI: A Workbook (Use Case 1Êµ AI-Assisted Recruitment Tool) 2021 https:/ /arxiv.org/ftp/arxiv/papers/2104/2104.03906.pdf deeplearningbook_intro Introduction Ian Goodfellow, Yoshua Bengio; Aaron Courville Deep Learning 2016 https:/ /www.deeplearningbook.org/contents/intro.html privacy-enhancing_technologies Chapter 5Ê¼ Privacy-enhancing technologies (PETs) UK Information Commissioner's Office DRAFT Anonymisation, pseudonymisation and privacy enhancing technologies guidance 2022 https:/ /ico.org.uk/media/about-the-ico/consultations/4021464/chapter-5-anonymisation-pets.pdf Joseph_Rocca_Ensemble_ methods Ensemble methods: bagging, boosting and stacking Joseph Rocca Towards Data Science 2019 https:/ /towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205 google_dev_classification-true-false-positive-negative Classification: True vs. False and Positive vs. Negative Google Google Machine Learning Education Foundational Courses https:/ /developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative Public_Health_and_Inform atics_MIE_2021 A Preliminary Scoping Study of Federated Learning for the Internet of Medical Things Arshad Farhad; Sandra I. Woolley; Peter Andras Public Health and Informatics: Proceedings of MIE 2021 504-505 2021 https:/ /www.google.com/books/edition/Public_Health_and_Informatics/81A2EAAAQBAJ?hl=en&gbpv=1&dq=% 22Federated+learning+1+2+is+a+learning+model+which+addresses+the+problem+of+data+governance+and+privacy+by+training+algorithms+collaboratively +without+transferring+the+data+to+another+location%22&pg=PA504&printsec=frontcover Black's_Law_Dictionary_h arm harm The Law Dictionary / Blackâ€™s Law Dictionary Second Edition The Law Dictionary / Blackâ€™s Law Dictionary Second Edition https:/ /thelawdictionary.org/harm/ dataiku_ML_and_linear_m odels Machine Learning and Linear Models: How They Work (In Plain English) Katie Gross Dataiku 2020 https:/ /blog.dataiku.com/top-machine-learning-algorithms-how-they-work-in-plain-english-1 ORM_model_inventory Model Inventory Open Risk Manual https:/ /www.openriskmanual.org/wiki/Model_Inventory yields.io_model_validation What Is Model Validation? Eimee V Yields.io 2020 https:/ /www.yields.io/blog/what-is-model-validation/ Open_Risk_Manual_model _validation Model Validation Open Risk Manual Open Risk Manual https:/ /www.openriskmanual.org/wiki/Model_Validation misuse_of_public_data Implicit data crimes: Machine learning bias arising from misuse of public data Efrat Shimron; Jonathan I. Tamir; Ke Wang; Michael Lustig PNAS 119 13 2022 https:/ /www.pnas.org/doi/full/10.1073/pnas.2117203119 MIT_Protected_Attributes Module 3Ê¼ Pedagogical Framework for Addressing Ethical Challenges - Protected Attritbutes and \"Fairness Through Unawareness\" MIT Open Courseware Exploring Fairness in Machine Learning for International Development 2020 https:/ /ocw.mit.edu/courses/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/pages/module-three-framework/protected-attributes/ Practical_Law_protected_ class Protected Class Thomson Reuters Practical Law Thomson Reuters Practical Law https:/ /content.next.westlaw.com/practical-law/document/Ibb0a38daef0511e28578f7ccc38dcbee/Protected-Class? viewType=FullText&transitionType=Default&contextData=(sc.Default)&firstPage=true0law. Dave_Salvator_sparsity How Sparsity Adds Umph to AI Inference Dave Salvator NVIDIA Blogs 2020 https:/ /blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/ saurabh_label_2020 A Unified View of Label Shift Estimation Garg, Saurabh, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton Advances in Neural Information Processing Systems 2020 https:/ /proceedings.neurips.cc/paper/2020/hash/219e052492f4008818b8adb6366c7ed6-Abstract.html provost_data_2013 Data Science and its Relationship to Big Data and Data-Driven Decision Making Provost, Foster and Tom Fawcett Big Data 1 1 51â€“59 2013 https:/ /www.liebertpub.com/doi/full/10.1089/big.2013.1508 hameed_data_2020 Data Preparation: A Survey of Commercial Tools Hameed, Mazhar and Felix Naumann ACM SIGMOD Record 49 3 18â€“29 2020 https:/ /doi.org/10.1145/3444831.3444835 Merriam-Webster_ranking ranking Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/ranking hunter_differential_1979 Differential validity of employment tests by race: A comprehensive review and analysis Hunter, John E. and Frank L. Schmidt and Ronda Hunter Psychological Bulletin 86 721â€“735 1979 https:/ /psycnet.apa.org/record/1979-30107-001 kelley_dogfooding_2022 â€˜Dogfoodingâ€™ Kelley, Lora The New York Times 2022 https:/ /www.nytimes.com/2022/11/14/business/dogfooding.html wolfram_math_2022 Wolfram MathWorld: The Web's Most Extensive Mathematics Resource 2022 https:/ /mathworld.wolfram.com/ aivodji_fairwashing_2019 Fairwashing: the risk of rationalization Aivodji, Ulrich, Hiromi Arai, Olivier Fortineau, SÃ©bastien Gambs, Satoshi Hara, and Alain Tapp Proceedings of the 36th International Conference on Machine Learning 2019 https:/ /proceedings.mlr.press/v97/aivodji19a.html khalid_feature_2014 A survey of feature selection and feature extraction techniques in machine learning Khalid, Samina, Tehmina Khalil, and Shamila Nasreen. 2014 Science and Information Conference 372â€“378 2014 https:/ /ieeexplore.ieee.org/abstract/document/6918213 tabassi_adversarial_2019 A Taxonomy and Terminology of Adversarial Machine Learning Tabassi, Elham, Kevin Burns, Michael Hadjimichael, Andres Molina-Markham, and Julian Sexton. NIST Internal or Interagency Report (NISTIR) 8269 (Draft) 2019 https:/ /doi.org/10.6028/NIST.IR.8269-draft measurement_iso22989_20 22 ISO/IEC 22989Ê¼2022 Information technology â€” Artificial intelligence â€” Artificial intelligence concepts and terminology ISO/IEC 22989Ê¼2022 2022 https:/ /www.iso.org/standard/74296.html aime_measruement_2022 Notes on Measurement NIST AIME Team Unpublished Manuscript 2022 <None> saarela_feature_2021 Comparison of feature importance measures as explanations for classification models Mirka Saarela and Susanne Jauhiainen SN Applied Sciences 3 2021 https:/ /link.springer.com/article/10.1007/s42452-021-04148-9 poole_mackworth_observa tion 5.3.1 Background Knowledge and Observations David Poole and Alan Mackworth Artificial Intelligence: Foundations of Computational Agents 2010 https:/ /artint.info/html/ArtInt_112.html kathleen_walch_operationa lization Operationalizing AI Kathleen Walch Forbes 2020 https:/ /www.forbes.com/sites/cognitiveworld/2020/01/26/operationalizing-ai/?sh=42ea4b2733df about_ML_packages About ML Packages UiPath UiPath AI Center Guide 2021 https:/ /docs.uipath.com/ai-fabric/v0/docs/about-ml-packages TechTarget_data_point data point Katie Terrell Hanna and Ivy Wigmore TechTarget 2022 https:/ /www.techtarget.com/whatis/definition/data-point Morris_John_data_point What is a data point in a machine learning model? John Morris VProexpert 2022 https:/ /www.vproexpert.com/what-is-a-data-point-in-a-machine-learning-model/ Artasanchez_Joshi_AI_wit h_Python Natural Language Processing Alberto Artasanchez and Prateek Joshi \n\nArtificial Intelligence with Python: Your Complete Guide to Building Intelligent Apps Using Python 3.x, 2nd Edition 351-378 2020 https:/ /www.google.com/books/edition/Artificial_Intelligence_with_Python/P0fODwAAQBAJ?hl=en&gbpv=1&dq=% 22Lemmatization+is+the+process+of+grouping+together+the+different+inflected+forms+of+a+word+so+they+can+be+analyzed+as+a+single+item% 22&pg=PA356&printsec=frontcover Techopedia_lemmatization Lemmatization Techopedia Techopedia https:/ /www.techopedia.com/definition/33256/lemmatization Techslang_lemmatization What is Lemmatization? Techslang Techslang https:/ /www.techslang.com/definition/what-is-lemmatization/ TechTarget_lemmatization Lemmatization TechTarget contributor TechTarget 2018 https:/ /www.techtarget.com/searchenterpriseai/definition/lemmatization Lim_Swee_Kiat_harms Understanding Bias Part I Lim Sweet Kiat Machines Gone Wrong 2019 https:/ /machinesgonewrong.com/bias_i/#two-types-of-harms ICO_data_minimisation Data minimisation and privacy-preserving techniques in AI systems Information Commissioner's Office ICO AI Blog 2022 https:/ /ico.org.uk/about-the-ico/media-centre/ai-blog-data-minimisation-and-privacy-preserving-techniques-in-ai-systems/ EDPS_data_minimization Data Minimization European Data Protection Supervisor Data Protection Glossary https:/ /edps.europa.eu/data-protection/data-protection/glossary/d_en Arjun_Subramonian_bias_ mitigation An Introduction to Fairness and Bias Mitigation with AllenNLP Arjun Subramonian AI2Blog 2021 https:/ /blog.allenai.org/an-introduction-to-fairness-and-bias-mitigation-with-allennlp-d1b478d44d4c Chi,_Gao,_Ma Tik Tok Unwrapped Nicole Chi, Keming Gao, Joanne Ma Tik Tok Unwrapped 2022 https:/ /www.ischool.berkeley.edu/sites/default/files/sproject_attachments/tiktok_unwrapped_zine_final.pdf ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nTechTarget_decision_supp ort_system decision support system TechTarget TechTarget 2021 https:/ /www.techtarget.com/searchcio/definition/decision-support-system Burstein_Holsapple Spreadsheet-Based Decision Support Systems Frada Burstein and Clyde W. Holsapple Handbook on Decision Support Systems 1Êµ Basic Themes 2008 https:/ /www.google.com/books/edition/Handbook_on_Decision_Support_Systems_1/q_3sRkRKZQwC?hl=en&gbpv=0 Sourabh_Mehta_determini stic Deterministic vs Stochastic Machine Learning Sourabh Mehta Analytics India Magazine 2022 https:/ /analyticsindiamag.com/deterministic-vs-stochastic-machine-learning/ jansen_graphical_1998 The Graphical User Interface. Jansen, Bernard J. ACM SIGCHI Bulletin 30 2 22â€“26 1998 https:/ /doi.org/10.1145/279044.279051 rudin_interpretable_2022 Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges. Rudin, Cynthia, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and Chudi Zhong. Statistics Surveys 16 1â€“85 2022 https:/ /doi.org/10.1214/21-SS133 NISTIR_8312_Full Four Principles of Explainable Artificial Intelligence Phillips, P. Jonathon; Carina A. Hahn; Peter C. Fontana; David A. Broniatowski; Mark A. Przybocki NISTIR 8312 2021 https:/ /nvlpubs.nist.gov/nistpubs/ir/2021/NIST.IR.8312.pdf arun_opportunities_2020 Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey. Das, Arun, and Paul Rad. Arxiv 2020 https:/ /doi.org/10.48550/arXiv.2006.11371 open_risk_2022 Open Risk Manual: Midel Governancde Open Risk Manual 2022 https:/ /www.openriskmanual.org/wiki/Model_Governance merriam_webster_outcom e outcome Merriam-Webster Merriam-Webster Dictionary 2022 https:/ /www.merriam-webster.com/dictionary/outcome sutton_reinforcement_201 8 Reinforcement Learning: An Introduction Sutton, Richard, and Andrew Barto Book, Published by MIT Press 2018 Covert_et_al Towards a Triad for Data Privacy Quentin Covert, Mary Francis, Dustin Steinhagen, and Kevin Streff Proceedings of the 53rd Hawaii International Conference on System Sciences 2020 https:/ /scholarspace.manoa.hawaii.edu/server/api/core/bitstreams/5486a250-cc3c-4227-a752-7d08378afbdf/content Cami_Rosso The Human Bias in the AI Machine: How artificial intelligence is subject to cognitive bias Cami Rosso Psychology Today 2018 https:/ /www.psychologytoday.com/us/blog/the-future-brain/201802/the-human-bias-in-the-ai-machine NIST_CSRC_man-in-the-middle_attack man-in-the-middle attack (MitM) NIST Computer Security Resource Center NIST Computer Security Resource Center https:/ /csrc.nist.gov/glossary/term/man_in_the_middle_attack cambridge_dictionary_202 2 Cambridge Dictionary 2022 https:/ /dictionary.cambridge.org/us/ kulinski_feature_2020 Feature Shift Detection: Localizing Which Features Have Shifted via Conditional Distribution Tests Kulinski, Sean M., Saurabh Bagchi, and David I. Inouye NIPS 2020 https:/ /proceedings.neurips.cc/paper/2020/file/e2d52448d36918c575fa79d88647ba66-Paper.pdf settles_active_2009 Active Learning Literature Survey Burr Settles Technical Report, University of Wisconsim-Madison, Department of Computer Sciences 2009 https:/ /minds.wisconsin.edu/handle/1793/60660 informs_analytics_2022 Operations Research & Analytics INFORMS 2022 https:/ /www.informs.org/Explore/Operations-Research-Analytics guresen_definition_2011 Definition of Artificial Neural Networks with Comparison to Other Networks Guresen, Erkam, and Gulgun Kayakutlu Procedia Computer Science, World Conference on Information Technology 3 2011 https:/ /doi.org/10.1016/j.procs.2010.12.071 olson_pmlb_2017 PMLB: A Large Benchmark Suite for Machine Learning Evaluation and Comparison. Olson, Randal S., William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore BioData Mining 10 1 2017 https:/ /doi.org/10.1186/s13040-017-0154-4 devore_probability_2004 Probability and Statistics for Engineering and the Sciences Jay L. Devore Book, Published by Thompson, Brooks/Cole, Sixth Edition 2004 aggarwal_clustering_2013 Data Clustering: Algorithms and Applications Aggarwal, Charu C., and Chandan K. Reddy Book, Published by Chapman & Hall/CRC, First Edition 2013 techopedia_column_2022 Database Column Techopedia 2022 https:/ /www.techopedia.com/definition/8/database-column#:~:text=In%20the%20context%20of%20relational,documents%20or%20even%20video% 20clips. box_statistics_2005 Statistics for Experimenters: Design, Innovation, and Discovery George E. P. Box, J. Stuart Hunter, William G. Hunter Book, Published by Wiley, Second Edition 2005 james_statistical_2014 An Introduction to Statistical Learning: With Applications in R James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani Book, Published by Springer 2014 http:/ /www-bcf.usc.edu/~gareth/ISL/ friedler_comparative_2019 A Comparative Study of Fairness-Enhancing Interventions in Machine Learning Friedler, Sorelle A., Carlos Scheidegger, Suresh Venkatasubramanian, Sonam Choudhary, Evan P. Hamilton, and Derek Roth In Proceedings of the Conference on Fairness, Accountability, and Transparency 2019 https:/ /doi.org/10.1145/3287560.3287589 gong_differential_2020 A Survey on Differentially Private Machine Learning [Review Article]. Gong, Maoguo, Yu Xie, Ke Pan, Kaiyuan Feng, and A.K. Qin IEEE Computational Intelligence Magazine 15 2 2020 https:/ /doi.org/10.1109/MCI.2020.2976185 Bolukbasi_et_al_Debiasing _Word_Embeddings Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain 2016 https:/ /papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf google_glossary_2023 Machine Learning Glossary Google 2023 https:/ /developers.google.com/machine-learning/glossary wikipedia_graph_2023 Graph (discrete mathematics) Wikipedia Wikipedia 2023 https:/ /en.wikipedia.org/wiki/Graph_(discrete_mathematics)#Definitions Mark_Ciampa_2021 Utilizing Threat Data and Intelligence Mark Ciampa CompTIA CySA+ Guide to Cybersecurity Analyst (CS0-002) 2021 https:/ /www.google.com/books/edition/CompTIA_CySA+_Guide_to_Cybersecurity_Ana/NwpIEAAAQBAJ?hl=en&gbpv=1&dq=% 22In+cybersecurity+a+threat+actor+is+a+term+used+to+describe+individuals+or+entities+who+are+responsible+for+cyber+incidents+against+enterprises+ governments+and+users%22&pg=PA29&printsec=frontcover David_Lyon_2007 Security, Suspicion, Social Sorting David Lyon Surveillance Studies: An Overview 2007 https:/ /www.google.com/books/edition/Surveillance_Studies/_dTHJgh3-f0C?hl=en&gbpv=1&bsq=surveillance%20is Hartley_and_Zisserman_2 003 Projective Geometry and Transformations of 2D Richard Hartley; Andrew Zisserman Multiple View Geometry in Computer Vision 2003 https:/ /www.google.com/books/edition/Multiple_View_Geometry_in_Computer_Visio/si3R3Pfa98QC?hl=en&gbpv=1&dq=% 22Projection+along+rays+through+a+common+point+the+centre+of+pro+jection+defines+a+mapping+from+one+plane+to+another% 22&pg=PA34&printsec=frontcover Sloane_et_al_2020 Participation is not a Design Fix for Machine Learning Mona Sloane, Emanuel Moss, Olaitan Awomolo, Laura Forlano Proceedings of the 37th International Conference on Machine Learning, PMLR 2020 https:/ /arxiv.org/ftp/arxiv/papers/2007/2007.02423.pdf NIST_CSRC_parity parity NIST Computer Security Resource Center NIST Computer Security Resource Center https:/ /csrc.nist.gov/glossary/term/parity Dennis_Mercadal L Dennis Mercadal Dictionary of Artificial Intelligence 1990 https:/ /archive.org/details/dictionaryofarti0000merc/page/162/mode/2up?view=theater Ekaterina_et_al_2020 Why Are We Averse towards Algorithms? A Comprehensive Literature Review on Algorithmic Aversion Ekaterina Jussupow, Izak Benbasat, and Armin Heinzl Proceedings of the 28th European Conference on Information Systems (ECIS), An Online AIS Conference 2020 https:/ /aisel.aisnet.org/ecis2020_rp/168/ Gabriel_2020 Artificial Intelligence, Values, and Alignment Iason Gabriel Minds and Machines 30 2020 https:/ /link.springer.com/article/10.1007/s11023-020-09539-2 nist_800_2010 A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications. Rukhin, Andrew, Juan Soto, James Nechvatal, Miles Smid, Elaine Barker, Stefan Leigh, Mark Levenson, et al. NIST SP-800-22ra 2010 https:/ /doi.org/10.6028/NIST.SP.800-22r1a iso_2382_1997 Information technology â€” Vocabulary â€” Part 31Ê¼ Artificial intelligence â€” Machine learning ISO ISO/IEC 2382-31 1997 iso_22989_2022 Information technology â€” Artificial intelligence â€” Artificial intelligence concepts and terminology ISO ISO/IEC 22989 2022 https:/ /www.iso.org/standard/74296.html european_ethics_2019 Ethics Guidelines for Trustworthy AI High-Level Expert Group on Artificial Intelligence Draft Report 2019 https:/ /ec.europa.eu/futurium/en/ai-alliance-consultation.1.html Friedman_et_al_2017 A Survey of Value Sensitive Design Methods Batya Friedman, David G. Hendry, and Alan Borning Foundations and TrendsÂ® in Humanâ€“Computer Interaction 2017 https:/ /www.nowpublishers.com/article/Details/HCI-015 Bipartisan_Policy_Center_i mpact_assessments Explainer: Impact Assessments for Artificial Intelligence Sean Long, Jeremy Pesner, and Tom Romanoff Bipartisan Policy Center 2022 https:/ /bipartisanpolicy.org/blog/impact-assessments-for-ai/ Kimiz_Dalkir_2011 The Value of Knowledge Management Kimiz Dalkir Knowledge Management in Theory and Practice 2011 https:/ /www.google.com/books/edition/Knowledge_Management_in_Theory_and_Pract/_MrxCwAAQBAJ?hl=en&gbpv=1&dq=% 22Qualitative+measures+provide+more+context+and+details+about+the+value+e+g+perceptions+which+are+often+difficult+to+measure+quantitatively% 22&pg=PA343&printsec=frontcover Cost_Management_ch2 Basic Cost Management Concepts Don R. Hansen; Maryanne M. Mowen; Dan L. Heitger Cost Management 2021 https:/ /www.google.com/books/edition/Cost_Management/HhQcEAAAQBAJ?hl=en&gbpv=1&dq=% 22Qualitative+measurement+implies+the+use+of+data+expressed+in+categories+such+as+customer+reviews+of+new+model+jet+skis% 22&pg=PA38&printsec=frontcover Virginia_Dignum_Responsi bility_and_Artificial_Intelli gence Responsibility and Artificial Intelligence Virginia Dignum The Oxford Handbook of Ethics of AI 215-232 2020 https:/ /www.google.com/books/edition/The_Oxford_Handbook_of_Ethics_of_AI/8PQTEAAAQBAJ?hl=en&gbpv=1&dq=% 22Understanding+the+values+behind+the+technology+and+deciding+on+how+we+want+our+values+to+be+incorporated+in+AI+systems+requires+that+we +are+also+able+to+decide+on+how+and+what+we+want+AI+to+mean+in+our+societies%22&pg=PA221&printsec=frontcover yeom_avoiding_2021 Avoiding Disparity Amplification under Different Worldviews Yeom, Samuel, and Michael Carl Tschantz FAccT 2021Ê¼ In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency 273-283 2021 https:/ /doi.org/10.1145/3442188.3445892 Merriam-Webster_context context Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/context jacobs_measurement_2023 Measurement and Fairness Jacobs, Abigail Z., and Hanna Wallach FAccT 2021Ê¼ In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency 2021 https:/ /doi.org/10.1145/3442188.3445901 Merriam-Webster_impact impact Merriam-Webster Merriam-Webster https:/ /www.merriam-webster.com/dictionary/impact Lisa_M._Given_SAGE M: Mixed Methods Research Lisa M. Given The SAGE Encyclopedia of Qualitative Research Methods 491-538 2008 https:/ /www.google.com/books/edition/The_SAGE_Encyclopedia_of_Qualitative_Res/byh1AwAAQBAJ? hl=en&gbpv=1&dq=Mixed+methods+is+defined+as+research+in+which+the+inquirer+or+investigator+collects+and+analyzes+data,+integrates+the+findings, +and+draws+inferences+using+both+qualitative+and+quantitative+approaches+or+methods+in+a+single+study+or+a+program+of+study. &pg=PT584&printsec=frontcover NIST_AI_RMF_1.0 NIST AI RMF 1.0 NIST NIST AI RMF 1.0 2023 https:/ /nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf 45_CFR_46_2018_Require ments_ (2018_Common_Rule) 2018 Requirements (2018 Common Rule) United States Department of Health and Human Services (HHS) 45 CFR 46 2018 https:/ /www.hhs.gov/ohrp/regulations-and-policy/regulations/45-cfr-46/revised-common-rule-regulatory-text/index.html cambridge_causative_2023 Cambridge Dictionary: causative Cambridge Cambridge Dictionary 2023 https:/ /dictionary.cambridge.org/us/dictionary/english/causative lyons_contestability_2021 Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions Lyons, Henrietta, Eduardo Velloso, and Tim Miller Proceedings of the ACM on Human-Computer Interaction 2021 https:/ /doi.org/10.1145/3449180 cambridge_contestable_20 23 Contestable Cambridge Dictionary 2023 https:/ /dictionary.cambridge.org/us/dictionary/english/contestable Saleh_Alkhalifa_ML_in_Bi otech Supervised Machine Learning Saleh Alkhalifa \n\nMachine Learning in Biotechnology and Life Sciences: Build Machine Learning Models Using Python and Deploy Them on the Cloud 168-233 2022 https:/ /www.google.com/books/edition/Machine_Learning_in_Biotechnology_and_Li/KUVWEAAAQBAJ?hl=en&gbpv=1&dq=% 22We+can+define+supervised+learning+as+a+general+subset+of+machine+learning+in+which+data+like+its+associated+labels+is+used+to+train+models+th at+can+learn+or+generalize+from+the+data+to+make+predictions+preferably+with+a+high+degree+of+certainty%22&pg=PA168&printsec=frontcover Schneider_McGrew_in_Fla nagan_McDonough_2018 The Cattell-Horn-Carroll Theory of Cognitive Abilities W. Joel Schneider and Kevin S. McGrew; edited by Dawn P. Flanagan and Erin M. McDonough Contemporary Intellectual Assessment: Theories, Tests, and Issues 73-163 2018 https:/ /www.google.com/books/edition/Contemporary_Intellectual_Assessment/JA1mDwAAQBAJ?hl=en&gbpv=1&dq=% 22Formal+expertise+is+the+result+of+a+selfselection+of+a+domain+of+knowledge+that+is+mastered+deliberately+and+for+which+there+are+clear+bench marks+of+success+Fisher+Keil+2016%22&pg=PA117&printsec=frontcover Little_2013 The Measurement Model Todd D. Little Longitudinal Structural Equation Modeling 71-105 2013 https:/ /www.google.com/books/edition/Longitudinal_Structural_Equation_Modelin/gzeCu3FjZf4C?hl=en&gbpv=1&dq=%22Measurement+model% 22&pg=PA103&printsec=frontcover Merriam-Webster_executive executive Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/executive Dorf_2018 Measurement and Instrumentation Richard C. Dorf The Engineering Handbook 151-160 2018 https:/ /www.google.com/books/edition/The_Engineering_Handbook/l_TLBQAAQBAJ?hl=en&gbpv=1&dq=% 22The+propagation+of+uncertainty+is+defined+as+the+way+in+which+uncertainties+in+the+variables+affect+the+uncertainty+in+the+calculated+results% 22&pg=SA99-PA711&printsec=frontcover Merriam-Webster_example example Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/example Merriam-Webster_ethic ethic Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/ethic Merriam-Webster_anthropomorphis m anthropomorphism Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/anthropomorphism berthold_guide_2020 Guide to Intelligent Data Science: How to Intelligently Make Use of Real Data Berthold, Michael R., Christian Borgelt, Frank HÃ¶ppner, Frank Klawonn, and Rosaria Silipo Springer International Publishing 2020 https:/ /doi.org/10.1007/978-3-030-45574-3 alon-barkat_human_2023 Humanâ€“AI Interactions in Public Sector Decision Making: â€˜Automation Biasâ€™ and â€˜Selective Adherenceâ€™ to Algorithmic Advice. Alon-Barkat, Saar, and Madalina Busuioc Journal of Public Administration Research and Theory 33 1 153â€“169 2023 https:/ /doi.org/10.1093/jopart/muac007 humphrey_addressing_202 0 Addressing Harmful Bias and Eliminating Discrimination in Health Professions Learning Environments: An Urgent Challenge. Humphrey, Holly J., Dana Levinson, Marc A. Nivet, and Stephen C. Schoenbaum Academic Medicine 95 12S 2020 https:/ /doi.org/10.1097/ACM.0000000000003679 knuth_art_1981 The Art of Computer Programming, Volume 2Ê¼ Seminumerical Algorithms Donald Knuth Addison-Wesley 2 1981 garey_computers_1979 Computers and Intractability: A Guide to the Theory of NP-Completeness Michael Garey and David Johnson W. H. Freeman 1979 cambridge_impact_2023 Impact Cambridge Dictionary 2023 https:/ /dictionary.cambridge.org/us/dictionary/english/impact ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nlaw_policy_2023 Policy The Law Dictionary 2023 https:/ /thelawdictionary.org/policy/#:~:text=Definition%20%26%20Citations%3A,as%20directed%20to%20the%20POLICY fernandez_residual_1992 Residual Analysis and Data Transformations: Important Tools in Statistical Analysis Fernandez, George C. J. HortScience 27 4 297â€“300 1992 https:/ /journals.ashs.org/hortsci/view/journals/hortsci/27/4/article-p297.xml Leavy_OHQR_Intro Introduction Patricia Leavy The Oxford Handbook of Qualitative Research 1-13 2014 https:/ /academic.oup.com/edited-volume/38166/chapter-abstract/332997092?redirectedFrom=fulltext Barbour_2014 The scope and contribution of qualitative research Rosaline S. Barbour Introducing Qualitative Research: A Student's Guide, Second Edition 11-27 2008; 2014 https:/ /archive.org/details/introducingquali0000barb_j9h1/page/12/mode/2up?view=theater Merriam-Webster_engineer engineer Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/engineer AI_Incident_Editors Editor's Guide AI Incident Database AI Incident Database 2023 https:/ /incidentdatabase.ai/editors-guide/ interaction_context_2023 Context of Use Interaction Design: The Glossary of Human Computer Interaction Interaction Design Foundation 2023 https:/ /www.interaction-design.org/literature/book/the-glossary-of-human-computer-interaction/context-of-use AAAS_AI_and_Bias_2022-09 Artificial Intelligence and Bias â€“ An Evaluation M. Karanicolas and M. Knodel Artificial Intelligence and the Courts: Materials for Judges 2022 https:/ /doi.org/10.1126/aaas.adf0788 Seth_Boden_2020 Start Here: A Primer on Diversity and Inclusion (Part 1 of 2) Seth Boden Harvard Business Publishing 2020 https:/ /www.harvardbusiness.org/start-here-a-primer-on-diversity-and-inclusion-part-1-of-2/ GWU_diversity_and_inclus ion Diversity and Inclusion Defined George Washington University George Washington University Office for Diversity, Equity and Community Engagement https:/ /diversity.gwu.edu/diversity-and-inclusion-defined HUD_diversity_and_inclus ion Diversity and Inclusion Definitions U.S. Department of Housing and Urban Development U.S. Department of Housing and Urban Development https:/ /www.hud.gov/program_offices/administration/admabout/diversity_inclusion/definitions Jamieson_Govaart_Pownall Reflexivity in quantitative research: A rationale and beginner's guide Michelle K. Jamieson, Gisela H. Govaart, and Madeleine Pownall Social and Personality Psychology Compass e12735 2023 https:/ /doi.org/10.1111/spc3.12735 Industrial_Network_Securi ty_2011 Monitoring Enclaves Eric D. Knapp and Joel Langill \n\nIndustrial Network Security: Securing Critical Infrastructure Networks for Smart Grid, SCADA, and Other Industrial Control Systems 2011 https:/ /www.google.com/books/edition/Industrial_Network_Security/PlOtqouTwaUC?hl=en&gbpv=1&dq=% 22Data+retention+refers+to+the+amount+of+information+that+is+stored+long-term,+and+can+be+measured+in+volume+ (the+size+of+the+total+collected+logs+in+bytes)+and+time+(the+number+of+months+or+years+that+logs+are+stored+for).% 22&pg=PA243&printsec=frontcover ChatGPT ChatGPT OpenAI https:/ /chat.openai.com/chat Merriam-Webster_parity Parity Merriam-Webster Dictionary 2023 https:/ /www.merriam-webster.com/dictionary/parity barredo_explainable_2020 Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI Barredo Arrieta, Alejandro, Natalia DÃ­az-RodrÃ­guez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, et al. Information Fusion 58 82â€“115 2020 https:/ /doi.org/10.1016/j.inffus.2019.12.012 jackman_oxford_2008 Measurement Simon Jackman The Oxford Handbook of Political Methodology 2008 Hammarberg_2016_Busett o_2020 1. Qualitative research methods: when to use them and how to judge them; 2. How to use and assess qualitative research methods 1. K. Hammarberg, M. Kirkman, and S. de Lacey; 2. Loraine Busetto, Wolfgang Wick, and Christoph Gumbinger 1. Human Reproduction ; 2. Neurological Research and Practice 1. 31 1. 3; 2. 14 1. 2016; 2. 2020 1. https:/ /academic.oup.com/humrep/article/31/3/498/2384737; 2. https:/ /neurolrespract.biomedcentral.com/articles/10.1186/s42466-020-00059-z Russell_2003_Brannen_20 05 1. Evaluation of qualitative research studies; 2. Mixing Methods: The Entry of Qualitative and Quantitative Approaches into the Research Process 1. Cynthia K. Russell and David M. Gregory; 2. Julia Brannen 1. Evidence-Based Nursing ; 2. International Journal of Social Research Methodology 1. 6; 2. 8 2. 8 1. 36-40; 2. 173-184 1. 2003; 2. 2007 1. http:/ /dx.doi.org/10.1136/ebn.6.2.36; 2. https:/ /www.tandfonline.com/doi/full/10.1080/13645570500154642 Pamela_Goh_2021 Humans as the Weakest Link in Maintaining Cybersecurity: Building Cyber Resilience in Humans Pamela Goh. Edited by Majeed Khader, Loo Seng Neo, and Whistine Xiau Ting Chai \n\nIntroduction to Cyber Forensic Psychology: Understanding the Mind of the Cyber Deviant Perpetrators 287-305 2021 https:/ /www.worldscientific.com/doi/abs/10.1142/9789811232411_0014 Annie_Jacobsen_2015 Total Information Awareness Annie Jacobsen \n\nThe Pentagon's Brain: An Uncensored History of DARPA, America's Top-Secret Military Research Agency 336-352 2015 https:/ /archive.org/details/pentagonsbrainun0000jaco_c8o3/page/342/mode/2up?q=%22a+role-playing+exercise+in+which+a+problem+is+examined+from+an+adversary%E2%80%99s+or+enemy%E2%80%99s+perspective.%22 Ben_Auffarth_2021 Online Learning for Time-Series Ben Auffarth \n\nMachine Learning for Time-Series with Python: Forecast, Predict, and Detect Anomalies with State-of-the-art Machine Learning Methods 209-259 2021 https:/ /www.google.com/books/edition/Machine_Learning_for_Time_Series_with_Py/a7tLEAAAQBAJ?hl=en&gbpv=1&dq=% 22On+the+other+hand+offline+learning+the+more+commonly+known+approach+implies+that+you+have+a+static+dataset+that+you+know+from+the+start +and+the+parameters+of+your+machine+learning+algorithm+are+adjusted+to+the+whole+dataset+at+once+often+loading+the+whole+dataset+into+memo ry+or+in+batches%22&pg=PA210&printsec=frontcover FWS_062_FW_1 062 FW 1, Affirmative Employment Program and Plans Office for Human Resources of the U.S. Fish & Wildlife Service U.S. Fish & Wildlife Service 1996 https:/ /www.fws.gov/policy/062fw1.html Merriam-Webster_assessment assessment Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/assessment Anthropomorphism_in_AI_ 2020 anthropomorphism Arleen Salles, Kathinka Evers, and Michele Farisco AJOB Neuroscience 2 88-95 2020 https:/ /doi.org/10.1080/21507740.2020.1740350 OECD_Artificial_Intelligenc e_in_Society The technical landscape OECD Artificial Intelligence in Society 19-34 2019 https:/ /www.google.com/books/edition/Artificial_Intelligence_in_Society/eRmdDwAAQBAJ?hl=en&gbpv=1&dq=% 22Artificial+narrow+intelligence+ANI+or+applied+AI+is+designed+to+accomplish+a+specific+problem+solving+or+reasoning+task% 22&pg=PA22&printsec=frontcover AI_in_Medical_Imaging_gl ossary Glossary Erik R. Ranschaert, Sergey Morozov, and Paul R. Algra, eds. Artificial Intelligence in Medical Imaging: Opportunities, Applications and Risks 349-364 2019 https:/ /www.google.com/books/edition/Artificial_Intelligence_in_Medical_Imagi/ss6FDwAAQBAJ?hl=en&gbpv=1&dq=% 22The+definition+of+artificial+narrow+intelligence+is+in+contrast+to+that+of+strong+AI+or+artificial+general+intelligence+which+aims+at+providing+a+sy stem+with+consciousness+or+the+ability+to+solve+any+problems%22&pg=PA350&printsec=frontcover DOL_Practical_Significanc e Practical Significance in EEO Analysis Frequently Asked Questions U.S. Department of Labor Office of Federal Contract Compliance Programs U.S. Department of Labor Office of Federal Contract Compliance Programs 2021 https:/ /www.dol.gov/agencies/ofccp/faqs/practical-significance Cambridge_Dictionary_no n-discrimination non-discrimination Cambridge Dictionary Cambridge Dictionary https:/ /dictionary.cambridge.org/us/dictionary/english/non-discrimination Signal_Detection_Theory Signal Detection Theory N.A. Macmillan International Encyclopedia of the Social & Behavioral Sciences 14075-14078 2001 https:/ /doi.org/10.1016/B0-08-043076-7/00677-X Techopedia_kill_switch Kill Switch Techopedia Techopedia 2019 https:/ /www.techopedia.com/definition/4001/kill-switch Batya_Friedman_VSD_Intr oduction Introduction Batya Friedman and David G. Hendry Value Sensitive Design: Shaping Technology with Moral Imagination 1-17 2019 https:/ /doi-org.proxy.library.georgetown.edu/10.7551/mitpress/7585.003.0002 C3.ai_feedback_loop What Is a Feedback Loop? C3.ai C3.ai https:/ /c3.ai/glossary/features/feedback-loop/ Collins_Dictionary_ground _truth ground truth Collins Dictionary Collins Dictionary https:/ /www.collinsdictionary.com/us/dictionary/english/ground-truth Wikipedia_Decision-making Decision-making Wikipedia Wikipedia https:/ /en.wikipedia.org/wiki/Decision-making Shevlin_et_al_2019 The limits of machine intelligence Henry Shevlin, Karina Vold, Matthew Crosby, and Marta Halina EMBO Reports 20 10 e49177 2019 https:/ /www.ncbi.nlm.nih.gov/pmc/articles/PMC6776890/ EEOC_ADA_AI The Americans with Disabilities Act and the Use of Software, Algorithms, and Artificial Intelligence to Assess Job Applicants and Employees U.S. Equal Employment Opportunity Commission U.S. Equal Employment Opportunity Commission 2022 https:/ /www.eeoc.gov/laws/guidance/americans-disabilities-act-and-use-software-algorithms-and-artificial-intelligence Merriam-Webster_screen_out screen out Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/screen%20out apa_experiment_2023 experiment American Psychological Association (APA) APA Dictionary of Psychology 2023 https:/ /dictionary.apa.org/experiment APA_DoP_laboratory_rese arch laboratory research American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/laboratory-research UNODC_Glossary_QA_GL P Glossary of Terms for Quality Assurance and Good Laboratory Practices Laboratory and Scientific Section of the United Nations Office on Drugs and Crime Glossary of Terms for Quality Assurance and Good Laboratory Practices 2009 https:/ /www.unodc.org/documents/scientific/ST_NAR_26_E.pdf World_Wide_Words_In_si lico In silico World Wide Words World Wide Words http:/ /www.worldwidewords.org/weirdwords/ww-ins1.htm Bassiouni_Baffes_Evrard An Appraisal of Human Experimentation in International Law and Practice: The Need for International Regulation of Human Experimentation M. Cheriff Bassiouni, Thomas G. Baffes, and John T. Evrard Journal of Criminal Law and Criminology 72 4 1597-1666 1981 https:/ /scholarlycommons.law.northwestern.edu/cgi/viewcontent.cgi?article=6276&context=jclc Merriam-Webster_amplify amplify Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/amplify Gupta_et_al_HAR_2022 Human activity recognition in artificial intelligence framework: a narrative review Neha Gupta, Suneet K. Gupta, Rajesh K. Pathak, Vanita Jain, Parisa Rashidi, and Jasjit S. Suri Artificial Intelligence Review 55 4755â€“4808 2022 https:/ /link.springer.com/article/10.1007/s10462-021-10116-x Merriam-Webster_annotate annotate Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/annotate Freeman_et_al_2014 Active learning increases student performance in science, engineering, and mathematics Scott Freeman, Sarah L. Eddy, Miles McDonough, Michelle K. Smith, Nnadozie Okoroafor, Hannah Jordt, and Mary Pat Wenderoth PNAS 111 23 8410-8415 2014 https:/ /www.pnas.org/doi/full/10.1073/pnas.1319030111 AI_Assurance_2022 Assuring AI methods for economic policymaking Anderson Monken, William Ampeh, Flora Haberkorn, Uma Krishnaswamy, and Feras A. Batarseh AI Assurance: Towards Trustworthy, Explainable, Safe, and Ethical AI 371-428 2022 https:/ /www.google.com/books/edition/AI_Assurance/dch6EAAAQBAJ?hl=en&gbpv=1&dq=% 22Large+language+models+LLMs+are+a+class+of+language+models+that+use+deep+learning+algorithms+and+are+trained+on+extremely+large+textual+da tasets+that+can+be+multiple+terabytes+in+size%22&pg=PA376&printsec=frontcover Poore_Lawrence_ARLIS_2 023-01 AI Engineering: An Academic Research Roadmap Joshua Poore and Craig Lawrence Applied Research Laboratory for Intelligence and Security (ARLIS) 2023 Survey_of_Hallucination_i n_NLG Survey of Hallucination in Natural Language Generation Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung ACM Computing Surveys 55 12 1-38 2023 https:/ /dl.acm.org/doi/10.1145/3571730 APA_clustering clustering American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/clustering APA_content_validity content validity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/content-validity ISO_9241-11Ê¼2018 ISO 9241-11Ê¼2018(en) Ergonomics of human-system interaction â€” Part 11Ê¼ Usability: Definitions and concepts ISO ISO Online Browsing Platform 2018 https:/ /www.iso.org/obp/ui/#iso:std:isoÊ¼9241Ê¼-11Ê¼ed-2Ê¼v1Ê¼en APA_criterion_validity criterion validity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/criterion-validity APA_data_analysis data analysis American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/data-analysis APA_decision_making decision making American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/decision-making Lehto_Nanda_2021 Decision-Making Models, Decision Support, and Problem Solving Mark R. Lehto and Gaurav Nanda Handbook of Human Factors and Ergonomics , Fifth Edition 159-202 2021 https:/ /www.wiley.com/en-us/Handbook+of+Human+Factors+and+Ergonomics%2C+5th+Edition-p-9781119636083 Baron_Thinking_and_Deci ding Jonathan Baron Thinking and Deciding 2008 EO_DEIA_2021 Executive Order on Diversity, Equity, Inclusion, and Accessibility in the Federal Workforce Joseph R. Biden Jr. The White House 2021 https:/ /www.whitehouse.gov/briefing-room/presidential-actions/2021/06/25/executive-order-on-diversity-equity-inclusion-and-accessibility-in-the-federal-workforce/ APA_ethics ethics American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/ethics APA_external_validity external validity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/external-validity CSRC_false_negative False Negative NIST CSRC Information Technology Laboratory Computer Security Resource Center Glossary https:/ /csrc.nist.gov/glossary/term/false_negative CSRC_false_positive False Positive NIST CSRC Information Technology Laboratory Computer Security Resource Center Glossary https:/ /csrc.nist.gov/glossary/term/false_positive Wilke_Mata_2012 Cognitive Bias A. Wilke and R. Mata Encyclopedia of Human Behavior 1 531-535 2012 https:/ /s3.amazonaws.com/arena-attachments/557491/b16d97da35ed37a0a022e806cc931a0d.pdf AIID_incident_response Defining an \"AI Incident Response\" Sean McGregor Artificial Intelligence Incident Database https:/ /incidentdatabase.ai/research/5-response/ APA_integrity integrity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/integrity APA_internal_validity internal validity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/internal-validity APA_learning learning American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/learning McNamara_Fallacy The McNamara Fallacy Jonathan Cook The McNamara Fallacy 2023 https:/ /mcnamarafallacy.com/ Creswell_Clark_mixed_me thods John W. Creswell and Vicki L. Plano Clark Designing and Conducting Mixed Methods Research , Third Edition 2017 APA_observation observation American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/observation ID Title of article, chapter, or page Author(s) and/or Editor(s) Publication or website (either the main domain or major subdomain) Volume Issue Page(s) Year URL \n\nGlossary_of_Statistical_Te rms Glossary of Statistical Terms Philip B. Stark SticiGui 2019 https:/ /www.stat.berkeley.edu/~stark/SticiGui/Text/gloss.htm Wikipedia_RMSD Root-mean-square-deviation Wikipedia Wikipedia https:/ /en.wikipedia.org/wiki/Root-mean-square_deviation APA_recognition recognition American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/recognition APA_recall recall American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/recall APA_stereotype stereotype American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/stereotype Augoustinos_Walker_1998 The Construction of Stereotypes within Social Psychology: From Social Cognition to Ideology Martha Augoustinos and Iain Walker Theory & Psychology 8 5 629-652 1998 https:/ /doi.org/10.1177/0959354398085003 APA_autonomy autonomy American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/autonomy Charmaz_Henwood Grounded Theory Methods for Qualitative Psychology Kathy Charmaz and Karen Henwood The SAGE Handbook of Qualitative Research in Psychology 238-255 2017 https:/ /doi.org/10.4135/9781526405555 APA_reflexivity reflexivity American Psychological Association (APA) APA Dictionary of Psychology https:/ /dictionary.apa.org/reflexivity Lee_See_2004 Trust in Automation: Designing for Appropriate Reliance John D. Lee and Katrina A. See Human Factors: The Journal of the Human Factors and Ergonomics Society 46 1 50-80 2004 https:/ /doi.org/10.1518/hfes.46.1.50_30392 Mayer_Davis_Schoorman_ 1995 An Integrative Model of Organizational Trust Roger C. Mayer, James H. Davis, and F. David Schoorman The Academy of Management Review 20 3 709-734 1995 https:/ /doi.org/10.2307/258792 NISTIR_8280 NISTIR 8280. Face Recognition Vendor Test (FRVT). Part 3Ê¼ Demographic Effects Patrick Grother, Mei Ngan, and Kayee Hanaoka NIST 2019 https:/ /nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8280.pdf Usabilitygov Usability Testing Usability.gov Usabillity.gov https:/ /www.usability.gov/how-to-and-tools/methods/usability-testing.html Encyclopedia. com_underrepresentation Underrepresentation Encyclopedia.com Encyclopedia.com https:/ /www.encyclopedia.com/social-sciences/applied-and-social-sciences-magazines/underrepresentation Arham_Islam_History_202 3 A History of Generative AI: From GAN to GPT-4 Arham Islam MarkTechPost 2023 https:/ /www.marktechpost.com/2023/03/21/a-history-of-generative-ai-from-gan-to-gpt-4/ McKinsey_generative_AI What is generative AI? McKinsey & Company McKinsey & Company 2023 https:/ /www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-generative-ai Seshia_et_al_2022 Toward Verified Artificial Intelligence Sanjit A. Seshia, Dorsa Sadigh, and S. Shankar Sastry Communications of the ACM 65 7 46-55 2022 https:/ /cacm.acm.org/magazines/2022/7/262079-toward-verified-artificial-intelligence/fulltext Liam_Tung_2022_Meta_h allucination Meta warns its new chatbot may forget that it's a bot Liam Tung ZDNet 2022 https:/ /www.zdnet.com/article/meta-warns-its-new-chatbot-may-not-tell-you-the-truth/ Merriam-Webster_requirement requirement Merriam-Webster Merriam-Webster Dictionary https:/ /www.merriam-webster.com/dictionary/requirement TTC6_Taxonomy_Terminol ogy EU-U.S. Terminology and Taxonomy for Artificial Intelligence - Second Edition EU-US Trade and Technology Council (TTC) Working Group 1 (WG1) [1] Add citation to citations sheet and only list ID in these columns [2] Make sure the spelling matches another term (value in A column)", "fetched_at_utc": "2026-02-08T19:09:13Z", "sha256": "ca7880ea8842efa0458e4251b29a081d20a53817c005bc4bf63025d7b567d69e", "meta": {"file_name": "The Language of Trustworthy AI Glossary - NIST.pdf", "file_size": 640715, "relative_path": "pdfs\\The Language of Trustworthy AI Glossary - NIST.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 59669}}}
{"doc_id": "pdf-pdfs-the-protect-framework-managing-data-risks-in-the-ai-era-oliver-patel-56aa43c63ee4", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel.pdf", "title": "The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era \n\nhttps://oliverpatel.substack.com/p/the-protect-framework-managing-data  2/20 This weekâ€™s newsletter presents, for the first time, my PROTECT Framework for Managing Data Risks in the AI Era Â© 2025. Last week I published Part 1 of my 2-part series on the Top 10 Challenges for AI Governance Leaders in 2025. It focused on how the democratisation of AI, coupled with the sheer volume and rapid velocity of AI initiatives, is putting serious strain on the enterprise AI governance function. It outlined how, in response to these challenges, AI governance leaders must refine and update their risk-based approach and narrow the focus of their teamsâ€™ work, to avoid being overwhelmed, distracted, or neglecting the most serious risks. Although I promised Part 2 this week, I ended up writing a full article on the fourth challenge: protecting confidential business data. This is a hugely important topic and there was simply too much to say. The beauty of having your own Substack is that you can follow whichever creative or intellectual direction most appeals to you, rather than rigidly sticking to prior plans. I hope that you will indulge my partial detour and that you find value in this article for your work. The â€œrealâ€ Part 2, covering challenges 5-10, will have to wait another week. \n\nChallenge 4. Protecting Confidential Business Data \n\nHow can enterprises protect confidential business data when there is immense hunger to   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 3/20\n\nexperiment with and use the latest AI applications that are released on the market? \n\nThe generative AI boom has amplified and exacerbated a plethora of data risks that all enterprises are exposed to. It has never been more important to ensure that your AI governance, cybersecurity, and information security frameworks are designed to, and capable of, mitigating these risks. However, designing, implementing, and scaling robust controls that actually protect your organisationâ€™s data and intellectual property requires precise understanding of what these risks are and how they are impacted by AI. Thatâ€™s where my PROTECT Framework comes in. The PROTECT Framework empowers you to understand, map, and mitigate the most pertinent data risks that are fuelled by widespread adoption of generative AI. Below is a high-level summary of the framework, followed by a detailed breakdown of each of the 7 themes. \n\nPROTECT: Managing Data Risks in the AI Era \n\nThe PROTECT Framework focuses primarily on protecting ( no surprises there )confidential business data from exposure, disclosure, and misuseâ€”as well as associated data privacy and security risks fuelled by AI. It also outlines how   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 4/20\n\norganisations can use data in a compliant way, in the context of AI development, deployment, and use. \n\nP - Public AI Tool Usage R - Rogue Internal AI Projects O - Opportunistic Vendors T - Technical Attacks and Vulnerabilities E - Embedded Assistants and Agents C - Compliance, Copyright, and Contractual Breaches T - Transfer Violations \n\nThe rest of the article breaks down each of the seven themes, highlighting both the core risks that enterprises are exposed to, as well as practical mitigations that can be implemented to manage and control these risks. My forthcoming book, Fundamentals of AI Governance , provides a comprehensive visual overview of the PROTECT Framework, as well as a deep-dive on the Top 10 AI   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 5/20\n\nRisks impacting the modern enterprise. To secure a 25% discount during the pre-launch period, sign up at the link below. \n\nEnterprise risks: Use of publicly available AI tools is arguably the most severe data risk, because of how easy it is to do and how difficult it is to prevent. Simply put, there are thousands of publicly available AI tools that anyone can access via the internet, most of which are free or cheap. As well as mainstream generative AI chatbots like Claude, Gemini and ChatGPT, there are countless AI tools for creating presentations, managing email inboxes, transcribing meetings, and generating videos. No matter how mature your enterprise AI capabilities areâ€”and even if you are a frontier AI companyâ€” it is not going to be feasible to keep up with the latest and greatest AI tools and AI models that are released on the market each day, whilst also performing robust due diligence on AI vendors. Even with enterprise-grade licenses to mainstream generative AI services, you will not always get immediate access to the latest features included in the consumer version. This fuels immense hunger for employees to experiment with and use the most cutting-edge AI tools, irrespective of whether they are â€œinternalâ€ and approved or â€œpublicly availableâ€ and unapproved. \n\nP - Public AI Tool Usage   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 6/20\n\nThis inability to keep pace with the market, coupled with a lack of awareness regarding the risks of using publicly available AI tools and the pressure that employees and teams are under to become â€œAI-firstâ€, exacerbates the risks. Many employees may not understand the difference, from a data risk perspective, between using internal AI tools and public AI tools. But the risk is real. For example, when enterprise data is shared with publicly available AI tools, the organisation no longer has any control over what happens to it. This confidential business data could be used to train the AI models that power publicly available AI tools and in turn be disclosed, via future AI outputs, to competitors or malicious actors. It may even end up on the public internetâ€”as we saw when various shared AI chat logs were indexed and publicly accessible onlineâ€”or be retained indefinitely, as a result of court orders like the one OpenAI faced from the New York Court. Simply put, if you want to be in control of how your data and intellectual property is used, who has access to it, and for how long it is retained, your employees should avoid using publicly available AI tools. \n\nPractical mitigations: Although shadow AI use is ubiquitous, there are various controls you can implement to mitigate this risk. My 3 Gs for Governing AI Democratisation offers a useful starting point: \n\nGuidance: educate and train the workforce on the risks of using publicly available AI tools and the importance of protecting confidential business data.   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 7/20\n\nGreenlight: provide access to secure, best-in-class AI tools, platforms, and capabilities that are approved for internal use and can process confidential business data. \n\nGuardrails: implement guardrailsâ€”including technical and legal mitigationsâ€”to mitigate outstanding data risks that the use of internally approved AI tools entails. This can include scanning and blocking certain types of data from being uploaded as an input or generated as an output. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nEnterprise risks: Bypassing governance, especially when it happens at scale, creates compliance blind spots. Various risks emerge, and are difficult to manage, when teams develop and deploy AI systems, or procure AI solutions from vendors, without adhering to the mandatory AI governance, privacy, and cyber security processes. In such scenarios, there is unlikely to have been any legal or compliance review, privacy assessment, or security evaluation. In turn, this means that genuine risks are \n\nR - Rogue Internal AI Projects   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 8/20\n\nunlikely to be understood, required documentation and artefacts may not have been produced, and robust mitigations will not be in place to address any important risks. This increases organisational technical debt, which can lead to costly and burdensome efforts to retrospectively re-engineer non-compliant AI systems that are already deployed in production. Finally, it increases the likelihood that data is used without authorisation, and in a manner that constitutes a contractual breach or potential compliance violation. In most cases, this does not happen because of malicious internal actors or intentional rule-breaking. Rather, it is more likely due to enthusiasm, competitive internal pressures and competing priorities, or a lack of awareness of internal governance processes and how to navigate them. \n\nPractical mitigations: To mitigate the risk of rogue internal AI projects bypassing compliance checks, a proportionate degree of oversight must be applied to all AI projects. The level of governance scrutiny and oversight should flex in relation to the type of data being used. The use of sensitive personal data, confidential business data, or copyright protected material should entail more rigorous oversight, both at the project outset and throughout the AI lifecycle. Oversight should also be more rigorous for scenarios that involve sharing data with external vendors and the applications they provide.   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 9/20\n\nThe most important thing you can do is to make your AI and digital governance processes as easy to navigate as possible, by integrating different processes where possible, providing accessible guidance and support, and using automation to streamline processes and improve the user experience. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nEnterprise risks: Almost all enterprises must work with, and procure from, external organisations to progress with their AI ambitions. In the generative AI era, the trend is from build to buy. Whether it is leveraging pre-trained foundation models and generative AI chatbots, or working with vendors that provide bespoke AI products, exposure to third-party AI risk is unavoidable. In some cases, AI vendors and service providers may seek to use your confidential business data to train, develop, and improve their AI models and servicesâ€”potentially without your explicit knowledge or consent. \n\nO - Opportunistic Vendors   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 10/20\n\nThe terms of service for many AI platforms and products are ambiguous or difficult to understand, and grant vendors broad rights over customer data. The key risk is whether your organisationâ€™s data is used to train AI models that other customers can access and use. If so, competitors (or any other organisation) using that same vendorâ€™s products and services may benefit from insights derived from your data. The risk is lowerâ€”or potentially fully mitigatedâ€”if your data is only used to train AI models and services that only your organisation has access to. This can enable you to benefit from feature improvements and customisation whilst mitigating data exposure and leakage risks. \n\nPractical mitigations: Consider contractually prohibiting AI vendors from using your data (e.g., prompt, input, log, and output data), to train AI models and improve services that are accessible to other customers. This requires robust vendor due diligence, and a specific AI governance process pathway for AI procurement. It also requires clear guidance and training on third-party AI risks, acceptable data use terms, as well as template contracts and addendums that can be used across the business. Although the demand for AI vendors (and the products they provide) is high, the presence of opportunistic or shady operators in the marketâ€”and the immense value of the data they can obtain from enterprise customersâ€”makes rigorous due diligence and contractual safeguards essential.   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 11/20\n\nEnterprise risks: AI systems introduce novel attack vectors that are linked to the distinct vulnerabilities of these systems. In particular, generative AI and agentic AI systems can be compromised and exploited, leading to confidential dataâ€”that was part of the AI modelâ€™s training, input, or output dataâ€”being extracted and stolen. Such data exfiltration is a known and widely documented AI vulnerability and can be caused by various attack methods. Prompt injection attacks, for example, are when an AI model is provided with malicious inputs (during inference) that are designed to manipulate and steer the outputs it generates, by jailbreaking model guardrails. Simply put, the goal of prompt injection is to make the AI model do something that it is not supposed to. This includes, but is not limited to, data exfiltration, as well as reconstruction of training data. This risk of prompt injection is amplified with agentic AI, given the ability of AI agents to use tools and execute actions that can have a material impact (rather than â€œjustâ€ generate outputs for consumption). \n\nPractical Mitigations: Although there are no foolproof mitigations against prompt injection attacks, there are nonetheless important steps you can take. Consider implementing AI system-level security controls and guardrails including input \n\nT - Technical Attacks and Vulnerabilities   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 12/20\n\nvalidation, prompt sanitisation, output filtering, and incident monitoring and detection. Also, for high risk applications, conduct red-teaming and adversarial testing. Cybersecurity best practice emphasises the importance of multiple overlapping security layers. However, no technical controls can fully prevent prompt injection or data exfiltration. Therefore, carefully control who has access to sensitive AI systems, what data they can access, and what actions agentic AI can execute. \n\nEnterprise risks: The AI assistants and agents that are increasingly embedded in the core workplace software we all use pose novel data risks. In particular, these embedded AI tools can inappropriately disclose and disseminate data to people and groups that were not supposed to have access to it. For example, a personalised AI assistant that summarises your emails and daily tasks can analyse wider organisational data that you have access to, such as document libraries and shared calendars. If that data has not been protected with appropriate file sharing permissionsâ€”and moreover has erroneously been made available to the entire organisationâ€”then elements of it may be surfaced to you via your handy AI assistant. Although the root cause of this is often inappropriate or inadequate file sharing \n\nE - Embedded Assistants and Agents   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 13/20\n\npermissions, AI significantly increases the likelihood of such data being shared with the wrong person. It also provides malicious internal actors with a powerful tool for mischief. AI meeting assistants and note-taking applications are of particular concern. It is commonplace to join calls with external organisations, only to find that a random AI bot is also on the call, recording and transcribing everything that is said. From an enterprise perspective, this is akin to uploading all of this information into a publicly available AI tool (which poses similar risks to those outlined in â€˜Public AI Tool Usage â€™), unless you have assurances from the external organisation regarding the technology they are using and how it processes your data. Furthermore, be wary of individuals having access to AI meeting recordings and transcripts of parts of the discussion they were not part of. Increasingly autonomous agentic AI systems exacerbate these risks. In order to effectively determine the best course of action and use tools to execute tasks, LLM-based AI agents will need to mine, retrieve from, and synthesise myriad enterprise data sources. Establishing appropriate access controls, and maintaining AI agent audit trails, will become increasingly complex yet important. \n\nPractical mitigations: Robust data governance and data risk management is critical to ensuring your increasingly autonomous AI tools do not cause havoc. Ensuring   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 14/20\n\nappropriate file sharing permissions for sensitive and critical data sources is paramount, given the ways in which AI agents can mine through your document libraries and other repositories, as well as the obvious value this capability provides. Also, when deploying agentic AI, start with lower risk use cases, applications, and data sources. Furthermore, apply the principle of least privilege, to ensure that AI agents only have access to data that is necessary for their tasks. \n\nEnterprise risks: Data science, AI, and business teams are under significant pressure to leverage AI to deliver value for the business. To do so, they require seamless access to vast amounts of high-quality, business-critical data. However, the increasingly stringent data and AI regulatory landscapeâ€”particularly in the EUâ€”creates numerous compliance risks when using data for AI activities. It is therefore crucial to have robust controls in place to prevent unauthorised or non-compliant use of data. The most important regulatory and legal domains to consider are: \n\nPrivacy and data protection: Privacy and data protection laws restrict the way in which personal dataâ€”in particular sensitive personal dataâ€”can be used. For example, under the EUâ€™s GDPR, you must have a lawful basis to process personal data. Personal \n\nC - Compliance, Copyright, and Contractual Breaches   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 15/20\n\ndata processing is therefore only lawful if at least one of the following lawful bases apply: i) consent, ii) performance of a contract, iii) legal compliance, iv) protection of vital interests, v) performance of a task in the public interest, or vi) legitimate interests. Therefore, just because you have access to personal data, does not mean you are permitted by default to use it to develop or deploy AI. \n\nCopyright and intellectual property: Organisations must be cautious when using external data for AI development and deployment, as it is often copyright protected. Different data sources come with different licenses, terms, and conditions. This cautiousness must extend to â€œeverydayâ€ employee use of generative AI toolsâ€”in particular their prompts and document uploads. \n\nContracts: Your organisation may have access to data that another organisation provided in the course of an engagement, such as the use of a product or service you provide, that is governed by a bespoke legal agreement. Therefore, you must protect and handle that data in accordance with the applicable legal agreement. \n\nAI-specific laws: Finally, AI regulations like the EU AI Act typically include provisions that stipulate how data should be used in the context of AI activities. For example, the AI Act requires providers of high-risk AI systems to use high quality, accurate, and â€œrepresentativeâ€ training, validation, and testing data sets, in order to mitigate bias risks and promote reliable AI performance.   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 16/20\n\nPractical mitigations: The legal and regulatory domains outlined above are vast; comprehensive risk mitigation across all of them is beyond the scope of the PROTECT Framework. However, the overarching principle is that you must embed proportionate governance and oversight throughout the AI lifecycle, to prevent non-compliant or unauthorised data use. This means legal and compliance review must occur at critical stages, including before data is sourced, before AI models are trained, and before AI systems are deployed in production or released on the market. As ever, this governance must be complemented and reinforced by company-wide and role-specific training. When these governance checkpoints are bypassedâ€”as discussed in the â€˜Rogue Internal AI Projectsâ€™ themeâ€”the aforementioned data-related compliance and legal risks materialise. \n\nEnterprise risks: Leveraging cloud-based AI services, such as platforms for accessing and using foundation models, almost always involves international data transfers. Given that AI processing is rarely confined to one jurisdiction, navigating international data transfer compliance is an important part of AI governance. \n\nT - Transfer Violations   \n\n> 1/3/26, 5:24 PM The PROTECT Framework: Managing Data Risks in the AI Era\n> https://oliverpatel.substack.com/p/the-protect-framework-managing-data 17/20\n\nPrivacy and data protection regimes worldwide restrict the way in which personal data can be transferred internationally. Under the GDPR, organisations can transfer personal data freely from the EU to entities in a non-EU country if there is an EU adequacy decision in place. An adequacy decision is the EUâ€™s way of â€œ protecting the rights of its citizens by insisting upon a high standard of data protection in foreign countries where their data is processed â€. 15 jurisdictions are recognised as â€œadequateâ€ by the EU. This includes the U.S.â€”but it only applies to commercial organisations participating in and certified under the EU-U.S. Data Privacy Framework. If there is no EU adequacy decision, alternative legal safeguards must be put in place (e.g., Standard Contractual Clauses), before personal data can be transferred from the EU to the non-EU jurisdiction. On a separate note, the U.S. â€œBulk Data Transfer Ruleâ€ prohibits or restricts organisations from transferring â€œU.S. sensitive personal dataâ€ and â€œgovernment-related dataâ€ to â€œcountries of concernâ€, including China, Cuba, Iran, North Korea, Russia, and Venezuela. The Rule was issued by the Department of Justice in January 2025. \n\nPractical mitigations: Although the above was far from an exhaustive overview of international data transfer regulations, the key point is that you must implement specific mitigationsâ€”as required by the applicable lawâ€”prior to transferring personal data across borders. For example, before onboarding U.S. AI vendors and service", "fetched_at_utc": "2026-02-08T19:09:16Z", "sha256": "56aa43c63ee41a83ad105145f93f00e6c12e4aa66027e4251dce4277262ee1b3", "meta": {"file_name": "The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel.pdf", "file_size": 931949, "relative_path": "pdfs\\The PROTECT Framework - Managing Data Risks in the AI Era - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4756}}}
{"doc_id": "pdf-pdfs-the-ultimate-guide-to-ai-literacy-oliver-patel-bda94abbbf34", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\The Ultimate Guide to AI Literacy - Oliver Patel.pdf", "title": "The Ultimate Guide to AI Literacy - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel \n\nhttps://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy  2/20 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, subscribe below and share it with your colleagues and friends. Thank you for your support! This weekâ€™s edition is a practical guide to implementing and scaling an impactful AI literacy programme. It covers: \n\nâœ… What is AI literacy and why does it matter? \n\nâœ… What does the EU AI Act require? \n\nâœ… The 4 Layers of AI Literacy (the â€˜whatâ€™) \n\nâœ… 8 Practical Tips for AI Literacy Success (the â€˜howâ€™) \n\nâœ… The 3 Es for Impact: Educate, Engage, Empower (the â€˜whyâ€™) \n\nâœ… AI Literacy Cheat Sheet - scroll to the end to download the high-res pdf! \n\nPlease note that this analysis is geared towards larger organisations in the private and public sector, as opposed to educational institutions like schools and universities, which have a unique set of challenges and considerations.   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 3/20\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week. \n\nAI literacy is a crucial part of modern business in 2025. Any organisation investing in AI technology and implementation will struggle to maximise the value of AI without embedding AI knowledge, skills, and understanding across its workforce. If people donâ€™t know how to use AI, you are not going to achieve ROI. According to McKinsey , 48% of employees rate training as the most important factor driving their adoption of generative AI. However, many feel that they receive inadequate support. AI literacy means educating and upskilling employees on AI. A comprehensive, dual approach to AI literacy focuses on both AI risks and opportunities. Specifically, AI literacy deepens understanding of how to anticipate and mitigate AI risks, avoid harms, comply with regulations, and use AI in a safe and responsible way. It also advances \n\nWhat is AI literacy and why does it matter?   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 4/20\n\nunderstanding of what AI is, what it can and cannot do, how the technology is developing, and how it can be used in a positive, impactful, and transformative way. AI literacy matters because AI is far more sophisticated and complex than most other tools and technologies we use. AI can mimic, replicate, and even surpass the cognitive, intellectual, and creative abilities of human experts across an infinite number of tasks and topics, from coding to poetry. Across hundreds of thousands of years of human history, this has never before been possible. Therefore, unlike many other tools and technologies we use each day, safe, responsible, effective, and impactful use of AI is far from obvious or intuitive. It requires insight, understanding, and the curiousity to keep up with the latest developments. According to DataCamp :62% of leaders believe AI literacy is important for their teamsâ€™ daily tasks. Bear in mind this covers leaders working across many business functions, not just AI and data. â€˜Basic understanding of AI conceptsâ€™ and â€˜AI ethics and responsible AI best practicesâ€™ are ranked by leaders (70% and 69% respectively) as the two most important AI skills for their teams.   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 5/20\n\nArticle 4 of the EU AI Act is simply titled â€˜AI literacyâ€™. This humble article is undeniably a big part of why this has become such a hot topic. AI literacy is now mandatory. It was one of the first EU AI Act provisions to become applicable, alongside prohibited AI practices, on 2 February 2025. Here is the full text of Article 4: \n\nProviders and deployers of AI systems shall take measures to ensure, to their best extent, a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, education and training and the context the AI systems are to be used in, and considering the persons or groups of persons on whom the AI systems are to be used. \n\nAI literacy is also defined in Article 3(56) of the EU AI Act: \n\nâ€˜AI literacyâ€™ means skills, knowledge and understanding that allow providers, deployers and affected persons, taking into account their respective rights and obligations in the context of this Regulation, to make an informed deployment of AI systems, as well as to gain awareness about the opportunities and risks of AI and possible harm it can cause. \n\nWhat does the EU AI Act require?   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 6/20\n\nHere is a simplified breakdown of what this means and what you need to do. Organisations which develop and use AI must ensure their workforceâ€”especially those who develop, use, and operate high-risk AI systemsâ€”have the requisite skills, knowledge, and understanding to enable AI risk mitigation, regulatory compliance, and the protection of people from potential harms and other negative impacts of AI. Although organisations will not be fined for failure to comply with the AI literacy provision in Article 4 (as it is not explicitly covered in the penalty and enforcement regime), such non-compliance will certainly not help in the event of investigations, enforcement action, or legal proceedings relating to other EU AI Act provisions. Furthermore, it will be practically impossible to comply with many other aspects of the EU AI Act without implementing robust AI literacy. For example, Article 14 obliges deployers to: assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support .This effectively mandates tailored, role-based training for those tasked with oversight of high-risk AI systems. Although AI literacy should be interpreted as applying to the whole organisation, this is a key part of it. \n\nThe 4 Layers of AI Literacy   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 7/20\n\nThe 4 Layers of AI Literacy is a framework which I developed that describes the â€˜whatâ€™ of AI literacy. For your AI literacy programme to be robust, compliant, and effective, all 4 of these elements must be part of it. \n\n1. AI governance fundamentals \n\na. Mandatory training for the entire organisation. \n\nb. Educate the workforce on the key pillars of responsible AI and the AI policies and processes. The high level Doâ€™s and Donâ€™ts. \n\nc. Must be straightforward, accessible, and easy for all to understand. \n\n2. Generative AI empowerment \n\na. Upskill and empower the workforce to adopt and embrace AI tools and technologies, from generative AI to agentic AI. \n\nb. Incentivise uptake via gamification and leverage external expertise and resources. \n\nc. Must be inspirational, interactive, and hands-on. Provide safe environments for experimentation and discovery. \n\n3. Persona and role-based training   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 8/20\n\na. Tailored training for specific personas who build, buy, use, deploy, or govern AI as a core part of their work. \n\nb. Target key personas including AI governance, privacy, procurement, data scientists, and IT business partners. \n\nc. Must be engaging, practical, and relevant for the role, as well as likely scenarios which will arise. \n\n4. AI system specific \n\na. Mandatory training for end users and others responsible for operating high-risk AI systems. \n\nb. Bespoke instructions and guidance on implementing human oversight, transparency, and other risk mitigation measures. \n\nc. Must be context-specific, ensuring end users can interpret AI outputs and detect serious incidents. \n\nd. Requires collaboration between providers and deployers. AI literacy is about much more than generic company-wide training. Although this is important for compliance, it is only the first layer. For example, without implementing Layer 4, you cannot comply with the AI Act's human oversight requirements.   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 9/20\n\nThis AI literacy framework is not intended to cover absolutely everything. It should be complementary to relevant educational initiatives focusing on data literacy, cyber security awareness, ethics training etc. Now that you know what your AI literacy programme should consist of, here are 8 practical tips that will help you succeed. These tips are the â€˜howâ€™ of AI literacy. \n\n1. Focus on the â€˜so whatâ€™. Identify the end goal of AI literacy, then reverse engineer it. There is no point in rolling out AI literacy merely for the sake of it. In the planning phase, carefully consider what you are hoping to achieve and design your programme accordingly. Consider the skills, capabilities, and knowledge you need to embed and the type of organisation you want to become. Critical objectives should include: ensuring all employees have a baseline understanding of responsible and compliant use of AI; driving understanding regarding what AI is, how to use it effectively, which AI tools and technologies are available, and how to identify promising use cases; \n\n8 Practical Tips for AI Literacy Success   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 10/20\n\nproviding employees with high-quality training and development opportunities, to boost morale, engagement, and retention; and preparing the workforce for the jobs of the future and fostering the skills required to succeed. A world-class AI literacy programme will be designed and structured to achieve all of these objectives. But if your resources are constrained, start with the first. \n\n2. Diversify your content, formats, and pathways. Combine live, self-paced, virtual, and in-person learning. We have never had access to more high-quality, engaging, and free educational content. Your audienceâ€”whose attention you are competing forâ€”has high standards for what keeps them engaged. Therefore, your content needs to be at least as good, if not better than, what can be found externally. The best thing you can do is diversify. Offer a range of different types of content, including live keynotes, panel discussions, workshops, in-person events and conferences, and online modules with videos, infographics, and synthesised materials. Traditional teaching and learning methods are becoming archaic in the era of YouTube, TikTok, DuoLingo, and AI coaching apps; meet your learners where they are. \n\n3. Leverage internal and external experts. Partner with leading educational institutions. There are thought leaders and experts all over your organisation. Your job is to find them and give them a voice. Your AI upskilling initiatives represent an ideal opportunity to provide a platform to individuals and teams   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 11/20\n\nleading on impactful and transformative work. Being proactive in convening a broad range of voices is win-win. Your learners will benefit from a richer experience and you will forge strong relationships with key stakeholders across different functions, who will appreciate the opportunity for visibility. You should also mix it up by bringing in external experts, who can offer thought-provoking, challenging, and even divergent views. This could be leading academics and researchers, policymakers and regulators, or industry leaders from other sectors. \n\n4. Gamify your learning pathways to drive engagement and incentivise uptake. \n\nGamification, defined as â€œusing game design elements in non-game contextsâ€, is an increasingly popular approach to designing and delivering effective learning experiences, in both educational and professional settings. Gamification covers a broad range of different techniques, including points, badges, and leaderboards (PBL), levels, feedback, challenges, and even missions. Many studies prove that introducing such elements leads to more engaged, motivated, and proactive learners. To keep your learning participants inspired and excited, and to generate momentum across the organisation, offer badges, accreditations, and awards, for a more meaningful and rewarding learning journey. \n\n5. Hands-on, practical, and interactive learning is always most effective and impactful. If it's not hands on, it will quickly be forgotten. The biggest flaw with most AI trainings is that participants do nothing with AI. This makes no sense. You need to be creative in incorporating practical and scenario-based activities, which   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 12/20\n\ngive people a chance to experiment with AI tools, understand their capabilities and limitations, and tackle realistic problems and challenges. Simply learning how to 'prompt' is not going to cut it in 2025. Get people involved in deeper activities, such as hackathons, AI model red-teaming, reviewing live cases, AI incident response, use case ideation, and product development workshops. \n\n6. Enable tailored, role-based learning to be provided for every function and business unit. Different people and teams will require specialisation in different aspects of AI and AI governance, based upon the core focus of their role or work. You should offer advanced pathways which cater to different audiences. You should also provide frameworks, tools, and resources which enable every function to develop bespoke, role-based AI literacy programmes, building on the company-wide curriculum. You wonâ€™t have the bandwidth to create training for every group, but you can enable and empower this work to be done by others (e.g., local L&D teams). \n\n7. Build a community of engaged learners and create spaces for vibrant discussions. Community building is the best way you can elevate AI literacy beyond a training programme. Content on AI is abundant and never-ending. There is no shortage of educational materials for people to study and consume. What is harder to come by is opportunities for meaningful and in-depth discussions, with people from many different geographies, departments, roles, and backgrounds. Given the concurrently high levels of concern, confusion, and   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 13/20\n\nexcitement about the impact of AI on the future of work, keeping the discussion going and encouraging this collaboration is key. \n\n8. Provide successful learners with tangible opportunities for impact and career growth. There should be ample opportunities for employees to apply what they have learnt, to benefit their career and the wider organisation. Donâ€™t just reward and acknowledge with certificates and badges, without providing a meaningful follow up or next step. Connect AI literacy and upskilling achievements with opportunities for promotion, career advancement, and change. For example, empower people with specific roles or leadership positionsâ€”such as mentoring and guiding others, chairing forums, championing AI initiatives, and leading on parts of future AI literacy initiativesâ€”which they can perform alongside their day job. Alongside gamification, providing meaningful opportunities for growth and impact is the best way to keep learners engaged. Weâ€™ve covered the â€˜whatâ€™ (The 4 Layers of AI Literacy) and the â€˜howâ€™ (8 Practical Tips for AI Literacy Success). To conclude, we will focus on the â€˜whyâ€™. AI governance is change management. And AI literacy and upskilling is at the heart of driving that cultural change. \n\nThe 3 Es for Impact: Educate, engage, empower   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 14/20\n\nFirst and foremost, AI literacy is about serving others. Your role is to ensure that colleagues across your organisation feel like they are playing an active part in the AI revolution, rather than it being something passively happening to them, or worse, passing them by. You can use my 3 Es for Impact framework to shape and guide your approach, to ensure you fulfil this overall objective of serving others. Prioritising these 3 Es is vital for delivering the meaningful change and impact you hope to achieve, for your organisation and workforce. \n\nEducate: It is challenging to track and keep up with all of the developments in AI and AI governance, even for professionals working at the cutting-edge. It is even harder to cut through the noise and figure out what trends and developments really matter. Therefore, by synthesising and simplifying the vast amount of AI-related educational content which is out there, rendering it digestible, useful, and actionable for your organisation, and outlining the foundational and conceptual basis required for deeper understanding, you are providing immense value. \n\nEngage: Be creative and use all of the tools and resources at your disposal to cultivate buzz and excitement, motivate learning, facilitate community building and, ultimately, engage learners with a diverse range of practical and interactive offerings, delivered by a diverse cast of educators. As an AI governance leader, you cannot bury your head in   \n\n> 1/3/26, 4:51 PM The Ultimate Guide to AI Literacy - by Oliver Patel\n> https://oliverpatel.substack.com/p/the-ultimate-guide-to-ai-literacy 15/20\n\nthe sand. Being an impactful, visible, and engaged thought leader in your organisation is non negotiable. \n\nEmpower: AI literacy should be an empowerment programme, not a training programme. There is a palpable sense of concern, and even fear, about the impact of AI on the future of careers and work. People are uncertain regarding which jobs will be automated or outsourced, what skills they should develop, and what to prioritise. \n\nAccording to Pew Research , 52% of employed adults are worried about the future impact of AI in the workforce. By providing a safe and inspiring space for people across the organisation to come together and discuss, debate, and collaborate, you are empowering people to shape and take control of their future. This action drives empowerment. \n\nAI Literacy Cheat Sheet", "fetched_at_utc": "2026-02-08T19:09:34Z", "sha256": "bda94abbbf34e7a7e63a114749152367dff0ac99df9da45ac1e4eb58260486ac", "meta": {"file_name": "The Ultimate Guide to AI Literacy - Oliver Patel.pdf", "file_size": 1523395, "relative_path": "pdfs\\The Ultimate Guide to AI Literacy - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4047}}}
{"doc_id": "pdf-pdfs-u-s-ai-law-policy-explained-oliver-patel-77b328bb624a", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\U.S. AI Law & Policy Explained - Oliver Patel.pdf", "title": "U.S. AI Law & Policy Explained - Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel \n\nhttps://oliverpatel.substack.com/p/us-ai-law-and-policy-explained  2/27 This free newsletter delivers practical, actionable, and timely insights for AI governance professionals. My goal is simple: to empower you to understand, implement, and master AI governance. If you havenâ€™t already, sign up below and share it with your colleagues. Thank you! A lot has changed with respect to U.S. AI policy in recent months; and it can be hard for AI governance professionals to keep up. To help navigate these recent shifts, this article provides a comprehensive, up-to-date, and accessible overview of U.S. federal AI law and policy. It does not cover U.S. state AI laws and initiatives, which will be the focus of next weekâ€™s edition of Enterprise AI Governance. Despite lacking comprehensive EU-style regulation, the U.S. does have several important AI laws. In fact, there have been dozens of federal laws, regulations, and initiatives on AI, many of which will be discussed below.   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 3/27\n\nAside from President Bidenâ€™s (ultimately unsuccessful) attempts to initiate private sector AI regulation, the U.S. governmentâ€™s core focus in recent years has been on maintaining and promoting U.S. AI leadership, restricting the export of AI-related technologies, and encouraging responsible and innovative federal government use of AI. The Trump administration, which is primarily concerned with strengthening â€œU.S. global AI dominanceâ€, is pivoting away from the Biden administrationâ€™s AI governance and safety agenda. However, this does not mean that AI governance is completely off the menu. Its importance was stressed in a recent memorandum published by the White Houseâ€™s Office of Management and Budget, which described effective AI governance as â€key to accelerated innovationâ€. This overview covers: \n\nâœ… U.S. global AI leadership and the race with China \n\nâœ… Bidenâ€™s AI governance agenda \n\nâœ… Trumpâ€™s agenda: what â€˜America Firstâ€™ means for AI \n\nâœ… AI export controls and investment restrictions \n\nâœ… Federal government use and acquisition of AI \n\nâœ… NIST and the U.S. AI Safety Institute \n\nâœ… Whatâ€™s coming next?   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 4/27\n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts each week. \n\nThe U.S. is the undisputed global AI leader, by nearly every metric. The global AI industry is dominated by U.S. companies, AI models, and hardware. Here are some stats from Stanfordâ€™s 2025 AI Index Report , to illustrate the point: In 2024, U.S. private investment in AI was $109 billion. In contrast, it was $9.3 billion in China and $4.5 billion in the UK. U.S. organisations produced 40 â€˜notableâ€™ AI models in 2024, significantly more than Chinaâ€™s 15 and Europeâ€™s 3. However, in some areas, China is quickly catching up and the U.S. is taking nothing for granted. For example, although U.S. AI model production output is higher, Chinaâ€™s advanced AI models are getting closer to U.S. models in terms of quality ( see chart below ). \n\nU.S. global AI leadership and the race with China   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 5/27\n\nSource: Stanford AI Index Report 2025 [ original ]\n\nDeepSeek recently demonstrated that it can develop AI models, which have similar performance capabilities to leading U.S. models, at a fraction of the cost. This prompted a sell-off in U.S. tech stocks, with the S&P 500 falling 1.5% on the day DeepSeek released its open-source R1 model.   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 6/27\n\nFurthermore, China is charging ahead in AI talent, research, and patent filing. \n\nIncreasing numbers of â€œtop-tierâ€ AI researchers originate from China ( see chart below ), with the share originating from the U.S. declining in recent years. Also, 300,510 AI-related patents were filed in China in 2024, compared with 67,773 in the U.S.   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 7/27\n\nImage source: Information Technology & Innovation Foundation, 2025 [ original ]\n\nThese trends explain one of the core drivers behind much of the U.S. AI policy agenda in recent years, from AI export controls and investment restrictions, to substantial support for AI infrastructure funding. Although there is no comprehensive federal AI law, like the EU AI Act, there have been a number of federal laws and initiatives, across the past few administrations, which seek to maintain and strengthen the U.S. position of global leadership. Some of the most relevant laws include: \n\nExecutive Order 14141 : Advancing United States Leadership in AI Infrastructure \n\nThis Executive Order was signed by President Biden in January 2025. At the time of writing, it has not been revoked by President Trump. The purpose of this Executive Order is to promote and encourage domestic AI infrastructure development, to â€œprotect U.S. national securityâ€ and â€œadvance U.S. economic competitivenessâ€. This includes using federal sites to build data centres for AI and prioritising clean energy techniques. \n\nCHIPS and Science Act of 2022   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 8/27\n\nEnacted in August 2022, the CHIPS Act ('Creating Helpful Incentives to Produce Semiconductors') was a key pillar of Biden's AI policy. The headline impact of this Act was to authorise and release approximately $280 billion in spending on the hardware components and infrastructure most critical for AI development. \n\nNational AI Initiative Act of 2020 \n\nThis law was enacted in January 2021 as part of the National Defense Authorization Act (NDAA) for Fiscal Year 2021. It provided over $6 billion in funding for AI R&D, education, and standards development, with the ultimate goal of strengthening U.S. AI leadership. This included a mandate which led to NIST developing the NIST AI Risk Management Framework ( discussed below ). The Act also established the National AI Advisory Committee, a high-level group of experts which advise the President on AI policy matters. \n\nExecutive Order 13859 : Maintaining American Leadership in AI \n\nThis Executive Order was signed by President Trump in February 2019. It remains in force today.   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 9/27\n\nThe purpose of this Executive Order is to promote investment and use of AI across the federal government, as well as to â€œfacilitate AI R&Dâ€ and the development of â€œbreakthrough technologyâ€. During the Biden administration, AI governance and safety was a top policy priority. Although this appeared to signal the beginning of a shift away from the historically free market approach to technology regulation adopted by previous U.S. administrations, it did not last for too long. President Biden attempted to balance promoting U.S. global AI leadership with upholding civil liberties and protecting citizens from unfair and harmful practices. As described throughout this article, various federal initiatives designed to strengthen the U.S.â€™ global positionâ€“such as on AI export controls, investment restrictions, and AI infrastructure and manufacturing spendingâ€“were complemented with initial efforts to regulate private sector AI activities and promote responsible AI. The Blueprint for an AI Bill of Rights , for example, was developed by the White House Office of Science and Technology Policy. It outlined Bidenâ€™s AI policy vision. The \n\nBidenâ€™s AI governance agenda   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 10/27\n\nBlueprint was defined by five core principles for AI development and use: i) Safe & Effective Systems, ii) Algorithmic Discrimination Protections, iii) Data Privacy, iv) Notice & Explanation and v) Human alternatives & Fallback. The Blueprint argued that AI systems used in healthcare have â€œproven unsafe, ineffective, or biasedâ€ and algorithms used in recruitment and credit scoring â€œreflect and reproduce existing unwanted inequities or embed new harmful bias and discriminationâ€. President Biden also secured â€˜Voluntary Commitments â€™ from 15 leading AI companies. These included conducting AI model security testing and sharing and publishing information on AI safety. The core purpose of these commitments was to ensure that advanced AI models were safe before being released. Building on these initiatives, Executive Order 14110 : Safe, Secure, and Trustworthy Development and Use of AI was signed by President Biden in October 2023. This represented the most comprehensive U.S. federal AI governance initiative to date. It mandated a major programme of work, entailing over 100 specific actions across over 50 federal entities. Tangible resulting actions included the establishment of the U.S. AI Safety Institute and the publication of various NIST AI safety standards, guidelines, and toolkits ( discussed below ). Also, developers of the most powerful AI models were obliged to perform   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 11/27\n\nsafety and security testing, and report results back to the U.S. government. However, this AI model evaluation regime was never fully operationalised. President Trumpâ€™s â€˜America Firstâ€™ mantra is not just about tariffs, defence spending, and immigration; it is also relevant for AI. The AI policy ambition of Trumpâ€™s second term is to strengthen U.S. global AI leadership and dominance, promote AI innovation, and advance deregulation. Two decisive actions were taken by President Trump within days of his second term commencing. On his first day in office, 20 January 2025, President Trump signed Executive Order 14148 : Initial Rescissions of Harmful Executive Orders and Actions .The purpose of this was simple: to revoke dozens of Executive Orders and Presidential Memorandums issued by President Biden. This included revocation of Executive Order 14110 : Safe, Secure, and Trustworthy Development and Use of AI , which was the cornerstone of Bidenâ€™s AI governance agenda. \n\nTrumpâ€™s agenda: what â€˜America Firstâ€™ means for AI   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 12/27\n\nThe second decisive move came 3 days later, when President Trump signed Executive Order 14179 : Removing Barriers to American Leadership in AI. Doubling down on the revocation of Bidenâ€™s AI Executive Order, this announcement deemed the previous administrationâ€™s wider AI policy agenda as a â€œbarrier to American AI innovationâ€. The stated policy of the new administration is to â€œsustain and enhance Americaâ€™s global AI dominance in order to promote human flourishing, economic competitiveness, and national securityâ€. Trumpâ€™s Executive Order mandates formulation of an â€˜AI Action Planâ€™, by July 2025, which can achieve this policy objective. Top U.S. officials are now working on this. As part of this, the federal government will run an exercise to identify, halt, and shut down any AI-related government activities or initiatives which are deemed as contrary to achieving the policy objective stated above. This may include AI governance and safety related initiatives which were pursued following the mandate from Bidenâ€™s AI Executive Order. The development of the AI Action Plan has received significant interest, with the public consultation (which has now closed) receiving 8,755 comments in under 2 months.   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 13/27\n\nDespite the policy shift, it is important to note that the Trump administration has not dismantled Bidenâ€™s entire AI policy agenda, nor has it abandoned the concept of AI governance and risk management. For example, much of what was previously implemented on AI-related export controls and investment restrictions, as well as Bidenâ€™s Executive Order on AI Infrastructure, remains in force. Furthermore, both of the recent memoranda on federal agency use and acquisition of AI, published by the Office of Management and Budget (OMB), emphasise the importance of responsible AI adoption and sound AI governance and risk management practices. What is clear, however, is that the Trump administration has no intention to impose any major AI governance related regulations or restrictions on private sector AI development and deployment. One potential point of relative harmony between the Biden and Trump administrations is the stance on AI-related export controls and investment restrictions. \n\nAI export controls and investment restrictions   \n\n> 1/3/26, 4:50 PM U.S. AI Law & Policy Explained - by Oliver Patel\n> https://oliverpatel.substack.com/p/us-ai-law-and-policy-explained 14/27\n\nThe U.S. deems the development of AI in â€œcountries of concernâ€ as a â€œnational security threatâ€. Concerted attempts to control how, where, and at what pace AI capabilities are developed has become a fundamental element of U.S. federal AI policy and the broader mission to sustain U.S. leadership in this foundational technology. Various laws and regulations were passed during Bidenâ€™s presidency which significantly restrict which countries U.S. advanced AI computing chips and AI model weights can be exported to, as well as which countriesâ€™ AI industries U.S. persons can invest in. The key laws are summarised below: \n\nFramework for AI Diffusion \n\nAn interim final rule issued by the U.S. Department of Commerceâ€™s Bureau of Industry and Security (BIS). This is the most comprehensive U.S. regulation restricting the export of AI-related technologies. It became effective on 13 January 2025, but most requirements are not applicable until 15 May 2025, when the consultation period closes. The purpose of this rule is twofold: to ensure that a) model weights of the most advanced â€˜closedâ€™ (i.e., not open-source) U.S. AI models are only stored outside of", "fetched_at_utc": "2026-02-08T19:09:41Z", "sha256": "77b328bb624a89d0a6a2f92fd63c3b0e3eec52d835dfa04d66bae418ff299dcb", "meta": {"file_name": "U.S. AI Law & Policy Explained - Oliver Patel.pdf", "file_size": 981207, "relative_path": "pdfs\\U.S. AI Law & Policy Explained - Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 3240}}}
{"doc_id": "pdf-pdfs-what-is-president-trump-s-ai-policy-by-oliver-patel-f55c8d182549", "source_type": "local_pdf", "source": "C:\\Users\\tidemanlem\\Documents\\Course_Alexey_Grigorev\\MyAgent\\pdfs\\What is President Trump_s AI policy - by Oliver Patel.pdf", "title": "What is President Trump_s AI policy - by Oliver Patel", "text": "Hey ðŸ‘‹ \n\nIâ€™m Oliver Patel , author and creator of Enterprise AI Governance .\n\n1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel \n\nhttps://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy  2/18 On Thursday 11 December 2025, President Trump issued an Executive Order on Ensuring a National Policy Framework for AI. This represents the administrationâ€™s latest attempt at blocking and constraining how U.S. states regulate AI. This article summarises the new Executive Order, situates it in the wider context of Trumpâ€™s AI policy, assesses the challenges the administration faces in preempting state AI laws, and reflects on what companies should do next. \n\nFor a detailed, up-to-date, and visual guide to U.S. AI law and policy (covering the federal and state levels), as well as U.S, China, and EU comparison charts, sign up to secure a 25% discount for my forthcoming book, Fundamentals of AI Governance \n\n(2026). On 11 December 2025, President Trump issued a new Executive Order that attempts to block states from enforcing existing AI regulations and deter states from enacting new AI laws. Just five months after the Senate rejected the proposed 10-year moratorium on state AI laws by 99 votes to 1, the administration is having another bite of the cherryâ€”this \n\nWhat just happened?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 3/18\n\ntime through litigation, funding restrictions, and federal preemption, potentially via new legislation. However, it is important to note that the latest development is an Executive Order (i.e., a presidential directive), not legislation. Meaningfully preempting state laws in this way would require legislation, which requires approval from both the House of Representatives and the Senate. It does not appear that substantive political changes have occurred in the past few months to render this more likely than it was back in the summer. Nonetheless, this latest Executive Order is a powerful signal of intent that highlights the administration is digging in on this particular issue, despite the large-scale opposition to the previous 10-year moratorium proposal. Indeed, the administration has repeatedly stated that its AI policy objective is to â€œsustain and enhance Americaâ€™s global AI dominanceâ€. State AI laws have been in the firing line, as part of the wider focus on pursuing deregulation as a means to propel the U.S. as the worldâ€™s dominant AI power. Preemption is a U.S. constitutional principle whereby federal law (i.e., law passed by Congress) takes precedence over state law when there are conflicts between the two. When a federal law â€œpreemptsâ€ a state law, the state law is effectively nullified.   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 4/18\n\nHowever, the controversy regarding state AI law preemption is partly due to the fact that there is no comprehensive federal AI law. And the federal AI laws that are on the books cover narrower domains such as strengthening the U.S. AI industry, support and funding for AI research and infrastructure, and export controls. Where state AI laws focus on responsible AI topics like transparency or bias, there is arguably a lack of federal law to preempt these laws. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nPresident Trump signed the Executive Order: Ensuring a National Policy Framework for Artificial Intelligence on Thursday 11 December 2025, An Executive Order is a directive issued by the President to federal agencies and executive branch officials. It is issued by the President unilaterally, does not require Congressional approval and cannot, by itself, override state laws. \n\nWhat is the new Executive Order on AI?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 5/18\n\nThe core argument Trump presents in this Executive Order is that the patchwork of many state AI laws creates compliance burdens and administrative complexity that could undermine U.S. competitiveness. Having 50 different AI regulatory regimes, the Executive Order argues, â€œ makes compliance challenging, particularly for startups â€œ. Coloradoâ€™s AI lawâ€” Consumer Protections for AI (SB24-205), effective date 30 June 2026â€”is singled out for criticism, with the Executive Order claiming that such laws tackling â€œalgorithmic discriminationâ€ may force AI models to produce false results in order to avoid differential treatment of protected groups. The declared policy of the administration is to establish a â€œminimally burdensome national standardâ€ for AI, as opposed to â€œ50 discordant State onesâ€. To achieve this, the Executive Order directs a multi-pronged set of actions and broader strategy for the U.S. federal government to pursue: First, the Attorney General must establish an AI Litigation Task Force within 30 days (of the Executive Order), with the goal of challenging state AI laws in court. The focus will be on identifying state laws that are inconsistent with the U.S. AI policy of â€œsustaining and advancing U.S. global dominance in AIâ€. The grounds for legal challenge could include â€œunconstitutional regulationâ€ of interstate commerce or preemption by existing federal regulations.   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 6/18\n\nSecond, the Secretary of Commerce, in consultation with other government leaders, must publish an evaluation of existing state AI laws within 90 days (of the Executive Order), identifying â€œonerousâ€ laws that conflict with the U.S. AI policy objective mentioned above. This evaluation must focus on laws that â€œrequire AI models to alter truthful outputsâ€ or that compel organisations to â€œdisclose information in a manner that would violate the First Amendmentâ€. This is significant, as it explicitly targets laws relating to AI transparency and bias mitigation, two core threads throughout many existing state AI laws. Third, the Order imposes funding restrictions on states that are deemed to have â€œonerous AI lawsâ€. States identified through the evaluation referenced above will be ineligible for certain categories of federal funding under the Broadband Equity Access and Deployment (BEAD) Program. U.S. government executive departments and agencies are also directed to assess whether they can condition discretionary grants on states agreeing not to enforce their existing AI laws and/or not enacting new AI laws. This could result in government agencies withholding certain types of grant funding from states. Fourth, the Federal Communications Commission (FCC), the agency that regulates U.S. communications (e.g., broadcasting, internet, and telecommunications), must initiate work to determine whether to adopt a federal reporting and disclosure standard for AI models that would preempt conflicting state laws. It is important to note that the Executive Order requires the FCC to â€œinitiate a proceedingâ€; it   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 7/18\n\ndoes not require the FCC to adopt such a standard for AI transparency. However, the implied thinking is that if there is a national standard, this could be argued to supersede state AI laws covering AI reporting and disclosure. Fifth, the FTC must issue a policy statement explaining when state laws requiring the alteration of the â€œtruthful outputsâ€ are preempted by federal prohibitions on deceptive practices, which largely stem from the FTC Act that prohibits â€œunfair and deceptive practicesâ€. Again, the implication is that such guidance would strengthen the case for existing federal laws superseding state AI laws in this domain too. Finally, the administration will prepare a legislative proposal for a uniform federal AI framework that preempts state laws. However, the proposal would be narrow and would not seek to preempt state laws covering any of the following policy areas: child safety; AI compute and data centre infrastructure; state government procurement and use of AI; and other topics to be determined. This is significant as it highlights the policy areas which the administration deem to be legitimate domains of autonomous state AI lawmaking, even if the result is regulatory   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 8/18\n\ndivergence within the U.S. \n\nThanks for reading Enterprise AI Governance! Subscribe for free to receive new posts and support my work. \n\nThis latest Executive Order is not the administrationâ€™s first attempt to constrain state AI laws. Back in May 2025, the administration proposed adding a 10-year moratorium on state AI laws enforcement to Trumpâ€™s flagship domestic policy and taxation bill, the â€˜One Big Beautiful Bill Actâ€™ (H.R.1). I covered the 10-year moratorium in detail in a previous edition of Enterprise AI Governance: Unpacking the 10-year Moratorium on U.S. State AI Laws . Here is a summary of that article. The proposed moratorium was designed to prevent U.S. states from being able to enforce â€œany law or regulation limiting, restricting, or otherwise regulating AI models, AI systems, or automated decision systemsâ€ for a period of 10 years. Although this \n\nHow did Trump attempt to block state AI laws previously?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 9/18\n\nwould not technically have prevented states from introducing and passing new AI laws, the broad restriction on enforcement would have rendered doing so pointless. This was an attempt by the federal government to preempt state AI laws by blocking states from enforcing laws they had already passed, as well as any new laws they might pass in the future. The stated goal was to halt the â€œproliferation of a complex and fragmented patchwork of state AI lawsâ€, in support of AI innovation across the country. The House of Representatives voted to pass the state AI law moratorium by a narrow margin, largely along party lines, with 215 in favour and 214 against. However, following this, the moratorium was decisively rejected. In July 2025, the Senate voted 99 to 1 to remove it from the bill. This followed a significant bipartisan campaign against the provision. A June 2025 letter , signed by 260 state lawmakers, stated that â€œstates are laboratories of democracy accountable to their citizens and must maintain the flexibility to respond to new digital concernsâ€. Several Republican state governors also campaigned against the proposal. The main reason the moratorium proved so controversial was not really about AI. The case (against it) centred on the philosophical objection to the federal government constraining states in this way, particularly given that there is no comprehensive federal AI law to take precedence. The argument was that the statesâ€™ hands were being   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 10/18\n\ntied, without an alternative federal framework on the table. The fundamentals of the situation do not appear to have meaningfully changed in the months that have passed. However, the December 2025 Executive Order represents a pivot in strategy. Rather than seeking blanket preemption through legislationâ€”which requires Congressional approvalâ€”the administration is now pursuing litigation, agency action, and funding leverage. These approaches can be initiated by the executive branch alone. However, as noted above, the Presidentâ€™s legal authority to actually preempt state laws remains limited without legislation. The administration may also pursue new legislation, but this will likely face similar challenges in Congress. In lieu of comprehensive federal AI law, dozens of U.S. states have enacted AI-related laws. 131 such laws were passed between 2016 and 2024, and over 700 AI-related bills were proposed in 2024 alone. The states with the most AI-related laws are California, Colorado, Maryland, Utah, and Virginia. California has been particularly active, enacting dozens of laws that regulate AI in different ways. These laws cover themes including fairness and accountability, transparency, data privacy, deepfakes, and government use of AI. \n\nWhat AI laws do states have and will Trump succeed in blocking them?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 11/18\n\nHowever, the administration faces an uphill task in its efforts to block the enforcement and enactment of these laws. Now that the moratorium has been rejected by the Senate due to concerns regarding preemption and the ways in which the federal government can constrain the states, it is fair to argue that subsequent federal legislative proposals will be met with similar levels of scrutiny and controversy. The new Executive Order sits within a broader pro-business and pro-innovation AI policy agenda. Trump has pivoted away from the Biden administrationâ€™s AI governance and safety agenda. Promoting U.S. AI leadershipâ€”which has always been a core federal AI policy objectiveâ€”now takes centre stage. One of the first actions taken by Trump at the start of this second term, in January 2025, was to revoke various Biden-era executive orders, including the flagship Executive Order on Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. \n\nIn July 2025, the administration published Winning the Race: Americaâ€™s AI Action Plan ,which outlines how the U.S. can achieve and maintain â€œunquestioned and \n\nWhat other AI policy actions has this administration taken?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 12/18\n\nunchallenged global technological dominanceâ€ in AI. I also covered the administrationâ€™s AI Action Plan in a previous edition of Enterprise AI Governance: What Americaâ€™s AI Action Plan Means for AI Governance . A summary of that explainer piece is provided below. The AI Action Plan contains dozens of policy recommendations across three pillars: Pillar 1. Accelerate AI Innovation Pillar 2. Build American AI Infrastructure Pillar 3. Lead in International AI Diplomacy and Security Notably, under Pillar 1, the AI Action Plan recommended withholding funding for AI-related initiatives from states with â€œburdensome AI regulationsâ€. This core idea has now been operationalised through the new Executive Order and its funding restrictions. Moreover, the rejection of the 10-year moratorium by the Senate did not extinguish the underlying policy objective; rather, it has been repackaged and is now pursued through alternative means. The AI Action Plan is significant because it directly links AI regulations with the ability (or lack thereof) of companies to innovate at speed. Put simply, the Trump   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 13/18\n\nadministration believes that AI should not be constrained by regulations and that doing so would impede the U.S. economic and security prospects in a damaging way. Other recommended policy actions in the AI Action Plan included: Taking action to review and remove any existing Federal regulations that impede AI innovation. Ensure the federal government only procures â€œunbiasedâ€ and â€œideologically neutralâ€ large language models. Fast-track and streamline processes for data centre construction review, approval, and licensing. Advocate for â€œpro-innovationâ€ approaches to international AI governance, that reflect â€œAmerican valuesâ€ and shift away from â€œburdensome regulationsâ€. Other notable AI policy measures pursued by this administration include Executive Orders on advancing AI education for American youth (April 2025), accelerating federal permitting for data centre infrastructure (July 2025), preventing â€œwoke AIâ€ in federal government procurement (July 2025), and promoting the export of the American AI technology stack (July 2025). The TAKE IT DOWN Act, which criminalises the publication of non-consensual intimate deepfakes, was signed into law in May 2025. \n\nWhat should companies operating in the U.S. do now?   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 14/18\n\nIf you think AI governance is not relevant for your organisation because the current administration is pro-AI and against restrictive AI regulation, you could be in for a rude awakening if things go wrong. Deregulation does not mean that AI risks no longer apply to you or that you are not exposed. Indeed, the AI Action Plan itself highlights various AI-related risks that could slow innovation, including interpretability, robustness, and misalignment. Furthermore, all enterprises using generative AI at scale are exposed to a litany of (relatively novel) data-related risks. I outlined these in my article on the PROTECT Framework: Managing Data Risks in the AI Era .The PROTECT Framework empowers you to understand, map, and mitigate the most pertinent data risks that are fuelled by widespread adoption of generative AI, covering themes such as public AI tool usage, rogue internal AI projects, opportunistic vendors, and compliance and copyright breaches. These risks cannot be mitigated without a robust approach to AI governance, which serves as a reminder that AI-specific regulatory compliance is not the sole driver for enterprise AI governance. Moreover, even the White Houseâ€™s Office of Management and Budget (OMB) describes effective AI governance as â€œkey to accelerated innovationâ€. Indeed, the AI governance framework that OMB directs federal agencies to implementâ€”covering AI   \n\n> 1/3/26, 5:09 PM What is President Trump's AI policy? - by Oliver Patel\n> https://oliverpatel.substack.com/p/what-is-president-trumps-ai-policy 15/18\n\ndevelopment, deployment, procurement, and useâ€”is robust. This suggests that although the U.S. government does not want there to be any regulatory measures getting in the way of U.S. companiesâ€™ AI activities, it nonetheless recognises that a well-designed and proportionate AI governance framework is important for both risk mitigation and value generation, especially in sensitive domains. For U.S. companies, the reality today is the same as it was yesterday. There still exists a complex patchwork of many state AI laws to contend with, as well as federal and state laws that meaningfully regulate or implicate AI in specific ways, such as privacy, copyright, employment, and consumer protection laws. Given the complexity of developing different internal AI governance frameworks for different jurisdictions, I always recommend having a company-wide AI governance framework that promotes and facilitates compliance, risk management, and AI-enablement across all the important jurisdictions you operate in. \n\nThanks for reading! Subscribe below for weekly updates from Enterprise AI Governance.", "fetched_at_utc": "2026-02-08T19:09:43Z", "sha256": "f55c8d182549c50122606bab0bceb73884ac63ec81696ee81822147dc1eab25f", "meta": {"file_name": "What is President Trump_s AI policy - by Oliver Patel.pdf", "file_size": 809321, "relative_path": "pdfs\\What is President Trump_s AI policy - by Oliver Patel.pdf", "jina_status": 20000, "jina_code": 200, "usage": {"tokens": 4061}}}
